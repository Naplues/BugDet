File,Bug,SRC
tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.cli;

import javax.xml.transform.OutputKeys;
import javax.xml.transform.TransformerConfigurationException;
import javax.xml.transform.sax.SAXTransformerFactory;
import javax.xml.transform.sax.TransformerHandler;
import javax.xml.transform.stream.StreamResult;
import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.io.OutputStreamWriter;
import java.io.PrintStream;
import java.io.PrintWriter;
import java.io.UnsupportedEncodingException;
import java.io.Writer;
import java.lang.reflect.Field;
import java.net.ServerSocket;
import java.net.Socket;
import java.net.URI;
import java.net.URL;
import java.nio.charset.Charset;
import java.util.Arrays;
import java.util.Comparator;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Locale;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Set;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.log4j.BasicConfigurator;
import org.apache.log4j.Level;
import org.apache.log4j.Logger;
import org.apache.log4j.SimpleLayout;
import org.apache.log4j.WriterAppender;
import org.apache.poi.poifs.filesystem.DirectoryEntry;
import org.apache.poi.poifs.filesystem.DocumentEntry;
import org.apache.poi.poifs.filesystem.DocumentInputStream;
import org.apache.poi.poifs.filesystem.POIFSFileSystem;
import org.apache.tika.Tika;
import org.apache.tika.config.TikaConfig;
import org.apache.tika.detect.CompositeDetector;
import org.apache.tika.detect.DefaultDetector;
import org.apache.tika.detect.Detector;
import org.apache.tika.exception.TikaException;
import org.apache.tika.extractor.EmbeddedDocumentExtractor;
import org.apache.tika.fork.ForkParser;
import org.apache.tika.gui.TikaGUI;
import org.apache.tika.io.CloseShieldInputStream;
import org.apache.tika.io.FilenameUtils;
import org.apache.tika.io.IOUtils;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.language.LanguageProfilerBuilder;
import org.apache.tika.language.ProfilingHandler;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.serialization.JsonMetadata;
import org.apache.tika.metadata.serialization.JsonMetadataList;
import org.apache.tika.mime.MediaType;
import org.apache.tika.mime.MediaTypeRegistry;
import org.apache.tika.mime.MimeTypeException;
import org.apache.tika.parser.AutoDetectParser;
import org.apache.tika.parser.CompositeParser;
import org.apache.tika.parser.NetworkParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.Parser;
import org.apache.tika.parser.ParserDecorator;
import org.apache.tika.parser.PasswordProvider;
import org.apache.tika.parser.RecursiveParserWrapper;
import org.apache.tika.parser.html.BoilerpipeContentHandler;
import org.apache.tika.sax.BasicContentHandlerFactory;
import org.apache.tika.sax.BodyContentHandler;
import org.apache.tika.sax.ContentHandlerFactory;
import org.apache.tika.sax.ExpandedTitleContentHandler;
import org.apache.tika.xmp.XMPMetadata;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.DefaultHandler;

/**
 * Simple command line interface for Apache Tika.
 */
public class TikaCLI {
    private File extractDir = new File(".");

    private static final Log logger = LogFactory.getLog(TikaCLI.class);

    public static void main(String[] args) throws Exception {
        BasicConfigurator.configure(
                new WriterAppender(new SimpleLayout(), System.err));
        Logger.getRootLogger().setLevel(Level.INFO);

        TikaCLI cli = new TikaCLI();
        if (args.length > 0) {
            for (int i = 0; i < args.length; i++) {
                cli.process(args[i]);
            }
            if (cli.pipeMode) {
                cli.process("-");
            }
        } else {
            // Started with no arguments. Wait for up to 0.1s to see if
            // we have something waiting in standard input and use the
            // pipe mode if we have. If no input is seen, start the GUI.
            if (System.in.available() == 0) {
                Thread.sleep(100);
            }
            if (System.in.available() > 0) {
                cli.process("-");
            } else {
                cli.process("--gui");
            }
        }
    }

    private class OutputType {

        public void process(
                InputStream input, OutputStream output, Metadata metadata)
                throws Exception {
            Parser p = parser;
            if (fork) {
                p = new ForkParser(TikaCLI.class.getClassLoader(), p);
            }
            ContentHandler handler = getContentHandler(output, metadata);
            p.parse(input, handler, metadata, context);
            // fix for TIKA-596: if a parser doesn't generate
            // XHTML output, the lack of an output document prevents
            // metadata from being output: this fixes that
            if (handler instanceof NoDocumentMetHandler){
                NoDocumentMetHandler metHandler = (NoDocumentMetHandler)handler;
                if(!metHandler.metOutput()){
                    metHandler.endDocument();
                }
            }
        }

        protected ContentHandler getContentHandler(
                OutputStream output, Metadata metadata) throws Exception {
            throw new UnsupportedOperationException();
        }
        
    }

    private final OutputType XML = new OutputType() {
        @Override
        protected ContentHandler getContentHandler(
                OutputStream output, Metadata metadata) throws Exception {
            return getTransformerHandler(output, "xml", encoding, prettyPrint);
        }
    };

    private final OutputType HTML = new OutputType() {
        @Override
        protected ContentHandler getContentHandler(
                OutputStream output, Metadata metadata) throws Exception {
            return new ExpandedTitleContentHandler(getTransformerHandler(output, "html", encoding, prettyPrint));
        }
    };

    private final OutputType TEXT = new OutputType() {
        @Override
        protected ContentHandler getContentHandler(
                OutputStream output, Metadata metadata) throws Exception {
            return new BodyContentHandler(getOutputWriter(output, encoding));
        }
    };

    private final OutputType NO_OUTPUT = new OutputType() {
        @Override
        protected ContentHandler getContentHandler(
                OutputStream output, Metadata metadata) {
            return new DefaultHandler();
        }
    };

    private final OutputType TEXT_MAIN = new OutputType() {
        @Override
        protected ContentHandler getContentHandler(
                OutputStream output, Metadata metadata) throws Exception {
            return new BoilerpipeContentHandler(getOutputWriter(output, encoding));
        }
    };
    
    private final OutputType METADATA = new OutputType() {
        @Override
        protected ContentHandler getContentHandler(
                OutputStream output, Metadata metadata) throws Exception {
            final PrintWriter writer =
                new PrintWriter(getOutputWriter(output, encoding));
            return new NoDocumentMetHandler(metadata, writer);
        }
    };

    private final OutputType JSON = new OutputType() {
        @Override
        protected ContentHandler getContentHandler(
                OutputStream output, Metadata metadata) throws Exception {
            final PrintWriter writer =
                    new PrintWriter(getOutputWriter(output, encoding));
            return new NoDocumentJSONMetHandler(metadata, writer);
        }
    };

    private final OutputType XMP = new OutputType() {
        @Override
        protected ContentHandler getContentHandler(
                OutputStream output, final Metadata metadata) throws Exception {
            final PrintWriter writer =
                    new PrintWriter(getOutputWriter(output, encoding));
            return new NoDocumentXMPMetaHandler(metadata, writer);
        }
    };

    private final OutputType LANGUAGE = new OutputType() {
        @Override
        protected ContentHandler getContentHandler(
                OutputStream output, Metadata metadata) throws Exception {
            final PrintWriter writer =
                new PrintWriter(getOutputWriter(output, encoding));
            return new ProfilingHandler() {
                public void endDocument() {
                    writer.println(getLanguage().getLanguage());
                    writer.flush();
                }
            };
        }
    };

    private final OutputType DETECT = new OutputType() {
        @Override
        public void process(
                InputStream stream, OutputStream output, Metadata metadata)
                throws Exception {
            PrintWriter writer =
                new PrintWriter(getOutputWriter(output, encoding));
            writer.println(detector.detect(stream, metadata).toString());
            writer.flush();
        }
    };
    
    
    /* Creates ngram profile */
    private final OutputType CREATE_PROFILE = new OutputType() {
        @Override
        public void process(
                InputStream stream, OutputStream output, Metadata metadata)
                throws Exception {
            ngp = LanguageProfilerBuilder.create(profileName, stream, encoding);
            FileOutputStream fos = new FileOutputStream(new File(profileName + ".ngp"));
            ngp.save(fos);//saves ngram profile
            fos.close();
            PrintWriter writer = new PrintWriter(getOutputWriter(output, encoding));
            writer.println("ngram profile location:=" + new File(ngp.getName()).getCanonicalPath());
            writer.flush();
        }
    };

    private ParseContext context;
    
    private Detector detector;

    private Parser parser;

    private String configFilePath;

    private OutputType type = XML;

    private boolean recursiveJSON = false;
    
    private LanguageProfilerBuilder ngp = null;

    /**
     * Output character encoding, or <code>null</code> for platform default
     */
    private String encoding = null;

    /**
     * Password for opening encrypted documents, or <code>null</code>.
     */
    private String password = System.getenv("TIKA_PASSWORD");

    private boolean pipeMode = true;

    private boolean serverMode = false;

    private boolean fork = false;

    private String profileName = null;

    private boolean prettyPrint;
    
    public TikaCLI() throws Exception {
        context = new ParseContext();
        detector = new DefaultDetector();
        parser = new AutoDetectParser(detector);
        context.set(Parser.class, parser);
        context.set(PasswordProvider.class, new PasswordProvider() {
            public String getPassword(Metadata metadata) {
                return password;
            }
        });
    }

    public void process(String arg) throws Exception {
        if (arg.equals("-?") || arg.equals("--help")) {
            pipeMode = false;
            usage();
        } else if (arg.equals("-V") || arg.equals("--version")) {
            pipeMode = false;
            version();
        } else if (arg.equals("-v") || arg.equals("--verbose")) {
            Logger.getRootLogger().setLevel(Level.DEBUG);
        } else if (arg.equals("-g") || arg.equals("--gui")) {
            pipeMode = false;
            if (configFilePath != null){
                TikaGUI.main(new String[]{configFilePath});
            } else {
                TikaGUI.main(new String[0]);
            }
        } else if (arg.equals("--list-parser") || arg.equals("--list-parsers")) {
            pipeMode = false;
            displayParsers(false, false);
        } else if (arg.equals("--list-detector") || arg.equals("--list-detectors")) {
           pipeMode = false;
           displayDetectors();
        } else if (arg.equals("--list-parser-detail") || arg.equals("--list-parser-details")) {
            pipeMode = false;
            displayParsers(true, false);
        } else if (arg.equals("--list-parser-detail-apt") || arg.equals("--list-parser-details-apt")) {
            pipeMode = false;
            displayParsers(true, true);
        } else if(arg.equals("--list-met-models")){
            pipeMode = false;
            displayMetModels();
        } else if(arg.equals("--list-supported-types")){
            pipeMode = false;
            displaySupportedTypes();
        } else if (arg.equals("--container-aware")
                || arg.equals("--container-aware-detector")) {
            // ignore, as container-aware detectors are now always used
        } else if (arg.equals("-f") || arg.equals("--fork")) {
            fork = true;
        } else if (arg.startsWith("--config=")) {
            configure(arg.substring("--config=".length()));
        } else if (arg.startsWith("-e")) {
            encoding = arg.substring("-e".length());
        } else if (arg.startsWith("--encoding=")) {
            encoding = arg.substring("--encoding=".length());
        } else if (arg.startsWith("-p") && !arg.equals("-p")) {
            password = arg.substring("-p".length());
        } else if (arg.startsWith("--password=")) {
            password = arg.substring("--password=".length());
        } else  if (arg.equals("-j") || arg.equals("--json")) {
            type = JSON;
        } else if (arg.equals("-J") || arg.equals("--jsonRecursive")) {
            recursiveJSON = true;
        } else if (arg.equals("-y") || arg.equals("--xmp")) {
            type = XMP;
        } else if (arg.equals("-x") || arg.equals("--xml")) {
            type = XML;
        } else if (arg.equals("-h") || arg.equals("--html")) {
            type = HTML;
        } else if (arg.equals("-t") || arg.equals("--text")) {
            type = TEXT;
        } else if (arg.equals("-T") || arg.equals("--text-main")) {
            type = TEXT_MAIN;
        } else if (arg.equals("-m") || arg.equals("--metadata")) {
            type = METADATA;
        } else if (arg.equals("-l") || arg.equals("--language")) {
            type = LANGUAGE;
        } else if (arg.equals("-d") || arg.equals("--detect")) {
            type = DETECT;
        } else if (arg.startsWith("--extract-dir=")) {
            extractDir = new File(arg.substring("--extract-dir=".length()));
        } else if (arg.equals("-z") || arg.equals("--extract")) {
            type = NO_OUTPUT;
            context.set(EmbeddedDocumentExtractor.class, new FileEmbeddedDocumentExtractor());
        } else if (arg.equals("-r") || arg.equals("--pretty-print")) {
            prettyPrint = true;
        } else if (arg.equals("-p") || arg.equals("--port")
                || arg.equals("-s") || arg.equals("--server")) {
            serverMode = true;
            pipeMode = false;
        } else if (arg.startsWith("-c")) {
            URI uri = new URI(arg.substring("-c".length()));
            parser = new NetworkParser(uri);
        } else if (arg.startsWith("--client=")) {
            URI uri = new URI(arg.substring("--client=".length()));
            parser = new NetworkParser(uri);
        } else if(arg.startsWith("--create-profile=")){
            profileName = arg.substring("--create-profile=".length());
            type = CREATE_PROFILE;
        } else {
            pipeMode = false;
            if (serverMode) {
                new TikaServer(Integer.parseInt(arg)).start();
            } else if (arg.equals("-")) {
                InputStream stream =
                    TikaInputStream.get(new CloseShieldInputStream(System.in));
                try {
                    type.process(stream, System.out, new Metadata());
                } finally {
                    stream.close();
                }
            } else {
                URL url;
                File file = new File(arg);
                if (file.isFile()) {
                    url = file.toURI().toURL();
                } else {
                    url = new URL(arg);
                }
                if (recursiveJSON) {
                    handleRecursiveJson(url, System.out);
                } else {
                    Metadata metadata = new Metadata();
                    InputStream input = TikaInputStream.get(url, metadata);
                    try {
                        type.process(input, System.out, metadata);
                    } finally {
                        input.close();
                        System.out.flush();
                    }
                }
            }
        }
    }

    private void handleRecursiveJson(URL url, OutputStream output) throws IOException, SAXException, TikaException {
        Metadata metadata = new Metadata();
        InputStream input = TikaInputStream.get(url, metadata);
        RecursiveParserWrapper wrapper = new RecursiveParserWrapper(parser, getContentHandlerFactory(type));
        try {
            wrapper.parse(input, null, metadata, context);
        } finally {
            input.close();
        }
        JsonMetadataList.setPrettyPrinting(prettyPrint);
        Writer writer = getOutputWriter(output, encoding);
        try {
            JsonMetadataList.toJson(wrapper.getMetadata(), writer);
        } finally {
            writer.flush();
        }
    }

    private ContentHandlerFactory getContentHandlerFactory(OutputType type) {
        BasicContentHandlerFactory.HANDLER_TYPE handlerType = BasicContentHandlerFactory.HANDLER_TYPE.IGNORE;
        if (type.equals(HTML)) {
            handlerType = BasicContentHandlerFactory.HANDLER_TYPE.HTML;
        } else if (type.equals(XML)) {
            handlerType = BasicContentHandlerFactory.HANDLER_TYPE.XML;
        } else if (type.equals(TEXT)) {
            handlerType = BasicContentHandlerFactory.HANDLER_TYPE.TEXT;
        } else if (type.equals(TEXT_MAIN)) {
            handlerType = BasicContentHandlerFactory.HANDLER_TYPE.BODY;
        } else if (type.equals(METADATA)) {
            handlerType = BasicContentHandlerFactory.HANDLER_TYPE.IGNORE;
        }
        return new BasicContentHandlerFactory(handlerType, -1);
    }
    private void usage() {
        PrintStream out = System.out;
        out.println("usage: java -jar tika-app.jar [option...] [file|port...]");
        out.println();
        out.println("Options:");
        out.println("    -?  or --help          Print this usage message");
        out.println("    -v  or --verbose       Print debug level messages");
        out.println("    -V  or --version       Print the Apache Tika version number");
        out.println();
        out.println("    -g  or --gui           Start the Apache Tika GUI");
        out.println("    -s  or --server        Start the Apache Tika server");
        out.println("    -f  or --fork          Use Fork Mode for out-of-process extraction");
        out.println();
        out.println("    --config=<tika-config.xml>");
        out.println("        TikaConfig file. Must be specified before -g, -s or -f!");
        out.println("");
        out.println("    -x  or --xml           Output XHTML content (default)");
        out.println("    -h  or --html          Output HTML content");
        out.println("    -t  or --text          Output plain text content");
        out.println("    -T  or --text-main     Output plain text content (main content only)");
        out.println("    -m  or --metadata      Output only metadata");
        out.println("    -j  or --json          Output metadata in JSON");
        out.println("    -y  or --xmp           Output metadata in XMP");
        out.println("    -J  or --jsonRecursive Output metadata and content from all");
        out.println("                           embedded files (choose content type");
        out.println("                           with -x, -h, -t or -m; default is -x)");
        out.println("    -l  or --language      Output only language");
        out.println("    -d  or --detect        Detect document type");
        out.println("    -eX or --encoding=X    Use output encoding X");
        out.println("    -pX or --password=X    Use document password X");
        out.println("    -z  or --extract       Extract all attachements into current directory");
        out.println("    --extract-dir=<dir>    Specify target directory for -z");
        out.println("    -r  or --pretty-print  For JSON, XML and XHTML outputs, adds newlines and");
        out.println("                           whitespace, for better readability");
        out.println();
        out.println("    --create-profile=X");
        out.println("         Create NGram profile, where X is a profile name");
        out.println("    --list-parsers");
        out.println("         List the available document parsers");
        out.println("    --list-parser-details");
        out.println("         List the available document parsers and their supported mime types");
        out.println("    --list-parser-details-apt");
        out.println("         List the available document parsers and their supported mime types in apt format.");
        out.println("    --list-detectors");
        out.println("         List the available document detectors");
        out.println("    --list-met-models");
        out.println("         List the available metadata models, and their supported keys");
        out.println("    --list-supported-types");
        out.println("         List all known media types and related information");
        out.println();
        out.println("Description:");
        out.println("    Apache Tika will parse the file(s) specified on the");
        out.println("    command line and output the extracted text content");
        out.println("    or metadata to standard output.");
        out.println();
        out.println("    Instead of a file name you can also specify the URL");
        out.println("    of a document to be parsed.");
        out.println();
        out.println("    If no file name or URL is specified (or the special");
        out.println("    name \"-\" is used), then the standard input stream");
        out.println("    is parsed. If no arguments were given and no input");
        out.println("    data is available, the GUI is started instead.");
        out.println();
        out.println("- GUI mode");
        out.println();
        out.println("    Use the \"--gui\" (or \"-g\") option to start the");
        out.println("    Apache Tika GUI. You can drag and drop files from");
        out.println("    a normal file explorer to the GUI window to extract");
        out.println("    text content and metadata from the files.");
        out.println();
        out.println("- Server mode");
        out.println();
        out.println("    Use the \"--server\" (or \"-s\") option to start the");
        out.println("    Apache Tika server. The server will listen to the");
        out.println("    ports you specify as one or more arguments.");
        out.println();
    }

    private void version() {
        System.out.println(new Tika().toString());
    }


    private void configure(String configFilePath) throws Exception {
        this.configFilePath = configFilePath;
        TikaConfig config = new TikaConfig(new File(configFilePath));
        parser = new AutoDetectParser(config);
        detector = config.getDetector();
        context.set(Parser.class, parser);
    }

    private void displayMetModels(){
        Class<?>[] modelClasses = Metadata.class.getInterfaces();
        Arrays.sort(modelClasses, new Comparator<Class<?>>() {
            public int compare(Class<?> o1, Class<?> o2) {
                return o1.getName().compareTo(o2.getName());
            }
        });

        for (Class<?> modelClass: modelClasses) {
            // we don't care about internal Tika met classes
            // if we do, then we can take this conditional out
            if (!modelClass.getSimpleName().contains("Tika")) {
                System.out.println(modelClass.getSimpleName());
                Field[] keyFields = modelClass.getFields();
                Arrays.sort(keyFields, new Comparator<Field>() {
                    public int compare(Field o1, Field o2) {
                        return o1.getName().compareTo(o2.getName());
                    }
                });
                for (Field keyField: keyFields) {
                    System.out.println(" "+keyField.getName());
                }
            }
        }
    }

    /*
     * Displays loaded parsers and their mime types
     * If a parser is a composite parser, it will list the
     * sub parsers and their mime-types.
     */
    private void displayParsers(boolean includeMimeTypes, boolean aptListFormat) {
        displayParser(parser, includeMimeTypes, aptListFormat, 3);
    }
     
    private void displayParser(Parser p, boolean includeMimeTypes, boolean apt, int i) {
        boolean isComposite = (p instanceof CompositeParser);
        String name = (p instanceof ParserDecorator) ?
                      ((ParserDecorator) p).getWrappedParser().getClass().getName() :
                      p.getClass().getName();
        if (apt){
            name = name.substring(0, name.lastIndexOf(".") + 1) + "{{{./api/" + name.replace(".", "/") + "}" + name.substring(name.lastIndexOf(".") + 1) + "}}";
        }
        if ((apt && !isComposite) || !apt) {    // Don't display Composite parsers in the apt output.
            System.out.println(indent(i) + ((apt) ? "* " : "") + name + (isComposite ? " (Composite Parser):" : ""));
            if (apt) System.out.println();
            if (includeMimeTypes && !isComposite) {
                for (MediaType mt : p.getSupportedTypes(context)) {
                    System.out.println(indent(i + 3) + ((apt) ? "* " : "") + mt);
                    if (apt) System.out.println();
                }
            }
        }
        
        if (isComposite) {
            Parser[] subParsers = sortParsers(invertMediaTypeMap(((CompositeParser) p).getParsers()));
            for(Parser sp : subParsers) {
                displayParser(sp, includeMimeTypes, apt, i + ((apt) ? 0 : 3));  // Don't indent for Composites in apt.
            }
        }
    }

    /*
     * Displays loaded detectors and their mime types
     * If a detector is a composite detector, it will list the
     *  sub detectors.
     */
    private void displayDetectors() {
        displayDetector(detector, 0);
    }
     
    private void displayDetector(Detector d, int i) {
        boolean isComposite = (d instanceof CompositeDetector);
        String name = d.getClass().getName();
        System.out.println(indent(i) + name + (isComposite ? " (Composite Detector):" : ""));
        if (isComposite) {
            List<Detector> subDetectors = ((CompositeDetector)d).getDetectors();
            for(Detector sd : subDetectors) {
                displayDetector(sd, i+2);
            }
        }
    }

    private String indent(int indent) {
        return "                     ".substring(0, indent);
    }

    private Parser[] sortParsers(Map<Parser, Set<MediaType>> parsers) {
        // Get a nicely sorted list of the parsers
        Parser[] sortedParsers = parsers.keySet().toArray(new Parser[parsers.size()]);
        Arrays.sort(sortedParsers, new Comparator<Parser>() {
            public int compare(Parser p1, Parser p2) {
                String name1 = p1.getClass().getName();
                String name2 = p2.getClass().getName();
                return name1.compareTo(name2);
            }
        });
        return sortedParsers;
    }

    private Map<Parser, Set<MediaType>> invertMediaTypeMap(Map<MediaType, Parser> supported) {
        Map<Parser,Set<MediaType>> parsers = new HashMap<Parser, Set<MediaType>>();
        for(Entry<MediaType, Parser> e : supported.entrySet()) {
            if (!parsers.containsKey(e.getValue())) {
                parsers.put(e.getValue(), new HashSet<MediaType>());
            }
            parsers.get(e.getValue()).add(e.getKey());
        }
        return parsers;
    }

    /**
     * Prints all the known media types, aliases and matching parser classes.
     */
    private void displaySupportedTypes() {
        AutoDetectParser parser = new AutoDetectParser();
        MediaTypeRegistry registry = parser.getMediaTypeRegistry();
        Map<MediaType, Parser> parsers = parser.getParsers();

        for (MediaType type : registry.getTypes()) {
            System.out.println(type);
            for (MediaType alias : registry.getAliases(type)) {
                System.out.println("  alias:     " + alias);
            }
            MediaType supertype = registry.getSupertype(type);
            if (supertype != null) {
                System.out.println("  supertype: " + supertype);
            }
            Parser p = parsers.get(type);
            if (p != null) {
                if (p instanceof CompositeParser) {
                    p = ((CompositeParser)p).getParsers().get(type);
                }
                System.out.println("  parser:    " + p.getClass().getName());
            }
        }
    }

    /**
     * Returns a output writer with the given encoding.
     *
     * @see <a href="https://issues.apache.org/jira/browse/TIKA-277">TIKA-277</a>
     * @param output output stream
     * @param encoding output encoding,
     *                 or <code>null</code> for the platform default
     * @return output writer
     * @throws UnsupportedEncodingException
     *         if the given encoding is not supported
     */
    private static Writer getOutputWriter(OutputStream output, String encoding)
            throws UnsupportedEncodingException {
        if (encoding != null) {
            return new OutputStreamWriter(output, encoding);
        } else if (System.getProperty("os.name")
                .toLowerCase(Locale.ROOT).startsWith("mac os x")) {
            // TIKA-324: Override the default encoding on Mac OS X
            return new OutputStreamWriter(output, "UTF-8");
        } else {
            return new OutputStreamWriter(output, Charset.defaultCharset());
        }
    }

    /**
     * Returns a transformer handler that serializes incoming SAX events
     * to XHTML or HTML (depending the given method) using the given output
     * encoding.
     *
     * @see <a href="https://issues.apache.org/jira/browse/TIKA-277">TIKA-277</a>
     * @param output output stream
     * @param method "xml" or "html"
     * @param encoding output encoding,
     *                 or <code>null</code> for the platform default
     * @return {@link System#out} transformer handler
     * @throws TransformerConfigurationException
     *         if the transformer can not be created
     */
    private static TransformerHandler getTransformerHandler(
            OutputStream output, String method, String encoding, boolean prettyPrint)
            throws TransformerConfigurationException {
        SAXTransformerFactory factory = (SAXTransformerFactory)
                SAXTransformerFactory.newInstance();
        TransformerHandler handler = factory.newTransformerHandler();
        handler.getTransformer().setOutputProperty(OutputKeys.METHOD, method);
        handler.getTransformer().setOutputProperty(OutputKeys.INDENT, prettyPrint ? "yes" : "no");
        if (encoding != null) {
            handler.getTransformer().setOutputProperty(
                    OutputKeys.ENCODING, encoding);
        }
        handler.setResult(new StreamResult(output));
        return handler;
    }

    private class FileEmbeddedDocumentExtractor
            implements EmbeddedDocumentExtractor {

        private int count = 0;
        private final TikaConfig config = TikaConfig.getDefaultConfig();

        public boolean shouldParseEmbedded(Metadata metadata) {
            return true;
        }

        public void parseEmbedded(InputStream inputStream, ContentHandler contentHandler, Metadata metadata, boolean outputHtml) throws SAXException, IOException {
            String name = metadata.get(Metadata.RESOURCE_NAME_KEY);

            if (name == null) {
                name = "file" + count++;
            }

            MediaType contentType = detector.detect(inputStream, metadata);

            if (name.indexOf('.')==-1 && contentType!=null) {
                try {
                    name += config.getMimeRepository().forName(
                            contentType.toString()).getExtension();
                } catch (MimeTypeException e) {
                    e.printStackTrace();
                }
            }

            String relID = metadata.get(Metadata.EMBEDDED_RELATIONSHIP_ID);
            if (relID != null && !name.startsWith(relID)) {
                name = relID + "_" + name;
            }

            File outputFile = new File(extractDir, FilenameUtils.normalize(name));
            File parent = outputFile.getParentFile();
            if (!parent.exists()) {
                if (!parent.mkdirs()) {
                    throw new IOException("unable to create directory \"" + parent + "\"");
                }
            }
            System.out.println("Extracting '"+name+"' ("+contentType+") to " + outputFile);

            FileOutputStream os = null;

            try {
                os = new FileOutputStream(outputFile);

                if (inputStream instanceof TikaInputStream) {
                    TikaInputStream tin = (TikaInputStream) inputStream;

                    if (tin.getOpenContainer() != null && tin.getOpenContainer() instanceof DirectoryEntry) {
                        POIFSFileSystem fs = new POIFSFileSystem();
                        copy((DirectoryEntry) tin.getOpenContainer(), fs.getRoot());
                        fs.writeFilesystem(os);
                    } else {
                        IOUtils.copy(inputStream, os);
                    }
                } else {
                    IOUtils.copy(inputStream, os);
                }
            } catch (Exception e) {
                //
                // being a CLI program messages should go to the stderr too
                //
                String msg = String.format(
                    Locale.ROOT,
                    "Ignoring unexpected exception trying to save embedded file %s (%s)",
                    name,
                    e.getMessage()
                );
                System.err.println(msg);
                logger.warn(msg, e);
            } finally {
                if (os != null) {
                    os.close();
                }
            }
        }

        protected void copy(DirectoryEntry sourceDir, DirectoryEntry destDir)
                throws IOException {
            for (org.apache.poi.poifs.filesystem.Entry entry : sourceDir) {
                if (entry instanceof DirectoryEntry) {
                    // Need to recurse
                    DirectoryEntry newDir = destDir.createDirectory(entry.getName());
                    copy((DirectoryEntry) entry, newDir);
                } else {
                    // Copy entry
                    InputStream contents = new DocumentInputStream((DocumentEntry) entry);
                    try {
                        destDir.createDocument(entry.getName(), contents);
                    } finally {
                        contents.close();
                    }
                }
            }
        }
    }

    private class TikaServer extends Thread {

        private final ServerSocket server;

        public TikaServer(int port) throws IOException {
            super("Tika server at port " + port);
            server = new ServerSocket(port);
        }

        @Override
        public void run() {
            try {
                try {
                    while (true) {
                        processSocketInBackground(server.accept());
                    }
                } finally {
                    server.close();
                }
            } catch (IOException e) { 
                e.printStackTrace();
            }
        }

        private void processSocketInBackground(final Socket socket) {
            Thread thread = new Thread() {
                @Override
                public void run() {
                    try {
                        InputStream input = null;
                        try {
                            InputStream rawInput = socket.getInputStream();
                            OutputStream output = socket.getOutputStream();
                            input = TikaInputStream.get(rawInput);
                            type.process(input, output, new Metadata());
                            output.flush();
                        } finally {
                            if (input != null) {
                                input.close();
                            }
                            socket.close();
                        }
                    } catch (Exception e) {
                        e.printStackTrace();
                    }
                }
            };
            thread.setDaemon(true);
            thread.start();
        }

    }
    
    private class NoDocumentMetHandler extends DefaultHandler {

        protected final Metadata metadata;

        protected PrintWriter writer;
        
        private boolean metOutput;

        public NoDocumentMetHandler(Metadata metadata, PrintWriter writer){
            this.metadata = metadata;
            this.writer = writer;
            this.metOutput = false;
        }
        
        @Override
        public void endDocument() {
            String[] names = metadata.names();
            Arrays.sort(names);
            outputMetadata(names);
            writer.flush();
            this.metOutput = true;
        }
        
        public void outputMetadata(String[] names) {
           for (String name : names) {
              for(String value : metadata.getValues(name)) {
                 writer.println(name + ": " + value);
              }
           }
        }
        
        public boolean metOutput(){
            return this.metOutput;
        }
        
    }

    /**
     * Outputs the Tika metadata as XMP using the Tika XMP module
     */
    private class NoDocumentXMPMetaHandler extends DefaultHandler
    {
    	protected final Metadata metadata;
    	
        protected PrintWriter writer;
        
        public NoDocumentXMPMetaHandler(Metadata metadata, PrintWriter writer){
        	this.metadata = metadata;
            this.writer = writer;
        }
        
        @Override
        public void endDocument() throws SAXException 
        {
        	try 
        	{
        		XMPMetadata xmp = new XMPMetadata(metadata);
        		String result;
        		result = xmp.toString();
        		writer.write(result);
        		writer.flush();
        	} 
        	catch (TikaException e) 
        	{
        		throw new SAXException(e);
        	}
        }
    }

    private class NoDocumentJSONMetHandler extends DefaultHandler {

        protected final Metadata metadata;
        
        protected PrintWriter writer;

        public NoDocumentJSONMetHandler(Metadata metadata, PrintWriter writer) {
            this.metadata = metadata;
            this.writer = writer;
        }
        
        @Override
        public void endDocument() throws SAXException {
            try {
                JsonMetadata.setPrettyPrinting(prettyPrint);
                JsonMetadata.toJson(metadata, writer);
                writer.flush();
            } catch (TikaException e) {
                throw new SAXException(e);
            }
        }        
    }
}
"
tika-app/src/main/java/org/apache/tika/gui/ParsingTransferHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.gui;

import java.awt.datatransfer.Clipboard;
import java.awt.datatransfer.DataFlavor;
import java.awt.datatransfer.Transferable;
import java.awt.event.InputEvent;
import java.io.File;
import java.net.URI;
import java.net.URL;
import java.util.ArrayList;
import java.util.List;
import java.util.StringTokenizer;

import javax.swing.Icon;
import javax.swing.JComponent;
import javax.swing.TransferHandler;

/**
 * Utility class that turns drag-and-drop events into Tika parse requests.
 */
class ParsingTransferHandler extends TransferHandler {

    /**
     * Serial version UID.
     */
    private static final long serialVersionUID = -557932290014044494L;

    private final TransferHandler delegate;

    private final TikaGUI tika;

    private static DataFlavor uriListFlavor;
    private static DataFlavor urlListFlavor;
    static {
         try {
             uriListFlavor = new DataFlavor("text/uri-list;class=java.lang.String");
             urlListFlavor = new DataFlavor("text/plain;class=java.lang.String");
         } catch (ClassNotFoundException e) {
         }
    }

    public ParsingTransferHandler(TransferHandler delegate, TikaGUI tika) {
        this.delegate = delegate;
        this.tika = tika;
    }

    public boolean canImport(JComponent component, DataFlavor[] flavors) {
        for (DataFlavor flavor : flavors) {
            if (flavor.equals(DataFlavor.javaFileListFlavor) || flavor.equals(uriListFlavor) || flavor.equals(urlListFlavor)) {
                return true;
            }
        }
        return false;
    }

    @SuppressWarnings("unchecked")
    public boolean importData(
            JComponent component, Transferable transferable) {
        try {
            if (transferable.isDataFlavorSupported(DataFlavor.javaFileListFlavor)) {
                importFiles((List<File>) transferable.getTransferData(
                        DataFlavor.javaFileListFlavor));
            } else if (transferable.isDataFlavorSupported(urlListFlavor)) {
                Object data = transferable.getTransferData(urlListFlavor);
                tika.openURL(new URL(data.toString()));
            } else if (transferable.isDataFlavorSupported(uriListFlavor)) {
                importFiles(uriToFileList(
                        transferable.getTransferData(uriListFlavor)));
            }
            return true;
        } catch (Exception e) {
            return false;
        }
    }

    private void importFiles(List<File> files) {
        for (File file : files) {
            tika.openFile(file);
        }
    }

    public void exportAsDrag(JComponent arg0, InputEvent arg1, int arg2) {
        delegate.exportAsDrag(arg0, arg1, arg2);
    }

    public void exportToClipboard(JComponent arg0, Clipboard arg1, int arg2)
            throws IllegalStateException {
        delegate.exportToClipboard(arg0, arg1, arg2);
    }

    public int getSourceActions(JComponent arg0) {
        return delegate.getSourceActions(arg0);
    }

    public Icon getVisualRepresentation(Transferable arg0) {
        return delegate.getVisualRepresentation(arg0);
    }

    private static List<File> uriToFileList(Object data) {
        List<File> list = new ArrayList<File>();
        StringTokenizer st = new StringTokenizer(data.toString(), "\r\n");
        while (st.hasMoreTokens())
        {
            String s = st.nextToken();
            if (s.startsWith("#")) {
                continue;
            }
            try {
                list.add(new File(new URI(s)));
            } catch (Exception e) {
            }
        }
        return list;
    }
}
"
tika-app/src/main/java/org/apache/tika/gui/TikaGUI.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.gui;

import javax.swing.Box;
import javax.swing.JDialog;
import javax.swing.JEditorPane;
import javax.swing.JFileChooser;
import javax.swing.JFrame;
import javax.swing.JMenu;
import javax.swing.JMenuBar;
import javax.swing.JMenuItem;
import javax.swing.JOptionPane;
import javax.swing.JPanel;
import javax.swing.JScrollPane;
import javax.swing.JTextPane;
import javax.swing.ProgressMonitorInputStream;
import javax.swing.SwingUtilities;
import javax.swing.UIManager;
import javax.swing.event.HyperlinkEvent;
import javax.swing.event.HyperlinkEvent.EventType;
import javax.swing.event.HyperlinkListener;
import javax.xml.transform.OutputKeys;
import javax.xml.transform.TransformerConfigurationException;
import javax.xml.transform.sax.SAXTransformerFactory;
import javax.xml.transform.sax.TransformerHandler;
import javax.xml.transform.stream.StreamResult;
import java.awt.CardLayout;
import java.awt.Color;
import java.awt.Dimension;
import java.awt.Toolkit;
import java.awt.event.ActionEvent;
import java.awt.event.ActionListener;
import java.awt.event.KeyEvent;
import java.awt.event.WindowEvent;
import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.PrintWriter;
import java.io.StringWriter;
import java.io.Writer;
import java.net.MalformedURLException;
import java.net.URL;
import java.util.Arrays;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;

import org.apache.tika.config.TikaConfig;
import org.apache.tika.exception.TikaException;
import org.apache.tika.extractor.DocumentSelector;
import org.apache.tika.io.IOUtils;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.serialization.JsonMetadataList;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.AutoDetectParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.Parser;
import org.apache.tika.parser.RecursiveParserWrapper;
import org.apache.tika.parser.html.BoilerpipeContentHandler;
import org.apache.tika.sax.BasicContentHandlerFactory;
import org.apache.tika.sax.BodyContentHandler;
import org.apache.tika.sax.ContentHandlerDecorator;
import org.apache.tika.sax.TeeContentHandler;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.Attributes;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.AttributesImpl;

/**
 * Simple Swing GUI for Apache Tika. You can drag and drop files on top
 * of the window to have them parsed.
 */
public class TikaGUI extends JFrame
        implements ActionListener, HyperlinkListener {

    /**
     * Serial version UID.
     */
    private static final long serialVersionUID = 5883906936187059495L;

    /**
     * Main method. Sets the Swing look and feel to the operating system
     * settings, and starts the Tika GUI with an {@link AutoDetectParser}
     * instance as the default parser.
     *
     * @param args ignored
     * @throws Exception if an error occurs
     */
    public static void main(String[] args) throws Exception {
        TikaConfig config = TikaConfig.getDefaultConfig();
        if (args.length > 0) {
            File configFile = new File(args[0]);
            config = new TikaConfig(configFile);
        }
        UIManager.setLookAndFeel(UIManager.getSystemLookAndFeelClassName());
        final TikaConfig finalConfig = config;
        SwingUtilities.invokeLater(new Runnable() {
            public void run() {
                new TikaGUI(new AutoDetectParser(finalConfig)).setVisible(true);
            }
        });
    }

    //maximum length to allow for mark for reparse to get JSON
    private final int MAX_MARK = 20*1024*1024;//20MB
    /**
     * Parsing context.
     */
    private final ParseContext context;

    /**
     * Configured parser instance.
     */
    private final Parser parser;
    
    /**
     * Captures requested embedded images
     */
    private final ImageSavingParser imageParser;

    /**
     * The card layout for switching between different views.
     */
    private final CardLayout layout = new CardLayout();

    /**
     * Container for the editor cards.
     */
    private final JPanel cards;

    /**
     * Formatted XHTML output.
     */
    private final JEditorPane html;

    /**
     * Plain text output.
     */
    private final JEditorPane text;

    /**
     * Main content output.
     */
    private final JEditorPane textMain;
    
    /**
     * Raw XHTML source.
     */
    private final JEditorPane xml;

    /**
     * Raw JSON source.
     */
    private final JEditorPane json;

    /**
     * Document metadata.
     */
    private final JEditorPane metadata;

    /**
     * File chooser.
     */
    private final JFileChooser chooser = new JFileChooser();

    public TikaGUI(Parser parser) {
        super("Apache Tika");
        setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE);

        addMenuBar();

        cards = new JPanel(layout);
        addWelcomeCard(cards, "welcome");
        metadata = addCard(cards, "text/plain", "metadata");
        html = addCard(cards, "text/html", "html");
        text = addCard(cards, "text/plain", "text");
        textMain = addCard(cards, "text/plain", "main");
        xml = addCard(cards, "text/plain", "xhtml");
        json = addCard(cards, "text/plain", "json");
        add(cards);
        layout.show(cards, "welcome");

        setPreferredSize(new Dimension(640, 480));
        pack();

        this.context = new ParseContext();
        this.parser = parser;

        this.imageParser = new ImageSavingParser(parser);
        this.context.set(DocumentSelector.class, new ImageDocumentSelector());
        this.context.set(Parser.class, imageParser);
    }

    private void addMenuBar() {
        JMenuBar bar = new JMenuBar();

        JMenu file = new JMenu("File");
        file.setMnemonic(KeyEvent.VK_F);
        addMenuItem(file, "Open...", "openfile", KeyEvent.VK_O);
        addMenuItem(file, "Open URL...", "openurl", KeyEvent.VK_U);
        file.addSeparator();
        addMenuItem(file, "Exit", "exit", KeyEvent.VK_X);
        bar.add(file);

        JMenu view = new JMenu("View");
        view.setMnemonic(KeyEvent.VK_V);
        addMenuItem(view, "Metadata", "metadata", KeyEvent.VK_M);
        addMenuItem(view, "Formatted text", "html", KeyEvent.VK_F);
        addMenuItem(view, "Plain text", "text", KeyEvent.VK_P);
        addMenuItem(view, "Main content", "main", KeyEvent.VK_C);
        addMenuItem(view, "Structured text", "xhtml", KeyEvent.VK_S);
        addMenuItem(view, "Recursive JSON", "json", KeyEvent.VK_J);
        bar.add(view);

        bar.add(Box.createHorizontalGlue());
        JMenu help = new JMenu("Help");
        help.setMnemonic(KeyEvent.VK_H);
        addMenuItem(help, "About Tika", "about", KeyEvent.VK_A);
        bar.add(help);

        setJMenuBar(bar);
    }

    private void addMenuItem(
            JMenu menu, String title, String command, int key) {
        JMenuItem item = new JMenuItem(title, key);
        item.setActionCommand(command);
        item.addActionListener(this);
        menu.add(item);
    }

    public void actionPerformed(ActionEvent e) {
        String command = e.getActionCommand();
        if ("openfile".equals(command)) {
            int rv = chooser.showOpenDialog(this);
            if (rv == JFileChooser.APPROVE_OPTION) {
                openFile(chooser.getSelectedFile());
            }
        } else if ("openurl".equals(command)) {
            Object rv = JOptionPane.showInputDialog(
                    this, "Enter the URL of the resource to be parsed:",
                    "Open URL", JOptionPane.PLAIN_MESSAGE,
                    null, null, "");
            if (rv != null && rv.toString().length() > 0) {
                try {
                    openURL(new URL(rv.toString().trim()));
                } catch (MalformedURLException exception) {
                    JOptionPane.showMessageDialog(
                            this, "The given string is not a valid URL",
                            "Invalid URL", JOptionPane.ERROR_MESSAGE);
                }
            }
        } else if ("html".equals(command)) {
            layout.show(cards, command);
        } else if ("text".equals(command)) {
            layout.show(cards, command);
        } else if ("main".equals(command)) {
            layout.show(cards, command);
        } else if ("xhtml".equals(command)) {
            layout.show(cards, command);
        } else if ("metadata".equals(command)) {
            layout.show(cards, command);
        } else if ("json".equals(command)) {
            layout.show(cards, command);
        } else if ("about".equals(command)) {
            textDialog(
                    "About Apache Tika",
                    TikaGUI.class.getResource("about.html"));
        } else if ("exit".equals(command)) {
            Toolkit.getDefaultToolkit().getSystemEventQueue().postEvent(
                    new WindowEvent(this, WindowEvent.WINDOW_CLOSING));
        }
    }

    public void openFile(File file) {
        try {
            Metadata metadata = new Metadata();
            TikaInputStream stream = TikaInputStream.get(file, metadata);
            try {
                handleStream(stream, metadata);
            } finally {
                stream.close();
            }
        } catch (Throwable t) {
            handleError(file.getPath(), t);
        }
    }

    public void openURL(URL url) {
        try {
            Metadata metadata = new Metadata();
            TikaInputStream stream = TikaInputStream.get(url, metadata);
            try {
                handleStream(stream, metadata);
            } finally {
                stream.close();
            }
        } catch (Throwable t) {
            handleError(url.toString(), t);
        }
    }

    private void handleStream(InputStream input, Metadata md)
            throws Exception {
        StringWriter htmlBuffer = new StringWriter();
        StringWriter textBuffer = new StringWriter();
        StringWriter textMainBuffer = new StringWriter();
        StringWriter xmlBuffer = new StringWriter();
        StringBuilder metadataBuffer = new StringBuilder();

        ContentHandler handler = new TeeContentHandler(
                getHtmlHandler(htmlBuffer),
                getTextContentHandler(textBuffer),
                getTextMainContentHandler(textMainBuffer),
                getXmlContentHandler(xmlBuffer));

        context.set(DocumentSelector.class, new ImageDocumentSelector());
        if (input.markSupported()) {
            input.mark(MAX_MARK);
        }
        input = new ProgressMonitorInputStream(
                this, "Parsing stream", input);
        parser.parse(input, handler, md, context);

        String[] names = md.names();
        Arrays.sort(names);
        for (String name : names) {
            metadataBuffer.append(name);
            metadataBuffer.append(": ");
            metadataBuffer.append(md.get(name));
            metadataBuffer.append("\n");
        }

        String name = md.get(Metadata.RESOURCE_NAME_KEY);
        if (name != null && name.length() > 0) {
            setTitle("Apache Tika: " + name);
        } else {
            setTitle("Apache Tika: unnamed document");
        }

        setText(metadata, metadataBuffer.toString());
        setText(xml, xmlBuffer.toString());
        setText(text, textBuffer.toString());
        setText(textMain, textMainBuffer.toString());
        setText(html, htmlBuffer.toString());
        if (!input.markSupported()) {
            setText(json, "InputStream does not support mark/reset for Recursive Parsing");
            layout.show(cards, "metadata");
            return;
        }
        boolean isReset = false;
        try {
            input.reset();
            isReset = true;
        } catch (IOException e) {
            setText(json, "Error during stream reset.\n"+
                    "There's a limit of "+MAX_MARK + " bytes for this type of processing in the GUI.\n"+
                    "Try the app with command line argument of -J."
            );
        }
        if (isReset) {
            RecursiveParserWrapper wrapper = new RecursiveParserWrapper(parser,
                    new BasicContentHandlerFactory(
                            BasicContentHandlerFactory.HANDLER_TYPE.BODY, -1));
            wrapper.parse(input, null, new Metadata(), new ParseContext());
            StringWriter jsonBuffer = new StringWriter();
            JsonMetadataList.setPrettyPrinting(true);
            JsonMetadataList.toJson(wrapper.getMetadata(), jsonBuffer);
            setText(json, jsonBuffer.toString());
        }
        layout.show(cards, "metadata");
    }

    private void handleError(String name, Throwable t) {
        StringWriter writer = new StringWriter();
        writer.append("Apache Tika was unable to parse the document\n");
        writer.append("at " + name + ".\n\n");
        writer.append("The full exception stack trace is included below:\n\n");
        t.printStackTrace(new PrintWriter(writer));

        JEditorPane editor =
            new JEditorPane("text/plain", writer.toString());
        editor.setEditable(false);
        editor.setBackground(Color.WHITE);
        editor.setCaretPosition(0);
        editor.setPreferredSize(new Dimension(600, 400));

        JDialog dialog = new JDialog(this, "Apache Tika error");
        dialog.add(new JScrollPane(editor));
        dialog.pack();
        dialog.setVisible(true);
    }

    private void addWelcomeCard(JPanel panel, String name) {
        try {
            JEditorPane editor =
                new JEditorPane(TikaGUI.class.getResource("welcome.html"));
            editor.setContentType("text/html");
            editor.setEditable(false);
            editor.setBackground(Color.WHITE);
            editor.setTransferHandler(new ParsingTransferHandler(
                    editor.getTransferHandler(), this));
            panel.add(new JScrollPane(editor), name);
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    private JEditorPane addCard(JPanel panel, String type, String name) {
        JEditorPane editor = new JTextPane();
        editor.setBackground(Color.WHITE);
        editor.setContentType(type);
        editor.setTransferHandler(new ParsingTransferHandler(
                editor.getTransferHandler(), this));
        panel.add(new JScrollPane(editor), name);
        return editor;
    }

    private void textDialog(String title, URL resource) {
        try {
            JDialog dialog = new JDialog(this, title);
            JEditorPane editor = new JEditorPane(resource);
            editor.setContentType("text/html");
            editor.setEditable(false);
            editor.setBackground(Color.WHITE);
            editor.setPreferredSize(new Dimension(400, 250));
            editor.addHyperlinkListener(this);
            dialog.add(editor);
            dialog.pack();
            dialog.setVisible(true);
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    public void hyperlinkUpdate(HyperlinkEvent e) {
        if (e.getEventType() == EventType.ACTIVATED) {
            try {
                URL url = e.getURL();
                InputStream stream = url.openStream();
                try {
                    StringWriter writer = new StringWriter();
                    IOUtils.copy(stream, writer, "UTF-8");

                    JEditorPane editor =
                        new JEditorPane("text/plain", writer.toString());
                    editor.setEditable(false);
                    editor.setBackground(Color.WHITE);
                    editor.setCaretPosition(0);
                    editor.setPreferredSize(new Dimension(600, 400));

                    String name = url.toString();
                    name = name.substring(name.lastIndexOf('/') + 1);

                    JDialog dialog = new JDialog(this, "Apache Tika: " + name);
                    dialog.add(new JScrollPane(editor));
                    dialog.pack();
                    dialog.setVisible(true);
                } finally {
                    stream.close();
                }
            } catch (IOException exception) {
                exception.printStackTrace();
            }
        }
    }

    private void setText(JEditorPane editor, String text) {
        editor.setText(text);
        editor.setCaretPosition(0);
    }

    /**
     * Creates and returns a content handler that turns XHTML input to
     * simplified HTML output that can be correctly parsed and displayed
     * by {@link JEditorPane}.
     * <p>
     * The returned content handler is set to output <code>html</code>
     * to the given writer. The XHTML namespace is removed from the output
     * to prevent the serializer from using the &lt;tag/&gt; empty element
     * syntax that causes extra "&gt;" characters to be displayed.
     * The &lt;head&gt; tags are dropped to prevent the serializer from
     * generating a &lt;META&gt; content type tag that makes
     * {@link JEditorPane} fail thinking that the document character set
     * is inconsistent.
     * <p>
     * Additionally, it will use ImageSavingParser to re-write embedded:(image) 
     * image links to be file:///(temporary file) so that they can be loaded.
     *
     * @param writer output writer
     * @return HTML content handler
     * @throws TransformerConfigurationException if an error occurs
     */
    private ContentHandler getHtmlHandler(Writer writer)
            throws TransformerConfigurationException {
        SAXTransformerFactory factory = (SAXTransformerFactory)
            SAXTransformerFactory.newInstance();
        TransformerHandler handler = factory.newTransformerHandler();
        handler.getTransformer().setOutputProperty(OutputKeys.METHOD, "html");
        handler.setResult(new StreamResult(writer));
        return new ContentHandlerDecorator(handler) {
            @Override
            public void startElement(
                    String uri, String localName, String name, Attributes atts)
                    throws SAXException {
                if (XHTMLContentHandler.XHTML.equals(uri)) {
                    uri = null;
                }
                if (!"head".equals(localName)) {
                    if("img".equals(localName)) {
                       AttributesImpl newAttrs;
                       if(atts instanceof AttributesImpl) {
                          newAttrs = (AttributesImpl)atts;
                       } else {
                          newAttrs = new AttributesImpl(atts);
                       }
                       
                       for(int i=0; i<newAttrs.getLength(); i++) {
                          if("src".equals(newAttrs.getLocalName(i))) {
                             String src = newAttrs.getValue(i);
                             if(src.startsWith("embedded:")) {
                                String filename = src.substring(src.indexOf(':')+1);
                                try {
                                   File img = imageParser.requestSave(filename);
                                   String newSrc = img.toURI().toString();
                                   newAttrs.setValue(i, newSrc);
                                } catch(IOException e) {
                                   System.err.println("Error creating temp image file " + filename);
                                   // The html viewer will show a broken image too to alert them
                                }
                             }
                          }
                       }
                       super.startElement(uri, localName, name, newAttrs);
                    } else {
                       super.startElement(uri, localName, name, atts);
                    }
                }
            }
            @Override
            public void endElement(String uri, String localName, String name)
                    throws SAXException {
                if (XHTMLContentHandler.XHTML.equals(uri)) {
                    uri = null;
                }
                if (!"head".equals(localName)) {
                    super.endElement(uri, localName, name);
                }
            }
            @Override
            public void startPrefixMapping(String prefix, String uri) {
            }
            @Override
            public void endPrefixMapping(String prefix) {
            }
        };
    }

    private ContentHandler getTextContentHandler(Writer writer) {
        return new BodyContentHandler(writer);
    }
    private ContentHandler getTextMainContentHandler(Writer writer) {
        return new BoilerpipeContentHandler(writer);
    }

    private ContentHandler getXmlContentHandler(Writer writer)
            throws TransformerConfigurationException {
        SAXTransformerFactory factory = (SAXTransformerFactory)
            SAXTransformerFactory.newInstance();
        TransformerHandler handler = factory.newTransformerHandler();
        handler.getTransformer().setOutputProperty(OutputKeys.METHOD, "xml");
        handler.setResult(new StreamResult(writer));
        return handler;
    }

    /**
     * A {@link DocumentSelector} that accepts only images.
     */
    private static class ImageDocumentSelector implements DocumentSelector {
      public boolean select(Metadata metadata) {
         String type = metadata.get(Metadata.CONTENT_TYPE);
         return type != null && type.startsWith("image/");
      }
    }
    
    /**
     * A recursive parser that saves certain images into the temporary
     *  directory, and delegates everything else to another downstream
     *  parser.
     */
    private static class ImageSavingParser extends AbstractParser {
      private Map<String,File> wanted = new HashMap<String,File>();
      private Parser downstreamParser;
      private File tmpDir;
      
      private ImageSavingParser(Parser downstreamParser) {
         this.downstreamParser = downstreamParser;
         
         try {
            File t = File.createTempFile("tika", ".test");
            tmpDir = t.getParentFile();
         } catch(IOException e) {}
      }
      
      public File requestSave(String embeddedName) throws IOException {
         String suffix = ".tika";
         
         int splitAt = embeddedName.lastIndexOf('.');
         if (splitAt > 0) {
            embeddedName.substring(splitAt);
         }
         
         File tmp = File.createTempFile("tika-embedded-", suffix);
         wanted.put(embeddedName, tmp);
         return tmp;
      }
      
      public Set<MediaType> getSupportedTypes(ParseContext context) {
         // Never used in an auto setup
         return null;
      }

      public void parse(InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context) throws IOException,
            SAXException, TikaException {
         String name = metadata.get(Metadata.RESOURCE_NAME_KEY);
         if(name != null && wanted.containsKey(name)) {
            FileOutputStream out = new FileOutputStream(wanted.get(name));
            IOUtils.copy(stream, out);
            out.close();
         } else {
            if(downstreamParser != null) {
               downstreamParser.parse(stream, handler, metadata, context);
            }
         }
      }

    }

}
"
tika-core/src/main/java/org/apache/tika/package-info.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Apache Tika.
 */
@aQute.bnd.annotation.Version("1.0.0")
package org.apache.tika;
"
tika-core/src/main/java/org/apache/tika/Tika.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika;

import java.io.BufferedInputStream;
import java.io.File;
import java.io.IOException;
import java.io.InputStream;
import java.io.Reader;
import java.net.URL;
import java.util.Properties;

import org.apache.tika.config.TikaConfig;
import org.apache.tika.detect.Detector;
import org.apache.tika.exception.TikaException;
import org.apache.tika.io.IOUtils;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.language.translate.Translator;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.parser.AutoDetectParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.Parser;
import org.apache.tika.parser.ParsingReader;
import org.apache.tika.sax.BodyContentHandler;
import org.apache.tika.sax.WriteOutContentHandler;
import org.xml.sax.SAXException;

/**
 * Facade class for accessing Tika functionality. This class hides much of
 * the underlying complexity of the lower level Tika classes and provides
 * simple methods for many common parsing and type detection operations.
 *
 * @since Apache Tika 0.5
 * @see Parser
 * @see Detector
 */
public class Tika {

    /**
     * The detector instance used by this facade.
     */
    private final Detector detector;

    /**
     * The parser instance used by this facade.
     */
    private final Parser parser;

    /**
     * The Translator instance used by this facade.
     */
    private final Translator translator;

    /**
     * Maximum length of the strings returned by the parseToString methods.
     * Used to prevent out of memory problems with huge input documents.
     * The default setting is 100k characters.
     */
    private int maxStringLength = 100 * 1000;

    /**
     * Creates a Tika facade using the given detector and parser instances, but the default Translator.
     *
     * @since Apache Tika 0.8
     * @param detector type detector
     * @param parser document parser
     */
    public Tika(Detector detector, Parser parser) {
        this.detector = detector;
        this.parser = parser;
        this.translator = TikaConfig.getDefaultConfig().getTranslator();
    }

    /**
     * Creates a Tika facade using the given detector, parser, and translator instances.
     *
     * @since Apache Tika 1.6
     * @param detector type detector
     * @param parser document parser
     * @param translator text translator
     */
    public Tika(Detector detector, Parser parser, Translator translator) {
        this.detector = detector;
        this.parser = parser;
        this.translator = translator;
    }

    /**
     * Creates a Tika facade using the given configuration.
     *
     * @param config Tika configuration
     */
    public Tika(TikaConfig config) {
        this(config.getDetector(), new AutoDetectParser(config), config.getTranslator());
    }

    /**
     * Creates a Tika facade using the default configuration.
     */
    public Tika() {
        this(TikaConfig.getDefaultConfig());
    }

    /**
     * Creates a Tika facade using the given detector instance, the
     * default parser configuration, and the default Translator.
     *
     * @since Apache Tika 0.8
     * @param detector type detector
     */
    public Tika(Detector detector) {
        this(detector, new AutoDetectParser(detector));
    }

    
    /**
     * Detects the media type of the given document. The type detection is
     * based on the content of the given document stream and any given
     * document metadata. The document stream can be <code>null</code>,
     * in which case only the given document metadata is used for type
     * detection.
     * <p>
     * If the document stream supports the
     * {@link InputStream#markSupported() mark feature}, then the stream is
     * marked and reset to the original position before this method returns.
     * Only a limited number of bytes are read from the stream.
     * <p>
     * The given document stream is <em>not</em> closed by this method.
     * <p>
     * Unlike in the {@link #parse(InputStream, Metadata)} method, the
     * given document metadata is <em>not</em> modified by this method.
     *
     * @param stream the document stream, or <code>null</code>
     * @param metadata document metadata
     * @return detected media type
     * @throws IOException if the stream can not be read
     */
    public String detect(InputStream stream, Metadata metadata)
            throws IOException {
        if (stream == null || stream.markSupported()) {
            return detector.detect(stream, metadata).toString();
        } else {
            return detector.detect(
                    new BufferedInputStream(stream), metadata).toString();
        }
    }

    /**
     * Detects the media type of the given document. The type detection is
     * based on the content of the given document stream and the name of the
     * document.
     * <p>
     * If the document stream supports the
     * {@link InputStream#markSupported() mark feature}, then the stream is
     * marked and reset to the original position before this method returns.
     * Only a limited number of bytes are read from the stream.
     * <p>
     * The given document stream is <em>not</em> closed by this method.
     *
     * @since Apache Tika 0.9
     * @param stream the document stream
     * @param name document name
     * @return detected media type
     * @throws IOException if the stream can not be read
     */
    public String detect(InputStream stream, String name) throws IOException {
        Metadata metadata = new Metadata();
        metadata.set(Metadata.RESOURCE_NAME_KEY, name);
        return detect(stream, metadata);
    }

    /**
     * Detects the media type of the given document. The type detection is
     * based on the content of the given document stream.
     * <p>
     * If the document stream supports the
     * {@link InputStream#markSupported() mark feature}, then the stream is
     * marked and reset to the original position before this method returns.
     * Only a limited number of bytes are read from the stream.
     * <p>
     * The given document stream is <em>not</em> closed by this method.
     *
     * @param stream the document stream
     * @return detected media type
     * @throws IOException if the stream can not be read
     */
    public String detect(InputStream stream) throws IOException {
        return detect(stream, new Metadata());
    }

    /**
     * Detects the media type of the given document. The type detection is
     * based on the first few bytes of a document and the document name.
     * <p>
     * For best results at least a few kilobytes of the document data
     * are needed. See also the other detect() methods for better
     * alternatives when you have more than just the document prefix
     * available for type detection.
     *
     * @since Apache Tika 0.9
     * @param prefix first few bytes of the document
     * @param name document name
     * @return detected media type
     */
    public String detect(byte[] prefix, String name) {
        try {
            InputStream stream = TikaInputStream.get(prefix);
            try {
                return detect(stream, name);
            } finally {
                stream.close();
            }
        } catch (IOException e) {
            throw new IllegalStateException("Unexpected IOException", e);
        }
    }

    /**
     * Detects the media type of the given document. The type detection is
     * based on the first few bytes of a document.
     * <p>
     * For best results at least a few kilobytes of the document data
     * are needed. See also the other detect() methods for better
     * alternatives when you have more than just the document prefix
     * available for type detection.
     *
     * @since Apache Tika 0.9
     * @param prefix first few bytes of the document
     * @return detected media type
     */
    public String detect(byte[] prefix) {
        try {
            InputStream stream = TikaInputStream.get(prefix);
            try {
                return detect(stream);
            } finally {
                stream.close();
            }
        } catch (IOException e) {
            throw new IllegalStateException("Unexpected IOException", e);
        }
    }

    /**
     * Detects the media type of the given file. The type detection is
     * based on the document content and a potential known file extension.
     * <p>
     * Use the {@link #detect(String)} method when you want to detect the
     * type of the document without actually accessing the file.
     *
     * @param file the file
     * @return detected media type
     * @throws IOException if the file can not be read
     */
    public String detect(File file) throws IOException {
        return detect(file.toURI().toURL());
    }

    /**
     * Detects the media type of the resource at the given URL. The type
     * detection is based on the document content and a potential known
     * file extension included in the URL.
     * <p>
     * Use the {@link #detect(String)} method when you want to detect the
     * type of the document without actually accessing the URL.
     *
     * @param url the URL of the resource
     * @return detected media type
     * @throws IOException if the resource can not be read
     */
    public String detect(URL url) throws IOException {
        Metadata metadata = new Metadata();
        InputStream stream = TikaInputStream.get(url, metadata);
        try {
            return detect(stream, metadata);
        } finally {
            stream.close();
        }
    }

    /**
     * Detects the media type of a document with the given file name.
     * The type detection is based on known file name extensions.
     * <p>
     * The given name can also be a URL or a full file path. In such cases
     * only the file name part of the string is used for type detection. 
     *
     * @param name the file name of the document
     * @return detected media type
     */
    public String detect(String name) {
        try {
            return detect((InputStream) null, name);
        } catch (IOException e) {
            throw new IllegalStateException("Unexpected IOException", e);
        }
    }

    /**
     * Translate the given text String to and from the given languages.
     * @see org.apache.tika.language.translate.Translator
     * @param text The text to translate.
     * @param sourceLanguage The input text language (for example, "hi").
     * @param targetLanguage The desired output language (for example, "fr").
     * @return The translated text. If translation is unavailable (client keys not set), returns the same text back.
     */
    public String translate(String text, String sourceLanguage, String targetLanguage){
        try {
            return translator.translate(text, sourceLanguage, targetLanguage);
        } catch (Exception e){
            throw new IllegalStateException("Error translating data.", e);
        }
    }

    /**
     * Translate the given text String to the given language, attempting to auto-detect the source language.
     * @see org.apache.tika.language.translate.Translator
     * @param text The text to translate.
     * @param targetLanguage The desired output language (for example, "en").
     * @return The translated text. If translation is unavailable (client keys not set), returns the same text back.
     */
    public String translate(String text, String targetLanguage){
        try {
            return translator.translate(text, targetLanguage);
        } catch (Exception e){
            throw new IllegalStateException("Error translating data.", e);
        }
    }

    /**
     * Translate the given text InputStream to and from the given languages.
     * @see org.apache.tika.language.translate.Translator
     * @param text The text to translate.
     * @param sourceLanguage The input text language (for example, "hi").
     * @param targetLanguage The desired output language (for example, "fr").
     * @return The translated text. If translation is unavailable (client keys not set), returns the same text back.
     */
    public String translate(InputStream text, String sourceLanguage, String targetLanguage){
        try {
            return translator.translate(IOUtils.toString(text), sourceLanguage, targetLanguage);
        } catch (Exception e){
            throw new IllegalStateException("Error translating data.", e);
        }
    }

    /**
     * Translate the given text InputStream to the given language, attempting to auto-detect the source language.
     * This does not close the stream, so the caller has the responsibility of closing it.
     * @see org.apache.tika.language.translate.Translator
     * @param text The text to translate.
     * @param targetLanguage The desired output language (for example, "en").
     * @return The translated text. If translation is unavailable (client keys not set), returns the same text back.
     */
    public String translate(InputStream text, String targetLanguage){
        try {
            return translator.translate(IOUtils.toString(text), targetLanguage);
        } catch (Exception e){
            throw new IllegalStateException("Error translating data.", e);
        }
    }

    /**
     * Parses the given document and returns the extracted text content.
     * Input metadata like a file name or a content type hint can be passed
     * in the given metadata instance. Metadata information extracted from
     * the document is returned in that same metadata instance.
     * <p>
     * The returned reader will be responsible for closing the given stream.
     * The stream and any associated resources will be closed at or before
     * the time when the {@link Reader#close()} method is called.
     *
     * @param stream the document to be parsed
     * @param metadata document metadata
     * @return extracted text content
     * @throws IOException if the document can not be read or parsed
     */
    public Reader parse(InputStream stream, Metadata metadata)
            throws IOException {
        ParseContext context = new ParseContext();
        context.set(Parser.class, parser);
        return new ParsingReader(parser, stream, metadata, context);
    }

    /**
     * Parses the given document and returns the extracted text content.
     * <p>
     * The returned reader will be responsible for closing the given stream.
     * The stream and any associated resources will be closed at or before
     * the time when the {@link Reader#close()} method is called.
     *
     * @param stream the document to be parsed
     * @return extracted text content
     * @throws IOException if the document can not be read or parsed
     */
    public Reader parse(InputStream stream) throws IOException {
        return parse(stream, new Metadata());
    }

    /**
     * Parses the given file and returns the extracted text content.
     *
     * @param file the file to be parsed
     * @return extracted text content
     * @throws IOException if the file can not be read or parsed
     */
    public Reader parse(File file) throws IOException {
        return parse(file.toURI().toURL());
    }

    /**
     * Parses the resource at the given URL and returns the extracted
     * text content.
     *
     * @param url the URL of the resource to be parsed
     * @return extracted text content
     * @throws IOException if the resource can not be read or parsed
     */
    public Reader parse(URL url) throws IOException {
        Metadata metadata = new Metadata();
        InputStream stream = TikaInputStream.get(url, metadata);
        return parse(stream, metadata);
    }

    /**
     * Parses the given document and returns the extracted text content.
     * The given input stream is closed by this method.
     * <p>
     * To avoid unpredictable excess memory use, the returned string contains
     * only up to {@link #getMaxStringLength()} first characters extracted
     * from the input document. Use the {@link #setMaxStringLength(int)}
     * method to adjust this limitation.
     * <p>
     * <strong>NOTE:</strong> Unlike most other Tika methods that take an
     * {@link InputStream}, this method will close the given stream for
     * you as a convenience. With other methods you are still responsible
     * for closing the stream or a wrapper instance returned by Tika.
     *
     * @param stream the document to be parsed
     * @param metadata document metadata
     * @return extracted text content
     * @throws IOException if the document can not be read
     * @throws TikaException if the document can not be parsed
     */
    public String parseToString(InputStream stream, Metadata metadata)
            throws IOException, TikaException {
        WriteOutContentHandler handler =
            new WriteOutContentHandler(maxStringLength);
        try {
            ParseContext context = new ParseContext();
            context.set(Parser.class, parser);
            parser.parse(
                    stream, new BodyContentHandler(handler), metadata, context);
        } catch (SAXException e) {
            if (!handler.isWriteLimitReached(e)) {
                // This should never happen with BodyContentHandler...
                throw new TikaException("Unexpected SAX processing failure", e);
            }
        } finally {
            stream.close();
        }
        return handler.toString();
    }

    /**
     * Parses the given document and returns the extracted text content.
     * The given input stream is closed by this method. This method lets
     * you control the maxStringLength per call.
     * <p>
     * To avoid unpredictable excess memory use, the returned string contains
     * only up to maxLength (parameter) first characters extracted
     * from the input document.
     * <p>
     * <strong>NOTE:</strong> Unlike most other Tika methods that take an
     * {@link InputStream}, this method will close the given stream for
     * you as a convenience. With other methods you are still responsible
     * for closing the stream or a wrapper instance returned by Tika.
     *
     * @param stream the document to be parsed
     * @param metadata document metadata
     * @param maxLength maximum length of the returned string
     * @return extracted text content
     * @throws IOException if the document can not be read
     * @throws TikaException if the document can not be parsed
     */
    public String parseToString(InputStream stream, Metadata metadata, int maxLength)
        throws IOException, TikaException {
        WriteOutContentHandler handler =
            new WriteOutContentHandler(maxLength);
        try {
            ParseContext context = new ParseContext();
            context.set(Parser.class, parser);
            parser.parse(
                         stream, new BodyContentHandler(handler), metadata, context);
        } catch (SAXException e) {
            if (!handler.isWriteLimitReached(e)) {
                // This should never happen with BodyContentHandler...
                throw new TikaException("Unexpected SAX processing failure", e);
            }
        } finally {
            stream.close();
        }
        return handler.toString();
    }

    /**
     * Parses the given document and returns the extracted text content.
     * The given input stream is closed by this method.
     * <p>
     * To avoid unpredictable excess memory use, the returned string contains
     * only up to {@link #getMaxStringLength()} first characters extracted
     * from the input document. Use the {@link #setMaxStringLength(int)}
     * method to adjust this limitation.
     * <p>
     * <strong>NOTE:</strong> Unlike most other Tika methods that take an
     * {@link InputStream}, this method will close the given stream for
     * you as a convenience. With other methods you are still responsible
     * for closing the stream or a wrapper instance returned by Tika.
     *
     * @param stream the document to be parsed
     * @return extracted text content
     * @throws IOException if the document can not be read
     * @throws TikaException if the document can not be parsed
     */
    public String parseToString(InputStream stream)
            throws IOException, TikaException {
        return parseToString(stream, new Metadata());
    }

    /**
     * Parses the given file and returns the extracted text content.
     * <p>
     * To avoid unpredictable excess memory use, the returned string contains
     * only up to {@link #getMaxStringLength()} first characters extracted
     * from the input document. Use the {@link #setMaxStringLength(int)}
     * method to adjust this limitation.
     *
     * @param file the file to be parsed
     * @return extracted text content
     * @throws IOException if the file can not be read
     * @throws TikaException if the file can not be parsed
     */
    public String parseToString(File file) throws IOException, TikaException {
        return parseToString(file.toURI().toURL());
    }

    /**
     * Parses the resource at the given URL and returns the extracted
     * text content.
     * <p>
     * To avoid unpredictable excess memory use, the returned string contains
     * only up to {@link #getMaxStringLength()} first characters extracted
     * from the input document. Use the {@link #setMaxStringLength(int)}
     * method to adjust this limitation.
     *
     * @param url the URL of the resource to be parsed
     * @return extracted text content
     * @throws IOException if the resource can not be read
     * @throws TikaException if the resource can not be parsed
     */
    public String parseToString(URL url) throws IOException, TikaException {
        Metadata metadata = new Metadata();
        InputStream stream = TikaInputStream.get(url, metadata);
        return parseToString(stream, metadata);
    }

    /**
     * Returns the maximum length of strings returned by the
     * parseToString methods.
     *
     * @since Apache Tika 0.7
     * @return maximum string length, or -1 if the limit has been disabled
     */
    public int getMaxStringLength() {
        return maxStringLength;
    }

    /**
     * Sets the maximum length of strings returned by the parseToString
     * methods.
     *
     * @since Apache Tika 0.7
     * @param maxStringLength maximum string length,
     *                        or -1 to disable this limit
     */
    public void setMaxStringLength(int maxStringLength) {
        this.maxStringLength = maxStringLength;
    }

    /**
     * Returns the parser instance used by this facade.
     *
     * @since Apache Tika 0.10
     * @return parser instance
     */
    public Parser getParser() {
        return parser;
    }

    /**
     * Returns the detector instance used by this facade.
     *
     * @since Apache Tika 0.10
     * @return detector instance
     */
    public Detector getDetector() {
        return detector;
    }

    /**
     * Returns the translator instance used by this facade.
     *
     * @since Tika 1.6
     * @return translator instance
     */
    public Translator getTranslator() {
        return translator;
    }

    //--------------------------------------------------------------< Object >

    public String toString() {
        String version = null;

        try {
            InputStream stream = Tika.class.getResourceAsStream(
                    "/META-INF/maven/org.apache.tika/tika-core/pom.properties");
            if (stream != null) {
                try {
                    Properties properties = new Properties();
                    properties.load(stream);
                    version = properties.getProperty("version");
                } finally {
                    stream.close();
                }
            }
        } catch (Exception ignore) {
        }

        if (version != null) {
            return "Apache Tika " + version;
        } else {
            return "Apache Tika";
        }
    }

}
"
tika-core/src/main/java/org/apache/tika/config/LoadErrorHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.config;

import java.util.logging.Level;
import java.util.logging.Logger;

/**
 * Interface for error handling strategies in service class loading.
 * You can implement this interface for a custom error handling mechanism,
 * or use one of the predefined strategies.
 *
 * @since Apache Tika 0.9
 */
public interface LoadErrorHandler {

    /**
     * Handles a problem encountered when trying to load the specified
     * service class. The implementation can log or otherwise process
     * the given error information. If the method returns normally, then
     * the service loader simply skips this class and continues with the
     * next one.
     *
     * @param classname name of the service class
     * @param throwable the encountered problem
     */
    void handleLoadError(String classname, Throwable throwable);

    /**
     * Strategy that simply ignores all problems.
     */
    LoadErrorHandler IGNORE = new LoadErrorHandler() {
        public void handleLoadError(String classname, Throwable throwable) {
        }
    };

    /**
     * Strategy that logs warnings of all problems using a {@link Logger}
     * created using the given class name.
     */
    LoadErrorHandler WARN = new LoadErrorHandler() {
        public void handleLoadError(String classname, Throwable throwable) {
            Logger.getLogger(classname).log(
                    Level.WARNING, "Unable to load " + classname, throwable);
        }
    };

    /**
     * Strategy that throws a {@link RuntimeException} with the given
     * throwable as the root cause, thus interrupting the entire service
     * loading operation.
     */
    LoadErrorHandler THROW = new LoadErrorHandler() {
        public void handleLoadError(String classname, Throwable throwable) {
            throw new RuntimeException("Unable to load " + classname, throwable);
        }
    };

}
"
tika-core/src/main/java/org/apache/tika/config/package-info.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Tika configuration tools.
 */
@aQute.bnd.annotation.Version("1.0.0")
package org.apache.tika.config;
"
tika-core/src/main/java/org/apache/tika/config/ServiceLoader.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.config;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.net.URL;
import java.util.ArrayList;
import java.util.Collection;
import java.util.Collections;
import java.util.Enumeration;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.regex.Pattern;

/**
 * Internal utility class that Tika uses to look up service providers.
 *
 * @since Apache Tika 0.9
 */
public class ServiceLoader {

    /**
     * The default context class loader to use for all threads, or
     * <code>null</code> to automatically select the context class loader.
     */
    private static volatile ClassLoader contextClassLoader = null;

    private static class RankedService implements Comparable<RankedService> {
        private Object service;
        private int rank;

        public RankedService(Object service, int rank) {
            this.service = service;
            this.rank = rank;
        }

        public boolean isInstanceOf(Class<?> iface) {
            return iface.isAssignableFrom(service.getClass());
        }

        public int compareTo(RankedService that) {
            return that.rank - rank; // highest number first
        }

    }

    /**
     * The dynamic set of services available in an OSGi environment.
     * Managed by the {@link TikaActivator} class and used as an additional
     * source of service instances in the {@link #loadServiceProviders(Class)}
     * method.
     */
    private static final Map<Object, RankedService> services =
            new HashMap<Object, RankedService>();

    /**
     * Returns the context class loader of the current thread. If such
     * a class loader is not available, then the loader of this class or
     * finally the system class loader is returned.
     *
     * @see <a href="https://issues.apache.org/jira/browse/TIKA-441">TIKA-441</a>
     * @return context class loader, or <code>null</code> if no loader
     *         is available
     */
    static ClassLoader getContextClassLoader() {
        ClassLoader loader = contextClassLoader;
        if (loader == null) {
            loader = ServiceLoader.class.getClassLoader();
        }
        if (loader == null) {
            loader = ClassLoader.getSystemClassLoader();
        }
        return loader;
    }

    /**
     * Sets the context class loader to use for all threads that access
     * this class. Used for example in an OSGi environment to avoid problems
     * with the default context class loader.
     *
     * @param loader default context class loader,
     *               or <code>null</code> to automatically pick the loader
     */
    public static void setContextClassLoader(ClassLoader loader) {
        contextClassLoader = loader;
    }

    static void addService(Object reference, Object service, int rank) {
        synchronized (services) {
            services.put(reference, new RankedService(service, rank));
        }
    }

    static Object removeService(Object reference) {
        synchronized (services) {
            return services.remove(reference);
        }
    }

    private final ClassLoader loader;

    private final LoadErrorHandler handler;

    private final boolean dynamic;

    public ServiceLoader(
            ClassLoader loader, LoadErrorHandler handler, boolean dynamic) {
        this.loader = loader;
        this.handler = handler;
        this.dynamic = dynamic;
    }

    public ServiceLoader(ClassLoader loader, LoadErrorHandler handler) {
        this(loader, handler, false);
    }

    public ServiceLoader(ClassLoader loader) {
        this(loader, LoadErrorHandler.IGNORE);
    }

    public ServiceLoader() {
        this(getContextClassLoader(), LoadErrorHandler.IGNORE, true);
    }

    /**
     * Returns the load error handler used by this loader.
     *
     * @return load error handler
     * @since Apache Tika 1.3
     */
    public LoadErrorHandler getLoadErrorHandler() {
        return handler;
    }

    /**
     * Returns an input stream for reading the specified resource from the
     * configured class loader.
     *
     * @param name resource name
     * @return input stream, or <code>null</code> if the resource was not found
     * @see ClassLoader#getResourceAsStream(String)
     * @since Apache Tika 1.1
     */
    public InputStream getResourceAsStream(String name) {
        if (loader != null) {
            return loader.getResourceAsStream(name);
        } else {
            return null;
        }
    }

    /**
     * Loads and returns the named service class that's expected to implement
     * the given interface.
     *
     * @param iface service interface
     * @param name service class name
     * @return service class
     * @throws ClassNotFoundException if the service class can not be found
     *                                or does not implement the given interface
     * @see Class#forName(String, boolean, ClassLoader)
     * @since Apache Tika 1.1
     */
    @SuppressWarnings("unchecked")
    public <T> Class<? extends T> getServiceClass(Class<T> iface, String name)
            throws ClassNotFoundException {
        if (loader == null) {
            throw new ClassNotFoundException(
                    "Service class " + name + " is not available");
        }
        Class<?> klass = Class.forName(name, true, loader);
        if (klass.isInterface()) {
            throw new ClassNotFoundException(
                    "Service class " + name + " is an interface");
        } else if (!iface.isAssignableFrom(klass)) {
            throw new ClassNotFoundException(
                    "Service class " + name
                    + " does not implement " + iface.getName());
        } else {
            return (Class<? extends T>) klass;
        }
    }

    /**
     * Returns all the available service resources matching the
     *  given pattern, such as all instances of tika-mimetypes.xml 
     *  on the classpath, or all org.apache.tika.parser.Parser 
     *  service files.
     */
    public Enumeration<URL> findServiceResources(String filePattern) {
       try {
          Enumeration<URL> resources = loader.getResources(filePattern);
          return resources;
       } catch (IOException ignore) {
          // We couldn't get the list of service resource files
          List<URL> empty = Collections.emptyList();
          return Collections.enumeration( empty );
      }
    }

    /**
     * Returns all the available service providers of the given type.
     *
     * @param iface service provider interface
     * @return available service providers
     */
    public <T> List<T> loadServiceProviders(Class<T> iface) {
        List<T> providers = new ArrayList<T>();
        providers.addAll(loadDynamicServiceProviders(iface));
        providers.addAll(loadStaticServiceProviders(iface));
        return providers;
    }

    /**
     * Returns the available dynamic service providers of the given type.
     * The returned list is newly allocated and may be freely modified
     * by the caller.
     *
     * @since Apache Tika 1.2
     * @param iface service provider interface
     * @return dynamic service providers
     */
    @SuppressWarnings("unchecked")
    public <T> List<T> loadDynamicServiceProviders(Class<T> iface) {
        if (dynamic) {
            synchronized (services) {
                List<RankedService> list =
                        new ArrayList<RankedService>(services.values());
                Collections.sort(list);

                List<T> providers = new ArrayList<T>(list.size());
                for (RankedService service : list) {
                    if (service.isInstanceOf(iface)) {
                        providers.add((T) service.service);
                    }
                }
                return providers;
            }
        } else {
            return new ArrayList<T>(0);
        }
    }

    /**
     * Returns the defined static service providers of the given type, without
     * attempting to load them.
     * The providers are loaded using the service provider mechanism using
     * the configured class loader (if any).
     *
     * @since Apache Tika 1.6
     * @param iface service provider interface
     * @return static list of uninitialised service providers
     */
    protected <T> List<String> identifyStaticServiceProviders(Class<T> iface) {
        List<String> names = new ArrayList<String>();

        if (loader != null) {
            String serviceName = iface.getName();
            Enumeration<URL> resources =
                    findServiceResources("META-INF/services/" + serviceName);
            for (URL resource : Collections.list(resources)) {
                try {
                    collectServiceClassNames(resource, names);
                } catch (IOException e) {
                    handler.handleLoadError(serviceName, e);
                }
            }
        }
        
        return names;
    }

    /**
     * Returns the available static service providers of the given type.
     * The providers are loaded using the service provider mechanism using
     * the configured class loader (if any). The returned list is newly
     * allocated and may be freely modified by the caller.
     *
     * @since Apache Tika 1.2
     * @param iface service provider interface
     * @return static service providers
     */
    @SuppressWarnings("unchecked")
    public <T> List<T> loadStaticServiceProviders(Class<T> iface) {
        List<T> providers = new ArrayList<T>();

        if (loader != null) {
            List<String> names = identifyStaticServiceProviders(iface);

            for (String name : names) {
                try {
                    Class<?> klass = loader.loadClass(name);
                    if (iface.isAssignableFrom(klass)) {
                        providers.add((T) klass.newInstance());
                    }
                } catch (Throwable t) {
                    handler.handleLoadError(name, t);
                }
            }
        }

        return providers;
    }

    private static final Pattern COMMENT = Pattern.compile("#.*");

    private static final Pattern WHITESPACE = Pattern.compile("\\s+");

    private void collectServiceClassNames(URL resource, Collection<String> names)
            throws IOException {
        InputStream stream = resource.openStream();
        try {
            BufferedReader reader =
                new BufferedReader(new InputStreamReader(stream, "UTF-8"));
            String line = reader.readLine();
            while (line != null) {
                line = COMMENT.matcher(line).replaceFirst("");
                line = WHITESPACE.matcher(line).replaceAll("");
                if (line.length() > 0) {
                    names.add(line);
                }
                line = reader.readLine();
            }
        } finally {
            stream.close();
        }
    }

}
"
tika-core/src/main/java/org/apache/tika/config/TikaActivator.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.config;

import org.apache.tika.detect.Detector;
import org.apache.tika.parser.Parser;
import org.osgi.framework.BundleActivator;
import org.osgi.framework.BundleContext;
import org.osgi.framework.Constants;
import org.osgi.framework.ServiceReference;
import org.osgi.util.tracker.ServiceTracker;
import org.osgi.util.tracker.ServiceTrackerCustomizer;

/**
 * Bundle activator that adjust the class loading mechanism of the
 * {@link ServiceLoader} class to work correctly in an OSGi environment.
 * <p>
 * Note that you should <strong>not</strong> access this class directly.
 * Instead the OSGi environment (if present) will automatically invoke the
 * methods of this class based on the Bundle-Activator setting in the bundle
 * manifest.
 *
 * @since Apache Tika 0.9
 */
public class TikaActivator implements BundleActivator, ServiceTrackerCustomizer {

    private ServiceTracker detectorTracker;

    private ServiceTracker parserTracker;

    private BundleContext bundleContext;
    //-----------------------------------------------------< BundleActivator >

    public void start(final BundleContext context) throws Exception {
        bundleContext = context;

        detectorTracker = new ServiceTracker(context, Detector.class.getName(), this);
        parserTracker = new ServiceTracker(context, Parser.class.getName(), this);

        detectorTracker.open();
        parserTracker.open();
    }

    public void stop(BundleContext context) throws Exception {
        parserTracker.close();
        detectorTracker.close();
    }

    public Object addingService(ServiceReference reference) {
        int rank = 0;
        Object property = reference.getProperty(Constants.SERVICE_RANKING);
        if (property instanceof Integer) {
            rank = (Integer) property;
        }

        Object service = bundleContext.getService(reference);
        ServiceLoader.addService(reference, service, rank);
        return service;
    }

    public void modifiedService(ServiceReference reference, Object service) {
    }

    public void removedService(ServiceReference reference, Object service) {
        ServiceLoader.removeService(reference);
        bundleContext.ungetService(reference);
    }

}
"
tika-core/src/main/java/org/apache/tika/config/TikaConfig.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.config;

import java.io.File;
import java.io.FileInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.net.URL;
import java.util.ArrayList;
import java.util.Collections;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

import javax.imageio.spi.ServiceRegistry;
import javax.xml.parsers.DocumentBuilder;
import javax.xml.parsers.DocumentBuilderFactory;
import javax.xml.parsers.ParserConfigurationException;

import org.apache.tika.detect.CompositeDetector;
import org.apache.tika.detect.DefaultDetector;
import org.apache.tika.detect.Detector;
import org.apache.tika.exception.TikaException;
import org.apache.tika.language.translate.DefaultTranslator;
import org.apache.tika.language.translate.Translator;
import org.apache.tika.mime.MediaType;
import org.apache.tika.mime.MediaTypeRegistry;
import org.apache.tika.mime.MimeTypeException;
import org.apache.tika.mime.MimeTypes;
import org.apache.tika.mime.MimeTypesFactory;
import org.apache.tika.parser.AutoDetectParser;
import org.apache.tika.parser.CompositeParser;
import org.apache.tika.parser.DefaultParser;
import org.apache.tika.parser.Parser;
import org.apache.tika.parser.ParserDecorator;
import org.w3c.dom.Document;
import org.w3c.dom.Element;
import org.w3c.dom.Node;
import org.w3c.dom.NodeList;
import org.xml.sax.SAXException;

/**
 * Parse xml config file.
 */
public class TikaConfig {

    private static MimeTypes getDefaultMimeTypes(ClassLoader loader) {
        return MimeTypes.getDefaultMimeTypes(loader);
    }

    private static Detector getDefaultDetector(
            MimeTypes types, ServiceLoader loader) {
        return new DefaultDetector(types, loader);
    }

    private static CompositeParser getDefaultParser(
            MimeTypes types, ServiceLoader loader) {
        return new DefaultParser(types.getMediaTypeRegistry(), loader);
    }

    private static Translator getDefaultTranslator(ServiceLoader loader) {
        return new DefaultTranslator(loader);
    }
    private final CompositeParser parser;
    private final Detector detector;
    private final Translator translator;

    private final MimeTypes mimeTypes;

    public TikaConfig(String file)
            throws TikaException, IOException, SAXException {
        this(new File(file));
    }

    public TikaConfig(File file)
            throws TikaException, IOException, SAXException {
        this(getBuilder().parse(file));
    }

    public TikaConfig(URL url)
            throws TikaException, IOException, SAXException {
        this(url, ServiceLoader.getContextClassLoader());
    }

    public TikaConfig(URL url, ClassLoader loader)
            throws TikaException, IOException, SAXException {
        this(getBuilder().parse(url.toString()).getDocumentElement(), loader);
    }

    public TikaConfig(InputStream stream)
            throws TikaException, IOException, SAXException {
        this(getBuilder().parse(stream));
    }

    public TikaConfig(Document document) throws TikaException, IOException {
        this(document.getDocumentElement());
    }

    public TikaConfig(Element element) throws TikaException, IOException {
        this(element, new ServiceLoader());
    }

    public TikaConfig(Element element, ClassLoader loader)
            throws TikaException, IOException {
        this(element, new ServiceLoader(loader));
    }

    private TikaConfig(Element element, ServiceLoader loader)
            throws TikaException, IOException {
        this.mimeTypes = typesFromDomElement(element);
        this.detector = detectorFromDomElement(element, mimeTypes, loader);
        this.parser = parserFromDomElement(element, mimeTypes, loader);
        this.translator = translatorFromDomElement(element, loader);
    }

    /**
     * Creates a Tika configuration from the built-in media type rules
     * and all the {@link Parser} implementations available through the
     * {@link ServiceRegistry service provider mechanism} in the given
     * class loader.
     *
     * @since Apache Tika 0.8
     * @param loader the class loader through which parser implementations
     *               are loaded, or <code>null</code> for no parsers
     * @throws MimeTypeException if the built-in media type rules are broken
     * @throws IOException  if the built-in media type rules can not be read
     */
    public TikaConfig(ClassLoader loader)
            throws MimeTypeException, IOException {
        ServiceLoader serviceLoader = new ServiceLoader(loader);
        this.mimeTypes = getDefaultMimeTypes(loader);
        this.detector = getDefaultDetector(mimeTypes, serviceLoader);
        this.parser = getDefaultParser(mimeTypes, serviceLoader);
        this.translator = getDefaultTranslator(serviceLoader);
    }

    /**
     * Creates a default Tika configuration.
     * First checks whether an XML config file is specified, either in
     * <ol>
     * <li>System property "tika.config", or</li>
     * <li>Environment variable TIKA_CONFIG</li>
     * </ol>
     * <p>If one of these have a value, try to resolve it relative to file
     * system or classpath.</p>
     * <p>If XML config is not specified, initialize from the built-in media
     * type rules and all the {@link Parser} implementations available through
     * the {@link ServiceRegistry service provider mechanism} in the context
     * class loader of the current thread.</p>
     *
     * @throws IOException if the configuration can not be read
     * @throws TikaException if problem with MimeTypes or parsing XML config
     */
    public TikaConfig() throws TikaException, IOException {
        ServiceLoader loader = new ServiceLoader();

        String config = System.getProperty("tika.config");
        if (config == null) {
            config = System.getenv("TIKA_CONFIG");
        }

        if (config == null) {
            this.mimeTypes = getDefaultMimeTypes(ServiceLoader.getContextClassLoader());
            this.parser = getDefaultParser(mimeTypes, loader);
            this.detector = getDefaultDetector(mimeTypes, loader);
            this.translator = getDefaultTranslator(loader);
        } else {
            // Locate the given configuration file
            InputStream stream = null;
            File file = new File(config);
            if (file.isFile()) {
                stream = new FileInputStream(file);
            }
            if (stream == null) {
                try {
                    stream = new URL(config).openStream();
                } catch (IOException ignore) {
                }
            }
            if (stream == null) {
                stream = loader.getResourceAsStream(config);
            }
            if (stream == null) {
                throw new TikaException(
                        "Specified Tika configuration not found: " + config);
            }

            try {
                Element element =
                        getBuilder().parse(stream).getDocumentElement();
                this.mimeTypes = typesFromDomElement(element);
                this.parser =
                        parserFromDomElement(element, mimeTypes, loader);
                this.detector =
                        detectorFromDomElement(element, mimeTypes, loader);
                this.translator = translatorFromDomElement(element, loader);
            } catch (SAXException e) {
                throw new TikaException(
                        "Specified Tika configuration has syntax errors: "
                                + config, e);
            } finally {
                stream.close();
            }
        }
    }

    private static String getText(Node node) {
        if (node.getNodeType() == Node.TEXT_NODE) {
            return node.getNodeValue();
        } else if (node.getNodeType() == Node.ELEMENT_NODE) {
            StringBuilder builder = new StringBuilder();
            NodeList list = node.getChildNodes();
            for (int i = 0; i < list.getLength(); i++) {
                builder.append(getText(list.item(i)));
            }
            return builder.toString();
        } else {
            return "";
        }
    }

    /**
     * @deprecated Use the {@link #getParser()} method instead
     */
    public Parser getParser(MediaType mimeType) {
        return parser.getParsers().get(mimeType);
    }

    /**
     * Returns the configured parser instance.
     *
     * @return configured parser
     */
    public Parser getParser() {
        return parser;
    }

    /**
     * Returns the configured detector instance.
     *
     * @return configured detector
     */
    public Detector getDetector() {
        return detector;
    }

    /**
     * Returns the configured translator instance.
     *
     * @return configured translator
     */
    public Translator getTranslator() {
        return translator;
    }

    public MimeTypes getMimeRepository(){
        return mimeTypes;
    }

    public MediaTypeRegistry getMediaTypeRegistry() {
        return mimeTypes.getMediaTypeRegistry();
    }

    /**
     * Provides a default configuration (TikaConfig).  Currently creates a
     * new instance each time it's called; we may be able to have it
     * return a shared instance once it is completely immutable.
     *
     * @return default configuration
     */
    public static TikaConfig getDefaultConfig() {
        try {
            return new TikaConfig();
        } catch (IOException e) {
            throw new RuntimeException(
                    "Unable to read default configuration", e);
        } catch (TikaException e) {
            throw new RuntimeException(
                    "Unable to access default configuration", e);
        }
    }

    private static DocumentBuilder getBuilder() throws TikaException {
        try {
            return DocumentBuilderFactory.newInstance().newDocumentBuilder();
        } catch (ParserConfigurationException e) {
            throw new TikaException("XML parser not available", e);
        }
    }

    private static Element getChild(Element element, String name) {
        Node child = element.getFirstChild();
        while (child != null) {
            if (child.getNodeType() == Node.ELEMENT_NODE
                    && name.equals(child.getNodeName())) {
                return (Element) child;
            }
            child = child.getNextSibling();
        }
        return null;
    }

    private static MimeTypes typesFromDomElement(Element element)
            throws TikaException, IOException {
        Element mtr = getChild(element, "mimeTypeRepository");
        if (mtr != null && mtr.hasAttribute("resource")) {
            return MimeTypesFactory.create(mtr.getAttribute("resource"));
        } else {
            return getDefaultMimeTypes(null);
        }
    }

    private static CompositeParser parserFromDomElement(
            Element element, MimeTypes mimeTypes, ServiceLoader loader)
            throws TikaException, IOException {
        List<Parser> parsers = new ArrayList<Parser>();
        NodeList nodes = element.getElementsByTagName("parser");
        for (int i = 0; i < nodes.getLength(); i++) {
            Element node = (Element) nodes.item(i);
            String name = node.getAttribute("class");

            try {
                Class<? extends Parser> parserClass =
                        loader.getServiceClass(Parser.class, name);
                // https://issues.apache.org/jira/browse/TIKA-866
                if (AutoDetectParser.class.isAssignableFrom(parserClass)) {
                    throw new TikaException(
                            "AutoDetectParser not supported in a <parser>"
                            + " configuration element: " + name);
                }
                Parser parser = parserClass.newInstance();

                // Is there an explicit list of mime types for this to handle?
                Set<MediaType> parserTypes = mediaTypesListFromDomElement(node, "mime");
                if (! parserTypes.isEmpty()) {
                    parser = ParserDecorator.withTypes(parser, parserTypes);
                }
                // Is there an explicit list of mime types this shouldn't handle?
                Set<MediaType> parserExclTypes = mediaTypesListFromDomElement(node, "mime-exclude");
                if (! parserExclTypes.isEmpty()) {
                    parser = ParserDecorator.withoutTypes(parser, parserExclTypes);
                }

                // All done with setup
                parsers.add(parser);
            } catch (ClassNotFoundException e) {
                throw new TikaException(
                        "Unable to find a parser class: " + name, e);
            } catch (IllegalAccessException e) {
                throw new TikaException(
                        "Unable to access a parser class: " + name, e);
            } catch (InstantiationException e) {
                throw new TikaException(
                        "Unable to instantiate a parser class: " + name, e);
            }
        }
        if (parsers.isEmpty()) {
            return getDefaultParser(mimeTypes, loader);
        } else {
            MediaTypeRegistry registry = mimeTypes.getMediaTypeRegistry();
            return new CompositeParser(registry, parsers);
        }
    }
    private static Set<MediaType> mediaTypesListFromDomElement(
            Element node, String tag) 
            throws TikaException, IOException {
        NodeList mimes = node.getElementsByTagName(tag);
        if (mimes.getLength() > 0) {
            Set<MediaType> types = new HashSet<MediaType>();
            for (int j = 0; j < mimes.getLength(); j++) {
                String mime = getText(mimes.item(j));
                MediaType type = MediaType.parse(mime);
                if (type != null) {
                    types.add(type);
                } else {
                    throw new TikaException(
                            "Invalid media type name: " + mime);
                }
            }
            return types;
        }
        return Collections.emptySet();
    }

    private static Detector detectorFromDomElement(
          Element element, MimeTypes mimeTypes, ServiceLoader loader)
          throws TikaException, IOException {
       List<Detector> detectors = new ArrayList<Detector>();
       NodeList nodes = element.getElementsByTagName("detector");
       for (int i = 0; i < nodes.getLength(); i++) {
           Element node = (Element) nodes.item(i);
           String name = node.getAttribute("class");

           try {
               Class<? extends Detector> detectorClass =
                       loader.getServiceClass(Detector.class, name);
               detectors.add(detectorClass.newInstance());
           } catch (ClassNotFoundException e) {
               throw new TikaException(
                       "Unable to find a detector class: " + name, e);
           } catch (IllegalAccessException e) {
               throw new TikaException(
                       "Unable to access a detector class: " + name, e);
           } catch (InstantiationException e) {
               throw new TikaException(
                       "Unable to instantiate a detector class: " + name, e);
           }
       }
       if (detectors.isEmpty()) {
           return getDefaultDetector(mimeTypes, loader);
       } else {
           MediaTypeRegistry registry = mimeTypes.getMediaTypeRegistry();
           return new CompositeDetector(registry, detectors);
       }
    }

    private static Translator translatorFromDomElement(
            Element element, ServiceLoader loader)
            throws TikaException, IOException {
        List<Translator> translators = new ArrayList<Translator>();
        NodeList nodes = element.getElementsByTagName("translator");
        for (int i = 0; i < nodes.getLength(); i++) {
            Element node = (Element) nodes.item(i);
            String name = node.getAttribute("class");

            try {
                Class<? extends Translator> translatorClass =
                        loader.getServiceClass(Translator.class, name);
                translators.add(translatorClass.newInstance());
            } catch (ClassNotFoundException e) {
                throw new TikaException(
                        "Unable to find a translator class: " + name, e);
            } catch (IllegalAccessException e) {
                throw new TikaException(
                        "Unable to access a translator class: " + name, e);
            } catch (InstantiationException e) {
                throw new TikaException(
                        "Unable to instantiate a translator class: " + name, e);
            }
        }
        if (translators.isEmpty()) {
            return getDefaultTranslator(loader);
        } else {
            return translators.get(0);
        }
    }
}
"
tika-core/src/main/java/org/apache/tika/detect/AutoDetectReader.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.detect;

import java.io.BufferedInputStream;
import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.nio.charset.Charset;
import java.util.List;

import org.apache.tika.config.LoadErrorHandler;
import org.apache.tika.config.ServiceLoader;
import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.utils.CharsetUtils;
import org.xml.sax.InputSource;

/**
 * An input stream reader that automatically detects the character encoding
 * to be used for converting bytes to characters.
 *
 * @since Apache Tika 1.2
 */
public class AutoDetectReader extends BufferedReader {

    private static final ServiceLoader DEFAULT_LOADER =
            new ServiceLoader(AutoDetectReader.class.getClassLoader());

    private static Charset detect(
            InputStream input, Metadata metadata,
            List<EncodingDetector> detectors, LoadErrorHandler handler)
            throws IOException, TikaException {
        // Ask all given detectors for the character encoding
        for (EncodingDetector detector : detectors) {
            try {
                Charset charset = detector.detect(input, metadata);
                if (charset != null) {
                    return charset;
                }
            } catch (NoClassDefFoundError e) {
                // TIKA-1041: Detector dependencies not present.
                handler.handleLoadError(detector.getClass().getName(), e);
            }
        }

        // Try determining the encoding based on hints in document metadata
        MediaType type = MediaType.parse(metadata.get(Metadata.CONTENT_TYPE));
        if (type != null) {
            String charset = type.getParameters().get("charset");
            if (charset != null) {
                try {
                    return CharsetUtils.forName(charset);
                } catch (Exception e) {
                    // ignore
                }
            }
        }

        throw new TikaException(
                "Failed to detect the character encoding of a document");
    }

    private final Charset charset;

    private AutoDetectReader(InputStream stream, Charset charset)
            throws IOException {
        super(new InputStreamReader(stream, charset));
        this.charset = charset;

        // TIKA-240: Drop the BOM if present
        mark(1);
        if (read() != '\ufeff') { // zero-width no-break space
            reset();
        }
    }

    private AutoDetectReader(
            BufferedInputStream stream, Metadata metadata,
            List<EncodingDetector> detectors, LoadErrorHandler handler)
            throws IOException, TikaException {
        this(stream, detect(stream, metadata, detectors, handler));
    }

    public AutoDetectReader(
            InputStream stream, Metadata metadata,
            ServiceLoader loader) throws IOException, TikaException {
        this(new BufferedInputStream(stream), metadata,
                loader.loadServiceProviders(EncodingDetector.class),
                loader.getLoadErrorHandler());
    }

    public AutoDetectReader(InputStream stream, Metadata metadata)
            throws IOException, TikaException {
        this(new BufferedInputStream(stream), metadata, DEFAULT_LOADER);
    }

    public AutoDetectReader(InputStream stream)
            throws IOException, TikaException {
        this(stream, new Metadata());
    }

    public Charset getCharset() {
        return charset;
    }

    public InputSource asInputSource() {
        InputSource source = new InputSource(this);
        source.setEncoding(charset.name());
        return source;
    }

}
"
tika-core/src/main/java/org/apache/tika/detect/CompositeDetector.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.detect;

import java.io.IOException;
import java.io.InputStream;
import java.util.Arrays;
import java.util.Collections;
import java.util.List;

import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.mime.MediaTypeRegistry;

/**
 * Content type detector that combines multiple different detection mechanisms.
 */
public class CompositeDetector implements Detector {

    /**
     * Serial version UID
     */
    private static final long serialVersionUID = 5980683158436430252L;

    private final MediaTypeRegistry registry;

    private final List<Detector> detectors;

    public CompositeDetector(
            MediaTypeRegistry registry, List<Detector> detectors) {
        this.registry = registry;
        this.detectors = detectors;
    }

    public CompositeDetector(List<Detector> detectors) {
        this(new MediaTypeRegistry(), detectors);
    }

    public CompositeDetector(Detector... detectors) {
        this(Arrays.asList(detectors));
    }

    public MediaType detect(InputStream input, Metadata metadata)
            throws IOException { 
        MediaType type = MediaType.OCTET_STREAM;
        for (Detector detector : getDetectors()) {
            MediaType detected = detector.detect(input, metadata);
            if (registry.isSpecializationOf(detected, type)) {
                type = detected;
            }
        }
        return type;
    }

    /**
     * Returns the component detectors.
     */
    public List<Detector> getDetectors() {
       return Collections.unmodifiableList(detectors);
    }
}
"
tika-core/src/main/java/org/apache/tika/detect/DefaultDetector.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.detect;

import java.util.Collections;
import java.util.Comparator;
import java.util.List;

import javax.imageio.spi.ServiceRegistry;

import org.apache.tika.config.ServiceLoader;
import org.apache.tika.mime.MimeTypes;

/**
 * A composite detector based on all the {@link Detector} implementations
 * available through the {@link ServiceRegistry service provider mechanism}.
 * 
 * Detectors are loaded and returned in a specified order, of user supplied
 *  followed by non-MimeType Tika, followed by the Tika MimeType class.
 * If you need to control the order of the Detectors, you should instead
 *  construct your own {@link CompositeDetector} and pass in the list
 *  of Detectors in the required order.
 *
 * @since Apache Tika 0.9
 */
public class DefaultDetector extends CompositeDetector {

    /** Serial version UID */
    private static final long serialVersionUID = -8170114575326908027L;

    /**
     * Finds all statically loadable detectors and sort the list by name,
     * rather than discovery order. Detectors are used in the given order,
     * so put the Tika parsers last so that non-Tika (user supplied)
     * parsers can take precedence.
     *
     * @param loader service loader
     * @return ordered list of statically loadable detectors
     */
    private static List<Detector> getDefaultDetectors(
            MimeTypes types, ServiceLoader loader) {
        List<Detector> detectors =
                loader.loadStaticServiceProviders(Detector.class);
        Collections.sort(detectors, new Comparator<Detector>() {
            public int compare(Detector d1, Detector d2) {
                String n1 = d1.getClass().getName();
                String n2 = d2.getClass().getName();
                boolean t1 = n1.startsWith("org.apache.tika.");
                boolean t2 = n2.startsWith("org.apache.tika.");
                if (t1 == t2) {
                    return n1.compareTo(n2);
                } else if (t1) {
                    return 1;
                } else {
                    return -1;
                }
            }
        });
        // Finally the Tika MimeTypes as a fallback
        detectors.add(types);
        return detectors;
    }

    private transient final ServiceLoader loader;

    public DefaultDetector(MimeTypes types, ServiceLoader loader) {
        super(types.getMediaTypeRegistry(), getDefaultDetectors(types, loader));
        this.loader = loader;
    }

    public DefaultDetector(MimeTypes types, ClassLoader loader) {
        this(types, new ServiceLoader(loader));
    }

    public DefaultDetector(ClassLoader loader) {
        this(MimeTypes.getDefaultMimeTypes(), loader);
    }

    public DefaultDetector(MimeTypes types) {
        this(types, new ServiceLoader());
    }

    public DefaultDetector() {
        this(MimeTypes.getDefaultMimeTypes());
    }

    @Override
    public List<Detector> getDetectors() {
        if (loader != null) {
            List<Detector> detectors =
                    loader.loadDynamicServiceProviders(Detector.class);
            detectors.addAll(super.getDetectors());
            return detectors;
        } else {
            return super.getDetectors();
        }
    }

}
"
tika-core/src/main/java/org/apache/tika/detect/Detector.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.detect;

import java.io.IOException;
import java.io.InputStream;
import java.io.Serializable;

import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;

/**
 * Content type detector. Implementations of this interface use various
 * heuristics to detect the content type of a document based on given
 * input metadata or the first few bytes of the document stream.
 *
 * @since Apache Tika 0.3
 */
public interface Detector extends Serializable {

    /**
     * Detects the content type of the given input document. Returns
     * <code>application/octet-stream</code> if the type of the document
     * can not be detected.
     * <p>
     * If the document input stream is not available, then the first
     * argument may be <code>null</code>. Otherwise the detector may
     * read bytes from the start of the stream to help in type detection.
     * The given stream is guaranteed to support the
     * {@link InputStream#markSupported() mark feature} and the detector
     * is expected to {@link InputStream#mark(int) mark} the stream before
     * reading any bytes from it, and to {@link InputStream#reset() reset}
     * the stream before returning. The stream must not be closed by the
     * detector.
     * <p>
     * The given input metadata is only read, not modified, by the detector.
     *
     * @param input document input stream, or <code>null</code>
     * @param metadata input metadata for the document
     * @return detected media type, or <code>application/octet-stream</code>
     * @throws IOException if the document input stream could not be read
     */
    MediaType detect(InputStream input, Metadata metadata) throws IOException;

}
"
tika-core/src/main/java/org/apache/tika/detect/EmptyDetector.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.detect;

import java.io.IOException;
import java.io.InputStream;

import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;

/**
 * Dummy detector that returns application/octet-stream for all documents.
 */
public class EmptyDetector implements Detector {

    /**
     * Singleton instance of this class.
     */
    public static final EmptyDetector INSTANCE = new EmptyDetector();

    public MediaType detect(InputStream input, Metadata metadata)
            throws IOException {
        return MediaType.OCTET_STREAM;
    }

}
"
tika-core/src/main/java/org/apache/tika/detect/EncodingDetector.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.detect;

import java.io.IOException;
import java.io.InputStream;
import java.nio.charset.Charset;

import org.apache.tika.metadata.Metadata;

/**
 * Character encoding detector. Implementations of this interface use
 * various heuristics to detect the character encoding of a text document
 * based on given input metadata or the first few bytes of the document stream.
 *
 * @since Apache Tika 0.4
 */
public interface EncodingDetector {

    /**
     * Detects the character encoding of the given text document, or
     * <code>null</code> if the encoding of the document can not be detected.
     * <p>
     * If the document input stream is not available, then the first
     * argument may be <code>null</code>. Otherwise the detector may
     * read bytes from the start of the stream to help in encoding detection.
     * The given stream is guaranteed to support the
     * {@link InputStream#markSupported() mark feature} and the detector
     * is expected to {@link InputStream#mark(int) mark} the stream before
     * reading any bytes from it, and to {@link InputStream#reset() reset}
     * the stream before returning. The stream must not be closed by the
     * detector.
     * <p>
     * The given input metadata is only read, not modified, by the detector.
     *
     * @param input text document input stream, or <code>null</code>
     * @param metadata input metadata for the document
     * @return detected character encoding, or <code>null</code>
     * @throws IOException if the document input stream could not be read
     */
    Charset detect(InputStream input, Metadata metadata) throws IOException;

}
"
tika-core/src/main/java/org/apache/tika/detect/MagicDetector.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.detect;

import java.io.CharArrayWriter;
import java.io.IOException;
import java.io.InputStream;
import java.io.UnsupportedEncodingException;
import java.nio.ByteBuffer;
import java.nio.CharBuffer;
import java.nio.charset.Charset;
import java.util.Locale;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;

/**
 * Content type detection based on magic bytes, i.e. type-specific patterns
 * near the beginning of the document input stream.
 *
 * Because this works on bytes, not characters, by default any string
 *  matching is done as ISO_8859_1. To use an explicit different
 *  encoding, supply a type other than "string" / "stringignorecase"
 *
 * @since Apache Tika 0.3
 */
public class MagicDetector implements Detector {

    private static final Charset ISO_8859_1 = Charset.forName("ISO-8859-1");

    public static MagicDetector parse(
            MediaType mediaType,
            String type, String offset, String value, String mask) {
        int start = 0;
        int end = 0;
        if (offset != null) {
            int colon = offset.indexOf(':');
            if (colon == -1) {
                start = Integer.parseInt(offset);
                end = start;
            } else {
                start = Integer.parseInt(offset.substring(0, colon));
                end = Integer.parseInt(offset.substring(colon + 1));
            }
        }

        byte[] patternBytes = decodeValue(value, type);
        byte[] maskBytes = null;
        if (mask != null) {
            maskBytes = decodeValue(mask, type);
        }

        return new MagicDetector(
                mediaType, patternBytes, maskBytes,
                type.equals("regex"), type.equals("stringignorecase"),
                start, end);
    }

    private static byte[] decodeValue(String value, String type) {
        // Preliminary check
        if ((value == null) || (type == null)) {
            return null;
        }

        byte[] decoded = null;
        String tmpVal = null;
        int radix = 8;

        // hex
        if (value.startsWith("0x")) {
            tmpVal = value.substring(2);
            radix = 16;
        } else {
            tmpVal = value;
            radix = 8;
        }

        if (type.equals("string")
                || type.equals("regex")
                || type.equals("unicodeLE")
                || type.equals("unicodeBE")) {
            decoded = decodeString(value, type);
        } else if (type.equals("stringignorecase")) {
            decoded = decodeString(value.toLowerCase(Locale.ROOT), type);
        } else if (type.equals("byte")) {
            try {
                decoded = tmpVal.getBytes("UTF-8");
            } catch (UnsupportedEncodingException e) {
                throw new AssertionError("UTF-8 not supported.");
            }
        } else if (type.equals("host16") || type.equals("little16")) {
            int i = Integer.parseInt(tmpVal, radix);
            decoded = new byte[] { (byte) (i & 0x00FF), (byte) (i >> 8) };
        } else if (type.equals("big16")) {
            int i = Integer.parseInt(tmpVal, radix);
            decoded = new byte[] { (byte) (i >> 8), (byte) (i & 0x00FF) };
        } else if (type.equals("host32") || type.equals("little32")) {
            long i = Long.parseLong(tmpVal, radix);
            decoded = new byte[] {
                    (byte) ((i & 0x000000FF)),
                    (byte) ((i & 0x0000FF00) >> 8),
                    (byte) ((i & 0x00FF0000) >> 16),
                    (byte) ((i & 0xFF000000) >> 24) };
        } else if (type.equals("big32")) {
            long i = Long.parseLong(tmpVal, radix);
            decoded = new byte[] {
                    (byte) ((i & 0xFF000000) >> 24),
                    (byte) ((i & 0x00FF0000) >> 16),
                    (byte) ((i & 0x0000FF00) >> 8),
                    (byte) ((i & 0x000000FF)) };
        }
        return decoded;
    }

    private static byte[] decodeString(String value, String type) {
        if (value.startsWith("0x")) {
            byte[] vals = new byte[(value.length() - 2) / 2];
            for (int i = 0; i < vals.length; i++) {
                vals[i] = (byte)
                Integer.parseInt(value.substring(2 + i * 2, 4 + i * 2), 16);
            }
            return vals;
        }

        CharArrayWriter decoded = new CharArrayWriter();

        for (int i = 0; i < value.length(); i++) {
            if (value.charAt(i) == '\\') {
                if (value.charAt(i + 1) == '\\') {
                    decoded.write('\\');
                    i++;
                } else if (value.charAt(i + 1) == 'x') {
                    decoded.write(Integer.parseInt(
                            value.substring(i + 2, i + 4), 16));
                    i += 3;
                } else if (value.charAt(i + 1) == 'r') {
                    decoded.write((int)'\r');
                    i++;
                } else if (value.charAt(i + 1) == 'n') {
                   decoded.write((int)'\n');
                   i++;
                } else {
                    int j = i + 1;
                    while ((j < i + 4) && (j < value.length())
                            && (Character.isDigit(value.charAt(j)))) {
                        j++;
                    }
                    decoded.write(Short.decode(
                            "0" + value.substring(i + 1, j)).byteValue());
                    i = j - 1;
                }
            } else {
                decoded.write(value.charAt(i));
            }
        }

        // Now turn the chars into bytes
        char[] chars = decoded.toCharArray();
        byte[] bytes;
        if ("unicodeLE".equals(type)) {
            bytes = new byte[chars.length * 2];
            for (int i = 0; i < chars.length; i++) {
                bytes[i * 2] = (byte) (chars[i] & 0xff);
                bytes[i * 2 + 1] = (byte) (chars[i] >> 8);
            }
        } else if ("unicodeBE".equals(type)) {
            bytes = new byte[chars.length * 2];
            for(int i = 0; i < chars.length; i++) {
                bytes[i * 2] = (byte) (chars[i] >> 8);
                bytes[i * 2 + 1] = (byte) (chars[i] & 0xff);
            }
        } else {
            // Copy with truncation
            bytes = new byte[chars.length];
            for(int i = 0; i < bytes.length; i++) {
                bytes[i] = (byte) chars[i];
            }
        }
        return bytes;
    }

    /**
     * The matching media type. Returned by the
     * {@link #detect(InputStream, Metadata)} method if a match is found.
     */
    private final MediaType type;

    /**
     * Length of the comparison window.
     */
    private final int length;

    /**
     * The magic match pattern. If this byte pattern is equal to the
     * possibly bit-masked bytes from the input stream, then the type
     * detection succeeds and the configured {@link #type} is returned.
     */
    private final byte[] pattern;
    
    /**
     * Length of the pattern, which in the case of regular expressions will
     * not be the same as the comparison window length.
     */
    private final int patternLength;
    
    /**
     * True if pattern is a regular expression, false otherwise.
     */
    private final boolean isRegex;

    /**
     * True if we're doing a case-insensitive string match, false otherwise.
     */
    private final boolean isStringIgnoreCase;

    /**
     * Bit mask that is applied to the source bytes before pattern matching.
     */
    private final byte[] mask;

    /**
     * First offset (inclusive) of the comparison window within the
     * document input stream. Greater than or equal to zero.
     */
    private final int offsetRangeBegin;

    /**
     * Last offset (inclusive) of the comparison window within the document
     * input stream. Greater than or equal to the
     * {@link #offsetRangeBegin first offset}.
     * <p>
     * Note that this is <em>not</em> the offset of the last byte read from
     * the document stream. Instead, the last window of bytes to be compared
     * starts at this offset.
     */
    private final int offsetRangeEnd;

    /**
     * Creates a detector for input documents that have the exact given byte
     * pattern at the beginning of the document stream.
     *
     * @param type matching media type
     * @param pattern magic match pattern
     */
    public MagicDetector(MediaType type, byte[] pattern) {
        this(type, pattern, 0);
    }

    /**
     * Creates a detector for input documents that have the exact given byte
     * pattern at the given offset of the document stream.
     *
     * @param type matching media type
     * @param pattern magic match pattern
     * @param offset offset of the pattern match
     */
    public MagicDetector(MediaType type, byte[] pattern, int offset) {
        this(type, pattern, null, offset, offset);
    }
    
    /**
     * Creates a detector for input documents that meet the specified magic
     * match.  {@code pattern} must NOT be a regular expression.
     * Constructor maintained for legacy reasons.
     */
    public MagicDetector(
        MediaType type, byte[] pattern, byte[] mask,
        int offsetRangeBegin, int offsetRangeEnd) {
        this(type, pattern, mask, false, offsetRangeBegin, offsetRangeEnd);
    }

    /**
     * Creates a detector for input documents that meet the specified
     * magic match.
     */
    public MagicDetector(
            MediaType type, byte[] pattern, byte[] mask,
            boolean isRegex,
            int offsetRangeBegin, int offsetRangeEnd) {
        this(type, pattern, mask, isRegex, false, offsetRangeBegin, offsetRangeEnd);
    }
    /**
     * Creates a detector for input documents that meet the specified
     * magic match.
     */
    public MagicDetector(
            MediaType type, byte[] pattern, byte[] mask,
            boolean isRegex, boolean isStringIgnoreCase,
            int offsetRangeBegin, int offsetRangeEnd) {
        if (type == null) {
            throw new IllegalArgumentException("Matching media type is null");
        } else if (pattern == null) {
            throw new IllegalArgumentException("Magic match pattern is null");
        } else if (offsetRangeBegin < 0
                || offsetRangeEnd < offsetRangeBegin) {
            throw new IllegalArgumentException(
                    "Invalid offset range: ["
                    + offsetRangeBegin + "," + offsetRangeEnd + "]");
        }

        this.type = type;

        this.isRegex = isRegex;
        this.isStringIgnoreCase = isStringIgnoreCase;

        this.patternLength = Math.max(pattern.length, mask != null ? mask.length : 0);

        if (this.isRegex) {
            // 8K buffer should cope with most regex patterns
            this.length = 8 * 1024;
        } else {
            this.length = patternLength;
        }

        this.mask = new byte[this.patternLength];
        this.pattern = new byte[this.patternLength];

        for (int i = 0; i < this.patternLength; i++) {
            if (mask != null && i < mask.length) {
                this.mask[i] = mask[i];
            } else {
                this.mask[i] = -1;
            }

            if (i < pattern.length) {
                this.pattern[i] = (byte) (pattern[i] & this.mask[i]);
            } else {
                this.pattern[i] = 0;
            }
        }

        this.offsetRangeBegin = offsetRangeBegin;
        this.offsetRangeEnd = offsetRangeEnd;
    }

    /**
     * 
     * @param input document input stream, or <code>null</code>
     * @param metadata ignored
     */
    public MediaType detect(InputStream input, Metadata metadata)
            throws IOException {
        if (input == null) {
            return MediaType.OCTET_STREAM;
        }

        input.mark(offsetRangeEnd + length);
        try {
            int offset = 0;

            // Skip bytes at the beginning, using skip() or read()
            while (offset < offsetRangeBegin) {
                long n = input.skip(offsetRangeBegin - offset);
                if (n > 0) {
                    offset += n;
                } else if (input.read() != -1) {
                    offset += 1;
                } else {
                    return MediaType.OCTET_STREAM;
                }
            }

            // Fill in the comparison window
            byte[] buffer =
                new byte[length + (offsetRangeEnd - offsetRangeBegin)];
            int n = input.read(buffer);
            if (n > 0) {
                offset += n;
            }
            while (n != -1 && offset < offsetRangeEnd + length) {
                int bufferOffset = offset - offsetRangeBegin;
                n = input.read(
                        buffer, bufferOffset, buffer.length - bufferOffset);
                // increment offset - in case not all read (see testDetectStreamReadProblems)
                if (n > 0) {
                    offset += n;
                }
            }

            if (this.isRegex) {
                int flags = 0;
                if (this.isStringIgnoreCase) {
                    flags = Pattern.CASE_INSENSITIVE;
                }
                
                Pattern p = Pattern.compile(new String(this.pattern, "UTF-8"), flags);

                ByteBuffer bb = ByteBuffer.wrap(buffer);
                CharBuffer result = ISO_8859_1.decode(bb);
                Matcher m = p.matcher(result);

                boolean match = false;
                // Loop until we've covered the entire offset range
                for (int i = 0; i <= offsetRangeEnd - offsetRangeBegin; i++) {
                    m.region(i,  length+i);
                    match = m.lookingAt(); // match regex from start of region
                    if (match) {
                        return type;
                    }
                }
            } else {
                if (offset < offsetRangeBegin + length) {
                    return MediaType.OCTET_STREAM;
                }
                // Loop until we've covered the entire offset range
                for (int i = 0; i <= offsetRangeEnd - offsetRangeBegin; i++) {
                    boolean match = true;
                    int masked;
                    for (int j = 0; match && j < length; j++) {
                        masked = (buffer[i + j] & mask[j]);
                        if (this.isStringIgnoreCase) {
                            masked = Character.toLowerCase(masked);
                        }
                        match = (masked == pattern[j]);
                    }
                    if (match) {
                        return type;
                    }
                }
            }

            return MediaType.OCTET_STREAM;
        } finally {
            input.reset();
        }
    }

    public int getLength() {
        return this.patternLength;
    }

    /**
     * Returns a string representation of the Detection Rule.
     * Should sort nicely by type and details, as we sometimes
     *  compare these.
     */
    public String toString() {
        // Needs to be unique, as these get compared.
        return "Magic Detection for " + type +
                " looking for " + pattern.length + 
                " bytes = " + this.pattern + 
                " mask = " + this.mask;
    }
}
"
tika-core/src/main/java/org/apache/tika/detect/NameDetector.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.detect;

import java.io.InputStream;
import java.io.UnsupportedEncodingException;
import java.net.URLDecoder;
import java.util.Map;
import java.util.regex.Pattern;

import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;

/**
 * Content type detection based on the resource name. An instance of this
 * class contains a set of regular expression patterns that are matched
 * against the resource name potentially given as a part of the input metadata.
 * <p>
 * If a pattern matches the given name, then the media type associated with
 * that pattern is returned as the likely content type of the input document.
 * Otherwise the returned type is <code>application/octet-stream</code>.
 * <p>
 * See the {@link #detect(InputStream, Metadata)} method for more details
 * of the matching algorithm.
 *
 * @since Apache Tika 0.3
 */
public class NameDetector implements Detector {

    /**
     * The regular expression patterns used for type detection.
     */
    private final Map<Pattern, MediaType> patterns;

    /**
     * Creates a new content type detector based on the given name patterns.
     * The given pattern map is not copied, so the caller may update the
     * mappings even after this detector instance has been created. However,
     * the map <em>must not be concurrently modified</em> while this instance
     * is used for type detection.
     *
     * @param patterns map from name patterns to corresponding media types
     */
    public NameDetector(Map<Pattern, MediaType> patterns) {
        this.patterns = patterns;
    }

    /**
     * Detects the content type of an input document based on the document
     * name given in the input metadata. The RESOURCE_NAME_KEY attribute of
     * the given input metadata is expected to contain the name (normally
     * a file name or a URL) of the input document.
     * <p>
     * If a resource name is given, then it is first processed as follows.
     * <ol>
     *   <li>
     *     Potential URL query (?...) and fragment identifier (#...)
     *     parts are removed from the end of the resource name.
     *   </li>
     *   <li>
     *     Potential leading path elements (up to the last slash or backslash)
     *     are removed from the beginning of the resource name.
     *   </li>
     *   <li>
     *     Potential URL encodings (%nn, in UTF-8) are decoded.
     *   </li>
     *   <li>
     *     Any leading and trailing whitespace is removed.
     *   </li>
     * </ol>
     * <p>
     * The resulting name string (if any) is then matched in sequence against
     * all the configured name patterns. If a match is found, then the (first)
     * matching media type is returned.
     *
     * @param input ignored
     * @param metadata input metadata, possibly with a RESOURCE_NAME_KEY value
     * @return detected media type, or <code>application/octet-stream</code>
     */
    public MediaType detect(InputStream input, Metadata metadata) {
        // Look for a resource name in the input metadata
        String name = metadata.get(Metadata.RESOURCE_NAME_KEY);
        if (name != null) {
            // If the name is a URL, skip the trailing query and fragment parts
            int question = name.indexOf('?');
            if (question != -1) {
                name = name.substring(0, question);
            }
            int hash = name.indexOf('#');
            if (hash != -1) {
                name = name.substring(0, hash);
            }

            // If the name is a URL or a path, skip all but the last component
            int slash = name.lastIndexOf('/');
            if (slash != -1) {
                name = name.substring(slash + 1);
            }
            int backslash = name.lastIndexOf('\\');
            if (backslash != -1) {
                name = name.substring(backslash + 1);
            }

            // Decode any potential URL encoding
            int percent = name.indexOf('%');
            if (percent != -1) {
                try {
                    name = URLDecoder.decode(name, "UTF-8");
                } catch (UnsupportedEncodingException e) {
                    throw new IllegalStateException("UTF-8 not supported", e);
                }
            }

            // Skip any leading or trailing whitespace
            name = name.trim();
            if (name.length() > 0) {
                // Match the name against the registered patterns
                for (Pattern pattern : patterns.keySet()) {
                    if (pattern.matcher(name).matches()) {
                        return patterns.get(pattern);
                    }
                }
            }
        }

        return MediaType.OCTET_STREAM;
    }

}
"
tika-core/src/main/java/org/apache/tika/detect/package-info.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Media type detection.
 */
@aQute.bnd.annotation.Version("1.0.0")
package org.apache.tika.detect;
"
tika-core/src/main/java/org/apache/tika/detect/TextDetector.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.detect;

import java.io.IOException;
import java.io.InputStream;
import java.util.Arrays;

import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;

/**
 * Content type detection of plain text documents. This detector looks at the
 * beginning of the document input stream and considers the document to be
 * a text document if no ASCII (ISO-Latin-1, UTF-8, etc.) control bytes are
 * found. As a special case some control bytes (up to 2% of all characters)
 * are also allowed in a text document if it also contains no or just a few
 * (less than 10%) characters above the 7-bit ASCII range.
 * <p>
 * Note that text documents with a character encoding like UTF-16 are better
 * detected with {@link MagicDetector} and an appropriate magic byte pattern.
 *
 * @since Apache Tika 0.3
 */
public class TextDetector implements Detector {

    /** Serial version UID */
    private static final long serialVersionUID = 4774601079503507765L;

    /**
     * The number of bytes from the beginning of the document stream
     * to test for control bytes.
     */
    private static final int DEFAULT_NUMBER_OF_BYTES_TO_TEST = 512;

    /**
     * Lookup table for all the ASCII/ISO-Latin/UTF-8/etc. control bytes
     * in the range below 0x20 (the space character). If an entry in this
     * table is <code>true</code> then that byte is very unlikely to occur
     * in a plain text document.
     * <p>
     * The contents of this lookup table are based on the following definition
     * from section 4 of the "Content-Type Processing Model" Internet-draft
     * (<a href="http://webblaze.cs.berkeley.edu/2009/mime-sniff/mime-sniff.txt"
     * >draft-abarth-mime-sniff-01</a>).
     * <pre>
     * +-------------------------+
     * | Binary data byte ranges |
     * +-------------------------+
     * | 0x00 -- 0x08            |
     * | 0x0B                    |
     * | 0x0E -- 0x1A            |
     * | 0x1C -- 0x1F            |
     * +-------------------------+
     * </pre>
     *
     * @see <a href="https://issues.apache.org/jira/browse/TIKA-154">TIKA-154</a>
     */
    private static final boolean[] IS_CONTROL_BYTE = new boolean[0x20];

    static {
        Arrays.fill(IS_CONTROL_BYTE, true);
        IS_CONTROL_BYTE[0x09] = false; // tabulator
        IS_CONTROL_BYTE[0x0A] = false; // new line
        IS_CONTROL_BYTE[0x0C] = false; // new page
        IS_CONTROL_BYTE[0x0D] = false; // carriage return
        IS_CONTROL_BYTE[0x1B] = false; // escape
    }

    private final int bytesToTest;
    
    /**
     * Constructs a {@link TextDetector} which will look at the default number
     * of bytes from the beginning of the document.
     */
    public TextDetector() {
        this(DEFAULT_NUMBER_OF_BYTES_TO_TEST);
    }

    /**
     * Constructs a {@link TextDetector} which will look at a given number of
     * bytes from the beginning of the document.
     */
    public TextDetector(int bytesToTest) {
        this.bytesToTest = bytesToTest;
    }
    
    /**
     * Looks at the beginning of the document input stream to determine
     * whether the document is text or not.
     *
     * @param input document input stream, or <code>null</code>
     * @param metadata ignored
     * @return "text/plain" if the input stream suggest a text document,
     *         "application/octet-stream" otherwise
     */
    public MediaType detect(InputStream input, Metadata metadata)
            throws IOException {
        if (input == null) {
            return MediaType.OCTET_STREAM;
        }

        input.mark(bytesToTest);
        try {
            TextStatistics stats = new TextStatistics();

            byte[] buffer = new byte[1024];
            int n = 0;
            int m = input.read(buffer, 0, Math.min(bytesToTest, buffer.length));
            while (m != -1 && n < bytesToTest) {
                stats.addData(buffer, 0, m);
                n += m;
                m = input.read(buffer, 0, Math.min(bytesToTest - n, buffer.length));
            }

            if (stats.isMostlyAscii() || stats.looksLikeUTF8()) {
                return MediaType.TEXT_PLAIN;
            } else {
                return MediaType.OCTET_STREAM;
            }
        } finally {
            input.reset();
        }
    }

}
"
tika-core/src/main/java/org/apache/tika/detect/TextStatistics.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.detect;

/**
 * Utility class for computing a histogram of the bytes seen in a stream.
 *
 * @since Apache Tika 1.2
 */
public class TextStatistics {

    private final int[] counts = new int[256];

    private int total = 0;

    public void addData(byte[] buffer, int offset, int length) {
        for (int i = 0; i < length; i++) {
            counts[buffer[offset + i] & 0xff]++;
            total++;
        }
    }

    /**
     * Checks whether at least one byte was seen and that the bytes that
     * were seen were mostly plain text (i.e. < 2% control, > 90% ASCII range).
     *
     * @see <a href="https://issues.apache.org/jira/browse/TIKA-483">TIKA-483</a>
     * @see <a href="https://issues.apache.org/jira/browse/TIKA-688">TIKA-688</a>
     * @return <code>true</code> if the seen bytes were mostly safe ASCII,
     *         <code>false</code> otherwise
     */
    public boolean isMostlyAscii() {
        int control = count(0, 0x20);
        int ascii = count(0x20, 128);
        int safe = countSafeControl();
        return total > 0
                && (control - safe) * 100 < total * 2
                && (ascii + safe) * 100 > total * 90;
    }

    /**
     * Checks whether the observed byte stream looks like UTF-8 encoded text.
     *
     * @since Apache Tika 1.3
     * @return <code>true</code> if the seen bytes look like UTF-8,
     *         <code>false</code> otherwise
     */
    public boolean looksLikeUTF8() {
        int control = count(0, 0x20);
        int utf8 = count(0x20, 0x80);
        int safe = countSafeControl();

        int expectedContinuation = 0;
        int[] leading = new int[] {
                count(0xc0, 0xe0), count(0xe0, 0xf0), count(0xf0, 0xf8) };
        for (int i = 0; i < leading.length; i++) {
            utf8 += leading[i];
            expectedContinuation += (i + 1) * leading[i];
        }

        int continuation = count(0x80, 0xc0);
        return utf8 > 0
                && continuation <= expectedContinuation
                && continuation >= expectedContinuation - 3
                && count(0xf80, 0x100) == 0
                && (control - safe) * 100 < utf8 * 2;
    }

    /**
     * Returns the total number of bytes seen so far.
     *
     * @return count of all bytes
     */
    public int count() {
        return total;
    }

    /**
     * Returns the number of occurrences of the given byte.
     *
     * @param b byte
     * @return count of the given byte
     */
    public int count(int b) {
        return counts[b & 0xff];
    }

    /**
     * Counts control characters (i.e. < 0x20, excluding tab, CR, LF,
     * page feed and escape).
     * <p>
     * This definition of control characters is based on section 4 of the
     * "Content-Type Processing Model" Internet-draft
     * (<a href="http://webblaze.cs.berkeley.edu/2009/mime-sniff/mime-sniff.txt"
     * >draft-abarth-mime-sniff-01</a>).
     * <pre>
     * +-------------------------+
     * | Binary data byte ranges |
     * +-------------------------+
     * | 0x00 -- 0x08            |
     * | 0x0B                    |
     * | 0x0E -- 0x1A            |
     * | 0x1C -- 0x1F            |
     * +-------------------------+
     * </pre>
     *
     * @see <a href="https://issues.apache.org/jira/browse/TIKA-154">TIKA-154</a>
     * @return count of control characters
     */
    public int countControl() {
        return count(0, 0x20) - countSafeControl();
    }

    /**
     * Counts "safe" (i.e. seven-bit non-control) ASCII characters.
     *
     * @see #countControl()
     * @return count of safe ASCII characters
     */
    public int countSafeAscii() {
        return count(0x20, 128) + countSafeControl();
    }

    /**
     * Counts eight bit characters, i.e. bytes with their highest bit set.
     *
     * @return count of eight bit characters
     */
    public int countEightBit() {
        return count(128, 256);
    }

    private int count(int from, int to) {
        assert 0 <= from && to <= counts.length;
        int count = 0;
        for (int i = from; i < to; i++) {
            count += counts[i];
        }
        return count;
    }

    private int countSafeControl() {
        return count('\t') + count('\n') + count('\r') // tab, LF, CR
                + count(0x0c) + count(0x1b);           // new page, escape
    }

}
"
tika-core/src/main/java/org/apache/tika/detect/TypeDetector.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.detect;

import java.io.InputStream;

import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;

/**
 * Content type detection based on a content type hint. This detector simply
 * trusts any valid content type hint given in the input metadata, and returns
 * that as the likely type of the input document.
 *
 * @since Apache Tika 0.3
 */
public class TypeDetector implements Detector {

    /**
     * Detects the content type of an input document based on a type hint
     * given in the input metadata. The CONTENT_TYPE attribute of the given
     * input metadata is expected to contain the type of the input document.
     * If that attribute exists and contains a valid type name, then that
     * type is returned.
     *
     * @param input ignored
     * @param metadata input metadata, possibly with a CONTENT_TYPE value
     * @return detected media type, or <code>application/octet-stream</code>
     */
    public MediaType detect(InputStream input, Metadata metadata) {
        // Look for a type hint in the input metadata
        String hint = metadata.get(Metadata.CONTENT_TYPE);
        if (hint != null) {
            MediaType type = MediaType.parse(hint);
            if (type != null) {
                return type;
            }
        }
        return MediaType.OCTET_STREAM;
    }

}
"
tika-core/src/main/java/org/apache/tika/detect/XmlRootExtractor.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.detect;

import java.io.ByteArrayInputStream;
import java.io.InputStream;

import javax.xml.XMLConstants;
import javax.xml.namespace.QName;
import javax.xml.parsers.SAXParserFactory;

import org.apache.tika.io.CloseShieldInputStream;
import org.apache.tika.sax.OfflineContentHandler;
import org.xml.sax.Attributes;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.DefaultHandler;

/**
 * Utility class that uses a {@link javax.xml.parsers.SAXParser} to determine
 * the namespace URI and local name of the root element of an XML file.
 *
 * @since Apache Tika 0.4
 */
public class XmlRootExtractor {

    public QName extractRootElement(byte[] data) {
        return extractRootElement(new ByteArrayInputStream(data));
    }

    /**
     * @since Apache Tika 0.9
     */
    public QName extractRootElement(InputStream stream) {
        ExtractorHandler handler = new ExtractorHandler();
        try {
            SAXParserFactory factory = SAXParserFactory.newInstance();
            factory.setNamespaceAware(true);
            factory.setValidating(false);
            factory.setFeature(XMLConstants.FEATURE_SECURE_PROCESSING, true);
            factory.newSAXParser().parse(
                    new CloseShieldInputStream(stream),
                    new OfflineContentHandler(handler));
        } catch (Exception ignore) {
        }
        return handler.rootElement;
    }

    private static class ExtractorHandler extends DefaultHandler {

        private QName rootElement = null;

        @Override
        public void startElement(
                String uri, String local, String name, Attributes attributes)
                throws SAXException {
            this.rootElement = new QName(uri, local);
            throw new SAXException("Aborting: root element received");
        }

    }

}
"
tika-core/src/main/java/org/apache/tika/embedder/Embedder.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.embedder;

import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.io.Serializable;
import java.util.Set;

import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.Parser;

/**
 * Tika embedder interface
 *
 * @since Apache Tika 1.3
 */
public interface Embedder extends Serializable {

    /**
     * Returns the set of media types supported by this embedder when used with
     * the given parse context.
     * <p>
     * The name differs from the precedence of {@link Parser#getSupportedTypes(ParseContext)}
     * so that parser implementations may also choose to implement this interface.
     *
     * @param context parse context
     * @return immutable set of media types
     */
    Set<MediaType> getSupportedEmbedTypes(ParseContext context);

    /**
     * Embeds related document metadata from the given metadata object into the
     * given output stream.
     * <p>
     * The given document stream is consumed but not closed by this method. The
     * responsibility to close the stream remains on the caller.
     * <p>
     * Information about the parsing context can be passed in the context
     * parameter. See the parser implementations for the kinds of context
     * information they expect.
     * <p>
     * In general implementations should favor preserving the source file's metadata 
     * unless an update to a field is explicitly defined in the Metadata object.
     * More specifically:
     * <ul>
     *  <li>Embedder implementations should only attempt to update metadata fields
     *  present in the given Metadata object.  Other fields should be left untouched.</li>
     *  <li>Embedder implementations should set properties as empty when the 
     *  corresponding field in the Metadata object is an empty string, i.e. ""</li>
     *  <li>Embedder implementations should nullify or delete properties 
     *  corresponding to fields with a null value in the given Metadata object.</li>
     *  <li>Embedder implementations should set the property 
     *  corresponding to a particular field in the given Metadata object in all 
     *  metadata containers whenever possible and appropriate for the file format at the time. 
     *  If a particular metadata container falls out of use and/or is superseded by another 
     *  (such as IIC vs XMP for IPTC) it is up to the implementation to decide if and when 
     *  to cease embedding in the alternate container.</li>
     *  <li>Embedder implementations should attempt to embed as much of the metadata 
     *  as accurately as possible. An implementation may choose a strict approach 
     *  and throw an exception if a value to be embedded exceeds the length allowed 
     *  or may choose to truncate the value.</li>
     * </ul>
     * 
     * @param metadata document metadata (input and output)
     * @param originalStream the document stream (input)
     * @param outputStream the output stream to write the metadata embedded data to
     * @param context parse context
     * @throws IOException if the document stream could not be read
     * @throws TikaException if the document could not be parsed
     */
    void embed(Metadata metadata, InputStream originalStream,
            OutputStream outputStream, ParseContext context)
            throws IOException, TikaException;

}
"
tika-core/src/main/java/org/apache/tika/embedder/ExternalEmbedder.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.embedder;

import java.io.ByteArrayOutputStream;
import java.io.File;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collections;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;

import org.apache.tika.exception.TikaException;
import org.apache.tika.io.IOUtils;
import org.apache.tika.io.TemporaryResources;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.Property;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.external.ExternalParser;

/**
 * Embedder that uses an external program (like sed or exiftool) to embed text
 * content and metadata into a given document.
 *
 * @since Apache Tika 1.3
 */
public class ExternalEmbedder implements Embedder {

    private static final long serialVersionUID = -2828829275642475697L;

    /**
     * Token to be replaced with a String array of metadata assignment command
     * arguments
     */
    public static final String METADATA_COMMAND_ARGUMENTS_TOKEN = "${METADATA}";

    /**
     * Token to be replaced with a String array of metadata assignment command
     * arguments
     */
    public static final String METADATA_COMMAND_ARGUMENTS_SERIALIZED_TOKEN = "${METADATA_SERIALIZED}";

    /**
     * Media types supported by the external program.
     */
    private Set<MediaType> supportedEmbedTypes = Collections.emptySet();

    /**
     * Mapping of Tika metadata to command line parameters.
     */
    private Map<Property, String[]> metadataCommandArguments = null;

    /**
     * The external command to invoke.
     *
     * @see Runtime#exec(String[])
     */
    private String[] command = new String[] {
            "sed", "-e",
            "$a\\\n" + METADATA_COMMAND_ARGUMENTS_SERIALIZED_TOKEN,
            ExternalParser.INPUT_FILE_TOKEN
    };

    private String commandAssignmentOperator = "=";
    private String commandAssignmentDelimeter = ", ";
    private String commandAppendOperator = "=";

    private boolean quoteAssignmentValues = false;

    private TemporaryResources tmp = new TemporaryResources();

    public Set<MediaType> getSupportedEmbedTypes(ParseContext context) {
        return getSupportedEmbedTypes();
    }

    public Set<MediaType> getSupportedEmbedTypes() {
        return supportedEmbedTypes;
    }

    public void setSupportedEmbedTypes(Set<MediaType> supportedEmbedTypes) {
        this.supportedEmbedTypes = Collections
                .unmodifiableSet(new HashSet<MediaType>(supportedEmbedTypes));
    }

    /**
     * Gets the command to be run. This can include either of
     * {@link #INPUT_FILE_TOKEN} or {@link #OUTPUT_FILE_TOKEN} if the command
     * needs filenames.
     *
     * @return
     */
    public String[] getCommand() {
        return command;
    }

    /**
     * Sets the command to be run. This can include either of
     * {@link #INPUT_FILE_TOKEN} or {@link #OUTPUT_FILE_TOKEN} if the command
     * needs filenames.
     *
     * @see Runtime#exec(String[])
     */
    public void setCommand(String... command) {
        this.command = command;
    }

    /**
     * Gets the assignment operator for the command line tool, i.e. "=".
     *
     * @return the assignment operator
     */
    public String getCommandAssignmentOperator() {
        return commandAssignmentOperator;
    }

    /**
     * Sets the assignment operator for the command line tool, i.e. "=".
     *
     * @param commandAssignmentOperator
     */
    public void setCommandAssignmentOperator(String commandAssignmentOperator) {
        this.commandAssignmentOperator = commandAssignmentOperator;
    }

    /**
     * Gets the delimiter for multiple assignments for the command line tool,
     * i.e. ", ".
     *
     * @return the assignment delimiter
     */
    public String getCommandAssignmentDelimeter() {
        return commandAssignmentDelimeter;
    }

    /**
     * Sets the delimiter for multiple assignments for the command line tool,
     * i.e. ", ".
     *
     * @param commandAssignmentDelimeter
     */
    public void setCommandAssignmentDelimeter(String commandAssignmentDelimeter) {
        this.commandAssignmentDelimeter = commandAssignmentDelimeter;
    }

    /**
     * Gets the operator to append rather than replace a value for the command
     * line tool, i.e. "+=".
     *
     * @return the append operator
     */
    public String getCommandAppendOperator() {
        return commandAppendOperator;
    }

    /**
     * Sets the operator to append rather than replace a value for the command
     * line tool, i.e. "+=".
     *
     * @param commandAppendOperator
     */
    public void setCommandAppendOperator(String commandAppendOperator) {
        this.commandAppendOperator = commandAppendOperator;
    }

    /**
     * Gets whether or not to quote assignment values, i.e. tag='value'. The
     * default is false.
     *
     * @return whether or not to quote assignment values
     */
    public boolean isQuoteAssignmentValues() {
        return quoteAssignmentValues;
    }

    /**
     * Sets whether or not to quote assignment values, i.e. tag='value'.
     *
     * @param quoteAssignmentValues
     */
    public void setQuoteAssignmentValues(boolean quoteAssignmentValues) {
        this.quoteAssignmentValues = quoteAssignmentValues;
    }

    /**
     * Gets the map of Metadata keys to command line parameters.
     *
     * @return the metadata to CLI param map
     */
    public Map<Property, String[]> getMetadataCommandArguments() {
        return metadataCommandArguments;
    }

    /**
     * Sets the map of Metadata keys to command line parameters. Set this to
     * null to disable Metadata embedding.
     *
     * @param arguments
     */
    public void setMetadataCommandArguments(Map<Property, String[]> arguments) {
        this.metadataCommandArguments = arguments;
    }

    /**
     * Constructs a collection of command line arguments responsible for setting
     * individual metadata fields based on the given <code>metadata</code>.
     *
     * @param metadata the metadata to embed
     * @return the metadata-related command line arguments
     */
    protected List<String> getCommandMetadataSegments(Metadata metadata) {
        List<String> commandMetadataSegments = new ArrayList<String>();
        if (metadata == null || metadata.names() == null) {
            return commandMetadataSegments;
        }
        for (String metadataName : metadata.names()) {
            for (Property property : getMetadataCommandArguments().keySet()) {
                if (metadataName.equals(property.getName())) {
                    String[] metadataCommandArguments = getMetadataCommandArguments().get(property);
                    if (metadataCommandArguments != null) {
                        for (String metadataCommandArgument : metadataCommandArguments) {
                            if (metadata.isMultiValued(metadataName)) {
                                for (String metadataValue : metadata.getValues(metadataName)) {
                                    String assignmentValue = metadataValue;
                                    if (quoteAssignmentValues) {
                                        assignmentValue = "'" + assignmentValue + "'";
                                    }
                                    commandMetadataSegments.add(metadataCommandArgument
                                            + commandAppendOperator
                                            + assignmentValue);
                                }
                            } else {
                                String assignmentValue = metadata.get(metadataName);
                                if (quoteAssignmentValues) {
                                    assignmentValue = "'" + assignmentValue + "'";
                                }
                                commandMetadataSegments.add(metadataCommandArgument
                                        + commandAssignmentOperator
                                        + assignmentValue);
                            }
                        }
                    }
                }
            }
        }
        return commandMetadataSegments;
    }

    /**
     * Serializes a collection of metadata command line arguments into a single
     * string.
     *
     * @param metadataCommandArguments
     * @return the serialized metadata arguments string
     */
    protected static String serializeMetadata(
            List<String> metadataCommandArguments) {
        if (metadataCommandArguments != null) {
            return Arrays.toString(metadataCommandArguments.toArray());
        }
        return "";
    }

    /**
     * Executes the configured external command and passes the given document
     * stream as a simple XHTML document to the given SAX content handler.
     * Metadata is only extracted if {@link #setMetadataCommandArguments(Map)}
     * has been called to set arguments.
     */
    public void embed(final Metadata metadata, final InputStream inputStream,
            final OutputStream outputStream, final ParseContext context)
            throws IOException, TikaException {

        boolean inputToStdIn = true;
        boolean outputFromStdOut = true;
        boolean hasMetadataCommandArguments =
                (metadataCommandArguments != null && !metadataCommandArguments.isEmpty());
        boolean serializeMetadataCommandArgumentsToken = false;
        boolean replacedMetadataCommandArgumentsToken = false;

        TikaInputStream tikaInputStream = TikaInputStream.get(inputStream);
        File tempOutputFile = null;

        List<String> commandMetadataSegments = null;
        if (hasMetadataCommandArguments) {
            commandMetadataSegments = getCommandMetadataSegments(metadata);
        }

        // Build our command
        List<String> origCmd = Arrays.asList(command);
        List<String> cmd = new ArrayList<String>();
        for (String commandSegment : origCmd) {
            if (commandSegment.indexOf(ExternalParser.INPUT_FILE_TOKEN) != -1) {
                commandSegment = commandSegment.replace(
                        ExternalParser.INPUT_FILE_TOKEN,
                        tikaInputStream.getFile().toString());
                inputToStdIn = false;
            }
            if (commandSegment.indexOf(ExternalParser.OUTPUT_FILE_TOKEN) != -1) {
                tempOutputFile = tmp.createTemporaryFile();
                commandSegment = commandSegment.replace(
                        ExternalParser.OUTPUT_FILE_TOKEN,
                        tempOutputFile.toString());
                outputFromStdOut = false;
            }
            if (commandSegment
                    .indexOf(METADATA_COMMAND_ARGUMENTS_SERIALIZED_TOKEN) != -1) {
                serializeMetadataCommandArgumentsToken = true;
            }
            if (commandSegment.indexOf(METADATA_COMMAND_ARGUMENTS_TOKEN) != -1) {
                if (hasMetadataCommandArguments) {
                    for (String commandMetadataSegment : commandMetadataSegments) {
                        cmd.add(commandMetadataSegment);
                    }
                }
                replacedMetadataCommandArgumentsToken = true;
            } else {
                cmd.add(commandSegment);
            }
        }
        if (hasMetadataCommandArguments) {
            if (serializeMetadataCommandArgumentsToken) {
                // Find all metadata tokens and replace with encapsulated metadata
                int i = 0;
                for (String commandSegment : cmd) {
                    if (commandSegment
                            .indexOf(METADATA_COMMAND_ARGUMENTS_SERIALIZED_TOKEN) != -1) {
                        commandSegment = commandSegment.replace(
                                METADATA_COMMAND_ARGUMENTS_SERIALIZED_TOKEN,
                                serializeMetadata(commandMetadataSegments));
                        cmd.set(i, commandSegment);
                    }
                    i++;
                }
            } else if (!replacedMetadataCommandArgumentsToken
                    && !serializeMetadataCommandArgumentsToken) {
                // Tack metadata onto the end of the cmd as arguments
                cmd.addAll(commandMetadataSegments);
            }
        }

        // Execute
        Process process;
        if (cmd.toArray().length == 1) {
            process = Runtime.getRuntime().exec(cmd.toArray(new String[] {})[0]);
        } else {
            process = Runtime.getRuntime().exec(cmd.toArray(new String[] {}));
        }

        ByteArrayOutputStream stdErrOutputStream = new ByteArrayOutputStream();

        try {
            sendStdErrToOutputStream(process, stdErrOutputStream);

            if (inputToStdIn) {
                sendInputStreamToStdIn(inputStream, process);
            } else {
                // We're not writing to std in this case so close
                process.getOutputStream().close();
            }

            if (outputFromStdOut) {
                sendStdOutToOutputStream(process, outputStream);
            } else {
                tmp.dispose();
                try {
                    process.waitFor();
                } catch (InterruptedException ignore) {
                }
                // The command is finished, read the output file into the given output stream
                InputStream tempOutputFileInputStream = TikaInputStream.get(tempOutputFile);
                IOUtils.copy(tempOutputFileInputStream, outputStream);
            }
        } finally {
            if (outputFromStdOut) {
                try {
                    process.waitFor();
                } catch (InterruptedException ignore) {
                }
            } else {
                try {
                    // Clean up temp output files
                    tempOutputFile.delete();
                } catch (Exception e) {
                }
            }
            if (!inputToStdIn) {
                // Close input file (and delete if created by up TemporaryResources.createTemporaryFile) 
                IOUtils.closeQuietly(tikaInputStream);
            }
            IOUtils.closeQuietly(outputStream);
            IOUtils.closeQuietly(stdErrOutputStream);
            if (process.exitValue() != 0) {
                throw new TikaException("There was an error executing the command line" +
                        "\nExecutable Command:\n\n" + cmd +
                        "\nExecutable Error:\n\n" + stdErrOutputStream.toString("UTF-8"));
            }
        }
    }

    /**
     * Creates a new thread for copying a given input stream to a given output stream.
     *
     * @param inputStream the source input stream
     * @param outputStream the target output stream
     */
    private void multiThreadedStreamCopy(
            final InputStream inputStream,
            final OutputStream outputStream) {
        new Thread(new Runnable() {
            public void run() {
                try {
                    IOUtils.copy(inputStream, outputStream);
                } catch (IOException e) {
                    System.out.println("ERROR: " + e.getMessage());
                }
            }
        }).start();
    }

    /**
     * Sends the contents of the given input stream to the
     * standard input of the given process. Potential exceptions are
     * ignored.
     * <p>
     * Note that the given input stream is <em>not</em> closed by this method.
     *
     * @param process the process
     * @param inputStream the input stream to send to standard input of the process
     */
    private void sendInputStreamToStdIn(
            final InputStream inputStream,
            final Process process) {
        multiThreadedStreamCopy(inputStream, process.getOutputStream());
    }

    /**
     * Sends the standard output of the given
     * process to the given output stream. Potential exceptions are
     * ignored.
     * <p>
     * Note that the given output stream is <em>not</em> closed by this method.
     *
     * @param process the process
     * @param outputStream the putput stream to send to standard input of the process
     */
    private void sendStdOutToOutputStream(
            final Process process,
            final OutputStream outputStream) {
        try {
            IOUtils.copy(process.getInputStream(), outputStream);
        } catch (IOException e) {
            System.out.println("ERROR: " + e.getMessage());
        }
    }

    /**
     * Starts a thread that reads and discards the contents of the standard
     * stream of the given process. Potential exceptions are ignored, and the
     * stream is closed once fully processed.
     *
     * @param process the process
     * param outputStream the output stream to send to standard error of the process
     */
    private void sendStdErrToOutputStream(
            final Process process,
            final OutputStream outputStream) {
        multiThreadedStreamCopy(process.getErrorStream(), outputStream);
    }

    /**
     * Checks to see if the command can be run. Typically used with something
     * like "myapp --version" to check to see if "myapp" is installed and on the
     * path.
     *
     * @param checkCmd the check command to run
     * @param errorValue what is considered an error value?
     * @return whether or not the check completed without error
     */
    public static boolean check(String checkCmd, int... errorValue) {
        return check(new String[] { checkCmd }, errorValue);
    }

    /**
     * Checks to see if the command can be run. Typically used with something
     * like "myapp --version" to check to see if "myapp" is installed and on the
     * path.
     *
     * @param checkCmd the check command to run
     * @param errorValue what is considered an error value?
     * @return whether or not the check completed without error
     */
    public static boolean check(String[] checkCmd, int... errorValue) {
        if (errorValue.length == 0) {
            errorValue = new int[] { 127 };
        }

        try {
            Process process;
            if (checkCmd.length == 1) {
                process = Runtime.getRuntime().exec(checkCmd[0]);
            } else {
                process = Runtime.getRuntime().exec(checkCmd);
            }
            int result = process.waitFor();

            for (int err : errorValue) {
                if (result == err)
                    return false;
            }
            return true;
        } catch (IOException e) {
            // Some problem, command is there or is broken
            return false;
        } catch (InterruptedException ie) {
            // Some problem, command is there or is broken
            return false;
        }
    }
}
"
tika-core/src/main/java/org/apache/tika/exception/EncryptedDocumentException.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.exception;

public class EncryptedDocumentException extends TikaException {
    public EncryptedDocumentException() {
        super("Unable to process: document is encrypted");
    }

    public EncryptedDocumentException(Throwable th) {
        super("Unable to process: document is encrypted", th);
    }

    public EncryptedDocumentException(String info) {
        super(info);
    }
    
    public EncryptedDocumentException(String info, Throwable th) {
        super(info, th);
    }
}
"
tika-core/src/main/java/org/apache/tika/exception/package-info.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Tika exception.
 */
@aQute.bnd.annotation.Version("1.0.0")
package org.apache.tika.exception;
"
tika-core/src/main/java/org/apache/tika/exception/TikaException.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.exception;

/**
 * Tika exception 
 */
public class TikaException extends Exception {

    public TikaException(String msg) {
        super(msg);
    }

    public TikaException(String msg, Throwable cause) {
        super(msg, cause);
    }

}
"
tika-core/src/main/java/org/apache/tika/extractor/ContainerExtractor.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.extractor;

import java.io.IOException;
import java.io.Serializable;

import org.apache.tika.exception.TikaException;
import org.apache.tika.io.TikaInputStream;

/**
 * Tika container extractor interface.
 * Container Extractors provide access to the embedded
 *  resources within container formats such as .zip and .doc 
 */
public interface ContainerExtractor extends Serializable {
    /**
     * Is this Container Extractor able to process the
     *  supplied container?
     * @since Apache Tika 0.8
     */
    boolean isSupported(TikaInputStream input) throws IOException;

    /**
     * Processes a container file, and extracts all the embedded
     * resources from within it.
     * <p>
     * The {@link EmbeddedResourceHandler} you supply will
     * be called for each embedded resource in the container. It is
     * up to you whether you process the contents of the resource or not. 
     * <p>
     * The given document stream is consumed but not closed by this method.
     * The responsibility to close the stream remains on the caller.
     * <p>
     * If required, nested containers (such as a .docx within a .zip)
     * can automatically be recursed into, and processed inline. If
     * no recurseExtractor is given, the nested containers will be
     * treated as with any other embedded resources.
     *
     * @since Apache Tika 0.8
     * @param stream the document stream (input)
     * @param recurseExtractor the extractor to use on any embedded containers 
     * @param handler handler for the embedded files (output)
     * @throws IOException if the document stream could not be read
     * @throws TikaException if the container could not be parsed
     */
    void extract(
            TikaInputStream stream, ContainerExtractor recurseExtractor,
            EmbeddedResourceHandler handler)
            throws IOException, TikaException;
}
"
tika-core/src/main/java/org/apache/tika/extractor/DocumentSelector.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.extractor;

import org.apache.tika.metadata.Metadata;

/**
 * Interface for different document selection strategies for purposes like
 * embedded document extraction by a {@link ContainerExtractor} instance.
 * An implementation of this interface defines some specific selection
 * criteria to be applied against the document metadata passed to the
 * {@link #select(Metadata)} method.
 *
 * @since Apache Tika 0.8
 */
public interface DocumentSelector {

    /**
     * Checks if a document with the given metadata matches the specified
     * selection criteria.
     *
     * @param metadata document metadata
     * @return <code>true</code> if the document matches the selection criteria,
     *         <code>false</code> otherwise
     */
    boolean select(Metadata metadata);

}
"
tika-core/src/main/java/org/apache/tika/extractor/EmbeddedDocumentExtractor.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.extractor;

import org.apache.tika.metadata.Metadata;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

import java.io.IOException;
import java.io.InputStream;

public interface EmbeddedDocumentExtractor {
    boolean shouldParseEmbedded(Metadata metadata);

    /**
     * Processes the supplied embedded resource, calling the delegating
     *  parser with the appropriate details.
     * @param stream The embedded resource
     * @param handler The handler to use
     * @param metadata The metadata for the embedded resource
     * @param outputHtml Should we output HTML for this resource, or has the parser already done so?
     * @throws org.xml.sax.SAXException
     * @throws java.io.IOException
     */
    void parseEmbedded(
            InputStream stream, ContentHandler handler, Metadata metadata, boolean outputHtml)
            throws SAXException, IOException;
}
"
tika-core/src/main/java/org/apache/tika/extractor/EmbeddedResourceHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.extractor;

import java.io.InputStream;

import org.apache.tika.mime.MediaType;

/**
 * Tika container extractor callback interface.
 * To work with a {@link ContainerExtractor}, your code needs
 *  to implement this interface.
 */
public interface EmbeddedResourceHandler {
    /**
     * Called to process an embedded resource within the container.
     * This will be called once per embedded resource within the
     *  container, along with whatever details are available on
     *  the embedded resource.
     *  
     * @since Apache Tika 0.8
     * @param filename The filename of the embedded resource, if known
     * @param mediaType The media type of the embedded resource, if known
     * @param stream The contents of the embedded resource
     */
    void handle(String filename, MediaType mediaType, InputStream stream);
}
"
tika-core/src/main/java/org/apache/tika/extractor/package-info.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Extraction of component documents.
 */
@aQute.bnd.annotation.Version("1.0.0")
package org.apache.tika.extractor;
"
tika-core/src/main/java/org/apache/tika/extractor/ParserContainerExtractor.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.extractor;

import java.io.File;
import java.io.IOException;
import java.io.InputStream;
import java.util.Set;

import org.apache.tika.config.TikaConfig;
import org.apache.tika.detect.DefaultDetector;
import org.apache.tika.detect.Detector;
import org.apache.tika.exception.TikaException;
import org.apache.tika.io.TemporaryResources;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.AutoDetectParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.Parser;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.DefaultHandler;

/**
 * An implementation of {@link ContainerExtractor} powered by the regular
 * {@link Parser} API. This allows you to easily extract out all the
 * embedded resources from within container files supported by normal Tika
 * parsers. By default the {@link AutoDetectParser} will be used, to allow
 * extraction from the widest range of containers.
 */
public class ParserContainerExtractor implements ContainerExtractor {

    /** Serial version UID */
    private static final long serialVersionUID = 2261131045580861514L;

    private final Parser parser;

    private final Detector detector;

    public ParserContainerExtractor() {
        this(TikaConfig.getDefaultConfig());
    }

    public ParserContainerExtractor(TikaConfig config) {
        this(new AutoDetectParser(config),
                new DefaultDetector(config.getMimeRepository()));
    }

    public ParserContainerExtractor(Parser parser, Detector detector) {
        this.parser = parser;
        this.detector = detector;
    }

    public boolean isSupported(TikaInputStream input) throws IOException {
        MediaType type = detector.detect(input, new Metadata());
        return parser.getSupportedTypes(new ParseContext()).contains(type);
    }

    public void extract(
            TikaInputStream stream, ContainerExtractor recurseExtractor,
            EmbeddedResourceHandler handler)
            throws IOException, TikaException {
        ParseContext context = new ParseContext();
        context.set(Parser.class, new RecursiveParser(recurseExtractor, handler));
        try {
            parser.parse(stream, new DefaultHandler(), new Metadata(), context);
        } catch (SAXException e) {
            throw new TikaException("Unexpected SAX exception", e);
        }
    }

    private class RecursiveParser extends AbstractParser {

        private final ContainerExtractor extractor;

        private final EmbeddedResourceHandler handler;

        private RecursiveParser(
                ContainerExtractor extractor,
                EmbeddedResourceHandler handler) {
            this.extractor = extractor;
            this.handler = handler;
        }

        public Set<MediaType> getSupportedTypes(ParseContext context) {
            return parser.getSupportedTypes(context);
        }

        public void parse(
                InputStream stream, ContentHandler ignored,
                Metadata metadata, ParseContext context)
                throws IOException, SAXException, TikaException {
            TemporaryResources tmp = new TemporaryResources();
            try {
                TikaInputStream tis = TikaInputStream.get(stream, tmp);

                // Figure out what we have to process
                String filename = metadata.get(Metadata.RESOURCE_NAME_KEY);
                MediaType type = detector.detect(tis, metadata);

                if (extractor == null) {
                    // Let the handler process the embedded resource 
                    handler.handle(filename, type, tis);
                } else {
                    // Use a temporary file to process the stream twice
                    File file = tis.getFile();

                    // Let the handler process the embedded resource
                    InputStream input = TikaInputStream.get(file);
                    try {
                        handler.handle(filename, type, input);
                    } finally {
                        input.close();
                    }

                    // Recurse
                    extractor.extract(tis, extractor, handler);
                }
            } finally {
                tmp.dispose();
            }
        }

    }

}
"
tika-core/src/main/java/org/apache/tika/extractor/ParsingEmbeddedDocumentExtractor.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.extractor;

import java.io.File;
import java.io.FilenameFilter;
import java.io.IOException;
import java.io.InputStream;

import org.apache.tika.exception.TikaException;
import org.apache.tika.io.CloseShieldInputStream;
import org.apache.tika.io.TemporaryResources;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.parser.DelegatingParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.Parser;
import org.apache.tika.sax.BodyContentHandler;
import org.apache.tika.sax.EmbeddedContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.AttributesImpl;

import static org.apache.tika.sax.XHTMLContentHandler.XHTML;

/**
 * Helper class for parsers of package archives or other compound document
 * formats that support embedded or attached component documents.
 *
 * @since Apache Tika 0.8
 */
public class ParsingEmbeddedDocumentExtractor implements EmbeddedDocumentExtractor {

    private static final File ABSTRACT_PATH = new File("");

    private static final Parser DELEGATING_PARSER = new DelegatingParser();

    private final ParseContext context;

    public ParsingEmbeddedDocumentExtractor(ParseContext context) {
        this.context = context;
    }

    public boolean shouldParseEmbedded(Metadata metadata) {
        DocumentSelector selector = context.get(DocumentSelector.class);
        if (selector != null) {
            return selector.select(metadata);
        }

        FilenameFilter filter = context.get(FilenameFilter.class);
        if (filter != null) {
            String name = metadata.get(Metadata.RESOURCE_NAME_KEY);
            if (name != null) {
                return filter.accept(ABSTRACT_PATH, name);
            }
        }

        return true;
    }

    public void parseEmbedded(
            InputStream stream, ContentHandler handler, Metadata metadata, boolean outputHtml)
            throws SAXException, IOException {
        if(outputHtml) {
           AttributesImpl attributes = new AttributesImpl();
           attributes.addAttribute("", "class", "class", "CDATA", "package-entry");
           handler.startElement(XHTML, "div", "div", attributes);
        }

        String name = metadata.get(Metadata.RESOURCE_NAME_KEY);
        if (name != null && name.length() > 0 && outputHtml) {
            handler.startElement(XHTML, "h1", "h1", new AttributesImpl());
            char[] chars = name.toCharArray();
            handler.characters(chars, 0, chars.length);
            handler.endElement(XHTML, "h1", "h1");
        }

        // Use the delegate parser to parse this entry
        TemporaryResources tmp = new TemporaryResources();
        try {
            final TikaInputStream newStream = TikaInputStream.get(new CloseShieldInputStream(stream), tmp);
            if (stream instanceof TikaInputStream) {
                final Object container = ((TikaInputStream) stream).getOpenContainer();
                if (container != null) {
                    newStream.setOpenContainer(container);
                }
            }
            DELEGATING_PARSER.parse(
                                    newStream,
                                    new EmbeddedContentHandler(new BodyContentHandler(handler)),
                                    metadata, context);
        } catch (TikaException e) {
            // TODO: can we log a warning somehow?
            // Could not parse the entry, just skip the content
        } finally {
            tmp.close();
        }

        if(outputHtml) {
           handler.endElement(XHTML, "div", "div");
        }
    }

}
"
tika-core/src/main/java/org/apache/tika/fork/ClassLoaderProxy.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.fork;

import java.io.ByteArrayOutputStream;
import java.io.DataInputStream;
import java.io.DataOutputStream;
import java.io.IOException;
import java.net.URL;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Enumeration;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

class ClassLoaderProxy extends ClassLoader implements ForkProxy {

    /** Serial version UID */
    private static final long serialVersionUID = -7303109260448540420L;

    /**
     * Names of resources that could not be found. Used to avoid repeated
     * lookup of commonly accessed, but often not present, resources like
     * <code>META-INF/services/javax.xml.parsers.SAXParserFactory</code>.
     */
    private final Set<String> notFound = new HashSet<String>();

    private final int resource;

    private transient DataInputStream input;

    private transient DataOutputStream output;

    public ClassLoaderProxy(int resource) {
        this.resource = resource;
    }

    public void init(DataInputStream input, DataOutputStream output) {
        this.input = input;
        this.output = output;
    }

    @Override
    protected synchronized URL findResource(String name) {
        if (notFound.contains(name)) {
            return null;
        }
        try {
            // Send a request to load the resource data
            output.write(ForkServer.RESOURCE);
            output.write(resource);
            output.write(1);
            output.writeUTF(name);
            output.flush();

            // Receive the response
            if (input.readBoolean()) {
                return MemoryURLStreamHandler.createURL(readStream());
            } else {
                notFound.add(name);
                return null;
            }
        } catch (IOException e) {
            return null;
        }
    }

    @Override
    protected synchronized Enumeration<URL> findResources(String name)
            throws IOException {
        // Send a request to load the resources
        output.write(ForkServer.RESOURCE);
        output.write(resource);
        output.write(2);
        output.writeUTF(name);
        output.flush();

        // Receive the response
        List<URL> resources = new ArrayList<URL>();
        while (input.readBoolean()) {
            resources.add(MemoryURLStreamHandler.createURL(readStream()));
        }
        return Collections.enumeration(resources);
    }

    @Override
    protected synchronized Class<?> findClass(String name)
            throws ClassNotFoundException {
        try {
            // Send a request to load the class data
            output.write(ForkServer.RESOURCE);
            output.write(resource);
            output.write(1);
            output.writeUTF(name.replace('.', '/') + ".class");
            output.flush();

            // Receive the response
            if (input.readBoolean()) {
                byte[] data = readStream();
                return defineClass(name, data, 0, data.length);
            } else {
                throw new ClassNotFoundException("Unable to find class " + name);
            }
        } catch (IOException e) {
            throw new ClassNotFoundException("Unable to load class " + name, e);
        }
    }

    private byte[] readStream() throws IOException {
        ByteArrayOutputStream stream = new ByteArrayOutputStream();
        byte[] buffer = new byte[0xffff];
        int n;
        while ((n = input.readUnsignedShort()) > 0) {
            input.readFully(buffer, 0, n);
            stream.write(buffer, 0, n);
        }
        return stream.toByteArray();
    }

}
"
tika-core/src/main/java/org/apache/tika/fork/ClassLoaderResource.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.fork;

import java.io.DataInputStream;
import java.io.DataOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.net.URL;
import java.util.Enumeration;

class ClassLoaderResource implements ForkResource {

    private final ClassLoader loader;

    public ClassLoaderResource(ClassLoader loader) {
        this.loader = loader;
    }

    /**
     * Processes a request for one (code 1) or many (code 2) class loader
     * resources. The requested resources are sent preceded with a boolean
     * <code>true</code> value. If the resource was not found (code 1) or
     * when the last resource has been sent (code 2), a boolean
     * <code>false</code> value is sent instead.
     *
     * @param name resource name
     * @throws IOException if the resource could not be sent
     */
    public Throwable process(DataInputStream input, DataOutputStream output)
            throws IOException {
        byte type = input.readByte();
        String name = input.readUTF();
        if (type == 1) {
            InputStream stream = loader.getResourceAsStream(name);
            if (stream != null) {
                output.writeBoolean(true);
                writeAndCloseStream(output, stream);
            } else {
                output.writeBoolean(false);
            }
        } else if (type == 2) {
            Enumeration<URL> resources = loader.getResources(name);
            while (resources.hasMoreElements()) {
                output.writeBoolean(true);
                InputStream stream = resources.nextElement().openStream();
                writeAndCloseStream(output, stream);
            }
            output.writeBoolean(false);
        }
        output.flush();
        return null;
    }

    /**
     * Sends the contents of the given input stream to the given output.
     * The stream is sent in chunks of less than 64kB, each preceded by
     * a 16-bit integer value that indicates the length of the following
     * chunk. A zero short value is sent at the end to signify the end of
     * the stream.
     * <p>
     * The stream is guaranteed to be closed by this method, regardless of
     * the way it returns.
     *
     * @param stream the stream to be sent
     * @throws IOException if the stream could not be sent
     */
    private void writeAndCloseStream(
            DataOutputStream output, InputStream stream) throws IOException {
        try {
            byte[] buffer = new byte[0x10000 - 1];
            int n;
            while ((n = stream.read(buffer)) != -1) {
                output.writeShort(n);
                output.write(buffer, 0, n);
            }
            output.writeShort(0);
        } finally {
            stream.close();
        }
    }

}
"
tika-core/src/main/java/org/apache/tika/fork/ContentHandlerProxy.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.fork;

import java.io.DataInputStream;
import java.io.DataOutputStream;
import java.io.IOException;

import org.xml.sax.Attributes;
import org.xml.sax.ContentHandler;
import org.xml.sax.Locator;
import org.xml.sax.SAXException;

class ContentHandlerProxy implements ContentHandler, ForkProxy {

    public static final int START_DOCUMENT         =  1;
    public static final int END_DOCUMENT           =  2;
    public static final int START_PREFIX_MAPPING   =  3;
    public static final int END_PREFIX_MAPPING     =  4;
    public static final int START_ELEMENT          =  5;
    public static final int END_ELEMENT            =  6;
    public static final int CHARACTERS             =  7;
    public static final int IGNORABLE_WHITESPACE   =  8;
    public static final int PROCESSING_INSTRUCTION =  9;
    public static final int SKIPPED_ENTITY         = 10;

    /** Serial version UID */
    private static final long serialVersionUID = 737511106054617524L;

    private final int resource;

    private transient DataOutputStream output;

    public ContentHandlerProxy(int resource) {
        this.resource = resource;
    }

    public void init(DataInputStream input, DataOutputStream output) {
        this.output = output;
    }

    private void sendRequest(int type) throws SAXException {
        try {
            output.writeByte(ForkServer.RESOURCE);
            output.writeByte(resource);
            output.writeByte(type);
        } catch (IOException e) {
            throw new SAXException("Unexpected fork proxy problem", e);
        }
    }

    private void sendString(String string) throws SAXException {
        try {
            if (string != null) {
                output.writeBoolean(true);
                output.writeUTF(string);
            } else {
                output.writeBoolean(false);
            }
        } catch (IOException e) {
            throw new SAXException("Unexpected fork proxy problem", e);
        }
    }

    private void sendCharacters(char[] ch, int start, int length)
            throws SAXException {
        try {
            output.writeInt(length);
            for (int i = 0; i < length; i++) {
                output.writeChar(ch[start + i]);
            }
        } catch (IOException e) {
            throw new SAXException("Unexpected fork proxy problem", e);
        }
    }

    private void doneSending() throws SAXException {
        try {
            output.flush();
        } catch (IOException e) {
            throw new SAXException("Unexpected fork proxy problem", e);
        }
    }

    public void setDocumentLocator(Locator locator) {
        // skip
    }

    public void startDocument() throws SAXException {
        sendRequest(START_DOCUMENT);
        doneSending();
    }

    public void endDocument() throws SAXException {
        sendRequest(END_DOCUMENT);
        doneSending();
    }

    public void startPrefixMapping(String prefix, String uri)
            throws SAXException {
        sendRequest(START_PREFIX_MAPPING);
        sendString(prefix);
        sendString(uri);
        doneSending();
    }

    public void endPrefixMapping(String prefix) throws SAXException {
        sendRequest(END_PREFIX_MAPPING);
        sendString(prefix);
        doneSending();
    }

    public void startElement(
            String uri, String localName, String qName, Attributes atts)
            throws SAXException {
        sendRequest(START_ELEMENT);
        sendString(uri);
        sendString(localName);
        sendString(qName);
        int n = -1;
        if (atts != null) {
            n = atts.getLength();
        }
        try {
            output.writeInt(n);
        } catch (IOException e) {
            throw new SAXException("Unexpected fork proxy problem", e);
        }
        for (int i = 0; i < n; i++) {
            sendString(atts.getURI(i));
            sendString(atts.getLocalName(i));
            sendString(atts.getQName(i));
            sendString(atts.getType(i));
            sendString(atts.getValue(i));
        }
        doneSending();
    }

    public void endElement(String uri, String localName, String qName)
            throws SAXException {
        sendRequest(END_ELEMENT);
        sendString(uri);
        sendString(localName);
        sendString(qName);
        doneSending();
    }

    public void characters(char[] ch, int start, int length)
            throws SAXException {
        sendRequest(CHARACTERS);
        sendCharacters(ch, start, length);
        doneSending();
    }

    public void ignorableWhitespace(char[] ch, int start, int length)
            throws SAXException {
        sendRequest(IGNORABLE_WHITESPACE);
        sendCharacters(ch, start, length);
        doneSending();
    }

    public void processingInstruction(String target, String data)
            throws SAXException {
        sendRequest(PROCESSING_INSTRUCTION);
        sendString(target);
        sendString(data);
        doneSending();
    }

    public void skippedEntity(String name) throws SAXException {
        sendRequest(SKIPPED_ENTITY);
        sendString(name);
        doneSending();
    }

}
"
tika-core/src/main/java/org/apache/tika/fork/ContentHandlerResource.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.fork;

import java.io.DataInputStream;
import java.io.DataOutputStream;
import java.io.IOException;

import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.AttributesImpl;

class ContentHandlerResource implements ForkResource {

    private final ContentHandler handler;

    public ContentHandlerResource(ContentHandler handler) {
        this.handler = handler;
    }

    public Throwable process(DataInputStream input, DataOutputStream output)
            throws IOException {
        try {
            internalProcess(input);
            return null;
        } catch (SAXException e) {
            return e;
        }
    }

    private void internalProcess(DataInputStream input)
            throws IOException, SAXException {
        int type = input.readUnsignedByte();
        if (type == ContentHandlerProxy.START_DOCUMENT) {
            handler.startDocument();
        } else if (type == ContentHandlerProxy.END_DOCUMENT) {
            handler.endDocument();
        } else if (type == ContentHandlerProxy.START_PREFIX_MAPPING) {
            handler.startPrefixMapping(readString(input), readString(input));
        } else if (type == ContentHandlerProxy.END_PREFIX_MAPPING) {
            handler.endPrefixMapping(readString(input));
        } else if (type == ContentHandlerProxy.START_ELEMENT) {
            String uri = readString(input);
            String localName = readString(input);
            String qName = readString(input);
            AttributesImpl atts = null;
            int n = input.readInt();
            if (n >= 0) {
                atts = new AttributesImpl();
                for (int i = 0; i < n; i++) {
                    atts.addAttribute(
                            readString(input), readString(input),
                            readString(input), readString(input),
                            readString(input));
                }
            }
            handler.startElement(uri, localName, qName, atts);
        } else if (type == ContentHandlerProxy.END_ELEMENT) {
            String uri = readString(input);
            String localName = readString(input);
            String qName = readString(input);
            handler.endElement(uri, localName, qName);
        } else if (type == ContentHandlerProxy.CHARACTERS) {
            char[] ch = readCharacters(input);
            handler.characters(ch, 0, ch.length);
        } else if (type == ContentHandlerProxy.IGNORABLE_WHITESPACE) {
            char[] ch = readCharacters(input);
            handler.characters(ch, 0, ch.length);
        } else if (type == ContentHandlerProxy.PROCESSING_INSTRUCTION) {
            handler.processingInstruction(readString(input), readString(input));
        } else if (type == ContentHandlerProxy.SKIPPED_ENTITY) {
            handler.skippedEntity(readString(input));
        }
    }

    private String readString(DataInputStream input) throws IOException {
        if (input.readBoolean()) {
            return input.readUTF();
        } else {
            return null;
        }
    }

    private char[] readCharacters(DataInputStream input) throws IOException {
        int n = input.readInt();
        char[] ch = new char[n];
        for (int i = 0; i < n; i++) {
            ch[i] = input.readChar();
        }
        return ch;
    }

}
"
tika-core/src/main/java/org/apache/tika/fork/ForkClient.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.fork;

import java.io.DataInputStream;
import java.io.DataOutputStream;
import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.NotSerializableException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.jar.JarEntry;
import java.util.jar.JarOutputStream;
import java.util.zip.ZipEntry;

import org.apache.tika.exception.TikaException;
import org.apache.tika.io.IOExceptionWithCause;
import org.apache.tika.io.IOUtils;
import org.xml.sax.ContentHandler;

class ForkClient {

    private final List<ForkResource> resources = new ArrayList<ForkResource>();

    private final ClassLoader loader;

    private final File jar;

    private final Process process;

    private final DataOutputStream output;

    private final DataInputStream input;

    private final InputStream error;

    public ForkClient(ClassLoader loader, Object object, String java)
            throws IOException, TikaException {
        boolean ok = false;
        try {
            this.loader = loader;
            this.jar = createBootstrapJar();

            ProcessBuilder builder = new ProcessBuilder();
            List<String> command = new ArrayList<String>();
            command.addAll(Arrays.asList(java.split("\\s+")));
            command.add("-jar");
            command.add(jar.getPath());
            builder.command(command);
            this.process = builder.start();

            this.output = new DataOutputStream(process.getOutputStream());
            this.input = new DataInputStream(process.getInputStream());
            this.error = process.getErrorStream();

            waitForStartBeacon();

            sendObject(loader, resources);
            sendObject(object, resources);

            ok = true;
        } finally {
            if (!ok) {
                close();
            }
        }
    }

    private void waitForStartBeacon() throws IOException {
        while (true) {
            consumeErrorStream();
            int type = input.read();
            if ((byte) type == ForkServer.READY) {
                consumeErrorStream();
                return;
            }
        }
    }

    public synchronized boolean ping() {
        try {
            output.writeByte(ForkServer.PING);
            output.flush();
            while (true) {
                consumeErrorStream();
                int type = input.read();
                if (type == ForkServer.PING) {
                    consumeErrorStream();
                    return true;
                } else {
                    return false;
                }
            }
        } catch (IOException e) {
            return false;
        }
    }


    public synchronized Throwable call(String method, Object... args)
            throws IOException, TikaException {
        List<ForkResource> r = new ArrayList<ForkResource>(resources);
        output.writeByte(ForkServer.CALL);
        output.writeUTF(method);
        for (int i = 0; i < args.length; i++) {
            sendObject(args[i], r);
        }
        return waitForResponse(r);
    }

    /**
     * Serializes the object first into an in-memory buffer and then
     * writes it to the output stream with a preceding size integer.
     *
     * @param object object to be serialized
     * @param resources list of fork resources, used when adding proxies
     * @throws IOException if the object could not be serialized
     */
    private void sendObject(Object object, List<ForkResource> resources)
            throws IOException, TikaException {
        int n = resources.size();
        if (object instanceof InputStream) {
            resources.add(new InputStreamResource((InputStream) object));
            object = new InputStreamProxy(n);
        } else if (object instanceof ContentHandler) {
            resources.add(new ContentHandlerResource((ContentHandler) object));
            object = new ContentHandlerProxy(n);
        } else if (object instanceof ClassLoader) {
            resources.add(new ClassLoaderResource((ClassLoader) object));
            object = new ClassLoaderProxy(n);
        }

        try {
           ForkObjectInputStream.sendObject(object, output);
        } catch(NotSerializableException nse) {
           // Build a more friendly error message for this
           throw new TikaException(
                 "Unable to serialize " + object.getClass().getSimpleName() +
                 " to pass to the Forked Parser", nse);
        }

        waitForResponse(resources);
    }

    public synchronized void close() {
        try {
            if (output != null) {
                output.close();
            }
            if (input != null) {
                input.close();
            }
            if (error != null) {
                error.close();
            }
        } catch (IOException ignore) {
        }
        if (process != null) {
            process.destroy();
        }
        if (jar != null) {
            jar.delete();
        }
    }

    private Throwable waitForResponse(List<ForkResource> resources)
            throws IOException {
        output.flush();
        while (true) {
            consumeErrorStream();
            int type = input.read();
            if (type == -1) {
                consumeErrorStream();
                throw new IOException(
                        "Lost connection to a forked server process");
            } else if (type == ForkServer.RESOURCE) {
                ForkResource resource =
                    resources.get(input.readUnsignedByte());
                resource.process(input, output);
            } else if ((byte) type == ForkServer.ERROR) {
                try {
                    return (Throwable) ForkObjectInputStream.readObject(
                            input, loader);
                } catch (ClassNotFoundException e) {
                    throw new IOExceptionWithCause(
                            "Unable to deserialize an exception", e);
                }
            } else {
                return null;
            }
        }
    }

    /**
     * Consumes all pending bytes from the standard error stream of the
     * forked server process, and prints them out to the standard error
     * stream of this process. This method should be called always before
     * expecting some output from the server, to prevent the server from
     * blocking due to a filled up pipe buffer of the error stream.
     *
     * @throws IOException if the error stream could not be read
     */
    private void consumeErrorStream() throws IOException {
        int n;
        while ((n = error.available()) > 0) {
            byte[] b = new byte[n];
            n = error.read(b);
            if (n > 0) {
                System.err.write(b, 0, n);
            }
        }
    }

    /**
     * Creates a temporary jar file that can be used to bootstrap the forked
     * server process. Remember to remove the file when no longer used.
     *
     * @return the created jar file
     * @throws IOException if the bootstrap archive could not be created
     */
    private static File createBootstrapJar() throws IOException {
        File file = File.createTempFile("apache-tika-fork-", ".jar");
        boolean ok = false;
        try {
            fillBootstrapJar(file);
            ok = true;
        } finally {
            if (!ok) {
                file.delete();
            }
        }
        return file;
    }

    /**
     * Fills in the jar file used to bootstrap the forked server process.
     * All the required <code>.class</code> files and a manifest with a
     * <code>Main-Class</code> entry are written into the archive.
     *
     * @param file file to hold the bootstrap archive
     * @throws IOException if the bootstrap archive could not be created
     */
    private static void fillBootstrapJar(File file) throws IOException {
        JarOutputStream jar = new JarOutputStream(new FileOutputStream(file));
        try {
            String manifest =
                "Main-Class: " + ForkServer.class.getName() + "\n";
            jar.putNextEntry(new ZipEntry("META-INF/MANIFEST.MF"));
            jar.write(manifest.getBytes("UTF-8"));

            Class<?>[] bootstrap = {
                    ForkServer.class, ForkObjectInputStream.class,
                    ForkProxy.class, ClassLoaderProxy.class,
                    MemoryURLConnection.class,
                    MemoryURLStreamHandler.class,
                    MemoryURLStreamHandlerFactory.class,
                    MemoryURLStreamRecord.class
            };
            ClassLoader loader = ForkServer.class.getClassLoader();
            for (Class<?> klass : bootstrap) {
                String path = klass.getName().replace('.', '/') + ".class";
                InputStream input = loader.getResourceAsStream(path);
                try {
                    jar.putNextEntry(new JarEntry(path));
                    IOUtils.copy(input, jar);
                } finally {
                    input.close();
                }
            }
        } finally {
            jar.close();
        }
    }

}
"
tika-core/src/main/java/org/apache/tika/fork/ForkObjectInputStream.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.fork;

import java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;
import java.io.DataInputStream;
import java.io.DataOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.ObjectInputStream;
import java.io.ObjectOutputStream;
import java.io.ObjectStreamClass;

/**
 * An object input stream that uses a given class loader when deserializing
 * objects.
 * <p>
 * Note that this functionality could easily be implemented as a simple
 * anonymous {@link ObjectInputStream} subclass, but since the
 * functionality is needed during the somewhat complicated bootstrapping
 * of the stdin/out communication channel of a forked server process,
 * it's better if class has a stable name that can be referenced at
 * compile-time by the {@link ForkClient} class.
 */
class ForkObjectInputStream extends ObjectInputStream {

    /** The class loader used when deserializing objects. */
    private final ClassLoader loader;

    /**
     * Creates a new object input stream that uses the given class loader
     * when deserializing objects.
     *
     * @param input underlying input stream
     * @param loader class loader used when deserializing objects
     * @throws IOException if this stream could not be initiated
     */
    public ForkObjectInputStream(InputStream input, ClassLoader loader)
            throws IOException {
        super(input);
        this.loader = loader;
    }

    /**
     * Loads the identified class from the specified class loader.
     *
     * @param desc class description
     * @return class loaded class
     * @throws ClassNotFoundException if the class can not be found
     */
    @Override
    protected Class<?> resolveClass(ObjectStreamClass desc)
            throws ClassNotFoundException {
        return Class.forName(desc.getName(), false, loader);
    }

    /**
     * Serializes the object first into an in-memory buffer and then
     * writes it to the output stream with a preceding size integer.
     *
     * @param object object to be serialized
     * @param output output stream
     * @throws IOException if the object could not be serialized
     */
    public static void sendObject(Object object, DataOutputStream output)
            throws IOException {
        ByteArrayOutputStream buffer = new ByteArrayOutputStream();
        ObjectOutputStream serializer = new ObjectOutputStream(buffer);
        serializer.writeObject(object);
        serializer.close();

        byte[] data = buffer.toByteArray();
        output.writeInt(data.length);
        output.write(data);
    }

    /**
     * Deserializes an object from the given stream. The serialized object
     * is expected to be preceded by a size integer, that is used for reading
     * the entire serialization into a memory before deserializing it.
     *
     * @param input input stream from which the serialized object is read
     * @param loader class loader to be used for loading referenced classes
     * @throws IOException if the object could not be deserialized
     * @throws ClassNotFoundException if a referenced class is not found
     */
    public static Object readObject(DataInputStream input, ClassLoader loader)
            throws IOException, ClassNotFoundException {
        int n = input.readInt();
        byte[] data = new byte[n];
        input.readFully(data);

        ObjectInputStream deserializer =
            new ForkObjectInputStream(new ByteArrayInputStream(data), loader);
        return deserializer.readObject();
    }

}
"
tika-core/src/main/java/org/apache/tika/fork/ForkParser.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.fork;

import java.io.IOException;
import java.io.InputStream;
import java.util.LinkedList;
import java.util.Queue;
import java.util.Set;

import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.AutoDetectParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.Parser;
import org.apache.tika.sax.TeeContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

public class ForkParser extends AbstractParser {

    /** Serial version UID */
    private static final long serialVersionUID = -4962742892274663950L;

    private final ClassLoader loader;

    private final Parser parser;

    /** Java command line */
    private String java = "java -Xmx32m";

    /** Process pool size */
    private int poolSize = 5;

    private int currentlyInUse = 0;

    private final Queue<ForkClient> pool =
        new LinkedList<ForkClient>();

    /**
     * @param loader The ClassLoader to use 
     * @param parser the parser to delegate to. This one cannot be another ForkParser
     */
    public ForkParser(ClassLoader loader, Parser parser) {
        if (parser instanceof ForkParser) {
            throw new IllegalArgumentException("The underlying parser of a ForkParser should not be a ForkParser, but a specific implementation.");
        }
        this.loader = loader;
        this.parser = parser;
    }

    public ForkParser(ClassLoader loader) {
        this(loader, new AutoDetectParser());
    }

    public ForkParser() {
        this(ForkParser.class.getClassLoader());
    }

    /**
     * Returns the size of the process pool.
     *
     * @return process pool size
     */
    public synchronized int getPoolSize() {
        return poolSize;
    }

    /**
     * Sets the size of the process pool.
     *
     * @param poolSize process pool size
     */
    public synchronized void setPoolSize(int poolSize) {
        this.poolSize = poolSize;
    }

    /**
     * Returns the command used to start the forked server process.
     *
     * @return java command line
     */
    public String getJavaCommand() {
        return java;
    }

    /**
     * Sets the command used to start the forked server process.
     * The given command line is split on whitespace and the arguments
     * "-jar" and "/path/to/bootstrap.jar" are appended to it when starting
     * the process. The default setting is "java -Xmx32m".
     *
     * @param java java command line
     */
    public void setJavaCommand(String java) {
        this.java = java;
    }

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return parser.getSupportedTypes(context);
    }

    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        if (stream == null) {
            throw new NullPointerException("null stream");
        }

        Throwable t;

        boolean alive = false;
        ForkClient client = acquireClient();
        try {
            ContentHandler tee = new TeeContentHandler(
                    handler, new MetadataContentHandler(metadata));
            t = client.call("parse", stream, tee, metadata, context);
            alive = true;
        } catch (TikaException te) {
            // Problem occurred on our side
            alive = true;
            throw te;
        } catch (IOException e) {
            // Problem occurred on the other side
            throw new TikaException(
                    "Failed to communicate with a forked parser process."
                    + " The process has most likely crashed due to some error"
                    + " like running out of memory. A new process will be"
                    + " started for the next parsing request.", e);
        } finally {
            releaseClient(client, alive);
        }

        if (t instanceof IOException) {
            throw (IOException) t;
        } else if (t instanceof SAXException) {
            throw (SAXException) t;
        } else if (t instanceof TikaException) {
            throw (TikaException) t;
        } else if (t != null) {
            throw new TikaException(
                    "Unexpected error in forked server process", t);
        }
    }

    public synchronized void close() {
        for (ForkClient client : pool) {
            client.close();
        }
        pool.clear();
        poolSize = 0;
    }

    private synchronized ForkClient acquireClient()
            throws IOException, TikaException {
        while (true) {
            ForkClient client = pool.poll();

            // Create a new process if there's room in the pool
            if (client == null && currentlyInUse < poolSize) {
                client = new ForkClient(loader, parser, java);
            }

            // Ping the process, and get rid of it if it's inactive
            if (client != null && !client.ping()) {
                client.close();
                client = null;
            }

            if (client != null) {
                currentlyInUse++;
                return client;
            } else if (currentlyInUse >= poolSize) {
                try {
                    wait();
                } catch (InterruptedException e) {
                    throw new TikaException(
                            "Interrupted while waiting for a fork parser", e);
                }
            }
        }
    }

    private synchronized void releaseClient(ForkClient client, boolean alive) {
        currentlyInUse--;
        if (currentlyInUse + pool.size() < poolSize && alive) {
            pool.offer(client);
            notifyAll();
        } else {
            client.close();
        }
    }

}
"
tika-core/src/main/java/org/apache/tika/fork/ForkProxy.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.fork;

import java.io.DataInputStream;
import java.io.DataOutputStream;
import java.io.Serializable;

public interface ForkProxy extends Serializable {

    void init(DataInputStream input, DataOutputStream output);

}
"
tika-core/src/main/java/org/apache/tika/fork/ForkResource.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.fork;

import java.io.DataInputStream;
import java.io.DataOutputStream;

import java.io.IOException;

public interface ForkResource {

    Throwable process(DataInputStream input, DataOutputStream output)
        throws IOException;

}
"
tika-core/src/main/java/org/apache/tika/fork/ForkServer.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.fork;

import java.io.ByteArrayInputStream;
import java.io.DataInputStream;
import java.io.DataOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.NotSerializableException;
import java.io.OutputStream;
import java.lang.reflect.InvocationTargetException;
import java.lang.reflect.Method;
import java.net.URL;
import java.util.zip.CheckedInputStream;
import java.util.zip.CheckedOutputStream;
import java.util.zip.Checksum;

import org.apache.tika.exception.TikaException;

class ForkServer implements Runnable, Checksum {

    public static final byte ERROR = -1;

    public static final byte DONE = 0;

    public static final byte CALL = 1;

    public static final byte PING = 2;

    public static final byte RESOURCE = 3;

    public static final byte READY = 4;

    /**
     * Starts a forked server process using the standard input and output
     * streams for communication with the parent process. Any attempts by
     * stray code to read from standard input or write to standard output
     * is redirected to avoid interfering with the communication channel.
     * 
     * @param args command line arguments, ignored
     * @throws Exception if the server could not be started
     */
    public static void main(String[] args) throws Exception {
        URL.setURLStreamHandlerFactory(new MemoryURLStreamHandlerFactory());

        ForkServer server = new ForkServer(System.in, System.out);
        System.setIn(new ByteArrayInputStream(new byte[0]));
        System.setOut(System.err);

        Thread watchdog = new Thread(server, "Tika Watchdog");
        watchdog.setDaemon(true);
        watchdog.start();

        server.processRequests();
    }

    /** Input stream for reading from the parent process */
    private final DataInputStream input;

    /** Output stream for writing to the parent process */
    private final DataOutputStream output;

    private volatile boolean active = true;

    /**
     * Sets up a forked server instance using the given stdin/out
     * communication channel.
     *
     * @param input input stream for reading from the parent process
     * @param output output stream for writing to the parent process
     * @throws IOException if the server instance could not be created
     */
    public ForkServer(InputStream input, OutputStream output)
            throws IOException {
        this.input =
            new DataInputStream(new CheckedInputStream(input, this));
        this.output =
            new DataOutputStream(new CheckedOutputStream(output, this));
    }

    public void run() {
        try {
            while (active) {
                active = false;
                Thread.sleep(5000);
            }
            System.exit(0);
        } catch (InterruptedException e) {
        }
    }

    public void processRequests() {
        try {
            output.writeByte(READY);
            output.flush();

            ClassLoader loader = (ClassLoader) readObject(
                    ForkServer.class.getClassLoader());
            Thread.currentThread().setContextClassLoader(loader);

            Object object = readObject(loader);
            while (true) {
                int request = input.read();
                if (request == -1) {
                    break;
                } else if (request == PING) {
                    output.writeByte(PING);
                } else if (request == CALL) {
                    call(loader, object);
                } else {
                    throw new IllegalStateException("Unexpected request");
                }
                output.flush();
            }
        } catch (Throwable t) {
            t.printStackTrace();
        }
        System.err.flush();
    }

    private void call(ClassLoader loader, Object object) throws Exception {
        Method method = getMethod(object, input.readUTF());
        Object[] args =
            new Object[method.getParameterTypes().length];
        for (int i = 0; i < args.length; i++) {
            args[i] = readObject(loader);
        }
        try {
            method.invoke(object, args);
            output.write(DONE);
        } catch (InvocationTargetException e) {
            output.write(ERROR);
            
            // Try to send the underlying Exception itself
            Throwable toSend = e.getCause();
            try {
               ForkObjectInputStream.sendObject(toSend, output);
            } catch (NotSerializableException nse) {
               // Need to build a serializable version of it
               TikaException te = new TikaException( toSend.getMessage() );
               te.setStackTrace( toSend.getStackTrace() );
               ForkObjectInputStream.sendObject(te, output);
            }
        }
    }

    private Method getMethod(Object object, String name) {
        Class<?> klass = object.getClass();
        while (klass != null) {
            for (Class<?> iface : klass.getInterfaces()) {
                for (Method method : iface.getMethods()) {
                    if (name.equals(method.getName())) {
                        return method;
                    }
                }
            }
            klass = klass.getSuperclass();
        }
        return null;
    }

    /**
     * Deserializes an object from the given stream. The serialized object
     * is expected to be preceded by a size integer, that is used for reading
     * the entire serialization into a memory before deserializing it.
     *
     * @param input input stream from which the serialized object is read
     * @param loader class loader to be used for loading referenced classes
     * @throws IOException if the object could not be deserialized
     * @throws ClassNotFoundException if a referenced class is not found
     */
    private Object readObject(ClassLoader loader)
            throws IOException, ClassNotFoundException {
        Object object = ForkObjectInputStream.readObject(input, loader);
        if (object instanceof ForkProxy) {
            ((ForkProxy) object).init(input, output);
        }

        // Tell the parent process that we successfully received this object
        output.writeByte(ForkServer.DONE);
        output.flush();

        return object;
    }

    //------------------------------------------------------------< Checksum >

    public void update(int b) {
        active = true;
    }

    public void update(byte[] b, int off, int len) {
        active = true;
    }

    public long getValue() {
        return 0;
    }

    public void reset() {
    }

}
"
tika-core/src/main/java/org/apache/tika/fork/InputStreamProxy.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.fork;

import java.io.DataInputStream;
import java.io.DataOutputStream;
import java.io.IOException;
import java.io.InputStream;

class InputStreamProxy extends InputStream implements ForkProxy {

    /** Serial version UID */
    private static final long serialVersionUID = 4350939227765568438L;

    private final int resource;

    private transient DataInputStream input;

    private transient DataOutputStream output;

    public InputStreamProxy(int resource) {
        this.resource = resource;
    }

    public void init(DataInputStream input, DataOutputStream output) {
        this.input = input;
        this.output = output;
    }

    @Override
    public int read() throws IOException {
        output.writeByte(ForkServer.RESOURCE);
        output.writeByte(resource);
        output.writeInt(1);
        output.flush();
        int n = input.readInt();
        if (n == 1) {
            return input.readUnsignedByte();
        } else {
            return n;
        }
    }

    @Override
    public int read(byte[] b, int off, int len) throws IOException {
        output.writeByte(ForkServer.RESOURCE);
        output.writeByte(resource);
        output.writeInt(len);
        output.flush();
        int n = input.readInt();
        if (n > 0) {
            input.readFully(b, off, n);
        }
        return n;
    }

}
"
tika-core/src/main/java/org/apache/tika/fork/InputStreamResource.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.fork;

import java.io.DataInputStream;
import java.io.DataOutputStream;
import java.io.IOException;
import java.io.InputStream;

class InputStreamResource implements ForkResource {

    private final InputStream stream;

    public InputStreamResource(InputStream stream) {
        this.stream = stream;
    }

    public Throwable process(DataInputStream input, DataOutputStream output)
            throws IOException {
        int n = input.readInt();
        byte[] buffer = new byte[n];
        int m;
        try {
            m = stream.read(buffer);
        } catch (IOException e) {
            return e;
        }
        output.writeInt(m);
        if (m > 0) {
            output.write(buffer, 0, m);
        }
        output.flush();
        return null;
    }

}
"
tika-core/src/main/java/org/apache/tika/fork/MemoryURLConnection.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.fork;

import java.io.ByteArrayInputStream;
import java.io.InputStream;
import java.net.URL;
import java.net.URLConnection;

class MemoryURLConnection extends URLConnection {

    private final byte[] data;

    MemoryURLConnection(URL url, byte[] data) {
        super(url);
        this.data = data;
    }

    @Override
    public void connect() {
    }

    @Override
    public InputStream getInputStream() {
        return new ByteArrayInputStream(data);
    }

}"
tika-core/src/main/java/org/apache/tika/fork/MemoryURLStreamHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.fork;

import java.io.IOException;
import java.lang.ref.WeakReference;
import java.net.MalformedURLException;
import java.net.URL;
import java.net.URLConnection;
import java.net.URLStreamHandler;
import java.util.Iterator;
import java.util.LinkedList;
import java.util.List;
import java.util.concurrent.atomic.AtomicInteger;

class MemoryURLStreamHandler extends URLStreamHandler {

    private static final AtomicInteger counter = new AtomicInteger();

    private static final List<MemoryURLStreamRecord> records =
        new LinkedList<MemoryURLStreamRecord>();

    public static URL createURL(byte[] data) {
        try {
            int i = counter.incrementAndGet();
            URL url =  new URL("tika-in-memory", "localhost", "/" + i);

            MemoryURLStreamRecord record = new MemoryURLStreamRecord();
            record.url = new WeakReference<URL>(url);
            record.data = data;
            records.add(record);

            return url;
        } catch (MalformedURLException e) {
            throw new RuntimeException(e);
        }
    }

    @Override
    protected URLConnection openConnection(URL u) throws IOException {
        Iterator<MemoryURLStreamRecord> iterator = records.iterator();
        while (iterator.hasNext()) {
            MemoryURLStreamRecord record = iterator.next();
            URL url = record.url.get();
            if (url == null) {
                iterator.remove();
            } else if (url == u) {
                return new MemoryURLConnection(u, record.data);
            }
        }
        throw new IOException("Unknown URL: " + u);
    }

}
"
tika-core/src/main/java/org/apache/tika/fork/MemoryURLStreamHandlerFactory.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.fork;

import java.net.URLStreamHandler;
import java.net.URLStreamHandlerFactory;

class MemoryURLStreamHandlerFactory implements URLStreamHandlerFactory {

    public URLStreamHandler createURLStreamHandler(String protocol) {
        if ("tika-in-memory".equals(protocol)) {
            return new MemoryURLStreamHandler();
        } else {
            return null;
        }
    }

}"
tika-core/src/main/java/org/apache/tika/fork/MemoryURLStreamRecord.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.fork;

import java.lang.ref.WeakReference;
import java.net.URL;

class MemoryURLStreamRecord {

    public WeakReference<URL> url;
    public byte[] data;

}"
tika-core/src/main/java/org/apache/tika/fork/MetadataContentHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.fork;

import org.apache.tika.metadata.Metadata;
import org.xml.sax.Attributes;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.DefaultHandler;

class MetadataContentHandler extends DefaultHandler {

    private final Metadata metadata;

    public MetadataContentHandler(Metadata metadata) {
        this.metadata = metadata;
    }

    public void startElement(
            String uri, String local, String name, Attributes attributes)
            throws SAXException {
        if ("meta".equals(local)) {
            String aname = attributes.getValue("name");
            String content = attributes.getValue("content");
            metadata.add(aname, content);
        }
    }

}
"
tika-core/src/main/java/org/apache/tika/fork/package-info.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Forked parser.
 */
@aQute.bnd.annotation.Version("1.0.0")
package org.apache.tika.fork;
"
tika-core/src/main/java/org/apache/tika/io/ClosedInputStream.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.io;

import java.io.InputStream;

/**
 * Closed input stream. This stream returns -1 to all attempts to read
 * something from the stream.
 * <p>
 * Typically uses of this class include testing for corner cases in methods
 * that accept input streams and acting as a sentinel value instead of a
 * <code>null</code> input stream.
 *
 * @since Apache Tika 0.4, copied from Commons IO 1.4
 */
public class ClosedInputStream extends InputStream {

    /**
     * Returns -1 to indicate that the stream is closed.
     *
     * @return always -1
     */
    @Override
    public int read() {
        return -1;
    }

}
"
tika-core/src/main/java/org/apache/tika/io/CloseShieldInputStream.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.io;

import java.io.InputStream;

/**
 * Proxy stream that prevents the underlying input stream from being closed.
 * <p>
 * This class is typically used in cases where an input stream needs to be
 * passed to a component that wants to explicitly close the stream even if
 * more input would still be available to other components.
 *
 * @since Apache Tika 0.4, copied from Commons IO 1.4
 */
public class CloseShieldInputStream extends ProxyInputStream {

    /**
     * Creates a proxy that shields the given input stream from being
     * closed.
     *
     * @param in underlying input stream
     */
    public CloseShieldInputStream(InputStream in) {
        super(in);
    }

    /**
     * Replaces the underlying input stream with a {@link ClosedInputStream}
     * sentinel. The original input stream will remain open, but this proxy
     * will appear closed.
     */
    @Override
    public void close() {
        in = new ClosedInputStream();
    }

}
"
tika-core/src/main/java/org/apache/tika/io/CountingInputStream.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.io;

import java.io.IOException;
import java.io.InputStream;

/**
 * A decorating input stream that counts the number of bytes that have passed
 * through the stream so far.
 * <p>
 * A typical use case would be during debugging, to ensure that data is being
 * read as expected.
 *
 * @author Marcelo Liberato
 * @since Apache Tika 0.4, copied from Commons IO 1.4
 */
public class CountingInputStream extends ProxyInputStream {

    /** The count of bytes that have passed. */
    private long count;

    /**
     * Constructs a new CountingInputStream.
     *
     * @param in  the InputStream to delegate to
     */
    public CountingInputStream(InputStream in) {
        super(in);
    }

    //-----------------------------------------------------------------------
    /**
     * Reads a number of bytes into the byte array, keeping count of the
     * number read.
     *
     * @param b  the buffer into which the data is read, not null
     * @return the total number of bytes read into the buffer, -1 if end of stream
     * @throws IOException if an I/O error occurs
     * @see java.io.InputStream#read(byte[]) 
     */
    @Override
    public int read(byte[] b) throws IOException {
        int found = super.read(b);
        this.count += (found >= 0) ? found : 0;
        return found;
    }

    /**
     * Reads a number of bytes into the byte array at a specific offset,
     * keeping count of the number read.
     *
     * @param b  the buffer into which the data is read, not null
     * @param off  the start offset in the buffer
     * @param len  the maximum number of bytes to read
     * @return the total number of bytes read into the buffer, -1 if end of stream
     * @throws IOException if an I/O error occurs
     * @see java.io.InputStream#read(byte[], int, int)
     */
    @Override
    public int read(byte[] b, int off, int len) throws IOException {
        int found = super.read(b, off, len);
        this.count += (found >= 0) ? found : 0;
        return found;
    }

    /**
     * Reads the next byte of data adding to the count of bytes received
     * if a byte is successfully read. 
     *
     * @return the byte read, -1 if end of stream
     * @throws IOException if an I/O error occurs
     * @see java.io.InputStream#read()
     */
    @Override
    public int read() throws IOException {
        int found = super.read();
        this.count += (found >= 0) ? 1 : 0;
        return found;
    }

    /**
     * Skips the stream over the specified number of bytes, adding the skipped
     * amount to the count.
     *
     * @param length  the number of bytes to skip
     * @return the actual number of bytes skipped
     * @throws IOException if an I/O error occurs
     * @see java.io.InputStream#skip(long)
     */
    @Override
    public long skip(final long length) throws IOException {
        final long skip = super.skip(length);
        this.count += skip;
        return skip;
    }

    //-----------------------------------------------------------------------
    /**
     * The number of bytes that have passed through this stream.
     * <p>
     * NOTE: From v1.3 this method throws an ArithmeticException if the
     * count is greater than can be expressed by an <code>int</code>.
     * See {@link #getByteCount()} for a method using a <code>long</code>.
     *
     * @return the number of bytes accumulated
     * @throws ArithmeticException if the byte count is too large
     */
    public synchronized int getCount() {
        long result = getByteCount();
        if (result > Integer.MAX_VALUE) {
            throw new ArithmeticException("The byte count " + result + " is too large to be converted to an int");
        }
        return (int) result;
    }

    /** 
     * Set the byte count back to 0. 
     * <p>
     * NOTE: From v1.3 this method throws an ArithmeticException if the
     * count is greater than can be expressed by an <code>int</code>.
     * See {@link #resetByteCount()} for a method using a <code>long</code>.
     *
     * @return the count previous to resetting
     * @throws ArithmeticException if the byte count is too large
     */
    public synchronized int resetCount() {
        long result = resetByteCount();
        if (result > Integer.MAX_VALUE) {
            throw new ArithmeticException("The byte count " + result + " is too large to be converted to an int");
        }
        return (int) result;
    }

    /**
     * The number of bytes that have passed through this stream.
     * <p>
     * NOTE: This method is an alternative for <code>getCount()</code>
     * and was added because that method returns an integer which will
     * result in incorrect count for files over 2GB.
     *
     * @return the number of bytes accumulated
     * @since Commons IO 1.3
     */
    public synchronized long getByteCount() {
        return this.count;
    }

    /** 
     * Set the byte count back to 0. 
     * <p>
     * NOTE: This method is an alternative for <code>resetCount()</code>
     * and was added because that method returns an integer which will
     * result in incorrect count for files over 2GB.
     *
     * @return the count previous to resetting
     * @since Commons IO 1.3
     */
    public synchronized long resetByteCount() {
        long tmp = this.count;
        this.count = 0;
        return tmp;
    }

    public String toString() {
       return "Tika Counting InputStream wrapping " + in.toString(); 
   }
}
"
tika-core/src/main/java/org/apache/tika/io/EndianUtils.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.io;

import java.io.IOException;
import java.io.InputStream;

import org.apache.tika.exception.TikaException;

/**
 * General Endian Related Utilties.
 * <p>
 * This class provides static utility methods for input/output operations
 *  on numbers in Big and Little Endian formats.
 * <p>
 * Origin of code: Based on the version in POI
 */
public class EndianUtils {
   /**
    * Get a LE short value from an InputStream
    *
    * @param  stream the InputStream from which the short is to be read
    * @return                              the short (16-bit) value
    * @exception  IOException              will be propagated back to the caller
    * @exception  BufferUnderrunException  if the stream cannot provide enough bytes
    */
   public static short readShortLE(InputStream stream) throws IOException, BufferUnderrunException {
      return (short) readUShortLE(stream);
   }
   /**
    * Get a BE short value from an InputStream
    *
    * @param  stream the InputStream from which the short is to be read
    * @return                              the short (16-bit) value
    * @exception  IOException              will be propagated back to the caller
    * @exception  BufferUnderrunException  if the stream cannot provide enough bytes
    */
   public static short readShortBE(InputStream stream) throws IOException, BufferUnderrunException {
      return (short) readUShortBE(stream);
   }

   public static int readUShortLE(InputStream stream) throws IOException, BufferUnderrunException {
      int ch1 = stream.read();
      int ch2 = stream.read();
      if ((ch1 | ch2) < 0) {
         throw new BufferUnderrunException();
      }
      return (ch2 << 8) + (ch1 << 0);
   }
   public static int readUShortBE(InputStream stream) throws IOException, BufferUnderrunException {
      int ch1 = stream.read();
      int ch2 = stream.read();
      if ((ch1 | ch2) < 0) {
         throw new BufferUnderrunException();
      }
      return (ch1 << 8) + (ch2 << 0);
   }

   /**
    * Get a LE int value from an InputStream
    *
    * @param  stream the InputStream from which the int is to be read
    * @return                              the int (32-bit) value
    * @exception  IOException              will be propagated back to the caller
    * @exception  BufferUnderrunException  if the stream cannot provide enough bytes
    */
   public static int readIntLE(InputStream stream) throws IOException, BufferUnderrunException {
      int ch1 = stream.read();
      int ch2 = stream.read();
      int ch3 = stream.read();
      int ch4 = stream.read();
      if ((ch1 | ch2 | ch3 | ch4) < 0) {
         throw new BufferUnderrunException();
      }
      return (ch4 << 24) + (ch3<<16) + (ch2 << 8) + (ch1 << 0);
   }
   /**
    * Get a BE int value from an InputStream
    *
    * @param  stream the InputStream from which the int is to be read
    * @return                              the int (32-bit) value
    * @exception  IOException              will be propagated back to the caller
    * @exception  BufferUnderrunException  if the stream cannot provide enough bytes
    */
   public static int readIntBE(InputStream stream) throws IOException, BufferUnderrunException {
      int ch1 = stream.read();
      int ch2 = stream.read();
      int ch3 = stream.read();
      int ch4 = stream.read();
      if ((ch1 | ch2 | ch3 | ch4) < 0) {
         throw new BufferUnderrunException();
      }
      return (ch1 << 24) + (ch2<<16) + (ch3 << 8) + (ch4 << 0);
   }

   /**
    * Get a LE long value from an InputStream
    *
    * @param  stream the InputStream from which the long is to be read
    * @return                              the long (64-bit) value
    * @exception  IOException              will be propagated back to the caller
    * @exception  BufferUnderrunException  if the stream cannot provide enough bytes
    */
   public static long readLongLE(InputStream stream) throws IOException, BufferUnderrunException {
      int ch1 = stream.read();
      int ch2 = stream.read();
      int ch3 = stream.read();
      int ch4 = stream.read();
      int ch5 = stream.read();
      int ch6 = stream.read();
      int ch7 = stream.read();
      int ch8 = stream.read();
      if ((ch1 | ch2 | ch3 | ch4 | ch5 | ch6 | ch7 | ch8) < 0) {
         throw new BufferUnderrunException();
      }

      return
      ((long)ch8 << 56) +
      ((long)ch7 << 48) +
      ((long)ch6 << 40) +
      ((long)ch5 << 32) +
      ((long)ch4 << 24) + // cast to long to preserve bit 31 (sign bit for ints)
      (ch3 << 16) +
      (ch2 <<  8) +
      (ch1 <<  0);
   }
   /**
    * Get a NE long value from an InputStream
    *
    * @param  stream the InputStream from which the long is to be read
    * @return                              the long (64-bit) value
    * @exception  IOException              will be propagated back to the caller
    * @exception  BufferUnderrunException  if the stream cannot provide enough bytes
    */
   public static long readLongBE(InputStream stream) throws IOException, BufferUnderrunException {
      int ch1 = stream.read();
      int ch2 = stream.read();
      int ch3 = stream.read();
      int ch4 = stream.read();
      int ch5 = stream.read();
      int ch6 = stream.read();
      int ch7 = stream.read();
      int ch8 = stream.read();
      if ((ch1 | ch2 | ch3 | ch4 | ch5 | ch6 | ch7 | ch8) < 0) {
         throw new BufferUnderrunException();
      }

      return
      ((long)ch1 << 56) +
      ((long)ch2 << 48) +
      ((long)ch3 << 40) +
      ((long)ch4 << 32) +
      ((long)ch5 << 24) + // cast to long to preserve bit 31 (sign bit for ints)
      (ch6 << 16) +
      (ch7 <<  8) +
      (ch8 <<  0);
   }
   
   /**
    * Gets the integer value that is stored in UTF-8 like fashion, in Big Endian
    *   but with the high bit on each number indicating if it continues or not
    */
   public static long readUE7(InputStream stream) throws IOException {
       int i;
       long v = 0;
       while ((i = stream.read()) >= 0) {
           v = v << 7;
           if ((i & 128) == 128) {
               // Continues
               v += (i&127);
           } else {
               // Last value
               v += i;
               break;
           }
       }
       return v;
   }
   
   
   /**
    * Get a LE short value from the beginning of a byte array
    *
    *@param  data  the byte array
    *@return       the short (16-bit) value
    */
   public static short getShortLE(byte[] data) {
      return getShortLE(data, 0);
   }
   /**
    * Get a LE short value from a byte array
    *
    *@param  data    the byte array
    *@param  offset  a starting offset into the byte array
    *@return         the short (16-bit) value
    */
   public static short getShortLE(byte[] data, int offset) {
      return (short)getUShortLE(data, offset);
   }

   /**
    * Get a LE unsigned short value from the beginning of a byte array
    *
    *@param  data  the byte array
    *@return       the unsigned short (16-bit) value in an int
    */
   public static int getUShortLE(byte[] data) {
      return getUShortLE(data, 0);
   }
   /**
    * Get a LE unsigned short value from a byte array
    *
    *@param  data    the byte array
    *@param  offset  a starting offset into the byte array
    *@return         the unsigned short (16-bit) value in an integer
    */
   public static int getUShortLE(byte[] data, int offset) {
      int b0 = data[offset] & 0xFF;
      int b1 = data[offset+1] & 0xFF;
      return (b1 << 8) + (b0 << 0);
   }
   
   /**
    * Get a BE short value from the beginning of a byte array
    *
    *@param  data  the byte array
    *@return       the short (16-bit) value
    */
   public static short getShortBE(byte[] data) {
      return getShortBE(data, 0);
   }
   /**
    * Get a BE short value from a byte array
    *
    *@param  data    the byte array
    *@param  offset  a starting offset into the byte array
    *@return         the short (16-bit) value
    */
   public static short getShortBE(byte[] data, int offset) {
      return (short)getUShortBE(data, offset);
   }

   /**
    * Get a BE unsigned short value from the beginning of a byte array
    *
    *@param  data  the byte array
    *@return       the unsigned short (16-bit) value in an int
    */
   public static int getUShortBE(byte[] data) {
      return getUShortBE(data, 0);
   }
   /**
    * Get a BE unsigned short value from a byte array
    *
    *@param  data    the byte array
    *@param  offset  a starting offset into the byte array
    *@return         the unsigned short (16-bit) value in an integer
    */
   public static int getUShortBE(byte[] data, int offset) {
      int b0 = data[offset] & 0xFF;
      int b1 = data[offset+1] & 0xFF;
      return (b0 << 8) + (b1 << 0);
   }

   /**
    * Get a LE int value from the beginning of a byte array
    *
    *@param  data  the byte array
    *@return the int (32-bit) value
    */
   public static int getIntLE(byte[] data) {
       return getIntLE(data, 0);
   }
   /**
    * Get a LE int value from a byte array
    *
    *@param  data    the byte array
    *@param  offset  a starting offset into the byte array
    *@return         the int (32-bit) value
    */
   public static int getIntLE(byte[] data, int offset) {
       int i=offset;
       int b0 = data[i++] & 0xFF;
       int b1 = data[i++] & 0xFF;
       int b2 = data[i++] & 0xFF;
       int b3 = data[i++] & 0xFF;
       return (b3 << 24) + (b2 << 16) + (b1 << 8) + (b0 << 0);
   }

   /**
    * Get a BE int value from the beginning of a byte array
    *
    *@param  data  the byte array
    *@return the int (32-bit) value
    */
   public static int getIntBE(byte[] data) {
       return getIntBE(data, 0);
   }
   /**
    * Get a BE int value from a byte array
    *
    *@param  data    the byte array
    *@param  offset  a starting offset into the byte array
    *@return         the int (32-bit) value
    */
   public static int getIntBE(byte[] data, int offset) {
       int i=offset;
       int b0 = data[i++] & 0xFF;
       int b1 = data[i++] & 0xFF;
       int b2 = data[i++] & 0xFF;
       int b3 = data[i++] & 0xFF;
       return (b0 << 24) + (b1 << 16) + (b2 << 8) + (b3 << 0);
   }

   /**
    * Get a LE unsigned int value from a byte array
    *
    *@param  data    the byte array
    *@return         the unsigned int (32-bit) value in a long
    */
   public static long getUIntLE(byte[] data) {
       return getUIntLE(data,0);
   }
   /**
    * Get a LE unsigned int value from a byte array
    *
    *@param  data    the byte array
    *@param  offset  a starting offset into the byte array
    *@return         the unsigned int (32-bit) value in a long
    */
   public static long getUIntLE(byte[] data, int offset) {
       long retNum = getIntLE(data, offset);
       return retNum & 0x00FFFFFFFFl;
   }

   /**
    * Get a BE unsigned int value from a byte array
    *
    *@param  data    the byte array
    *@return         the unsigned int (32-bit) value in a long
    */
   public static long getUIntBE(byte[] data) {
       return getUIntBE(data,0);
   }
   /**
    * Get a BE unsigned int value from a byte array
    *
    *@param  data    the byte array
    *@param  offset  a starting offset into the byte array
    *@return         the unsigned int (32-bit) value in a long
    */
   public static long getUIntBE(byte[] data, int offset) {
       long retNum = getIntBE(data, offset);
       return retNum & 0x00FFFFFFFFl;
   }

   /**
    * Get a LE long value from a byte array
    *
    *@param  data    the byte array
    *@param  offset  a starting offset into the byte array
    *@return         the long (64-bit) value
    */
   public static long getLongLE(byte[] data, int offset) {
      long result = 0;

      for (int j = offset + LONG_SIZE - 1; j >= offset; j--) {
         result <<= 8;
         result |= 0xff & data[j];
      }
      return result;
   }
   private static final int LONG_SIZE = 8;

   
   /**
    *  Convert an 'unsigned' byte to an integer. ie, don't carry across the
    *  sign.
    *
    * @param  b  Description of the Parameter
    * @return    Description of the Return Value
    */
   public static int ubyteToInt(byte b) {
      return b & 0xFF;
   }

   /**
    * get the unsigned value of a byte.
    * 
    * @param data
    *            the byte array.
    * @param offset
    *            a starting offset into the byte array.
    * @return the unsigned value of the byte as a 16 bit short
    */
   public static short getUByte( byte[] data, int offset )
   {
      return (short) ( data[offset] & 0xFF );
   }
   
   
   public static class BufferUnderrunException extends TikaException {
      private static final long serialVersionUID = 8358288231138076276L;
      public BufferUnderrunException() {
         super("Insufficient data left in stream for required read");
      }
   }
}
"
tika-core/src/main/java/org/apache/tika/io/FilenameUtils.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.io;

import java.util.HashSet;
import java.util.Locale;


public class FilenameUtils {


    /**
     * Reserved characters
     */
    public final static char[] RESERVED_FILENAME_CHARACTERS = {
        0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07,
        0x08, 0x09, 0x0A, 0x0B, 0x0C, 0x0D, 0x0E, 0x0F,
        0x10, 0x11, 0x12, 0x13, 0x14, 0x15, 0x16, 0x17,
        0x18, 0x19, 0x1A, 0x1B, 0x1C, 0x1D, 0x1E, 0x1F,
        '?', ':', '*', '<', '>', '|'
    };

    private final static HashSet<Character> RESERVED = new HashSet<Character>(38);


    static {
        for (int i=0; i<RESERVED_FILENAME_CHARACTERS.length; ++i) {
            RESERVED.add(RESERVED_FILENAME_CHARACTERS[i]);
        }
    }


    /**
     * Scans the given file name for reserved characters on different OSs and
     * file systems and returns a sanitized version of the name with the
     * reserved chars replaced by their hexadecimal value.
     *
     * For example <code>why?.zip</code> will be converted into <code>why%3F.zip</code>
     *
     * @param name the file name to be normalized - NOT NULL
     *
     * @return the normalized file name
     *
     * @throws IllegalArgumentException if name is null
     */
    public static String normalize(final String name) {
        if (name == null) {
            throw new IllegalArgumentException("name cannot be null");
        }

        StringBuilder sb = new StringBuilder();

        for (char c: name.toCharArray()) {
            if (RESERVED.contains(c)) {
                sb.append('%').append((c<16) ? "0" : "").append(Integer.toHexString(c).toUpperCase(Locale.ROOT));
            } else {
                sb.append(c);
            }
        }

        return sb.toString();
    }

    /**
     * This is a duplication of the algorithm and functionality
     * available in commons io FilenameUtils.  If Java's File were 
     * able handle Windows file paths correctly in linux,
     * we wouldn't need this.
     * <p>
     * The goal of this is to get a filename from a path.
     * The package parsers and some other embedded doc
     * extractors could put anything into Metadata.RESOURCE_NAME_KEY.
     * <p>
     * If a careless client used that filename as if it were a
     * filename and not a path when writing embedded files,
     * bad things could happen.  Consider: "../../../my_ppt.ppt".
     * <p>
     * Consider using this in combination with {@link #normalize(String)}.
     * 
     * @param path path to strip
     * @return empty string or a filename, never null
     */
    public static String getName(final String path) {
        
        if (path == null || path.length() == 0) {
            return "";
        }
        int unix = path.lastIndexOf("/");
        int windows = path.lastIndexOf("\\");
        //some macintosh file names are stored with : as the delimiter
        //also necessary to properly handle C:somefilename
        int colon = path.lastIndexOf(":");
        String cand = path.substring(Math.max(colon, Math.max(unix, windows))+1);
        if (cand.equals("..") || cand.equals(".")){
            return "";
        }
        return cand;
    }
}
"
tika-core/src/main/java/org/apache/tika/io/IOExceptionWithCause.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.io;

import java.io.IOException;

/**
 * Subclasses IOException with the {@link Throwable} constructors missing before Java 6. If you are using Java 6,
 * consider this class deprecated and use {@link IOException}.
 * 
 * @author <a href="http://commons.apache.org/io/">Apache Commons IO</a>
 * @since Apache Tika 0.4, copied from Commons IO 1.4
 */
public class IOExceptionWithCause extends IOException {

    /**
     * Defines the serial version UID.
     */
    private static final long serialVersionUID = 1L;

    /**
     * Constructs a new instance with the given message and cause.
     * <p>
     * As specified in {@link Throwable}, the message in the given <code>cause</code> is not used in this instance's
     * message.
     * </p>
     * 
     * @param message
     *            the message (see {@link #getMessage()})
     * @param cause
     *            the cause (see {@link #getCause()}). A <code>null</code> value is allowed.
     */
    public IOExceptionWithCause(String message, Throwable cause) {
        super(message);
        this.initCause(cause);
    }

    /**
     * Constructs a new instance with the given cause.
     * <p>
     * The message is set to <code>cause==null ? null : cause.toString()</code>, which by default contains the class
     * and message of <code>cause</code>. This constructor is useful for call sites that just wrap another throwable.
     * </p>
     * 
     * @param cause
     *            the cause (see {@link #getCause()}). A <code>null</code> value is allowed.
     */
    public IOExceptionWithCause(Throwable cause) {
        super(cause == null ? null : cause.toString());
        this.initCause(cause);
    }

}
"
tika-core/src/main/java/org/apache/tika/io/IOUtils.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.io;

import java.io.BufferedInputStream;
import java.io.BufferedReader;
import java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;
import java.io.CharArrayWriter;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.io.OutputStream;
import java.io.OutputStreamWriter;
import java.io.Reader;
import java.io.StringWriter;
import java.io.UnsupportedEncodingException;
import java.io.Writer;
import java.nio.channels.Channel;
import java.util.ArrayList;
import java.util.List;

/**
 * General IO stream manipulation utilities.
 * <p>
 * This class provides static utility methods for input/output operations.
 * <ul>
 * <li>closeQuietly - these methods close a stream ignoring nulls and exceptions
 * <li>toXxx/read - these methods read data from a stream
 * <li>write - these methods write data to a stream
 * <li>copy - these methods copy all the data from one stream to another
 * <li>contentEquals - these methods compare the content of two streams
 * </ul>
 * <p>
 * The byte-to-char methods and char-to-byte methods involve a conversion step.
 * Two methods are provided in each case, one that uses the platform default
 * encoding and the other which allows you to specify an encoding. You are
 * encouraged to always specify an encoding because relying on the platform
 * default can lead to unexpected results, for example when moving from
 * development to production.
 * <p>
 * All the methods in this class that read a stream are buffered internally.
 * This means that there is no cause to use a <code>BufferedInputStream</code>
 * or <code>BufferedReader</code>. The default buffer size of 4K has been shown
 * to be efficient in tests.
 * <p>
 * Wherever possible, the methods in this class do <em>not</em> flush or close
 * the stream. This is to avoid making non-portable assumptions about the
 * streams' origin and further use. Thus the caller is still responsible for
 * closing streams after use.
 * <p>
 * Origin of code: Excalibur.
 *
 * @author Peter Donald
 * @author Jeff Turner
 * @author Matthew Hawthorne
 * @author Stephen Colebourne
 * @author Gareth Davis
 * @author Ian Springer
 * @author Niall Pemberton
 * @author Sandy McArthur
 * @since Apache Tika 0.4, copied (partially) from Commons IO 1.4
 */
public class IOUtils {

    /**
     * The default buffer size to use.
     */
    private static final int DEFAULT_BUFFER_SIZE = 1024 * 4;

    /**
     * Instances should NOT be constructed in standard programming.
     */
    public IOUtils() {
        super();
    }

    //-----------------------------------------------------------------------
    /**
     * Unconditionally close an <code>Reader</code>.
     * <p>
     * Equivalent to {@link Reader#close()}, except any exceptions will be ignored.
     * This is typically used in finally blocks.
     *
     * @param input  the Reader to close, may be null or already closed
     */
    public static void closeQuietly(Reader input) {
        try {
            if (input != null) {
                input.close();
            }
        } catch (IOException ioe) {
            // ignore
        }
    }

    /**
     * Unconditionally close a <code>Channel</code>.
     * <p>
     * Equivalent to {@link Channel#close()}, except any exceptions will be ignored.
     * This is typically used in finally blocks.
     *
     * @param channel the Channel to close, may be null or already closed
     */
    public static void closeQuietly(Channel channel) {
        try {
            if (channel != null) {
                channel.close();
            }
        } catch (IOException ioe) {
            // ignore
        }
    }

    /**
     * Unconditionally close a <code>Writer</code>.
     * <p>
     * Equivalent to {@link Writer#close()}, except any exceptions will be ignored.
     * This is typically used in finally blocks.
     *
     * @param output  the Writer to close, may be null or already closed
     */
    public static void closeQuietly(Writer output) {
        try {
            if (output != null) {
                output.close();
            }
        } catch (IOException ioe) {
            // ignore
        }
    }

    /**
     * Unconditionally close an <code>InputStream</code>.
     * <p>
     * Equivalent to {@link InputStream#close()}, except any exceptions will be ignored.
     * This is typically used in finally blocks.
     *
     * @param input  the InputStream to close, may be null or already closed
     */
    public static void closeQuietly(InputStream input) {
        try {
            if (input != null) {
                input.close();
            }
        } catch (IOException ioe) {
            // ignore
        }
    }

    /**
     * Unconditionally close an <code>OutputStream</code>.
     * <p>
     * Equivalent to {@link OutputStream#close()}, except any exceptions will be ignored.
     * This is typically used in finally blocks.
     *
     * @param output  the OutputStream to close, may be null or already closed
     */
    public static void closeQuietly(OutputStream output) {
        try {
            if (output != null) {
                output.close();
            }
        } catch (IOException ioe) {
            // ignore
        }
    }

    // read toByteArray
    //-----------------------------------------------------------------------
    /**
     * Get the contents of an <code>InputStream</code> as a <code>byte[]</code>.
     * <p>
     * This method buffers the input internally, so there is no need to use a
     * <code>BufferedInputStream</code>.
     * 
     * @param input  the <code>InputStream</code> to read from
     * @return the requested byte array
     * @throws NullPointerException if the input is null
     * @throws IOException if an I/O error occurs
     */
    public static byte[] toByteArray(InputStream input) throws IOException {
        ByteArrayOutputStream output = new ByteArrayOutputStream();
        copy(input, output);
        return output.toByteArray();
    }

    /**
     * Get the contents of a <code>Reader</code> as a <code>byte[]</code>
     * using the default character encoding of the platform.
     * <p>
     * This method buffers the input internally, so there is no need to use a
     * <code>BufferedReader</code>.
     * 
     * @param input  the <code>Reader</code> to read from
     * @return the requested byte array
     * @throws NullPointerException if the input is null
     * @throws IOException if an I/O error occurs
     */
    public static byte[] toByteArray(Reader input) throws IOException {
        ByteArrayOutputStream output = new ByteArrayOutputStream();
        copy(input, output);
        return output.toByteArray();
    }

    /**
     * Get the contents of a <code>Reader</code> as a <code>byte[]</code>
     * using the specified character encoding.
     * <p>
     * Character encoding names can be found at
     * <a href="http://www.iana.org/assignments/character-sets">IANA</a>.
     * <p>
     * This method buffers the input internally, so there is no need to use a
     * <code>BufferedReader</code>.
     * 
     * @param input  the <code>Reader</code> to read from
     * @param encoding  the encoding to use, null means platform default
     * @return the requested byte array
     * @throws NullPointerException if the input is null
     * @throws IOException if an I/O error occurs
     * @since Commons IO 1.1
     */
    public static byte[] toByteArray(Reader input, String encoding)
            throws IOException {
        ByteArrayOutputStream output = new ByteArrayOutputStream();
        copy(input, output, encoding);
        return output.toByteArray();
    }

    /**
     * Get the contents of a <code>String</code> as a <code>byte[]</code>
     * using the default character encoding of the platform.
     * <p>
     * This is the same as {@link String#getBytes()}.
     * 
     * @param input  the <code>String</code> to convert
     * @return the requested byte array
     * @throws NullPointerException if the input is null
     * @throws IOException if an I/O error occurs (never occurs)
     * @deprecated Use {@link String#getBytes()}
     */
    @Deprecated
    public static byte[] toByteArray(String input) throws IOException {
        return input.getBytes("UTF-8");
    }

    // read char[]
    //-----------------------------------------------------------------------
    /**
     * Get the contents of an <code>InputStream</code> as a character array
     * using the default character encoding of the platform.
     * <p>
     * This method buffers the input internally, so there is no need to use a
     * <code>BufferedInputStream</code>.
     * 
     * @param is  the <code>InputStream</code> to read from
     * @return the requested character array
     * @throws NullPointerException if the input is null
     * @throws IOException if an I/O error occurs
     * @since Commons IO 1.1
     */
    public static char[] toCharArray(InputStream is) throws IOException {
        CharArrayWriter output = new CharArrayWriter();
        copy(is, output);
        return output.toCharArray();
    }

    /**
     * Get the contents of an <code>InputStream</code> as a character array
     * using the specified character encoding.
     * <p>
     * Character encoding names can be found at
     * <a href="http://www.iana.org/assignments/character-sets">IANA</a>.
     * <p>
     * This method buffers the input internally, so there is no need to use a
     * <code>BufferedInputStream</code>.
     * 
     * @param is  the <code>InputStream</code> to read from
     * @param encoding  the encoding to use, null means platform default
     * @return the requested character array
     * @throws NullPointerException if the input is null
     * @throws IOException if an I/O error occurs
     * @since Commons IO 1.1
     */
    public static char[] toCharArray(InputStream is, String encoding)
            throws IOException {
        CharArrayWriter output = new CharArrayWriter();
        copy(is, output, encoding);
        return output.toCharArray();
    }

    /**
     * Get the contents of a <code>Reader</code> as a character array.
     * <p>
     * This method buffers the input internally, so there is no need to use a
     * <code>BufferedReader</code>.
     * 
     * @param input  the <code>Reader</code> to read from
     * @return the requested character array
     * @throws NullPointerException if the input is null
     * @throws IOException if an I/O error occurs
     * @since Commons IO 1.1
     */
    public static char[] toCharArray(Reader input) throws IOException {
        CharArrayWriter sw = new CharArrayWriter();
        copy(input, sw);
        return sw.toCharArray();
    }

    // read toString
    //-----------------------------------------------------------------------
    /**
     * Get the contents of an <code>InputStream</code> as a String
     * using the default character encoding of the platform.
     * <p>
     * This method buffers the input internally, so there is no need to use a
     * <code>BufferedInputStream</code>.
     * 
     * @param input  the <code>InputStream</code> to read from
     * @return the requested String
     * @throws NullPointerException if the input is null
     * @throws IOException if an I/O error occurs
     */
    public static String toString(InputStream input) throws IOException {
        StringWriter sw = new StringWriter();
        copy(input, sw);
        return sw.toString();
    }

    /**
     * Get the contents of an <code>InputStream</code> as a String
     * using the specified character encoding.
     * <p>
     * Character encoding names can be found at
     * <a href="http://www.iana.org/assignments/character-sets">IANA</a>.
     * <p>
     * This method buffers the input internally, so there is no need to use a
     * <code>BufferedInputStream</code>.
     * 
     * @param input  the <code>InputStream</code> to read from
     * @param encoding  the encoding to use, null means platform default
     * @return the requested String
     * @throws NullPointerException if the input is null
     * @throws IOException if an I/O error occurs
     */
    public static String toString(InputStream input, String encoding)
            throws IOException {
        StringWriter sw = new StringWriter();
        copy(input, sw, encoding);
        return sw.toString();
    }

    /**
     * Get the contents of a <code>Reader</code> as a String.
     * <p>
     * This method buffers the input internally, so there is no need to use a
     * <code>BufferedReader</code>.
     * 
     * @param input  the <code>Reader</code> to read from
     * @return the requested String
     * @throws NullPointerException if the input is null
     * @throws IOException if an I/O error occurs
     */
    public static String toString(Reader input) throws IOException {
        StringWriter sw = new StringWriter();
        copy(input, sw);
        return sw.toString();
    }

    /**
     * Get the contents of a <code>byte[]</code> as a String
     * using the default character encoding of the platform.
     * 
     * @param input the byte array to read from
     * @return the requested String
     * @throws NullPointerException if the input is null
     * @throws IOException if an I/O error occurs (never occurs)
     * @deprecated Use {@link String#String(byte[])}
     */
    @Deprecated
    public static String toString(byte[] input) throws IOException {
        return new String(input, "UTF-8");
    }

    /**
     * Get the contents of a <code>byte[]</code> as a String
     * using the specified character encoding.
     * <p>
     * Character encoding names can be found at
     * <a href="http://www.iana.org/assignments/character-sets">IANA</a>.
     * 
     * @param input the byte array to read from
     * @param encoding  the encoding to use, null means platform default
     * @return the requested String
     * @throws NullPointerException if the input is null
     * @throws IOException if an I/O error occurs (never occurs)
     * @deprecated Use {@link String#String(byte[],String)}
     */
    @Deprecated
    public static String toString(byte[] input, String encoding)
            throws IOException {
        // If no encoding is specified, default to UTF-8.
        if (encoding == null) {
            return new String(input, "UTF-8");
        } else {
            return new String(input, encoding);
        }
    }

    // readLines
    //-----------------------------------------------------------------------
    /**
     * Get the contents of an <code>InputStream</code> as a list of Strings,
     * one entry per line, using the default character encoding of the platform.
     * <p>
     * This method buffers the input internally, so there is no need to use a
     * <code>BufferedInputStream</code>.
     *
     * @param input  the <code>InputStream</code> to read from, not null
     * @return the list of Strings, never null
     * @throws NullPointerException if the input is null
     * @throws IOException if an I/O error occurs
     * @since Commons IO 1.1
     */
    public static List<String> readLines(InputStream input) throws IOException {
        InputStreamReader reader = new InputStreamReader(input, "UTF-8");
        return readLines(reader);
    }

    /**
     * Get the contents of an <code>InputStream</code> as a list of Strings,
     * one entry per line, using the specified character encoding.
     * <p>
     * Character encoding names can be found at
     * <a href="http://www.iana.org/assignments/character-sets">IANA</a>.
     * <p>
     * This method buffers the input internally, so there is no need to use a
     * <code>BufferedInputStream</code>.
     *
     * @param input  the <code>InputStream</code> to read from, not null
     * @param encoding  the encoding to use, null means platform default
     * @return the list of Strings, never null
     * @throws NullPointerException if the input is null
     * @throws IOException if an I/O error occurs
     * @since Commons IO 1.1
     */
    public static List<String> readLines(InputStream input, String encoding) throws IOException {
        if (encoding == null) {
            return readLines(input);
        } else {
            InputStreamReader reader = new InputStreamReader(input, encoding);
            return readLines(reader);
        }
    }

    /**
     * Get the contents of a <code>Reader</code> as a list of Strings,
     * one entry per line.
     * <p>
     * This method buffers the input internally, so there is no need to use a
     * <code>BufferedReader</code>.
     *
     * @param input  the <code>Reader</code> to read from, not null
     * @return the list of Strings, never null
     * @throws NullPointerException if the input is null
     * @throws IOException if an I/O error occurs
     * @since Commons IO 1.1
     */
    public static List<String> readLines(Reader input) throws IOException {
        BufferedReader reader = new BufferedReader(input);
        List<String> list = new ArrayList<String>();
        String line = reader.readLine();
        while (line != null) {
            list.add(line);
            line = reader.readLine();
        }
        return list;
    }

    //-----------------------------------------------------------------------
    /**
     * Convert the specified CharSequence to an input stream, encoded as bytes
     * using the default character encoding of the platform.
     *
     * @param input the CharSequence to convert
     * @return an input stream
     * @since IO 2.0
     */
    public static InputStream toInputStream(CharSequence input) {
        return toInputStream(input.toString());
    }

    /**
     * Convert the specified CharSequence to an input stream, encoded as bytes
     * using the specified character encoding.
     * <p>
     * Character encoding names can be found at
     * <a href="http://www.iana.org/assignments/character-sets">IANA</a>.
     *
     * @param input the CharSequence to convert
     * @param encoding the encoding to use, null means platform default
     * @throws IOException if the encoding is invalid
     * @return an input stream
     * @since IO 2.0
     */
    public static InputStream toInputStream(CharSequence input, String encoding) throws IOException {
        return toInputStream(input.toString(), encoding);
    }

    //-----------------------------------------------------------------------
    /**
     * Convert the specified string to an input stream, encoded as bytes
     * using the default character encoding of the platform.
     *
     * @param input the string to convert
     * @return an input stream
     * @since Commons IO 1.1
     */
    public static InputStream toInputStream(String input) {
        try {
            byte[] bytes = input.getBytes("UTF-8");
            return new ByteArrayInputStream(bytes);
        } catch (UnsupportedEncodingException e) {
            throw new AssertionError("UTF-8 not supported.");
        }

    }

    /**
     * Convert the specified string to an input stream, encoded as bytes
     * using the specified character encoding.
     * <p>
     * Character encoding names can be found at
     * <a href="http://www.iana.org/assignments/character-sets">IANA</a>.
     *
     * @param input the string to convert
     * @param encoding the encoding to use, null means platform default
     * @throws IOException if the encoding is invalid
     * @return an input stream
     * @since Commons IO 1.1
     */
    public static InputStream toInputStream(String input, String encoding) throws IOException {
        byte[] bytes = encoding != null ? input.getBytes(encoding) : input.getBytes("UTF-8");
        return new ByteArrayInputStream(bytes);
    }

    // write byte[]
    //-----------------------------------------------------------------------
    /**
     * Writes bytes from a <code>byte[]</code> to an <code>OutputStream</code>.
     * 
     * @param data  the byte array to write, do not modify during output,
     * null ignored
     * @param output  the <code>OutputStream</code> to write to
     * @throws NullPointerException if output is null
     * @throws IOException if an I/O error occurs
     * @since Commons IO 1.1
     */
    public static void write(byte[] data, OutputStream output)
            throws IOException {
        if (data != null) {
            output.write(data);
        }
    }

    /**
     * Writes bytes from a <code>byte[]</code> to chars on a <code>Writer</code>
     * using the default character encoding of the platform.
     * <p>
     * This method uses {@link String#String(byte[])}.
     * 
     * @param data  the byte array to write, do not modify during output,
     * null ignored
     * @param output  the <code>Writer</code> to write to
     * @throws NullPointerException if output is null
     * @throws IOException if an I/O error occurs
     * @since Commons IO 1.1
     */
    public static void write(byte[] data, Writer output) throws IOException {
        if (data != null) {
            output.write(new String(data, "UTF-8"));
        }
    }

    /**
     * Writes bytes from a <code>byte[]</code> to chars on a <code>Writer</code>
     * using the specified character encoding.
     * <p>
     * Character encoding names can be found at
     * <a href="http://www.iana.org/assignments/character-sets">IANA</a>.
     * <p>
     * This method uses {@link String#String(byte[], String)}.
     * 
     * @param data  the byte array to write, do not modify during output,
     * null ignored
     * @param output  the <code>Writer</code> to write to
     * @param encoding  the encoding to use, null means platform default
     * @throws NullPointerException if output is null
     * @throws IOException if an I/O error occurs
     * @since Commons IO 1.1
     */
    public static void write(byte[] data, Writer output, String encoding)
            throws IOException {
        if (data != null) {
            if (encoding == null) {
                write(data, output);
            } else {
                output.write(new String(data, encoding));
            }
        }
    }

    // write char[]
    //-----------------------------------------------------------------------
    /**
     * Writes chars from a <code>char[]</code> to a <code>Writer</code>
     * using the default character encoding of the platform.
     * 
     * @param data  the char array to write, do not modify during output,
     * null ignored
     * @param output  the <code>Writer</code> to write to
     * @throws NullPointerException if output is null
     * @throws IOException if an I/O error occurs
     * @since Commons IO 1.1
     */
    public static void write(char[] data, Writer output) throws IOException {
        if (data != null) {
            output.write(data);
        }
    }

    /**
     * Writes chars from a <code>char[]</code> to bytes on an
     * <code>OutputStream</code>.
     * <p>
     * This method uses {@link String#String(char[])} and
     * {@link String#getBytes()}.
     * 
     * @param data  the char array to write, do not modify during output,
     * null ignored
     * @param output  the <code>OutputStream</code> to write to
     * @throws NullPointerException if output is null
     * @throws IOException if an I/O error occurs
     * @since Commons IO 1.1
     */
    public static void write(char[] data, OutputStream output)
            throws IOException {
        if (data != null) {
            output.write(new String(data).getBytes("UTF-8"));
        }
    }

    /**
     * Writes chars from a <code>char[]</code> to bytes on an
     * <code>OutputStream</code> using the specified character encoding.
     * <p>
     * Character encoding names can be found at
     * <a href="http://www.iana.org/assignments/character-sets">IANA</a>.
     * <p>
     * This method uses {@link String#String(char[])} and
     * {@link String#getBytes(String)}.
     * 
     * @param data  the char array to write, do not modify during output,
     * null ignored
     * @param output  the <code>OutputStream</code> to write to
     * @param encoding  the encoding to use, null means platform default
     * @throws NullPointerException if output is null
     * @throws IOException if an I/O error occurs
     * @since Commons IO 1.1
     */
    public static void write(char[] data, OutputStream output, String encoding)
            throws IOException {
        if (data != null) {
            if (encoding == null) {
                write(data, output);
            } else {
                output.write(new String(data).getBytes(encoding));
            }
        }
    }

    // write CharSequence
    //-----------------------------------------------------------------------
    /**
     * Writes chars from a <code>CharSequence</code> to a <code>Writer</code>.
     * 
     * @param data  the <code>CharSequence</code> to write, null ignored
     * @param output  the <code>Writer</code> to write to
     * @throws NullPointerException if output is null
     * @throws IOException if an I/O error occurs
     * @since Commons IO 2.0
     */
    public static void write(CharSequence data, Writer output) throws IOException {
        if (data != null) {
            write(data.toString(), output);
        }
    }

    /**
     * Writes chars from a <code>CharSequence</code> to bytes on an
     * <code>OutputStream</code> using the default character encoding of the
     * platform.
     * <p>
     * This method uses {@link String#getBytes()}.
     * 
     * @param data  the <code>CharSequence</code> to write, null ignored
     * @param output  the <code>OutputStream</code> to write to
     * @throws NullPointerException if output is null
     * @throws IOException if an I/O error occurs
     * @since Commons IO 2.0
     */
    public static void write(CharSequence data, OutputStream output)
            throws IOException {
        if (data != null) {
            write(data.toString(), output);
        }
    }

    /**
     * Writes chars from a <code>CharSequence</code> to bytes on an
     * <code>OutputStream</code> using the specified character encoding.
     * <p>
     * Character encoding names can be found at
     * <a href="http://www.iana.org/assignments/character-sets">IANA</a>.
     * <p>
     * This method uses {@link String#getBytes(String)}.
     * 
     * @param data  the <code>CharSequence</code> to write, null ignored
     * @param output  the <code>OutputStream</code> to write to
     * @param encoding  the encoding to use, null means platform default
     * @throws NullPointerException if output is null
     * @throws IOException if an I/O error occurs
     * @since Commons IO 2.0
     */
    public static void write(CharSequence data, OutputStream output, String encoding)
            throws IOException {
        if (data != null) {
            write(data.toString(), output, encoding);
        }
    }

    // write String
    //-----------------------------------------------------------------------
    /**
     * Writes chars from a <code>String</code> to a <code>Writer</code>.
     * 
     * @param data  the <code>String</code> to write, null ignored
     * @param output  the <code>Writer</code> to write to
     * @throws NullPointerException if output is null
     * @throws IOException if an I/O error occurs
     * @since Commons IO 1.1
     */
    public static void write(String data, Writer output) throws IOException {
        if (data != null) {
            output.write(data);
        }
    }

    /**
     * Writes chars from a <code>String</code> to bytes on an
     * <code>OutputStream</code> using the default character encoding of the
     * platform.
     * <p>
     * This method uses {@link String#getBytes()}.
     * 
     * @param data  the <code>String</code> to write, null ignored
     * @param output  the <code>OutputStream</code> to write to
     * @throws NullPointerException if output is null
     * @throws IOException if an I/O error occurs
     * @since Commons IO 1.1
     */
    public static void write(String data, OutputStream output)
            throws IOException {
        if (data != null) {
            output.write(data.getBytes("UTF-8"));
        }
    }

    /**
     * Writes chars from a <code>String</code> to bytes on an
     * <code>OutputStream</code> using the specified character encoding.
     * <p>
     * Character encoding names can be found at
     * <a href="http://www.iana.org/assignments/character-sets">IANA</a>.
     * <p>
     * This method uses {@link String#getBytes(String)}.
     * 
     * @param data  the <code>String</code> to write, null ignored
     * @param output  the <code>OutputStream</code> to write to
     * @param encoding  the encoding to use, null means platform default
     * @throws NullPointerException if output is null
     * @throws IOException if an I/O error occurs
     * @since Commons IO 1.1
     */
    public static void write(String data, OutputStream output, String encoding)
            throws IOException {
        if (data != null) {
            if (encoding == null) {
                write(data, output);
            } else {
                output.write(data.getBytes(encoding));
            }
        }
    }

    // write StringBuffer
    //-----------------------------------------------------------------------
    /**
     * Writes chars from a <code>StringBuffer</code> to a <code>Writer</code>.
     * 
     * @param data  the <code>StringBuffer</code> to write, null ignored
     * @param output  the <code>Writer</code> to write to
     * @throws NullPointerException if output is null
     * @throws IOException if an I/O error occurs
     * @since Commons IO 1.1
     * @deprecated replaced by write(CharSequence, Writer)
     */
    @Deprecated
    public static void write(StringBuffer data, Writer output)
            throws IOException {
        if (data != null) {
            output.write(data.toString());
        }
    }

    /**
     * Writes chars from a <code>StringBuffer</code> to bytes on an
     * <code>OutputStream</code> using the default character encoding of the
     * platform.
     * <p>
     * This method uses {@link String#getBytes()}.
     * 
     * @param data  the <code>StringBuffer</code> to write, null ignored
     * @param output  the <code>OutputStream</code> to write to
     * @throws NullPointerException if output is null
     * @throws IOException if an I/O error occurs
     * @since Commons IO 1.1
     * @deprecated replaced by write(CharSequence, OutputStream)
     */
    @Deprecated
    public static void write(StringBuffer data, OutputStream output)
            throws IOException {
        if (data != null) {
            output.write(data.toString().getBytes("UTF-8"));
        }
    }

    /**
     * Writes chars from a <code>StringBuffer</code> to bytes on an
     * <code>OutputStream</code> using the specified character encoding.
     * <p>
     * Character encoding names can be found at
     * <a href="http://www.iana.org/assignments/character-sets">IANA</a>.
     * <p>
     * This method uses {@link String#getBytes(String)}.
     * 
     * @param data  the <code>StringBuffer</code> to write, null ignored
     * @param output  the <code>OutputStream</code> to write to
     * @param encoding  the encoding to use, null means platform default
     * @throws NullPointerException if output is null
     * @throws IOException if an I/O error occurs
     * @since Commons IO 1.1
     * @deprecated replaced by write(CharSequence, OutputStream, String)
     */
    @Deprecated
    public static void write(StringBuffer data, OutputStream output,
            String encoding) throws IOException {
        if (data != null) {
            if (encoding == null) {
                write(data, output);
            } else {
                output.write(data.toString().getBytes(encoding));
            }
        }
    }

    // copy from InputStream
    //-----------------------------------------------------------------------
    /**
     * Copy bytes from an <code>InputStream</code> to an
     * <code>OutputStream</code>.
     * <p>
     * This method buffers the input internally, so there is no need to use a
     * <code>BufferedInputStream</code>.
     * <p>
     * Large streams (over 2GB) will return a bytes copied value of
     * <code>-1</code> after the copy has completed since the correct
     * number of bytes cannot be returned as an int. For large streams
     * use the <code>copyLarge(InputStream, OutputStream)</code> method.
     * 
     * @param input  the <code>InputStream</code> to read from
     * @param output  the <code>OutputStream</code> to write to
     * @return the number of bytes copied
     * @throws NullPointerException if the input or output is null
     * @throws IOException if an I/O error occurs
     * @throws ArithmeticException if the byte count is too large
     * @since Commons IO 1.1
     */
    public static int copy(InputStream input, OutputStream output) throws IOException {
        long count = copyLarge(input, output);
        if (count > Integer.MAX_VALUE) {
            return -1;
        }
        return (int) count;
    }

    /**
     * Copy bytes from a large (over 2GB) <code>InputStream</code> to an
     * <code>OutputStream</code>.
     * <p>
     * This method buffers the input internally, so there is no need to use a
     * <code>BufferedInputStream</code>.
     * 
     * @param input  the <code>InputStream</code> to read from
     * @param output  the <code>OutputStream</code> to write to
     * @return the number of bytes copied
     * @throws NullPointerException if the input or output is null
     * @throws IOException if an I/O error occurs
     * @since Commons IO 1.3
     */
    public static long copyLarge(InputStream input, OutputStream output)
            throws IOException {
        byte[] buffer = new byte[DEFAULT_BUFFER_SIZE];
        long count = 0;
        int n = 0;
        while (-1 != (n = input.read(buffer))) {
            output.write(buffer, 0, n);
            count += n;
        }
        return count;
    }

    /**
     * Copy bytes from an <code>InputStream</code> to chars on a
     * <code>Writer</code> using the default character encoding of the platform.
     * <p>
     * This method buffers the input internally, so there is no need to use a
     * <code>BufferedInputStream</code>.
     * <p>
     * This method uses {@link InputStreamReader}.
     *
     * @param input  the <code>InputStream</code> to read from
     * @param output  the <code>Writer</code> to write to
     * @throws NullPointerException if the input or output is null
     * @throws IOException if an I/O error occurs
     * @since Commons IO 1.1
     */
    public static void copy(InputStream input, Writer output)
            throws IOException {
        InputStreamReader in = new InputStreamReader(input, "UTF-8");
        copy(in, output);
    }

    /**
     * Copy bytes from an <code>InputStream</code> to chars on a
     * <code>Writer</code> using the specified character encoding.
     * <p>
     * This method buffers the input internally, so there is no need to use a
     * <code>BufferedInputStream</code>.
     * <p>
     * Character encoding names can be found at
     * <a href="http://www.iana.org/assignments/character-sets">IANA</a>.
     * <p>
     * This method uses {@link InputStreamReader}.
     *
     * @param input  the <code>InputStream</code> to read from
     * @param output  the <code>Writer</code> to write to
     * @param encoding  the encoding to use, null means platform default
     * @throws NullPointerException if the input or output is null
     * @throws IOException if an I/O error occurs
     * @since Commons IO 1.1
     */
    public static void copy(InputStream input, Writer output, String encoding)
            throws IOException {
        if (encoding == null) {
            copy(input, output);
        } else {
            InputStreamReader in = new InputStreamReader(input, encoding);
            copy(in, output);
        }
    }

    // copy from Reader
    //-----------------------------------------------------------------------
    /**
     * Copy chars from a <code>Reader</code> to a <code>Writer</code>.
     * <p>
     * This method buffers the input internally, so there is no need to use a
     * <code>BufferedReader</code>.
     * <p>
     * Large streams (over 2GB) will return a chars copied value of
     * <code>-1</code> after the copy has completed since the correct
     * number of chars cannot be returned as an int. For large streams
     * use the <code>copyLarge(Reader, Writer)</code> method.
     *
     * @param input  the <code>Reader</code> to read from
     * @param output  the <code>Writer</code> to write to
     * @return the number of characters copied
     * @throws NullPointerException if the input or output is null
     * @throws IOException if an I/O error occurs
     * @throws ArithmeticException if the character count is too large
     * @since Commons IO 1.1
     */
    public static int copy(Reader input, Writer output) throws IOException {
        long count = copyLarge(input, output);
        if (count > Integer.MAX_VALUE) {
            return -1;
        }
        return (int) count;
    }

    /**
     * Copy chars from a large (over 2GB) <code>Reader</code> to a <code>Writer</code>.
     * <p>
     * This method buffers the input internally, so there is no need to use a
     * <code>BufferedReader</code>.
     *
     * @param input  the <code>Reader</code> to read from
     * @param output  the <code>Writer</code> to write to
     * @return the number of characters copied
     * @throws NullPointerException if the input or output is null
     * @throws IOException if an I/O error occurs
     * @since Commons IO 1.3
     */
    public static long copyLarge(Reader input, Writer output) throws IOException {
        char[] buffer = new char[DEFAULT_BUFFER_SIZE];
        long count = 0;
        int n = 0;
        while (-1 != (n = input.read(buffer))) {
            output.write(buffer, 0, n);
            count += n;
        }
        return count;
    }

    /**
     * Copy chars from a <code>Reader</code> to bytes on an
     * <code>OutputStream</code> using the default character encoding of the
     * platform, and calling flush.
     * <p>
     * This method buffers the input internally, so there is no need to use a
     * <code>BufferedReader</code>.
     * <p>
     * Due to the implementation of OutputStreamWriter, this method performs a
     * flush.
     * <p>
     * This method uses {@link OutputStreamWriter}.
     *
     * @param input  the <code>Reader</code> to read from
     * @param output  the <code>OutputStream</code> to write to
     * @throws NullPointerException if the input or output is null
     * @throws IOException if an I/O error occurs
     * @since Commons IO 1.1
     */
    public static void copy(Reader input, OutputStream output)
            throws IOException {
        OutputStreamWriter out = new OutputStreamWriter(output, "UTF-8");
        copy(input, out);
        // XXX Unless anyone is planning on rewriting OutputStreamWriter, we
        // have to flush here.
        out.flush();
    }

    /**
     * Copy chars from a <code>Reader</code> to bytes on an
     * <code>OutputStream</code> using the specified character encoding, and
     * calling flush.
     * <p>
     * This method buffers the input internally, so there is no need to use a
     * <code>BufferedReader</code>.
     * <p>
     * Character encoding names can be found at
     * <a href="http://www.iana.org/assignments/character-sets">IANA</a>.
     * <p>
     * Due to the implementation of OutputStreamWriter, this method performs a
     * flush.
     * <p>
     * This method uses {@link OutputStreamWriter}.
     *
     * @param input  the <code>Reader</code> to read from
     * @param output  the <code>OutputStream</code> to write to
     * @param encoding  the encoding to use, null means platform default
     * @throws NullPointerException if the input or output is null
     * @throws IOException if an I/O error occurs
     * @since Commons IO 1.1
     */
    public static void copy(Reader input, OutputStream output, String encoding)
            throws IOException {
        if (encoding == null) {
            copy(input, output);
        } else {
            OutputStreamWriter out = new OutputStreamWriter(output, encoding);
            copy(input, out);
            // XXX Unless anyone is planning on rewriting OutputStreamWriter,
            // we have to flush here.
            out.flush();
        }
    }

    // content equals
    //-----------------------------------------------------------------------
    /**
     * Compare the contents of two Streams to determine if they are equal or
     * not.
     * <p>
     * This method buffers the input internally using
     * <code>BufferedInputStream</code> if they are not already buffered.
     *
     * @param input1  the first stream
     * @param input2  the second stream
     * @return true if the content of the streams are equal or they both don't
     * exist, false otherwise
     * @throws NullPointerException if either input is null
     * @throws IOException if an I/O error occurs
     */
    public static boolean contentEquals(InputStream input1, InputStream input2)
            throws IOException {
        if (!(input1 instanceof BufferedInputStream)) {
            input1 = new BufferedInputStream(input1);
        }
        if (!(input2 instanceof BufferedInputStream)) {
            input2 = new BufferedInputStream(input2);
        }

        int ch = input1.read();
        while (-1 != ch) {
            int ch2 = input2.read();
            if (ch != ch2) {
                return false;
            }
            ch = input1.read();
        }

        int ch2 = input2.read();
        return (ch2 == -1);
    }

    /**
     * Compare the contents of two Readers to determine if they are equal or
     * not.
     * <p>
     * This method buffers the input internally using
     * <code>BufferedReader</code> if they are not already buffered.
     *
     * @param input1  the first reader
     * @param input2  the second reader
     * @return true if the content of the readers are equal or they both don't
     * exist, false otherwise
     * @throws NullPointerException if either input is null
     * @throws IOException if an I/O error occurs
     * @since Commons IO 1.1
     */
    public static boolean contentEquals(Reader input1, Reader input2)
            throws IOException {
        if (!(input1 instanceof BufferedReader)) {
            input1 = new BufferedReader(input1);
        }
        if (!(input2 instanceof BufferedReader)) {
            input2 = new BufferedReader(input2);
        }

        int ch = input1.read();
        while (-1 != ch) {
            int ch2 = input2.read();
            if (ch != ch2) {
                return false;
            }
            ch = input1.read();
        }

        int ch2 = input2.read();
        return (ch2 == -1);
    }

}
"
tika-core/src/main/java/org/apache/tika/io/LookaheadInputStream.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.io;

import java.io.IOException;
import java.io.InputStream;

/**
 * Stream wrapper that make it easy to read up to n bytes ahead from
 * a stream that supports the mark feature. This class insulates the
 * underlying stream from things like possible mark(), reset() and close()
 * calls by external components that might otherwise invalidate the marked
 * state of a stream.
 * <p>
 * The recommended usage pattern of this class is:
 * <pre>
 *     InputStream lookahead = new LookaheadInputStream(stream, n);
 *     try {
 *         processStream(lookahead);
 *     } finally {
 *         lookahead.close();
 *     }
 * </pre>
 * <p>
 * This usage pattern guarantees that only up to n bytes from the original
 * stream can ever be read, and that the stream will have been marked and
 * then reset to its original state once the above code block exits. No
 * code in the fictional processStream() method can affect the the state of
 * the original stream.
 *
 * @since Apache Tika 0.10
 */
public class LookaheadInputStream extends InputStream {

    private InputStream stream;

    private final byte[] buffer;

    private int buffered = 0;

    private int position = 0;

    private int mark = 0;

    /**
     * Creates a lookahead wrapper for the given input stream.
     * The given input stream should support the mark feature,
     * as otherwise the state of that stream will be undefined
     * after the lookahead wrapper has been closed. As a special
     * case a <code>null</code> stream is treated as an empty stream.
     *
     * @param stream input stream, can be <code>null</code>
     * @param n maximum number of bytes to look ahead
     */
    public LookaheadInputStream(InputStream stream, int n) {
        this.stream = stream;
        this.buffer = new byte[n];
        if (stream != null) {
            stream.mark(n);
        }
    }

    @Override
    public void close() throws IOException {
        if (stream != null) {
            stream.reset();
            stream = null;
        }
    }

    private void fill() throws IOException {
        if (available() == 0 && buffered < buffer.length && stream != null) {
            int n = stream.read(buffer, buffered, buffer.length - buffered);
            if (n != -1) {
                buffered += n;
            } else {
                close();
            }
        }
    }

    @Override
    public int read() throws IOException {
        fill();
        if (buffered > position) {
            return 0xff & buffer[position++];
        } else {
            return -1;
        }
    }

    @Override
    public int read(byte[] b, int off, int len) throws IOException {
        fill();
        if (buffered > position) {
            len = Math.min(len, buffered - position);
            System.arraycopy(buffer, position, b, off, len);
            position += len;
            return len;
        } else {
            return -1;
        }
    }

    @Override
    public long skip(long n) throws IOException {
        fill();
        n = Math.min(n, available());
        position += n;
        return n;
    }

    @Override
    public int available() {
        return buffered - position;
    }

    @Override
    public boolean markSupported() {
        return true;
    }

    @Override
    public synchronized void mark(int readlimit) {
        mark = position;
    }

    @Override
    public synchronized void reset() {
        position = mark;
    }

}
"
tika-core/src/main/java/org/apache/tika/io/NullInputStream.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.io;

import java.io.EOFException;
import java.io.IOException;
import java.io.InputStream;

/**
 * A functional, light weight {@link InputStream} that emulates
 * a stream of a specified size.
 * <p>
 * This implementation provides a light weight
 * object for testing with an {@link InputStream}
 * where the contents don't matter.
 * <p>
 * One use case would be for testing the handling of
 * large {@link InputStream} as it can emulate that
 * scenario without the overhead of actually processing
 * large numbers of bytes - significantly speeding up
 * test execution times.
 * <p>
 * This implementation returns zero from the method that
 * reads a byte and leaves the array unchanged in the read
 * methods that are passed a byte array.
 * If alternative data is required the <code>processByte()</code> and
 * <code>processBytes()</code> methods can be implemented to generate
 * data, for example:
 *
 * <pre>
 *  public class TestInputStream extends NullInputStream {
 *      public TestInputStream(int size) {
 *          super(size);
 *      }
 *      protected int processByte() {
 *          return ... // return required value here
 *      }
 *      protected void processBytes(byte[] bytes, int offset, int length) {
 *          for (int i = offset; i < length; i++) {
 *              bytes[i] = ... // set array value here
 *          }
 *      }
 *  }
 * </pre>
 *
 * @since Apache Tika 0.4, copied from Commons IO 1.4
 */
public class NullInputStream extends InputStream {

    private final long size;
    private long position;
    private long mark = -1;
    private long readlimit;
    private boolean eof;
    private final boolean throwEofException;
    private final boolean markSupported;

    /**
     * Create an {@link InputStream} that emulates a specified size
     * which supports marking and does not throw EOFException.
     *
     * @param size The size of the input stream to emulate.
     */
    public NullInputStream(long size) {
       this(size, true, false);
    }

    /**
     * Create an {@link InputStream} that emulates a specified
     * size with option settings.
     *
     * @param size The size of the input stream to emulate.
     * @param markSupported Whether this instance will support
     * the <code>mark()</code> functionality.
     * @param throwEofException Whether this implementation
     * will throw an {@link EOFException} or return -1 when the
     * end of file is reached.
     */
    public NullInputStream(long size, boolean markSupported, boolean throwEofException) {
       this.size = size;
       this.markSupported = markSupported;
       this.throwEofException = throwEofException;
    }

    /**
     * Return the current position.
     *
     * @return the current position.
     */
    public long getPosition() {
        return position;
    }

    /**
     * Return the size this {@link InputStream} emulates.
     *
     * @return The size of the input stream to emulate.
     */
    public long getSize() {
        return size;
    }

    /**
     * Return the number of bytes that can be read.
     *
     * @return The number of bytes that can be read.
     */
    @Override
    public int available() {
        long avail = size - position;
        if (avail <= 0) {
            return 0;
        } else if (avail > Integer.MAX_VALUE) {
            return Integer.MAX_VALUE;
        } else {
            return (int)avail;
        }
    }

    /**
     * Close this input stream - resets the internal state to
     * the initial values.
     *
     * @throws IOException If an error occurs.
     */
    @Override
    public void close() throws IOException {
        eof = false;
        position = 0;
        mark = -1;
    }

    /**
     * Mark the current position.
     *
     * @param readlimit The number of bytes before this marked position
     * is invalid.
     * @throws UnsupportedOperationException if mark is not supported.
     */
    @Override
    public synchronized void mark(int readlimit) {
        if (!markSupported) {
            throw new UnsupportedOperationException("Mark not supported");
        }
        mark = position;
        this.readlimit = readlimit;
    }

    /**
     * Indicates whether <i>mark</i> is supported.
     *
     * @return Whether <i>mark</i> is supported or not.
     */
    @Override
    public boolean markSupported() {
        return markSupported;
    }

    /**
     * Read a byte.
     *
     * @return Either The byte value returned by <code>processByte()</code>
     * or <code>-1</code> if the end of file has been reached and
     * <code>throwEofException</code> is set to <code>false</code>.
     * @throws EOFException if the end of file is reached and
     * <code>throwEofException</code> is set to <code>true</code>.
     * @throws IOException if trying to read past the end of file.
     */
    @Override
    public int read() throws IOException {
        if (eof) {
            throw new IOException("Read after end of file");
        }
        if (position == size) {
            return doEndOfFile();
        }
        position++;
        return processByte();
    }

    /**
     * Read some bytes into the specified array.
     *
     * @param bytes The byte array to read into
     * @return The number of bytes read or <code>-1</code>
     * if the end of file has been reached and
     * <code>throwEofException</code> is set to <code>false</code>.
     * @throws EOFException if the end of file is reached and
     * <code>throwEofException</code> is set to <code>true</code>.
     * @throws IOException if trying to read past the end of file.
     */
    @Override
    public int read(byte[] bytes) throws IOException {
        return read(bytes, 0, bytes.length);
    }

    /**
     * Read the specified number bytes into an array.
     *
     * @param bytes The byte array to read into.
     * @param offset The offset to start reading bytes into.
     * @param length The number of bytes to read.
     * @return The number of bytes read or <code>-1</code>
     * if the end of file has been reached and
     * <code>throwEofException</code> is set to <code>false</code>.
     * @throws EOFException if the end of file is reached and
     * <code>throwEofException</code> is set to <code>true</code>.
     * @throws IOException if trying to read past the end of file.
     */
    @Override
    public int read(byte[] bytes, int offset, int length) throws IOException {
        if (eof) {
            throw new IOException("Read after end of file");
        }
        if (position == size) {
            return doEndOfFile();
        }
        position += length;
        int returnLength = length;
        if (position > size) {
            returnLength = length - (int)(position - size);
            position = size;
        }
        processBytes(bytes, offset, returnLength);
        return returnLength;
    }

    /**
     * Reset the stream to the point when mark was last called.
     *
     * @throws UnsupportedOperationException if mark is not supported.
     * @throws IOException If no position has been marked
     * or the read limit has been exceed since the last position was
     * marked.
     */
    @Override
    public synchronized void reset() throws IOException {
        if (!markSupported) {
            throw new UnsupportedOperationException("Mark not supported");
        }
        if (mark < 0) {
            throw new IOException("No position has been marked");
        }
        if (position > (mark + readlimit)) {
            throw new IOException("Marked position [" + mark +
                    "] is no longer valid - passed the read limit [" +
                    readlimit + "]");
        }
        position = mark;
        eof = false;
    }

    /**
     * Skip a specified number of bytes.
     *
     * @param numberOfBytes The number of bytes to skip.
     * @return The number of bytes skipped or <code>-1</code>
     * if the end of file has been reached and
     * <code>throwEofException</code> is set to <code>false</code>.
     * @throws EOFException if the end of file is reached and
     * <code>throwEofException</code> is set to <code>true</code>.
     * @throws IOException if trying to read past the end of file.
     */
    @Override
    public long skip(long numberOfBytes) throws IOException {
        if (eof) {
            throw new IOException("Skip after end of file");
        }
        if (position == size) {
            return doEndOfFile();
        }
        position += numberOfBytes;
        long returnLength = numberOfBytes;
        if (position > size) {
            returnLength = numberOfBytes - (position - size);
            position = size;
        }
        return returnLength;
    }

    /**
     * Return a byte value for the  <code>read()</code> method.
     * <p>
     * This implementation returns zero.
     *
     * @return This implementation always returns zero.
     */
    protected int processByte() {
        // do nothing - overridable by subclass
        return 0;
    }

    /**
     * Process the bytes for the <code>read(byte[], offset, length)</code>
     * method.
     * <p>
     * This implementation leaves the byte array unchanged.
     *
     * @param bytes The byte array
     * @param offset The offset to start at.
     * @param length The number of bytes.
     */
    protected void processBytes(byte[] bytes, int offset, int length) {
        // do nothing - overridable by subclass
    }

    /**
     * Handle End of File.
     *
     * @return <code>-1</code> if <code>throwEofException</code> is
     * set to <code>false</code>
     * @throws EOFException if <code>throwEofException</code> is set
     * to <code>true</code>.
     */
    private int doEndOfFile() throws EOFException {
        eof = true;
        if (throwEofException) {
            throw new EOFException();
        }
        return -1;
    }

}
"
tika-core/src/main/java/org/apache/tika/io/NullOutputStream.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.io;
 
import java.io.IOException;
import java.io.OutputStream;

/**
 * This OutputStream writes all data to the famous <b>/dev/null</b>.
 * <p>
 * This output stream has no destination (file/socket etc.) and all
 * bytes written to it are ignored and lost.
 * 
 * @author Jeremias Maerki
 * @since Apache Tika 0.4, copied from Commons IO 1.4
 */
public class NullOutputStream extends OutputStream {
    
    /**
     * A singleton.
     */
    public static final NullOutputStream NULL_OUTPUT_STREAM = new NullOutputStream();

    /**
     * Does nothing - output to <code>/dev/null</code>.
     * @param b The bytes to write
     * @param off The start offset
     * @param len The number of bytes to write
     */
    @Override
    public void write(byte[] b, int off, int len) {
        //to /dev/null
    }

    /**
     * Does nothing - output to <code>/dev/null</code>.
     * @param b The byte to write
     */
    @Override
    public void write(int b) {
        //to /dev/null
    }

    /**
     * Does nothing - output to <code>/dev/null</code>.
     * @param b The bytes to write
     * @throws IOException never
     */
    @Override
    public void write(byte[] b) throws IOException {
        //to /dev/null
    }

}
"
tika-core/src/main/java/org/apache/tika/io/package-info.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * IO utilities.
 */
@aQute.bnd.annotation.Version("1.0.0")
package org.apache.tika.io;
"
tika-core/src/main/java/org/apache/tika/io/ProxyInputStream.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.io;

import java.io.FilterInputStream;
import java.io.IOException;
import java.io.InputStream;

/**
 * A Proxy stream which acts as expected, that is it passes the method
 * calls on to the proxied stream and doesn't change which methods are
 * being called.
 * <p>
 * It is an alternative base class to FilterInputStream
 * to increase reusability, because FilterInputStream changes the
 * methods being called, such as read(byte[]) to read(byte[], int, int).
 * <p>
 * See the protected methods for ways in which a subclass can easily decorate
 * a stream with custom pre-, post- or error processing functionality.
 *
 * @author Stephen Colebourne
 * @version $Id$
 */
public abstract class ProxyInputStream extends FilterInputStream {

    /**
     * Constructs a new ProxyInputStream.
     *
     * @param proxy  the InputStream to delegate to
     */
    public ProxyInputStream(InputStream proxy) {
        super(proxy);
        // the proxy is stored in a protected superclass variable named 'in'
    }

    /**
     * Invokes the delegate's <code>read()</code> method.
     * @return the byte read or -1 if the end of stream
     * @throws IOException if an I/O error occurs
     */
    @Override
    public int read() throws IOException {
        try {
            beforeRead(1);
            int b = in.read();
            afterRead(b != -1 ? 1 : -1);
            return b;
        } catch (IOException e) {
            handleIOException(e);
            return -1;
        }
    }

    /**
     * Invokes the delegate's <code>read(byte[])</code> method.
     * @param bts the buffer to read the bytes into
     * @return the number of bytes read or -1 if the end of stream
     * @throws IOException if an I/O error occurs
     */
    @Override
    public int read(byte[] bts) throws IOException {
        try {
            beforeRead(bts.length);
            int n = in.read(bts);
            afterRead(n);
            return n;
        } catch (IOException e) {
            handleIOException(e);
            return -1;
        }
    }

    /**
     * Invokes the delegate's <code>read(byte[], int, int)</code> method.
     * @param bts the buffer to read the bytes into
     * @param off The start offset
     * @param len The number of bytes to read
     * @return the number of bytes read or -1 if the end of stream
     * @throws IOException if an I/O error occurs
     */
    @Override
    public int read(byte[] bts, int off, int len) throws IOException {
        try {
            beforeRead(len);
            int n = in.read(bts, off, len);
            afterRead(n);
            return n;
        } catch (IOException e) {
            handleIOException(e);
            return -1;
        }
    }

    /**
     * Invokes the delegate's <code>skip(long)</code> method.
     * @param ln the number of bytes to skip
     * @return the actual number of bytes skipped
     * @throws IOException if an I/O error occurs
     */
    @Override
    public long skip(long ln) throws IOException {
        try {
            return in.skip(ln);
        } catch (IOException e) {
            handleIOException(e);
            return 0;
        }
    }

    /**
     * Invokes the delegate's <code>available()</code> method.
     * @return the number of available bytes
     * @throws IOException if an I/O error occurs
     */
    @Override
    public int available() throws IOException {
        try {
            return super.available();
        } catch (IOException e) {
            handleIOException(e);
            return 0;
        }
    }

    /**
     * Invokes the delegate's <code>close()</code> method.
     * @throws IOException if an I/O error occurs
     */
    @Override
    public void close() throws IOException {
        try {
            in.close();
        } catch (IOException e) {
            handleIOException(e);
        }
    }

    /**
     * Invokes the delegate's <code>mark(int)</code> method.
     * @param readlimit read ahead limit
     */
    @Override
    public synchronized void mark(int readlimit) {
        in.mark(readlimit);
    }

    /**
     * Invokes the delegate's <code>reset()</code> method.
     * @throws IOException if an I/O error occurs
     */
    @Override
    public synchronized void reset() throws IOException {
        try {
            in.reset();
        } catch (IOException e) {
            handleIOException(e);
        }
    }

    /**
     * Invokes the delegate's <code>markSupported()</code> method.
     * @return true if mark is supported, otherwise false
     */
    @Override
    public boolean markSupported() {
        return in.markSupported();
    }

    /**
     * Invoked by the read methods before the call is proxied. The number
     * of bytes that the caller wanted to read (1 for the {@link #read()}
     * method, buffer length for {@link #read(byte[])}, etc.) is given as
     * an argument.
     * <p>
     * Subclasses can override this method to add common pre-processing
     * functionality without having to override all the read methods.
     * The default implementation does nothing.
     * <p>
     * Note this method is <em>not</em> called from {@link #skip(long)} or
     * {@link #reset()}. You need to explicitly override those methods if
     * you want to add pre-processing steps also to them.
     *
     * @since Commons IO 2.0
     * @param n number of bytes that the caller asked to be read
     * @throws IOException if the pre-processing fails
     */
    protected void beforeRead(int n) throws IOException {
    }

    /**
     * Invoked by the read methods after the proxied call has returned
     * successfully. The number of bytes returned to the caller (or -1 if
     * the end of stream was reached) is given as an argument.
     * <p>
     * Subclasses can override this method to add common post-processing
     * functionality without having to override all the read methods.
     * The default implementation does nothing.
     * <p>
     * Note this method is <em>not</em> called from {@link #skip(long)} or
     * {@link #reset()}. You need to explicitly override those methods if
     * you want to add post-processing steps also to them.
     *
     * @since Commons IO 2.0
     * @param n number of bytes read, or -1 if the end of stream was reached
     * @throws IOException if the post-processing fails
     */
    protected void afterRead(int n) throws IOException {
    }

    /**
     * Handle any IOExceptions thrown.
     * <p>
     * This method provides a point to implement custom exception
     * handling. The default behaviour is to re-throw the exception.
     * @param e The IOException thrown
     * @throws IOException if an I/O error occurs
     * @since Commons IO 2.0
     */
    protected void handleIOException(IOException e) throws IOException {
        throw e;
    }

}
"
tika-core/src/main/java/org/apache/tika/io/TaggedInputStream.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.io;

 import java.io.IOException;
import java.io.InputStream;
import java.io.Serializable;
import java.util.UUID;

/**
 * An input stream decorator that tags potential exceptions so that the
 * stream that caused the exception can easily be identified. This is
 * done by using the {@link TaggedIOException} class to wrap all thrown
 * {@link IOException}s. See below for an example of using this class.
 * <pre>
 * TaggedInputStream stream = new TaggedInputStream(...);
 * try {
 *     // Processing that may throw an IOException either from this stream
 *     // or from some other IO activity like temporary files, etc.
 *     processStream(stream);
 * } catch (IOException e) {
 *     if (stream.isCauseOf(e)) {
 *         // The exception was caused by this stream.
 *         // Use e.getCause() to get the original exception.
 *     } else {
 *         // The exception was caused by something else.
 *     }
 * }
 * </pre>
 * <p>
 * Alternatively, the {@link #throwIfCauseOf(Exception)} method can be
 * used to let higher levels of code handle the exception caused by this
 * stream while other processing errors are being taken care of at this
 * lower level.
 * <pre>
 * TaggedInputStream stream = new TaggedInputStream(...);
 * try {
 *     processStream(stream);
 * } catch (IOException e) {
 *     stream.throwIfCauseOf(e);
 *     // ... or process the exception that was caused by something else
 * }
 * </pre>
 *
 * @see TaggedIOException
 */
public class TaggedInputStream extends ProxyInputStream {

    /**
     * The unique (serializable) tag of this stream.
     */
    private final Serializable tag = UUID.randomUUID();

    /**
     * Creates a tagging decorator for the given input stream.
     *
     * @param proxy input stream to be decorated
     */
    public TaggedInputStream(InputStream proxy) {
        super(proxy);
    }
    
    /**
     * Casts or wraps the given stream to a TaggedInputStream instance.
     *
     * @param stream normal input stream
     * @return a TaggedInputStream instance
     */
    public static TaggedInputStream get(InputStream proxy) {
       if(proxy instanceof TaggedInputStream) {
          return (TaggedInputStream)proxy;
       }
       return new TaggedInputStream(proxy);
    }

    /**
     * Tests if the given exception was caused by this stream.
     *
     * @param exception an exception
     * @return <code>true</code> if the exception was thrown by this stream,
     *         <code>false</code> otherwise
     */
    public boolean isCauseOf(IOException exception) {
        if (exception instanceof TaggedIOException) {
            TaggedIOException tagged = (TaggedIOException) exception;
            return tag.equals(tagged.getTag());
        } else {
            return false;
        }
    }

    /**
     * Re-throws the original exception thrown by this stream. This method
     * first checks whether the given exception is a {@link TaggedIOException}
     * wrapper created by this decorator, and then unwraps and throws the
     * original wrapped exception. Returns normally if the exception was
     * not thrown by this stream.
     *
     * @param exception an exception
     * @throws IOException original exception, if any, thrown by this stream
     */
    public void throwIfCauseOf(Exception exception) throws IOException {
        if (exception instanceof TaggedIOException) {
            TaggedIOException tagged = (TaggedIOException) exception;
            if (tag.equals(tagged.getTag())) {
                throw tagged.getCause();
            }
        }
    }

    /**
     * Tags any IOExceptions thrown, wrapping and re-throwing.
     * 
     * @param e The IOException thrown
     * @throws IOException if an I/O error occurs
     */
    @Override
    protected void handleIOException(IOException e) throws IOException {
        throw new TaggedIOException(e, tag);
    }

    public String toString() {
        return "Tika Tagged InputStream wrapping " + in;
    }
}
"
tika-core/src/main/java/org/apache/tika/io/TaggedIOException.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.io;

import java.io.IOException;

/**
 * An {@link IOException} wrapper that tags the wrapped exception with
 * a given object reference. Both the tag and the wrapped original exception
 * can be used to determine further processing when this exception is caught.
 */
public class TaggedIOException extends IOExceptionWithCause {

    /**
     * The object reference used to tag the exception.
     */
    private final Object tag;

    /**
     * Creates a tagged wrapper for the given exception.
     *
     * @param original the exception to be tagged
     * @param tag tag object
     */
    public TaggedIOException(IOException original, Object tag) {
        super(original.getMessage(), original);
        this.tag = tag;
    }

    /**
     * Returns the object reference used as the tag this exception.
     *
     * @return tag object
     */
    public Object getTag() {
        return tag;
    }

    /**
     * Returns the wrapped exception. The only difference to the overridden
     * {@link Throwable#getCause()} method is the narrower return type.
     *
     * @return wrapped exception
     */
    @Override
    public IOException getCause() {
        return (IOException) super.getCause();
    }

}
"
tika-core/src/main/java/org/apache/tika/io/TailStream.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.io;

import java.io.FilterInputStream;
import java.io.IOException;
import java.io.InputStream;

/**
 * <p>
 * A specialized input stream implementation which records the last portion read
 * from an underlying stream.
 * </p>
 * <p>
 * This stream implementation is useful to deal with information which is known
 * to be located at the end of a stream (e.g. ID3 v1 tags). While reading bytes
 * from the underlying stream, a given number of bytes is kept in an internal
 * buffer. This buffer can then be queried after the whole stream was read. It
 * contains the last bytes read from the original input stream.
 * </p>
 * 
 * @param in the underlying input stream
 * @param tailSize the size of the tail buffer
 */
public class TailStream extends FilterInputStream
{
    /** Constant for the default skip buffer size. */
    private static final int SKIP_SIZE = 4096;
    
    /** The buffer in which the tail data is stored. */
    private final byte[] tailBuffer;

    /** The size of the internal tail buffer. */
    private final int tailSize;

    /** A copy of the internal tail buffer used for mark() operations. */
    private byte[] markBuffer;

    /** The number of bytes that have been read so far. */
    private long bytesRead;

    /** The number of bytes read at the last mark() operation. */
    private long markBytesRead;

    /** The current index into the tail buffer. */
    private int currentIndex;

    /** A copy of the current index used for mark() operations. */
    private int markIndex;

    /**
     * Creates a new instance of {@code TailStream}.
     * 
     * @param in the underlying input stream
     * @param size the size of the tail buffer
     */
    public TailStream(InputStream in, int size)
    {
        super(in);
        tailSize = size;
        tailBuffer = new byte[size];
    }

    /**
     * {@inheritDoc} This implementation adds the read byte to the internal tail
     * buffer.
     */
    @Override
    public int read() throws IOException
    {
        int c = super.read();
        if (c != -1)
        {
            appendByte((byte) c);
        }
        return c;
    }

    /**
     * {@inheritDoc} This implementation delegates to the underlying stream and
     * then adds the correct portion of the read buffer to the internal tail
     * buffer.
     */
    @Override
    public int read(byte[] buf) throws IOException
    {
        int read = super.read(buf);
        if (read > 0)
        {
            appendBuf(buf, 0, read);
        }
        return read;
    }

    /**
     * {@inheritDoc} This implementation delegates to the underlying stream and
     * then adds the correct portion of the read buffer to the internal tail
     * buffer.
     */
    @Override
    public int read(byte[] buf, int ofs, int length) throws IOException
    {
        int read = super.read(buf, ofs, length);
        if (read > 0)
        {
            appendBuf(buf, ofs, read);
        }
        return read;
    }
    
    /**
     * {@inheritDoc} This implementation delegates to the {@code read()} method
     * to ensure that the tail buffer is also filled if data is skipped.
     */
    @Override
    public long skip(long n) throws IOException
    {
        int bufSize = (int) Math.min(n, SKIP_SIZE);
        byte[] buf = new byte[bufSize];
        long bytesSkipped = 0;
        int bytesRead = 0;
        
        while(bytesSkipped < n && bytesRead != -1)
        {
            int len = (int) Math.min(bufSize, n - bytesSkipped);
            bytesRead = read(buf, 0, len);
            if(bytesRead != -1)
            {
                bytesSkipped += bytesRead;
            }
        }

        return (bytesRead < 0 && bytesSkipped == 0) ? -1 : bytesSkipped;
    }

    /**
     * {@inheritDoc} This implementation saves the internal state including the
     * content of the tail buffer so that it can be restored when ''reset()'' is
     * called later.
     */
    @Override
    public void mark(int limit)
    {
        markBuffer = new byte[tailSize];
        System.arraycopy(tailBuffer, 0, markBuffer, 0, tailSize);
        markIndex = currentIndex;
        markBytesRead = bytesRead;
    }

    /**
     * {@inheritDoc} This implementation restores this stream's state to the
     * state when ''mark()'' was called the last time. If ''mark()'' has not
     * been called before, this method has no effect.
     */
    @Override
    public void reset()
    {
        if (markBuffer != null)
        {
            System.arraycopy(markBuffer, 0, tailBuffer, 0, tailSize);
            currentIndex = markIndex;
            bytesRead = markBytesRead;
        }
    }

    /**
     * Returns an array with the last data read from the underlying stream. If
     * the underlying stream contained more data than the ''tailSize''
     * constructor argument, the returned array has a length of ''tailSize''.
     * Otherwise, its length equals the number of bytes read.
     * 
     * @return an array with the last data read from the underlying stream
     */
    public byte[] getTail()
    {
        int size = (int) Math.min(tailSize, bytesRead);
        byte[] result = new byte[size];
        System.arraycopy(tailBuffer, currentIndex, result, 0, size
                - currentIndex);
        System.arraycopy(tailBuffer, 0, result, size - currentIndex,
                currentIndex);
        return result;
    }

    /**
     * Adds the given byte to the internal tail buffer.
     * 
     * @param b the byte to be added
     */
    private void appendByte(byte b)
    {
        tailBuffer[currentIndex++] = b;
        if (currentIndex >= tailSize)
        {
            currentIndex = 0;
        }
        bytesRead++;
    }

    /**
     * Adds the content of the given buffer to the internal tail buffer.
     * 
     * @param buf the buffer
     * @param ofs the start offset in the buffer
     * @param length the number of bytes to be copied
     */
    private void appendBuf(byte[] buf, int ofs, int length)
    {
        if (length >= tailSize)
        {
            replaceTailBuffer(buf, ofs, length);
        }
        else
        {
            copyToTailBuffer(buf, ofs, length);
        }

        bytesRead += length;
    }

    /**
     * Replaces the content of the internal tail buffer by the last portion of
     * the given buffer. This method is called if a buffer was read from the
     * underlying stream whose length is larger than the tail buffer.
     * 
     * @param buf the buffer
     * @param ofs the start offset in the buffer
     * @param length the number of bytes to be copied
     */
    private void replaceTailBuffer(byte[] buf, int ofs, int length)
    {
        System.arraycopy(buf, ofs + length - tailSize, tailBuffer, 0, tailSize);
        currentIndex = 0;
    }

    /**
     * Copies the given buffer into the internal tail buffer at the current
     * position. This method is called if a buffer is read from the underlying
     * stream whose length is smaller than the tail buffer. In this case the
     * tail buffer is only partly overwritten.
     * 
     * @param buf the buffer
     * @param ofs the start offset in the buffer
     * @param length the number of bytes to be copied
     */
    private void copyToTailBuffer(byte[] buf, int ofs, int length)
    {
        int remaining = tailSize - currentIndex;
        int size1 = Math.min(remaining, length);
        System.arraycopy(buf, ofs, tailBuffer, currentIndex, size1);
        System.arraycopy(buf, ofs + size1, tailBuffer, 0, length - size1);
        currentIndex = (currentIndex + length) % tailSize;
    }
}
"
tika-core/src/main/java/org/apache/tika/io/TemporaryResources.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.io;

import java.io.Closeable;
import java.io.File;
import java.io.IOException;
import java.util.LinkedList;
import java.util.List;

import org.apache.tika.exception.TikaException;

/**
 * Utility class for tracking and ultimately closing or otherwise disposing
 * a collection of temporary resources.
 * <p>
 * Note that this class is not thread-safe.
 *
 * @since Apache Tika 0.10
 */
public class TemporaryResources implements Closeable {

    /**
     * Tracked resources in LIFO order.
     */
    private final LinkedList<Closeable> resources = new LinkedList<Closeable>();

    /**
     * Directory for temporary files, <code>null</code> for the system default.
     */
    private File tmp = null;

    /**
     * Sets the directory to be used for the temporary files created by
     * the {@link #createTemporaryFile()} method.
     *
     * @param tmp temporary file directory,
     *            or <code>null</code> for the system default
     */
    public void setTemporaryFileDirectory(File tmp) {
        this.tmp = tmp;
    }

    /**
     * Creates and returns a temporary file that will automatically be
     * deleted when the {@link #close()} method is called.
     *
     * @return
     * @throws IOException
     */
    public File createTemporaryFile() throws IOException {
        final File file = File.createTempFile("apache-tika-", ".tmp", tmp);
        addResource(new Closeable() {
            public void close() throws IOException {
                if (!file.delete()) {
                    throw new IOException(
                            "Could not delete temporary file "
                            + file.getPath());
                }
            }
        });
        return file;
    }

    /**
     * Adds a new resource to the set of tracked resources that will all be
     * closed when the {@link #close()} method is called.
     *
     * @param resource resource to be tracked
     */
    public void addResource(Closeable resource) {
        resources.addFirst(resource);
    }

    /**
     * Returns the latest of the tracked resources that implements or
     * extends the given interface or class.
     *
     * @param klass interface or class
     * @return matching resource, or <code>null</code> if not found
     */
    @SuppressWarnings("unchecked")
    public <T extends Closeable> T getResource(Class<T> klass) {
        for (Closeable resource : resources) {
            if (klass.isAssignableFrom(resource.getClass())) {
                return (T) resource;
            }
        }
        return null;
    }

    /**
     * Closes all tracked resources. The resources are closed in reverse order
     * from how they were added.
     * <p>
     * Any thrown exceptions from managed resources are collected and
     * then re-thrown only once all the resources have been closed.
     *
     * @throws IOException if one or more of the tracked resources
     *                     could not be closed
     */
    public void close() throws IOException {
        // Release all resources and keep track of any exceptions
        List<IOException> exceptions = new LinkedList<IOException>();
        for (Closeable resource : resources) {
            try {
                resource.close();
            } catch (IOException e) {
                exceptions.add(e);
            }
        }
        resources.clear();

        // Throw any exceptions that were captured from above
        if (!exceptions.isEmpty()) {
            if (exceptions.size() == 1) {
                throw exceptions.get(0);
            } else {
                throw new IOExceptionWithCause(
                        "Multiple IOExceptions" + exceptions,
                        exceptions.get(0));
            }
        }
    }

    /**
     * Calls the {@link #close()} method and wraps the potential
     * {@link IOException} into a {@link TikaException} for convenience
     * when used within Tika.
     *
     * @throws TikaException if one or more of the tracked resources
     *                       could not be closed
     */
    public void dispose() throws TikaException {
        try {
            close();
        } catch (IOException e) {
            throw new TikaException("Failed to close temporary resources", e);
        }
    }

}
"
tika-core/src/main/java/org/apache/tika/io/TikaInputStream.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.io;

import java.io.BufferedInputStream;
import java.io.ByteArrayInputStream;
import java.io.Closeable;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.net.URI;
import java.net.URISyntaxException;
import java.net.URL;
import java.net.URLConnection;
import java.nio.channels.FileChannel;
import java.sql.Blob;
import java.sql.SQLException;

import org.apache.tika.metadata.Metadata;

/**
 * Input stream with extended capabilities. The purpose of this class is
 * to allow files and other resources and information to be associated with
 * the {@link InputStream} instance passed through the
 * {@link org.apache.tika.parser.Parser} interface and other similar APIs.
 * <p>
 * TikaInputStream instances can be created using the various static
 * <code>get()</code> factory methods. Most of these methods take an optional
 * {@link Metadata} argument that is then filled with the available input
 * metadata from the given resource. The created TikaInputStream instance
 * keeps track of the original resource used to create it, while behaving
 * otherwise just like a normal, buffered {@link InputStream}.
 * A TikaInputStream instance is also guaranteed to support the
 * {@link #mark(int)} feature.
 * <p>
 * Code that wants to access the underlying file or other resources
 * associated with a TikaInputStream should first use the
 * {@link #get(InputStream)} factory method to cast or wrap a given
 * {@link InputStream} into a TikaInputStream instance.
 *
 * @since Apache Tika 0.8
 */
public class TikaInputStream extends TaggedInputStream {

    /**
     * Checks whether the given stream is a TikaInputStream instance.
     * The given stream can be <code>null</code>, in which case the return
     * value is <code>false</code>.
     * 
     * @param stream input stream, possibly <code>null</code>
     * @return <code>true</code> if the stream is a TikaInputStream instance,
     *         <code>false</code> otherwise
     */
    public static boolean isTikaInputStream(InputStream stream) {
        return stream instanceof TikaInputStream;
    }

    /**
     * Casts or wraps the given stream to a TikaInputStream instance.
     * This method can be used to access the functionality of this class
     * even when given just a normal input stream instance.
     * <p>
     * The given temporary file provider is used for any temporary files,
     * and should be disposed when the returned stream is no longer used.
     * <p>
     * Use this method instead of the {@link #get(InputStream)} alternative
     * when you <em>don't</em> explicitly close the returned stream. The
     * recommended access pattern is:
     * <pre>
     * TemporaryResources tmp = new TemporaryResources();
     * try {
     *     TikaInputStream stream = TikaInputStream.get(..., tmp);
     *     // process stream but don't close it
     * } finally {
     *     tmp.close();
     * }
     * </pre>
     * <p>
     * The given stream instance will <em>not</em> be closed when the
     * {@link TemporaryResources#close()} method is called. The caller
     * is expected to explicitly close the original stream when it's no
     * longer used.
     *
     * @since Apache Tika 0.10
     * @param stream normal input stream
     * @return a TikaInputStream instance
     */
    public static TikaInputStream get(
            InputStream stream, TemporaryResources tmp) {
        if (stream == null) {
            throw new NullPointerException("The Stream must not be null");
        }
        if (stream instanceof TikaInputStream) {
            return (TikaInputStream) stream;
        } else {
            // Make sure that the stream is buffered and that it
            // (properly) supports the mark feature
            if (!(stream instanceof BufferedInputStream)
                    && !(stream instanceof ByteArrayInputStream)) {
                stream = new BufferedInputStream(stream);
            }
            return new TikaInputStream(stream, tmp, -1);
        }
    }

    /**
     * Casts or wraps the given stream to a TikaInputStream instance.
     * This method can be used to access the functionality of this class
     * even when given just a normal input stream instance.
     * <p>
     * Use this method instead of the
     * {@link #get(InputStream, TemporaryResources)} alternative when you
     * <em>do</em> explicitly close the returned stream. The recommended
     * access pattern is:
     * <pre>
     * TikaInputStream stream = TikaInputStream.get(...);
     * try {
     *     // process stream
     * } finally {
     *     stream.close();
     * }
     * </pre>
     * <p>
     * The given stream instance will be closed along with any other resources
     * associated with the returned TikaInputStream instance when the
     * {@link #close()} method is called.
     *
     * @param stream normal input stream
     * @return a TikaInputStream instance
     */
    public static TikaInputStream get(InputStream stream) {
        return get(stream, new TemporaryResources());
    }

    /**
     * Returns the given stream casts to a TikaInputStream, or
     * <code>null</code> if the stream is not a TikaInputStream.
     *
     * @since Apache Tika 0.10
     * @param stream normal input stream
     * @return a TikaInputStream instance
     */
    public static TikaInputStream cast(InputStream stream) {
        if (stream instanceof TikaInputStream) {
            return (TikaInputStream) stream;
        } else {
            return null;
        }
    }

    /**
     * Creates a TikaInputStream from the given array of bytes.
     * <p>
     * Note that you must always explicitly close the returned stream as in
     * some cases it may end up writing the given data to a temporary file.
     *
     * @param data input data
     * @return a TikaInputStream instance
     */
    public static TikaInputStream get(byte[] data) {
        return get(data, new Metadata());
    }

    /**
     * Creates a TikaInputStream from the given array of bytes. The length of
     * the array is stored as input metadata in the given metadata instance.
     * <p>
     * Note that you must always explicitly close the returned stream as in
     * some cases it may end up writing the given data to a temporary file.
     *
     * @param data input data
     * @param metadata metadata instance
     * @return a TikaInputStream instance
     * @throws IOException
     */
    public static TikaInputStream get(byte[] data, Metadata metadata) {
        metadata.set(Metadata.CONTENT_LENGTH, Integer.toString(data.length));
        return new TikaInputStream(
                new ByteArrayInputStream(data),
                new TemporaryResources(), data.length);
    }

    /**
     * Creates a TikaInputStream from the given file.
     * <p>
     * Note that you must always explicitly close the returned stream to
     * prevent leaking open file handles.
     *
     * @param file input file
     * @return a TikaInputStream instance
     * @throws FileNotFoundException if the file does not exist
     */
    public static TikaInputStream get(File file) throws FileNotFoundException {
        return get(file, new Metadata());
    }

    /**
     * Creates a TikaInputStream from the given file. The file name and
     * length are stored as input metadata in the given metadata instance.
     * <p>
     * Note that you must always explicitly close the returned stream to
     * prevent leaking open file handles.
     *
     * @param file input file
     * @param metadata metadata instance
     * @return a TikaInputStream instance
     * @throws FileNotFoundException if the file does not exist
     */
    public static TikaInputStream get(File file, Metadata metadata)
            throws FileNotFoundException {
        metadata.set(Metadata.RESOURCE_NAME_KEY, file.getName());
        metadata.set(Metadata.CONTENT_LENGTH, Long.toString(file.length()));
        return new TikaInputStream(file);
    }

    /**
     * Creates a TikaInputStream from the given database BLOB.
     * <p>
     * Note that the result set containing the BLOB may need to be kept open
     * until the returned TikaInputStream has been processed and closed.
     * You must also always explicitly close the returned stream as in
     * some cases it may end up writing the blob data to a temporary file.
     *
     * @param blob database BLOB
     * @return a TikaInputStream instance
     * @throws SQLException if BLOB data can not be accessed
     */
    public static TikaInputStream get(Blob blob) throws SQLException {
        return get(blob, new Metadata());
    }

    /**
     * Blob size threshold that limits the largest BLOB size to be
     * buffered fully in memory by the {@link #get(Blob, Metadata)}
     * method.
     */
    private static final int BLOB_SIZE_THRESHOLD = 1024 * 1024;

    /**
     * Creates a TikaInputStream from the given database BLOB. The BLOB
     * length (if available) is stored as input metadata in the given
     * metadata instance.
     * <p>
     * Note that the result set containing the BLOB may need to be kept open
     * until the returned TikaInputStream has been processed and closed.
     * You must also always explicitly close the returned stream as in
     * some cases it may end up writing the blob data to a temporary file.
     *
     * @param blob database BLOB
     * @param metadata metadata instance
     * @return a TikaInputStream instance
     * @throws SQLException if BLOB data can not be accessed
     */
    public static TikaInputStream get(Blob blob, Metadata metadata)
            throws SQLException {
        long length = -1;
        try {
            length = blob.length();
            metadata.set(Metadata.CONTENT_LENGTH, Long.toString(length));
        } catch (SQLException ignore) {
        }

        // Prefer an in-memory buffer for reasonably sized blobs to reduce
        // the likelihood of problems caused by long-lived database accesses
        if (0 <= length && length <= BLOB_SIZE_THRESHOLD) {
            // the offset in Blob.getBytes() starts at 1
            return get(blob.getBytes(1, (int) length), metadata);
        } else {
            return new TikaInputStream(
                    new BufferedInputStream(blob.getBinaryStream()),
                    new TemporaryResources(), length);
        }
    }

    /**
     * Creates a TikaInputStream from the resource at the given URI.
     * <p>
     * Note that you must always explicitly close the returned stream as in
     * some cases it may end up writing the resource to a temporary file.
     *
     * @param uri resource URI
     * @return a TikaInputStream instance
     * @throws IOException if the resource can not be accessed
     */
    public static TikaInputStream get(URI uri) throws IOException {
        return get(uri, new Metadata());
    }

    /**
     * Creates a TikaInputStream from the resource at the given URI. The
     * available input metadata is stored in the given metadata instance.
     * <p>
     * Note that you must always explicitly close the returned stream as in
     * some cases it may end up writing the resource to a temporary file.
     *
     * @param uri resource URI
     * @param metadata metadata instance
     * @return a TikaInputStream instance
     * @throws IOException if the resource can not be accessed
     */
    public static TikaInputStream get(URI uri, Metadata metadata)
            throws IOException {
        // Special handling for file:// URIs
        if ("file".equalsIgnoreCase(uri.getScheme())) {
            File file = new File(uri);
            if (file.isFile()) {
                return get(file, metadata);
            }
        }

        return get(uri.toURL(), metadata);
    }

    /**
     * Creates a TikaInputStream from the resource at the given URL.
     * <p>
     * Note that you must always explicitly close the returned stream as in
     * some cases it may end up writing the resource to a temporary file.
     *
     * @param url resource URL
     * @return a TikaInputStream instance
     * @throws IOException if the resource can not be accessed
     */
    public static TikaInputStream get(URL url) throws IOException {
        return get(url, new Metadata());
    }

    /**
     * Creates a TikaInputStream from the resource at the given URL. The
     * available input metadata is stored in the given metadata instance.
     * <p>
     * Note that you must always explicitly close the returned stream as in
     * some cases it may end up writing the resource to a temporary file.
     *
     * @param url resource URL
     * @param metadata metadata instance
     * @return a TikaInputStream instance
     * @throws IOException if the resource can not be accessed
     */
    public static TikaInputStream get(URL url, Metadata metadata)
            throws IOException {
        // Special handling for file:// URLs
        if ("file".equalsIgnoreCase(url.getProtocol())) {
            try {
                File file = new File(url.toURI());
                if (file.isFile()) {
                    return get(file, metadata);
                }
            } catch (URISyntaxException e) {
                // fall through
            }
        }

        URLConnection connection = url.openConnection();

        String path = url.getPath();
        int slash = path.lastIndexOf('/');
        if (slash + 1 < path.length()) { // works even with -1!
            metadata.set(Metadata.RESOURCE_NAME_KEY, path.substring(slash + 1));
        }

        String type = connection.getContentType();
        if (type != null) {
            metadata.set(Metadata.CONTENT_TYPE, type);
        }

        String encoding = connection.getContentEncoding();
        if (encoding != null) {
            metadata.set(Metadata.CONTENT_ENCODING, encoding);
        }

        int length = connection.getContentLength();
        if (length >= 0) {
            metadata.set(Metadata.CONTENT_LENGTH, Integer.toString(length));
        }

        return new TikaInputStream(
                new BufferedInputStream(connection.getInputStream()),
                new TemporaryResources(), length);
    }

    /**
     * The file that contains the contents of this stream. This is either
     * the original file passed to the {@link #TikaInputStream(File)}
     * constructor or a temporary file created by a call to the
     * {@link #getFile()} method. If neither has been called, then
     * the value is <code>null</code>.
     */
    private File file;

    /**
     * Tracker of temporary resources.
     */
    private final TemporaryResources tmp;

    /**
     * Total length of the stream, or -1 if unknown.
     */
    private long length;

    /**
     * Current read position within this stream.
     */
    private long position = 0;

    /**
     * Marked position, or -1 if there is no current mark.
     */
    private long mark = -1;

    /**
     * A opened container, such as a POIFS FileSystem
     *  for an OLE2 document, or a Zip file for a
     *  zip based (eg ooxml, odf) document.
     */
    private Object openContainer;

    /**
     * Creates a TikaInputStream instance. This private constructor is used
     * by the static factory methods based on the available information.
     *
     * @param file the file that contains the stream
     * @throws FileNotFoundException if the file does not exist
     */
    private TikaInputStream(File file) throws FileNotFoundException {
        super(new BufferedInputStream(new FileInputStream(file)));
        this.file = file;
        this.tmp = new TemporaryResources();
        this.length = file.length();
    }

    /**
     * Creates a TikaInputStream instance. This private constructor is used
     * by the static factory methods based on the available information.
     * <p>
     * The given stream needs to be included in the given temporary resource
     * collection if the caller wants it also to get closed when the
     * {@link #close()} method is invoked.
     *
     * @param stream <em>buffered</em> stream (must support the mark feature)
     * @param tmp tracker for temporary resources associated with this stream
     * @param length total length of the stream, or -1 if unknown
     */
    private TikaInputStream(
            InputStream stream, TemporaryResources tmp, long length) {
        super(stream);
        this.file = null;
        this.tmp = tmp;
        this.length = length;
    }

    /**
     * Fills the given buffer with upcoming bytes from this stream without
     * advancing the current stream position. The buffer is filled up unless
     * the end of stream is encountered before that. This method will block
     * if not enough bytes are immediately available.
     *
     * @param buffer byte buffer
     * @return number of bytes written to the buffer
     * @throws IOException if the stream can not be read
     */
    public int peek(byte[] buffer) throws IOException {
        int n = 0;

        mark(buffer.length);

        int m = read(buffer);
        while (m != -1) {
            n += m;
            if (n < buffer.length) {
                m = read(buffer, n, buffer.length - n);
            } else {
                m = -1;
            }
        }

        reset();

        return n;
    }
    
    /**
     * Returns the open container object, such as a
     *  POIFS FileSystem in the event of an OLE2
     *  document being detected and processed by
     *  the OLE2 detector. 
     */
    public Object getOpenContainer() {
        return openContainer;
    }
    
    /**
     * Stores the open container object against
     *  the stream, eg after a Zip contents 
     *  detector has loaded the file to decide
     *  what it contains.
     */
    public void setOpenContainer(Object container) {
        openContainer = container;
        if (container instanceof Closeable) {
            tmp.addResource((Closeable) container);
        }
    }

    public boolean hasFile() {
        return file != null;
    }

    public File getFile() throws IOException {
        if (file == null) {
            if (position > 0) {
                throw new IOException("Stream is already being read");
            } else {
                // Spool the entire stream into a temporary file
                file = tmp.createTemporaryFile();
                OutputStream out = new FileOutputStream(file);
                try {
                    IOUtils.copy(in, out);
                } finally {
                    out.close();
                }

                // Create a new input stream and make sure it'll get closed
                FileInputStream newStream = new FileInputStream(file);
                tmp.addResource(newStream);

                // Replace the spooled stream with the new stream in a way
                // that still ends up closing the old stream if or when the
                // close() method is called. The closing of the new stream
                // is already being handled as noted above.
                final InputStream oldStream = in;
                in = new BufferedInputStream(newStream) {
                    @Override
                    public void close() throws IOException {
                        oldStream.close();
                    }
                };

                length = file.length();
            }
        }
        return file;
    }

    public FileChannel getFileChannel() throws IOException {
        FileInputStream fis = new FileInputStream(getFile());
        tmp.addResource(fis);
        FileChannel channel = fis.getChannel();
        tmp.addResource(channel);
        return channel;
    }

    public boolean hasLength() {
        return length != -1;
    }

    /**
     * Returns the length (in bytes) of this stream. Note that if the length
     * was not available when this stream was instantiated, then this method
     * will use the {@link #getFile()} method to buffer the entire stream to
     * a temporary file in order to calculate the stream length. This case
     * will only work if the stream has not yet been consumed.
     *
     * @return stream length
     * @throws IOException if the length can not be determined
     */
    public long getLength() throws IOException {
        if (length == -1) {
            length = getFile().length();
        }
        return length;
    }

    /**
     * Returns the current position within the stream.
     *
     * @return stream position
     */
    public long getPosition() {
        return position;
    }

    @Override
    public long skip(long ln) throws IOException {
        long n = super.skip(ln);
        position += n;
        return n;
    }

    @Override
    public void mark(int readlimit) {
        super.mark(readlimit);
        mark = position;
    }

    @Override
    public boolean markSupported() {
        return true;
    }

    @Override
    public void reset() throws IOException {
        super.reset();
        position = mark;
        mark = -1;
    }

    @Override
    public void close() throws IOException {
        file = null;
        mark = -1;

        // The close method was explicitly called, so we indeed
        // are expected to close the input stream. Handle that
        // by adding that stream as a resource to be tracked before
        // closing all of them. This way also possible exceptions from
        // the close() calls get managed properly.
        tmp.addResource(in);
        tmp.close();
    }

    @Override
    protected void afterRead(int n) {
        if (n != -1) {
            position += n;
        }
    }

    public String toString() {
        String str = "TikaInputStream of ";
        if (hasFile()) {
            str += file.toString();
        } else {
            str += in.toString();
        }
        if (openContainer != null) {
            str += " (in " + openContainer + ")";
        }
        return str;
    }
}
"
tika-core/src/main/java/org/apache/tika/language/LanguageIdentifier.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.language;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.util.HashMap;
import java.util.Map;
import java.util.Properties;
import java.util.Set;

/**
 * Identifier of the language that best matches a given content profile.
 * The content profile is compared to generic language profiles based on
 * material from various sources.
 *
 * @since Apache Tika 0.5
 * @see <a href="http://www.iccs.inf.ed.ac.uk/~pkoehn/publications/europarl/">
 *      Europarl: A Parallel Corpus for Statistical Machine Translation</a>
 * @see <a href="http://www.loc.gov/standards/iso639-2/php/code_list.php">
 *      ISO 639 Language Codes</a>
 */
public class LanguageIdentifier {
    
    /**
     * The available language profiles.
     */
    private static final Map<String, LanguageProfile> PROFILES =
        new HashMap<String, LanguageProfile>();
    private static final String PROFILE_SUFFIX = ".ngp";
    private static final String PROFILE_ENCODING = "UTF-8";

    private static Properties props = new Properties();
    private static String errors = "";
    
    private static final String PROPERTIES_OVERRIDE_FILE = "tika.language.override.properties";
    private static final String PROPERTIES_FILE = "tika.language.properties";
    private static final String LANGUAGES_KEY = "languages";
    private static final double CERTAINTY_LIMIT = 0.022;

    private final String language;

    private final double distance;

    /*
     * Always attempt initializing language profiles when class is loaded first time
     */
    static {
        initProfiles();
    }
    
    /*
     * Add one language profile based on config in property file
     */
    private static void addProfile(String language) throws Exception {
        try {
            LanguageProfile profile = new LanguageProfile();

            InputStream stream =
                LanguageIdentifier.class.getResourceAsStream(language + PROFILE_SUFFIX);
            try {
                BufferedReader reader =
                    new BufferedReader(new InputStreamReader(stream, PROFILE_ENCODING));
                String line = reader.readLine();
                while (line != null) {
                    if (line.length() > 0 && !line.startsWith("#")) {
                        int space = line.indexOf(' ');
                        profile.add(
                                line.substring(0, space),
                                Long.parseLong(line.substring(space + 1)));
                    }
                    line = reader.readLine();
                }
            } finally {
                stream.close();
            }

            addProfile(language, profile);
        } catch (Throwable t) {
            throw new Exception("Failed trying to load language profile for language \""+language+"\". Error: "+t.getMessage());
        }
    }
    
    /**
     * Adds a single language profile
     * @param language an ISO 639 code representing language
     * @param profile the language profile
     */
    public static void addProfile(String language, LanguageProfile profile) {
        PROFILES.put(language, profile);
    }
    
    /**
     * Constructs a language identifier based on a LanguageProfile
     * @param profile the language profile
     */
    public LanguageIdentifier(LanguageProfile profile) {
        String minLanguage = "unknown";
        double minDistance = 1.0;
        for (Map.Entry<String, LanguageProfile> entry : PROFILES.entrySet()) {
            double distance = profile.distance(entry.getValue());
            if (distance < minDistance) {
                minDistance = distance;
                minLanguage = entry.getKey();
            }
        }

        this.language = minLanguage;
        this.distance = minDistance;
    }

    /**
     * Constructs a language identifier based on a String of text content
     * @param content the text
     */
    public LanguageIdentifier(String content) {
        this(new LanguageProfile(content));
    }

    /**
     * Gets the identified language
     * @return an ISO 639 code representing the detected language
     */
    public String getLanguage() {
        return language;
    }

    /**
     * Tries to judge whether the identification is certain enough
     * to be trusted.
     * WARNING: Will never return true for small amount of input texts. 
     * @return <code>true</code> if the distance is smaller then {@value #CERTAINTY_LIMIT}, <code>false</code> otherwise
     */
    public boolean isReasonablyCertain() {
        return distance < CERTAINTY_LIMIT;
    }

    /**
     * Builds the language profiles.
     * The list of languages are fetched from a property file named "tika.language.properties"
     * If a file called "tika.language.override.properties" is found on classpath, this is used instead
     * The property file contains a key "languages" with values being comma-separated language codes
     */
    public static void initProfiles() {
        clearProfiles();
        
        errors = "";
        InputStream stream;
        stream = LanguageIdentifier.class.getResourceAsStream(PROPERTIES_OVERRIDE_FILE);
        if(stream == null) {
            stream = LanguageIdentifier.class.getResourceAsStream(PROPERTIES_FILE);
        }

        if(stream != null){
            try {
                props = new Properties();
                props.load(stream);
            } catch (IOException e) {
                errors += "IOException while trying to load property file. Message: " + e.getMessage() + "\n";
            }
        }
        
        String[] languages = props.getProperty(LANGUAGES_KEY).split(",");
        for(String language : languages) {
            language = language.trim();
            String name = props.getProperty("name."+language, "Unknown");
            try {
                addProfile(language);
            } catch (Exception e) {
                errors += "Language " + language + " (" + name + ") not initialized. Message: " + e.getMessage() + "\n";
            }
        }
    }

    /**
     * Initializes the language profiles from a user supplied initialized Map.
     * This overrides the default set of profiles initialized at startup,
     * and provides an alternative to configuring profiles through property file
     *
     * @param profilesMap map of language profiles
     */
    public static void initProfiles(Map<String, LanguageProfile> profilesMap) {
        clearProfiles();
        for(Map.Entry<String, LanguageProfile> entry : profilesMap.entrySet()) {
            addProfile(entry.getKey(), entry.getValue());
        }
    }
    
    /**
     * Clears the current map of language profiles
     */
    public static void clearProfiles() {
        PROFILES.clear();
    }
    
    /**
     * Tests whether there were errors initializing language config
     * @return true if there are errors. Use getErrors() to retrieve.
     */
    public static boolean hasErrors() {
        return errors != "";
    }
    
    /**
     * Returns a string of error messages related to initializing langauge profiles
     * @return the String containing the error messages
     */
    public static String getErrors() {
        return errors;
    }
    
    /**
     * Returns what languages are supported for language identification
     * @return A set of Strings being the ISO 639 language codes
     */
    public static Set<String> getSupportedLanguages() {
        return PROFILES.keySet();
    }

    @Override
    public String toString() {
        return language + " (" + distance + ")";
    }

}
"
tika-core/src/main/java/org/apache/tika/language/LanguageProfile.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.language;

import java.util.HashMap;
import java.util.HashSet;
import java.util.Map;
import java.util.Set;

/**
 * Language profile based on ngram counts.
 *
 * @since Apache Tika 0.5
 */
public class LanguageProfile {

    public static final int DEFAULT_NGRAM_LENGTH = 3;

    private final int length;

    /**
     * The ngrams that make up this profile.
     */
    private final Map<String, Counter> ngrams =
        new HashMap<String, Counter>();

    /**
     * The sum of all ngram counts in this profile.
     * Used to calculate relative ngram frequency.
     */
    private long count = 0;

    private static class Counter {
        private long count = 0;
        public String toString() {
            return Long.toString(count);
        }
    }

    public LanguageProfile(int length) {
        this.length = length;
    }

    public LanguageProfile() {
        this(DEFAULT_NGRAM_LENGTH);
    }

    public LanguageProfile(String content, int length) {
        this(length);

        ProfilingWriter writer = new ProfilingWriter(this);
        char[] ch = content.toCharArray();
        writer.write(ch, 0, ch.length);
    }

    public LanguageProfile(String content) {
        this(content, DEFAULT_NGRAM_LENGTH);
    }

    public long getCount() {
        return count;
    }

    public long getCount(String ngram) {
        Counter counter = ngrams.get(ngram);
        if (counter != null) {
            return counter.count;
        } else {
            return 0;
        }
    }

    /**
     * Adds a single occurrence of the given ngram to this profile.
     *
     * @param ngram the ngram
     */
    public void add(String ngram) {
        add(ngram, 1);
    }

    /**
     * Adds multiple occurrences of the given ngram to this profile.
     *
     * @param ngram the ngram
     * @param count number of occurrences to add
     */
    public void add(String ngram, long count) {
        if (length != ngram.length()) {
            throw new IllegalArgumentException(
                    "Unable to add an ngram of incorrect length: "
                    + ngram.length() + " != " + length);
        }

        Counter counter = ngrams.get(ngram);
        if (counter == null) {
            counter = new Counter();
            ngrams.put(ngram, counter);
        }
        counter.count += count;
        this.count += count;
    }

    /**
     * Calculates the geometric distance between this and the given
     * other language profile.
     *
     * @param that the other language profile
     * @return distance between the profiles
     */
    public double distance(LanguageProfile that) {
        if (length != that.length) {
            throw new IllegalArgumentException(
                    "Unable to calculage distance of language profiles"
                    + " with different ngram lengths: "
                    + that.length + " != " + length);
        }

        double sumOfSquares = 0.0;
        double thisCount = Math.max(this.count, 1.0);
        double thatCount = Math.max(that.count, 1.0);

        Set<String> ngrams = new HashSet<String>();
        ngrams.addAll(this.ngrams.keySet());
        ngrams.addAll(that.ngrams.keySet());
        for (String ngram : ngrams) {
            double thisFrequency = this.getCount(ngram) / thisCount;
            double thatFrequency = that.getCount(ngram) / thatCount;
            double difference = thisFrequency - thatFrequency;
            sumOfSquares += difference * difference;
        }

        return Math.sqrt(sumOfSquares);
    }

    @Override
    public String toString() {
        return ngrams.toString();
    }

}
"
tika-core/src/main/java/org/apache/tika/language/LanguageProfilerBuilder.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.language;

// JDK imports
import java.io.BufferedInputStream;
import java.io.BufferedReader;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.io.OutputStream;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Date;
import java.util.HashMap;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import org.apache.tika.exception.TikaException;

/**
 * This class runs a ngram analysis over submitted text, results might be used
 * for automatic language identification.
 * 
 * The similarity calculation is at experimental level. You have been warned.
 * 
 * Methods are provided to build new NGramProfiles profiles.
 * 
 * @author Sami Siren
 * @author Jerome Charron - http://frutch.free.fr/
 */
public class LanguageProfilerBuilder {

    // public static final Log LOG =
    // LogFactory.getLog(LanguageProfilerBuilder.class);

    /** The minimum length allowed for a ngram. */
    final static int ABSOLUTE_MIN_NGRAM_LENGTH = 3; /* was 1 */

    /** The maximum length allowed for a ngram. */
    final static int ABSOLUTE_MAX_NGRAM_LENGTH = 3; /* was 4 */

    /** The default min length of ngram */
    final static int DEFAULT_MIN_NGRAM_LENGTH = 3;

    /** The default max length of ngram */
    final static int DEFAULT_MAX_NGRAM_LENGTH = 3;

    /** The ngram profile file extension */
    final static String FILE_EXTENSION = "ngp";

    /** The profile max size (number of ngrams of the same size) */
    final static int MAX_SIZE = 1000;

    /** separator char */
    final static char SEPARATOR = '_';
    /** The String form of the separator char */
    private final static String SEP_CHARSEQ = new String(
            new char[] { SEPARATOR });

    /** The profile's name */
    private String name = null;

    /** The NGrams of this profile sorted on the number of occurrences */
    private List<NGramEntry> sorted = null;

    /** The min length of ngram */
    private int minLength = DEFAULT_MIN_NGRAM_LENGTH;

    /** The max length of ngram */
    private int maxLength = DEFAULT_MAX_NGRAM_LENGTH;

    /** The total number of ngrams occurences */
    private int[] ngramcounts = null;

    /** An index of the ngrams of the profile */
    private Map<CharSequence, NGramEntry> ngrams = null;

    /** A StringBuffer used during analysis */
    private QuickStringBuffer word = new QuickStringBuffer();

    /**
     * Constructs a new ngram profile
     * 
     * @param name is the name of the profile
     * @param minlen is the min length of ngram sequences
     * @param maxlen is the max length of ngram sequences
     */
    public LanguageProfilerBuilder(String name, int minlen, int maxlen) {
        // TODO: Compute the initial capacity using minlen and maxlen.
        this.ngrams = new HashMap<CharSequence, NGramEntry>(4000);
        this.minLength = minlen;
        this.maxLength = maxlen;
        this.name = name;
    }
  
    /**
     * Constructs a new ngram profile where minlen=3, maxlen=3
     * 
     * @param name is a name of profile, usually two length string
     * @since Tika 1.0
     */
    public LanguageProfilerBuilder(String name) {
        this.ngrams = new HashMap<CharSequence, NGramEntry>(4000);
        this.minLength = ABSOLUTE_MIN_NGRAM_LENGTH;
        this.maxLength = ABSOLUTE_MAX_NGRAM_LENGTH;
        this.name = name;
    }

    /**
     * @return Returns the name.
     */
    public String getName() {
        return name;
    }
  
    // This method was commented because it depends on org.apache.lucene.analysis.Token
    // that is not a part of the Tika
    // /**
    // * Adds ngrams from a token to this profile
    // *
    // * @param t is the Token to be added
    // */
    // public void add(Token t) {
    // add(new StringBuffer().append(SEPARATOR)
    // .append(t.term())
    // .append(SEPARATOR));
    // }

    /**
     * Adds ngrams from a single word to this profile
     * 
     * @param word is the word to add
     */
    public void add(StringBuffer word) {
        for (int i = minLength; (i <= maxLength) && (i < word.length()); i++) {
            add(word, i);
        }
    }

    /**
     * Adds the last NGrams from the specified word.
     */
    private void add(QuickStringBuffer word) {
        int wlen = word.length();
        if (wlen >= minLength) {
            int max = Math.min(maxLength, wlen);
            for (int i = minLength; i <= max; i++) {
                add(word.subSequence(wlen - i, wlen));
            }
        }
    }

    /**
     * Adds ngrams from a single word in this profile
     * 
     * @param word is the word to add
     * @param n is the ngram size
     */
    private void add(CharSequence cs) {

        if (cs.equals(SEP_CHARSEQ)) {
            return;
        }
        NGramEntry nge = ngrams.get(cs);
        if (nge == null) {
            nge = new NGramEntry(cs);
            ngrams.put(cs, nge);
        }
        nge.inc();
    }

    /**
     * Analyzes a piece of text
     * 
     * @param text
     *            the text to be analyzed
     */
    public void analyze(StringBuilder text) {

        if (ngrams != null) {
            ngrams.clear();
            sorted = null;
            ngramcounts = null;
        }

        word.clear().append(SEPARATOR);
        for (int i = 0; i < text.length(); i++) {
            char c = Character.toLowerCase(text.charAt(i));

            if (Character.isLetter(c)) {
                add(word.append(c));
            } else {
                // found word boundary
                if (word.length() > 1) {
                    // we have a word!
                    add(word.append(SEPARATOR));
                    word.clear().append(SEPARATOR);
                }
            }
        }

        if (word.length() > 1) {
            // we have a word!
            add(word.append(SEPARATOR));
        }
        normalize();
    }

    /**
     * @param word
     * @param n sequence length
     */
    private void add(StringBuffer word, int n) {
        for (int i = 0; i <= word.length() - n; i++) {
            add(word.subSequence(i, i + n));
        }
    }
    
    /**
     * Normalizes the profile (calculates the ngrams frequencies)
     */
    protected void normalize() {
        NGramEntry e = null;
        Iterator<NGramEntry> i = ngrams.values().iterator();

        // Calculates ngram count if not already done
        if (ngramcounts == null) {
            ngramcounts = new int[maxLength + 1];
            while (i.hasNext()) {
                e = i.next();
                ngramcounts[e.size()] += e.count;
            }
        }

        i = ngrams.values().iterator();
        while (i.hasNext()) {
            e = i.next();
            e.frequency = (float) e.count / (float) ngramcounts[e.size()];
        }
    }

    /**
     * Returns a sorted list of ngrams (sort done by 1. frequency 2. sequence)
     * 
     * @return sorted vector of ngrams
     */
    public List<NGramEntry> getSorted() {
        // make sure sorting is done only once
        if (sorted == null) {
            sorted = new ArrayList<NGramEntry>(ngrams.values());
            Collections.sort(sorted);

            // trim at NGRAM_LENGTH entries
            if (sorted.size() > MAX_SIZE) {
                sorted = sorted.subList(0, MAX_SIZE);
            }
        }
        return sorted;
    }

    // Inherited JavaDoc
    public String toString() {

        StringBuffer s = new StringBuffer().append("NGramProfile: ")
                                           .append(name).append("\n");

        Iterator<NGramEntry> i = getSorted().iterator();

        while (i.hasNext()) {
            NGramEntry entry = i.next();
            s.append("[").append(entry.seq).append("/").append(entry.count)
                         .append("/").append(entry.frequency).append("]\n");
        }
        return s.toString();
    }

    /**
     * Calculates a score how well NGramProfiles match each other
     * 
     * @param another
     *            ngram profile to compare against
     * @return similarity 0=exact match
     * @throws TikaException
     *             if could not calculate a score
     */
    public float getSimilarity(LanguageProfilerBuilder another)
            throws TikaException {

        float sum = 0;

        try {
            Iterator<NGramEntry> i = another.getSorted().iterator();
            while (i.hasNext()) {
                NGramEntry other = i.next();
                if (ngrams.containsKey(other.seq)) {
                    sum += Math.abs((other.frequency - ngrams.get(other.seq).frequency)) / 2;
                } else {
                    sum += other.frequency;
                }
            }
            i = getSorted().iterator();
            while (i.hasNext()) {
                NGramEntry other = i.next();
                if (another.ngrams.containsKey(other.seq)) {
                    sum += Math.abs((other.frequency - another.ngrams
                            .get(other.seq).frequency)) / 2;
                } else {
                    sum += other.frequency;
                }
            }
        } catch (Exception e) {
            throw new TikaException("Could not calculate a score how well NGramProfiles match each other");
        }
        return sum;
    }

    /**
     * Loads a ngram profile from an InputStream (assumes UTF-8 encoded content)
     * 
     * @param is the InputStream to read
     */
    public void load(InputStream is) throws IOException {

        ngrams.clear();
        ngramcounts = new int[maxLength + 1];
        BufferedReader reader = new BufferedReader(new InputStreamReader(is, "UTF-8"));
        String line = null;

        while ((line = reader.readLine()) != null) {

            // # starts a comment line
            if (line.charAt(0) != '#') {
                int spacepos = line.indexOf(' ');
                String ngramsequence = line.substring(0, spacepos).trim();
                int len = ngramsequence.length();
                if ((len >= minLength) && (len <= maxLength)) {
                    int ngramcount = Integer.parseInt(line.substring(spacepos + 1));
                    NGramEntry en = new NGramEntry(ngramsequence, ngramcount);
                    ngrams.put(en.getSeq(), en);
                    ngramcounts[len] += ngramcount;
                }
            }
        }
        normalize();
    }
    
    /**
     * Creates a new Language profile from (preferably quite large - 5-10k of
     * lines) text file
     * 
     * @param name to be given for the profile
     * @param is a stream to be read
     * @param encoding is the encoding of stream
     * 
     * @throws TikaException if could not create a language profile
     *  
     */
    public static LanguageProfilerBuilder create(String name, InputStream is, String encoding) throws TikaException {

        LanguageProfilerBuilder newProfile = new LanguageProfilerBuilder(name,
                ABSOLUTE_MIN_NGRAM_LENGTH, ABSOLUTE_MAX_NGRAM_LENGTH);
        BufferedInputStream bis = new BufferedInputStream(is);

        byte buffer[] = new byte[4096];
        StringBuilder text = new StringBuilder();
        int len;

        try {
            while ((len = bis.read(buffer)) != -1) {
                text.append(new String(buffer, 0, len, encoding));
            }
        } catch (IOException e) {
            throw new TikaException("Could not create profile, " + e.getMessage());
        }

        newProfile.analyze(text);
        return newProfile;
    }

    /**
     * Writes NGramProfile content into OutputStream, content is outputted with
     * UTF-8 encoding
     * 
     * @param os the Stream to output to
     * 
     * @throws IOException
     */
    public void save(OutputStream os) throws IOException {
        os.write(("# NgramProfile generated at " + new Date() + 
                  " for Apache Tika Language Identification\n").getBytes("UTF-8"));

        // And then each ngram

        // First dispatch ngrams in many lists depending on their size
        // (one list for each size, in order to store MAX_SIZE ngrams for each
        // size of ngram)
        List<NGramEntry> list = new ArrayList<NGramEntry>();
        List<NGramEntry> sublist = new ArrayList<NGramEntry>();
        NGramEntry[] entries = ngrams.values().toArray(
                new NGramEntry[ngrams.size()]);
        for (int i = minLength; i <= maxLength; i++) {
            for (int j = 0; j < entries.length; j++) {
                if (entries[j].getSeq().length() == i) {
                    sublist.add(entries[j]);
                }
            }
            Collections.sort(sublist);
            if (sublist.size() > MAX_SIZE) {
                sublist = sublist.subList(0, MAX_SIZE);
            }
            list.addAll(sublist);
            sublist.clear();
        }
        for (int i = 0; i < list.size(); i++) {
            NGramEntry e = list.get(i);
            String line = e.toString() + " " + e.getCount() + "\n";
            os.write(line.getBytes("UTF-8"));
        }
        os.flush();
    }

    /**
     * main method used for testing only
     * 
     * @param args
     */
    public static void main(String args[]) {

        // -create he sample_he.txt utf-8

        String usage = "Usage: NGramProfile "
                + "[-create profilename filename encoding] "
                + "[-similarity file1 file2] "
                + "[-score profile-name filename encoding]";
        int command = 0;

        final int CREATE = 1;
        final int SIMILARITY = 2;
        final int SCORE = 3;

        String profilename = "";
        String filename = "";
        String filename2 = "";
        String encoding = "";

        if (args.length == 0) {
            System.err.println(usage);
            System.exit(-1);
        }

        for (int i = 0; i < args.length; i++) { // parse command line
            if (args[i].equals("-create")) { // found -create option
                command = CREATE;
                profilename = args[++i];
                filename = args[++i];
                encoding = args[++i];
            }

            if (args[i].equals("-similarity")) { // found -similarity option
                command = SIMILARITY;
                filename = args[++i];
                filename2 = args[++i];
                encoding = args[++i];
            }

            if (args[i].equals("-score")) { // found -Score option
                command = SCORE;
                profilename = args[++i];
                filename = args[++i];
                encoding = args[++i];
            }
        }

        try {

            switch (command) {

            case CREATE:

                File f = new File(filename);
                FileInputStream fis = new FileInputStream(f);
                LanguageProfilerBuilder newProfile = LanguageProfilerBuilder
                        .create(profilename, fis, encoding);
                fis.close();
                f = new File(profilename + "." + FILE_EXTENSION);
                FileOutputStream fos = new FileOutputStream(f);
                newProfile.save(fos);
                System.out.println("new profile " + profilename + "."
                        + FILE_EXTENSION + " was created.");
                break;

            case SIMILARITY:

                f = new File(filename);
                fis = new FileInputStream(f);
                newProfile = LanguageProfilerBuilder.create(filename, fis,
                        encoding);
                newProfile.normalize();

                f = new File(filename2);
                fis = new FileInputStream(f);
                LanguageProfilerBuilder newProfile2 = LanguageProfilerBuilder
                        .create(filename2, fis, encoding);
                newProfile2.normalize();
                System.out.println("Similarity is "
                        + newProfile.getSimilarity(newProfile2));
                break;

            case SCORE:
                f = new File(filename);
                fis = new FileInputStream(f);
                newProfile = LanguageProfilerBuilder.create(filename, fis,
                        encoding);

                f = new File(profilename + "." + FILE_EXTENSION);
                fis = new FileInputStream(f);
                LanguageProfilerBuilder compare = new LanguageProfilerBuilder(
                        profilename, DEFAULT_MIN_NGRAM_LENGTH,
                        DEFAULT_MAX_NGRAM_LENGTH);
                compare.load(fis);
                System.out.println("Score is "
                        + compare.getSimilarity(newProfile));
                break;

            }

        } catch (Exception e) {
            e.printStackTrace();
            // throw new TikaException("");
        }
    }

  
    /**
     * Inner class that describes a NGram
     */
    static class NGramEntry implements Comparable<NGramEntry> {

        /** The NGRamProfile this NGram is related to */
        private LanguageProfilerBuilder profile = null;

        /** The sequence of characters of the ngram */
        CharSequence seq = null;

        /** The number of occurences of this ngram in its profile */
        private int count = 0;

        /** The frequency of this ngram in its profile */
        private float frequency = 0.0F;

        /**
         * Constructs a new NGramEntry
         * 
         * @param seq is the sequence of characters of the ngram
         */
        public NGramEntry(CharSequence seq) {
            this.seq = seq;
        }

        /**
         * Constructs a new NGramEntry
         * 
         * @param seq is the sequence of characters of the ngram
         * @param count is the number of occurrences of this ngram
         */
        public NGramEntry(String seq, int count) {
            this.seq = new StringBuffer(seq).subSequence(0, seq.length());
            this.count = count;
        }

        /**
         * Returns the number of occurrences of this ngram in its profile
         * 
         * @return the number of occurrences of this ngram in its profile
         */
        public int getCount() {
            return count;
        }

        /**
         * Returns the frequency of this ngram in its profile
         * 
         * @return the frequency of this ngram in its profile
         */
        public float getFrequency() {
            return frequency;
        }

        /**
         * Returns the sequence of characters of this ngram
         * 
         * @return the sequence of characters of this ngram
         */
        public CharSequence getSeq() {
            return seq;
        }

        /**
         * Returns the size of this ngram
         * 
         * @return the size of this ngram
         */
        public int size() {
            return seq.length();
        }

        // Inherited JavaDoc
        public int compareTo(NGramEntry ngram) {
            int diff = Float.compare(ngram.getFrequency(), frequency);
            if (diff != 0) {
                return diff;
            } else {
                return (toString().compareTo(ngram.toString()));
            }
        }

        /**
         * Increments the number of occurrences of this ngram.
         */
        public void inc() {
            count++;
        }

        /**
         * Associated a profile to this ngram
         * 
         * @param profile
         *            is the profile associated to this ngram
         */
        public void setProfile(LanguageProfilerBuilder profile) {
            this.profile = profile;
        }

        /**
         * Returns the profile associated to this ngram
         * 
         * @return the profile associated to this ngram
         */
        public LanguageProfilerBuilder getProfile() {
            return profile;
        }

        // Inherited JavaDoc
        public String toString() {
            return seq.toString();
        }

        // Inherited JavaDoc
        public int hashCode() {
            return seq.hashCode();
        }

        // Inherited JavaDoc
        public boolean equals(Object obj) {

            NGramEntry ngram = null;
            try {
                ngram = (NGramEntry) obj;
                return ngram.seq.equals(seq);
            } catch (Exception e) {
                return false;
            }
        }

    }

    private static class QuickStringBuffer implements CharSequence {

        private char value[];

        private int count;

        QuickStringBuffer() {
            this(16);
        }

        QuickStringBuffer(char[] value) {
            this.value = value;
            count = value.length;
        }

        QuickStringBuffer(int length) {
            value = new char[length];
        }

        QuickStringBuffer(String str) {
            this(str.length() + 16);
            append(str);
        }

        public int length() {
            return count;
        }

        private void expandCapacity(int minimumCapacity) {
            int newCapacity = (value.length + 1) * 2;
            if (newCapacity < 0) {
                newCapacity = Integer.MAX_VALUE;
            } else if (minimumCapacity > newCapacity) {
                newCapacity = minimumCapacity;
            }

            char newValue[] = new char[newCapacity];
            System.arraycopy(value, 0, newValue, 0, count);
            value = newValue;
        }

        QuickStringBuffer clear() {
            count = 0;
            return this;
        }

        public char charAt(int index) {
            return value[index];
        }

        QuickStringBuffer append(String str) {
            if (str == null) {
                str = String.valueOf(str);
            }

            int len = str.length();
            int newcount = count + len;
            if (newcount > value.length) {
                expandCapacity(newcount);
            }
            str.getChars(0, len, value, count);
            count = newcount;
            return this;
        }

        QuickStringBuffer append(char c) {
            int newcount = count + 1;
            if (newcount > value.length) {
                expandCapacity(newcount);
            }
            value[count++] = c;
            return this;
        }

        public CharSequence subSequence(int start, int end) {
            return new String(value, start, end - start);
        }

        public String toString() {
            return new String(this.value);
        }
    }
}
"
tika-core/src/main/java/org/apache/tika/language/package-info.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Language detection.
 */
@aQute.bnd.annotation.Version("1.0.0")
package org.apache.tika.language;
"
tika-core/src/main/java/org/apache/tika/language/ProfilingHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.language;

import org.apache.tika.sax.WriteOutContentHandler;

/**
 * SAX content handler that builds a language profile based on all the
 * received character content.
 *
 * @since Apache Tika 0.5
 */
public class ProfilingHandler extends WriteOutContentHandler {

    private final ProfilingWriter writer;

    public ProfilingHandler(ProfilingWriter writer) {
        super(writer);
        this.writer = writer;
    }

    public ProfilingHandler(LanguageProfile profile) {
        this(new ProfilingWriter(profile));
    }

    public ProfilingHandler() {
        this(new ProfilingWriter());
    }

    /**
     * Returns the language profile being built by this content handler.
     * Note that the returned profile gets updated whenever new SAX events
     * are received by this content handler. Use the {@link #getLanguage()}
     * method to get the language that best matches the current state of
     * the profile.
     *
     * @return language profile
     */
    public LanguageProfile getProfile() {
        return writer.getProfile();
    }

    /**
     * Returns the language that best matches the current state of the
     * language profile.
     *
     * @return language that best matches the current profile
     */
    public LanguageIdentifier getLanguage() {
        return writer.getLanguage();
    }

}
"
tika-core/src/main/java/org/apache/tika/language/ProfilingWriter.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.language;

import java.io.IOException;
import java.io.Writer;

/**
 * Writer that builds a language profile based on all the written content.
 *
 * @since Apache Tika 0.5
 */
public class ProfilingWriter extends Writer {

    private final LanguageProfile profile;

    private char[] buffer = new char[] { 0, 0, '_' };

    private int n = 1;

    public ProfilingWriter(LanguageProfile profile) {
        this.profile = profile;
    }

    public ProfilingWriter() {
        this(new LanguageProfile());
    }

    /**
     * Returns the language profile being built by this writer. Note that
     * the returned profile gets updated whenever new characters are written.
     * Use the {@link #getLanguage()} method to get the language that best
     * matches the current state of the profile.
     *
     * @return language profile
     */
    public LanguageProfile getProfile() {
        return profile;
    }

    /**
     * Returns the language that best matches the current state of the
     * language profile.
     *
     * @return language that best matches the current profile
     */
    public LanguageIdentifier getLanguage() {
        return new LanguageIdentifier(profile);
    }

    @Override
    public void write(char[] cbuf, int off, int len) {
        for (int i = 0; i < len; i++) {
            char c = Character.toLowerCase(cbuf[off + i]);
            if (Character.isLetter(c)) {
                addLetter(c);
            } else {
                addSeparator();
            }
        }
    }

    private void addLetter(char c) {
        System.arraycopy(buffer, 1, buffer, 0, buffer.length - 1);
        buffer[buffer.length - 1] = c;
        n++;
        if (n >= buffer.length) {
            profile.add(new String(buffer));
        }
    }

    private void addSeparator() {
        addLetter('_');
        n = 1;
    }

    @Override
    public void close() throws IOException {
        addSeparator();
    }

    /**
     * Ignored.
     */
    @Override
    public void flush() {
    }

}
"
tika-core/src/main/java/org/apache/tika/language/translate/DefaultTranslator.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.language.translate;

import org.apache.tika.config.ServiceLoader;
import java.util.Collections;
import java.util.Comparator;
import java.util.List;

public class DefaultTranslator implements Translator{
    private transient final ServiceLoader loader;

    public DefaultTranslator(ServiceLoader loader) {
        this.loader = loader;
    }

    /**
     * Finds all statically loadable translators and sort the list by name,
     * rather than discovery order.
     *
     * @param loader service loader
     * @return ordered list of statically loadable parsers
     */
    private static List<Translator> getDefaultTranslators(ServiceLoader loader) {
        List<Translator> translators = loader.loadStaticServiceProviders(Translator.class);
        Collections.sort(translators, new Comparator<Translator>() {
            public int compare(Translator t1, Translator t2) {
                String n1 = t1.getClass().getName();
                String n2 = t2.getClass().getName();
                boolean tika1 = n1.startsWith("org.apache.tika.");
                boolean tika2 = n2.startsWith("org.apache.tika.");
                if (tika1 == tika2) {
                    return n1.compareTo(n2);
                } else if (tika1) {
                    return -1;
                } else {
                    return 1;
                }
            }
        });
        return translators;
    }

    public String translate(String text, String sourceLanguage, String targetLanguage) throws Exception {
        return getDefaultTranslators(loader).get(0).translate(text, sourceLanguage, targetLanguage);
    }

    public String translate(String text, String targetLanguage) throws Exception {
        return getDefaultTranslators(loader).get(0).translate(text, targetLanguage);
    }

    public boolean isAvailable() {
        return getDefaultTranslators(loader).get(0).isAvailable();
    }

}
"
tika-core/src/main/java/org/apache/tika/language/translate/Translator.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.language.translate;

/**
 * Interface for Translator services.
 * @since Tika 1.6
 */
public interface Translator {
    /**
     * Translate text between given languages. The following languages are supported:
     * Arabic("ar"), Bulgarian("bg"), Catalan("ca"), Chinese-Simplified("zh-CHS"), Chinese-Traditional("zh-CHT"),
     * Czech("cs"), Danish("da"), Dutch("nl"), English("en"), Estonian("et"),  Innish("fi"), French("fr"), German("de"),
     * Greek("el"), Haitian-Creole("ht"), Hebrew("he"), Hindi("hi"), Hmong-Daw("mww"), Hungarian("hu"),
     * Indonesian("id"), Italian("it"), Japanese("ja"), Korean("ko"), Latvian("lv"), Lithuanian("lt"), Malay("ms"),
     * Norwegian("no"), Persian("fa"), Polish("pl"), Portuguese("pt"), Romanian("ro"), Russian("ru"), Slovak("sk"),
     * Slovenian("sl"), Spanish("es"), Swedish("sv"), Thai("th"), Turkish("tr"), Ukranian("uk"), Urdu("ur"),
     * Vietnemese("vi").
     * @param text The text to translate.
     * @param sourceLanguage The input text language (for example, "en").
     * @param targetLanguage The desired language to translate to (for example, "fr").
     * @return The translation result. If translation is unavailable, returns the same text back.
     * @throws Exception When there is an error with the API call.
     * @since Tika 1.6
     */
    public String translate(String text, String sourceLanguage, String targetLanguage) throws Exception;

    /**
     * Translate text to the given language. This method attempts to auto-detect the source language of the text.
     * The following languages are supported:
     * Arabic("ar"), Bulgarian("bg"), Catalan("ca"), Chinese-Simplified("zh-CHS"), Chinese-Traditional("zh-CHT"),
     * Czech("cs"), Danish("da"), Dutch("nl"), English("en"), Estonian("et"),  Innish("fi"), French("fr"), German("de"),
     * Greek("el"), Haitian-Creole("ht"), Hebrew("he"), Hindi("hi"), Hmong-Daw("mww"), Hungarian("hu"),
     * Indonesian("id"), Italian("it"), Japanese("ja"), Korean("ko"), Latvian("lv"), Lithuanian("lt"), Malay("ms"),
     * Norwegian("no"), Persian("fa"), Polish("pl"), Portuguese("pt"), Romanian("ro"), Russian("ru"), Slovak("sk"),
     * Slovenian("sl"), Spanish("es"), Swedish("sv"), Thai("th"), Turkish("tr"), Ukranian("uk"), Urdu("ur"),
     * Vietnemese("vi").
     * @param text The text to translate.
     * @param targetLanguage The desired language to translate to (for example, "hi").
     * @return The translation result. If translation is unavailable, returns the same text back.
     * @throws Exception When there is an error with the API call.
     * @since Tika 1.6
     */
    public String translate(String text, String targetLanguage) throws Exception;

    /**
     * @return true if this Translator is probably able to translate right now.
     * @since Tika 1.6
     */
    public boolean isAvailable();
}
"
tika-core/src/main/java/org/apache/tika/metadata/ClimateForcast.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.metadata;

/**
 * Met keys from NCAR CCSM files in the <a
 * href="http://cf-pcmdi.llnl.gov/">Climate Forecast Convention</a>.
 */
public interface ClimateForcast {

    public static final String PROGRAM_ID = "prg_ID";

    public static final String COMMAND_LINE = "cmd_ln";

    public static final String HISTORY = "history";

    public static final String TABLE_ID = "table_id";

    public static final String INSTITUTION = "institution";

    public static final String SOURCE = "source";

    public static final String CONTACT = "contact";

    public static final String PROJECT_ID = "project_id";

    public static final String CONVENTIONS = "Conventions";

    public static final String REFERENCES = "references";

    public static final String ACKNOWLEDGEMENT = "acknowledgement";

    public static final String REALIZATION = "realization";

    public static final String EXPERIMENT_ID = "experiment_id";

    public static final String COMMENT = "comment";

    public static final String MODEL_NAME_ENGLISH = "model_name_english";

}
"
tika-core/src/main/java/org/apache/tika/metadata/CreativeCommons.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.metadata;

/**
 * A collection of Creative Commons properties names.
 * 
 * @see <a href="http://www.creativecommons.org/">creativecommons.org</a>
 */
public interface CreativeCommons {

    String LICENSE_URL = "License-Url";

    String LICENSE_LOCATION = "License-Location";

    String WORK_TYPE = "Work-Type";

}
"
tika-core/src/main/java/org/apache/tika/metadata/DublinCore.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.metadata;

/**
 * A collection of Dublin Core metadata names.
 *
 * @see <a href="http://dublincore.org">dublincore.org</a>
 */
public interface DublinCore {

    public static final String NAMESPACE_URI_DC = "http://purl.org/dc/elements/1.1/";
    public static final String NAMESPACE_URI_DC_TERMS = "http://purl.org/dc/terms/";
    public static final String PREFIX_DC = "dc";
    public static final String PREFIX_DC_TERMS = "dcterms";

    /**
     * Typically, Format may include the media-type or dimensions of the
     * resource. Format may be used to determine the software, hardware or
     * other equipment needed to display or operate the resource. Examples
     * of dimensions include size and duration. Recommended best practice is
     * to select a value from a controlled vocabulary (for example, the list
     * of Internet Media Types [MIME] defining computer media formats).
     */
	Property FORMAT = Property.internalText(
    		PREFIX_DC + Metadata.NAMESPACE_PREFIX_DELIMITER + "format");

    /**
     * Recommended best practice is to identify the resource by means of
     * a string or number conforming to a formal identification system.
     * Example formal identification systems include the Uniform Resource
     * Identifier (URI) (including the Uniform Resource Locator (URL)),
     * the Digital Object Identifier (DOI) and the International Standard
     * Book Number (ISBN).
     */
	Property IDENTIFIER = Property.internalText(
    		PREFIX_DC + Metadata.NAMESPACE_PREFIX_DELIMITER + "identifier");

    /**
     * Date on which the resource was changed.
     */
	Property MODIFIED = Property.internalDate(
		PREFIX_DC_TERMS + Metadata.NAMESPACE_PREFIX_DELIMITER + "modified");

    /**
     * An entity responsible for making contributions to the content of the
     * resource. Examples of a Contributor include a person, an organisation,
     * or a service. Typically, the name of a Contributor should be used to
     * indicate the entity.
     */
	Property CONTRIBUTOR = Property.internalTextBag(
    		PREFIX_DC + Metadata.NAMESPACE_PREFIX_DELIMITER + "contributor");

    /**
     * The extent or scope of the content of the resource. Coverage will
     * typically include spatial location (a place name or geographic
     * coordinates), temporal period (a period label, date, or date range)
     * or jurisdiction (such as a named administrative entity). Recommended
     * best practice is to select a value from a controlled vocabulary (for
     * example, the Thesaurus of Geographic Names [TGN]) and that, where
     * appropriate, named places or time periods be used in preference to
     * numeric identifiers such as sets of coordinates or date ranges.
     */
	Property COVERAGE = Property.internalText(
    		PREFIX_DC + Metadata.NAMESPACE_PREFIX_DELIMITER + "coverage");

    /**
     * An entity primarily responsible for making the content of the resource.
     * Examples of a Creator include a person, an organisation, or a service.
     * Typically, the name of a Creator should be used to indicate the entity.
     */
	Property CREATOR = Property.internalTextBag(
    		PREFIX_DC + Metadata.NAMESPACE_PREFIX_DELIMITER + "creator");

    /**
     * Date of creation of the resource.
     */
        Property CREATED = Property.internalDate(
                PREFIX_DC_TERMS + Metadata.NAMESPACE_PREFIX_DELIMITER + "created");

    /**
     * A date associated with an event in the life cycle of the resource.
     * Typically, Date will be associated with the creation or availability of
     * the resource. Recommended best practice for encoding the date value is
     * defined in a profile of ISO 8601 [W3CDTF] and follows the YYYY-MM-DD
     * format.
     */
	Property DATE = Property.internalDate(
    		PREFIX_DC + Metadata.NAMESPACE_PREFIX_DELIMITER + "date");

    /**
     * An account of the content of the resource. Description may include
     * but is not limited to: an abstract, table of contents, reference to
     * a graphical representation of content or a free-text account of
     * the content.
     */
	Property DESCRIPTION = Property.internalText(
    		PREFIX_DC + Metadata.NAMESPACE_PREFIX_DELIMITER + "description");

    /**
     * A language of the intellectual content of the resource. Recommended
     * best practice is to use RFC 3066 [RFC3066], which, in conjunction
     * with ISO 639 [ISO639], defines two- and three-letter primary language
     * tags with optional subtags. Examples include "en" or "eng" for English,
     * "akk" for Akkadian, and "en-GB" for English used in the United Kingdom.
     */
	Property LANGUAGE = Property.internalText(
    		PREFIX_DC + Metadata.NAMESPACE_PREFIX_DELIMITER + "language");

    /**
     * An entity responsible for making the resource available. Examples of
     * a Publisher include a person, an organisation, or a service. Typically,
     * the name of a Publisher should be used to indicate the entity.
     */
	Property PUBLISHER = Property.internalText(
    		PREFIX_DC + Metadata.NAMESPACE_PREFIX_DELIMITER + "publisher");

    /**
     * A reference to a related resource. Recommended best practice is to
     * reference the resource by means of a string or number conforming to
     * a formal identification system.
     */
	Property RELATION = Property.internalText(
    		PREFIX_DC + Metadata.NAMESPACE_PREFIX_DELIMITER + "relation");

    /**
     * Information about rights held in and over the resource. Typically,
     * a Rights element will contain a rights management statement for
     * the resource, or reference a service providing such information.
     * Rights information often encompasses Intellectual Property Rights
     * (IPR), Copyright, and various Property Rights. If the Rights element
     * is absent, no assumptions can be made about the status of these and
     * other rights with respect to the resource.
     */
	Property RIGHTS = Property.internalText(
    		PREFIX_DC + Metadata.NAMESPACE_PREFIX_DELIMITER + "rights");

    /**
     * A reference to a resource from which the present resource is derived.
     * The present resource may be derived from the Source resource in whole
     * or in part. Recommended best practice is to reference the resource by
     * means of a string or number conforming to a formal identification
     * system.
     */
	Property SOURCE = Property.internalText(
    		PREFIX_DC + Metadata.NAMESPACE_PREFIX_DELIMITER + "source");

    /**
     * The topic of the content of the resource. Typically, a Subject will
     * be expressed as keywords, key phrases or classification codes that
     * describe a topic of the resource. Recommended best practice is to
     * select a value from a controlled vocabulary or formal classification
     * scheme.
     */
	Property SUBJECT = Property.internalTextBag(
    		PREFIX_DC + Metadata.NAMESPACE_PREFIX_DELIMITER + "subject");

    /**
     * A name given to the resource. Typically, a Title will be a name by
     * which the resource is formally known.
     */
	Property TITLE = Property.internalText(
    		PREFIX_DC + Metadata.NAMESPACE_PREFIX_DELIMITER + "title");

    /**
     * The nature or genre of the content of the resource. Type includes terms
     * describing general categories, functions, genres, or aggregation levels
     * for content. Recommended best practice is to select a value from a
     * controlled vocabulary (for example, the DCMI Type Vocabulary
     * [DCMITYPE]). To describe the physical or digital manifestation of
     * the resource, use the Format element.
     */
	Property TYPE = Property.internalText(
    		PREFIX_DC + Metadata.NAMESPACE_PREFIX_DELIMITER + "type");

}
"
tika-core/src/main/java/org/apache/tika/metadata/Geographic.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.metadata;

/**
 * Geographic schema. This is a collection of 
 * {@link Property property definition} constants for geographic
 * information, as defined in the W3C Geo Vocabularies.
 *
 * @since Apache Tika 0.8
 * @see <a href="http://www.w3.org/2003/01/geo/"
 *        >W3C Basic Geo Vocabulary</a>
 */
public interface Geographic {

    /**
     * The WGS84 Latitude of the Point
     */
    Property LATITUDE =
        Property.internalReal("geo:lat");

    /**
     * The WGS84 Longitude of the Point
     */
    Property LONGITUDE =
        Property.internalReal("geo:long");

    /**
     * The WGS84 Altitude of the Point
     */
    Property ALTITUDE =
        Property.internalReal("geo:alt");

}
"
tika-core/src/main/java/org/apache/tika/metadata/HttpHeaders.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.metadata;

/**
 * A collection of HTTP header names.
 * 
 * @see <a href="http://rfc-ref.org/RFC-TEXTS/2616/">Hypertext Transfer Protocol --
 *      HTTP/1.1 (RFC 2616)</a>
 */
public interface HttpHeaders {

    String CONTENT_ENCODING = "Content-Encoding";

    String CONTENT_LANGUAGE = "Content-Language";

    String CONTENT_LENGTH = "Content-Length";

    String CONTENT_LOCATION = "Content-Location";

    String CONTENT_DISPOSITION = "Content-Disposition";

    String CONTENT_MD5 = "Content-MD5";

    String CONTENT_TYPE = "Content-Type";

    Property LAST_MODIFIED = 
        Property.internalDate("Last-Modified");

    String LOCATION = "Location";

}
"
tika-core/src/main/java/org/apache/tika/metadata/IPTC.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 * IPTC Metadata Descriptions taken from the IPTC Photo Metadata (July 2010) 
 * standard. These parts Copyright 2010 International Press Telecommunications 
 * Council.
 */
package org.apache.tika.metadata;

/**
 * IPTC photo metadata schema.
 * 
 * A collection of
 * {@link Property property definition} constants for the photo metadata
 * properties defined in the IPTC standard.
 * 
 * @since Apache Tika 1.1
 * @see <a href="http://www.iptc.org/std/photometadata/specification/IPTC-PhotoMetadata-201007_1.pdf">IPTC Photo Metadata</a>
 */
public interface IPTC {

   String NAMESPACE_URI_IPTC_CORE = "http://iptc.org/std/Iptc4xmpCore/1.0/xmlns/";
   String NAMESPACE_URI_IPTC_EXT = "http://iptc.org/std/Iptc4xmpExt/2008-02-29/";
   String NAMESPACE_URI_PLUS = "http://ns.useplus.org/ldf/xmp/1.0/";

   String PREFIX_IPTC_CORE = "Iptc4xmpCore";
   String PREFIX_IPTC_EXT = "Iptc4xmpExt";
   String PREFIX_PLUS = "plus";

   /**
    * Name of the city the content is focussing on -- either the place shown
    * in visual media or referenced by text or audio media. This element is at
    * the third level of a top-down geographical hierarchy.
    * <p>
    * This is a detail of a location with blurred semantics as it does not
    * clearly indicate whether it is the location in the image or the location
    * the photo was taken - which can be different. Two more concise properties
    * are available in IPTC Extension with Location Created and Location Shown
    * in the Image.
    * <p>
    * Maps to this IIM property: 2:90 City
    * 
    * @see Photoshop#CITY
    */
   Property CITY = Photoshop.CITY;

   /**
    * Full name of the country the content is focussing on -- either the
    * country shown in visual media or referenced in text or audio media. This
    * element is at the top/first level of a top- down geographical hierarchy.
    * The full name should be expressed as a verbal name and not as a code, a
    * code should go to the element "CountryCode"
    * <p>
    * This is a detail of a location with blurred semantics as it does not
    * clearly indicate whether it is the location in the image or the location
    * the photo was taken - which can be different. Two more concise properties
    * are available in IPTC Extension with Location Created and Location Shown
    * in the Image.
    * <p>
    * Maps to this IIM property: 2:101 Country/Primary Location Name
    * 
    * @see Photoshop#COUNTRY
    */
   Property COUNTRY = Photoshop.COUNTRY;

   /**
    * Code of the country the content is focussing on -- either the country
    * shown in visual media or referenced in text or audio media. This element
    * is at the top/first level of a top-down geographical hierarchy. The code
    * should be taken from ISO 3166 two or three letter code. The full name of
    * a country should go to the "Country" element.
    * <p>
    * This is a detail of a location with blurred semantics as it does not
    * clearly indicate whether it is the location in the image or the location
    * the photo was taken - which can be different. Two more concise properties
    * are available in IPTC Extension with Location Created and Location Shown
    * in the Image.
    * <p>
    * Maps to this IIM property: 2:100 Country/Primary Location Code
    */
   Property COUNTRY_CODE = Property.internalText(
         PREFIX_IPTC_CORE + Metadata.NAMESPACE_PREFIX_DELIMITER + "CountryCode");

   /**
    * A textual description, including captions, of the item's content,
    * particularly used where the object is not text.
    * <p>
    * Note: the XMP property (dc:description) which stores the value of this
    * IPTC Core property is of type Lang Alt. Hence any software agent dealing
    * with this property must abide to the processing rules for
    * Lang Alt value type as specified by the XMP specifications.
    * <p>
    * Maps to this IIM property: 2:120 Caption/Abstract
    * 
    * @see DublinCore#DESCRIPTION
    */
   Property DESCRIPTION = DublinCore.DESCRIPTION;

   /**
    * A brief synopsis of the caption. Headline is not the same as Title.
    * <p>
    * Maps to this IIM property: 2:105 Headline
    * 
    * @see Photoshop#HEADLINE
    */
   Property HEADLINE = Photoshop.HEADLINE;

   /**
    * Describes the nature, intellectual, artistic or journalistic
    * characteristic of a item, not specifically its content.
    * <p>
    * The IPTC recognizes that the corresponding IPTC Genre NewsCodes needs
    * photo specific extension to be better usable with this field (as of the
    * release of this standard in the year 2008).
    * <p>
    * Maps to this IIM property: 2:04 Object Attribute Reference
    */
   Property INTELLECTUAL_GENRE = Property.internalText(
         PREFIX_IPTC_CORE + Metadata.NAMESPACE_PREFIX_DELIMITER + "IntellectualGenre");

   /**
    * Keywords to express the subject of the content. Keywords may be free
    * text and don't have to be taken from a controlled vocabulary. Codes from
    * the controlled vocabulary IPTC Subject NewsCodes must go to the
    * "Subject Code" field.
    * <p>
    * Single values of this field should not be restricted to single words
    * but must allow for phrases as well.
    * <p>
    * Maps to this IIM property: 2:25 Keywords
    * 
    * @see DublinCore#SUBJECT
    */
   Property KEYWORDS = DublinCore.SUBJECT;

   /**
    * Name of the subregion of a country -- either called province or state or
    * anything else -- the content is focussing on -- either the subregion
    * shown in visual media or referenced by text or audio media. This element
    * is at the second level of a top-down geographical hierarchy.
    * <p>
    * This is a detail of a location with blurred semantics as it does not
    * clearly indicate whether it is the location in the image or the location
    * the photo was taken - which can be different. Two more concise properties
    * are available in IPTC Extension with Location Created and Location Shown
    * in the Image.
    * <p>
    * Maps to this IIM property: 2:95 Province/State
    * 
    * @see Photoshop#STATE
    */
   Property PROVINCE_OR_STATE = Photoshop.STATE;

   /**
    * Describes the scene of a news content. Specifies one or more terms
    * from the IPTC "Scene-NewsCodes". Each Scene is represented as a string of
    * 6 digits in an unordered list.
    * <p>
    * Note: Only Scene values from this IPTC taxonomy should be used here. More
    * about the IPTC Scene-NewsCodes at www.newscodes.org.
    */
   Property SCENE_CODE = Property.internalTextBag(
         PREFIX_IPTC_CORE + Metadata.NAMESPACE_PREFIX_DELIMITER + "Scene");

   /**
    * Specifies one or more Subjects from the IPTC Subject-NewsCodes taxonomy
    * to categorise the content. Each Subject is represented as a string of 8
    * digits in an unordered list.
    * <p>
    * Note: Only Subjects from a controlled vocabulary should be used here,
    * free text has to be put into the Keyword element. More about
    * IPTC Subject-NewsCodes at www.newscodes.org.
    */
   Property SUBJECT_CODE = Property.internalTextBag(
         PREFIX_IPTC_CORE + Metadata.NAMESPACE_PREFIX_DELIMITER + "SubjectCode");

   /**
    * Name of a sublocation the content is focussing on -- either the
    * location shown in visual media or referenced by text or audio media. This
    * location name could either be the name of a sublocation to a city or the
    * name of a well known location or (natural) monument outside a city. In
    * the sense of a sublocation to a city this element is at the fourth level
    * of a top-down geographical hierarchy.
    * <p>
    * This is a detail of a location with blurred semantics as it does not
    * clearly indicate whether it is the location in the image or the location
    * the photo was taken - which can be different. Two more concise properties
    * are available in IPTC Extension with Location Created and Location Shown
    * in the Image.
    * <p>
    * Maps to this IIM property: 2:92 Sublocation
    */
   Property SUBLOCATION = Property.internalText(
         PREFIX_IPTC_CORE + Metadata.NAMESPACE_PREFIX_DELIMITER + "Location");

   /**
    * Designates the date and optionally the time the intellectual content was
    * created rather than the date of the creation of the physical
    * representation.
    * <p>
    * If a software system requires explicit time values and no time is given
    * by the Date Created property the software system should default the time
    * to 00:00:00. If the software system does not require an explicit time
    * value the time part should be left empty as it is.
    * <p>
    * Note 1: Any content of the IIM dataset 2:60, Time Created, should be
    * merged to this element.
    * Note 2: Implementers are encouraged to provide
    * the creation date and time from the EXIF data of a digital
    * camera to the user for entering this date for the first time.
    * <p>
    * Maps to this IIM property: 2:55 Date Created
    * 
    * @see Photoshop#DATE_CREATED
    */
   Property DATE_CREATED = Photoshop.DATE_CREATED;

   /**
    * Identifier or the name of the person involved in writing, editing or
    * correcting the description of the content.
    * <p>
    * Maps to this IIM property: 2:122 Writer/Editor
    * 
    * @see Photoshop#CAPTION_WRITER
    */
   Property DESCRIPTION_WRITER = Photoshop.CAPTION_WRITER;

   /**
    * Any of a number of instructions from the provider or creator to the
    * receiver of the item.
    * <p>
    * Maps to this IIM property: 2:40 Special Instruction
    * 
    * @see Photoshop#INSTRUCTIONS
    */
   Property INSTRUCTIONS = Photoshop.INSTRUCTIONS;

   /**
    * Number or identifier for the purpose of improved workflow handling. This
    * is a user created identifier related to the job for which the item is
    * supplied.
    * <p>
    * Note: As this identifier references a job of the receiver's workflow it
    * must first be issued by the receiver, then transmitted to the creator or
    * provider of the news object and finally added by the creator
    * to this field.
    * <p>
    * Maps to this IIM property: 2:103 Original Transmission Reference
    * 
    * @see Photoshop#TRANSMISSION_REFERENCE
    */
   Property JOB_ID = Photoshop.TRANSMISSION_REFERENCE;

   /**
    * A shorthand reference for the item. Title provides a short human readable
    * name which can be a text and/or numeric reference. It is not the same as
    * Headline.
    * <p>
    * Many use the Title field to store the filename of the image, though the
    * field may be used in many ways. Formal identifiers are provided by the
    * Digital Image Id, or the Registry Entry property of the IPTC Extension.
    * <p>
    * Note 1: This element aligns with the use of Dublin Core's "Title"
    * element.
    * Note 2: the XMP property (dc:title) which stores the value of
    * this IPTC Core property is of type Lang Alt. Hence any software agent
    * dealing with this property must abide to the processing rules for Lang
    * Alt value type as specified by the XMP specifications.
    * <p>
    * Maps to this IIM property: 2:05 Object Name
    * 
    * @see DublinCore#TITLE
    */
   Property TITLE = DublinCore.TITLE;

   /**
    * Contains any necessary copyright notice for claiming the intellectual
    * property for this item and should identify the current owner of the
    * copyright for the item. Other entities like the creator of the item may
    * be added in the corresponding field. Notes on usage rights should be
    * provided in "Rights usage terms".
    * <p>
    * Copyright ownership can be expressed in a more controlled way using the
    * PLUS fields "Copyright Owner", "Copyright Owner ID",
    * "Copyright Owner Name" of the IPTC Extension. It is the user's
    * responsibility to keep the values of the four fields in sync.
    * <p>
    * Note: the XMP property (dc:rights) which stores the value of this IPTC
    * Core property is of type Lang Alt. Hence any software agent dealing with
    * this property must abide to the processing rules for Lang Alt
    * value type as specified by the XMP specifications.
    * <p>
    * Maps to this IIM property: 2:116 Copyright Notice
    * 
    * @see DublinCore#RIGHTS
    */
   Property COPYRIGHT_NOTICE = DublinCore.RIGHTS;

   /**
    * Contains the name of the person who created the content of this item, a
    * photographer for photos, a graphic artist for graphics, or a writer for
    * textual news, but in cases where the photographer should not be
    * identified the name of a company or organisation may be appropriate.
    * <p>
    * The creator can be expressed in a more controlled way using the
    * "Image Creator" of PLUS in the IPTC Extension additionally. It is the
    * user's responsibility to keep the values of the IPTC Core and the PLUS
    * fields in sync.
    * <p>
    * Maps to this IIM property: 2:80 By-line
    * 
    * @see DublinCore#CREATOR
    */
   Property CREATOR = DublinCore.CREATOR;

   /**
    * The creator's contact information provides all necessary information to
    * get in contact with the creator of this item and comprises a set of
    * sub-properties for proper addressing.
    * <p>
    * The IPTC Extension Licensor fields should be used instead of these
    * Creator's Contact Info fields if you are using IPTC Extension fields. If
    * the creator is also the licensor his or her contact information should be
    * provided in the Licensor fields.
    * <p>
    * Note 1 to user interface implementers: All sub-properties of "Creator's
    * contact information" should be shown as group on the form.
    * Note 2: the
    * CreatorContactInfo sub-properties' naming aligns with the vCard
    * specification RFC 2426.
    */
   Property CREATORS_CONTACT_INFO = Property.internalText(
         PREFIX_IPTC_CORE + Metadata.NAMESPACE_PREFIX_DELIMITER + "CreatorContactInfo");

   /**
    * Contains the job title of the person who created the content of this
    * item. As this is sort of a qualifier the Creator element has to be filled
    * in as mandatory prerequisite for using Creator's Jobtitle.
    * <p>
    * Maps to this IIM property: 2:85 By-line Title
    * 
    * @see Photoshop#AUTHORS_POSITION
    */
   Property CREATORS_JOB_TITLE = Photoshop.AUTHORS_POSITION;

   /**
    * The credit to person(s) and/or organisation(s) required by the supplier
    * of the item to be used when published. This is a free-text field.
    * <p>
    * Note 1: For more formal identifications of the creator or the owner of
    * the copyrights of this image other rights properties may be used.
    * Note 2:
    * This property was named "Credit" by the IIM metadata, then it was renamed
    * to "Provider" in IPTC Core 1.0. In IPTC Core 1.1. it has been renamed to
    * "Credit Line" as the field is used for this purpose by many users.
    * <p>
    * Maps to this IIM property: 2:110 Credit
    * 
    * @see Photoshop#CREDIT_LINE
    */
   Property CREDIT_LINE = Photoshop.CREDIT;

   /**
    * The licensing parameters of the item expressed in free-text.
    * <p>
    * The PLUS fields of the IPTC Extension can be used in parallel to express
    * the licensed usage in more controlled terms.
    */
   Property RIGHTS_USAGE_TERMS = XMPRights.USAGE_TERMS;

   /**
    * Identifies the original owner of the copyright for the intellectual
    * content of the item. This could be an agency, a member of an agency or an
    * individual. Source could be different from Creator and from the entities
    * in the CopyrightNotice.
    * <p>
    * The original owner can never change. For that reason the content of this
    * property should never be changed or deleted after the information is
    * entered following the news object's initial creation.
    * <p>
    * Maps to this IIM property: 2:115 Source
    * 
    * @see Photoshop#SOURCE
    */
   Property SOURCE = Photoshop.SOURCE;

   /**
    * The contact information address part. Comprises an optional company name
    * and all required information to locate the building or postbox to which
    * mail should be sent. To that end, the address is a multiline field.
    * <p>
    * Note 1: to user interface implementers: This field should be part of a
    * "Contact information" group on the form.
    * Note 2: the ContactInfo naming aligns with the vCard specification RFC 2426.
    */
   Property CONTACT_INFO_ADDRESS = Property.internalTextBag(
         PREFIX_IPTC_CORE + Metadata.NAMESPACE_PREFIX_DELIMITER + "CiAdrExtadr");

   /**
    * The contact information city part.
    * <p>
    * Note 1: to user interface implementers: This field should be part of a
    * "Contact information" group on the form.
    * Note 2: the ContactInfo naming aligns with the vCard specification RFC 2426.
    */
   Property CONTACT_INFO_CITY = Property.internalText(
         PREFIX_IPTC_CORE + Metadata.NAMESPACE_PREFIX_DELIMITER + "CiAdrCity");

   /**
    * The contact information country part.
    * <p>
    * Note 1: to user interface implementers: This field should be part of a
    * "Contact information" group on the form.
    * Note 2: the ContactInfo naming aligns with the vCard specification RFC 2426.
    */
   Property CONTACT_INFO_COUNTRY = Property.internalText(
         PREFIX_IPTC_CORE + Metadata.NAMESPACE_PREFIX_DELIMITER + "CiAdrCtry");

   /**
    * The contact information email address part.
    * <p>
    * Multiple email addresses can be given. May have to be separated by a
    * comma in the user interface.
    * <p>
    * Note 1: to user interface implementers: This field should be part of a
    * "Contact information" group on the form.
    * Note 2 to user interface
    * implementers: provide sufficient space to fill in multiple e-mail
    * addresses.
    * Note 3: the ContactInfo naming aligns with the vCard
    * specification RFC 2426.
    */
   Property CONTACT_INFO_EMAIL = Property.internalTextBag(
         PREFIX_IPTC_CORE + Metadata.NAMESPACE_PREFIX_DELIMITER + "CiEmailWork");

   /**
    * The contact information phone number part.
    * <p>
    * Multiple numbers can be given. May have to be separated by a
    * comma in the user interface.
    * <p>
    * Note 1: to user interface implementers: This field should be part of a
    * "Contact information" group on the form.
    * Note 2 to user interface
    * implementers: provide sufficient space to fill in multiple international
    * numbers.
    * Note 3: the ContactInfo naming aligns with the vCard
    * specification RFC 2426.
    */
   Property CONTACT_INFO_PHONE = Property.internalTextBag(
         PREFIX_IPTC_CORE + Metadata.NAMESPACE_PREFIX_DELIMITER + "CiTelWork");

   /**
    * The contact information part denoting the local postal code.
    * <p>
    * Note 1: to user interface implementers: This field should be part of a
    * "Contact information" group on the form.
    * Note 2: the ContactInfo naming aligns with the vCard specification RFC 2426.
    */
   Property CONTACT_INFO_POSTAL_CODE = Property.internalText(
         PREFIX_IPTC_CORE + Metadata.NAMESPACE_PREFIX_DELIMITER + "CiAdrPcode");

   /**
    * The contact information part denoting regional information such as state or province.
    * <p>
    * Note 1: to user interface implementers: This field should be part of a
    * "Contact information" group on the form.
    * Note 2: the ContactInfo naming aligns with the vCard specification RFC 2426.
    */
   Property CONTACT_INFO_STATE_PROVINCE = Property.internalText(
         PREFIX_IPTC_CORE + Metadata.NAMESPACE_PREFIX_DELIMITER + "CiAdrRegion");

   /**
    * The contact information web address part. Multiple addresses can be given, separated by a comma.
    * <p>
    * Note 1: to user interface implementers: This field should be part of a
    * "Contact information" group on the form.
    * Note 2 to user interface
    * implementers: provide sufficient space to fill in multiple URLs.
    * Note 3: the ContactInfo naming aligns with the vCard
    * specification RFC 2426.
    */
   Property CONTACT_INFO_WEB_URL = Property.internalTextBag(
         PREFIX_IPTC_CORE + Metadata.NAMESPACE_PREFIX_DELIMITER + "CiUrlWork");

   /**
    * As this metadata element pertains to distribution management, it was not
    * adopted. However, this data is still synchronised with the XMP property
    * [photoshop:Urgency], and hence, available for future use, but outside the
    * IPTC Core.
    *
    * @deprecated
    */
   Property URGENCY = Photoshop.URGENCY;

   /**
    * As this metadata element was earmarked as deprecated already for IIM 4.1,
    * it was not adopted. However, this data is still synchronised with the XMP
    * property [photoshop:Category], and hence available for future use - but
    * outside the IPTC Core. For migrating from Category codes to Subject Codes
    * please read the Guideline for mapping Category Codes to Subject NewsCodes
    * section below.
    *
    * @deprecated
    */
   Property CATEGORY = Photoshop.CATEGORY;

   /**
    * As this metadata element was earmarked as deprecated already for IIM 4.1,
    * it was not adopted. However, this data is still synchronised with the XMP
    * property [photoshop:SupplementalCategories], and hence available for
    * future use - but outside the IPTC Core.
    *
    * @deprecated
    */
   Property SUPPLEMENTAL_CATEGORIES = Photoshop.SUPPLEMENTAL_CATEGORIES;

   /**
    * Information about the ethnicity and other facets of the model(s) in a
    * model-released image.
    * <p>
    * Use the Model Age field for the age of model(s).
    */
   Property ADDITIONAL_MODEL_INFO = Property.internalText(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "AddlModelInfo");

   /**
    * A set of metadata about artwork or an object in the item
    */
   Property ARTWORK_OR_OBJECT = Property.internalTextBag(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "ArtworkOrObject");

   /**
    * A set of metadata about artwork or an object in the item
    */
   Property ORGANISATION_CODE = Property.internalTextBag(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "OrganisationInImageCode");

   /**
    * A term to describe the content of the image by a value from a Controlled
    * Vocabulary.
    * <p>
    * This property is part of the Photo Metadata 2008 specifications, but
    * should not released to the public on the standard Adobe Custom Panels for
    * IPTC metadata or other user interfaces unless agreed by the IPTC.
    */
   Property CONTROLLED_VOCABULARY_TERM = Property.internalTextBag(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "CVterm");

   /**
    * A location the content of the item is about. For photos that is a
    * location shown in the image.
    * <p>
    * If the location the image was taken in is different from this location
    * the property Location Created should be used too.
    */
   Property LOCATION_SHOWN = Property.internalTextBag(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "LocationShown");

   /**
    * Age of the human model(s) at the time this image was taken in a model
    * released image.
    * <p>
    * The user should be aware of any legal implications of providing ages for
    * young models. Ages below 18 years should not be included.
    */
   Property MODEL_AGE = Property.internalTextBag(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "ModelAge");

   /**
    * Name of the organisation or company which is featured in the content.
    * <p>
    * May be supplemented by values from a controlled vocabulary in the
    * Organisation Code field.
    */
   Property ORGANISATION_NAME = Property.internalTextBag(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "OrganisationInImageName");

   /**
    * Name of a person the content of the item is about. For photos that is a
    * person shown in the image.
    */
   Property PERSON = Property.internalTextBag(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "PersonInImage");

   /**
    * Globally unique identifier for the item. It is created and applied by the
    * creator of the item at the time of its creation . This value shall not be
    * changed after that time.
    * <p>
    * The identifier will probably be generated by the technical means of an
    * imaging device or software and should be applied to the digital image
    * file as early as possible in its life cycle. This identifier does not
    * identify any pictured content, particularly in case of a scan of non-
    * digital images, only this digital representation.
    * <p>
    * Any algorithm to create this identifier has to comply with the technical
    * requirements to create a globally unique id. Any device creating digital
    * images - e.g. still image cameras, video cameras, scanners - should
    * create such an identifer right at the time of the creation of the digital
    * data and add the id to the set of metadata without compromising
    * performance. It is recommended that this image identifier allows
    * identifying the device by which the image data and the GUID were created.
    * IPTC's basic requirements for unique ids are:
    * - It must be globally unique. Algorithms for this purpose exist.
    * - It should identify the camera body.
    * - It should identify each individual photo from this camera body.
    * - It should identify the date and time of the creation of the picture.
    * - It should be secured against tampering.
    * This field should be implemented in a way to prove it has not been changed since its value has
    * been applied. If the identifier has been created by the imaging device
    * its type and brand can be found in the Exif/technical metadata.
    */
   Property DIGITAL_IMAGE_GUID = Property.internalText(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "DigImageGUID");

   /**
    * The type of the source digital file.
    * <p>
    * The IPTC recommends not to implement this property any longer.
    *
    * @deprecated
    */
   Property DIGITAL_SOURCE_FILE_TYPE = Property.internalText(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "DigitalSourcefileType");

   /**
    * The type of the source of this digital image
    */
   Property DIGITAL_SOURCE_TYPE = Property.internalText(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "DigitalSourceType");

   /**
    * Names or describes the specific event the content relates to.
    * <p>
    * Examples are: a press conference, dedication ceremony, etc. If this is a
    * sub-event of a larger event both can be provided by the field: e.g. XXXIX
    * Olympic Summer Games (Beijing): opening ceremony. Unplanned events could
    * be named by this property too.
    */
   Property EVENT = Property.internalText(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "Event");

   /**
    * Both a Registry Item Id and a Registry Organisation Id to record any
    * registration of this item with a registry.
    * <p>
    * Typically an id from a registry is negotiated and applied after the
    * creation of the digital image.
    * <p>
    * Any user interface implementation must show both sub-properties - Item Id
    * and Organisation Id - as corresponding values. Further an input to both
    * fields should be made mandatory.
    */
   Property IMAGE_REGISTRY_ENTRY = Property.internalTextBag(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "RegistryId");

   /**
    * Identifies the most recent supplier of the item, who is not necessarily
    * its owner or creator.
    * <p>
    * For identifying the supplier either a well known and/or registered
    * company name or a URL of the company's web site may be used. This
    * property succeeds the Provider property of IPTC Core 1.0 by its semantics
    * as that Provider was renamed to Credit Line.
    * <p>
    * This is a PLUS version 1.2 property included in the IPTC Extension
    * schema.
    */
   Property IMAGE_SUPPLIER = Property.internalText(
         PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "ImageSupplier");

   /**
    * Identifies the most recent supplier of the item, who is not necessarily
    * its owner or creator.
    * <p>
    * For identifying the supplier either a well known and/or registered
    * company name or a URL of the company's web site may be used. This
    * property succeeds the Provider property of IPTC Core 1.0 by its semantics
    * as that Provider was renamed to Credit Line.
    * <p>
    * This is a PLUS version 1.2 property included in the IPTC Extension
    * schema.
    */
   Property IMAGE_SUPPLIER_ID = Property.composite(
           Property.internalText(
                   PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "ImageSupplierID"),
           new Property[] { Property.internalText(IPTC.IMAGE_SUPPLIER_ID_WRONG_CASE) });
   
   /** @deprecated use {@link IPTC#IMAGE_SUPPLIER_ID} */
   public static final String IMAGE_SUPPLIER_ID_WRONG_CASE = 
         PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "ImageSupplierId";

   /**
    * Identifies the most recent supplier of the item, who is not necessarily
    * its owner or creator.
    * <p>
    * For identifying the supplier either a well known and/or registered
    * company name or a URL of the company's web site may be used. This
    * property succeeds the Provider property of IPTC Core 1.0 by its semantics
    * as that Provider was renamed to Credit Line.
    * <p>
    * This is a PLUS version 1.2 property included in the IPTC Extension
    * schema.
    */
   Property IMAGE_SUPPLIER_NAME = Property.internalText(
         PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "ImageSupplierName");

   /**
    * Optional identifier assigned by the Image Supplier to the image.
    * <p>
    * This is a PLUS version 1.2 property included in the IPTC Extension
    * schema.
    */
   Property IMAGE_SUPPLIER_IMAGE_ID = Property.internalText(
         PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "ImageSupplierImageID");

   /**
    * The date and optionally time when any of the IPTC photo metadata fields
    * has been last edited
    * <p>
    * The public use of this property is deprecated by IPTC Extension version
    * 1.1. It may only still be used by a private user interface for a use
    * scoped to a company. If used this field should be a timestamp of the
    * latest change applied to any of the fields.
    * <p>
    * The value of this property should never be set by software. XMP-aware
    * software should reflect any changes to metadata by the xmp:MetadataDate
    * property of the XMP Basic scheme.
    */
   Property IPTC_LAST_EDITED = Property.internalDate(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "IptcLastEdited");

   /**
    * The location the content of the item was created.
    * <p>
    * If the location in the image is different from the location the photo was
    * taken the IPTC Extension property Location Shown in the Image should be
    * used.
    */
   Property LOCATION_CREATED = Property.internalTextBag(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "LocationCreated");

   /**
    * The maximum available height in pixels of the original photo from which
    * this photo has been derived by downsizing.
    */
   Property MAX_AVAIL_HEIGHT = Property.internalInteger(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "MaxAvailHeight");

   /**
    * The maximum available width in pixels of the original photo from which
    * this photo has been derived by downsizing.
    */
   Property MAX_AVAIL_WIDTH = Property.internalInteger(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "MaxAvailWidth");

   /**
    * The version number of the PLUS standards in place at the time of the
    * transaction.
    * <p>
    * This property was included into the IPTC Extension schema from PLUS
    * version 1.2 as all other PLUS properties. To reflect this the value of
    * "PLUS Version" should be set to the string "1.2.0"
    */
   Property PLUS_VERSION = Property.internalText(
         PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "Version");

   /**
    * Owner or owners of the copyright in the licensed image.
    * <p>
    * Serves to identify the rights holder/s for the image. The Copyright
    * Owner, Image Creator and Licensor may be the same or different entities.
    * <p>
    * This is a PLUS version 1.2 property included in the IPTC Extension
    * schema.
    */
   Property COPYRIGHT_OWNER = Property.internalTextBag(
         PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "CopyrightOwner");

   /**
    * The ID of the owner or owners of the copyright in the licensed image.
    * <p>
    * Serves to identify the rights holder/s for the image. The Copyright
    * Owner, Image Creator and Licensor may be the same or different entities.
    * <p>
    * This is a PLUS version 1.2 property included in the IPTC Extension
    * schema.
    */
   Property COPYRIGHT_OWNER_ID = Property.composite(
           Property.internalTextBag(
                   PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "CopyrightOwnerID"),
           new Property[] { Property.internalTextBag(IPTC.COPYRIGHT_OWNER_ID_WRONG_CASE) });
   
   /** @deprecated use {@link IPTC#COPYRIGHT_OWNER_ID} */
   public static final String COPYRIGHT_OWNER_ID_WRONG_CASE = 
         PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "CopyrightOwnerId";

   /**
    * The name of the owner or owners of the copyright in the licensed image.
    * <p>
    * Serves to identify the rights holder/s for the image. The Copyright
    * Owner, Image Creator and Licensor may be the same or different entities.
    * <p>
    * This is a PLUS version 1.2 property included in the IPTC Extension
    * schema.
    */
   Property COPYRIGHT_OWNER_NAME = Property.internalTextBag(
         PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "CopyrightOwnerName");

   /**
    * Creator or creators of the image.
    * <p>
    * The creator can be additionally expressed in free-text using the IPTC
    * Core Creator field. In many countries, the Image Creator must be
    * attributed in association with any use of the image. The Image Creator,
    * Copyright Owner, Image Supplier and Licensor may be the same or different
    * entities.
    * <p>
    * This is a PLUS version 1.2 property included in the IPTC Extension
    * schema.
    */
   Property IMAGE_CREATOR = Property.internalTextBag(
         PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "ImageCreator");

   /**
    * The ID of the creator or creators of the image.
    * <p>
    * The creator can be additionally expressed in free-text using the IPTC
    * Core Creator field. In many countries, the Image Creator must be
    * attributed in association with any use of the image. The Image Creator,
    * Copyright Owner, Image Supplier and Licensor may be the same or different
    * entities.
    * <p>
    * This is a PLUS version 1.2 property included in the IPTC Extension
    * schema.
    */
   Property IMAGE_CREATOR_ID = Property.composite(
           Property.internalTextBag(
                   PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "ImageCreatorID"),
           new Property[] { Property.internalTextBag(IPTC.IMAGE_CREATOR_ID_WRONG_CASE) });
   
   /** @deprecated use {@link IPTC#IMAGE_CREATOR_ID} */
   public static final String IMAGE_CREATOR_ID_WRONG_CASE = 
         PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "ImageCreatorId";

   /**
    * The name of the creator or creators of the image.
    * <p>
    * The creator can be additionally expressed in free-text using the IPTC
    * Core Creator field. In many countries, the Image Creator must be
    * attributed in association with any use of the image. The Image Creator,
    * Copyright Owner, Image Supplier and Licensor may be the same or different
    * entities.
    * <p>
    * This is a PLUS version 1.2 property included in the IPTC Extension
    * schema.
    */
   Property IMAGE_CREATOR_NAME = Property.internalTextBag(
         PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "ImageCreatorName");

   /**
    * A person or company that should be contacted to obtain a licence for
    * using the item or who has licensed the item.
    * <p>
    * This is a PLUS version 1.2 property included in the IPTC Extension
    * schema.
    */
   Property LICENSOR = Property.internalTextBag(
         PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "Licensor");

   /**
    * The ID of the person or company that should be contacted to obtain a licence for
    * using the item or who has licensed the item.
    * <p>
    * This is a PLUS version 1.2 property included in the IPTC Extension
    * schema.
    */
   Property LICENSOR_ID = Property.composite(
           Property.internalTextBag(
                   PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "LicensorID"),
           new Property[] { Property.internalTextBag(IPTC.LICENSOR_ID_WRONG_CASE) });
   
   /** @deprecated use {@link IPTC#LICENSOR_ID} */
   public static final String LICENSOR_ID_WRONG_CASE = 
         PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "LicensorId";

   /**
    * The name of the person or company that should be contacted to obtain a licence for
    * using the item or who has licensed the item.
    * <p>
    * This is a PLUS version 1.2 property included in the IPTC Extension
    * schema.
    */
   Property LICENSOR_NAME = Property.internalTextBag(
         PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "LicensorName");

   /**
    * The city of a person or company that should be contacted to obtain a licence for
    * using the item or who has licensed the item.
    * <p>
    * This is a PLUS version 1.2 property included in the IPTC Extension
    * schema.
    */
   Property LICENSOR_CITY = Property.internalTextBag(
         PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "LicensorCity");

   /**
    * The country of a person or company that should be contacted to obtain a licence for
    * using the item or who has licensed the item.
    * <p>
    * This is a PLUS version 1.2 property included in the IPTC Extension
    * schema.
    */
   Property LICENSOR_COUNTRY = Property.internalTextBag(
         PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "LicensorCountry");

   /**
    * The email of a person or company that should be contacted to obtain a licence for
    * using the item or who has licensed the item.
    * <p>
    * This is a PLUS version 1.2 property included in the IPTC Extension
    * schema.
    */
   Property LICENSOR_EMAIL = Property.internalTextBag(
         PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "LicensorEmail");

   /**
    * The extended address of a person or company that should be contacted to obtain a licence for
    * using the item or who has licensed the item.
    * <p>
    * This is a PLUS version 1.2 property included in the IPTC Extension
    * schema.
    */
   Property LICENSOR_EXTENDED_ADDRESS = Property.internalTextBag(
         PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "LicensorExtendedAddress");

   /**
    * The postal code of a person or company that should be contacted to obtain a licence for
    * using the item or who has licensed the item.
    * <p>
    * This is a PLUS version 1.2 property included in the IPTC Extension
    * schema.
    */
   Property LICENSOR_POSTAL_CODE = Property.internalTextBag(
         PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "LicensorPostalCode");

   /**
    * The region of a person or company that should be contacted to obtain a licence for
    * using the item or who has licensed the item.
    * <p>
    * This is a PLUS version 1.2 property included in the IPTC Extension
    * schema.
    */
   Property LICENSOR_REGION = Property.internalTextBag(
         PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "LicensorRegion");

   /**
    * The street address of a person or company that should be contacted to obtain a licence for
    * using the item or who has licensed the item.
    * <p>
    * This is a PLUS version 1.2 property included in the IPTC Extension
    * schema.
    */
   Property LICENSOR_STREET_ADDRESS = Property.internalTextBag(
         PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "LicensorStreetAddress");

   /**
    * The phone number of a person or company that should be contacted to obtain a licence for
    * using the item or who has licensed the item.
    * <p>
    * This is a PLUS version 1.2 property included in the IPTC Extension
    * schema.
    */
   Property LICENSOR_TELEPHONE_1 = Property.internalTextBag(
         PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "LicensorTelephone1");

   /**
    * The phone number of a person or company that should be contacted to obtain a licence for
    * using the item or who has licensed the item.
    * <p>
    * This is a PLUS version 1.2 property included in the IPTC Extension
    * schema.
    */
   Property LICENSOR_TELEPHONE_2 = Property.internalTextBag(
         PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "LicensorTelephone2");

   /**
    * The URL of a person or company that should be contacted to obtain a licence for
    * using the item or who has licensed the item.
    * <p>
    * This is a PLUS version 1.2 property included in the IPTC Extension
    * schema.
    */
   Property LICENSOR_URL = Property.internalTextBag(
         PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "LicensorURL");

   /**
    * Age of the youngest model pictured in the image, at the time that the
    * image was made.
    * <p>
    * This age should not be displayed to the public on open web portals and
    * the like. But it may be used by image repositories in a
    * B2B enviroment.
    * <p>
    * This is a PLUS version 1.2 property included in the IPTC Extension
    * schema.
    */
   Property MINOR_MODEL_AGE_DISCLOSURE = Property.internalText(
         PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "MinorModelAgeDisclosure");

   /**
    * Optional identifier associated with each Model Release.
    * <p>
    * This is a PLUS version 1.2 property included in the IPTC Extension
    * schema.
    */
   Property MODEL_RELEASE_ID = Property.internalTextBag(
         PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "ModelReleaseID");

   /**
    * Summarizes the availability and scope of model releases authorizing usage
    * of the likenesses of persons appearing in the photograph.
    * <p>
    * It is recommended to apply the PLUS controlled value Unlimited Model
    * Releases (MR- UMR) very carefully and to check the wording of the model
    * release thoroughly before applying it.
    * <p>
    * This is a PLUS version 1.2 property included in the IPTC Extension
    * schema.
    */
   Property MODEL_RELEASE_STATUS = Property.internalText(
         PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "ModelReleaseStatus");

   /**
    * Optional identifier associated with each Property Release.
    * <p>
    * This is a PLUS version 1.2 property included in the IPTC Extension
    * schema.
    */
   Property PROPERTY_RELEASE_ID = Property.internalTextBag(
         PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "PropertyReleaseID");

   /**
    * Summarises the availability and scope of property releases authorizing
    * usage of the properties appearing in the photograph.
    * <p>
    * It is recommended to apply the value PR-UPR very carefully and to check
    * the wording of the property release thoroughly before applying it.
    * <p>
    * This is a PLUS version 1.2 property included in the IPTC Extension
    * schema.
    */
   Property PROPERTY_RELEASE_STATUS = Property.internalText(
         PREFIX_PLUS + Metadata.NAMESPACE_PREFIX_DELIMITER + "PropertyReleaseStatus");

   /**
    * Contains any necessary copyright notice for claiming the intellectual
    * property for artwork or an object in the image and should identify the
    * current owner of the copyright of this work with associated intellectual
    * property rights.
    */
   Property ARTWORK_OR_OBJECT_DETAIL_COPYRIGHT_NOTICE = Property.internalTextBag(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "AOCopyrightNotice");

   /**
    * Contains the name of the artist who has created artwork or an object in the image.
    */
   Property ARTWORK_OR_OBJECT_DETAIL_CREATOR = Property.internalTextBag(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "AOCreator");

   /**
    * Designates the date and optionally the time the artwork or object in the
    * image was created. This relates to artwork or objects with associated
    * intellectual property rights.
    */
   Property ARTWORK_OR_OBJECT_DETAIL_DATE_CREATED = Property.internalTextBag(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "AODateCreated");

   /**
    * The organisation or body holding and registering the artwork or object in
    * the image for inventory purposes.
    */
   Property ARTWORK_OR_OBJECT_DETAIL_SOURCE = Property.internalTextBag(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "AOSource");

   /**
    * The inventory number issued by the organisation or body holding and
    * registering the artwork or object in the image.
    */
   Property ARTWORK_OR_OBJECT_DETAIL_SOURCE_INVENTORY_NUMBER = Property.internalTextBag(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "AOSourceInvNo");

   /**
    * A reference for the artwork or object in the image.
    */
   Property ARTWORK_OR_OBJECT_DETAIL_TITLE = Property.internalTextBag(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "AOTitle");

   /**
    * Name of the city of a location. This element is at the fourth level of a
    * top-down geographical hierarchy.
    */
   Property LOCATION_SHOWN_CITY = Property.internalTextBag(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "LocationShownCity");

   /**
    * The ISO code of a country of a location. This element is at the second
    * level of a top-down geographical hierarchy.
    * <p>
    * Note 1: an implementer would have to derive from the length of the value
    * string whether this is the country code from the two or three letter
    * scheme as no explicit indication can be provided.
    */
   Property LOCATION_SHOWN_COUNTRY_CODE = Property.internalTextBag(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "LocationShownCountryCode");

   /**
    * The name of a country of a location. This element is at the second level
    * of a top-down geographical hierarchy.
    */
   Property LOCATION_SHOWN_COUNTRY_NAME = Property.internalTextBag(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "LocationShownCountryName");

   /**
    * The name of a subregion of a country - a province or state - of a
    * location. This element is at the third level of a top-down geographical
    * hierarchy.
    */
   Property LOCATION_SHOWN_PROVINCE_OR_STATE = Property.internalTextBag(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "LocationShownProvinceState");

   /**
    * Name of a sublocation. This sublocation name could either be the name of
    * a sublocation to a city or the name of a well known location or (natural)
    * monument outside a city. In the sense of a sublocation to a city this
    * element is at the fifth level of a top-down geographical hierarchy.
    */
   Property LOCATION_SHOWN_SUBLOCATION = Property.internalTextBag(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "LocationShownSublocation");

   /**
    * The name of a world region of a location. This element is at the first
    * (topI) level of a top- down geographical hierarchy.
    */
   Property LOCATION_SHOWN_WORLD_REGION = Property.internalTextBag(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "LocationShownWorldRegion");

   /**
    * Name of the city of a location. This element is at the fourth level of a
    * top-down geographical hierarchy.
    */
   Property LOCATION_CREATED_CITY = Property.internalText(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "LocationCreatedCity");

   /**
    * The ISO code of a country of a location. This element is at the second
    * level of a top-down geographical hierarchy.
    * <p>
    * Note 1: an implementer would have to derive from the length of the value
    * string whether this is the country code from the two or three letter
    * scheme as no explicit indication can be provided.
    */
   Property LOCATION_CREATED_COUNTRY_CODE = Property.internalText(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "LocationCreatedCountryCode");

   /**
    * The name of a country of a location. This element is at the second level
    * of a top-down geographical hierarchy.
    */
   Property LOCATION_CREATED_COUNTRY_NAME = Property.internalText(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "LocationCreatedCountryName");

   /**
    * The name of a subregion of a country - a province or state - of a
    * location. This element is at the third level of a top-down geographical
    * hierarchy.
    */
   Property LOCATION_CREATED_PROVINCE_OR_STATE = Property.internalText(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "LocationCreatedProvinceState");

   /**
    * Name of a sublocation. This sublocation name could either be the name of
    * a sublocation to a city or the name of a well known location or (natural)
    * monument outside a city. In the sense of a sublocation to a city this
    * element is at the fifth level of a top-down geographical hierarchy.
    */
   Property LOCATION_CREATED_SUBLOCATION = Property.internalText(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "LocationCreatedSublocation");

   /**
    * The name of a world region of a location. This element is at the first
    * (topI) level of a top- down geographical hierarchy.
    */
   Property LOCATION_CREATED_WORLD_REGION = Property.internalText(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "LocationCreatedWorldRegion");

   /**
    * A unique identifier created by a registry and applied by the creator of
    * the item. This value shall not be changed after being applied. This
    * identifier is linked to a corresponding Registry Organisation Identifier.
    */
   Property REGISTRY_ENTRY_CREATED_ITEM_ID = Property.internalTextBag(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "RegItemId");

   /**
    * An identifier for the registry which issued the corresponding Registry Image Id.
    */
   Property REGISTRY_ENTRY_CREATED_ORGANISATION_ID = Property.internalTextBag(
         PREFIX_IPTC_EXT + Metadata.NAMESPACE_PREFIX_DELIMITER + "RegOrgId");


   Property[] PROPERTY_GROUP_IPTC_CORE = new Property[] {
         CITY,
         COUNTRY,
         COUNTRY_CODE,
         DESCRIPTION,
         HEADLINE,
         INTELLECTUAL_GENRE,
         KEYWORDS,
         PROVINCE_OR_STATE,
         SCENE_CODE,
         SUBJECT_CODE,
         SUBLOCATION,
         DATE_CREATED,
         DESCRIPTION_WRITER,
         INSTRUCTIONS,
         JOB_ID,
         TITLE,
         COPYRIGHT_NOTICE,
         CREATOR,
         CREATORS_JOB_TITLE,
         CREDIT_LINE,
         RIGHTS_USAGE_TERMS,
         SOURCE,
         CONTACT_INFO_ADDRESS,
         CONTACT_INFO_CITY,
         CONTACT_INFO_COUNTRY,
         CONTACT_INFO_EMAIL,
         CONTACT_INFO_PHONE,
         CONTACT_INFO_POSTAL_CODE,
         CONTACT_INFO_STATE_PROVINCE,
         CONTACT_INFO_WEB_URL
   };

   Property[] PROPERTY_GROUP_IPTC_EXT = new Property[] {
         ADDITIONAL_MODEL_INFO,
         ORGANISATION_CODE,
         CONTROLLED_VOCABULARY_TERM,
         MODEL_AGE,
         ORGANISATION_NAME,
         PERSON,
         DIGITAL_IMAGE_GUID,
         DIGITAL_SOURCE_TYPE,
         EVENT,
         IMAGE_SUPPLIER_ID,
         IMAGE_SUPPLIER_NAME,
         IMAGE_SUPPLIER_IMAGE_ID,
         IPTC_LAST_EDITED,
         MAX_AVAIL_HEIGHT,
         MAX_AVAIL_WIDTH,
         PLUS_VERSION,
         COPYRIGHT_OWNER_ID,
         COPYRIGHT_OWNER_NAME,
         IMAGE_CREATOR_ID,
         IMAGE_CREATOR_NAME,
         LICENSOR_ID,
         LICENSOR_NAME,
         LICENSOR_CITY,
         LICENSOR_COUNTRY,
         LICENSOR_EMAIL,
         LICENSOR_EXTENDED_ADDRESS,
         LICENSOR_POSTAL_CODE,
         LICENSOR_REGION,
         LICENSOR_STREET_ADDRESS,
         LICENSOR_TELEPHONE_1,
         LICENSOR_TELEPHONE_2,
         LICENSOR_URL,
         MINOR_MODEL_AGE_DISCLOSURE,
         MODEL_RELEASE_ID,
         MODEL_RELEASE_STATUS,
         PROPERTY_RELEASE_ID,
         PROPERTY_RELEASE_STATUS,
         ARTWORK_OR_OBJECT_DETAIL_COPYRIGHT_NOTICE,
         ARTWORK_OR_OBJECT_DETAIL_CREATOR,
         ARTWORK_OR_OBJECT_DETAIL_DATE_CREATED,
         ARTWORK_OR_OBJECT_DETAIL_SOURCE,
         ARTWORK_OR_OBJECT_DETAIL_SOURCE_INVENTORY_NUMBER,
         ARTWORK_OR_OBJECT_DETAIL_TITLE,
         LOCATION_SHOWN_CITY,
         LOCATION_SHOWN_COUNTRY_CODE,
         LOCATION_SHOWN_COUNTRY_NAME,
         LOCATION_SHOWN_PROVINCE_OR_STATE,
         LOCATION_SHOWN_SUBLOCATION,
         LOCATION_SHOWN_WORLD_REGION,
         LOCATION_CREATED_CITY,
         LOCATION_CREATED_COUNTRY_CODE,
         LOCATION_CREATED_COUNTRY_NAME,
         LOCATION_CREATED_PROVINCE_OR_STATE,
         LOCATION_CREATED_SUBLOCATION,
         LOCATION_CREATED_WORLD_REGION,
         REGISTRY_ENTRY_CREATED_ITEM_ID,
         REGISTRY_ENTRY_CREATED_ORGANISATION_ID
   };
}"
tika-core/src/main/java/org/apache/tika/metadata/Message.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.metadata;

/**
 * A collection of Message related property names.
 */
public interface Message {
    String MESSAGE_RECIPIENT_ADDRESS = "Message-Recipient-Address";
    
    String MESSAGE_FROM = "Message-From";
    
    String MESSAGE_TO = "Message-To";
    
    String MESSAGE_CC = "Message-Cc";
    
    String MESSAGE_BCC = "Message-Bcc";
}
"
tika-core/src/main/java/org/apache/tika/metadata/Metadata.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.metadata;

import static org.apache.tika.utils.DateUtils.MIDDAY;
import static org.apache.tika.utils.DateUtils.UTC;
import static org.apache.tika.utils.DateUtils.formatDate;

import java.io.Serializable;
import java.text.DateFormat;
import java.text.DateFormatSymbols;
import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.Calendar;
import java.util.Date;
import java.util.Enumeration;
import java.util.HashMap;
import java.util.Locale;
import java.util.Map;
import java.util.Properties;
import java.util.TimeZone;

import org.apache.tika.metadata.Property.PropertyType;

/**
 * A multi-valued metadata container.
 */
public class Metadata implements CreativeCommons, Geographic, HttpHeaders,
        Message, MSOffice, ClimateForcast, TIFF, TikaMetadataKeys, TikaMimeKeys,
        Serializable {

    /** Serial version UID */
    private static final long serialVersionUID = 5623926545693153182L;

    /**
     * A map of all metadata attributes.
     */
    private Map<String, String[]> metadata = null;

    /**
     * The common delimiter used between the namespace abbreviation and the property name
     */
    public static final String NAMESPACE_PREFIX_DELIMITER = ":";

    /** @deprecated use TikaCoreProperties#FORMAT */
    public static final String FORMAT = "format";
    /** @deprecated use TikaCoreProperties#IDENTIFIER */
    public static final String IDENTIFIER = "identifier";
    /** @deprecated use TikaCoreProperties#MODIFIED */
    public static final String MODIFIED = "modified";
    /** @deprecated use TikaCoreProperties#CONTRIBUTOR */
    public static final String CONTRIBUTOR = "contributor";
    /** @deprecated use TikaCoreProperties#COVERAGE */
    public static final String COVERAGE = "coverage";
    /** @deprecated use TikaCoreProperties#CREATOR */
    public static final String CREATOR = "creator";
    /** @deprecated use TikaCoreProperties#CREATED */
    public static final Property DATE = Property.internalDate("date");
    /** @deprecated use TikaCoreProperties#DESCRIPTION */
    public static final String DESCRIPTION = "description";
    /** @deprecated use TikaCoreProperties#LANGUAGE */
    public static final String LANGUAGE = "language";
    /** @deprecated use TikaCoreProperties#PUBLISHER */
    public static final String PUBLISHER = "publisher";
    /** @deprecated use TikaCoreProperties#RELATION */
    public static final String RELATION = "relation";
    /** @deprecated use TikaCoreProperties#RIGHTS */
    public static final String RIGHTS = "rights";
    /** @deprecated use TikaCoreProperties#SOURCE */
    public static final String SOURCE = "source";
    /** @deprecated use TikaCoreProperties#KEYWORDS */
    public static final String SUBJECT = "subject";
    /** @deprecated use TikaCoreProperties#TITLE */
    public static final String TITLE = "title";
    /** @deprecated use TikaCoreProperties#TYPE */
    public static final String TYPE = "type";

    /**
     * Some parsers will have the date as a ISO-8601 string
     *  already, and will set that into the Metadata object.
     * So we can return Date objects for these, this is the
     *  list (in preference order) of the various ISO-8601
     *  variants that we try when processing a date based
     *  property.
     */
    private static final DateFormat[] iso8601InputFormats = new DateFormat[] {
        // yyyy-mm-ddThh...
        createDateFormat("yyyy-MM-dd'T'HH:mm:ss'Z'", UTC),   // UTC/Zulu
        createDateFormat("yyyy-MM-dd'T'HH:mm:ssZ", null),    // With timezone
        createDateFormat("yyyy-MM-dd'T'HH:mm:ss", null),     // Without timezone
        // yyyy-mm-dd hh...
        createDateFormat("yyyy-MM-dd' 'HH:mm:ss'Z'", UTC),   // UTC/Zulu
        createDateFormat("yyyy-MM-dd' 'HH:mm:ssZ", null),    // With timezone
        createDateFormat("yyyy-MM-dd' 'HH:mm:ss", null),     // Without timezone
        // Date without time, set to Midday UTC
        createDateFormat("yyyy-MM-dd", MIDDAY),              // Normal date format
        createDateFormat("yyyy:MM:dd", MIDDAY),              // Image (IPTC/EXIF) format
    };

    private static DateFormat createDateFormat(String format, TimeZone timezone) {
        SimpleDateFormat sdf =
            new SimpleDateFormat(format, new DateFormatSymbols(Locale.US));
        if (timezone != null) {
            sdf.setTimeZone(timezone);
        }
        return sdf;
    }

    /**
     * Parses the given date string. This method is synchronized to prevent
     * concurrent access to the thread-unsafe date formats.
     *
     * @see <a href="https://issues.apache.org/jira/browse/TIKA-495">TIKA-495</a>
     * @param date date string
     * @return parsed date, or <code>null</code> if the date can't be parsed
     */
    private static synchronized Date parseDate(String date) {
        // Java doesn't like timezones in the form ss+hh:mm
        // It only likes the hhmm form, without the colon
        int n = date.length();
        if (date.charAt(n - 3) == ':'
            && (date.charAt(n - 6) == '+' || date.charAt(n - 6) == '-')) {
            date = date.substring(0, n - 3) + date.substring(n - 2);
        }

        // Try several different ISO-8601 variants
        for (DateFormat format : iso8601InputFormats) {
            try {
                return format.parse(date);
            } catch (ParseException ignore) {
            }
        }
        return null;
    }

    /**
     * Constructs a new, empty metadata.
     */
    public Metadata() {
        metadata = new HashMap<String, String[]>();
    }

    /**
     * Returns true if named value is multivalued.
     * 
     * @param property
     *          metadata property
     * @return true is named value is multivalued, false if single value or null
     */
    public boolean isMultiValued(final Property property) {
        return metadata.get(property.getName()) != null && metadata.get(property.getName()).length > 1;
    }
    
    /**
     * Returns true if named value is multivalued.
     * 
     * @param name
     *          name of metadata
     * @return true is named value is multivalued, false if single value or null
     */
    public boolean isMultiValued(final String name) {
        return metadata.get(name) != null && metadata.get(name).length > 1;
    }

    /**
     * Returns an array of the names contained in the metadata.
     * 
     * @return Metadata names
     */
    public String[] names() {
        return metadata.keySet().toArray(new String[metadata.keySet().size()]);
    }

    /**
     * Get the value associated to a metadata name. If many values are assiociated
     * to the specified name, then the first one is returned.
     * 
     * @param name
     *          of the metadata.
     * @return the value associated to the specified metadata name.
     */
    public String get(final String name) {
        String[] values = metadata.get(name);
        if (values == null) {
            return null;
        } else {
            return values[0];
        }
    }

    /**
     * Returns the value (if any) of the identified metadata property.
     *
     * @since Apache Tika 0.7
     * @param property property definition
     * @return property value, or <code>null</code> if the property is not set
     */
    public String get(Property property) {
        return get(property.getName());
    }
    
    /**
     * Returns the value of the identified Integer based metadata property.
     * 
     * @since Apache Tika 0.8
     * @param property simple integer property definition
     * @return property value as a Integer, or <code>null</code> if the property is not set, or not a valid Integer
     */
    public Integer getInt(Property property) {
        if(property.getPrimaryProperty().getPropertyType() != Property.PropertyType.SIMPLE) {
            return null;
        }
        if(property.getPrimaryProperty().getValueType() != Property.ValueType.INTEGER) {
            return null;
        }
        
        String v = get(property);
        if(v == null) {
            return null;
        }
        try {
            return Integer.valueOf(v);
        } catch(NumberFormatException e) {
            return null;
        }
    }

    /**
     * Returns the value of the identified Date based metadata property.
     * 
     * @since Apache Tika 0.8
     * @param property simple date property definition
     * @return property value as a Date, or <code>null</code> if the property is not set, or not a valid Date
     */
    public Date getDate(Property property) {
        if(property.getPrimaryProperty().getPropertyType() != Property.PropertyType.SIMPLE) {
            return null;
        }
        if(property.getPrimaryProperty().getValueType() != Property.ValueType.DATE) {
            return null;
        }
        
        String v = get(property);
        if (v != null) {
            return parseDate(v);
        } else {
            return null;
        }
    }
    
    /**
     * Get the values associated to a metadata name.
     * 
     * @param property
     *          of the metadata.
     * @return the values associated to a metadata name.
     */
    public String[] getValues(final Property property) {
        return _getValues(property.getName());
    }

    /**
     * Get the values associated to a metadata name.
     * 
     * @param name
     *          of the metadata.
     * @return the values associated to a metadata name.
     */
    public String[] getValues(final String name) {
        return _getValues(name);
    }

    private String[] _getValues(final String name) {
        String[] values = metadata.get(name);
        if (values == null) {
            values = new String[0];
        }
        return values;
    }
    
    private String[] appendedValues(String[] values, final String value) {
        String[] newValues = new String[values.length + 1];
        System.arraycopy(values, 0, newValues, 0, values.length);
        newValues[newValues.length - 1] = value;
        return newValues;
    }

    /**
     * Add a metadata name/value mapping. Add the specified value to the list of
     * values associated to the specified metadata name.
     * 
     * @param name
     *          the metadata name.
     * @param value
     *          the metadata value.
     */
    public void add(final String name, final String value) {
        String[] values = metadata.get(name);
        if (values == null) {
            set(name, value);
        } else {
            metadata.put(name, appendedValues(values, value));
        }
    }
    
    /**
     * Add a metadata property/value mapping. Add the specified value to the list of
     * values associated to the specified metadata property.
     * 
     * @param property
     *          the metadata property.
     * @param value
     *          the metadata value.
     */
    public void add(final Property property, final String value) {
        String[] values = metadata.get(property.getName());
        if (values == null) {
            set(property, value);
        } else {
             if (property.isMultiValuePermitted()) {
                 set(property, appendedValues(values, value));
             } else {
                 throw new PropertyTypeException(property.getPropertyType());
             }
        }
    }

    /**
     * Copy All key-value pairs from properties.
     * 
     * @param properties
     *          properties to copy from
     */
    @SuppressWarnings("unchecked")
    public void setAll(Properties properties) {
        Enumeration<String> names =
            (Enumeration<String>) properties.propertyNames();
        while (names.hasMoreElements()) {
            String name = names.nextElement();
            metadata.put(name, new String[] { properties.getProperty(name) });
        }
    }

    /**
     * Set metadata name/value. Associate the specified value to the specified
     * metadata name. If some previous values were associated to this name,
     * they are removed. If the given value is <code>null</code>, then the
     * metadata entry is removed.
     *
     * @param name the metadata name.
     * @param value  the metadata value, or <code>null</code>
     */
    public void set(String name, String value) {
        if (value != null) {
            metadata.put(name, new String[] { value });
        } else {
            metadata.remove(name);
        }
    }

    /**
     * Sets the value of the identified metadata property.
     *
     * @since Apache Tika 0.7
     * @param property property definition
     * @param value    property value
     */
    public void set(Property property, String value) {
        if (property == null) {
            throw new NullPointerException("property must not be null");
        }
        if (property.getPropertyType() == PropertyType.COMPOSITE) {
            set(property.getPrimaryProperty(), value);
            if (property.getSecondaryExtractProperties() != null) {
                for (Property secondaryExtractProperty : property.getSecondaryExtractProperties()) {
                    set(secondaryExtractProperty, value);
                }
            }
        } else {
            set(property.getName(), value);
        }
    }
    
    /**
     * Sets the values of the identified metadata property.
     *
     * @since Apache Tika 1.2
     * @param property property definition
     * @param values    property values
     */
    public void set(Property property, String[] values) {
        if (property == null) {
            throw new NullPointerException("property must not be null");
        }
        if (property.getPropertyType() == PropertyType.COMPOSITE) {
            set(property.getPrimaryProperty(), values);
            if (property.getSecondaryExtractProperties() != null) {
                for (Property secondaryExtractProperty : property.getSecondaryExtractProperties()) {
                    set(secondaryExtractProperty, values);
                }
            }
        } else {
            metadata.put(property.getName(), values);
        }
    }

    /**
     * Sets the integer value of the identified metadata property.
     *
     * @since Apache Tika 0.8
     * @param property simple integer property definition
     * @param value    property value
     */
    public void set(Property property, int value) {
        if(property.getPrimaryProperty().getPropertyType() != Property.PropertyType.SIMPLE) {
            throw new PropertyTypeException(Property.PropertyType.SIMPLE, property.getPrimaryProperty().getPropertyType());
        }
        if(property.getPrimaryProperty().getValueType() != Property.ValueType.INTEGER) {
            throw new PropertyTypeException(Property.ValueType.INTEGER, property.getPrimaryProperty().getValueType());
        }
        set(property, Integer.toString(value));
    }

    /**
     * Sets the real or rational value of the identified metadata property.
     *
     * @since Apache Tika 0.8
     * @param property simple real or simple rational property definition
     * @param value    property value
     */
    public void set(Property property, double value) {
        if(property.getPrimaryProperty().getPropertyType() != Property.PropertyType.SIMPLE) {
            throw new PropertyTypeException(Property.PropertyType.SIMPLE, property.getPrimaryProperty().getPropertyType());
        }
        if(property.getPrimaryProperty().getValueType() != Property.ValueType.REAL &&
              property.getPrimaryProperty().getValueType() != Property.ValueType.RATIONAL) {
            throw new PropertyTypeException(Property.ValueType.REAL, property.getPrimaryProperty().getValueType());
        }
        set(property, Double.toString(value));
    }

    /**
     * Sets the date value of the identified metadata property.
     *
     * @since Apache Tika 0.8
     * @param property simple integer property definition
     * @param date     property value
     */
    public void set(Property property, Date date) {
        if(property.getPrimaryProperty().getPropertyType() != Property.PropertyType.SIMPLE) {
            throw new PropertyTypeException(Property.PropertyType.SIMPLE, property.getPrimaryProperty().getPropertyType());
        }
        if(property.getPrimaryProperty().getValueType() != Property.ValueType.DATE) {
            throw new PropertyTypeException(Property.ValueType.DATE, property.getPrimaryProperty().getValueType());
        }
        String dateString = null;
        if (date != null) {
            dateString = formatDate(date);
        }
        set(property, dateString);
    }

    /**
     * Sets the date value of the identified metadata property.
     *
     * @since Apache Tika 0.8
     * @param property simple integer property definition
     * @param date     property value
     */
    public void set(Property property, Calendar date) {
        if(property.getPrimaryProperty().getPropertyType() != Property.PropertyType.SIMPLE) {
            throw new PropertyTypeException(Property.PropertyType.SIMPLE, property.getPrimaryProperty().getPropertyType());
        }
        if(property.getPrimaryProperty().getValueType() != Property.ValueType.DATE) {
            throw new PropertyTypeException(Property.ValueType.DATE, property.getPrimaryProperty().getValueType());
        }
        String dateString = null;
        if (date != null) {
            dateString = formatDate(date);
        }
        set(property, dateString);
    }

    /**
     * Remove a metadata and all its associated values.
     * 
     * @param name
     *          metadata name to remove
     */
    public void remove(String name) {
        metadata.remove(name);
    }

    /**
     * Returns the number of metadata names in this metadata.
     * 
     * @return number of metadata names
     */
    public int size() {
        return metadata.size();
    }

    public boolean equals(Object o) {

        if (o == null) {
            return false;
        }

        Metadata other = null;
        try {
            other = (Metadata) o;
        } catch (ClassCastException cce) {
            return false;
        }

        if (other.size() != size()) {
            return false;
        }

        String[] names = names();
        for (int i = 0; i < names.length; i++) {
            String[] otherValues = other._getValues(names[i]);
            String[] thisValues = _getValues(names[i]);
            if (otherValues.length != thisValues.length) {
                return false;
            }
            for (int j = 0; j < otherValues.length; j++) {
                if (!otherValues[j].equals(thisValues[j])) {
                    return false;
                }
            }
        }
        return true;
    }

    public String toString() {
        StringBuffer buf = new StringBuffer();
        String[] names = names();
        for (int i = 0; i < names.length; i++) {
            String[] values = _getValues(names[i]);
            for (int j = 0; j < values.length; j++) {
                buf.append(names[i]).append("=").append(values[j]).append(" ");
            }
        }
        return buf.toString();
    }

}
"
tika-core/src/main/java/org/apache/tika/metadata/MSOffice.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.metadata;

/**
 * A collection of Microsoft Office and Open Document property names.
 * 
 * This is being replaced with cleaner, better defined properties in
 *  {@link Office}.
 */
public interface MSOffice {

    @Deprecated String KEYWORDS = "Keywords";

    @Deprecated String COMMENTS = "Comments";

    @Deprecated String LAST_AUTHOR = "Last-Author";

    @Deprecated String AUTHOR = "Author";

    @Deprecated String APPLICATION_NAME = "Application-Name";

    @Deprecated String REVISION_NUMBER = "Revision-Number";

    @Deprecated String TEMPLATE = "Template";

    @Deprecated String TOTAL_TIME = "Total-Time";

    @Deprecated String PRESENTATION_FORMAT = "Presentation-Format";

    @Deprecated String NOTES = "Notes";

    @Deprecated String MANAGER = "Manager";

    @Deprecated String APPLICATION_VERSION = "Application-Version";

    @Deprecated String VERSION = "Version";

    @Deprecated String CONTENT_STATUS = "Content-Status";

    @Deprecated String CATEGORY = "Category";

    @Deprecated String COMPANY = "Company";

    @Deprecated String SECURITY = "Security";

    
    /** The number of Slides are there in the (presentation) document */
    @Deprecated Property SLIDE_COUNT = 
       Property.internalInteger("Slide-Count");
    
    /** The number of Pages are there in the (paged) document */
    @Deprecated Property PAGE_COUNT = 
       Property.internalInteger("Page-Count");

    /** The number of individual Paragraphs in the document */ 
    @Deprecated Property PARAGRAPH_COUNT = 
       Property.internalInteger("Paragraph-Count");
    
    /** The number of lines in the document */
    @Deprecated Property LINE_COUNT = 
       Property.internalInteger("Line-Count");

    /** The number of Words in the document */
    @Deprecated Property WORD_COUNT = 
       Property.internalInteger("Word-Count");

    /** The number of Characters in the document */
    @Deprecated Property CHARACTER_COUNT = 
       Property.internalInteger("Character Count");
    
    /** The number of Characters in the document, including spaces */
    @Deprecated Property CHARACTER_COUNT_WITH_SPACES = 
       Property.internalInteger("Character-Count-With-Spaces");

    /** The number of Tables in the document */
    @Deprecated Property TABLE_COUNT = 
       Property.internalInteger("Table-Count");
    
    /** The number of Images in the document */
    @Deprecated Property IMAGE_COUNT = 
       Property.internalInteger("Image-Count");
    
    /** 
     * The number of Objects in the document.
     * This is typically non-Image resources embedded in the
     *  document, such as other documents or non-Image media. 
     */
    @Deprecated Property OBJECT_COUNT = 
       Property.internalInteger("Object-Count");

    
    /** How long has been spent editing the document? */ 
    String EDIT_TIME = "Edit-Time"; 

    /** When was the document created? */
    @Deprecated Property CREATION_DATE = 
        Property.internalDate("Creation-Date");

    /** When was the document last saved? */
    @Deprecated Property LAST_SAVED = 
       Property.internalDate("Last-Save-Date");
    
    /** When was the document last printed? */
    @Deprecated Property LAST_PRINTED = 
       Property.internalDate("Last-Printed");
    
    /** 
     * For user defined metadata entries in the document,
     *  what prefix should be attached to the key names.
     * eg <meta:user-defined meta:name="Info1">Text1</meta:user-defined> becomes custom:Info1=Text1
     */
    String USER_DEFINED_METADATA_NAME_PREFIX = "custom:";
}
"
tika-core/src/main/java/org/apache/tika/metadata/Office.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.metadata;

/**
 * Office Document properties collection. These properties apply to 
 *  Office / Productivity Documents of all forms, including (but not limited 
 *  to) MS Office and OpenDocument formats.
 * This is a logical collection of properties, which may be drawn from a
 *  few different external definitions.
 * 
 * Note that some of the legacy properties from the {@link MSOffice}
 *  collection still need to be migrated over
 *  
 * @since Apache Tika 1.2
 */
public interface Office {
   // These are taken from the OpenDocumentFormat specification
   public static final String NAMESPACE_URI_DOC_META = "urn:oasis:names:tc:opendocument:xmlns:meta:1.0";
   public static final String PREFIX_DOC_META = "meta";

   /** 
    * For user defined metadata entries in the document,
    *  what prefix should be attached to the key names.
    * eg <meta:user-defined meta:name="Info1">Text1</meta:user-defined> becomes custom:Info1=Text1
    */
   public static final String USER_DEFINED_METADATA_NAME_PREFIX = "custom:";

   
   /**
    * Keywords pertaining to a document. 
    */
   Property KEYWORDS = Property.internalTextBag(
         PREFIX_DOC_META + Metadata.NAMESPACE_PREFIX_DELIMITER + "keyword");

   /**
    * Name of the initial creator/author of a document
    */
   Property INITIAL_AUTHOR = Property.internalText(
         PREFIX_DOC_META + Metadata.NAMESPACE_PREFIX_DELIMITER + "initial-author");

   /**
    * Name of the last (most recent) author of a document
    */
   Property LAST_AUTHOR = Property.internalText(
         PREFIX_DOC_META + Metadata.NAMESPACE_PREFIX_DELIMITER + "last-author");

   /**
    * Name of the principal author(s) of a document
    */
   Property AUTHOR = Property.internalTextBag(
         PREFIX_DOC_META + Metadata.NAMESPACE_PREFIX_DELIMITER + "author");

   
   /** When was the document created? */
   Property CREATION_DATE = Property.internalDate(
         PREFIX_DOC_META + Metadata.NAMESPACE_PREFIX_DELIMITER + "creation-date");

   /** When was the document last saved? */
   Property SAVE_DATE = Property.internalDate(
         PREFIX_DOC_META + Metadata.NAMESPACE_PREFIX_DELIMITER + "save-date");
   
   /** When was the document last printed? */
   Property PRINT_DATE = Property.internalDate(
         PREFIX_DOC_META + Metadata.NAMESPACE_PREFIX_DELIMITER + "print-date");


    
    /** The number of Slides are there in the (presentation) document */
    Property SLIDE_COUNT = Property.internalInteger(
    	  PREFIX_DOC_META + Metadata.NAMESPACE_PREFIX_DELIMITER + "slide-count");
    
    /** The number of Pages are there in the (paged) document */
    Property PAGE_COUNT = Property.internalInteger(
          PREFIX_DOC_META + Metadata.NAMESPACE_PREFIX_DELIMITER + "page-count");

    /** The number of individual Paragraphs in the document */ 
    Property PARAGRAPH_COUNT = Property.internalInteger(
          PREFIX_DOC_META + Metadata.NAMESPACE_PREFIX_DELIMITER + "paragraph-count");
    
    /** The number of lines in the document */
    Property LINE_COUNT = Property.internalInteger(
          PREFIX_DOC_META + Metadata.NAMESPACE_PREFIX_DELIMITER + "line-count");

    /** The number of Words in the document */
    Property WORD_COUNT = Property.internalInteger(
          PREFIX_DOC_META + Metadata.NAMESPACE_PREFIX_DELIMITER + "word-count");

    /** The number of Characters in the document */
    Property CHARACTER_COUNT = Property.internalInteger(
          PREFIX_DOC_META + Metadata.NAMESPACE_PREFIX_DELIMITER + "character-count");
    
    /** The number of Characters in the document, including spaces */
    Property CHARACTER_COUNT_WITH_SPACES = Property.internalInteger(
          PREFIX_DOC_META + Metadata.NAMESPACE_PREFIX_DELIMITER + "character-count-with-spaces");

    /** The number of Tables in the document */
    Property TABLE_COUNT = Property.internalInteger(
          PREFIX_DOC_META + Metadata.NAMESPACE_PREFIX_DELIMITER + "table-count");
    
    /** The number of Images in the document */
    Property IMAGE_COUNT = Property.internalInteger(
          PREFIX_DOC_META + Metadata.NAMESPACE_PREFIX_DELIMITER + "image-count");
    
    /** 
     * The number of Objects in the document. These are typically non-Image resources 
     * embedded in the document, such as other documents or non-Image media. 
     */
    Property OBJECT_COUNT = Property.internalInteger(
          PREFIX_DOC_META + Metadata.NAMESPACE_PREFIX_DELIMITER + "object-count");
}
"
tika-core/src/main/java/org/apache/tika/metadata/OfficeOpenXMLCore.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.metadata;

/**
 * Core properties as defined in the Office Open XML specification part Two that are not
 * in the DublinCore namespace.
 * There is also a keyword property definition in the specification which is omitted here, 
 * because Tika should stick to the DublinCore/IPTC definition. 
 * 
 * @see <a href="http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=59575"
 *        >ISO document of Office Open XML specification</a>
 * @see <a href="http://www.ecma-international.org/publications/standards/Ecma-376.htm
 *        >ECMA document of Office Open XML specification</a> 
 */
public interface OfficeOpenXMLCore 
{
	String NAMESPACE_URI = "http://schemas.openxmlformats.org/package/2006/metadata/core-properties/";
	String PREFIX = "cp";
	
	/**
     * A categorization of the content of this package.
     */
    Property CATEGORY = Property.externalText(
    		PREFIX + Metadata.NAMESPACE_PREFIX_DELIMITER + "category");
    
    /**
     * The status of the content.
     */
    Property CONTENT_STATUS = Property.externalText(
    		PREFIX + Metadata.NAMESPACE_PREFIX_DELIMITER + "contentStatus");
    
    /**
     * The user who performed the last modification. The identification is environment-specific.
     */
    Property LAST_MODIFIED_BY = Property.externalText(
    		PREFIX + Metadata.NAMESPACE_PREFIX_DELIMITER + "lastModifiedBy");
    
    /**
     * The date and time of the last printing.
     */
    Property LAST_PRINTED = Property.externalDate(
    		PREFIX + Metadata.NAMESPACE_PREFIX_DELIMITER + "lastPrinted");
    
    /**
     * The revision number.
     */
    Property REVISION = Property.externalText(
    		PREFIX + Metadata.NAMESPACE_PREFIX_DELIMITER + "revision");
    
    /**
     * The version number. This value is set by the user or by the application.
     */
    Property VERSION = Property.externalText(
    		PREFIX + Metadata.NAMESPACE_PREFIX_DELIMITER + "version");
    
    /**
     * The document's subject.
     */
    Property SUBJECT = Property.externalText(
                PREFIX + Metadata.NAMESPACE_PREFIX_DELIMITER + "subject");
}
"
tika-core/src/main/java/org/apache/tika/metadata/OfficeOpenXMLExtended.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.metadata;

/**
 * Extended properties as defined in the Office Open XML specification part Four.
 * Those properties are omitted which have equivalent properties defined in the ODF
 * namespace like "word count".
 * Also not all properties from the specification are defined here, yet. Only those which have been in
 * use by the parsers so far.
 * 
 * @see <a href="http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=59575"
 *        >ISO document of Office Open XML specification</a>
 * @see <a href="http://www.ecma-international.org/publications/standards/Ecma-376.htm
 *        >ECMA document of Office Open XML specification</a> 
 */
public interface OfficeOpenXMLExtended 
{
    String NAMESPACE_URI = "http://schemas.openxmlformats.org/officeDocument/2006/extended-properties/";
    String WORD_PROCESSING_NAMESPACE_URI = "http://schemas.openxmlformats.org/wordprocessingml/2006/main";
    String PREFIX = "extended-properties";
    String WORD_PROCESSING_PREFIX = "w";

    Property TEMPLATE = Property.externalText(
    		PREFIX + Metadata.NAMESPACE_PREFIX_DELIMITER + "Template");
    
    Property MANAGER = Property.externalText(
    		PREFIX + Metadata.NAMESPACE_PREFIX_DELIMITER + "Manager");
    
    Property COMPANY = Property.externalText(
    		PREFIX + Metadata.NAMESPACE_PREFIX_DELIMITER + "Company");
    
    Property PRESENTATION_FORMAT = Property.externalText(
    		PREFIX + Metadata.NAMESPACE_PREFIX_DELIMITER + "PresentationFormat");
    
    Property NOTES = Property.externalInteger(
    		PREFIX + Metadata.NAMESPACE_PREFIX_DELIMITER + "Notes");
    
    Property TOTAL_TIME = Property.externalInteger(
    		PREFIX + Metadata.NAMESPACE_PREFIX_DELIMITER + "TotalTime");
    
    Property HIDDEN_SLIDES = Property.externalInteger(
    		PREFIX + Metadata.NAMESPACE_PREFIX_DELIMITER + "HiddedSlides");
    
    Property APPLICATION = Property.externalText(
    		PREFIX + Metadata.NAMESPACE_PREFIX_DELIMITER + "Application");
    
    Property APP_VERSION = Property.externalText(
    		PREFIX + Metadata.NAMESPACE_PREFIX_DELIMITER + "AppVersion");
    
    Property DOC_SECURITY = Property.externalInteger(
    		PREFIX + Metadata.NAMESPACE_PREFIX_DELIMITER + "DocSecurity");
    
    Property COMMENTS = Property.externalTextBag(
            WORD_PROCESSING_PREFIX + Metadata.NAMESPACE_PREFIX_DELIMITER + "comments");
}"
tika-core/src/main/java/org/apache/tika/metadata/package-info.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Multi-valued metadata container, and set of constant metadata fields.
 */
@aQute.bnd.annotation.Version("1.0.0")
package org.apache.tika.metadata;
"
tika-core/src/main/java/org/apache/tika/metadata/PagedText.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.metadata;

/**
 * XMP Paged-text schema. This is a collection of
 * {@link Property property definition} constants for the paged text
 * properties defined in the XMP standard.
 *
 * @since Apache Tika 0.8
 * @see <a href="http://wwwimages.adobe.com/content/dam/Adobe/en/devnet/xmp/pdfs/cc-201306/XMPSpecificationPart2.pdf"
 *        >XMP Specification, Part 2: Standard Schemas</a>
 */
public interface PagedText {

    /**
     * "The number of pages in the document (including any in contained
     * documents)."
     */
    Property N_PAGES = Property.internalInteger("xmpTPg:NPages");

}
"
tika-core/src/main/java/org/apache/tika/metadata/Photoshop.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 * IPTC Metadata Descriptions taken from the IPTC Photo Metadata (July 2010) 
 * standard. These parts Copyright 2010 International Press Telecommunications 
 * Council.
 */
package org.apache.tika.metadata;

/**
 * XMP Photoshop metadata schema. 
 * 
 * A collection of property constants for the 
 * Photo Metadata properties defined in the XMP Photoshop
 * standard.
 * 
 * @since Apache Tika 1.2
 * @see <a href="http://partners.adobe.com/public/developer/en/xmp/sdk/XMPspecification.pdf">XMP Photoshop</a>
 */
public interface Photoshop {

    String NAMESPACE_URI_PHOTOSHOP = "http://ns.adobe.com/photoshop/1.0/";
    String PREFIX_PHOTOSHOP = "photoshop";

    Property AUTHORS_POSITION = Property.internalText(
            PREFIX_PHOTOSHOP + Metadata.NAMESPACE_PREFIX_DELIMITER + "AuthorsPosition");

    // TODO Replace this with proper indexed choices support
    String[] _COLOR_MODE_CHOICES_INDEXED = { "Bitmap", "Greyscale", "Indexed Colour", 
            "RGB Color", "CMYK Colour", "Multi-Channel", "Duotone", "LAB Colour",
            "reserved", "reserved", "YCbCr Colour", "YCgCo Colour", "YCbCrK Colour"};
    Property COLOR_MODE = Property.internalClosedChoise(
            PREFIX_PHOTOSHOP + Metadata.NAMESPACE_PREFIX_DELIMITER + "ColorMode",
            _COLOR_MODE_CHOICES_INDEXED);
    
    Property CAPTION_WRITER = Property.internalText(
            PREFIX_PHOTOSHOP + Metadata.NAMESPACE_PREFIX_DELIMITER + "CaptionWriter");

    Property CATEGORY = Property.internalText(
            PREFIX_PHOTOSHOP + Metadata.NAMESPACE_PREFIX_DELIMITER + "Category");

    Property CITY = Property.internalText(
            PREFIX_PHOTOSHOP + Metadata.NAMESPACE_PREFIX_DELIMITER + "City");

    Property COUNTRY = Property.internalText(
            PREFIX_PHOTOSHOP + Metadata.NAMESPACE_PREFIX_DELIMITER + "Country");

    Property CREDIT = Property.internalText(
            PREFIX_PHOTOSHOP + Metadata.NAMESPACE_PREFIX_DELIMITER + "Credit");

    Property DATE_CREATED = Property.internalDate(
            PREFIX_PHOTOSHOP + Metadata.NAMESPACE_PREFIX_DELIMITER + "DateCreated");

    Property HEADLINE = Property.internalText(
            PREFIX_PHOTOSHOP + Metadata.NAMESPACE_PREFIX_DELIMITER + "Headline");

    Property INSTRUCTIONS = Property.internalText(
            PREFIX_PHOTOSHOP + Metadata.NAMESPACE_PREFIX_DELIMITER + "Instructions");

    Property SOURCE = Property.internalText(
            PREFIX_PHOTOSHOP + Metadata.NAMESPACE_PREFIX_DELIMITER + "Source");

    Property STATE = Property.internalText(
            PREFIX_PHOTOSHOP + Metadata.NAMESPACE_PREFIX_DELIMITER + "State");

    Property SUPPLEMENTAL_CATEGORIES = Property.internalTextBag(
            PREFIX_PHOTOSHOP + Metadata.NAMESPACE_PREFIX_DELIMITER + "SupplementalCategories");

    Property TRANSMISSION_REFERENCE = Property.internalText(
            PREFIX_PHOTOSHOP + Metadata.NAMESPACE_PREFIX_DELIMITER + "TransmissionReference");

    Property URGENCY = Property.internalText(
            PREFIX_PHOTOSHOP + Metadata.NAMESPACE_PREFIX_DELIMITER + "Urgency");

}"
tika-core/src/main/java/org/apache/tika/metadata/Property.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.metadata;

import java.util.Arrays;
import java.util.Collections;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Map;
import java.util.Set;
import java.util.SortedSet;
import java.util.TreeSet;

/**
 * XMP property definition. Each instance of this class defines a single
 * metadata property like "dc:format". In addition to the property name,
 * the {@link ValueType value type} and category (internal or external)
 * of the property are included in the property definition. The available
 * choice values are also stored for open and closed choice value types.
 *
 * @since Apache Tika 0.7
 */
public final class Property implements Comparable<Property> {

    public static enum PropertyType {
        /** A single value */
        SIMPLE, 
        STRUCTURE, 
        /** An un-ordered array */
        BAG, 
        /** An ordered array */
        SEQ, 
        /** An ordered array with some sort of criteria */
        ALT, 
        /** Multiple child properties */
        COMPOSITE
    }

    public static enum ValueType {
        BOOLEAN, OPEN_CHOICE, CLOSED_CHOICE, DATE, INTEGER, LOCALE,
        MIME_TYPE, PROPER_NAME, RATIONAL, REAL, TEXT, URI, URL, XPATH, PROPERTY
    }

    private static final Map<String, Property> properties =
            new HashMap<String, Property>();

    private final String name;

    private final boolean internal;

    private final PropertyType propertyType;

    private final ValueType valueType;
    
    private final Property primaryProperty;
    
    private final Property[] secondaryExtractProperties;

    /**
     * The available choices for the open and closed choice value types.
     */
    private final Set<String> choices;

    private Property(
            String name, boolean internal, PropertyType propertyType,
            ValueType valueType, String[] choices, Property primaryProperty, Property[] secondaryExtractProperties) {
        this.name = name;
        this.internal = internal;
        this.propertyType = propertyType;
        this.valueType = valueType;
        if (choices != null) {
            this.choices = Collections.unmodifiableSet(
                    new HashSet<String>(Arrays.asList(choices.clone())));
        } else {
            this.choices = null;
        }
        
        if (primaryProperty != null) {
            this.primaryProperty = primaryProperty;
            this.secondaryExtractProperties = secondaryExtractProperties;
        } else {
            this.primaryProperty = this;
            this.secondaryExtractProperties = null;
            
            // Only store primary properties for lookup, not composites
            synchronized (properties) {
               properties.put(name, this);
           }
        }
    }
    
    private Property(
            String name, boolean internal, PropertyType propertyType,
            ValueType valueType, String[] choices) {
    	this(name, internal, propertyType, valueType, choices, null, null);
    }

    private Property(
            String name, boolean internal,
            ValueType valueType, String[] choices) {
        this(name, internal, PropertyType.SIMPLE, valueType, choices);
    }

    private Property(String name, boolean internal, ValueType valueType) {
        this(name, internal, PropertyType.SIMPLE, valueType, null);
    }

    private Property(
            String name, boolean internal,
            PropertyType propertyType, ValueType valueType) {
        this(name, internal, propertyType, valueType, null);
    }
    
    public String getName() {
        return name;
    }

    public boolean isInternal() {
        return internal;
    }

    public boolean isExternal() {
        return !internal;
    }
    
    /**
     * Is the PropertyType one which accepts multiple values?
     */
    public boolean isMultiValuePermitted() {
        if (propertyType == PropertyType.BAG || propertyType == PropertyType.SEQ ||
            propertyType == PropertyType.ALT) {
           return true;
        } else if (propertyType == PropertyType.COMPOSITE) {
           // Base it on the primary property's behaviour
           return primaryProperty.isMultiValuePermitted();
        }
        return false;
    }

    /**
     * Get the type of a property
     * @param key name of the property
     * @return the type of the property
     */
    public static PropertyType getPropertyType(String key) {
        PropertyType type = null;
        Property prop = properties.get(key);
        if (prop != null) {
            type = prop.getPropertyType();
        }
        return type;
    }

    /**
     * Retrieve the property object that corresponds to the given key
     * @param key the property key or name
     * @return the Property object
     */
    public static Property get(String key) {
        return properties.get(key);
    }

    public PropertyType getPropertyType() {
        return propertyType;
    }

    public ValueType getValueType() {
        return valueType;
    }

    /**
     * Returns the (immutable) set of choices for the values of this property.
     * Only defined for {@link ValueType#OPEN_CHOICE open} and
     * {@link ValueType#CLOSED_CHOICE closed choice} value types.
     *
     * @return available choices, or <code>null</code>
     */
    public Set<String> getChoices() {
        return choices;
    }
    
    /**
     * Gets the primary property for a composite property
     * 
     * @return the primary property
     */
    public Property getPrimaryProperty() {
        return primaryProperty;
    }

    /**
     * Gets the secondary properties for a composite property
     * 
     * @return the secondary properties
     */
    public Property[] getSecondaryExtractProperties() {
		return secondaryExtractProperties;
	}
    
    public static SortedSet<Property> getProperties(String prefix) {
        SortedSet<Property> set = new TreeSet<Property>();
        String p = prefix + ":";
        synchronized (properties) {
            for (String name : properties.keySet()) {
                if (name.startsWith(p)) {
                    set.add(properties.get(name));
                }
            }
        }
        return set;
    }

    public static Property internalBoolean(String name) {
        return new Property(name, true, ValueType.BOOLEAN);
    }

    public static Property internalClosedChoise(
            String name, String... choices) {
        return new Property(name, true, ValueType.CLOSED_CHOICE, choices);
    }

    public static Property internalDate(String name) {
        return new Property(name, true, ValueType.DATE);
    }

    public static Property internalInteger(String name) {
        return new Property(name, true, ValueType.INTEGER);
    }

    public static Property internalIntegerSequence(String name) {
        return new Property(name, true, PropertyType.SEQ, ValueType.INTEGER);
    }

    public static Property internalRational(String name) {
        return new Property(name, true, ValueType.RATIONAL);
    }

    public static Property internalOpenChoise(
            String name, String... choices) {
        return new Property(name, true, ValueType.OPEN_CHOICE, choices);
    }
    public static Property internalReal(String name) {
        return new Property(name, true, ValueType.REAL);
    }

    public static Property internalText(String name) {
        return new Property(name, true, ValueType.TEXT);
    }
    
    public static Property internalTextBag(String name) {
        return new Property(name, true, PropertyType.BAG, ValueType.TEXT);
    }

    public static Property internalURI(String name) {
        return new Property(name, true, ValueType.URI);
    }

    public static Property externalClosedChoise(
            String name, String... choices) {
        return new Property(name, false, ValueType.CLOSED_CHOICE, choices);
    }

    public static Property externalOpenChoise(
            String name, String... choices) {
        return new Property(name, false, ValueType.OPEN_CHOICE, choices);
    }

    public static Property externalDate(String name) {
        return new Property(name, false, ValueType.DATE);
    }

    public static Property externalReal(String name) {
       return new Property(name, false, ValueType.REAL);
   }

    public static Property externalInteger(String name) {
        return new Property(name, false, ValueType.INTEGER);
    }

    public static Property externalBoolean(String name) {
       return new Property(name, false, ValueType.BOOLEAN);
   }

    public static Property externalText(String name) {
        return new Property(name, false, ValueType.TEXT);
    }

    public static Property externalTextBag(String name) {
        return new Property(name, false, PropertyType.BAG, ValueType.TEXT);
    }

    /**
     * Constructs a new composite property from the given primary and array of secondary properties.
     * <p>
     * Note that name of the composite property is taken from its primary property, 
     * and primary and secondary properties must not be composite properties themselves.
     * 
     * @param primaryProperty
     * @param secondaryExtractProperties
     * @return the composite property
     */
    public static Property composite(Property primaryProperty, Property[] secondaryExtractProperties) {
        if (primaryProperty == null) {
            throw new NullPointerException("primaryProperty must not be null");
        }
        if (primaryProperty.getPropertyType() == PropertyType.COMPOSITE) {
            throw new PropertyTypeException(primaryProperty.getPropertyType());
        }
        if (secondaryExtractProperties != null) {
            for (Property secondaryExtractProperty : secondaryExtractProperties) {
                if (secondaryExtractProperty.getPropertyType() == PropertyType.COMPOSITE) {
                    throw new PropertyTypeException(secondaryExtractProperty.getPropertyType());
                }
            }
        }
        String[] choices = null;
        if (primaryProperty.getChoices() != null) {
            choices = primaryProperty.getChoices().toArray(
                    new String[primaryProperty.getChoices().size()]);
        }
        return new Property(primaryProperty.getName(),
                primaryProperty.isInternal(), PropertyType.COMPOSITE,
                ValueType.PROPERTY, choices, primaryProperty,
                secondaryExtractProperties);
    }

    //----------------------------------------------------------< Comparable >

    public int compareTo(Property o) {
        return name.compareTo(o.name);
    }

    //--------------------------------------------------------------< Object >

    public boolean equals(Object o) {
        return o instanceof Property && name.equals(((Property) o).name);
    }

    public int hashCode() {
        return name.hashCode();
    }

}
"
tika-core/src/main/java/org/apache/tika/metadata/PropertyTypeException.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.metadata;

import org.apache.tika.metadata.Property.PropertyType;
import org.apache.tika.metadata.Property.ValueType;


/**
 * XMP property definition violation exception. This is thrown when
 * you try to set a {@link Property} value with an incorrect type,
 * such as storing an Integer when the property is of type Date.
 *
 * @since Apache Tika 0.8
 */
public final class PropertyTypeException extends IllegalArgumentException {

    public PropertyTypeException(String msg) {
        super(msg);
    }

    public PropertyTypeException(PropertyType expected, PropertyType found) {
        super("Expected a property of type " + expected + ", but received " + found);
    }

    public PropertyTypeException(ValueType expected, ValueType found) {
        super("Expected a property with a " + expected + " value, but received a " + found);
    }

    public PropertyTypeException(PropertyType unsupportedPropertyType) {
        super((unsupportedPropertyType != PropertyType.COMPOSITE)
                ? unsupportedPropertyType + " is not supported"
                : "Composite Properties must not include other Composite"
                   + " Properties as either Primary or Secondary");
    }
}
"
tika-core/src/main/java/org/apache/tika/metadata/RTFMetadata.java,false,"package org.apache.tika.metadata; /*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */ import org.apache.tika.metadata.Metadata; import org.apache.tika.metadata.Property; public interface 
RTFMetadata {
    public static final String PREFIX_RTF_META = "rtf_meta";
    
    
    public static final String RTF_PICT_META_PREFIX = "rtf_pict:";
    
    /**
     * if set to true, this means that an image file is probably a "thumbnail"
     * any time a pict/emf/wmf is in an object
     */
    Property THUMBNAIL = Property.internalBoolean(PREFIX_RTF_META+
            Metadata.NAMESPACE_PREFIX_DELIMITER+"thumbnail");
    
    /**
     * if an application and version is given as part of the
     * embedded object, this is the literal string
     */
    Property EMB_APP_VERSION = Property.internalText(PREFIX_RTF_META+
            Metadata.NAMESPACE_PREFIX_DELIMITER+"emb_app_version");
    
    Property EMB_CLASS = Property.internalText(PREFIX_RTF_META+
            Metadata.NAMESPACE_PREFIX_DELIMITER+"emb_class");
    
    Property EMB_TOPIC = Property.internalText(PREFIX_RTF_META+
            Metadata.NAMESPACE_PREFIX_DELIMITER+"emb_topic");
    
    Property EMB_ITEM = Property.internalText(PREFIX_RTF_META+
            Metadata.NAMESPACE_PREFIX_DELIMITER+"emb_item");
    
}
"
tika-core/src/main/java/org/apache/tika/metadata/TIFF.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.metadata;

/**
 * XMP Exif TIFF schema. This is a collection of
 * {@link Property property definition} constants for the Exif TIFF
 * properties defined in the XMP standard.
 *
 * @since Apache Tika 0.8
 * @see <a href="http://wwwimages.adobe.com/content/dam/Adobe/en/devnet/xmp/pdfs/cc-201306/XMPSpecificationPart2.pdf"
 *        >XMP Specification, Part 2: Standard Schemas</a>
 */
public interface TIFF {

    /**
     * "Number of bits per component in each channel."
     */
    Property BITS_PER_SAMPLE =
        Property.internalIntegerSequence("tiff:BitsPerSample");

    /**
     * "Image height in pixels."
     */
    Property IMAGE_LENGTH =
        Property.internalInteger("tiff:ImageLength");

    /**
     * "Image width in pixels."
     */
    Property IMAGE_WIDTH =
        Property.internalInteger("tiff:ImageWidth");

    /**
     * "Number of components per pixel."
     */
    Property SAMPLES_PER_PIXEL =
        Property.internalInteger("tiff:SamplesPerPixel");

    /**
     * Did the Flash fire when taking this image?
     */
    Property FLASH_FIRED =
       Property.internalBoolean("exif:Flash");
    
    /**
     * "Exposure time in seconds."
     */
    Property EXPOSURE_TIME =
       Property.internalRational("exif:ExposureTime");
    
    /**
     * "F-Number."
     * The f-number is the focal length divided by the "effective" aperture 
     *  diameter. It is a dimensionless number that is a measure of lens speed. 
     */
    Property F_NUMBER =
       Property.internalRational("exif:FNumber");
    
    /**
     * "Focal length of the lens, in millimeters."
     */
    Property FOCAL_LENGTH =
       Property.internalRational("exif:FocalLength");
    
    /**
     * "ISO Speed and ISO Latitude of the input device as specified in ISO 12232"
     */
    Property ISO_SPEED_RATINGS =
       Property.internalIntegerSequence("exif:IsoSpeedRatings");
    
    /**
     * "Manufacturer of the recording equipment."
     */
    Property EQUIPMENT_MAKE =
       Property.internalText("tiff:Make");
    
    /**
     * "Model name or number of the recording equipment."
     */
    Property EQUIPMENT_MODEL =
       Property.internalText("tiff:Model");
    
    /**
     * "Software or firmware used to generate the image."
     */
    Property SOFTWARE =
       Property.internalText("tiff:Software");

    /**
     * "The Orientation of the image."
     *  1 = 0th row at top, 0th column at left
     *  2 = 0th row at top, 0th column at right
     *  3 = 0th row at bottom, 0th column at right
     *  4 = 0th row at bottom, 0th column at left
     *  5 = 0th row at left, 0th column at top
     *  6 = 0th row at right, 0th column at top
     *  7 = 0th row at right, 0th column at bottom
     *  8 = 0th row at left, 0th column at bottom
     */
    Property ORIENTATION =
       Property.internalClosedChoise("tiff:Orientation", "1", "2", "3", "4", "5", "6", "7", "8");
    
    /**
     * "Horizontal resolution in pixels per unit."
     */
    Property RESOLUTION_HORIZONTAL =
       Property.internalRational("tiff:XResolution");
    
    /**
     * "Vertical resolution in pixels per unit."
     */
    Property RESOLUTION_VERTICAL =
       Property.internalRational("tiff:YResolution");
    
    /**
     * "Units used for Horizontal and Vertical Resolutions."
     * One of "Inch" or "cm"
     */
    Property RESOLUTION_UNIT =
       Property.internalClosedChoise("tiff:ResolutionUnit", "Inch", "cm"); 
    
    /**
     * "Date and time when original image was generated"
     */
    Property ORIGINAL_DATE =
       Property.internalDate("exif:DateTimeOriginal");
}
"
tika-core/src/main/java/org/apache/tika/metadata/TikaCoreProperties.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.metadata;

/**
 * Contains a core set of basic Tika metadata properties, which all parsers
 *  will attempt to supply (where the file format permits). These are all
 *  defined in terms of other standard namespaces.
 *  
 * Users of Tika who wish to have consistent metadata across file formats
 *  can make use of these Properties, knowing that where present they will
 *  have consistent semantic meaning between different file formats. (No 
 *  matter if one file format calls it Title, another Long-Title and another
 *  Long-Name, if they all mean the same thing as defined by 
 *  {@link DublinCore#TITLE} then they will all be present as such)
 *
 * For now, most of these properties are composite ones including the deprecated
 *  non-prefixed String properties from the Metadata class. In Tika 2.0, most
 *  of these will revert back to simple assignments.
 * 
 * @since Apache Tika 1.2
 */
@SuppressWarnings("deprecation")
public interface TikaCoreProperties {

    /**
     * A file might contain different types of embedded documents.
     * The most common is the ATTACHEMENT.
     * An INLINE embedded resource should be used for embedded image
     * files that are used to render the page image (as in PDXObjImages in PDF files).
     * <p>
     * Not all parsers have yet implemented this. 
     *
     */
    public enum EmbeddedResourceType {
        INLINE,
        ATTACHMENT
    };

    /**
     * Use this to prefix metadata properties that store information
     * about the parsing process.  Users should be able to distinguish
     * between metadata that was contained within the document and
     * metadata about the parsing process.
     * In Tika 2.0 (or earlier?), let's change X-ParsedBy to X-TIKA-Parsed-By.
     */
    public static String TIKA_META_PREFIX = "X-TIKA"+Metadata.NAMESPACE_PREFIX_DELIMITER;

    /**
     * Use this to store parse exception information in the Metadata object.
     */
    public static String TIKA_META_EXCEPTION_PREFIX = TIKA_META_PREFIX+"EXCEPTION"+
            Metadata.NAMESPACE_PREFIX_DELIMITER;
    /**
     * @see DublinCore#FORMAT
     */
    public static final Property FORMAT = Property.composite(DublinCore.FORMAT, 
            new Property[] { Property.internalText(Metadata.FORMAT) });
    
   /**
    * @see DublinCore#IDENTIFIER
    */
   public static final Property IDENTIFIER = Property.composite(DublinCore.IDENTIFIER, 
            new Property[] { Property.internalText(Metadata.IDENTIFIER) });
    
   /**
    * @see DublinCore#CONTRIBUTOR
    */
    public static final Property CONTRIBUTOR = Property.composite(DublinCore.CONTRIBUTOR, 
            new Property[] { Property.internalText(Metadata.CONTRIBUTOR) });
    
   /**
    * @see DublinCore#COVERAGE
    */
    public static final Property COVERAGE = Property.composite(DublinCore.COVERAGE, 
            new Property[] { Property.internalText(Metadata.COVERAGE) });
    
   /**
    * @see DublinCore#CREATOR
    */
    public static final Property CREATOR = Property.composite(DublinCore.CREATOR, 
            new Property[] { 
                Office.AUTHOR,
                Property.internalTextBag(Metadata.CREATOR),
                Property.internalTextBag(Metadata.AUTHOR)
            });
    
    /**
     * @see Office#LAST_AUTHOR
     */
     public static final Property MODIFIER = Property.composite(Office.LAST_AUTHOR, 
             new Property[] { Property.internalText(Metadata.LAST_AUTHOR) });
    
    /**
     * @see XMP#CREATOR_TOOL
     */
     public static final Property CREATOR_TOOL = XMP.CREATOR_TOOL;
    
   /**
    * @see DublinCore#LANGUAGE
    */
    public static final Property LANGUAGE = Property.composite(DublinCore.LANGUAGE, 
            new Property[] { Property.internalText(Metadata.LANGUAGE) });
    
   /**
    * @see DublinCore#PUBLISHER
    */
    public static final Property PUBLISHER = Property.composite(DublinCore.PUBLISHER, 
            new Property[] { Property.internalText(Metadata.PUBLISHER) });
    
   /**
    * @see DublinCore#RELATION
    */
    public static final Property RELATION = Property.composite(DublinCore.RELATION, 
            new Property[] { Property.internalText(Metadata.RELATION) });
    
   /**
    * @see DublinCore#RIGHTS
    */
    public static final Property RIGHTS = Property.composite(DublinCore.RIGHTS, 
            new Property[] { Property.internalText(Metadata.RIGHTS) });
    
   /**
    * @see DublinCore#SOURCE
    */
    public static final Property SOURCE = Property.composite(DublinCore.SOURCE, 
            new Property[] { Property.internalText(Metadata.SOURCE) });
    
   /**
    * @see DublinCore#TYPE
    */
    public static final Property TYPE = Property.composite(DublinCore.TYPE, 
            new Property[] { Property.internalText(Metadata.TYPE) });

    
    // Descriptive properties
    
    /**
     * @see DublinCore#TITLE
     */
    public static final Property TITLE = Property.composite(DublinCore.TITLE, 
            new Property[] { Property.internalText(Metadata.TITLE) });
     
    /**
     * @see DublinCore#DESCRIPTION
     */
    public static final Property DESCRIPTION = Property.composite(DublinCore.DESCRIPTION, 
            new Property[] { Property.internalText(Metadata.DESCRIPTION) });
     
    /**
     * @see DublinCore#SUBJECT
     * @see Office#KEYWORDS
     */
    public static final Property KEYWORDS = Property.composite(DublinCore.SUBJECT,
            new Property[] { 
                Office.KEYWORDS, 
                Property.internalTextBag(MSOffice.KEYWORDS),
                Property.internalTextBag(Metadata.SUBJECT)
            });
    
    // Date related properties
    
     /** 
      * @see DublinCore#DATE 
      * @see Office#CREATION_DATE 
      */
     public static final Property CREATED = Property.composite(DublinCore.CREATED,
             new Property[] { 
                     Office.CREATION_DATE, 
                     MSOffice.CREATION_DATE
             });
     
     /** 
      * @see DublinCore#MODIFIED
      * @see Metadata#DATE
      * @see Office#SAVE_DATE 
      */
     public static final Property MODIFIED = Property.composite(DublinCore.MODIFIED,
             new Property[] { 
                     Metadata.DATE,
                     Office.SAVE_DATE, 
                     MSOffice.LAST_SAVED, 
                     Property.internalText(Metadata.MODIFIED),
                     Property.internalText("Last-Modified")
             });
     
     /** @see Office#PRINT_DATE */
     public static final Property PRINT_DATE = Property.composite(Office.PRINT_DATE, 
             new Property[] { MSOffice.LAST_PRINTED });
     
     /**
      * @see XMP#METADATA_DATE
      */
     public static final Property METADATA_DATE = XMP.METADATA_DATE;
    
     
    // Geographic related properties
     
    /**
     * @see Geographic#LATITUDE
     */
    public static final Property LATITUDE = Geographic.LATITUDE;
    
    /**
     * @see Geographic#LONGITUDE
     */
    public static final Property LONGITUDE = Geographic.LONGITUDE;
    
    /**
     * @see Geographic#ALTITUDE
     */
    public static final Property ALTITUDE = Geographic.ALTITUDE;
    
    
    // Comment and rating properties
    
    /**
     * @see XMP#RATING
     */
    public static final Property RATING = XMP.RATING;
    
    /** 
     * @see OfficeOpenXMLExtended#COMMENTS 
     */
    public static final Property COMMENTS = Property.composite(OfficeOpenXMLExtended.COMMENTS, 
            new Property[] { 
                Property.internalTextBag(ClimateForcast.COMMENT),
                Property.internalTextBag(MSOffice.COMMENTS)
            });
    
    // TODO: Remove transition properties in Tika 2.0
    
    /** 
     * @see DublinCore#SUBJECT 
     * @deprecated use TikaCoreProperties#KEYWORDS
     */
    @Deprecated
    public static final Property TRANSITION_KEYWORDS_TO_DC_SUBJECT = Property.composite(DublinCore.SUBJECT, 
            new Property[] { Property.internalTextBag(MSOffice.KEYWORDS) });
    
    /** 
     * @see OfficeOpenXMLExtended#COMMENTS 
     * @deprecated use TikaCoreProperties#DESCRIPTION
     */
    @Deprecated
    public static final Property TRANSITION_SUBJECT_TO_DC_DESCRIPTION = Property.composite(DublinCore.DESCRIPTION, 
            new Property[] { Property.internalText(Metadata.SUBJECT) });
    
    /** 
     * @see DublinCore#TITLE 
     * @deprecated use TikaCoreProperties#TITLE
     */
    @Deprecated
    public static final Property TRANSITION_SUBJECT_TO_DC_TITLE = Property.composite(DublinCore.TITLE, 
            new Property[] { Property.internalText(Metadata.SUBJECT) });
    
    /** 
     * @see OfficeOpenXMLCore#SUBJECT 
     * @deprecated use OfficeOpenXMLCore#SUBJECT
     */
    @Deprecated
    public static final Property TRANSITION_SUBJECT_TO_OO_SUBJECT = Property.composite(OfficeOpenXMLCore.SUBJECT, 
            new Property[] { Property.internalText(Metadata.SUBJECT) });

    /**
     * See {@link #EMBEDDED_RESOURCE_TYPE}
     */
    public static final Property EMBEDDED_RESOURCE_TYPE = 
            Property.internalClosedChoise(TikaMetadataKeys.EMBEDDED_RESOURCE_TYPE, 
                    new String[]{EmbeddedResourceType.ATTACHMENT.toString(), EmbeddedResourceType.INLINE.toString()});

    
}
"
tika-core/src/main/java/org/apache/tika/metadata/TikaMetadataKeys.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.metadata;

/**
 * Contains keys to properties in Metadata instances.
 */
public interface TikaMetadataKeys {

    String RESOURCE_NAME_KEY = "resourceName";

    String PROTECTED = "protected";

    String EMBEDDED_RELATIONSHIP_ID = "embeddedRelationshipId";

    String EMBEDDED_RESOURCE_TYPE = "embeddedResourceType";

}
"
tika-core/src/main/java/org/apache/tika/metadata/TikaMimeKeys.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.metadata;

/**
 * A collection of Tika metadata keys used in Mime Type resolution
 */
public interface TikaMimeKeys {

    String TIKA_MIME_FILE = "tika.mime.file";

    String MIME_TYPE_MAGIC = "mime.type.magic";

}
"
tika-core/src/main/java/org/apache/tika/metadata/XMP.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.metadata;

public interface XMP {

    String NAMESPACE_URI = "http://ns.adobe.com/xap/1.0/";

    String PREFIX = "xmp";

    /** The xmp prefix followed by the colon delimiter */
    String PREFIX_ = PREFIX + Metadata.NAMESPACE_PREFIX_DELIMITER;

    /**
     * The date and time the resource was created. For a digital file, this need not
     * match a file-system creation time. For a freshly created resource, it should
     * be close to that time, modulo the time taken to write the file. Later file
     * transfer, copying, and so on, can make the file-system time arbitrarily different.
     */
    Property CREATE_DATE = Property.externalDate(PREFIX_ + "CreateDate");

    /**
     * The name of the first known tool used to create the resource.
     */
    Property CREATOR_TOOL = Property.externalText(PREFIX_ + "CreatorTool");

    /**
     * An unordered array of text strings that unambiguously identify the resource
     * within a given context. An array item may be qualified with xmpidq:Scheme
     * (see 8.7, xmpidq namespace) to denote the formal identification system to
     * which that identifier conforms.
     */
    Property IDENTIFIER = Property.externalTextBag(PREFIX_ + "Identifier");

    /**
     * A word or short phrase that identifies a resource as a member of a userdefined collection.
     */
    Property LABEL = Property.externalDate(PREFIX_ + "Label");

    /**
     * The date and time that any metadata for this resource was last changed. It
     * should be the same as or more recent than xmp:ModifyDate
     */
    Property METADATA_DATE = Property.externalDate(PREFIX_ + "MetadataDate");

    /**
     * The date and time the resource was last modified.
     */
    Property MODIFY_DATE = Property.externalDate(PREFIX_ + "ModifyDate");

    /**
     * A user-assigned rating for this file. The value shall be -1 or in the range
     * [0..5], where -1 indicates rejected and 0 indicates unrated. If xmp:Rating
     * is not present, a value of 0 should be assumed.
     */
    Property RATING = Property.externalReal(PREFIX_ + "Rating");

}
"
tika-core/src/main/java/org/apache/tika/metadata/XMPDM.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.metadata;

import java.util.Date;

/**
 * XMP Dynamic Media schema. This is a collection of
 * {@link Property property definition} constants for the dynamic media
 * properties defined in the XMP standard.
 *
 * @since Apache Tika 0.7
 * @see <a href="http://wwwimages.adobe.com/content/dam/Adobe/en/devnet/xmp/pdfs/cc-201306/XMPSpecificationPart2.pdf"
 *        >XMP Specification, Part 2: Standard Schemas</a>
 */
public interface XMPDM {

    /**
     * "The absolute path to the file's peak audio file. If empty, no peak
     * file exists."
     */
    Property ABS_PEAK_AUDIO_FILE_PATH =
        Property.internalURI("xmpDM:absPeakAudioFilePath");

    /**
     * "The name of the album."
     */
    Property ALBUM = Property.externalText("xmpDM:album");

    /**
     * "An alternative tape name, set via the project window or timecode
     * dialog in Premiere. If an alternative name has been set and has not
     * been reverted, that name is displayed."
     */
    Property ALT_TAPE_NAME = Property.externalText("xmpDM:altTapeName");

//    /**
//     * "A timecode set by the user. When specified, it is used instead
//     * of the startTimecode."
//     */
//    Property ALT_TIMECODE = "xmpDM:altTimecode";

    /**
     * "The name of the artist or artists."
     */
    Property ARTIST = Property.externalText("xmpDM:artist");

    /**
     * "The name of the album artist or group for compilation albums."
     */
    Property ALBUM_ARTIST = Property.externalText("xmpDM:albumArtist");

    /**
     * "The date and time when the audio was last modified."
     */
    Property AUDIO_MOD_DATE = Property.internalDate("xmpDM:audioModDate");

    /**
     * "The audio sample rate. Can be any value, but commonly 32000, 41100,
     * or 48000."
     */
    Property AUDIO_SAMPLE_RATE =
        Property.internalInteger("xmpDM:audioSampleRate");

    /**
     * "The audio sample type."
     */
    Property AUDIO_SAMPLE_TYPE = Property.internalClosedChoise(
            "xmpDM:audioSampleType", "8Int", "16Int", "32Int", "32Float");

    /**
     * "The audio channel type."
     */
    Property AUDIO_CHANNEL_TYPE = Property.internalClosedChoise(
            "xmpDM:audioChannelType", "Mono", "Stereo", "5.1", "7.1");
    /**
     * Converter for {@link XMPDM#AUDIO_CHANNEL_TYPE}
     * @deprecated Experimental method, will change shortly
     */
    @Deprecated
    static class ChannelTypePropertyConverter {
       private static Property property = AUDIO_CHANNEL_TYPE;

       /**
        * How a standalone converter might work
        */
       public static String convert(Object value) {
          if (value instanceof String) {
             // Assume already done
             return (String)value;
          }
          if (value instanceof Integer) {
             int channelCount = (Integer)value;
             if(channelCount == 1) {
                return "Mono";
             } else if(channelCount == 2) {
                return "Stereo";
             } else if(channelCount == 5) {
                return "5.1";
             } else if(channelCount == 7) {
                return "7.1";
             }
          }
          return null;
       }
       /**
        * How convert+set might work
        */
       public static void convertAndSet(Metadata metadata, Object value) {
          if (value instanceof Integer || value instanceof Long) {
             metadata.set(property, convert(value));
          }
          if (value instanceof Date) {
             // Won't happen in this case, just an example of already
             //  converted to a type metadata.set(property) handles
             metadata.set(property, (Date)value);
          }
          if (value instanceof String) {
             // Already converted, or so we hope!
             metadata.set(property, (String)value);
          }
       }
    }

    /**
     * "The audio compression used. For example, MP3."
     */
    Property AUDIO_COMPRESSOR = Property.internalText("xmpDM:audioCompressor");

//    /**
//     * "Additional parameters for Beat Splice stretch mode."
//     */
//    Property BEAT_SPLICE_PARAMS = "xmpDM:beatSpliceParams";

    /**
     * "An album created by various artists."
     */
    Property COMPILATION = Property.externalInteger("xmpDM:compilation");

    /**
     * "The composer's name."
     */
    Property COMPOSER = Property.externalText("xmpDM:composer");

//    /**
//     * "An unordered list of all media used to create this media."
//     */
//    Property CONTRIBUTED_MEDIA = "xmpDM:contributedMedia";

    /**
     * "The copyright information."
     */
    Property COPYRIGHT = Property.externalText("xmpDM:copyright");

    /**
     * "The disc number for part of an album set."
     */
    Property DISC_NUMBER = Property.externalInteger("xmpDM:discNumber");

    /**
     * "The duration of the media file."
     */
    Property DURATION = Property.externalReal("xmpDM:duration");

    /**
     * "The engineer's name."
     */
    Property ENGINEER = Property.externalText("xmpDM:engineer");

    /**
     * "The file data rate in megabytes per second. For example:
     * '36/10' = 3.6 MB/sec"
     */
    Property FILE_DATA_RATE = Property.internalRational("xmpDM:fileDataRate");

    /**
     * "The name of the genre."
     */
    Property GENRE = Property.externalText("xmpDM:genre");

    /**
     * "The musical instrument."
     */
    Property INSTRUMENT = Property.externalText("xmpDM:instrument");

//    /**
//     * "The duration of lead time for queuing music."
//     */
//    Property INTRO_TIME = "xmpDM:introTime";

    /**
     * "The audio's musical key."
     */
    Property KEY = Property.internalClosedChoise(
            "xmpDM:key", "C", "C#", "D", "D#", "E", "F", "F#",
            "G", "G#", "A", "A#", "B");

    /**
     * "User's log comments."
     */
    Property LOG_COMMENT = Property.externalText("xmpDM:logComment");

    /**
     * "When true, the clip can be looped seamlessly."
     */
    Property LOOP = Property.internalBoolean("xmpDM:loop");

    /**
     * "The number of beats."
     */
    Property NUMBER_OF_BEATS = Property.internalReal("xmpDM:numberOfBeats");

//    /**
//     * An ordered list of markers. See also {@link #TRACKS xmpDM:Tracks}.
//     */
//    Property MARKERS = "xmpDM:markers";

    /**
     * "The date and time when the metadata was last modified."
     */
    Property METADATA_MOD_DATE = Property.internalDate("xmpDM:metadataModDate");

//    /**
//     * "The time at which to fade out."
//     */
//    Property OUT_CUE = "xmpDM:outCue";

//    /**
//     * "A reference to the project that created this file."
//     */
//    Property PROJECT_REF = "xmpDM:projectRef";

    /**
     * "The sampling phase of film to be converted to video (pull-down)."
     */
    Property PULL_DOWN = Property.internalClosedChoise(
            "xmpDM:pullDown", "WSSWW", "SSWWW", "SWWWS", "WWWSS", "WWSSW",
            "WSSWW_24p", "SSWWW_24p", "SWWWS_24p", "WWWSS_24p", "WWSSW_24p");

    /**
     * "The relative path to the file's peak audio file. If empty, no peak
     * file exists."
     */
    Property RELATIVE_PEAK_AUDIO_FILE_PATH =
        Property.internalURI("xmpDM:relativePeakAudioFilePath");

//    /**
//     * "The start time of the media inside the audio project."
//     */
//    Property RELATIVE_TIMESTAMP = "xmpDM:relativeTimestamp";

    /**
     * "The date the title was released."
     */
    Property RELEASE_DATE = Property.externalDate("xmpDM:releaseDate");

//    /**
//     * "Additional parameters for Resample stretch mode."
//     */
//    Property RESAMPLE_PARAMS = "xmpDM:resampleParams";

    /**
     * "The musical scale used in the music. 'Neither' is most often used
     * for instruments with no associated scale, such as drums."
     */
    Property SCALE_TYPE = Property.internalClosedChoise(
            "xmpDM:scaleType", "Major", "Minor", "Both", "Neither");

    /**
     * "The name of the scene."
     */
    Property SCENE = Property.externalText("xmpDM:scene");

    /**
     * "The date and time when the video was shot."
     */
    Property SHOT_DATE = Property.externalDate("xmpDM:shotDate");

    /**
     * "The name of the location where the video was shot. For example:
     * 'Oktoberfest, Munich, Germany'. For more accurate  positioning,
     * use the EXIF GPS values."
     */
    Property SHOT_LOCATION = Property.externalText("xmpDM:shotLocation");

    /**
     * "The name of the shot or take."
     */
    Property SHOT_NAME = Property.externalText("xmpDM:shotName");

    /**
     * "A description of the speaker angles from center front in degrees.
     * For example: 'Left = -30, Right = 30, Center = 0, LFE = 45,
     * Left Surround = -110, Right Surround = 110'"
     */
    Property SPEAKER_PLACEMENT =
        Property.externalText("xmpDM:speakerPlacement");

//    /**
//     * "The timecode of the first frame of video in the file, as obtained
//     * from the device control."
//     */
//    Property START_TIMECODE = "xmpDM:startTimecode";

    /**
     * "The audio stretch mode."
     */
    Property STRETCH_MODE = Property.internalClosedChoise(
            "xmpDM:stretchMode", "Fixed length", "Time-Scale", "Resample",
            "Beat Splice", "Hybrid");

    /**
     * "The name of the tape from which the clip was captured, as set during
     * the capture process."
     */
    Property TAPE_NAME = Property.externalText("xmpDM:tapeName");

    /**
     * "The audio's tempo."
     */
    Property TEMPO = Property.internalReal("xmpDM:tempo");

//    /**
//     * "Additional parameters for Time-Scale stretch mode."
//     */
//    Property TIME_SCALE_PARAMS = "xmpDM:timeScaleParams";

    /**
     * "The time signature of the music."
     */
    Property TIME_SIGNATURE = Property.internalClosedChoise(
            "xmpDM:timeSignature", "2/4", "3/4", "4/4", "5/4", "7/4",
            "6/8", "9/8", "12/8", "other");

    /**
     * "A numeric value indicating the order of the audio file within its
     * original recording."
     */
    Property TRACK_NUMBER = Property.externalInteger("xmpDM:trackNumber");

//    /**
//     * "An unordered list of tracks. A track is a named set of markers,
//     * which can specify a frame rate for all markers in the set.
//     * See also {@link #MARKERS xmpDM:markers}."
//     */
//    Property TRACKS = "xmpDM:Tracks";

    /**
     * "The alpha mode."
     */
    Property VIDEO_ALPHA_MODE = Property.externalClosedChoise(
            "xmpDM:videoAlphaMode", "straight", "pre-multiplied");

//    /**
//     * "A color in CMYK or RGB to be used as the pre-multiple color when
//     * alpha mode is pre-multiplied."
//     */
//    Property VIDEO_ALPHA_PREMULTIPLE_COLOR = "xmpDM:videoAlphaPremultipleColor";

    /**
     * "When true, unity is clear, when false, it is opaque."
     */
    Property VIDEO_ALPHA_UNITY_IS_TRANSPARENT =
        Property.internalBoolean("xmpDM:videoAlphaUnityIsTransparent");

    /**
     * "The color space."
     */
    Property VIDEO_COLOR_SPACE = Property.internalClosedChoise(
            "xmpDM:videoColorSpace", "sRGB", "CCIR-601", "CCIR-709");

    /**
     * "Video compression used. For example, jpeg."
     */
    Property VIDEO_COMPRESSOR = Property.internalText("xmpDM:videoCompressor");

    /**
     * "The field order for video."
     */
    Property VIDEO_FIELD_ORDER = Property.internalClosedChoise(
            "xmpDM:videoFieldOrder", "Upper", "Lower", "Progressive");

    /**
     * "The video frame rate."
     */
    Property VIDEO_FRAME_RATE = Property.internalOpenChoise(
            "xmpDM:videoFrameRate", "24", "NTSC", "PAL");

//    /**
//     * "The frame size. For example: w:720, h: 480, unit:pixels"
//     */
//    Property VIDEO_FRAME_SIZE = "xmpDM:videoFrameSize";

    /**
     * "The date and time when the video was last modified."
     */
    Property VIDEO_MOD_DATE = Property.internalDate("xmpDM:videoModDate");

    /**
     * "The size in bits of each color component of a pixel. Standard
     *  Windows 32-bit pixels have 8 bits per component."
     */
    Property VIDEO_PIXEL_DEPTH = Property.internalClosedChoise(
            "xmpDM:videoPixelDepth", "8Int", "16Int", "32Int", "32Float");

    /**
     * "The aspect ratio, expressed as wd/ht. For example: '648/720' = 0.9"
     */
    Property VIDEO_PIXEL_ASPECT_RATIO =
        Property.internalRational("xmpDM:videoPixelAspectRatio");

}
"
tika-core/src/main/java/org/apache/tika/metadata/XMPIdq.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.metadata;

public interface XMPIdq {

    String NAMESPACE_URI = "http://ns.adobe.com/xmp/identifier/qual/1.0/";

    String PREFIX = "xmpidq";

    /** The xmpidq prefix followed by the colon delimiter */
    String PREFIX_ = PREFIX + ":";

    /**
     * A qualifier providing the name of the formal identification
     * scheme used for an item in the xmp:Identifier array.
     */
    Property SCHEME = Property.externalText(PREFIX_ + "Scheme");

}
"
tika-core/src/main/java/org/apache/tika/metadata/XMPMM.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.metadata;

public interface XMPMM {

    String NAMESPACE_URI = "http://ns.adobe.com/xap/1.0/mm/";

    String PREFIX = "xmpMM";

    /** The xmpMM prefix followed by the colon delimiter */
    String PREFIX_ = PREFIX + ":";

    /**
     * A reference to the resource from which this one is derived.
     * This should be a minimal reference, in which missing
     * components can be assumed to be unchanged.
     * 
     * TODO This property is of type RessourceRef which is a struct
     */
//    Property DERIVED_FROM = Property.externalText(PREFIX_ + "DerivedFrom");

    /**
     * The common identifier for all versions and renditions of a resource.
     */
    Property DOCUMENTID = Property.externalText(PREFIX_ + "DocumentID");

    /**
     * An identifier for a specific incarnation of a resource, updated
     * each time a file is saved.
     */
    Property INSTANCEID = Property.externalText(PREFIX_ + "InstanceID");

    /**
     * The common identifier for the original resource from which
     * the current resource is derived. For example, if you save a
     * resource to a different format, then save that one to another
     * format, each save operation should generate a new
     * xmpMM:DocumentID that uniquely identifies the resource in
     * that format, but should retain the ID of the source file here.
     */
    Property ORIGINAL_DOCUMENTID = Property.externalText(
            PREFIX_ + "OriginalDocumentID");

    /**
     * The rendition class name for this resource. This property
     * should be absent or set to default for a resource that is not
     * a derived rendition
     */
    Property RENDITION_CLASS = Property.externalOpenChoise(
            PREFIX_ + "RenditionClass",
            "default", "draft", "low-res", "proof", "screen", "thumbnail");

    /**
     * Can be used to provide additional rendition parameters that
     * are too complex or verbose to encode in xmpMM:RenditionClass
     */
    Property RENDITION_PARAMS = Property.externalText(
            PREFIX_ + "RenditionParams");

}
"
tika-core/src/main/java/org/apache/tika/metadata/XMPRights.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 * IPTC Metadata Descriptions taken from the IPTC Photo Metadata (July 2010) 
 * standard. These parts Copyright 2010 International Press Telecommunications 
 * Council.
 */
package org.apache.tika.metadata;

/**
 * XMP Rights management schema. 
 * 
 * A collection of property constants for the 
 * rights management properties defined in the XMP 
 * standard.
 * 
 * @since Apache Tika 1.2
 * @see <a href="http://partners.adobe.com/public/developer/en/xmp/sdk/XMPspecification.pdf">XMP Photoshop</a>
 */
public interface XMPRights {

    String NAMESPACE_URI_XMP_RIGHTS = "http://ns.adobe.com/xap/1.0/rights/";
    String PREFIX_XMP_RIGHTS = "xmpRights";

    /** The xmpRights prefix followed by the colon delimiter */
    String PREFIX_ = PREFIX_XMP_RIGHTS + ":";

    /**
     * A Web URL for a rights management certificate.
     */
    Property CERTIFICATE = Property.internalText(PREFIX_ + "Certificate");

    /**
     * When true, indicates that this is a rights-managed resource. When
     * false, indicates that this is a public-domain resource. Omit if the
     * state is unknown.
     */
    Property MARKED = Property.internalBoolean(PREFIX_ + "Marked");

    /**
     * A list of legal owners of the resource.
     */
    Property OWNER = Property.internalTextBag(PREFIX_ + "Owner");

    /**
     * A word or short phrase that identifies a resource as a member of a userdefined collection.
     * TODO This is actually a language alternative property
     */
    Property USAGE_TERMS = Property.internalText(PREFIX_ + "UsageTerms");

    /**
     * A Web URL for a statement of the ownership and usage rights for this resource.
     */
    Property WEB_STATEMENT = Property.internalText(PREFIX_ + "WebStatement");

}
"
tika-core/src/main/java/org/apache/tika/mime/AndClause.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.mime;

import java.util.Arrays;

class AndClause implements Clause {

    private final Clause[] clauses;

    AndClause(Clause... clauses) {
        this.clauses = clauses;
    }

    public boolean eval(byte[] data) {
        for (Clause clause : clauses) {
            if (!clause.eval(data)) {
                return false;
            }
        }
        return true;
    }

    public int size() {
        int size = 0;
        for (Clause clause : clauses) {
            size += clause.size();
        }
        return size;
    }

    public String toString() {
        return "and" + Arrays.toString(clauses);
    }

}
"
tika-core/src/main/java/org/apache/tika/mime/Clause.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.mime;

import java.io.Serializable;

/**
 * Defines a clause to be evaluated.
 */
interface Clause extends Serializable {

    /**
     * Evaluates this clause with the specified chunk of data.
     */
    boolean eval(byte[] data);

    /**
     * Returns the size of this clause. The size of a clause is the number of
     * chars it is composed of.
     */
    int size();

}
"
tika-core/src/main/java/org/apache/tika/mime/HexCoDec.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.mime;

/**
 * 
 * A set of Hex encoding and decoding utility methods.
 * 
 */
public class HexCoDec {

    private static final char[] HEX_CHARS = { '0', '1', '2', '3', '4', '5',
            '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f' };

    /**
     * Decode a hex string
     * 
     * @param hexValue
     *            the string of hex characters
     * @return the decode hex string as bytes.
     */
    public static byte[] decode(String hexValue) {
        return decode(hexValue.toCharArray());
    }

    /**
     * Decode an array of hex chars
     * 
     * @param hexChars
     *            an array of hex characters.
     * @return the decode hex chars as bytes.
     */
    public static byte[] decode(char[] hexChars) {
        return decode(hexChars, 0, hexChars.length);
    }

    /**
     * Decode an array of hex chars.
     * 
     * @param hexChars
     *            an array of hex characters.
     * @param startIndex
     *            the index of the first character to decode
     * @param length
     *            the number of characters to decode.
     * @return the decode hex chars as bytes.
     */
    public static byte[] decode(char[] hexChars, int startIndex, int length) {
        if ((length & 1) != 0)
            throw new IllegalArgumentException("Length must be even");

        byte[] result = new byte[length / 2];
        for (int j = 0; j < result.length; j++) {
            result[j] = (byte) (hexCharToNibble(hexChars[startIndex++]) * 16 + hexCharToNibble(hexChars[startIndex++]));
        }
        return result;
    }

    /**
     * Hex encode an array of bytes
     * 
     * @param bites
     *            the array of bytes to encode.
     * @return the array of hex characters.
     */
    public static char[] encode(byte[] bites) {
        return encode(bites, 0, bites.length);
    }

    /**
     * Hex encode an array of bytes
     * 
     * @param bites
     *            the array of bytes to encode.
     * @param startIndex
     *            the index of the first character to encode.
     * @param length
     *            the number of characters to encode.
     * @return the array of hex characters.
     */
    public static char[] encode(byte[] bites, int startIndex, int length) {
        char[] result = new char[length * 2];
        for (int i = 0, j = 0; i < length; i++) {
            int bite = bites[startIndex++] & 0xff;
            result[j++] = HEX_CHARS[bite >> 4];
            result[j++] = HEX_CHARS[bite & 0xf];
        }
        return result;
    }

    /**
     * Internal method to turn a hex char into a nibble.
     */
    private static int hexCharToNibble(char ch) {
        if ((ch >= '0') && (ch <= '9')) {
            return ch - '0';
        } else if ((ch >= 'a') && (ch <= 'f')) {
            return ch - 'a' + 10;
        } else if ((ch >= 'A') && (ch <= 'F')) {
            return ch - 'A' + 10;
        } else {
            throw new IllegalArgumentException("Not a hex char - '" + ch + "'");
        }
    }

}
"
tika-core/src/main/java/org/apache/tika/mime/Magic.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.mime;

/**
 * Defines a magic for a MimeType. A magic is made of one or several
 * MagicClause.
 * 
 * 
 */
class Magic implements Clause, Comparable<Magic> {

    private final MimeType type;

    private final int priority;

    private final Clause clause;

    private final String string;

    Magic(MimeType type, int priority, Clause clause) {
        this.type = type;
        this.priority = priority;
        this.clause = clause;
        this.string = "[" + priority + "/" + clause + "]";
    }

    MimeType getType() {
        return type;
    }

    int getPriority() {
        return priority;
    }

    public boolean eval(byte[] data) {
        return clause.eval(data);
    }

    public int size() {
        return clause.size();
    }

    public String toString() {
        return string;
    }

    public int compareTo(Magic o) {
        int diff = o.priority - priority;
        if (diff == 0) {
            diff = o.size() - size();
        }
        if (diff == 0) {
            diff = o.type.compareTo(type);
        }
        if (diff == 0) {
            diff = o.string.compareTo(string);
        }
        return diff;
    }

    public boolean equals(Object o) {
        if (o instanceof Magic) {
            Magic that = (Magic) o;
            return type.equals(that.type) && string.equals(that.string);
        }
        return false;
    }

    public int hashCode() {
        return type.hashCode() ^ string.hashCode();
    }

}
"
tika-core/src/main/java/org/apache/tika/mime/MagicMatch.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.mime;

import java.io.ByteArrayInputStream;
import java.io.IOException;

import org.apache.tika.detect.MagicDetector;
import org.apache.tika.metadata.Metadata;

/**
 * Defines a magic match.
 */
class MagicMatch implements Clause {

    private final MediaType mediaType;

    private final String type;

    private final String offset;

    private final String value;

    private final String mask;

    private MagicDetector detector = null;

    MagicMatch(
            MediaType mediaType,
            String type, String offset, String value, String mask) {
        this.mediaType = mediaType;
        this.type = type;
        this.offset = offset;
        this.value = value;
        this.mask = mask;
    }

    private synchronized MagicDetector getDetector() {
        if (detector == null) {
            detector = MagicDetector.parse(mediaType, type, offset, value, mask);
        }
        return detector;
    }

    public boolean eval(byte[] data) {
        try {
            return getDetector().detect(
                    new ByteArrayInputStream(data), new Metadata())
                    != MediaType.OCTET_STREAM;
        } catch (IOException e) {
            // Should never happen with a ByteArrayInputStream
            return false;
        }
    }

    public int size() {
        return getDetector().getLength();
    }

    public String toString() {
        return mediaType.toString()
                + " " + type + " " + offset + " " +  value + " " + mask;
    }

}
"
tika-core/src/main/java/org/apache/tika/mime/MediaType.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.mime;

import java.io.Serializable;
import java.nio.charset.Charset;
import java.util.Collections;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Locale;
import java.util.Map;
import java.util.Set;
import java.util.SortedMap;
import java.util.TreeMap;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

/**
 * Internet media type.
 */
public final class MediaType implements Comparable<MediaType>, Serializable {

    /**
     * Serial version UID.
     */
    private static final long serialVersionUID = -3831000556189036392L;

    private static final Pattern SPECIAL =
        Pattern.compile("[\\(\\)<>@,;:\\\\\"/\\[\\]\\?=]");

    private static final Pattern SPECIAL_OR_WHITESPACE =
        Pattern.compile("[\\(\\)<>@,;:\\\\\"/\\[\\]\\?=\\s]");

    /**
     * See http://www.ietf.org/rfc/rfc2045.txt for valid mime-type characters.
     */
    private static final String VALID_CHARS =
            "([^\\c\\(\\)<>@,;:\\\\\"/\\[\\]\\?=\\s]+)";

    private static final Pattern TYPE_PATTERN = Pattern.compile(
                    "(?s)\\s*" + VALID_CHARS + "\\s*/\\s*" + VALID_CHARS
                    + "\\s*($|;.*)");

    // TIKA-350: handle charset as first element in content-type
    private static final Pattern CHARSET_FIRST_PATTERN = Pattern.compile(
            "(?is)\\s*(charset\\s*=\\s*[^\\c;\\s]+)\\s*;\\s*"
            + VALID_CHARS + "\\s*/\\s*" + VALID_CHARS + "\\s*");

    /**
     * Set of basic types with normalized "type/subtype" names.
     * Used to optimize type lookup and to avoid having too many
     * {@link MediaType} instances in memory.
     */
    private static final Map<String, MediaType> SIMPLE_TYPES =
            new HashMap<String, MediaType>();

    public static final MediaType OCTET_STREAM =
            parse("application/octet-stream");

    public static final MediaType TEXT_PLAIN = parse("text/plain");

    public static final MediaType TEXT_HTML = parse("text/html");

    public static final MediaType APPLICATION_XML = parse("application/xml");

    public static final MediaType APPLICATION_ZIP = parse("application/zip");

    public static MediaType application(String type) {
        return MediaType.parse("application/" + type);
    }

    public static MediaType audio(String type) {
        return MediaType.parse("audio/" + type);
    }

    public static MediaType image(String type) {
        return MediaType.parse("image/" + type);
    }

    public static MediaType text(String type) {
        return MediaType.parse("text/" + type);
    }

    public static MediaType video(String type) {
        return MediaType.parse("video/" + type);
    }

    /**
     * Convenience method that returns an unmodifiable set that contains
     * all the given media types.
     *
     * @since Apache Tika 1.2
     * @param types media types
     * @return unmodifiable set of the given types
     */
    public static Set<MediaType> set(MediaType... types) {
        Set<MediaType> set = new HashSet<MediaType>();
        for (MediaType type : types) {
            if (type != null) {
                set.add(type);
            }
        }
        return Collections.unmodifiableSet(set);
    }

    /**
     * Convenience method that parses the given media type strings and
     * returns an unmodifiable set that contains all the parsed types.
     *
     * @since Apache Tika 1.2
     * @param types media type strings
     * @return unmodifiable set of the parsed types
     */
    public static Set<MediaType> set(String... types) {
        Set<MediaType> set = new HashSet<MediaType>();
        for (String type : types) {
            MediaType mt = parse(type);
            if (mt != null) {
                set.add(mt);
            }
        }
        return Collections.unmodifiableSet(set);
    }

    /**
     * Parses the given string to a media type. The string is expected
     * to be of the form "type/subtype(; parameter=...)*" as defined in
     * RFC 2045, though we also handle "charset=xxx; type/subtype" for
     * broken web servers.
     *
     * @param string media type string to be parsed
     * @return parsed media type, or <code>null</code> if parsing fails
     */
    public static MediaType parse(String string) {
        if (string == null) {
            return null;
        }

        // Optimization for the common cases
        synchronized (SIMPLE_TYPES) {
            MediaType type = SIMPLE_TYPES.get(string);
            if (type == null) {
                int slash = string.indexOf('/');
                if (slash == -1) {
                    return null;
                } else if (SIMPLE_TYPES.size() < 10000
                        && isSimpleName(string.substring(0, slash))
                        && isSimpleName(string.substring(slash + 1))) {
                    type = new MediaType(string, slash);
                    SIMPLE_TYPES.put(string, type);
                }
            }
            if (type != null) {
                return type;
            }
        }

        Matcher matcher;
        matcher = TYPE_PATTERN.matcher(string);
        if (matcher.matches()) {
            return new MediaType(
                    matcher.group(1), matcher.group(2),
                    parseParameters(matcher.group(3)));
        }
        matcher = CHARSET_FIRST_PATTERN.matcher(string);
        if (matcher.matches()) {
            return new MediaType(
                    matcher.group(2), matcher.group(3),
                    parseParameters(matcher.group(1)));
        }

        return null;
    }

    private static boolean isSimpleName(String name) {
        for (int i = 0; i < name.length(); i++) {
            char c = name.charAt(i);
            if (c != '-' && c != '+' && c != '.' && c != '_'
                    && !('0' <= c && c <= '9')
                    && !('a' <= c && c <= 'z')) {
                return false;
            }
        }
        return name.length() > 0;
    }

    private static Map<String, String> parseParameters(String string) {
        if (string.length() == 0) {
            return Collections.<String, String>emptyMap();
        }

        // Extracts k1=v1, k2=v2 from mime/type; k1=v1; k2=v2
        // Note - this logic isn't fully RFC2045 compliant yet, as it
        //  doesn't fully handle quoted keys or values (eg containing ; or =)
        Map<String, String> parameters = new HashMap<String, String>();
        while (string.length() > 0) {
            String key = string;
            String value = "";

            int semicolon = string.indexOf(';');
            if (semicolon != -1) {
                key = string.substring(0, semicolon);
                string = string.substring(semicolon + 1);
            } else {
                string = "";
            }

            int equals = key.indexOf('=');
            if (equals != -1) {
                value = key.substring(equals + 1);
                key = key.substring(0, equals);
            }

            key = key.trim();
            if (key.length() > 0) {
                parameters.put(key, unquote(value.trim()));
            }
        }
        return parameters;
    }

    /**
     * Fuzzy unquoting mechanism that works also with somewhat malformed
     * quotes.
     *
     * @param s string to unquote
     * @return unquoted string
     */
    private static String unquote(String s) {
        while (s.startsWith("\"") || s.startsWith("'")) {
            s = s.substring(1);
        }
        while (s.endsWith("\"") || s.endsWith("'")) {
            s = s.substring(0, s.length() - 1);
        }
        return s;
    }

    /**
     * Canonical string representation of this media type.
     */
    private final String string;

    /**
     * Location of the "/" character separating the type and the subtype
     * tokens in {@link #string}.
     */
    private final int slash;

    /**
     * Location of the first ";" character separating the type part of
     * {@link #string} from possible parameters. Length of {@link #string}
     * in case there are no parameters.
     */
    private final int semicolon;

    /**
     * Immutable sorted map of media type parameters.
     */
    private final Map<String, String> parameters;

    public MediaType(
            String type, String subtype, Map<String, String> parameters) {
        type = type.trim().toLowerCase(Locale.ENGLISH);
        subtype = subtype.trim().toLowerCase(Locale.ENGLISH);

        this.slash = type.length();
        this.semicolon = slash + 1 + subtype.length();

        if (parameters.isEmpty()) {
            this.parameters = Collections.emptyMap();
            this.string = type + '/' + subtype;
        } else {
            StringBuilder builder = new StringBuilder();
            builder.append(type);
            builder.append('/');
            builder.append(subtype);

            SortedMap<String, String> map = new TreeMap<String, String>();
            for (Map.Entry<String, String> entry : parameters.entrySet()) {
                String key = entry.getKey().trim().toLowerCase(Locale.ENGLISH);
                map.put(key, entry.getValue());
            }
            for (Map.Entry<String, String> entry : map.entrySet()) {
                builder.append("; ");
                builder.append(entry.getKey());
                builder.append("=");
                String value = entry.getValue();
                if (SPECIAL_OR_WHITESPACE.matcher(value).find()) {
                    builder.append('"');
                    builder.append(SPECIAL.matcher(value).replaceAll("\\\\$0"));
                    builder.append('"');
                } else {
                    builder.append(value);
                }
            }

            this.string = builder.toString();
            this.parameters = Collections.unmodifiableSortedMap(map);
        }
    }

    public MediaType(String type, String subtype) {
        this(type, subtype, Collections.<String, String>emptyMap());
    }

    private MediaType(String string, int slash) {
        assert slash != -1;
        assert string.charAt(slash) == '/';
        assert isSimpleName(string.substring(0, slash));
        assert isSimpleName(string.substring(slash + 1));
        this.string = string;
        this.slash = slash;
        this.semicolon = string.length();
        this.parameters = Collections.emptyMap();
    }

    private static Map<String, String> union(
            Map<String, String> a, Map<String, String> b) {
        if (a.isEmpty()) {
            return b;
        } else if (b.isEmpty()) {
            return a;
        } else {
            Map<String, String> union = new HashMap<String, String>();
            union.putAll(a);
            union.putAll(b);
            return union;
        }
    }

    public MediaType(MediaType type, Map<String, String> parameters) {
        this(type.getType(), type.getSubtype(),
                union(type.parameters, parameters));
    }

    /**
     * Creates a media type by adding a parameter to a base type.
     *
     * @param type base type
     * @param name parameter name
     * @param value parameter value
     * @since Apache Tika 1.2
     */
    public MediaType(MediaType type, String name, String value) {
        this(type, Collections.singletonMap(name, value));
    }

    /**
     * Creates a media type by adding the "charset" parameter to a base type.
     *
     * @param type base type
     * @param charset charset value
     * @since Apache Tika 1.2
     */
    public MediaType(MediaType type, Charset charset) {
        this(type, "charset", charset.name());
    }
    /**
     * Returns the base form of the MediaType, excluding
     *  any parameters, such as "text/plain" for
     *  "text/plain; charset=utf-8"
     */
    public MediaType getBaseType() {
        if (parameters.isEmpty()) {
            return this;
        } else {
            return MediaType.parse(string.substring(0, semicolon));
        }
    }

    /**
     * Return the Type of the MediaType, such as
     *  "text" for "text/plain"
     */
    public String getType() {
        return string.substring(0, slash);
    }

    /**
     * Return the Sub-Type of the MediaType, 
     *  such as "plain" for "text/plain"
     */
    public String getSubtype() {
        return string.substring(slash + 1, semicolon);
    }

    /**
     * Checks whether this media type contains parameters.
     *
     * @since Apache Tika 0.8
     * @return <code>true</code> if this type has one or more parameters,
     *         <code>false</code> otherwise
     */
    public boolean hasParameters() {
        return !parameters.isEmpty();
    }

    /**
     * Returns an immutable sorted map of the parameters of this media type.
     * The parameter names are guaranteed to be trimmed and in lower case.
     *
     * @return sorted map of parameters
     */
    public Map<String, String> getParameters() {
        return parameters;
    }

    public String toString() {
        return string;
    }

    public boolean equals(Object object) {
        if (object instanceof MediaType) {
            MediaType that = (MediaType) object;
            return string.equals(that.string);
        } else {
            return false;
        }
    }

    public int hashCode() {
        return string.hashCode();
    }

    public int compareTo(MediaType that) {
        return string.compareTo(that.string);
    }

}
"
tika-core/src/main/java/org/apache/tika/mime/MediaTypeRegistry.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.mime;

import java.io.Serializable;
import java.util.HashMap;
import java.util.Map;
import java.util.SortedSet;
import java.util.TreeSet;

/**
 * Registry of known Internet media types.
 */
public class MediaTypeRegistry implements Serializable {

    /** Serial version UID */
    private static final long serialVersionUID = 4710974869988895410L;

    /**
     * Returns the built-in media type registry included in Tika.
     *
     * @since Apache Tika 0.8
     * @return default media type registry
     */
    public static MediaTypeRegistry getDefaultRegistry() {
        return MimeTypes.getDefaultMimeTypes().getMediaTypeRegistry();
    }

    /**
     * Registry of known media types, including type aliases. A canonical
     * media type is handled as an identity mapping, while an alias is stored
     * as a mapping from the alias to the corresponding canonical type.
     */
    private final Map<MediaType, MediaType> registry =
        new HashMap<MediaType, MediaType>();

    /**
     * Known type inheritance relationships. The mapping is from a media type
     * to the closest supertype.
     */
    private final Map<MediaType, MediaType> inheritance =
        new HashMap<MediaType, MediaType>();

    /**
     * Returns the set of all known canonical media types. Type aliases are
     * not included in the returned set.
     *
     * @since Apache Tika 0.8
     * @return canonical media types
     */
    public SortedSet<MediaType> getTypes() {
        return new TreeSet<MediaType>(registry.values());
    }

    /**
     * Returns the set of known aliases of the given canonical media type.
     *
     * @since Apache Tika 0.8
     * @param type canonical media type
     * @return known aliases
     */
    public SortedSet<MediaType> getAliases(MediaType type) {
        SortedSet<MediaType> aliases = new TreeSet<MediaType>();
        for (Map.Entry<MediaType, MediaType> entry : registry.entrySet()) {
            if (entry.getValue().equals(type) && !entry.getKey().equals(type)) {
                aliases.add(entry.getKey());
            }
        }
        return aliases;
    }

    public void addType(MediaType type) {
        registry.put(type, type);
    }

    public void addAlias(MediaType type, MediaType alias) {
        registry.put(alias, type);
    }

    public void addSuperType(MediaType type, MediaType supertype) {
        inheritance.put(type, supertype);
    }

    public MediaType normalize(MediaType type) {
        if (type == null) {
            return null;
        }
        MediaType canonical = registry.get(type.getBaseType());
        if (canonical == null) {
            return type;
        } else if (type.hasParameters()) {
            return new MediaType(canonical, type.getParameters());
        } else {
            return canonical;
        }
    }

    /**
     * Checks whether the given media type a is a specialization of a more
     * generic type b. Both types should be already normalised.
     *
     * @since Apache Tika 0.8
     * @param a media type, normalised
     * @param b suspected supertype, normalised
     * @return <code>true</code> if b is a supertype of a,
     *         <code>false</code> otherwise
     */
    public boolean isSpecializationOf(MediaType a, MediaType b) {
        return isInstanceOf(getSupertype(a), b);
    }

    /**
     * Checks whether the given media type equals the given base type or
     * is a specialization of it. Both types should be already normalised.
     *
     * @since Apache Tika 1.2
     * @param a media type, normalised
     * @param b base type, normalised
     * @return <code>true</code> if b equals a or is a specialization of it,
     *         <code>false</code> otherwise
     */
    public boolean isInstanceOf(MediaType a, MediaType b) {
        return a != null && (a.equals(b) || isSpecializationOf(a, b));
    }

    /**
     * Parses and normalises the given media type string and checks whether
     * the result equals the given base type or is a specialization of it.
     * The given base type should already be normalised.
     *
     * @since Apache Tika 1.2
     * @param a media type
     * @param b base type, normalised
     * @return <code>true</code> if b equals a or is a specialization of it,
     *         <code>false</code> otherwise
     */
    public boolean isInstanceOf(String a, MediaType b) {
        return isInstanceOf(normalize(MediaType.parse(a)), b);
    }

    /**
     * Returns the supertype of the given type. If the media type database
     * has an explicit inheritance rule for the type, then that is used. 
     * Next, if the given type has any parameters, then the respective base 
     * type (parameter-less) is returned. Otherwise built-in heuristics like 
     * text/... -&gt; text/plain and .../...+xml -&gt; application/xml are used. 
     * Finally application/octet-stream is returned for all types for which no other
     * supertype is known, and the return value for application/octet-stream
     * is <code>null</code>.
     *
     * @since Apache Tika 0.8
     * @param type media type
     * @return supertype, or <code>null</code> for application/octet-stream
     */
    public MediaType getSupertype(MediaType type) {
        if (type == null) {
            return null;
        } else if (inheritance.containsKey(type)) {
            return inheritance.get(type);
        } else if (type.hasParameters()) {
            return type.getBaseType();
        } else if (type.getSubtype().endsWith("+xml")) {
            return MediaType.APPLICATION_XML;
        } else if (type.getSubtype().endsWith("+zip")) {
            return MediaType.APPLICATION_ZIP;
        } else if ("text".equals(type.getType())
                && !MediaType.TEXT_PLAIN.equals(type)) {
            return MediaType.TEXT_PLAIN;
        } else if (!MediaType.OCTET_STREAM.equals(type)) {
            return MediaType.OCTET_STREAM;
        } else {
            return null;
        }
    }

}
"
tika-core/src/main/java/org/apache/tika/mime/MimeType.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.mime;

import java.io.Serializable;
import java.net.URI;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collections;
import java.util.List;

/**
 * Internet media type.
 */
public final class MimeType implements Comparable<MimeType>, Serializable {

    /**
     * Serial version UID.
     */
    private static final long serialVersionUID = 4357830439860729201L;

    /**
     * Checks that the given string is a valid Internet media type name
     * based on rules from RFC 2054 section 5.3. For validation purposes the
     * rules can be simplified to the following:
     * <pre>
     * name := token "/" token
     * token := 1*&lt;any (US-ASCII) CHAR except SPACE, CTLs, or tspecials&gt;
     * tspecials :=  "(" / ")" / "&lt;" / "&gt;" / "@" / "," / ";" / ":" /
     *               "\" / <"> / "/" / "[" / "]" / "?" / "="
     * </pre>
     *
     * @param name name string
     * @return <code>true</code> if the string is a valid media type name,
     *         <code>false</code> otherwise
     */
    public static boolean isValid(String name) {
        if (name == null) {
            throw new IllegalArgumentException("Name is missing");
        }

        boolean slash = false;
        for (int i = 0; i < name.length(); i++) {
            char ch = name.charAt(i);
            if (ch <= ' ' || ch >= 127 || ch == '(' || ch == ')' ||
                    ch == '<' || ch == '>' || ch == '@' || ch == ',' ||
                    ch == ';' || ch == ':' || ch == '\\' || ch == '"' ||
                    ch == '[' || ch == ']' || ch == '?' || ch == '=') {
                return false;
            } else if (ch == '/') {
                if (slash || i == 0 || i + 1 == name.length()) {
                    return false;
                }
                slash = true;
            }
        }
        return slash;
    }

    /**
     * The normalized media type name.
     */
    private final MediaType type;

    /**
     * The MimeType acronym
     */
    private String acronym = "";

    /**
     * The http://en.wikipedia.org/wiki/Uniform_Type_Identifier
     */
    private String uti = "";
    
    /**
     * Documentation Links
     */
    private List<URI> links = Collections.emptyList();
    
    /**
     * Description of this media type.
     */
    private String description = "";

    /** The magics associated to this Mime-Type */
    private List<Magic> magics = null;

    /** The root-XML associated to this Mime-Type */
    private List<RootXML> rootXML = null;

    /** The minimum length of data to provides for magic analyzis */
    private int minLength = 0;

    /**
     * All known file extensions of this type, in order of preference
     * (best first).
     */
    private List<String> extensions = null;

    /**
     * Creates a media type with the give name and containing media type
     * registry. The name is expected to be valid and normalized to lower
     * case. This constructor should only be called by
     * {@link MimeTypes#forName(String)} to keep the media type registry
     * up to date.
     *
     * @param type normalized media type name
     */
    MimeType(MediaType type) {
        if (type == null) {
            throw new IllegalArgumentException("Media type name is missing");
        }
        this.type = type;
    }

    /**
     * Returns the normalized media type name.
     *
     * @return media type
     */
    public MediaType getType() {
        return type;
    }

    /**
     * Returns the name of this media type.
     *
     * @return media type name (lower case)
     */
    public String getName() {
        return type.toString();
    }

    /**
     * Returns the description of this media type.
     *
     * @return media type description
     */
    public String getDescription() {
        return description;
    }

    /**
     * Set the description of this media type.
     *
     * @param description media type description
     */
    public void setDescription(String description) {
        if (description == null) {
            throw new IllegalArgumentException("Description is missing");
        }
        this.description = description;
    }
    

    /**
     * Returns an acronym for this mime type.
     *
     * @return mime type acronym
     */
    public String getAcronym() {
        return acronym;
    }

    /**
     * Set an acronym for the mime type
     *
     * @param acronym
     */
    void setAcronym(String v) {
        if (v == null) {
            throw new IllegalArgumentException("Acronym is missing");
        }
        acronym = v;
    }
    
    /**
     * Get the UTI for this mime type.
     * 
     * @see http://en.wikipedia.org/wiki/Uniform_Type_Identifier
     * 
     * @return The Uniform Type Identifier
     */
    public String getUniformTypeIdentifier() {
        return uti;
    }

    /**
     * Set The Uniform Type Identifier
     *
     * @param uti
     */
    void setUniformTypeIdentifier(String v) {
        if (v == null) {
            throw new IllegalArgumentException("Uniform Type Identifier is missing");
        }
        uti = v;
    }

    /**
     * Get a list of links to help document this mime type
     * 
     * @return an array of links (will never be null)
     */
    public List<URI> getLinks() {
      return links; // this is already unmodifiable
    }

    /**
     * Add a link to this mime type
     * @param link
     */
    void addLink(URI link) {
        if(link==null) {
            throw new IllegalArgumentException("Missing Link");
        }
        List<URI> copy = new ArrayList<URI>(links.size()+1);
        copy.addAll(links);
        copy.add(link);
        links = Collections.unmodifiableList(copy);
    }


    /**
     * Add some rootXML info to this mime-type
     *
     * @param namespaceURI
     * @param localName
     */
    void addRootXML(String namespaceURI, String localName) {
        if (rootXML == null) {
            rootXML = new ArrayList<RootXML>();
        }
        rootXML.add(new RootXML(this, namespaceURI, localName));
    }

    boolean matchesXML(String namespaceURI, String localName) {
        if (rootXML != null) {
            for (RootXML xml : rootXML) {
                if (xml.matches(namespaceURI, localName)) {
                    return true;
                }
            }
        }
        return false;
    }

    boolean hasRootXML() {
        return rootXML != null;
    }

    List<Magic> getMagics() {
        if (magics != null) {
            return magics;
        } else {
            return Collections.emptyList();
        }
    }

    void addMagic(Magic magic) {
        if (magic == null) {
            return;
        }
        if (magics == null) {
            magics = new ArrayList<Magic>();
        }
        magics.add(magic);
    }

    int getMinLength() {
        return minLength;
    }

    public boolean hasMagic() {
        return magics != null;
    }

    public boolean matchesMagic(byte[] data) {
        for (int i = 0; magics != null && i < magics.size(); i++) {
            Magic magic = magics.get(i);
            if (magic.eval(data)) {
                return true;
            }
        }
        return false;
    }

    public boolean matches(byte[] data) {
        return matchesMagic(data);
    }

    /**
     * Defines a RootXML description. RootXML is made of a localName and/or a
     * namespaceURI.
     */
    static class RootXML implements Serializable {

        /**
         * Serial version UID.
         */
        private static final long serialVersionUID = 5140496601491000730L;

        private MimeType type = null;

        private String namespaceURI = null;

        private String localName = null;

        RootXML(MimeType type, String namespaceURI, String localName) {
            if (isEmpty(namespaceURI) && isEmpty(localName)) {
                throw new IllegalArgumentException(
                        "Both namespaceURI and localName cannot be empty");
            }
            this.type = type;
            this.namespaceURI = namespaceURI;
            this.localName = localName;
        }

        boolean matches(String namespaceURI, String localName) {
            //Compare namespaces
            if (!isEmpty(this.namespaceURI)) {
                if (!this.namespaceURI.equals(namespaceURI)) {
                    return false;
                }
            }
            else{
                // else if it was empty then check to see if the provided namespaceURI
                // is empty. If it is not, then these two aren't equal and return false
                if(!isEmpty(namespaceURI)){
                    return false;
                }
            }

            //Compare root element's local name
            if (!isEmpty(this.localName)) {
                if (!this.localName.equals(localName)) {
                    return false;
                }
            }
            else{
                // else if it was empty then check to see if the provided localName
                // is empty. If it is not, then these two aren't equal and return false 
                if(!isEmpty(localName)){
                    return false;
                }
            }
            return true;
        }

        /**
         * Checks if a string is null or empty.
         */
        private boolean isEmpty(String str) {
            return (str == null) || (str.equals(""));
        }

        MimeType getType() {
            return type;
        }

        String getNameSpaceURI() {
            return namespaceURI;
        }

        String getLocalName() {
            return localName;
        }

        public String toString() {
            return type + ", " + namespaceURI + ", " + localName;
        }
    }

    //----------------------------------------------------------< Comparable >

    public int compareTo(MimeType mime) {
        return type.compareTo(mime.type);
    }

    //--------------------------------------------------------------< Object >

    public boolean equals(Object o) {
        if (o instanceof MimeType) {
            MimeType that = (MimeType) o;
            return this.type.equals(that.type);
        }

        return false;
    }

    public int hashCode() {
        return type.hashCode();
    }

    /**
     * Returns the name of this media type.
     *
     * @return media type name
     */
    public String toString() {
        return type.toString();
    }

    /**
     * Returns the preferred file extension of this type, or an empty string
     * if no extensions are known. Use the {@link #getExtensions()} method to
     * get the full list of known extensions of this type.
     *
     * @since Apache Tika 0.9
     * @return preferred file extension or empty string
     */
    public String getExtension() {
        if (extensions == null) {
            return "";
        } else {
            return extensions.get(0);
        }
    }

    /**
     * Returns the list of all known file extensions of this media type.
     *
     * @since Apache Tika 0.10
     * @return known extensions in order of preference (best first)
     */
    public List<String> getExtensions() {
        if (extensions != null) {
            return Collections.unmodifiableList(extensions);
        } else {
            return Collections.emptyList();
        }
    }

    /**
     * Adds a known file extension to this type.
     *
     * @param extension file extension
     */
    void addExtension(String extension) {
        if (extensions == null) {
            extensions = Collections.singletonList(extension);
        } else if (extensions.size() == 1) {
            extensions = new ArrayList<String>(extensions);
        }
        if (!extensions.contains(extension)) {
            extensions.add(extension);
        }
    }

}
"
tika-core/src/main/java/org/apache/tika/mime/MimeTypeException.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.mime;

import org.apache.tika.exception.TikaException;

/**
 * A class to encapsulate MimeType related exceptions.
 */
public class MimeTypeException extends TikaException {

    /**
     * Constructs a MimeTypeException with the specified detail message.
     * 
     * @param message the detail message.
     */
    public MimeTypeException(String message) {
        super(message);
    }

    /**
     * Constructs a MimeTypeException with the specified detail message
     * and root cause.
     * 
     * @param message the detail message.
     * @param cause root cause
     */
    public MimeTypeException(String message, Throwable cause) {
        super(message, cause);
    }

}
"
tika-core/src/main/java/org/apache/tika/mime/MimeTypes.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.mime;

// JDK imports
import java.io.ByteArrayInputStream;
import java.io.File;
import java.io.IOException;
import java.io.InputStream;
import java.io.Serializable;
import java.net.URI;
import java.net.URISyntaxException;
import java.util.ArrayList;
import java.util.Collections;
import java.util.HashMap;
import java.util.List;
import java.util.Locale;
import java.util.Map;

import javax.xml.namespace.QName;

import org.apache.tika.Tika;
import org.apache.tika.detect.Detector;
import org.apache.tika.detect.TextDetector;
import org.apache.tika.detect.XmlRootExtractor;
import org.apache.tika.metadata.Metadata;

/**
 * This class is a MimeType repository. It gathers a set of MimeTypes and
 * enables to retrieves a content-type from its name, from a file name, or from
 * a magic character sequence.
 * <p>
 * The MIME type detection methods that take an {@link InputStream} as
 * an argument will never reads more than {@link #getMinLength()} bytes
 * from the stream. Also the given stream is never
 * {@link InputStream#close() closed}, {@link InputStream#mark(int) marked},
 * or {@link InputStream#reset() reset} by the methods. Thus a client can
 * use the {@link InputStream#markSupported() mark feature} of the stream
 * (if available) to restore the stream back to the state it was before type
 * detection if it wants to process the stream based on the detected type.
 */
public final class MimeTypes implements Detector, Serializable {

    /**
     * Serial version UID.
     */
    private static final long serialVersionUID = -1350863170146349036L;

    /**
     * Name of the {@link #rootMimeType root} type, application/octet-stream.
     */
    public static final String OCTET_STREAM = "application/octet-stream";

    /**
     * Name of the {@link #textMimeType text} type, text/plain.
     */
    public static final String PLAIN_TEXT = "text/plain";
    
    /**
     * Name of the {@link #xml xml} type, application/xml.
     */
    public static final String XML = "application/xml";

    /**
     * Root type, application/octet-stream.
     */
    private final MimeType rootMimeType;
    private final List<MimeType> rootMimeTypeL;

    /**
     * Text type, text/plain.
     */
    private final MimeType textMimeType;

    /*
     * xml type, application/xml
     */
    private final MimeType xmlMimeType;

    /**
     * Registered media types and their aliases.
     */
    private final MediaTypeRegistry registry = new MediaTypeRegistry();

    /** All the registered MimeTypes indexed on their canonical names */
    private final Map<MediaType, MimeType> types =
        new HashMap<MediaType, MimeType>();

    /** The patterns matcher */
    private Patterns patterns = new Patterns(registry);

    /** Sorted list of all registered magics */
    private final List<Magic> magics = new ArrayList<Magic>();

    /** Sorted list of all registered rootXML */
    private final List<MimeType> xmls = new ArrayList<MimeType>();

    public MimeTypes() {
        rootMimeType = new MimeType(MediaType.OCTET_STREAM);
        textMimeType = new MimeType(MediaType.TEXT_PLAIN);
        xmlMimeType = new MimeType(MediaType.APPLICATION_XML);
        
        rootMimeTypeL = Collections.singletonList(rootMimeType);

        add(rootMimeType);
        add(textMimeType);
        add(xmlMimeType);
    }

    /**
     * Find the Mime Content Type of a document from its name.
     * Returns application/octet-stream if no better match is found.
     *
     * @deprecated Use {@link Tika#detect(String)} instead
     * @param name of the document to analyze.
     * @return the Mime Content Type of the specified document name
     */
    public MimeType getMimeType(String name) {
        MimeType type = patterns.matches(name);
        if (type != null) {
            return type;
        }
        type = patterns.matches(name.toLowerCase(Locale.ENGLISH));
        if (type != null) {
            return type;
        } else {
            return rootMimeType;
        }
    }

    /**
     * Find the Mime Content Type of a document stored in the given file.
     * Returns application/octet-stream if no better match is found.
     *
     * @deprecated Use {@link Tika#detect(File)} instead
     * @param file file to analyze
     * @return the Mime Content Type of the specified document
     * @throws MimeTypeException if the type can't be detected
     * @throws IOException if the file can't be read
     */
    public MimeType getMimeType(File file)
            throws MimeTypeException, IOException {
        return forName(new Tika(this).detect(file));
    }

    /**
     * Returns the MIME type that best matches the given first few bytes
     * of a document stream. Returns application/octet-stream if no better
     * match is found. 
     * <p>
     * If multiple matches are found, the best (highest priority) matching
     * type is returned. If multiple matches are found with the same priority,
     * then all of these are returned.
     * <p>
     * The given byte array is expected to be at least {@link #getMinLength()}
     * long, or shorter only if the document stream itself is shorter.
     *
     * @param data first few bytes of a document stream
     * @return matching MIME type
     */
    private List<MimeType> getMimeType(byte[] data) {
        if (data == null) {
            throw new IllegalArgumentException("Data is missing");
        } else if (data.length == 0) {
            // See https://issues.apache.org/jira/browse/TIKA-483
            return rootMimeTypeL;
        }

        // Then, check for magic bytes
        List<MimeType> result = new ArrayList<MimeType>(1);
        int currentPriority = -1;
        for (Magic magic : magics) {
            if (currentPriority > 0 && currentPriority > magic.getPriority()) {
                break;
            }
            if (magic.eval(data)) {
                result.add(magic.getType());
                currentPriority = magic.getPriority();
            }
        }
 
        if (!result.isEmpty()) {
            for (int i=0; i<result.size(); i++) {
                final MimeType matched = result.get(i);
                
                // When detecting generic XML (or possibly XHTML),
                // extract the root element and match it against known types
                if ("application/xml".equals(matched.getName())
                        || "text/html".equals(matched.getName())) {
                    XmlRootExtractor extractor = new XmlRootExtractor();

                    QName rootElement = extractor.extractRootElement(data);
                    if (rootElement != null) {
                        for (MimeType type : xmls) {
                            if (type.matchesXML(
                                    rootElement.getNamespaceURI(),
                                    rootElement.getLocalPart())) {
                                result.set(i, type);
                                break;
                            }
                        }
                    } else if ("application/xml".equals(matched.getName())) {
                        // Downgrade from application/xml to text/plain since
                        // the document seems not to be well-formed.
                        result.set(i, textMimeType);
                    }
                }
            }
            return result;
        }

        // Finally, assume plain text if no control bytes are found
        try {
            TextDetector detector = new TextDetector(getMinLength());
            ByteArrayInputStream stream = new ByteArrayInputStream(data);
            MimeType type = forName(detector.detect(stream, new Metadata()).toString());
            return Collections.singletonList(type);
        } catch (Exception e) {
            return rootMimeTypeL;
        }
    }

    /**
     * Reads the first {@link #getMinLength()} bytes from the given stream.
     * If the stream is shorter, then the entire content of the stream is
     * returned.
     * <p>
     * The given stream is never {@link InputStream#close() closed},
     * {@link InputStream#mark(int) marked}, or
     * {@link InputStream#reset() reset} by this method.
     *
     * @param stream stream to be read
     * @return first {@link #getMinLength()} (or fewer) bytes of the stream
     * @throws IOException if the stream can not be read
     */
    private byte[] readMagicHeader(InputStream stream) throws IOException {
        if (stream == null) {
            throw new IllegalArgumentException("InputStream is missing");
        }

        byte[] bytes = new byte[getMinLength()];
        int totalRead = 0;

        int lastRead = stream.read(bytes);
        while (lastRead != -1) {
            totalRead += lastRead;
            if (totalRead == bytes.length) {
                return bytes;
            }
            lastRead = stream.read(bytes, totalRead, bytes.length - totalRead);
        }

        byte[] shorter = new byte[totalRead];
        System.arraycopy(bytes, 0, shorter, 0, totalRead);
        return shorter;
    }

    /**
     * Returns the registered media type with the given name (or alias).
     * The named media type is automatically registered (and returned) if
     * it doesn't already exist.
     *
     * @param name media type name (case-insensitive)
     * @return the registered media type with the given name or alias
     * @throws MimeTypeException if the given media type name is invalid
     */
    public MimeType forName(String name) throws MimeTypeException {
        MediaType type = MediaType.parse(name);
        if (type != null) {
            MediaType normalisedType = registry.normalize(type);
            MimeType mime = types.get(normalisedType);
            
            if (mime == null) {
                synchronized (this) {
                   // Double check it didn't already get added while 
                   //  we were waiting for the lock
                   mime = types.get(normalisedType);
                   if (mime == null) {
                      mime = new MimeType(type);
                      add(mime);
                      types.put(type, mime);
                   }
                }
            }
            return mime;
        } else {
            throw new MimeTypeException("Invalid media type name: " + name);
        }
    }

    /**
     * Returns the registered media type with the given name (or alias).
     * 
     * Unlike {@link #forName(String)}, this function will *not* create a new
     * MimeType and register it
     *
     * @param name media type name (case-insensitive)
     * @return the registered media type with the given name or alias
     * @throws MimeTypeException if the given media type name is invalid
     */
    public MimeType getRegisteredMimeType(String name) throws MimeTypeException {
        MediaType type = MediaType.parse(name);
        if (type != null) {
            MediaType normalisedType = registry.normalize(type);
            return types.get(normalisedType);
        } else {
            throw new MimeTypeException("Invalid media type name: " + name);
        }
    }
    
    public synchronized void setSuperType(MimeType type, MediaType parent) {
        registry.addSuperType(type.getType(), parent);
    }

    /**
     * Adds an alias for the given media type. This method should only
     * be called from {@link MimeType#addAlias(String)}.
     *
     * @param type media type
     * @param alias media type alias (normalized to lower case)
     */
    synchronized void addAlias(MimeType type, MediaType alias) {
        registry.addAlias(type.getType(), alias);
    }

    /**
     * Adds a file name pattern for the given media type. Assumes that the
     * pattern being added is <b>not</b> a JDK standard regular expression.
     *
     * @param type
     *            media type
     * @param pattern
     *            file name pattern
     * @throws MimeTypeException
     *             if the pattern conflicts with existing ones
     */
    public void addPattern(MimeType type, String pattern)
            throws MimeTypeException {
        this.addPattern(type, pattern, false);
    }

    /**
     * Adds a file name pattern for the given media type. The caller can specify
     * whether the pattern being added <b>is</b> or <b>is not</b> a JDK standard
     * regular expression via the <code>isRegex</code> parameter. If the value
     * is set to true, then a JDK standard regex is assumed, otherwise the
     * freedesktop glob type is assumed.
     *
     * @param type
     *            media type
     * @param pattern
     *            file name pattern
     * @param isRegex
     *            set to true if JDK std regexs are desired, otherwise set to
     *            false.
     * @throws MimeTypeException
     *             if the pattern conflicts with existing ones.
     *
     */
    public void addPattern(MimeType type, String pattern, boolean isRegex)
            throws MimeTypeException {
        patterns.add(pattern, isRegex, type);
    }

    public MediaTypeRegistry getMediaTypeRegistry() {
        return registry;
    }

    /**
     * Return the minimum length of data to provide to analyzing methods based
     * on the document's content in order to check all the known MimeTypes.
     *
     * @return the minimum length of data to provide.
     * @see #getMimeType(byte[])
     * @see #getMimeType(String, byte[])
     */
    public int getMinLength() {
        // This needs to be reasonably large to be able to correctly detect
        // things like XML root elements after initial comment and DTDs
        return 64 * 1024;
    }

    /**
     * Add the specified mime-type in the repository.
     *
     * @param type
     *            is the mime-type to add.
     */
    void add(MimeType type) {
        registry.addType(type.getType());
        types.put(type.getType(), type);

        // Update the magics index...
        if (type.hasMagic()) {
            magics.addAll(type.getMagics());
        }

        // Update the xml (xmlRoot) index...
        if (type.hasRootXML()) {
            xmls.add(type);
        }
    }

    /**
     * Called after all configured types have been loaded.
     * Initializes the magics and xmls sets.
     */
    void init() {
        for (MimeType type : types.values()) {
            magics.addAll(type.getMagics());
            if (type.hasRootXML()) {
                xmls.add(type);
            }
        }
        Collections.sort(magics);
        Collections.sort(xmls);
    }

    /**
     * Automatically detects the MIME type of a document based on magic
     * markers in the stream prefix and any given metadata hints.
     * <p>
     * The given stream is expected to support marks, so that this method
     * can reset the stream to the position it was in before this method
     * was called.
     *
     * @param input document stream, or <code>null</code>
     * @param metadata metadata hints
     * @return MIME type of the document
     * @throws IOException if the document stream could not be read
     */
    public MediaType detect(InputStream input, Metadata metadata)
            throws IOException {
        List<MimeType> possibleTypes = null;

        // Get type based on magic prefix
        if (input != null) {
            input.mark(getMinLength());
            try {
                byte[] prefix = readMagicHeader(input);
                possibleTypes = getMimeType(prefix);
            } finally {
                input.reset();
            }
        }

        // Get type based on resourceName hint (if available)
        String resourceName = metadata.get(Metadata.RESOURCE_NAME_KEY);
        if (resourceName != null) {
            String name = null;

            // Deal with a URI or a path name in as the resource  name
            try {
                URI uri = new URI(resourceName);
                String path = uri.getPath();
                if (path != null) {
                    int slash = path.lastIndexOf('/');
                    if (slash + 1 < path.length()) {
                        name = path.substring(slash + 1);
                    }
                }
            } catch (URISyntaxException e) {
                name = resourceName;
            }

            if (name != null) {
                MimeType hint = getMimeType(name);
                
                // If we have some types based on mime magic, try to specialise
                //  and/or select the type based on that
                // Otherwise, use the type identified from the name
                possibleTypes = applyHint(possibleTypes, hint);
            }
        }

        // Get type based on metadata hint (if available)
        String typeName = metadata.get(Metadata.CONTENT_TYPE);
        if (typeName != null) {
            try {
                MimeType hint = forName(typeName);
                possibleTypes = applyHint(possibleTypes, hint);
            } catch (MimeTypeException e) {
                // Malformed type name, ignore
            }
        }

        if (possibleTypes == null || possibleTypes.isEmpty()) {
            // Report that we don't know what it is
            return MediaType.OCTET_STREAM;
        } else {
            return possibleTypes.get(0).getType();
        }
    }
    /**
     * Use the MimeType hint to try to clarify or specialise the current
     *  possible types list.
     * If the hint is a specialised form, use that instead
     * If there are multiple possible types, use the hint to select one
     */
    private List<MimeType> applyHint(List<MimeType> possibleTypes, MimeType hint) {
        if (possibleTypes == null || possibleTypes.isEmpty()) {
            return Collections.singletonList(hint);
        } else {
            for (int i=0; i<possibleTypes.size(); i++) {
                final MimeType type = possibleTypes.get(i);
                if (hint.equals(type) ||
                    registry.isSpecializationOf(hint.getType(), type.getType())) {
                    // Use just this type
                    return Collections.singletonList(hint);
                }
            }
        }
        
        // Hint didn't help, sorry
        return possibleTypes;
    }

    private static MimeTypes DEFAULT_TYPES = null;
    private static Map<ClassLoader,MimeTypes> CLASSLOADER_SPECIFIC_DEFAULT_TYPES =
            new HashMap<ClassLoader, MimeTypes>();

    /**
     * Get the default MimeTypes. This includes all the build in
     * media types, and any custom override ones present.
     * 
     * @return MimeTypes default type registry
     */
    public static synchronized MimeTypes getDefaultMimeTypes() {
        return getDefaultMimeTypes(null);
    }
    /**
     * Get the default MimeTypes. This includes all the built-in
     * media types, and any custom override ones present.
     * 
     * @param ClassLoader to use, if not the default
     * @return MimeTypes default type registry
     */
    public static synchronized MimeTypes getDefaultMimeTypes(ClassLoader classLoader) {
        MimeTypes types = DEFAULT_TYPES;
        if (classLoader != null) {
            types = CLASSLOADER_SPECIFIC_DEFAULT_TYPES.get(classLoader);
        }
            
        if (types == null) {
            try {
                types = MimeTypesFactory.create(
                      "tika-mimetypes.xml", "custom-mimetypes.xml", classLoader);
            } catch (MimeTypeException e) {
                throw new RuntimeException(
                        "Unable to parse the default media type registry", e);
            } catch (IOException e) {
                throw new RuntimeException(
                        "Unable to read the default media type registry", e);
            }
            
            if (classLoader == null) {
                DEFAULT_TYPES = types;
            } else {
                CLASSLOADER_SPECIFIC_DEFAULT_TYPES.put(classLoader, types);
            }
        }
        return types;
    }
}
"
tika-core/src/main/java/org/apache/tika/mime/MimeTypesFactory.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.mime;

import java.io.InputStream;
import java.io.IOException;
import java.net.URL;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;

import org.w3c.dom.Document;

/**
 * Creates instances of MimeTypes.
 */
public class MimeTypesFactory {

    /**
     * Creates an empty instance; same as calling new MimeTypes().
     *
     * @return an empty instance
     */
    public static MimeTypes create() {
        return new MimeTypes();
    }

    /**
     * Creates and returns a MimeTypes instance from the specified document.
     * @throws MimeTypeException if the type configuration is invalid
     */
    public static MimeTypes create(Document document) throws MimeTypeException {
        MimeTypes mimeTypes = new MimeTypes();
        new MimeTypesReader(mimeTypes).read(document);
        mimeTypes.init();
        return mimeTypes;
    }

    /**
     * Creates and returns a MimeTypes instance from the specified input stream.
     * Does not close the input stream(s).
     * @throws IOException if the stream can not be read
     * @throws MimeTypeException if the type configuration is invalid
     */
    public static MimeTypes create(InputStream... inputStreams)
            throws IOException, MimeTypeException {
        MimeTypes mimeTypes = new MimeTypes();
        MimeTypesReader reader = new MimeTypesReader(mimeTypes);
        for(InputStream inputStream : inputStreams) {
           reader.read(inputStream);
        }
        mimeTypes.init();
        return mimeTypes;
    }

    /** @see #create(InputStream...) */
    public static MimeTypes create(InputStream stream)
            throws IOException, MimeTypeException {
        return create(new InputStream[] { stream });
    }

    /**
     * Creates and returns a MimeTypes instance from the resource
     * at the location specified by the URL.  Opens and closes the
     * InputStream from the URL.
     * If multiple URLs are supplied, then they are loaded in turn. 
     *
     * @throws IOException if the URL can not be accessed
     * @throws MimeTypeException if the type configuration is invalid
     */
    public static MimeTypes create(URL... urls)
            throws IOException, MimeTypeException {
        InputStream[] streams = new InputStream[urls.length];
        for(int i=0; i<streams.length; i++) {
           streams[i] = urls[i].openStream();
        }

        try {
            return create(streams);
        } finally {
            for(InputStream stream : streams) {
               stream.close();
            }
        }
    }

    /** @see #create(URL...) */
    public static MimeTypes create(URL url)
            throws IOException, MimeTypeException {
        return create(new URL[] { url });
    }

    /**
     * Creates and returns a MimeTypes instance from the specified file path,
     * as interpreted by the class loader in getResource().
     *
     * @throws IOException if the file can not be accessed
     * @throws MimeTypeException if the type configuration is invalid
     */
    public static MimeTypes create(String filePath)
            throws IOException, MimeTypeException {
        return create(MimeTypesReader.class.getResource(filePath));
    }

    /**
     * Creates and returns a MimeTypes instance. The core mimetypes
     *  will be loaded from the specified file path, and any custom
     *  override mimetypes found will loaded afterwards.
     * The file paths will be interpreted by the default class loader in 
     *  getResource().
     * 
     * @param coreFilePath The main MimeTypes file to load
     * @param extensionFilePath The name of extension MimeType files to load afterwards
     *
     * @throws IOException if the file can not be accessed
     * @throws MimeTypeException if the type configuration is invalid
     */
    public static MimeTypes create(String coreFilePath, String extensionFilePath)
            throws IOException, MimeTypeException {
        return create(coreFilePath, extensionFilePath, null);
    }
    /**
     * Creates and returns a MimeTypes instance. The core mimetypes
     *  will be loaded from the specified file path, and any custom
     *  override mimetypes found will loaded afterwards.
     * The file paths will be interpreted by the specified class  
     *  loader in getResource().
     * 
     * @param coreFilePath The main MimeTypes file to load
     * @param extensionFilePath The name of extension MimeType files to load afterwards
     *
     * @throws IOException if the file can not be accessed
     * @throws MimeTypeException if the type configuration is invalid
     */
    public static MimeTypes create(String coreFilePath, String extensionFilePath,
            ClassLoader classLoader) throws IOException, MimeTypeException {
        // If no specific classloader was requested, use our own class's one
        if (classLoader == null) {
            classLoader = MimeTypesReader.class.getClassLoader();
        }
        
        // This allows us to replicate class.getResource() when using
        //  the classloader directly
        String classPrefix = MimeTypesReader.class.getPackage().getName().replace('.', '/') + "/";
       
        // Get the core URL, and all the extensions URLs
        URL coreURL = classLoader.getResource(classPrefix+coreFilePath);
        List<URL> extensionURLs = Collections.list(
                classLoader.getResources(classPrefix+extensionFilePath));

        // Swap that into an Array, and process
        List<URL> urls = new ArrayList<URL>();
        urls.add(coreURL);
        urls.addAll(extensionURLs);
        
        return create( urls.toArray(new URL[urls.size()]) );
    }
}
"
tika-core/src/main/java/org/apache/tika/mime/MimeTypesReader.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.mime;

import java.io.ByteArrayInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.net.URI;
import java.net.URISyntaxException;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;

import javax.xml.parsers.ParserConfigurationException;
import javax.xml.parsers.SAXParser;
import javax.xml.parsers.SAXParserFactory;
import javax.xml.transform.Transformer;
import javax.xml.transform.TransformerException;
import javax.xml.transform.TransformerFactory;
import javax.xml.transform.dom.DOMSource;
import javax.xml.transform.sax.SAXResult;

import org.w3c.dom.Document;
import org.xml.sax.Attributes;
import org.xml.sax.InputSource;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.DefaultHandler;

/**
 * A reader for XML files compliant with the freedesktop MIME-info DTD.
 * 
 * <pre>
 *  &lt;!DOCTYPE mime-info [
 *    &lt;!ELEMENT mime-info (mime-type)+&gt;
 *    &lt;!ATTLIST mime-info xmlns CDATA #FIXED &quot;http://www.freedesktop.org/standards/shared-mime-info&quot;&gt;
 * 
 *    &lt;!ELEMENT mime-type (comment|acronym|expanded-acronym|glob|magic|root-XML|alias|sub-class-of)*&gt;
 *    &lt;!ATTLIST mime-type type CDATA #REQUIRED&gt;
 * 
 *    &lt;!-- a comment describing a document with the respective MIME type. Example: &quot;WMV video&quot; --&gt;
 *    &lt;!ELEMENT _comment (#PCDATA)&gt;
 *    &lt;!ATTLIST _comment xml:lang CDATA #IMPLIED&gt;
 * 
 *    &lt;!-- a comment describing a the respective unexpanded MIME type acronym. Example: &quot;WMV&quot; --&gt;
 *    &lt;!ELEMENT acronym (#PCDATA)&gt;
 *    &lt;!ATTLIST acronym xml:lang CDATA #IMPLIED&gt;
 * 
 *    &lt;!-- a comment describing a the respective unexpanded MIME type acronym. Example: &quot;Windows Media Video&quot; --&gt;
 *    &lt;!ELEMENT expanded-acronym (#PCDATA)&gt;
 *    &lt;!ATTLIST expanded-acronym xml:lang CDATA #IMPLIED&gt;
 * 
 *    &lt;!ELEMENT glob EMPTY&gt;
 *    &lt;!ATTLIST glob pattern CDATA #REQUIRED&gt;
 *    &lt;!ATTLIST glob isregex CDATA #IMPLIED&gt;
 * 
 *    &lt;!ELEMENT magic (match)+&gt;
 *    &lt;!ATTLIST magic priority CDATA #IMPLIED&gt;
 * 
 *    &lt;!ELEMENT match (match)*&gt;
 *    &lt;!ATTLIST match offset CDATA #REQUIRED&gt;
 *    &lt;!ATTLIST match type (string|big16|big32|little16|little32|host16|host32|byte) #REQUIRED&gt;
 *    &lt;!ATTLIST match value CDATA #REQUIRED&gt;
 *    &lt;!ATTLIST match mask CDATA #IMPLIED&gt;
 * 
 *    &lt;!ELEMENT root-XML EMPTY&gt;
 *    &lt;!ATTLIST root-XML
 *          namespaceURI CDATA #REQUIRED
 *          localName CDATA #REQUIRED&gt;
 * 
 *    &lt;!ELEMENT alias EMPTY&gt;
 *    &lt;!ATTLIST alias
 *          type CDATA #REQUIRED&gt;
 * 
 *   &lt;!ELEMENT sub-class-of EMPTY&gt;
 *   &lt;!ATTLIST sub-class-of
 *         type CDATA #REQUIRED&gt;
 *  ]&gt;
 * </pre>
 * 
 * In addition to the standard fields, this will also read two Tika specific fields:
 *  - link
 *  - uti
 * 
 *
 * @see http://freedesktop.org/wiki/Standards_2fshared_2dmime_2dinfo_2dspec
 */
public class MimeTypesReader extends DefaultHandler implements MimeTypesReaderMetKeys {
    protected final MimeTypes types;

    /** Current type */
    protected MimeType type = null;

    protected int priority;

    protected StringBuilder characters = null;

    protected MimeTypesReader(MimeTypes types) {
        this.types = types;
    }

    public void read(InputStream stream) throws IOException, MimeTypeException {
        try {
            SAXParserFactory factory = SAXParserFactory.newInstance();
            factory.setNamespaceAware(false);
            SAXParser parser = factory.newSAXParser();
            parser.parse(stream, this);
        } catch (ParserConfigurationException e) {
            throw new MimeTypeException("Unable to create an XML parser", e);
        } catch (SAXException e) {
            throw new MimeTypeException("Invalid type configuration", e);
        }
    }

    public void read(Document document) throws MimeTypeException {
        try {
            TransformerFactory factory = TransformerFactory.newInstance();
            Transformer transformer = factory.newTransformer();
            transformer.transform(new DOMSource(document), new SAXResult(this));
        } catch (TransformerException e) {
            throw new MimeTypeException("Failed to parse type registry", e);
        }
    }

    @Override
    public InputSource resolveEntity(String publicId, String systemId) {
        return new InputSource(new ByteArrayInputStream(new byte[0]));
    }

    @Override
    public void startElement(
            String uri, String localName, String qName,
            Attributes attributes) throws SAXException {
        if (type == null) {
            if (MIME_TYPE_TAG.equals(qName)) {
                String name = attributes.getValue(MIME_TYPE_TYPE_ATTR);
                try {
                    type = types.forName(name);
                } catch (MimeTypeException e) {
                    handleMimeError(name, e, qName, attributes);
                }
            }
        } else if (ALIAS_TAG.equals(qName)) {
            String alias = attributes.getValue(ALIAS_TYPE_ATTR);
            types.addAlias(type, MediaType.parse(alias));
        } else if (SUB_CLASS_OF_TAG.equals(qName)) {
            String parent = attributes.getValue(SUB_CLASS_TYPE_ATTR);
            types.setSuperType(type, MediaType.parse(parent));
        } else if (ACRONYM_TAG.equals(qName)||
                   COMMENT_TAG.equals(qName)||
                   TIKA_LINK_TAG.equals(qName)||
                   TIKA_UTI_TAG.equals(qName)) {
            characters = new StringBuilder();
        } else if (GLOB_TAG.equals(qName)) {
            String pattern = attributes.getValue(PATTERN_ATTR);
            String isRegex = attributes.getValue(ISREGEX_ATTR);
            if (pattern != null) {
                try {
                    types.addPattern(type, pattern, Boolean.valueOf(isRegex));
                } catch (MimeTypeException e) {
                  handleGlobError(type, pattern, e, qName, attributes);
                }
            }
        } else if (ROOT_XML_TAG.equals(qName)) {
            String namespace = attributes.getValue(NS_URI_ATTR);
            String name = attributes.getValue(LOCAL_NAME_ATTR);
            type.addRootXML(namespace, name);
        } else if (MATCH_TAG.equals(qName)) {
            String kind = attributes.getValue(MATCH_TYPE_ATTR);
            String offset = attributes.getValue(MATCH_OFFSET_ATTR);
            String value = attributes.getValue(MATCH_VALUE_ATTR);
            String mask = attributes.getValue(MATCH_MASK_ATTR);
            if (kind == null) {
                kind = "string";
            }
            current = new ClauseRecord(
                    new MagicMatch(type.getType(), kind, offset, value, mask));
        } else if (MAGIC_TAG.equals(qName)) {
            String value = attributes.getValue(MAGIC_PRIORITY_ATTR);
            if (value != null && value.length() > 0) {
                priority = Integer.parseInt(value);
            } else {
                priority = 50;
            }
            current = new ClauseRecord(null);
        }
    }

    @Override
    public void endElement(String uri, String localName, String qName) {
        if (type != null) {
            if (MIME_TYPE_TAG.equals(qName)) {
                type = null;
            } else if (COMMENT_TAG.equals(qName)) {
                type.setDescription(characters.toString().trim());
                characters = null;
            } else if (ACRONYM_TAG.equals(qName)) {
                type.setAcronym(characters.toString().trim());
                characters = null;
            } else if (TIKA_UTI_TAG.equals(qName)) {
                type.setUniformTypeIdentifier(characters.toString().trim());
                characters = null;
            } else if (TIKA_LINK_TAG.equals(qName)) {
                try {
                    type.addLink(new URI(characters.toString().trim()));
                } 
                catch (URISyntaxException e) {
                    throw new IllegalArgumentException("unable to parse link: "+characters, e);
                }
                characters = null;
            } else if (MATCH_TAG.equals(qName)) {
                current.stop();
            } else if (MAGIC_TAG.equals(qName)) {
                for (Clause clause : current.getClauses()) {
                    type.addMagic(new Magic(type, priority, clause));
                }
                current = null;
            }
        }
    }

    @Override
    public void characters(char[] ch, int start, int length) {
        if (characters != null) {
            characters.append(ch, start, length);
        }
    }

    protected void handleMimeError(String input, MimeTypeException ex, String qName, Attributes attributes) throws SAXException {
      throw new SAXException(ex);
    }
    
    protected void handleGlobError(MimeType type, String pattern, MimeTypeException ex, String qName, Attributes attributes) throws SAXException {
      throw new SAXException(ex);
    }

    private ClauseRecord current = new ClauseRecord(null);

    private class ClauseRecord {

        private ClauseRecord parent;

        private Clause clause;

        private List<Clause> subclauses = null;

        public ClauseRecord(Clause clause) {
            this.parent = current;
            this.clause = clause;
        }

        public void stop() {
            if (subclauses != null) {
                Clause subclause;
                if (subclauses.size() == 1) {
                    subclause = subclauses.get(0);
                } else {
                    subclause = new OrClause(subclauses);
                }
                clause = new AndClause(clause, subclause);
            }
            if (parent.subclauses == null) {
                parent.subclauses = Collections.singletonList(clause);
            } else {
                if (parent.subclauses.size() == 1) {
                    parent.subclauses = new ArrayList<Clause>(parent.subclauses);
                }
                parent.subclauses.add(clause);
            }

            current = current.parent;
        }
 
        public List<Clause> getClauses() {
            return subclauses;
        }

    }

}
"
tika-core/src/main/java/org/apache/tika/mime/MimeTypesReaderMetKeys.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.mime;

/**
 * Met Keys used by the {@link MimeTypesReader}.
 */
public interface MimeTypesReaderMetKeys {

    String MIME_INFO_TAG = "mime-info";

    String MIME_TYPE_TAG = "mime-type";

    String MIME_TYPE_TYPE_ATTR = "type";

    String ACRONYM_TAG = "acronym";

    String COMMENT_TAG = "_comment";

    String GLOB_TAG = "glob";

    String ISREGEX_ATTR = "isregex";

    String PATTERN_ATTR = "pattern";

    String MAGIC_TAG = "magic";

    String ALIAS_TAG = "alias";

    String ALIAS_TYPE_ATTR = "type";

    String ROOT_XML_TAG = "root-XML";

    String SUB_CLASS_OF_TAG = "sub-class-of";

    String SUB_CLASS_TYPE_ATTR = "type";

    String MAGIC_PRIORITY_ATTR = "priority";

    String MATCH_TAG = "match";

    String MATCH_OFFSET_ATTR = "offset";

    String MATCH_TYPE_ATTR = "type";

    String MATCH_VALUE_ATTR = "value";

    String MATCH_MASK_ATTR = "mask";

    String NS_URI_ATTR = "namespaceURI";

    String LOCAL_NAME_ATTR = "localName";

    String TIKA_LINK_TAG = "tika:link";
    
    String TIKA_UTI_TAG = "tika:uti";
}
"
tika-core/src/main/java/org/apache/tika/mime/OrClause.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.mime;

import java.util.List;

class OrClause implements Clause {

    private final List<Clause> clauses;

    OrClause(List<Clause> clauses) {
        this.clauses = clauses;
    }

    public boolean eval(byte[] data) {
        for (Clause clause : clauses) {
            if (clause.eval(data)) {
                return true;
            }
        }
        return false;
    }

    public int size() {
        int size = 0;
        for (Clause clause : clauses) {
            size = Math.max(size, clause.size());
        }
        return size;
    }

    public String toString() {
        return "or" + clauses;
    }

}
"
tika-core/src/main/java/org/apache/tika/mime/package-info.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Media type information.
 */
@aQute.bnd.annotation.Version("1.2.0")
package org.apache.tika.mime;
"
tika-core/src/main/java/org/apache/tika/mime/Patterns.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.mime;

import java.io.Serializable;
import java.util.Comparator;
import java.util.HashMap;
import java.util.Map;
import java.util.SortedMap;
import java.util.TreeMap;

/**
 * Defines a MimeType pattern.
 */
class Patterns implements Serializable {

    /**
     * Serial version UID.
     */
    private static final long serialVersionUID = -5778015347278111140L;

    private final MediaTypeRegistry registry;

    /**
     * Index of exact name patterns.
     */
    private final Map<String, MimeType> names = new HashMap<String, MimeType>();

    /**
     * Index of extension patterns of the form "*extension".
     */
    private final Map<String, MimeType> extensions =
        new HashMap<String, MimeType>();

    private int minExtensionLength = Integer.MAX_VALUE;

    private int maxExtensionLength = 0;

    /**
     * Index of generic glob patterns, sorted by length.
     */
    private final SortedMap<String, MimeType> globs =
        new TreeMap<String, MimeType>(new LengthComparator());

    private static final class LengthComparator
            implements Comparator<String>, Serializable {

        /**
         * Serial version UID.
         */
        private static final long serialVersionUID = 8468289702915532359L;

        public int compare(String a, String b) {
            int diff = b.length() - a.length();
            if (diff == 0) {
                diff = a.compareTo(b);
            }
            return diff;
        }

    }

    public Patterns(MediaTypeRegistry registry) {
        this.registry = registry;
    }

    public void add(String pattern, MimeType type) throws MimeTypeException {
        this.add(pattern, false, type);
    }
   
    public void add(String pattern, boolean isJavaRegex, MimeType type)
            throws MimeTypeException {
        if (pattern == null || type == null) {
            throw new IllegalArgumentException(
                    "Pattern and/or mime type is missing");
        }
        
        if (isJavaRegex) {
            // in this case, we don't need to build a regex pattern
            // it's already there for us, so just add the pattern as is
            addGlob(pattern, type);
        } else {

            if (pattern.indexOf('*') == -1 && pattern.indexOf('?') == -1
                    && pattern.indexOf('[') == -1) {
                addName(pattern, type);
            } else if (pattern.startsWith("*") && pattern.indexOf('*', 1) == -1
                    && pattern.indexOf('?') == -1 && pattern.indexOf('[') == -1) {
                String extension = pattern.substring(1);
                addExtension(extension, type);
                type.addExtension(extension);
            } else {
                addGlob(compile(pattern), type);
            }
        }
    }
    
    private void addName(String name, MimeType type) throws MimeTypeException {
        MimeType previous = names.get(name);
        if (previous == null
                || registry.isSpecializationOf(previous.getType(), type.getType())) {
            names.put(name, type);
        } else if (previous == type
                || registry.isSpecializationOf(type.getType(), previous.getType())) {
            // do nothing
        } else {
            throw new MimeTypeException("Conflicting name pattern: " + name);
        }
    }

    private void addExtension(String extension, MimeType type)
            throws MimeTypeException {
        MimeType previous = extensions.get(extension);
        if (previous == null
                || registry.isSpecializationOf(previous.getType(), type.getType())) {
            extensions.put(extension, type);
            int length = extension.length();
            minExtensionLength = Math.min(minExtensionLength, length);
            maxExtensionLength = Math.max(maxExtensionLength, length);
        } else if (previous == type
                || registry.isSpecializationOf(type.getType(), previous.getType())) {
            // do nothing
        } else {
            throw new MimeTypeException(
                    "Conflicting extension pattern: " + extension);
        }
    }

    private void addGlob(String glob, MimeType type)
            throws MimeTypeException {
        MimeType previous = globs.get(glob);
        if (previous == null
                || registry.isSpecializationOf(previous.getType(), type.getType())) {
            globs.put(glob, type);
        } else if (previous == type
                || registry.isSpecializationOf(type.getType(), previous.getType())) {
            // do nothing
        } else {
            throw new MimeTypeException("Conflicting glob pattern: " + glob);
        }
    }

    /**
     * Find the MimeType corresponding to a resource name.
     * 
     * It applies the recommendations detailed in FreeDesktop Shared MIME-info
     * Database for guessing MimeType from a resource name: It first tries a
     * case-sensitive match, then try again with the resource name converted to
     * lower-case if that fails. If several patterns match then the longest
     * pattern is used. In particular, files with multiple extensions (such as
     * Data.tar.gz) match the longest sequence of extensions (eg '*.tar.gz' in
     * preference to '*.gz'). Literal patterns (eg, 'Makefile') are matched
     * before all others. Patterns beginning with `*.' and containing no other
     * special characters (`*?[') are matched before other wildcarded patterns
     * (since this covers the majority of the patterns).
     */
    public MimeType matches(String name) {
        if (name == null) {
            throw new IllegalArgumentException("Name is missing");
        }

        // First, try exact match of the provided resource name
        if (names.containsKey(name)) {
            return names.get(name);
        }

        // Then try "extension" (*.xxx) matching
        int maxLength = Math.min(maxExtensionLength, name.length());
        for (int n = maxLength; n >= minExtensionLength; n--) {
            String extension = name.substring(name.length() - n);
            if (extensions.containsKey(extension)) {
                return extensions.get(extension);
            }
        }

        // And finally, try complex regexp matching
        for (Map.Entry<String, MimeType> entry : globs.entrySet()) {
            if (name.matches(entry.getKey())) {
                return entry.getValue();
            }
        }

        return null;
    }

    private String compile(String glob) {
        StringBuilder pattern = new StringBuilder();
        pattern.append("\\A");
        for (int i = 0; i < glob.length(); i++) {
            char ch = glob.charAt(i);
            if (ch == '?') {
                pattern.append('.');
            } else if (ch == '*') {
                pattern.append(".*");
            } else if ("\\[]^.-$+(){}|".indexOf(ch) != -1) {
                pattern.append('\\');
                pattern.append(ch);
            } else {
                pattern.append(ch);
            }
        }
        pattern.append("\\z");
        return pattern.toString();
    }

}
"
tika-core/src/main/java/org/apache/tika/parser/AbstractParser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser;

import java.io.IOException;
import java.io.InputStream;

import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * Abstract base class for new parsers. This method implements the old
 * deprecated parse method so subclasses won't have to.
 *
 * @since Apache Tika 0.10
 */
public abstract class AbstractParser implements Parser {

    /**
     * Serial version UID.
     */
    private static final long serialVersionUID = 7186985395903074255L;

    /**
     * Calls the
     * {@link Parser#parse(InputStream, ContentHandler, Metadata, ParseContext)}
     * method with an empty {@link ParseContext}. This method exists as a
     * leftover from Tika 0.x when the three-argument parse() method still
     * existed in the {@link Parser} interface. No new code should call this
     * method anymore, it's only here for backwards compatibility.
     *
     * @deprecated use the {@link Parser#parse(InputStream, ContentHandler, Metadata, ParseContext)} method instead
     */
    public void parse(
            InputStream stream, ContentHandler handler, Metadata metadata)
            throws IOException, SAXException, TikaException {
        parse(stream, handler, metadata, new ParseContext());
    }

}
"
tika-core/src/main/java/org/apache/tika/parser/AutoDetectParser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser;

import java.io.IOException;
import java.io.InputStream;

import org.apache.tika.config.TikaConfig;
import org.apache.tika.detect.DefaultDetector;
import org.apache.tika.detect.Detector;
import org.apache.tika.exception.TikaException;
import org.apache.tika.io.TemporaryResources;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.mime.MediaTypeRegistry;
import org.apache.tika.sax.SecureContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

public class AutoDetectParser extends CompositeParser {

    /** Serial version UID */
    private static final long serialVersionUID = 6110455808615143122L;

    /**
     * The type detector used by this parser to auto-detect the type
     * of a document.
     */
    private Detector detector; // always set in the constructor

    /**
     * Creates an auto-detecting parser instance using the default Tika
     * configuration.
     */
    public AutoDetectParser() {
        this(TikaConfig.getDefaultConfig());
    }

    public AutoDetectParser(Detector detector) {
        this(TikaConfig.getDefaultConfig());
        setDetector(detector);
    }

    /**
     * Creates an auto-detecting parser instance using the specified set of parser.
     * This allows one to create a Tika configuration where only a subset of the
     * available parsers have their 3rd party jars included, as otherwise the
     * use of the default TikaConfig will throw various "ClassNotFound" exceptions.
     * 
     * @param parsers
     */
    public AutoDetectParser(Parser...parsers) {
        this(new DefaultDetector(), parsers);
    }

    public AutoDetectParser(Detector detector, Parser...parsers) {
        super(MediaTypeRegistry.getDefaultRegistry(), parsers);
        setDetector(detector);
    }

    public AutoDetectParser(TikaConfig config) {
        super(config.getMediaTypeRegistry(), config.getParser());
        setDetector(config.getDetector());
    }

    /**
     * Returns the type detector used by this parser to auto-detect the type
     * of a document.
     *
     * @return type detector
     * @since Apache Tika 0.4
     */
    public Detector getDetector() {
        return detector;
    }

    /**
     * Sets the type detector used by this parser to auto-detect the type
     * of a document.
     *
     * @param detector type detector
     * @since Apache Tika 0.4
     */
    public void setDetector(Detector detector) {
        this.detector = detector;
    }

    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        TemporaryResources tmp = new TemporaryResources();
        try {
            TikaInputStream tis = TikaInputStream.get(stream, tmp);

            // Automatically detect the MIME type of the document
            MediaType type = detector.detect(tis, metadata);
            metadata.set(Metadata.CONTENT_TYPE, type.toString());

            // TIKA-216: Zip bomb prevention
            SecureContentHandler sch = 
                handler != null ? new SecureContentHandler(handler, tis) : null;
            try {
                // Parse the document
                super.parse(tis, sch, metadata, context);
            } catch (SAXException e) {
                // Convert zip bomb exceptions to TikaExceptions
                sch.throwIfCauseOf(e);
                throw e;
            }
        } finally {
            tmp.dispose();
        }
    }

    public void parse(
            InputStream stream, ContentHandler handler, Metadata metadata)
            throws IOException, SAXException, TikaException {
        ParseContext context = new ParseContext();
        context.set(Parser.class, this);
        parse(stream, handler, metadata, context);
    }

}
"
tika-core/src/main/java/org/apache/tika/parser/CompositeParser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser;

import org.apache.tika.exception.TikaException;
import org.apache.tika.io.TemporaryResources;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.mime.MediaTypeRegistry;
import org.apache.tika.sax.TaggedContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

import java.io.IOException;
import java.io.InputStream;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collections;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Set;

/**
 * Composite parser that delegates parsing tasks to a component parser
 * based on the declared content type of the incoming document. A fallback
 * parser is defined for cases where a parser for the given content type is
 * not available.
 */
public class CompositeParser extends AbstractParser {

    /** Serial version UID */
    private static final long serialVersionUID = 2192845797749627824L;

    /**
     * Media type registry.
     */
    private MediaTypeRegistry registry;

    /**
     * List of component parsers.
     */
    private List<Parser> parsers;

    /**
     * The fallback parser, used when no better parser is available.
     */
    private Parser fallback = new EmptyParser();

    public CompositeParser(MediaTypeRegistry registry, List<Parser> parsers) {
        this.parsers = parsers;
        this.registry = registry;
    }

    public CompositeParser(MediaTypeRegistry registry, Parser... parsers) {
        this(registry, Arrays.asList(parsers));
    }

    public CompositeParser() {
        this(new MediaTypeRegistry());
    }

    public Map<MediaType, Parser> getParsers(ParseContext context) {
        Map<MediaType, Parser> map = new HashMap<MediaType, Parser>();
        for (Parser parser : parsers) {
            for (MediaType type : parser.getSupportedTypes(context)) {
                map.put(registry.normalize(type), parser);
            }
        }
        return map;
    }

    /**
     * Utility method that goes through all the component parsers and finds
     * all media types for which more than one parser declares support. This
     * is useful in tracking down conflicting parser definitions.
     *
     * @since Apache Tika 0.10
     * @see <a href="https://issues.apache.org/jira/browse/TIKA-660">TIKA-660</a>
     * @param context parsing context
     * @return media types that are supported by at least two component parsers
     */
    public Map<MediaType, List<Parser>> findDuplicateParsers(
            ParseContext context) {
        Map<MediaType, Parser> types = new HashMap<MediaType, Parser>();
        Map<MediaType, List<Parser>> duplicates =
            new HashMap<MediaType, List<Parser>>();
        for (Parser parser : parsers) {
            for (MediaType type : parser.getSupportedTypes(context)) {
                MediaType canonicalType = registry.normalize(type);
                if (types.containsKey(canonicalType)) {
                    List<Parser> list = duplicates.get(canonicalType);
                    if (list == null) {
                        list = new ArrayList<Parser>();
                        list.add(types.get(canonicalType));
                        duplicates.put(canonicalType, list);
                    }
                    list.add(parser);
                } else {
                    types.put(canonicalType, parser);
                }
            }
        }
        return duplicates;
    }

    /**
     * Returns the media type registry used to infer type relationships.
     *
     * @since Apache Tika 0.8
     * @return media type registry
     */
    public MediaTypeRegistry getMediaTypeRegistry() {
        return registry;
    }

    /**
     * Sets the media type registry used to infer type relationships.
     *
     * @since Apache Tika 0.8
     * @param registry media type registry
     */
    public void setMediaTypeRegistry(MediaTypeRegistry registry) {
        this.registry = registry;
    }

    /**
     * Returns all parsers registered with the Composite Parser,
     *  including ones which may not currently be active.
     * This won't include the Fallback Parser, if defined
     */
    public List<Parser> getAllComponentParsers() {
        return Collections.unmodifiableList(parsers);
    }
    
    /**
     * Returns the component parsers.
     *
     * @return component parsers, keyed by media type
     */
    public Map<MediaType, Parser> getParsers() {
        return getParsers(new ParseContext());
    }

    /**
     * Sets the component parsers.
     *
     * @param parsers component parsers, keyed by media type
     */
    public void setParsers(Map<MediaType, Parser> parsers) {
        this.parsers = new ArrayList<Parser>(parsers.size());
        for (Map.Entry<MediaType, Parser> entry : parsers.entrySet()) {
            this.parsers.add(ParserDecorator.withTypes(
                    entry.getValue(), Collections.singleton(entry.getKey())));
        }
    }

    /**
     * Returns the fallback parser.
     *
     * @return fallback parser
     */
    public Parser getFallback() {
        return fallback;
    }

    /**
     * Sets the fallback parser.
     *
     * @param fallback fallback parser
     */
    public void setFallback(Parser fallback) {
        this.fallback = fallback;
    }

    /**
     * Returns the parser that best matches the given metadata. By default
     * looks for a parser that matches the content type metadata property,
     * and uses the fallback parser if a better match is not found. The
     * type hierarchy information included in the configured media type
     * registry is used when looking for a matching parser instance.
     * <p>
     * Subclasses can override this method to provide more accurate
     * parser resolution.
     *
     * @param metadata document metadata
     * @return matching parser
     */
    protected Parser getParser(Metadata metadata) {
        return getParser(metadata, new ParseContext());
    }

    protected Parser getParser(Metadata metadata, ParseContext context) {
        Map<MediaType, Parser> map = getParsers(context);
        MediaType type = MediaType.parse(metadata.get(Metadata.CONTENT_TYPE));
        if (type != null) {
           // We always work on the normalised, canonical form
           type = registry.normalize(type);
        }
        while (type != null) {
            // Try finding a parser for the type
            Parser parser = map.get(type);
            if (parser != null) {
                return parser;
            }
            
            // Failing that, try for the parent of the type
            type = registry.getSupertype(type);
        }
        return fallback;
    }

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return getParsers(context).keySet();
    }

    /**
     * Delegates the call to the matching component parser.
     * <p>
     * Potential {@link RuntimeException}s, {@link IOException}s and
     * {@link SAXException}s unrelated to the given input stream and content
     * handler are automatically wrapped into {@link TikaException}s to better
     * honor the {@link Parser} contract.
     */
    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        Parser parser = getParser(metadata, context);
        TemporaryResources tmp = new TemporaryResources();
        try {
            TikaInputStream taggedStream = TikaInputStream.get(stream, tmp);
            TaggedContentHandler taggedHandler = 
                handler != null ? new TaggedContentHandler(handler) : null;
            if (parser instanceof ParserDecorator){
                metadata.add("X-Parsed-By", ((ParserDecorator) parser).getWrappedParser().getClass().getName());
            } else {
                metadata.add("X-Parsed-By", parser.getClass().getName());
            }
            try {
                parser.parse(taggedStream, taggedHandler, metadata, context);
            } catch (RuntimeException e) {
                throw new TikaException(
                        "Unexpected RuntimeException from " + parser, e);
            } catch (IOException e) {
                taggedStream.throwIfCauseOf(e);
                throw new TikaException(
                        "TIKA-198: Illegal IOException from " + parser, e);
            } catch (SAXException e) {
                if (taggedHandler != null) taggedHandler.throwIfCauseOf(e);
                throw new TikaException(
                        "TIKA-237: Illegal SAXException from " + parser, e);
            }
        } finally {
            tmp.dispose();
        }
    }

}
"
tika-core/src/main/java/org/apache/tika/parser/CryptoParser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser;

import java.io.IOException;
import java.io.InputStream;
import java.security.AlgorithmParameters;
import java.security.GeneralSecurityException;
import java.security.Key;
import java.security.Provider;
import java.security.SecureRandom;
import java.util.Set;

import javax.crypto.Cipher;
import javax.crypto.CipherInputStream;

import org.apache.tika.exception.EncryptedDocumentException;
import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * Decrypts the incoming document stream and delegates further parsing to
 * another parser instance. The decryption key and other settings as well
 * as the delegate parser are taken from the parsing context.
 *
 * @since Apache Tika 0.10
 */
public abstract class CryptoParser extends DelegatingParser {

    /** Serial version UID */
    private static final long serialVersionUID = -3507995752666557731L;

    private final String transformation;

    private final Provider provider;

    private final Set<MediaType> types;

    public CryptoParser(
            String transformation, Provider provider, Set<MediaType> types) {
        this.transformation = transformation;
        this.provider = provider;
        this.types = types;
    }

    public CryptoParser(
            String transformation, Set<MediaType> types) {
        this(transformation, null, types);
    }

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return types;
    }

    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        try {
            Cipher cipher;
            if (provider != null) {
                cipher = Cipher.getInstance(transformation, provider);
            } else {
                cipher = Cipher.getInstance(transformation);
            }

            Key key = context.get(Key.class);
            if (key == null) {
                throw new EncryptedDocumentException("No decryption key provided");
            }

            AlgorithmParameters params = context.get(AlgorithmParameters.class);
            SecureRandom random = context.get(SecureRandom.class);
            if (params != null && random != null) {
                cipher.init(Cipher.DECRYPT_MODE, key, params, random);
            } else if (params != null) {
                cipher.init(Cipher.DECRYPT_MODE, key, params);
            } else if (random != null) {
                cipher.init(Cipher.DECRYPT_MODE, key, random);
            } else {
                cipher.init(Cipher.DECRYPT_MODE, key);
            }

            super.parse(
                    new CipherInputStream(stream, cipher),
                    handler, metadata, context);
        } catch (GeneralSecurityException e) {
            throw new TikaException("Unable to decrypt document stream", e);
        }
    }

}
"
tika-core/src/main/java/org/apache/tika/parser/DefaultParser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser;

import java.util.Collections;
import java.util.Comparator;
import java.util.List;
import java.util.Map;

import org.apache.tika.config.ServiceLoader;
import org.apache.tika.mime.MediaType;
import org.apache.tika.mime.MediaTypeRegistry;

/**
 * A composite parser based on all the {@link Parser} implementations
 * available through the
 * {@link javax.imageio.spi.ServiceRegistry service provider mechanism}.
 *
 * @since Apache Tika 0.8
 */
public class DefaultParser extends CompositeParser {

    /** Serial version UID */
    private static final long serialVersionUID = 3612324825403757520L;

    /**
     * Finds all statically loadable parsers and sort the list by name,
     * rather than discovery order. CompositeParser takes the last
     * parser for any given media type, so put the Tika parsers first
     * so that non-Tika (user supplied) parsers can take precedence.
     *
     * @param loader service loader
     * @return ordered list of statically loadable parsers
     */
    private static List<Parser> getDefaultParsers(ServiceLoader loader) {
        List<Parser> parsers =
                loader.loadStaticServiceProviders(Parser.class);
        Collections.sort(parsers, new Comparator<Parser>() {
            public int compare(Parser p1, Parser p2) {
                String n1 = p1.getClass().getName();
                String n2 = p2.getClass().getName();
                boolean t1 = n1.startsWith("org.apache.tika.");
                boolean t2 = n2.startsWith("org.apache.tika.");
                if (t1 == t2) {
                    return n1.compareTo(n2);
                } else if (t1) {
                    return -1;
                } else {
                    return 1;
                }
            }
        });
        return parsers;
    }

    private transient final ServiceLoader loader;

    public DefaultParser(MediaTypeRegistry registry, ServiceLoader loader) {
        super(registry, getDefaultParsers(loader));
        this.loader = loader;
    }

    public DefaultParser(MediaTypeRegistry registry, ClassLoader loader) {
        this(registry, new ServiceLoader(loader));
    }

    public DefaultParser(ClassLoader loader) {
        this(MediaTypeRegistry.getDefaultRegistry(), new ServiceLoader(loader));
    }

    public DefaultParser(MediaTypeRegistry registry) {
        this(registry, new ServiceLoader());
    }

    public DefaultParser() {
        this(MediaTypeRegistry.getDefaultRegistry());
    }

    @Override
    public Map<MediaType, Parser> getParsers(ParseContext context) {
        Map<MediaType, Parser> map = super.getParsers(context);

        if (loader != null) {
            // Add dynamic parser service (they always override static ones)
            MediaTypeRegistry registry = getMediaTypeRegistry();
            List<Parser> parsers =
                    loader.loadDynamicServiceProviders(Parser.class);
            Collections.reverse(parsers); // best parser last
            for (Parser parser : parsers) {
                for (MediaType type : parser.getSupportedTypes(context)) {
                    map.put(registry.normalize(type), parser);
                }
            }
        }

        return map;
    }

}
"
tika-core/src/main/java/org/apache/tika/parser/DelegatingParser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser;

import java.io.IOException;
import java.io.InputStream;
import java.util.Set;

import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * Base class for parser implementations that want to delegate parts of the
 * task of parsing an input document to another parser. The delegate parser
 * is looked up from the parsing context using the {@link Parser} class as
 * the key.
 *
 * @since Apache Tika 0.4, major changes in Tika 0.5
 */
public class DelegatingParser extends AbstractParser {

    /**
     * Returns the parser instance to which parsing tasks should be delegated.
     * The default implementation looks up the delegate parser from the given
     * parse context, and uses an {@link EmptyParser} instance as a fallback.
     * Subclasses can override this method to implement alternative delegation
     * strategies.
     *
     * @since Apache Tika 0.7
     * @param context parse context
     * @return delegate parser
     */
    protected Parser getDelegateParser(ParseContext context) {
        return context.get(Parser.class, EmptyParser.INSTANCE);
    }

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return getDelegateParser(context).getSupportedTypes(context);
    }

    /**
     * Looks up the delegate parser from the parsing context and
     * delegates the parse operation to it. If a delegate parser is not
     * found, then an empty XHTML document is returned.
     * <p>
     * Subclasses should override this method to parse the top level
     * structure of the given document stream. Parsed sub-streams can
     * be passed to this base class method to be parsed by the configured
     * delegate parser.
     */
    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws SAXException, IOException, TikaException {
        getDelegateParser(context).parse(stream, handler, metadata, context);
    }

}
"
tika-core/src/main/java/org/apache/tika/parser/EmptyParser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser;

import java.io.InputStream;
import java.util.Collections;
import java.util.Set;

import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * Dummy parser that always produces an empty XHTML document without even
 * attempting to parse the given document stream. Useful as a sentinel parser
 * for unknown document types.
 */
public class EmptyParser extends AbstractParser {
    /**
     * Serial version UID.
     */
    private static final long serialVersionUID = -4218649699095732123L;

    /**
     * Singleton instance of this class.
     */
    public static final EmptyParser INSTANCE = new EmptyParser();

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return Collections.emptySet();
    }

    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws SAXException {
        XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);
        xhtml.startDocument();
        xhtml.endDocument();
    }
}
"
tika-core/src/main/java/org/apache/tika/parser/ErrorParser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser;

import java.io.InputStream;
import java.util.Collections;
import java.util.Set;

import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.xml.sax.ContentHandler;

/**
 * Dummy parser that always throws a {@link TikaException} without even
 * attempting to parse the given document stream. Useful as a sentinel parser
 * for unknown document types.
 */
public class ErrorParser extends AbstractParser {
    private static final long serialVersionUID = 7727423956957641824L;
    
    /**
     * Singleton instance of this class.
     */
    public static final ErrorParser INSTANCE = new ErrorParser();

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return Collections.emptySet();
    }

    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws TikaException {
        throw new TikaException("Parse error");
    }
}
"
tika-core/src/main/java/org/apache/tika/parser/NetworkParser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser;

import java.io.FilterOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.net.Socket;
import java.net.URI;
import java.net.URL;
import java.net.URLConnection;
import java.util.Collections;
import java.util.Set;

import org.apache.tika.exception.TikaException;
import org.apache.tika.io.CloseShieldInputStream;
import org.apache.tika.io.IOUtils;
import org.apache.tika.io.TemporaryResources;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.sax.TaggedContentHandler;
import org.apache.tika.sax.TeeContentHandler;
import org.xml.sax.Attributes;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.DefaultHandler;

public class NetworkParser extends AbstractParser {

    private final URI uri;

    private final Set<MediaType> supportedTypes;

    public NetworkParser(URI uri, Set<MediaType> supportedTypes) {
        this.uri = uri;
        this.supportedTypes = supportedTypes;
    }

    public NetworkParser(URI uri) {
        this(uri, Collections.singleton(MediaType.OCTET_STREAM));
    }

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return supportedTypes;
    }

    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        TemporaryResources tmp = new TemporaryResources();
        try {
            TikaInputStream tis = TikaInputStream.get(stream, tmp);
            parse(tis, handler, metadata, context);
        } finally {
            tmp.dispose();
        }
    }

    private void parse(
            TikaInputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        if ("telnet".equals(uri.getScheme())) {
            final Socket socket = new Socket(uri.getHost(), uri.getPort());
            try {
                new ParsingTask(stream, new FilterOutputStream(socket.getOutputStream()) {
                    @Override
                    public void close() throws IOException {
                        socket.shutdownOutput();
                    }
                }).parse(
                        socket.getInputStream(), handler, metadata, context);
            } finally {
                socket.close();
            }
        } else {
            URL url = uri.toURL();
            URLConnection connection = url.openConnection();
            connection.setDoOutput(true);
            connection.connect();
            InputStream input = connection.getInputStream();
            try {
                new ParsingTask(stream, connection.getOutputStream()).parse(
                        new CloseShieldInputStream(input),
                        handler, metadata, context);
            } finally {
                input.close();
            }
        }

    }

    private static class ParsingTask implements Runnable {

        private final TikaInputStream input;

        private final OutputStream output;

        private volatile Exception exception = null;

        public ParsingTask(TikaInputStream input, OutputStream output) {
            this.input = input;
            this.output = output;
        }

        public void parse(
                InputStream stream, ContentHandler handler,
                Metadata metadata, ParseContext context)
                throws IOException, SAXException, TikaException {
            Thread thread = new Thread(this, "Tika network parser");
            thread.start();

            TaggedContentHandler tagged = new TaggedContentHandler(handler);
            try {
                context.getSAXParser().parse(
                        stream, new TeeContentHandler(
                                tagged, new MetaHandler(metadata)));
            } catch (SAXException e) {
                tagged.throwIfCauseOf(e);
                throw new TikaException(
                        "Invalid network parser output", e);
            } catch (IOException e) {
                throw new TikaException(
                        "Unable to read network parser output", e);
            } finally {
                try {
                    thread.join(1000);
                } catch (InterruptedException e) {
                    throw new TikaException("Network parser interrupted", e);
                }

                if (exception != null) {
                    input.throwIfCauseOf(exception);
                    throw new TikaException(
                            "Unexpected network parser error", exception);
                }
            }
        }

        //----------------------------------------------------------<Runnable>

        public void run() {
            try {
                try {
                    IOUtils.copy(input, output);
                } finally {
                    output.close();
                }
            } catch (Exception e) {
                exception = e;
            }
        }

    }

    private static class MetaHandler extends DefaultHandler {

        private final Metadata metadata;

        public MetaHandler(Metadata metadata) {
            this.metadata = metadata;
        }

        @Override
        public void startElement(
                String uri, String localName, String qName,
                Attributes attributes) throws SAXException {
            if ("http://www.w3.org/1999/xhtml".equals(uri)
                    && "meta".equals(localName)) {
                String name = attributes.getValue("", "name");
                String content = attributes.getValue("", "content");
                if (name != null && content != null) {
                    metadata.add(name, content);
                }
            }
        }

    }

}
"
tika-core/src/main/java/org/apache/tika/parser/package-info.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Tika parsers.
 */
@aQute.bnd.annotation.Version("1.0.0")
package org.apache.tika.parser;
"
tika-core/src/main/java/org/apache/tika/parser/ParseContext.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser;

import java.io.Serializable;
import java.util.HashMap;
import java.util.Map;

import javax.xml.XMLConstants;
import javax.xml.parsers.ParserConfigurationException;
import javax.xml.parsers.SAXParser;
import javax.xml.parsers.SAXParserFactory;

import org.apache.tika.exception.TikaException;
import org.xml.sax.SAXException;
import org.xml.sax.SAXNotRecognizedException;
import org.xml.sax.SAXNotSupportedException;

/**
 * Parse context. Used to pass context information to Tika parsers.
 *
 * @since Apache Tika 0.5
 * @see <a href="https://issues.apache.org/jira/browse/TIKA-275">TIKA-275</a>
 */
public class ParseContext implements Serializable {

    /** Serial version UID. */
    private static final long serialVersionUID = -5921436862145826534L;

    /** Map of objects in this context */
    private final Map<String, Object> context = new HashMap<String, Object>();
 
    /**
     * Adds the given value to the context as an implementation of the given
     * interface.
     *
     * @param key the interface implemented by the given value
     * @param value the value to be added, or <code>null</code> to remove
     */
    public <T> void set(Class<T> key, T value) {
        if (value != null) {
            context.put(key.getName(), value);
        } else {
            context.remove(key.getName());
        }
    }

    /**
     * Returns the object in this context that implements the given interface.
     *
     * @param key the interface implemented by the requested object
     * @return the object that implements the given interface,
     *         or <code>null</code> if not found
     */
    @SuppressWarnings("unchecked")
    public <T> T get(Class<T> key) {
        return (T) context.get(key.getName());
    }

    /**
     * Returns the object in this context that implements the given interface,
     * or the given default value if such an object is not found.
     *
     * @param key the interface implemented by the requested object
     * @param defaultValue value to return if the requested object is not found
     * @return the object that implements the given interface,
     *         or the given default value if not found
     */
    public <T> T get(Class<T> key, T defaultValue) {
        T value = get(key);
        if (value != null) {
            return value;
        } else {
            return defaultValue;
        }
    }

    /**
     * Returns the SAX parser specified in this parsing context. If a parser
     * is not explicitly specified, then one is created using the specified
     * or the default SAX parser factory.
     *
     * @see #getSAXParserFactory()
     * @since Apache Tika 0.8
     * @return SAX parser
     * @throws TikaException if a SAX parser could not be created
     */
    public SAXParser getSAXParser() throws TikaException {
        SAXParser parser = get(SAXParser.class);
        if (parser != null) {
            return parser;
        } else {
            try {
                return getSAXParserFactory().newSAXParser();
            } catch (ParserConfigurationException e) {
                throw new TikaException("Unable to configure a SAX parser", e);
            } catch (SAXException e) {
                throw new TikaException("Unable to create a SAX parser", e);
            }
        }
    }

    /**
     * Returns the SAX parser factory specified in this parsing context.
     * If a factory is not explicitly specified, then a default factory
     * instance is created and returned. The default factory instance is
     * configured to be namespace-aware and to use
     * {@link XMLConstants#FEATURE_SECURE_PROCESSING secure XML processing}.
     *
     * @since Apache Tika 0.8
     * @return SAX parser factory
     */
    public SAXParserFactory getSAXParserFactory() {
        SAXParserFactory factory = get(SAXParserFactory.class);
        if (factory == null) {
            factory = SAXParserFactory.newInstance();
            factory.setNamespaceAware(true);
            try {
                factory.setFeature(
                        XMLConstants.FEATURE_SECURE_PROCESSING, true);
            } catch (ParserConfigurationException e) {
            } catch (SAXNotSupportedException e) {
            } catch (SAXNotRecognizedException e) {
                // TIKA-271: Some XML parsers do not support the
                // secure-processing feature, even though it's required by
                // JAXP in Java 5. Ignoring the exception is fine here, as
                // deployments without this feature are inherently vulnerable
                // to XML denial-of-service attacks.
            }
        }
        return factory;
    }

}
"
tika-core/src/main/java/org/apache/tika/parser/Parser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser;

import java.io.IOException;
import java.io.InputStream;
import java.io.Serializable;
import java.util.Set;

import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * Tika parser interface.
 */
public interface Parser extends Serializable {

    /**
     * Returns the set of media types supported by this parser when used
     * with the given parse context.
     *
     * @since Apache Tika 0.7
     * @param context parse context
     * @return immutable set of media types
     */
    Set<MediaType> getSupportedTypes(ParseContext context);

    /**
     * Parses a document stream into a sequence of XHTML SAX events.
     * Fills in related document metadata in the given metadata object.
     * <p>
     * The given document stream is consumed but not closed by this method.
     * The responsibility to close the stream remains on the caller.
     * <p>
     * Information about the parsing context can be passed in the context
     * parameter. See the parser implementations for the kinds of context
     * information they expect.
     *
     * @since Apache Tika 0.5
     * @param stream the document stream (input)
     * @param handler handler for the XHTML SAX events (output)
     * @param metadata document metadata (input and output)
     * @param context parse context
     * @throws IOException if the document stream could not be read
     * @throws SAXException if the SAX events could not be processed
     * @throws TikaException if the document could not be parsed
     */
    void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException;

}
"
tika-core/src/main/java/org/apache/tika/parser/ParserDecorator.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser;

import java.io.IOException;
import java.io.InputStream;
import java.util.HashSet;
import java.util.Set;

import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * Decorator base class for the {@link Parser} interface. This class
 * simply delegates all parsing calls to an underlying decorated parser
 * instance. Subclasses can provide extra decoration by overriding the
 * parse method.
 */
public class ParserDecorator extends AbstractParser {

    /** Serial version UID */
    private static final long serialVersionUID = -3861669115439125268L;

    /**
     * Decorates the given parser so that it always claims to support
     * parsing of the given media types.
     *
     * @param parser the parser to be decorated
     * @param types supported media types
     * @return the decorated parser
     */
    public static final Parser withTypes(
            Parser parser, final Set<MediaType> types) {
        return new ParserDecorator(parser) {
            private static final long serialVersionUID = -7345051519565330731L;
            @Override
            public Set<MediaType> getSupportedTypes(ParseContext context) {
                return types;
            }
        };
    }

    /**
     * Decorates the given parser so that it never claims to support
     * parsing of the given media types, but will work for all others.
     *
     * @param parser the parser to be decorated
     * @param types excluded/ignored media types
     * @return the decorated parser
     */
    public static final Parser withoutTypes(
            Parser parser, final Set<MediaType> excludeTypes) {
        return new ParserDecorator(parser) {
            private static final long serialVersionUID = 7979614774021768609L;
            @Override
            public Set<MediaType> getSupportedTypes(ParseContext context) {
                // Get our own, writable copy of the types the parser supports
                Set<MediaType> parserTypes = 
                        new HashSet<MediaType>(super.getSupportedTypes(context));
                // Remove anything on our excludes list
                parserTypes.removeAll(excludeTypes);
                // Return whatever is left
                return parserTypes;
            }
        };
    }

    /**
     * The decorated parser instance.
     */
    private final Parser parser;

    /**
     * Creates a decorator for the given parser.
     *
     * @param parser the parser instance to be decorated
     */
    public ParserDecorator(Parser parser) {
        this.parser = parser;
    }

    /**
     * Delegates the method call to the decorated parser. Subclasses should
     * override this method (and use <code>super.getSupportedTypes()</code>
     * to invoke the decorated parser) to implement extra decoration.
     */
    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return parser.getSupportedTypes(context);
    }

    /**
     * Delegates the method call to the decorated parser. Subclasses should
     * override this method (and use <code>super.parse()</code> to invoke
     * the decorated parser) to implement extra decoration.
     */
    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        parser.parse(stream, handler, metadata, context);
    }


    /**
     * Gets the parser wrapped by this ParserDecorator
     * @return
     */
    public Parser getWrappedParser() {
        return this.parser;
    }

}
"
tika-core/src/main/java/org/apache/tika/parser/ParserPostProcessor.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser;

import java.io.IOException;
import java.io.InputStream;

import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.sax.BodyContentHandler;
import org.apache.tika.sax.TeeContentHandler;
import org.apache.tika.utils.RegexUtils;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * Parser decorator that post-processes the results from a decorated parser.
 * The post-processing takes care of filling in the "fulltext", "summary",
 * and "outlinks" metadata entries based on the full text content returned by
 * the decorated parser.
 */
public class ParserPostProcessor extends ParserDecorator {

    /**
     * Creates a post-processing decorator for the given parser.
     *
     * @param parser the parser to be decorated
     */
    public ParserPostProcessor(Parser parser) {
        super(parser);
    }

    /**
     * Forwards the call to the delegated parser and post-processes the
     * results as described above.
     */
    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        ContentHandler body = new BodyContentHandler();
        ContentHandler tee = new TeeContentHandler(handler, body);
        super.parse(stream, tee, metadata, context);

        String content = body.toString();
        metadata.set("fulltext", content);

        int length = Math.min(content.length(), 500);
        metadata.set("summary", content.substring(0, length));

        for (String link : RegexUtils.extractLinks(content)) {
            metadata.add("outlinks", link);
        }
    }

}
"
tika-core/src/main/java/org/apache/tika/parser/ParsingReader.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.io.InputStream;
import java.io.PipedReader;
import java.io.PipedWriter;
import java.io.Reader;
import java.io.Writer;
import java.util.concurrent.Executor;

import org.apache.tika.metadata.Metadata;
import org.apache.tika.sax.BodyContentHandler;
import org.xml.sax.ContentHandler;

/**
 * Reader for the text content from a given binary stream. This class
 * uses a background parsing task with a {@link Parser}
 * ({@link AutoDetectParser} by default) to parse the text content from
 * a given input stream. The {@link BodyContentHandler} class and a pipe
 * is used to convert the push-based SAX event stream to the pull-based
 * character stream defined by the {@link Reader} interface.
 *
 * @since Apache Tika 0.2
 */
public class ParsingReader extends Reader {

    /**
     * Parser instance used for parsing the given binary stream.
     */
    private final Parser parser;

    /**
     * Buffered read end of the pipe.
     */
    private final Reader reader;

    /**
     * Write end of the pipe.
     */
    private final Writer writer;

    /**
     * The binary stream being parsed.
     */
    private final InputStream stream;

    /**
     * Metadata associated with the document being parsed.
     */
    private final Metadata metadata;

    /**
     * The parse context.
     */
    private final ParseContext context;

    /**
     * An exception (if any) thrown by the parsing thread.
     */
    private transient Throwable throwable;

    /**
     * Utility method that returns a {@link Metadata} instance
     * for a document with the given name.
     *
     * @param name resource name (or <code>null</code>)
     * @return metadata instance
     */
    private static Metadata getMetadata(String name) {
        Metadata metadata = new Metadata();
        if (name != null && name.length() > 0) {
            metadata.set(Metadata.RESOURCE_NAME_KEY, name);
        }
        return metadata;
    }

    /**
     * Creates a reader for the text content of the given binary stream.
     *
     * @param stream binary stream
     * @throws IOException if the document can not be parsed
     */
    public ParsingReader(InputStream stream) throws IOException {
        this(new AutoDetectParser(), stream, new Metadata(), new ParseContext());
        context.set(Parser.class, parser);
    }

    /**
     * Creates a reader for the text content of the given binary stream
     * with the given name.
     *
     * @param stream binary stream
     * @param name document name
     * @throws IOException if the document can not be parsed
     */
    public ParsingReader(InputStream stream, String name) throws IOException {
        this(new AutoDetectParser(), stream, getMetadata(name), new ParseContext());
        context.set(Parser.class, parser);
    }

    /**
     * Creates a reader for the text content of the given file.
     *
     * @param file file
     * @throws FileNotFoundException if the given file does not exist
     * @throws IOException if the document can not be parsed
     */
    public ParsingReader(File file) throws FileNotFoundException, IOException {
        this(new FileInputStream(file), file.getName());
    }

    /**
     * Creates a reader for the text content of the given binary stream
     * with the given document metadata. The given parser is used for
     * parsing. A new background thread is started for the parsing task.
     * <p>
     * The created reader will be responsible for closing the given stream.
     * The stream and any associated resources will be closed at or before
     * the time when the {@link #close()} method is called on this reader.
     *
     * @param parser parser instance
     * @param stream binary stream
     * @param metadata document metadata
     * @throws IOException if the document can not be parsed
     */
    public ParsingReader(
            Parser parser, InputStream stream, final Metadata metadata,
            ParseContext context) throws IOException {
        this(parser, stream, metadata, context, new Executor() {
            public void execute(Runnable command) {
                String name = metadata.get(Metadata.RESOURCE_NAME_KEY);
                if (name != null) {
                    name = "Apache Tika: " + name;
                } else {
                    name = "Apache Tika";
                }
                Thread thread = new Thread(command, name);
                thread.setDaemon(true);
                thread.start();
            }
        });
    }

    /**
     * Creates a reader for the text content of the given binary stream
     * with the given document metadata. The given parser is used for the
     * parsing task that is run with the given executor. The given executor
     * <em>must</em> run the parsing task asynchronously in a separate thread,
     * since the current thread must return to the caller that can then
     * consume the parsed text through the {@link Reader} interface.
     * <p>
     * The created reader will be responsible for closing the given stream.
     * The stream and any associated resources will be closed at or before
     * the time when the {@link #close()} method is called on this reader.
     *
     * @param parser parser instance
     * @param stream binary stream
     * @param metadata document metadata
     * @param context parsing context
     * @param executor executor for the parsing task
     * @throws IOException if the document can not be parsed
     * @since Apache Tika 0.4
     */
    public ParsingReader(
            Parser parser, InputStream stream, Metadata metadata,
            ParseContext context, Executor executor) throws IOException {
        this.parser = parser;
        PipedReader pipedReader = new PipedReader();
        this.reader = new BufferedReader(pipedReader);
        try {
            this.writer = new PipedWriter(pipedReader);
        } catch (IOException e) {
            throw new IllegalStateException(e); // Should never happen
        }
        this.stream = stream;
        this.metadata = metadata;
        this.context = context;

        executor.execute(new ParsingTask());

        // TIKA-203: Buffer first character to force metadata extraction
        reader.mark(1);
        reader.read();
        reader.reset();
    }

    /**
     * The background parsing task.
     */
    private class ParsingTask implements Runnable {

        /**
         * Parses the given binary stream and writes the text content
         * to the write end of the pipe. Potential exceptions (including
         * the one caused if the read end is closed unexpectedly) are
         * stored before the input stream is closed and processing is stopped.
         */
        public void run() {
            try {
                ContentHandler handler = new BodyContentHandler(writer);
                parser.parse(stream, handler, metadata, context);
            } catch (Throwable t) {
                throwable = t;
            }

            try {
                stream.close();
            } catch (Throwable t) {
                if (throwable == null) {
                    throwable = t;
                }
            }

            try {
                writer.close();
            } catch (Throwable t) {
                if (throwable == null) {
                    throwable = t;
                }
            }
        }

    }

    /**
     * Reads parsed text from the pipe connected to the parsing thread.
     * Fails if the parsing thread has thrown an exception.
     *
     * @param cbuf character buffer
     * @param off start offset within the buffer
     * @param len maximum number of characters to read
     * @throws IOException if the parsing thread has failed or
     *                     if for some reason the pipe does not work properly
     */
    @Override
    public int read(char[] cbuf, int off, int len) throws IOException {
        if (throwable instanceof IOException) {
            throw (IOException) throwable;
        } else if (throwable != null) {
            IOException exception = new IOException("");
            exception.initCause(throwable);
            throw exception;
        }
        return reader.read(cbuf, off, len);
    }

    /**
     * Closes the read end of the pipe. If the parsing thread is still
     * running, next write to the pipe will fail and cause the thread
     * to stop. Thus there is no need to explicitly terminate the thread.
     *
     * @throws IOException if the pipe can not be closed
     */
    @Override
    public void close() throws IOException {
        reader.close();
    }

}
"
tika-core/src/main/java/org/apache/tika/parser/PasswordProvider.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser;

import org.apache.tika.metadata.Metadata;

/**
 * Interface for providing a password to a Parser for handling Encrypted
 *  and Password Protected Documents.
 * An implementation of this should be set on the {@link ParseContext}
 *  supplied to {@link Parser#parse(java.io.InputStream, org.xml.sax.ContentHandler, Metadata, ParseContext)}
 *  to provide a way to get the document password. 
 * An implementation of this interface defines some specific selection
 *  or lookup criteria, to be applied against the document metadata passed
 *  to the {@link #getPassword(Metadata)} method.
 *
 * @since Apache Tika 1.1
 */
public interface PasswordProvider {
    /**
     * Looks up the password for a document with the given metadata,
     * and returns it for the Parser. If no password is available
     * for the document, will return null.
     *
     * @param metadata document metadata
     * @return The document decryption password, or <code>null</code> if not known
     */
    String getPassword(Metadata metadata);
}
"
tika-core/src/main/java/org/apache/tika/parser/RecursiveParserWrapper.java,true,"package org.apache.tika.parser;

/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.IOException;
import java.io.InputStream;
import java.util.Date;
import java.util.LinkedList;
import java.util.List;
import java.util.Set;

import org.apache.tika.exception.TikaException;
import org.apache.tika.io.FilenameUtils;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.Property;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.metadata.TikaMetadataKeys;
import org.apache.tika.mime.MediaType;
import org.apache.tika.sax.ContentHandlerFactory;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.DefaultHandler;

/**
 * This is a helper class that wraps a parser in a recursive handler.
 * It takes care of setting the embedded parser in the ParseContext 
 * and handling the embedded path calculations.
 * <p>
 * After parsing a document, call getMetadata() to retrieve a list of 
 * Metadata objects, one for each embedded resource.  The first item
 * in the list will contain the Metadata for the outer container file.
 * <p>
 * Content can also be extracted and stored in the {@link #TIKA_CONTENT} field
 * of a Metadata object.  Select the type of content to be stored
 * at initialization.
 * <p>
 * If a WriteLimitReachedException is encountered, the wrapper will stop
 * processing the current resource, and it will not process
 * any of the child resources for the given resource.  However, it will try to 
 * parse as much as it can.  If a WLRE is reached in the parent document, 
 * no child resources will be parsed.
 * <p>
 * The implementation is based on Jukka's RecursiveMetadataParser
 * and Nick's additions. See: 
 * <a href="http://wiki.apache.org/tika/RecursiveMetadata#Jukka.27s_RecursiveMetadata_Parser">RecursiveMetadataParser</a>.
 * <p>
 * Note that this wrapper holds all data in memory and is not appropriate
 * for files with content too large to be held in memory.
 * <p>
 * Note, too, that this wrapper is not thread safe because it stores state.  
 * The client must initialize a new wrapper for each thread, and the client
 * is responsible for calling {@link #reset()} after each parse.
 * <p>
 * The unit tests for this class are in the tika-parsers module.
 * </p>
 */
public class RecursiveParserWrapper implements Parser {
    
    /**
     * Generated serial version
     */
    private static final long serialVersionUID = 9086536568120690938L;

    //move this to TikaCoreProperties?
    public final static Property TIKA_CONTENT = Property.internalText(TikaCoreProperties.TIKA_META_PREFIX+"content");
    public final static Property PARSE_TIME_MILLIS = Property.internalText(TikaCoreProperties.TIKA_META_PREFIX+"parse_time_millis");
    public final static Property WRITE_LIMIT_REACHED =
                Property.internalBoolean(TikaCoreProperties.TIKA_META_EXCEPTION_PREFIX+"write_limit_reached");
    public final static Property EMBEDDED_RESOURCE_LIMIT_REACHED = 
                Property.internalBoolean(TikaCoreProperties.TIKA_META_EXCEPTION_PREFIX+"embedded_resource_limit_reached");

    //move this to TikaCoreProperties?
    public final static Property EMBEDDED_RESOURCE_PATH = 
                Property.internalText(TikaCoreProperties.TIKA_META_PREFIX+"embedded_resource_path");
 
    private final Parser wrappedParser;
    private final ContentHandlerFactory contentHandlerFactory;
    private final List<Metadata> metadatas = new LinkedList<Metadata>();

    //used in naming embedded resources that don't have a name.
    private int unknownCount = 0;   
    private int maxEmbeddedResources = -1;
    private boolean hitMaxEmbeddedResources = false;
    
    public RecursiveParserWrapper(Parser wrappedParser, ContentHandlerFactory contentHandlerFactory) {
        this.wrappedParser = wrappedParser;
        this.contentHandlerFactory = contentHandlerFactory;
    }
    
    @Override
    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return wrappedParser.getSupportedTypes(context);
    }

    /**
     * Acts like a regular parser except it ignores the ContentHandler
     * and it automatically sets/overwrites the embedded Parser in the 
     * ParseContext object.
     * <p>
     * To retrieve the results of the parse, use {@link #getMetadata()}.
     * <p>
     * Make sure to call {@link #reset()} after each parse.
     */
    @Override
    public void parse(InputStream stream, ContentHandler ignore,
            Metadata metadata, ParseContext context) throws IOException,
            SAXException, TikaException {

        String name = getResourceName(metadata);
        EmbeddedParserDecorator decorator = new EmbeddedParserDecorator(name);
        context.set(Parser.class, decorator);
        ContentHandler localHandler = contentHandlerFactory.getNewContentHandler();
        long started = new Date().getTime();
        try {
            wrappedParser.parse(stream, localHandler, metadata, context);
        } catch (SAXException e) {
            boolean wlr = isWriteLimitReached(e);
            if (wlr == false) {
                throw e;
            }
            metadata.set(WRITE_LIMIT_REACHED, "true");
        }
        long elapsedMillis = new Date().getTime()-started;
        metadata.set(PARSE_TIME_MILLIS, Long.toString(elapsedMillis));
        addContent(localHandler, metadata);
        
        if (hitMaxEmbeddedResources) {
            metadata.set(EMBEDDED_RESOURCE_LIMIT_REACHED, "true");
        }
        metadatas.add(0, deepCopy(metadata));
    }

    /**
     * 
     * The first element in the returned list represents the 
     * data from the outer container file.  There is no guarantee
     * about the ordering of the list after that.
     * 
     * @return list of Metadata objects that were gathered during the parse
     */
    public List<Metadata> getMetadata() {
        return metadatas;
    }
    
    /**
     * Set the maximum number of embedded resources to store.
     * If the max is hit during parsing, the {@link #EMBEDDED_RESOURCE_LIMIT_REACHED}
     * property will be added to the container document's Metadata.
     * 
     * <p>
     * If this value is < 0 (the default), the wrapper will store all Metadata.
     * 
     * @param max maximum number of embedded resources to store
     */
    public void setMaxEmbeddedResources(int max) {
        maxEmbeddedResources = max;
    }
    

    /**
     * This clears the metadata list and resets {@link #unknownCount} and
     * {@link #hitMaxEmbeddedResources}
     */
    public void reset() {
        metadatas.clear();
        unknownCount = 0;
        hitMaxEmbeddedResources = false;
    }
    
    /**
     * Copied/modified from WriteOutContentHandler.  Couldn't make that 
     * static, and we need to have something that will work 
     * with exceptions thrown from both BodyContentHandler and WriteOutContentHandler
     * @param t
     * @return
     */
    private boolean isWriteLimitReached(Throwable t) {
        if (t.getMessage().indexOf("Your document contained more than") == 0) {
            return true;
        } else {
            return t.getCause() != null && isWriteLimitReached(t.getCause());
        }
    }
    
    //defensive copy
    private Metadata deepCopy(Metadata m) {
        Metadata clone = new Metadata();
        
        for (String n : m.names()){
            if (! m.isMultiValued(n)) {
                clone.set(n, m.get(n));
            } else {
                String[] vals = m.getValues(n);
                for (int i = 0; i < vals.length; i++) {
                    clone.add(n, vals[i]);
                }
            }
        }
        return clone;
    }
    
    private String getResourceName(Metadata metadata) {
        String objectName = "";
        if (metadata.get(TikaMetadataKeys.RESOURCE_NAME_KEY) != null) {
            objectName = metadata.get(TikaMetadataKeys.RESOURCE_NAME_KEY);
         } else if (metadata.get(TikaMetadataKeys.EMBEDDED_RELATIONSHIP_ID) != null) {
            objectName = metadata.get(TikaMetadataKeys.EMBEDDED_RELATIONSHIP_ID);
         } else {
            objectName = "embedded-" + (++unknownCount);
         }
         //make sure that there isn't any path info in the objectName
         //some parsers can return paths, not just file names
         objectName = FilenameUtils.getName(objectName);
         return objectName;
    }
    
    private void addContent(ContentHandler handler, Metadata metadata) {
        
        if (handler.getClass().equals(DefaultHandler.class)){
            //no-op: we can't rely on just testing for 
            //empty content because DefaultHandler's toString()
            //returns e.g. "org.xml.sax.helpers.DefaultHandler@6c8b1edd"
        } else {
            String content = handler.toString();
            if (content != null && content.trim().length() > 0 ) {
                metadata.add(TIKA_CONTENT, content);
            }
        }

    }
    
    /**
     * Override for different behavior.
     * 
     * @return handler to be used for each document
     */

    
    private class EmbeddedParserDecorator extends ParserDecorator {
        
        private static final long serialVersionUID = 207648200464263337L;
        
        private String location = null;

        
        private EmbeddedParserDecorator(String location) {
            super(wrappedParser);
            this.location = location;
            if (! this.location.endsWith("/")) {
               this.location += "/";
            }
        }

        @Override
        public void parse(InputStream stream, ContentHandler ignore,
                Metadata metadata, ParseContext context) throws IOException,
                SAXException, TikaException {
            //Test to see if we should avoid parsing
            if (maxEmbeddedResources > -1 && 
                    metadatas.size() >= maxEmbeddedResources) {
                hitMaxEmbeddedResources = true;
                return;
            }
            // Work out what this thing is
            String objectName = getResourceName(metadata);
            String objectLocation = this.location + objectName;
      
            metadata.add(EMBEDDED_RESOURCE_PATH, objectLocation);
            
            //ignore the content handler that is passed in
            //and get a fresh handler
            ContentHandler localHandler = contentHandlerFactory.getNewContentHandler();
            
            Parser preContextParser = context.get(Parser.class);
            context.set(Parser.class, new EmbeddedParserDecorator(objectLocation));

            try {
                super.parse(stream, localHandler, metadata, context);
            } catch (SAXException e) {
                boolean wlr = isWriteLimitReached(e);
                if (wlr == true) {
                    metadata.add(WRITE_LIMIT_REACHED, "true");
                } else {
                    throw e;
                }
            } finally {
                context.set(Parser.class, preContextParser);
            }
            
            //Because of recursion, we need
            //to re-test to make sure that we limit the 
            //number of stored resources
            if (maxEmbeddedResources > -1 && 
                    metadatas.size() >= maxEmbeddedResources) {
                hitMaxEmbeddedResources = true;
                return;
            }
            addContent(localHandler, metadata);
            metadatas.add(deepCopy(metadata));
        }        
    }


}
"
tika-core/src/main/java/org/apache/tika/parser/external/CompositeExternalParser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.external;

import java.io.IOException;
import java.util.List;

import org.apache.tika.exception.TikaException;
import org.apache.tika.mime.MediaTypeRegistry;
import org.apache.tika.parser.CompositeParser;
import org.apache.tika.parser.Parser;

/**
 * A Composite Parser that wraps up all the available External Parsers,
 *  and provides an easy way to access them.
 * Parser that uses an external program (like catdoc or pdf2txt) to extract
 *  text content and metadata from a given document.
 */
public class CompositeExternalParser extends CompositeParser {
   private static final long serialVersionUID = 6962436916649024024L;

   public CompositeExternalParser() throws IOException, TikaException {
      this(new MediaTypeRegistry());
   }
   
   @SuppressWarnings("unchecked")
   public CompositeExternalParser(MediaTypeRegistry registry)  throws IOException, TikaException {
      super(
            registry, 
            (List<Parser>)(List<? extends Parser>)ExternalParsersFactory.create()
      );
   }
}
"
tika-core/src/main/java/org/apache/tika/parser/external/ExternalParser.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.external;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.io.OutputStream;
import java.io.Reader;
import java.io.UnsupportedEncodingException;
import java.util.Collections;
import java.util.HashSet;
import java.util.Map;
import java.util.Set;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import org.apache.tika.exception.TikaException;
import org.apache.tika.io.IOUtils;
import org.apache.tika.io.NullOutputStream;
import org.apache.tika.io.TemporaryResources;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * Parser that uses an external program (like catdoc or pdf2txt) to extract
 *  text content and metadata from a given document.
 */
public class ExternalParser extends AbstractParser {
    private static final long serialVersionUID = -1079128990650687037L;
    
    /**
     * The token, which if present in the Command string, will
     *  be replaced with the input filename. 
     * Alternately, the input data can be streamed over STDIN.
     */
    public static final String INPUT_FILE_TOKEN = "${INPUT}";
    /**
     * The token, which if present in the Command string, will
     *  be replaced with the output filename. 
     * Alternately, the output data can be collected on STDOUT.
     */
    public static final String OUTPUT_FILE_TOKEN = "${OUTPUT}";

    /**
     * Media types supported by the external program.
     */
    private Set<MediaType> supportedTypes = Collections.emptySet();
    
    /**
     * Regular Expressions to run over STDOUT to
     *  extract Metadata.
     */
    private Map<Pattern,String> metadataPatterns = null;

    /**
     * The external command to invoke.
     * @see Runtime#exec(String[])
     */
    private String[] command = new String[] { "cat" };

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return getSupportedTypes();
    }

    public Set<MediaType> getSupportedTypes() {
        return supportedTypes;
    }

    public void setSupportedTypes(Set<MediaType> supportedTypes) {
        this.supportedTypes =
            Collections.unmodifiableSet(new HashSet<MediaType>(supportedTypes));
    }


    public String[] getCommand() {
        return command;
    }

    /**
     * Sets the command to be run. This can include either of
     *  {@link #INPUT_FILE_TOKEN} or {@link #OUTPUT_FILE_TOKEN}
     *  if the command needs filenames.
     * @see Runtime#exec(String[])
     */
    public void setCommand(String... command) {
        this.command = command;
    }
    
    
    public Map<Pattern,String> getMetadataExtractionPatterns() {
       return metadataPatterns;
    }
    
    /**
     * Sets the map of regular expression patterns and Metadata
     *  keys. Any matching patterns will have the matching
     *  metadata entries set.
     * Set this to null to disable Metadata extraction.
     */
    public void setMetadataExtractionPatterns(Map<Pattern,String> patterns) {
       this.metadataPatterns = patterns;
    }
    

    /**
     * Executes the configured external command and passes the given document
     *  stream as a simple XHTML document to the given SAX content handler.
     * Metadata is only extracted if {@link #setMetadataExtractionPatterns(Map)}
     *  has been called to set patterns.
     */
    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        XHTMLContentHandler xhtml =
            new XHTMLContentHandler(handler, metadata);

        TemporaryResources tmp = new TemporaryResources();
        try {
            parse(TikaInputStream.get(stream, tmp),
                    xhtml, metadata, tmp);
        } finally {
            tmp.dispose();
        }
    }

    private void parse(
            TikaInputStream stream, XHTMLContentHandler xhtml,
            Metadata metadata, TemporaryResources tmp)
            throws IOException, SAXException, TikaException {
        boolean inputToStdIn = true;
        boolean outputFromStdOut = true;
        boolean hasPatterns = (metadataPatterns != null && !metadataPatterns.isEmpty());

        File output = null;

        // Build our command
        String[] cmd = new String[command.length];
        System.arraycopy(command, 0, cmd, 0, command.length);
        for(int i=0; i<cmd.length; i++) {
           if(cmd[i].indexOf(INPUT_FILE_TOKEN) != -1) {
              cmd[i] = cmd[i].replace(INPUT_FILE_TOKEN, stream.getFile().getPath());
              inputToStdIn = false;
           }
           if(cmd[i].indexOf(OUTPUT_FILE_TOKEN) != -1) {
              output = tmp.createTemporaryFile();
              outputFromStdOut = false;
           }
        }

        // Execute
        Process process;
        if(cmd.length == 1) {
           process = Runtime.getRuntime().exec( cmd[0] );
        } else {
           process = Runtime.getRuntime().exec( cmd );
        }

        try {
            if(inputToStdIn) {
               sendInput(process, stream);
            } else {
               process.getOutputStream().close();
            }

            InputStream out = process.getInputStream();
            InputStream err = process.getErrorStream();
            
            if(hasPatterns) {
               extractMetadata(err, metadata);
               
               if(outputFromStdOut) {
                  extractOutput(out, xhtml);
               } else {
                  extractMetadata(out, metadata);
               }
            } else {
               ignoreStream(err);
               
               if(outputFromStdOut) {
                  extractOutput(out, xhtml);
               } else {
                  ignoreStream(out);
               }
            }
        } finally {
            try {
                process.waitFor();
            } catch (InterruptedException ignore) {
            }
        }

        // Grab the output if we haven't already
        if (!outputFromStdOut) {
            extractOutput(new FileInputStream(output), xhtml);
        }
    }

    /**
     * Starts a thread that extracts the contents of the standard output
     * stream of the given process to the given XHTML content handler.
     * The standard output stream is closed once fully processed.
     *
     * @param process process
     * @param xhtml XHTML content handler
     * @throws SAXException if the XHTML SAX events could not be handled
     * @throws IOException if an input error occurred
     */
    private void extractOutput(InputStream stream, XHTMLContentHandler xhtml)
            throws SAXException, IOException {
        Reader reader = new InputStreamReader(stream, "UTF-8");
        try {
            xhtml.startDocument();
            xhtml.startElement("p");
            char[] buffer = new char[1024];
            for (int n = reader.read(buffer); n != -1; n = reader.read(buffer)) {
                xhtml.characters(buffer, 0, n);
            }
            xhtml.endElement("p");
            xhtml.endDocument();
        } finally {
            reader.close();
        }
    }

    /**
     * Starts a thread that sends the contents of the given input stream
     * to the standard input stream of the given process. Potential
     * exceptions are ignored, and the standard input stream is closed
     * once fully processed. Note that the given input stream is <em>not</em>
     * closed by this method.
     *
     * @param process process
     * @param stream input stream
     */
    private void sendInput(final Process process, final InputStream stream) {
        new Thread() {
            public void run() {
                OutputStream stdin = process.getOutputStream();
                try {
                    IOUtils.copy(stream, stdin);
                } catch (IOException e) {
                }
            }
        }.start();
    }

    /**
     * Starts a thread that reads and discards the contents of the
     * standard stream of the given process. Potential exceptions
     * are ignored, and the stream is closed once fully processed.
     *
     * @param process process
     */
    private void ignoreStream(final InputStream stream) {
        new Thread() {
            public void run() {
                try {
                    IOUtils.copy(stream, new NullOutputStream());
                } catch (IOException e) {
                } finally {
                    IOUtils.closeQuietly(stream);
                }
            }
        }.start();
    }
    
    private void extractMetadata(final InputStream stream, final Metadata metadata) {
       new Thread() {
          public void run() {
             BufferedReader reader;
             try {
                 reader = new BufferedReader(new InputStreamReader(stream, "UTF-8"));
             } catch (UnsupportedEncodingException e) {
                 throw new AssertionError("UTF-8 not supported.");
             }
             try {
                String line;
                while ( (line = reader.readLine()) != null ) {
                   for(Pattern p : metadataPatterns.keySet()) {
                      Matcher m = p.matcher(line);
                      if(m.find()) {
                    	 if (metadataPatterns.get(p) != null && 
                    			 !metadataPatterns.get(p).equals("")){
                                   metadata.add( metadataPatterns.get(p), m.group(1) );
                    	 }
                    	 else{
                    		 metadata.add( m.group(1), m.group(2));
                    	 }
                      }
                   }
                }
             } catch (IOException e) {
                 // Ignore
             } finally {
                IOUtils.closeQuietly(reader);
                IOUtils.closeQuietly(stream);
            }
          }
       }.start();
    }
    
    /**
     * Checks to see if the command can be run. Typically used with
     *  something like "myapp --version" to check to see if "myapp"
     *  is installed and on the path.
     *  
     * @param checkCmd The check command to run
     * @param errorValue What is considered an error value? 
     */
    public static boolean check(String checkCmd, int... errorValue) {
       return check(new String[] {checkCmd}, errorValue);
    }
    public static boolean check(String[] checkCmd, int... errorValue) {
       if(errorValue.length == 0) {
          errorValue = new int[] { 127 };
       }
       
       try {
          Process process= Runtime.getRuntime().exec(checkCmd);
          int result = process.waitFor();
          
          for(int err : errorValue) {
             if(result == err) return false;
          }
          return true;
       } catch(IOException e) {
          // Some problem, command is there or is broken
          return false;
       } catch (InterruptedException ie) {
          // Some problem, command is there or is broken
          return false;
      }
    }
}
"
tika-core/src/main/java/org/apache/tika/parser/external/ExternalParsersConfigReader.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.external;

import java.io.IOException;
import java.io.InputStream;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.StringTokenizer;
import java.util.regex.Pattern;

import javax.xml.parsers.DocumentBuilder;
import javax.xml.parsers.DocumentBuilderFactory;
import javax.xml.parsers.ParserConfigurationException;

import org.apache.tika.exception.TikaException;
import org.apache.tika.mime.MediaType;
import org.apache.tika.mime.MimeTypeException;
import org.w3c.dom.Document;
import org.w3c.dom.Element;
import org.w3c.dom.Node;
import org.w3c.dom.NodeList;
import org.xml.sax.InputSource;
import org.xml.sax.SAXException;

/**
 * Builds up ExternalParser instances based on XML file(s)
 *  which define what to run, for what, and how to process
 *  any output metadata.
 * Typically used to configure up a series of external programs 
 *  (like catdoc or pdf2txt) to extract text content from documents.
 *  
 * <pre>
 *  TODO XML DTD Here
 * </pre>
 */
public final class ExternalParsersConfigReader implements ExternalParsersConfigReaderMetKeys {
   
   public static List<ExternalParser> read(InputStream stream) throws TikaException, IOException {
      try {
          DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();
          DocumentBuilder builder = factory.newDocumentBuilder();
          Document document = builder.parse(new InputSource(stream));
          return read(document);
      } catch (ParserConfigurationException e) {
          throw new TikaException("Unable to create an XML parser", e);
      } catch (SAXException e) {
          throw new TikaException("Invalid parser configuration", e);
      }
   }
   
   public static List<ExternalParser> read(Document document) throws TikaException, IOException {
      return read(document.getDocumentElement());
   }
   
   public static List<ExternalParser> read(Element element) throws TikaException, IOException {
      List<ExternalParser> parsers = new ArrayList<ExternalParser>();
      
      if (element != null && element.getTagName().equals(EXTERNAL_PARSERS_TAG)) {
         NodeList nodes = element.getChildNodes();
         for (int i = 0; i < nodes.getLength(); i++) {
            Node node = nodes.item(i);
            if (node.getNodeType() == Node.ELEMENT_NODE) {
               Element child = (Element) node;
               if (child.getTagName().equals(PARSER_TAG)) {
                  ExternalParser p = readParser(child);
                  if(p != null) {
                     parsers.add( p );
                  }
               }
            }
         }
      } else {
         throw new MimeTypeException(
               "Not a <" + EXTERNAL_PARSERS_TAG + "/> configuration document: "
               + element.getTagName());
      }
      
      return parsers;
   }
   
   /**
    * Builds and Returns an ExternalParser, or null if a check
    *  command was given that didn't match.
    */
   private static ExternalParser readParser(Element parserDef) throws TikaException {
      ExternalParser parser = new ExternalParser();

      NodeList children = parserDef.getChildNodes();
      for(int i=0; i<children.getLength(); i++) {
         Node node = children.item(i);
         if (node.getNodeType() == Node.ELEMENT_NODE) {
            Element child = (Element) node;
            if (child.getTagName().equals(CHECK_TAG)) {
               boolean present = readCheckTagAndCheck(child);
               if(! present) {
                  return null;
               }
            }
            else if (child.getTagName().equals(COMMAND_TAG)) {
               parser.setCommand( getString(child) );
            }
            else if (child.getTagName().equals(MIMETYPES_TAG)) {
               parser.setSupportedTypes(
                     readMimeTypes(child)
               );
            }
            else if (child.getTagName().equals(METADATA_TAG)) {
               parser.setMetadataExtractionPatterns(
                     readMetadataPatterns(child)
               );
            }
         }
      }
      
      return parser;
   }
   
   private static Set<MediaType> readMimeTypes(Element mimeTypes) {
      Set<MediaType> types = new HashSet<MediaType>();
      
      NodeList children = mimeTypes.getChildNodes();
      for(int i=0; i<children.getLength(); i++) {
         Node node = children.item(i);
         if (node.getNodeType() == Node.ELEMENT_NODE) {
            Element child = (Element) node;
            if (child.getTagName().equals(MIMETYPE_TAG)) {
               types.add( MediaType.parse( getString(child) ) );
            }
         }
      }
      
      return types;
   }
   
   private static Map<Pattern,String> readMetadataPatterns(Element metadataDef) {
      Map<Pattern, String> metadata = new HashMap<Pattern, String>();
      
      NodeList children = metadataDef.getChildNodes();
      for(int i=0; i<children.getLength(); i++) {
         Node node = children.item(i);
         if (node.getNodeType() == Node.ELEMENT_NODE) {
            Element child = (Element) node;
            if (child.getTagName().equals(METADATA_MATCH_TAG)) {
               String metadataKey = child.getAttribute(METADATA_KEY_ATTR);
               Pattern pattern = Pattern.compile( getString(child) );
               metadata.put(pattern, metadataKey);
            }
         }
      }
      
      return metadata;
   }
   
   private static boolean readCheckTagAndCheck(Element checkDef) {
      String command = null;
      List<Integer> errorVals = new ArrayList<Integer>(); 
      
      NodeList children = checkDef.getChildNodes();
      for(int i=0; i<children.getLength(); i++) {
         Node node = children.item(i);
         if (node.getNodeType() == Node.ELEMENT_NODE) {
            Element child = (Element) node;
            if (child.getTagName().equals(COMMAND_TAG)) {
               command = getString(child);
            }
            if (child.getTagName().equals(ERROR_CODES_TAG)) {
               String errs = getString(child);
               StringTokenizer st = new StringTokenizer(errs);
               while(st.hasMoreElements()) {
                  try {
                     String s = st.nextToken();
                     errorVals.add(Integer.parseInt(s));
                  } catch(NumberFormatException e) {}
               }
            }
         }
      }
      
      if(command != null) {
         int[] errVals = new int[errorVals.size()];
         for(int i=0; i<errVals.length; i++) {
            errVals[i] = errorVals.get(i);
         }
         
         return ExternalParser.check(command, errVals);
      }
      
      // No check command, so assume it's there
      return true;
   }
   
   private static String getString(Element element) {
      StringBuffer s = new StringBuffer();
      
      NodeList children = element.getChildNodes();
      for(int i=0; i<children.getLength(); i++) {
         Node node = children.item(i);
         if (node.getNodeType() == Node.TEXT_NODE) {
            s.append( node.getNodeValue() );
         }
      }
      
      return s.toString();
   }
}
"
tika-core/src/main/java/org/apache/tika/parser/external/ExternalParsersConfigReaderMetKeys.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.external;

/**
 * Met Keys used by the {@link ExternalParsersConfigReader}.
 */
public interface ExternalParsersConfigReaderMetKeys {

    String EXTERNAL_PARSERS_TAG = "external-parsers";

    String PARSER_TAG = "parser";

    String COMMAND_TAG = "command";
    
    String CHECK_TAG = "check";
    
    String ERROR_CODES_TAG = "error-codes";
    
    String MIMETYPES_TAG = "mime-types";
    
    String MIMETYPE_TAG = "mime-type";
    
    String METADATA_TAG = "metadata";
    
    String METADATA_MATCH_TAG = "match";
    
    String METADATA_KEY_ATTR = "key";
}
"
tika-core/src/main/java/org/apache/tika/parser/external/ExternalParsersFactory.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.external;

import java.io.IOException;
import java.io.InputStream;
import java.net.URL;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Enumeration;
import java.util.List;
import java.util.Map;

import org.apache.tika.config.ServiceLoader;
import org.apache.tika.config.TikaConfig;
import org.apache.tika.exception.TikaException;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.CompositeParser;
import org.apache.tika.parser.Parser;

/**
 * Creates instances of ExternalParser based on XML 
 *  configuration files.
 *  
 * @see ExternalParsersConfigReader
 */
public class ExternalParsersFactory {
   
   public static List<ExternalParser> create() throws IOException, TikaException {
      return create(new ServiceLoader());
   }
   
   public static List<ExternalParser> create(ServiceLoader loader) 
           throws IOException, TikaException {
      return create("tika-external-parsers.xml", loader);
   }
   
   public static List<ExternalParser> create(String filename, ServiceLoader loader) 
           throws IOException, TikaException {
      String filepath = ExternalParsersFactory.class.getPackage().getName().replace('.', '/') +
                     "/" + filename;
      Enumeration<URL> files = loader.findServiceResources(filepath);
      ArrayList<URL> list = Collections.list(files);
      URL[] urls = list.toArray(new URL[list.size()]);
      return create(urls);
   }
   
   public static List<ExternalParser> create(URL... urls) throws IOException, TikaException {
      List<ExternalParser> parsers = new ArrayList<ExternalParser>();
      for(URL url : urls) {
         InputStream stream = url.openStream();
         try {
            parsers.addAll(
                  ExternalParsersConfigReader.read(stream)
            );
         } finally {
            stream.close();
         }
      }
      return parsers;
   }
   
   public static void attachExternalParsers(TikaConfig config) throws IOException, TikaException {
      attachExternalParsers( create(), config );
   }
   
   public static void attachExternalParsers(List<ExternalParser> parsers, TikaConfig config) {
      Parser parser = config.getParser();
      if (parser instanceof CompositeParser) {
         CompositeParser cParser = (CompositeParser)parser;
         Map<MediaType,Parser> parserMap = cParser.getParsers();
      }
      // TODO
   }
}
"
tika-core/src/main/java/org/apache/tika/parser/external/package-info.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * External parser process.
 */
@aQute.bnd.annotation.Version("1.0.0")
package org.apache.tika.parser.external;
"
tika-core/src/main/java/org/apache/tika/sax/BasicContentHandlerFactory.java,false,"package org.apache.tika.sax;
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.OutputStream;
import java.io.OutputStreamWriter;
import java.io.UnsupportedEncodingException;

import org.xml.sax.ContentHandler;
import org.xml.sax.helpers.DefaultHandler;

/**
 * Basic factory for creating common types of ContentHandlers
 */
public class BasicContentHandlerFactory implements ContentHandlerFactory {

    /**
     * Common handler types for content.
     */
    public enum HANDLER_TYPE {
        BODY,
        IGNORE, //don't store content
        TEXT,
        HTML,
        XML
    };

    private final HANDLER_TYPE type;
    private final int writeLimit;

    /**
     *
     * @param type basic type of handler
     * @param writeLimit max number of characters to store; if < 0, the handler will store all characters
     */
    public BasicContentHandlerFactory(HANDLER_TYPE type, int writeLimit) {
        this.type = type;
        this.writeLimit = writeLimit;
    }

    @Override
    public ContentHandler getNewContentHandler() {

        if (type == HANDLER_TYPE.BODY) {
            return new BodyContentHandler(writeLimit);
        } else if (type == HANDLER_TYPE.IGNORE) {
            return new DefaultHandler();
        }
        if (writeLimit > -1) {
            switch(type) {
                case TEXT:
                    return new WriteOutContentHandler(new ToTextContentHandler(), writeLimit);
                case HTML:
                    return new WriteOutContentHandler(new ToHTMLContentHandler(), writeLimit);
                case XML:
                    return new WriteOutContentHandler(new ToXMLContentHandler(), writeLimit);
                default:
                    return new WriteOutContentHandler(new ToTextContentHandler(), writeLimit);
            }
        } else {
            switch (type) {
                case TEXT:
                    return new ToTextContentHandler();
                case HTML:
                    return new ToHTMLContentHandler();
                case XML:
                    return new ToXMLContentHandler();
                default:
                    return new ToTextContentHandler();

            }
        }
    }

    @Override
    public ContentHandler getNewContentHandler(OutputStream os, String encoding) throws UnsupportedEncodingException {

        if (type == HANDLER_TYPE.IGNORE) {
            return new DefaultHandler();
        }

        if (writeLimit > -1) {
            switch(type) {
                case BODY:
                    return new WriteOutContentHandler(
                            new BodyContentHandler(
                                    new OutputStreamWriter(os, encoding)), writeLimit);
                case TEXT:
                    return new WriteOutContentHandler(new ToTextContentHandler(os, encoding), writeLimit);
                case HTML:
                    return new WriteOutContentHandler(new ToHTMLContentHandler(os, encoding), writeLimit);
                case XML:
                    return new WriteOutContentHandler(new ToXMLContentHandler(os, encoding), writeLimit);
                default:
                    return new WriteOutContentHandler(new ToTextContentHandler(os, encoding), writeLimit);
            }
        } else {
            switch (type) {
                case BODY:
                    return new BodyContentHandler(new OutputStreamWriter(os, encoding));
                case TEXT:
                    return new ToTextContentHandler(os, encoding);
                case HTML:
                    return new ToHTMLContentHandler(os, encoding);
                case XML:
                    return new ToXMLContentHandler(os, encoding);
                default:
                    return new ToTextContentHandler(os, encoding);

            }
        }
    }

}
"
tika-core/src/main/java/org/apache/tika/sax/BodyContentHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax;

import java.io.OutputStream;
import java.io.Writer;

import org.apache.tika.sax.xpath.Matcher;
import org.apache.tika.sax.xpath.MatchingContentHandler;
import org.apache.tika.sax.xpath.XPathParser;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * Content handler decorator that only passes everything inside
 * the XHTML &lt;body/&gt; tag to the underlying handler. Note that
 * the &lt;body/&gt; tag itself is <em>not</em> passed on.
 */
public class BodyContentHandler extends ContentHandlerDecorator {

    /**
     * XHTML XPath parser.
     */
    private static final XPathParser PARSER =
        new XPathParser("xhtml", XHTMLContentHandler.XHTML);

    /**
     * The XPath matcher used to select the XHTML body contents.
     */
    private static final Matcher MATCHER =
        PARSER.parse("/xhtml:html/xhtml:body/descendant::node()");

    /**
     * Creates a content handler that passes all XHTML body events to the
     * given underlying content handler.
     *
     * @param handler content handler
     */
    public BodyContentHandler(ContentHandler handler) {
        super(new MatchingContentHandler(handler, MATCHER));
    }

    /**
     * Creates a content handler that writes XHTML body character events to
     * the given writer.
     *
     * @param writer writer
     */
    public BodyContentHandler(Writer writer) {
        this(new WriteOutContentHandler(writer));
    }

    /**
     * Creates a content handler that writes XHTML body character events to
     * the given output stream using the default encoding.
     *
     * @param stream output stream
     */
    public BodyContentHandler(OutputStream stream) {
        this(new WriteOutContentHandler(stream));
    }

    /**
     * Creates a content handler that writes XHTML body character events to
     * an internal string buffer. The contents of the buffer can be retrieved
     * using the {@link #toString()} method.
     * <p>
     * The internal string buffer is bounded at the given number of characters.
     * If this write limit is reached, then a {@link SAXException} is thrown.
     *
     * @since Apache Tika 0.7
     * @param writeLimit maximum number of characters to include in the string,
     *                   or -1 to disable the write limit
     */
    public BodyContentHandler(int writeLimit) {
        this(new WriteOutContentHandler(writeLimit));
    }

    /**
     * Creates a content handler that writes XHTML body character events to
     * an internal string buffer. The contents of the buffer can be retrieved
     * using the {@link #toString()} method.
     * <p>
     * The internal string buffer is bounded at 100k characters. If this write
     * limit is reached, then a {@link SAXException} is thrown.
     */
    public BodyContentHandler() {
        this(new WriteOutContentHandler());
    }

}
"
tika-core/src/main/java/org/apache/tika/sax/CleanPhoneText.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.sax;

import java.util.Locale;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
import java.util.ArrayList;

/**
 * Class to help de-obfuscate phone numbers in text.
 */
public class CleanPhoneText {
    // Regex to identify a phone number
    static final String cleanPhoneRegex = "([2-9]\\d{2}[2-9]\\d{6})";

    // Regex which attempts to ignore punctuation and other distractions.
    static final String phoneRegex = "([{(<]{0,3}[2-9][\\W_]{0,3}\\d[\\W_]{0,3}\\d[\\W_]{0,6}[2-9][\\W_]{0,3}\\d[\\W_]{0,3}\\d[\\W_]{0,6}\\d[\\W_]{0,3}\\d[\\W_]{0,3}\\d[\\W_]{0,3}\\d)";

    public static ArrayList<String> extractPhoneNumbers(String text) {
        text = clean(text);
        int idx = 0;
        Pattern p = Pattern.compile(cleanPhoneRegex);
        Matcher m = p.matcher(text);
        ArrayList<String> phoneNumbers = new ArrayList<String>();
        while (m.find(idx)) {
            String digits = m.group(1);
            int start = m.start(1);
            int end = m.end(1);
            String prefix = "";
            if (start > 0) {
                prefix = text.substring(start-1, start);
            }
            if (digits.substring(0, 2).equals("82") && prefix.equals("*")) {
                // this number overlaps with a *82 sequence
                idx += 2;
            } else {
                // seems good
                phoneNumbers.add(digits);
                idx = end;
            }
        }
        return phoneNumbers;
    }

    public static String clean(String text) {
        text = text.toLowerCase(Locale.ROOT);
        for (String[][] group : cleanSubstitutions) {
            for (String[] sub : group) {
                text = text.replaceAll(sub[0], sub[1]);
            }
        }
        // Delete all non-digits and white space.
        text = text.replaceAll("[\\D+\\s]", "");
        return text;
    }


    public static final String[][][] cleanSubstitutions = new String[][][]{
            {{"&#\\d{1,3};", ""}},         // first simply remove numeric entities
            {{"th0usand", "thousand"},    // handle common misspellings
                    {"th1rteen", "thirteen"},
                    {"f0urteen", "fourteen"},
                    {"e1ghteen", "eighteen"},
                    {"n1neteen", "nineteen"},
                    {"f1fteen", "fifteen"},
                    {"s1xteen", "sixteen"},
                    {"th1rty", "thirty"},
                    {"e1ghty", "eighty"},
                    {"n1nety", "ninety"},
                    {"fourty", "forty"},
                    {"f0urty", "forty"},
                    {"e1ght", "eight"},
                    {"f0rty", "forty"},
                    {"f1fty", "fifty"},
                    {"s1xty", "sixty"},
                    {"zer0", "zero"},
                    {"f0ur", "four"},
                    {"f1ve", "five"},
                    {"n1ne", "nine"},
                    {"0ne", "one"},
                    {"tw0", "two"},
                    {"s1x", "six"}},
            // mixed compound numeral words
            // consider 7teen, etc.
            {{"twenty[\\W_]{0,3}1", "twenty-one"},
                    {"twenty[\\W_]{0,3}2", "twenty-two"},
                    {"twenty[\\W_]{0,3}3", "twenty-three"},
                    {"twenty[\\W_]{0,3}4", "twenty-four"},
                    {"twenty[\\W_]{0,3}5", "twenty-five"},
                    {"twenty[\\W_]{0,3}6", "twenty-six"},
                    {"twenty[\\W_]{0,3}7", "twenty-seven"},
                    {"twenty[\\W_]{0,3}8", "twenty-eight"},
                    {"twenty[\\W_]{0,3}9", "twenty-nine"},
                    {"thirty[\\W_]{0,3}1", "thirty-one"},
                    {"thirty[\\W_]{0,3}2", "thirty-two"},
                    {"thirty[\\W_]{0,3}3", "thirty-three"},
                    {"thirty[\\W_]{0,3}4", "thirty-four"},
                    {"thirty[\\W_]{0,3}5", "thirty-five"},
                    {"thirty[\\W_]{0,3}6", "thirty-six"},
                    {"thirty[\\W_]{0,3}7", "thirty-seven"},
                    {"thirty[\\W_]{0,3}8", "thirty-eight"},
                    {"thirty[\\W_]{0,3}9", "thirty-nine"},
                    {"forty[\\W_]{0,3}1", "forty-one"},
                    {"forty[\\W_]{0,3}2", "forty-two"},
                    {"forty[\\W_]{0,3}3", "forty-three"},
                    {"forty[\\W_]{0,3}4", "forty-four"},
                    {"forty[\\W_]{0,3}5", "forty-five"},
                    {"forty[\\W_]{0,3}6", "forty-six"},
                    {"forty[\\W_]{0,3}7", "forty-seven"},
                    {"forty[\\W_]{0,3}8", "forty-eight"},
                    {"forty[\\W_]{0,3}9", "forty-nine"},
                    {"fifty[\\W_]{0,3}1", "fifty-one"},
                    {"fifty[\\W_]{0,3}2", "fifty-two"},
                    {"fifty[\\W_]{0,3}3", "fifty-three"},
                    {"fifty[\\W_]{0,3}4", "fifty-four"},
                    {"fifty[\\W_]{0,3}5", "fifty-five"},
                    {"fifty[\\W_]{0,3}6", "fifty-six"},
                    {"fifty[\\W_]{0,3}7", "fifty-seven"},
                    {"fifty[\\W_]{0,3}8", "fifty-eight"},
                    {"fifty[\\W_]{0,3}9", "fifty-nine"},
                    {"sixty[\\W_]{0,3}1", "sixty-one"},
                    {"sixty[\\W_]{0,3}2", "sixty-two"},
                    {"sixty[\\W_]{0,3}3", "sixty-three"},
                    {"sixty[\\W_]{0,3}4", "sixty-four"},
                    {"sixty[\\W_]{0,3}5", "sixty-five"},
                    {"sixty[\\W_]{0,3}6", "sixty-six"},
                    {"sixty[\\W_]{0,3}7", "sixty-seven"},
                    {"sixty[\\W_]{0,3}8", "sixty-eight"},
                    {"sixty[\\W_]{0,3}9", "sixty-nine"},
                    {"seventy[\\W_]{0,3}1", "seventy-one"},
                    {"seventy[\\W_]{0,3}2", "seventy-two"},
                    {"seventy[\\W_]{0,3}3", "seventy-three"},
                    {"seventy[\\W_]{0,3}4", "seventy-four"},
                    {"seventy[\\W_]{0,3}5", "seventy-five"},
                    {"seventy[\\W_]{0,3}6", "seventy-six"},
                    {"seventy[\\W_]{0,3}7", "seventy-seven"},
                    {"seventy[\\W_]{0,3}8", "seventy-eight"},
                    {"seventy[\\W_]{0,3}9", "seventy-nine"},
                    {"eighty[\\W_]{0,3}1", "eighty-one"},
                    {"eighty[\\W_]{0,3}2", "eighty-two"},
                    {"eighty[\\W_]{0,3}3", "eighty-three"},
                    {"eighty[\\W_]{0,3}4", "eighty-four"},
                    {"eighty[\\W_]{0,3}5", "eighty-five"},
                    {"eighty[\\W_]{0,3}6", "eighty-six"},
                    {"eighty[\\W_]{0,3}7", "eighty-seven"},
                    {"eighty[\\W_]{0,3}8", "eighty-eight"},
                    {"eighty[\\W_]{0,3}9", "eighty-nine"},
                    {"ninety[\\W_]{0,3}1", "ninety-one"},
                    {"ninety[\\W_]{0,3}2", "ninety-two"},
                    {"ninety[\\W_]{0,3}3", "ninety-three"},
                    {"ninety[\\W_]{0,3}4", "ninety-four"},
                    {"ninety[\\W_]{0,3}5", "ninety-five"},
                    {"ninety[\\W_]{0,3}6", "ninety-six"},
                    {"ninety[\\W_]{0,3}7", "ninety-seven"},
                    {"ninety[\\W_]{0,3}8", "ninety-eight"},
                    {"ninety[\\W_]{0,3}9", "ninety-nine"}},
            // now resolve compound numeral words
            {{"twenty-one", "21"},
                    {"twenty-two", "22"},
                    {"twenty-three", "23"},
                    {"twenty-four", "24"},
                    {"twenty-five", "25"},
                    {"twenty-six", "26"},
                    {"twenty-seven", "27"},
                    {"twenty-eight", "28"},
                    {"twenty-nine", "29"},
                    {"thirty-one", "31"},
                    {"thirty-two", "32"},
                    {"thirty-three", "33"},
                    {"thirty-four", "34"},
                    {"thirty-five", "35"},
                    {"thirty-six", "36"},
                    {"thirty-seven", "37"},
                    {"thirty-eight", "38"},
                    {"thirty-nine", "39"},
                    {"forty-one", "41"},
                    {"forty-two", "42"},
                    {"forty-three", "43"},
                    {"forty-four", "44"},
                    {"forty-five", "45"},
                    {"forty-six", "46"},
                    {"forty-seven", "47"},
                    {"forty-eight", "48"},
                    {"forty-nine", "49"},
                    {"fifty-one", "51"},
                    {"fifty-two", "52"},
                    {"fifty-three", "53"},
                    {"fifty-four", "54"},
                    {"fifty-five", "55"},
                    {"fifty-six", "56"},
                    {"fifty-seven", "57"},
                    {"fifty-eight", "58"},
                    {"fifty-nine", "59"},
                    {"sixty-one", "61"},
                    {"sixty-two", "62"},
                    {"sixty-three", "63"},
                    {"sixty-four", "64"},
                    {"sixty-five", "65"},
                    {"sixty-six", "66"},
                    {"sixty-seven", "67"},
                    {"sixty-eight", "68"},
                    {"sixty-nine", "69"},
                    {"seventy-one", "71"},
                    {"seventy-two", "72"},
                    {"seventy-three", "73"},
                    {"seventy-four", "74"},
                    {"seventy-five", "75"},
                    {"seventy-six", "76"},
                    {"seventy-seven", "77"},
                    {"seventy-eight", "78"},
                    {"seventy-nine", "79"},
                    {"eighty-one", "81"},
                    {"eighty-two", "82"},
                    {"eighty-three", "83"},
                    {"eighty-four", "84"},
                    {"eighty-five", "85"},
                    {"eighty-six", "86"},
                    {"eighty-seven", "87"},
                    {"eighty-eight", "88"},
                    {"eighty-nine", "89"},
                    {"ninety-one", "91"},
                    {"ninety-two", "92"},
                    {"ninety-three", "93"},
                    {"ninety-four", "94"},
                    {"ninety-five", "95"},
                    {"ninety-six", "96"},
                    {"ninety-seven", "97"},
                    {"ninety-eight", "98"},
                    {"ninety-nine", "99"}},
            // larger units function as suffixes now
            // assume never have three hundred four, three hundred and four
            {{"hundred", "00"},
                    {"thousand", "000"}},
            // single numeral words now
            // some would have been ambiguous
            {{"seventeen", "17"},
                    {"thirteen", "13"},
                    {"fourteen", "14"},
                    {"eighteen", "18"},
                    {"nineteen", "19"},
                    {"fifteen", "15"},
                    {"sixteen", "16"},
                    {"seventy", "70"},
                    {"eleven", "11"},
                    {"twelve", "12"},
                    {"twenty", "20"},
                    {"thirty", "30"},
                    {"eighty", "80"},
                    {"ninety", "90"},
                    {"three", "3"},
                    {"seven", "7"},
                    {"eight", "8"},
                    {"forty", "40"},
                    {"fifty", "50"},
                    {"sixty", "60"},
                    {"zero", "0"},
                    {"four", "4"},
                    {"five", "5"},
                    {"nine", "9"},
                    {"one", "1"},
                    {"two", "2"},
                    {"six", "6"},
                    {"ten", "10"}},
            // now do letter for digit substitutions
            {{"oh", "0"},
                    {"o", "0"},
                    {"i", "1"},
                    {"l", "1"}}
    };
}"
tika-core/src/main/java/org/apache/tika/sax/ContentHandlerDecorator.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax;

import org.xml.sax.Attributes;
import org.xml.sax.ContentHandler;
import org.xml.sax.Locator;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.DefaultHandler;

/**
 * Decorator base class for the {@link ContentHandler} interface. This class
 * simply delegates all SAX events calls to an underlying decorated handler
 * instance. Subclasses can provide extra decoration by overriding one or more
 * of the SAX event methods.
 */
public class ContentHandlerDecorator extends DefaultHandler {

    /**
     * Decorated SAX event handler.
     */
    private ContentHandler handler;

    /**
     * Creates a decorator for the given SAX event handler.
     *
     * @param handler SAX event handler to be decorated
     */
    public ContentHandlerDecorator(ContentHandler handler) {
        assert handler != null;
        this.handler = handler;
    }

    /**
     * Creates a decorator that by default forwards incoming SAX events to
     * a dummy content handler that simply ignores all the events. Subclasses
     * should use the {@link #setContentHandler(ContentHandler)} method to
     * switch to a more usable underlying content handler.
     */
    protected ContentHandlerDecorator() {
        this(new DefaultHandler());
    }

    /**
     * Sets the underlying content handler. All future SAX events will be
     * directed to this handler instead of the one that was previously used.
     *
     * @param handler content handler
     */
    protected void setContentHandler(ContentHandler handler) {
        assert handler != null;
        this.handler = handler;
    }

    @Override
    public void startPrefixMapping(String prefix, String uri)
            throws SAXException {
        try {
            handler.startPrefixMapping(prefix, uri);
        } catch (SAXException e) {
            handleException(e);
        }
    }

    @Override
    public void endPrefixMapping(String prefix) throws SAXException {
        try {
            handler.endPrefixMapping(prefix);
        } catch (SAXException e) {
            handleException(e);
        }
    }

    @Override
    public void processingInstruction(String target, String data)
            throws SAXException {
        try {
            handler.processingInstruction(target, data);
        } catch (SAXException e) {
            handleException(e);
        }
    }

    @Override
    public void setDocumentLocator(Locator locator) {
        handler.setDocumentLocator(locator);
    }

    @Override
    public void startDocument() throws SAXException {
        try {
            handler.startDocument();
        } catch (SAXException e) {
            handleException(e);
        }
    }

    @Override
    public void endDocument() throws SAXException {
        try {
            handler.endDocument();
        } catch (SAXException e) {
            handleException(e);
        }
    }

    @Override
    public void startElement(
            String uri, String localName, String name, Attributes atts)
            throws SAXException {
        try {
            handler.startElement(uri, localName, name, atts);
        } catch (SAXException e) {
            handleException(e);
        }
    }

    @Override
    public void endElement(String uri, String localName, String name)
            throws SAXException {
        try {
            handler.endElement(uri, localName, name);
        } catch (SAXException e) {
            handleException(e);
        }
    }

    @Override
    public void characters(char[] ch, int start, int length)
            throws SAXException {
        try {
            handler.characters(ch, start, length);
        } catch (SAXException e) {
            handleException(e);
        }
    }

    @Override
    public void ignorableWhitespace(char[] ch, int start, int length)
            throws SAXException {
        try {
            handler.ignorableWhitespace(ch, start, length);
        } catch (SAXException e) {
            handleException(e);
        }
    }

    @Override
    public void skippedEntity(String name) throws SAXException {
        try {
            handler.skippedEntity(name);
        } catch (SAXException e) {
            handleException(e);
        }
    }

    @Override
    public String toString() {
        return handler.toString();
    }

    /**
     * Handle any exceptions thrown by methods in this class. This method
     * provides a single place to implement custom exception handling. The
     * default behaviour is simply to re-throw the given exception, but
     * subclasses can also provide alternative ways of handling the situation.
     *
     * @param exception the exception that was thrown
     * @throws SAXException the exception (if any) thrown to the client
     */
    protected void handleException(SAXException exception) throws SAXException {
        throw exception;
    }

}
"
tika-core/src/main/java/org/apache/tika/sax/ContentHandlerFactory.java,false,"package org.apache.tika.sax;

/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import org.xml.sax.ContentHandler;

import java.io.OutputStream;
import java.io.UnsupportedEncodingException;

/**
 * Interface to allow easier injection of code for getting a new ContentHandler
 */
public interface ContentHandlerFactory {
    public ContentHandler getNewContentHandler();
    public ContentHandler getNewContentHandler(OutputStream os, String encoding) throws UnsupportedEncodingException;

}
"
tika-core/src/main/java/org/apache/tika/sax/ElementMappingContentHandler.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax;

import java.util.Map;
import java.util.Collections;
import javax.xml.namespace.QName;

import org.xml.sax.SAXException;
import org.xml.sax.ContentHandler;
import org.xml.sax.Attributes;
import org.xml.sax.helpers.AttributesImpl;

/**
 * Content handler decorator that maps element <code>QName</code>s using
 * a <code>Map</code>. Not mappable elements are not forwarded.
 * Attributes may also be mapped (for each element different using
 * a <code>Map</code> for attributes), not mappable attributes are not
 * forwarded. The default is to not map any attributes and therefore do
 * not forward any of them.
 */
public class ElementMappingContentHandler extends ContentHandlerDecorator {

    private final Map<QName, TargetElement> mappings;

    public ElementMappingContentHandler(
            ContentHandler handler, Map<QName, TargetElement> mappings) {
        super(handler);
        this.mappings = mappings;
    }

    @Override
    public void startElement(
            String namespaceURI, String localName, String qName,
            Attributes atts) throws SAXException {
        TargetElement mapping =
            mappings.get(new QName(namespaceURI, localName));
        if (mapping != null) {
            QName tag = mapping.getMappedTagName();
            super.startElement(
                    tag.getNamespaceURI(), tag.getLocalPart(),
                    getQNameAsString(tag), mapping.mapAttributes(atts));
        }
    }

    @Override
    public void endElement(String namespaceURI, String localName, String qName)
            throws SAXException {
        TargetElement mapping =
            mappings.get(new QName(namespaceURI, localName));
        if (mapping != null) {
            QName tag=mapping.getMappedTagName();
            super.endElement(
                    tag.getNamespaceURI(), tag.getLocalPart(),
                    getQNameAsString(tag));
        }
    }

    protected static final String getQNameAsString(QName qname) {
        String prefix = qname.getPrefix();
        if (prefix.length() > 0) {
            return prefix + ":" + qname.getLocalPart();
        } else {
            return qname.getLocalPart(); 
        }
    }

    public static class TargetElement {

        /**
         * Creates an TargetElement, attributes of this element will
         * be mapped as specified
         */
        public TargetElement(
                QName mappedTagName, Map<QName, QName> attributesMapping) {
            this.mappedTagName = mappedTagName;
            this.attributesMapping = attributesMapping;
        }

        /**
         * A shortcut that automatically creates the QName object
         */
        public TargetElement(
                String mappedTagURI, String mappedTagLocalName,
                Map<QName, QName> attributesMapping) {
            this(new QName(mappedTagURI, mappedTagLocalName), attributesMapping);
        }

        /**
         * Creates an TargetElement with no attributes, all attributes
         * will be deleted from SAX stream
         */
        public TargetElement(QName mappedTagName) {
            this(mappedTagName, Collections.<QName,QName>emptyMap());
        }

        /** A shortcut that automatically creates the QName object */
        public TargetElement(String mappedTagURI, String mappedTagLocalName) {
            this(mappedTagURI, mappedTagLocalName,
                    Collections.<QName,QName>emptyMap());
        }

        public QName getMappedTagName() {
            return mappedTagName;
        }

        public Map<QName, QName> getAttributesMapping() {
            return attributesMapping;
        }

        public Attributes mapAttributes(final Attributes atts) {
            AttributesImpl natts = new AttributesImpl();
            for (int i = 0; i < atts.getLength(); i++) {
                QName name = attributesMapping.get(
                        new QName(atts.getURI(i), atts.getLocalName(i)));
                if (name!=null) {
                    natts.addAttribute(
                            name.getNamespaceURI(), name.getLocalPart(),
                            getQNameAsString(name),
                            atts.getType(i), atts.getValue(i));
                }
            }
            return natts;
        }

        private final QName mappedTagName;

        private final Map<QName, QName> attributesMapping;

    }

}
"
tika-core/src/main/java/org/apache/tika/sax/EmbeddedContentHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax;

import org.xml.sax.ContentHandler;

/**
 * Content handler decorator that prevents the {@link #startDocument()}
 * and {@link #endDocument()} events from reaching the decorated handler.
 * This is useful when you want to direct the results of parsing multiple
 * different XML documents into a single target document without worrying
 * about the {@link #startDocument()} and {@link #endDocument()} methods
 * being called more than once.
 */
public class EmbeddedContentHandler extends ContentHandlerDecorator {

    /**
     * Created a decorator that prevents the given handler from
     * receiving {@link #startDocument()} and {@link #endDocument()}
     * events.
     *
     * @param handler the content handler to be decorated
     */
    public EmbeddedContentHandler(ContentHandler handler) {
        super(handler);
    }

    /**
     * Ignored.
     */
    @Override
    public void startDocument() {
    }

    /**
     * Ignored.
     */
    @Override
    public void endDocument() {
    }

}
"
tika-core/src/main/java/org/apache/tika/sax/EndDocumentShieldingContentHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax;

import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * A wrapper around a {@link ContentHandler} which will ignore normal
 *  SAX calls to {@link #endDocument()}, and only fire them later.
 * This is typically used to ensure that we can output the metadata
 *  before ending the document
 */
public class EndDocumentShieldingContentHandler extends ContentHandlerDecorator {
    private boolean endDocumentCalled;

    /**
     * Creates a decorator for the given SAX event handler.
     *
     * @param handler SAX event handler to be decorated
     */
    public EndDocumentShieldingContentHandler(ContentHandler handler) {
       super(handler);
       endDocumentCalled = false;
    }

    @Override
    public void endDocument() throws SAXException {
        endDocumentCalled = true;
    }
    
    public void reallyEndDocument() throws SAXException {
       super.endDocument();
    }
    
    public boolean getEndDocumentWasCalled() {
       return endDocumentCalled;
    }
}
"
tika-core/src/main/java/org/apache/tika/sax/ExpandedTitleContentHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax;

import javax.xml.transform.sax.TransformerHandler;

import org.xml.sax.Attributes;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * Content handler decorator which wraps a {@link TransformerHandler} in order to 
 * allow the <code>TITLE</code> tag to render as <code>&lt;title&gt;&lt;/title&gt;</code>
 * rather than <code>&lt;title/&gt;</code> which is accomplished
 * by calling the {@link TransformerHandler#characters(char[], int, int)} method
 * with a <code>length</code> of 1 but a zero length char array.
 * <p>
 * This workaround is an unfortunate circumstance of the limitations imposed by the
 * implementation of the XML serialization code in the JDK brought over from
 * the xalan project which no longer allows for the specification of an 
 * alternate <code>content-handler</code> via xslt templates or other means.
 * 
 * @see <a href="https://issues.apache.org/jira/browse/TIKA-725">TIKA-725</a>
 */
public class ExpandedTitleContentHandler extends ContentHandlerDecorator {
    
    private boolean isTitleTagOpen;
    private static final String TITLE_TAG = "TITLE";
    
    public ExpandedTitleContentHandler() {
        super();
    }

    public ExpandedTitleContentHandler(ContentHandler handler) {
        super(handler);
    }

    @Override
    public void startDocument() throws SAXException {
        super.startDocument();
        isTitleTagOpen = false;
    }

    @Override
    public void startElement(String uri, String localName, String qName,
            Attributes atts) throws SAXException {
        super.startElement(uri, localName, qName, atts);
        if (TITLE_TAG.equalsIgnoreCase(localName) && XHTMLContentHandler.XHTML.equals(uri)) {
            isTitleTagOpen = true;
        }
    }

    @Override
    public void endElement(String uri, String localName, String qName)
            throws SAXException {
        super.endElement(uri, localName, qName);
        if (TITLE_TAG.equalsIgnoreCase(localName) && XHTMLContentHandler.XHTML.equals(uri)) {
            isTitleTagOpen = false;
        }
    }

    @Override
    public void characters(char[] ch, int start, int length)
            throws SAXException {
        if (isTitleTagOpen && length == 0) {
            // Hack to close the title tag
            try {
                super.characters(new char[0], 0, 1);
            } catch (ArrayIndexOutOfBoundsException e) {
                // Expected, just wanted to close the title tag
            }
        } else {
            super.characters(ch, start, length);
        }
    }

}
"
tika-core/src/main/java/org/apache/tika/sax/Link.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax;

public class Link {

    private final String type;

    private final String uri;

    private final String title;

    private final String text;

    private final String rel;

    public Link(String type, String uri, String title, String text) {
        this.type = type;
        this.uri = uri;
        this.title = title;
        this.text = text;
        this.rel = "";
    }

    public Link(String type, String uri, String title, String text, String rel) {
        this.type = type;
        this.uri = uri;
        this.title = title;
        this.text = text;
        this.rel = rel;
    }

    public boolean isAnchor() {
        return "a".equals(type);
    }

    public boolean isImage() {
        return "img".equals(type);
    }

    public String getType() {
        return type;
    }

    public String getUri() {
        return uri;
    }

    public String getTitle() {
        return title;
    }

    public String getText() {
        return text;
    }

    public String getRel() {
      return rel;
    }

    public String toString() {
        StringBuilder builder = new StringBuilder();
        if (isImage()) {
            builder.append("<img src=\"");
            builder.append(uri);
            if (title != null && title.length() > 0) {
                builder.append("\" title=\"");
                builder.append(title);
            }
            if (text != null && text.length() > 0) {
                builder.append("\" alt=\"");
                builder.append(text);
            }
            builder.append("\"/>");
        } else {
            builder.append("<");
            builder.append(type);
            builder.append(" href=\"");
            builder.append(uri);
            if (title != null && title.length() > 0) {
                builder.append("\" title=\"");
                builder.append(title);
            }
            if (rel != null && rel.length() > 0) {
                builder.append("\" rel=\"");
                builder.append(rel);
            }
            builder.append("\">");
            builder.append(text);
            builder.append("</");
            builder.append(type);
            builder.append(">");
        }
        return builder.toString();
    }

}
"
tika-core/src/main/java/org/apache/tika/sax/LinkBuilder.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax;

class LinkBuilder {

    private final String type;

    private String uri = "";

    private String title = "";

    private String rel = "";

    private final StringBuilder text = new StringBuilder();

    public LinkBuilder(String type) {
        this.type = type;
    }

    public void setURI(String uri) {
        if (uri != null) {
            this.uri = uri;
        } else {
            this.uri = "";
        }
    }

    public void setTitle(String title) {
        if (title != null) {
            this.title = title;
        } else {
            this.title = "";
        }
    }

    public void setRel(String rel) {
        if (rel != null) {
            this.rel = rel;
        } else {
            this.rel = "";
        }
    }

    public void characters(char[] ch, int offset, int length) {
        text.append(ch, offset, length);
    }

    public Link getLink() {
        return getLink(false);
    }
    
    public Link getLink(boolean collapseWhitespace) {
        String anchor = text.toString();
        
        if (collapseWhitespace) {
            anchor = anchor.replaceAll("\\s+", " ").trim();
        }
        
        return new Link(type, uri, title, anchor, rel);
    }

}
"
tika-core/src/main/java/org/apache/tika/sax/LinkContentHandler.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax;

import static org.apache.tika.sax.XHTMLContentHandler.XHTML;

import java.util.ArrayList;
import java.util.LinkedList;
import java.util.List;

import org.xml.sax.Attributes;
import org.xml.sax.helpers.DefaultHandler;

/**
 * Content handler that collects links from an XHTML document.
 */
public class LinkContentHandler extends DefaultHandler {

    /**
     * Stack of link builders, one for each level of nested links currently
     * being processed. A usual case of a nested link would be a hyperlinked
     * image (<code>&a href="..."&gt;&lt;img src="..."&gt;&lt;&gt;</code>),
     * but it's possible (though unlikely) for also other kinds of nesting
     * to occur.
     */
    private final LinkedList<LinkBuilder> builderStack =
        new LinkedList<LinkBuilder>();

    /** Collected links */
    private final List<Link> links = new ArrayList<Link>();
    
    /** Whether to collapse whitespace in anchor text */
    private boolean collapseWhitespaceInAnchor;
    
    /**
     * Default constructor
     */
    public LinkContentHandler() { 
        this(false);
    }
    
    /**
     * Default constructor
     *
     * @boolean collapseWhitespaceInAnchor
     */
    public LinkContentHandler(boolean collapseWhitespaceInAnchor) {
      super();
      
      this.collapseWhitespaceInAnchor = collapseWhitespaceInAnchor;
    }

    /**
     * Returns the list of collected links.
     *
     * @return collected links
     */
    public List<Link> getLinks() {
        return links;
    }

    //-------------------------------------------------------< ContentHandler>

    @Override
    public void startElement(
            String uri, String local, String name, Attributes attributes) {
        if (XHTML.equals(uri)) {
            if ("a".equals(local)) {
                LinkBuilder builder = new LinkBuilder("a");
                builder.setURI(attributes.getValue("", "href"));
                builder.setTitle(attributes.getValue("", "title"));
                builder.setRel(attributes.getValue("", "rel"));
                builderStack.addFirst(builder);
            } else if ("img".equals(local)) {
                LinkBuilder builder = new LinkBuilder("img");
                builder.setURI(attributes.getValue("", "src"));
                builder.setTitle(attributes.getValue("", "title"));
                builder.setRel(attributes.getValue("", "rel"));
                builderStack.addFirst(builder);

                String alt = attributes.getValue("", "alt");
                if (alt != null) {
                    char[] ch = alt.toCharArray();
                    characters(ch, 0, ch.length);
                }
            }
        }
    }

    @Override
    public void characters(char[] ch, int start, int length) {
        for (LinkBuilder builder : builderStack) {
            builder.characters(ch, start, length);
        }
    }

    @Override
    public void ignorableWhitespace(char[] ch, int start, int length) {
        characters(ch, start, length);
    }

    @Override
    public void endElement(String uri, String local, String name) {
        if (XHTML.equals(uri)) {
            if ("a".equals(local) || "img".equals(local)) {
                links.add(builderStack.removeFirst().getLink(collapseWhitespaceInAnchor));
            }
        }
    }

}
"
tika-core/src/main/java/org/apache/tika/sax/OfflineContentHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax;

import org.apache.tika.io.ClosedInputStream;
import org.xml.sax.ContentHandler;
import org.xml.sax.InputSource;

/**
 * Content handler decorator that always returns an empty stream from the
 * {@link #resolveEntity(String, String)} method to prevent potential
 * network or other external resources from being accessed by an XML parser.
 *
 * @see <a href="https://issues.apache.org/jira/browse/TIKA-185">TIKA-185</a>
 */
public class OfflineContentHandler extends ContentHandlerDecorator {

    public OfflineContentHandler(ContentHandler handler) {
        super(handler);
    }

    /**
     * Returns an empty stream. This will make an XML parser silently
     * ignore any external entities.
     */
    @Override
    public InputSource resolveEntity(String publicId, String systemId) {
        return new InputSource(new ClosedInputStream());
    }

}
"
tika-core/src/main/java/org/apache/tika/sax/package-info.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * SAX utilities.
 */
@aQute.bnd.annotation.Version("1.0.0")
package org.apache.tika.sax;
"
tika-core/src/main/java/org/apache/tika/sax/PhoneExtractingContentHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.sax;

import org.apache.tika.metadata.Metadata;
import org.apache.tika.sax.CleanPhoneText;
import org.apache.tika.sax.ContentHandlerDecorator;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.DefaultHandler;

import java.util.Arrays;
import java.util.List;

/**
 * Class used to extract phone numbers while parsing.
 *
 * Every time a document is parsed in Tika, the content is split into SAX events.
 * Those SAX events are handled by a ContentHandler. You can think of these events
 * as marking a tag in an HTML file. Once you're finished parsing, you can call
 * handler.toString(), for example, to get the text contents of the file. On the other
 * hand, any of the metadata of the file will be added to the Metadata object passed
 * in during the parse() call.  So, the Parser class sends metadata to the Metadata
 * object and content to the ContentHandler.
 *
 * This class is an example of how to combine a ContentHandler and a Metadata.
 * As content is passed to the handler, we first check to see if it matches a
 * textual pattern for a phone number. If the extracted content is a phone number,
 * we add it to the metadata under the key "phonenumbers". So, if you used this
 * ContentHandler when you parsed a document, then called
 * metadata.getValues("phonenumbers"), you would get an array of Strings of phone
 * numbers found in the document.
 *
 * Please see the PhoneExtractingContentHandlerTest for an example of how to use
 * this class.
 *
 */
public class PhoneExtractingContentHandler extends ContentHandlerDecorator {
    private Metadata metadata;
    private static final String PHONE_NUMBERS = "phonenumbers";
    private StringBuilder stringBuilder;

    /**
     * Creates a decorator for the given SAX event handler and Metadata object.
     *
     * @param handler SAX event handler to be decorated
     */
    public PhoneExtractingContentHandler(ContentHandler handler, Metadata metadata) {
        super(handler);
        this.metadata = metadata;
        this.stringBuilder = new StringBuilder();
    }

    /**
     * Creates a decorator that by default forwards incoming SAX events to
     * a dummy content handler that simply ignores all the events. Subclasses
     * should use the {@link #setContentHandler(ContentHandler)} method to
     * switch to a more usable underlying content handler.
     * Also creates a dummy Metadata object to store phone numbers in.
     */
    protected PhoneExtractingContentHandler() {
        this(new DefaultHandler(), new Metadata());
    }

    /**
     * The characters method is called whenever a Parser wants to pass raw...
     * characters to the ContentHandler. But, sometimes, phone numbers are split
     * accross different calls to characters, depending on the specific Parser
     * used. So, we simply add all characters to a StringBuilder and analyze it
     * once the document is finished.
     */
    @Override
    public void characters(char[] ch, int start, int length) throws SAXException {
        try {
            String text = new String(Arrays.copyOfRange(ch, start, start + length));
            stringBuilder.append(text);
            super.characters(ch, start, length);
        } catch (SAXException e) {
            handleException(e);
        }
    }


    /**
     * This method is called whenever the Parser is done parsing the file. So,
     * we check the output for any phone numbers.
     */
    @Override
    public void endDocument() throws SAXException {
        super.endDocument();
        List<String> numbers = CleanPhoneText.extractPhoneNumbers(stringBuilder.toString());
        for (String number : numbers) {
            metadata.add(PHONE_NUMBERS, number);
        }
    }
}
"
tika-core/src/main/java/org/apache/tika/sax/SafeContentHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax;

/*
import java.util.ArrayList;
import java.util.List;
*/

import org.xml.sax.Attributes;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.AttributesImpl;

/**
 * Content handler decorator that makes sure that the character events
 * ({@link #characters(char[], int, int)} or
 * {@link #ignorableWhitespace(char[], int, int)}) passed to the decorated
 * content handler contain only valid XML characters. All invalid characters
 * are replaced with spaces.
 * <p>
 * The XML standard defines the following Unicode character ranges as
 * valid XML characters:
 * <pre>
 * #x9 | #xA | #xD | [#x20-#xD7FF] | [#xE000-#xFFFD] | [#x10000-#x10FFFF]
 * </pre>
 * <p>
 * Note that currently this class only detects those invalid characters whose
 * UTF-16 representation fits a single char. Also, this class does not ensure
 * that the UTF-16 encoding of incoming characters is correct.
 */
public class SafeContentHandler extends ContentHandlerDecorator {

    /**
     * Replacement for invalid characters.
     */
    private static final char[] REPLACEMENT = new char[] { '\ufffd' };

    /**
     * Internal interface that allows both character and
     * ignorable whitespace content to be filtered the same way.
     */
    protected interface Output {
        void write(char[] ch, int start, int length) throws SAXException;
    }

    private static class StringOutput implements Output {

        private final StringBuilder builder = new StringBuilder();

        public void write(char[] ch, int start, int length) {
            builder.append(ch, start, length);
        }

        public String toString() {
            return builder.toString();
        }

    }

    /**
     * Output through the {@link ContentHandler#characters(char[], int, int)}
     * method of the decorated content handler.
     */
    private final Output charactersOutput = new Output() {
        public void write(char[] ch, int start, int length)
                throws SAXException {
            SafeContentHandler.super.characters(ch, start, length);
        }
    };

    /**
     * Output through the
     * {@link ContentHandler#ignorableWhitespace(char[], int, int)}
     * method of the decorated content handler.
     */
    private final Output ignorableWhitespaceOutput = new Output() {
        public void write(char[] ch, int start, int length)
                throws SAXException {
            SafeContentHandler.super.ignorableWhitespace(ch, start, length);
        }
    };

    public SafeContentHandler(ContentHandler handler) {
        super(handler);
    }

    /**
     * Filters and outputs the contents of the given input buffer. Any
     * invalid characters in the input buffer area handled by sending a
     * replacement (a space character) to the given output. Any sequences
     * of valid characters are passed as-is to the given output. 
     * 
     * @param ch input buffer
     * @param start start offset within the buffer
     * @param length number of characters to read from the buffer
     * @param output output channel
     * @throws SAXException if the filtered characters could not be written out
     */
    private void filter(char[] ch, int start, int length, Output output)
            throws SAXException {
        int end = start + length;

        int i = start;
        while (i < end) {
            int c = Character.codePointAt(ch, i, end);
            int j = i + Character.charCount(c);

            if (isInvalid(c)) {
                // Output any preceding valid characters
                if (i > start) {
                    output.write(ch, start, i - start);
                }

                // Output the replacement for this invalid character
                writeReplacement(output);

                // Continue with the rest of the array
                start = j;
            }

            i = j;
        }

        // Output any remaining valid characters
        output.write(ch, start, end - start);
    }

    /**
     * Checks if the given string contains any invalid XML characters.
     *
     * @param value string to be checked
     * @return <code>true</code> if the string contains invalid XML characters,
     *         <code>false</code> otherwise
     */
    private boolean isInvalid(String value) {
        char[] ch = value.toCharArray();

        int i = 0;
        while (i < ch.length) {
            int c = Character.codePointAt(ch, i);
            if (isInvalid(c)) {
                return true;
            }
            i = i + Character.charCount(c);
        }

        return false;
    }

    /**
     * Checks whether the given Unicode character is an invalid XML character
     * and should be replaced for output. Subclasses can override this method
     * to use an alternative definition of which characters should be replaced
     * in the XML output. The default definition from the XML specification is:
     * <pre>
     * Char ::= #x9 | #xA | #xD | [#x20-#xD7FF] | [#xE000-#xFFFD] | [#x10000-#x10FFFF]
     * </pre>
     *
     * @param ch character
     * @return <code>true</code> if the character should be replaced,
     *         <code>false</code> otherwise
     */
    protected boolean isInvalid(int ch) {
        if (ch < 0x20) {
            return ch != 0x09 && ch != 0x0A && ch != 0x0D;
        } else if (ch < 0xE000) {
            return ch > 0xD7FF;
        } else if (ch < 0x10000) {
            return ch > 0xFFFD;
        } else {
            return ch > 0x10FFFF;
        }
    }

    /**
     * Outputs the replacement for an invalid character. Subclasses can
     * override this method to use a custom replacement.
     *
     * @param output where the replacement is written to
     * @throws SAXException if the replacement could not be written
     */
    protected void writeReplacement(Output output) throws SAXException {
        output.write(REPLACEMENT, 0, REPLACEMENT.length);
    }


    /*
    private final List<String> elements = new ArrayList<String>();

    // Called only from assert
    private boolean verifyStartElement(String name) {
        // TODO: we could strengthen this to do full
        // XTHML validation, eg you shouldn't start p inside
        // another p (but ODF parser, at least, seems to
        // violate this):
        //if (name.equals("p")) {
        //assert elements.size() == 0 || !elements.get(elements.size()-1).equals("p");
        //}
        elements.add(name);
        return true;
    }

    // Called only from assert
    private boolean verifyEndElement(String name) {
        assert elements.size() > 0: "end tag=" + name + " with no startElement";
        final String currentElement = elements.get(elements.size()-1);
        assert currentElement.equals(name): "mismatched elements open=" + currentElement + " close=" + name;
        elements.remove(elements.size()-1);
        return true;
    }

    // Called only from assert
    private boolean verifyEndDocument() {
        assert elements.size() == 0;
        return true;
    }
    */

    //------------------------------------------------------< ContentHandler >

    @Override
    public void startElement(
            String uri, String localName, String name, Attributes atts)
            throws SAXException {
        // TODO: enable this, but some parsers currently
        // trip it
        //assert verifyStartElement(name);
        // Look for any invalid characters in attribute values.
        for (int i = 0; i < atts.getLength(); i++) {
            if (isInvalid(atts.getValue(i))) {
                // Found an invalid character, so need to filter the attributes
                AttributesImpl filtered = new AttributesImpl();
                for (int j = 0; j < atts.getLength(); j++) {
                    String value = atts.getValue(j);
                    if (j >= i && isInvalid(value)) {
                        // Filter the attribute value when needed
                        Output buffer = new StringOutput();
                        filter(value.toCharArray(), 0, value.length(), buffer);
                        value = buffer.toString();
                    }
                    filtered.addAttribute(
                            atts.getURI(j), atts.getLocalName(j),
                            atts.getQName(j), atts.getType(j), value);
                }
                atts = filtered;
                break;
            }
        }
        super.startElement(uri, localName, name, atts);
    }

    @Override
    public void endElement(String uri, String localName, String name)
            throws SAXException {
        // TODO: enable this, but some parsers currently
        // trip it
        //assert verifyEndElement(name);
        super.endElement(uri, localName, name);
    }

    @Override
    public void endDocument() throws SAXException {
        // TODO: enable this, but some parsers currently
        // trip it
        //assert verifyEndDocument();
        super.endDocument();
    }

    @Override
    public void characters(char[] ch, int start, int length)
            throws SAXException {
        filter(ch, start, length, charactersOutput);
    }

    @Override
    public void ignorableWhitespace(char[] ch, int start, int length)
            throws SAXException {
        filter(ch, start, length, ignorableWhitespaceOutput);
    }

}
"
tika-core/src/main/java/org/apache/tika/sax/SecureContentHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax;

import java.io.IOException;
import java.util.LinkedList;

import org.apache.tika.exception.TikaException;
import org.apache.tika.io.TikaInputStream;
import org.xml.sax.Attributes;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * Content handler decorator that attempts to prevent denial of service
 * attacks against Tika parsers.
 * <p>
 * Currently this class simply compares the number of output characters
 * to to the number of input bytes and keeps track of the XML nesting levels.
 * An exception gets thrown if the output seems excessive compared to the
 * input document. This is a strong indication of a zip bomb.
 *
 * @since Apache Tika 0.4
 * @see <a href="https://issues.apache.org/jira/browse/TIKA-216">TIKA-216</a>
 */
public class SecureContentHandler extends ContentHandlerDecorator {

    /**
     * The input stream that Tika is parsing.
     */
    private final TikaInputStream stream;

    /**
     * Number of output characters that Tika has produced so far.
     */
    private long characterCount = 0;

    /**
     * The current XML element depth.
     */
    private int currentDepth = 0;

    /**
     * Current number of nested &lt;div class="package-entr"&gt; elements.
     */
    private LinkedList<Integer> packageEntryDepths = new LinkedList<Integer>();

    /**
     * Output threshold.
     */
    private long threshold = 1000000;

    /**
     * Maximum compression ratio.
     */
    private long ratio = 100;

    /**
     * Maximum XML element nesting level.
     */
    private int maxDepth = 100;

    /**
     * Maximum package entry nesting level.
     */
    private int maxPackageEntryDepth = 10;

    /**
     * Decorates the given content handler with zip bomb prevention based
     * on the count of bytes read from the given counting input stream.
     * The resulting decorator can be passed to a Tika parser along with
     * the given counting input stream.
     *
     * @param handler the content handler to be decorated
     * @param stream the input stream to be parsed
     */
    public SecureContentHandler(
            ContentHandler handler, TikaInputStream stream) {
        super(handler);
        this.stream = stream;
    }

    /**
     * Returns the configured output threshold.
     *
     * @return output threshold
     */
    public long getOutputThreshold() {
        return threshold;
    }


    /**
     * Sets the threshold for output characters before the zip bomb prevention
     * is activated. This avoids false positives in cases where an otherwise
     * normal document for some reason starts with a highly compressible
     * sequence of bytes.
     *
     * @param threshold new output threshold
     */
    public void setOutputThreshold(long threshold) {
        this.threshold = threshold;
    }


    /**
     * Returns the maximum compression ratio.
     *
     * @return maximum compression ratio
     */
    public long getMaximumCompressionRatio() {
        return ratio;
    }


    /**
     * Sets the ratio between output characters and input bytes. If this
     * ratio is exceeded (after the output threshold has been reached) then
     * an exception gets thrown.
     *
     * @param ratio new maximum compression ratio
     */
    public void setMaximumCompressionRatio(long ratio) {
        this.ratio = ratio;
    }

    /**
     * Returns the maximum XML element nesting level.
     *
     * @return maximum XML element nesting level
     */
    public int getMaximumDepth() {
        return maxDepth;
    }


    /**
     * Sets the maximum package entry nesting level. If this depth level is
     * exceeded then an exception gets thrown.
     *
     * @param depth maximum package entry nesting level
     */
    public void setMaximumPackageEntryDepth(int depth) {
        this.maxPackageEntryDepth = depth;
    }

    /**
     * Returns the maximum package entry nesting level.
     *
     * @return maximum package entry nesting level
     */
    public int getMaximumPackageEntryDepth() {
        return maxPackageEntryDepth;
    }


    /**
     * Sets the maximum XML element nesting level. If this depth level is
     * exceeded then an exception gets thrown.
     *
     * @param depth maximum XML element nesting level
     */
    public void setMaximumDepth(int depth) {
        this.maxDepth = depth;
    }

    /**
     * Converts the given {@link SAXException} to a corresponding
     * {@link TikaException} if it's caused by this instance detecting
     * a zip bomb.
     *
     * @param e SAX exception
     * @throws TikaException zip bomb exception
     */
    public void throwIfCauseOf(SAXException e) throws TikaException {
        if (e instanceof SecureSAXException
                && ((SecureSAXException) e).isCausedBy(this)) {
            throw new TikaException("Zip bomb detected!", e);
        }
    }

    private long getByteCount() throws SAXException {
        try {
            if (stream.hasLength()) {
                return stream.getLength();
            } else {
                return stream.getPosition();
            }
        } catch (IOException e) {
            throw new SAXException("Unable to get stream length", e);
        }
    }

    /**
     * Records the given number of output characters (or more accurately
     * UTF-16 code units). Throws an exception if the recorded number of
     * characters highly exceeds the number of input bytes read.
     *
     * @param length number of new output characters produced
     * @throws SAXException if a zip bomb is detected
     */
    private void advance(int length) throws SAXException {
        characterCount += length;
        long byteCount = getByteCount();
        if (characterCount > threshold
                && characterCount > byteCount * ratio) {
            throw new SecureSAXException(
                    "Suspected zip bomb: "
                    + byteCount + " input bytes produced "
                    + characterCount + " output characters");
        }
    }

    @Override
    public void startElement(
            String uri, String localName, String name, Attributes atts)
            throws SAXException {
        currentDepth++;
        if (currentDepth >= maxDepth) {
            throw new SecureSAXException(
                    "Suspected zip bomb: "
                    + currentDepth + " levels of XML element nesting");
        }

        if ("div".equals(name)
                && "package-entry".equals(atts.getValue("class"))) {
            packageEntryDepths.addLast(currentDepth);
            if (packageEntryDepths.size() >= maxPackageEntryDepth) {
                throw new SecureSAXException(
                        "Suspected zip bomb: "
                        + packageEntryDepths.size()
                        + " levels of package entry nesting");
            }
        }

        super.startElement(uri, localName, name, atts);
    }

    @Override
    public void endElement(
            String uri, String localName, String name) throws SAXException {
        super.endElement(uri, localName, name);

        if (!packageEntryDepths.isEmpty()
                && packageEntryDepths.getLast() == currentDepth) {
            packageEntryDepths.removeLast();
        }

        currentDepth--;
    }

    @Override
    public void characters(char[] ch, int start, int length)
            throws SAXException {
        advance(length);
        super.characters(ch, start, length);
    }

    @Override
    public void ignorableWhitespace(char[] ch, int start, int length)
            throws SAXException {
        advance(length);
        super.ignorableWhitespace(ch, start, length);
    }

    /**
     * Private exception class used to indicate a suspected zip bomb.
     *
     * @see SecureContentHandler#throwIfCauseOf(SAXException)
     */
    private class SecureSAXException extends SAXException {

        /** Serial version UID.*/
        private static final long serialVersionUID = 2285245380321771445L;

        public SecureSAXException(String message) throws SAXException {
            super(message);
        }

        public boolean isCausedBy(SecureContentHandler handler) {
            return SecureContentHandler.this == handler;
        }

    }

}
"
tika-core/src/main/java/org/apache/tika/sax/TaggedContentHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax;

import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * A content handler decorator that tags potential exceptions so that the
 * handler that caused the exception can easily be identified. This is
 * done by using the {@link TaggedSAXException} class to wrap all thrown
 * {@link SAXException}s. See below for an example of using this class.
 * <pre>
 * TaggedContentHandler handler = new TaggedContentHandler(...);
 * try {
 *     // Processing that may throw an SAXException either from this handler
 *     // or from some other XML parsing activity
 *     processXML(handler);
 * } catch (SAXException e) {
 *     if (handler.isCauseOf(e)) {
 *         // The exception was caused by this handler.
 *         // Use e.getCause() to get the original exception.
 *     } else {
 *         // The exception was caused by something else.
 *     }
 * }
 * </pre>
 * <p>
 * Alternatively, the {@link #throwIfCauseOf(Exception)} method can be
 * used to let higher levels of code handle the exception caused by this
 * stream while other processing errors are being taken care of at this
 * lower level.
 * <pre>
 * TaggedContentHandler handler = new TaggedContentHandler(...);
 * try {
 *     processXML(handler);
 * } catch (SAXException e) {
 *     stream.throwIfCauseOf(e);
 *     // ... or process the exception that was caused by something else
 * }
 * </pre>
 *
 * @see TaggedSAXException
 */
public class TaggedContentHandler extends ContentHandlerDecorator {

    /**
     * Creates a tagging decorator for the given content handler.
     *
     * @param proxy content handler to be decorated
     */
    public TaggedContentHandler(ContentHandler proxy) {
        super(proxy);
    }

    /**
     * Tests if the given exception was caused by this handler.
     *
     * @param exception an exception
     * @return <code>true</code> if the exception was thrown by this handler,
     *         <code>false</code> otherwise
     */
    public boolean isCauseOf(SAXException exception) {
        if (exception instanceof TaggedSAXException) {
            TaggedSAXException tagged = (TaggedSAXException) exception;
            return this == tagged.getTag();
        } else {
            return false;
        }
    }

    /**
     * Re-throws the original exception thrown by this handler. This method
     * first checks whether the given exception is a {@link TaggedSAXException}
     * wrapper created by this decorator, and then unwraps and throws the
     * original wrapped exception. Returns normally if the exception was
     * not thrown by this handler.
     *
     * @param exception an exception
     * @throws SAXException original exception, if any, thrown by this handler
     */
    public void throwIfCauseOf(Exception exception) throws SAXException {
        if (exception instanceof TaggedSAXException) {
            TaggedSAXException tagged = (TaggedSAXException) exception;
            if (this == tagged.getTag()) {
                throw tagged.getCause();
            }
        }
    }

    /**
     * Tags any {@link SAXException}s thrown, wrapping and re-throwing.
     * 
     * @param e The SAXException thrown
     * @throws SAXException if an XML error occurs
     */
    @Override
    protected void handleException(SAXException e) throws SAXException {
        throw new TaggedSAXException(e, this);
    }

}
"
tika-core/src/main/java/org/apache/tika/sax/TaggedSAXException.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0
 * 
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax;

import org.xml.sax.SAXException;

/**
 * A {@link SAXException} wrapper that tags the wrapped exception with
 * a given object reference. Both the tag and the wrapped original exception
 * can be used to determine further processing when this exception is caught.
 */
public class TaggedSAXException extends SAXException {

    /**
     * The object reference used to tag the exception.
     */
    private final Object tag;

    /**
     * Creates a tagged wrapper for the given exception.
     *
     * @param original the exception to be tagged
     * @param tag tag object
     */
    public TaggedSAXException(SAXException original, Object tag) {
        super(original.getMessage(), original);
        initCause(original); // SAXException has it's own chaining mechanism!
        this.tag = tag;
    }

    /**
     * Returns the object reference used as the tag this exception.
     *
     * @return tag object
     */
    public Object getTag() {
        return tag;
    }

    /**
     * Returns the wrapped exception. The only difference to the overridden
     * {@link Throwable#getCause()} method is the narrower return type.
     *
     * @return wrapped exception
     */
    @Override
    public SAXException getCause() {
        return (SAXException) super.getCause();
    }

}
"
tika-core/src/main/java/org/apache/tika/sax/TeeContentHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax;

import org.xml.sax.Attributes;
import org.xml.sax.ContentHandler;
import org.xml.sax.Locator;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.DefaultHandler;

/**
 * Content handler proxy that forwards the received SAX events to zero or
 * more underlying content handlers.
 */
public class TeeContentHandler extends DefaultHandler {

    private final ContentHandler[] handlers;

    public TeeContentHandler(ContentHandler... handlers) {
        this.handlers = handlers;
    }

    @Override
    public void startPrefixMapping(String prefix, String uri)
            throws SAXException {
        for (ContentHandler handler : handlers) {
            handler.startPrefixMapping(prefix, uri);
        }
    }

    @Override
    public void endPrefixMapping(String prefix) throws SAXException {
        for (ContentHandler handler : handlers) {
            handler.endPrefixMapping(prefix);
        }
    }

    @Override
    public void processingInstruction(String target, String data)
            throws SAXException {
        for (ContentHandler handler : handlers) {
            handler.processingInstruction(target, data);
        }
    }

    @Override
    public void setDocumentLocator(Locator locator) {
        for (ContentHandler handler : handlers) {
            handler.setDocumentLocator(locator);
        }
    }

    @Override
    public void startDocument() throws SAXException {
        for (ContentHandler handler : handlers) {
            handler.startDocument();
        }
    }

    @Override
    public void endDocument() throws SAXException {
        for (ContentHandler handler : handlers) {
            handler.endDocument();
        }
    }

    @Override
    public void startElement(
            String uri, String localName, String name, Attributes atts)
            throws SAXException {
        for (ContentHandler handler : handlers) {
            handler.startElement(uri, localName, name, atts);
        }
    }

    @Override
    public void endElement(String uri, String localName, String name)
            throws SAXException {
        for (ContentHandler handler : handlers) {
            handler.endElement(uri, localName, name);
        }
    }

    @Override
    public void characters(char[] ch, int start, int length)
            throws SAXException {
        for (ContentHandler handler : handlers) {
            handler.characters(ch, start, length);
        }
    }

    @Override
    public void ignorableWhitespace(char[] ch, int start, int length)
            throws SAXException {
        for (ContentHandler handler : handlers) {
            handler.ignorableWhitespace(ch, start, length);
        }
    }

    @Override
    public void skippedEntity(String name) throws SAXException {
        for (ContentHandler handler : handlers) {
            handler.skippedEntity(name);
        }
    }

}
"
tika-core/src/main/java/org/apache/tika/sax/TextContentHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax;

import org.xml.sax.Attributes;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.DefaultHandler;

/**
 * Content handler decorator that only passes the
 * {@link #characters(char[], int, int)} and
 * (@link {@link #ignorableWhitespace(char[], int, int)}
 * (plus {@link #startDocument()} and {@link #endDocument()} events to
 * the decorated content handler.
 */
public class TextContentHandler extends DefaultHandler {

    private static final char[] SPACE = new char[] {' '};

    private final ContentHandler delegate;
    private final boolean addSpaceBetweenElements;

    public TextContentHandler(ContentHandler delegate) {
        this(delegate, false);
    }

    public TextContentHandler(ContentHandler delegate, boolean addSpaceBetweenElements) {
        this.delegate = delegate;
        this.addSpaceBetweenElements = addSpaceBetweenElements;
    }

    @Override
    public void setDocumentLocator(org.xml.sax.Locator locator) {
	    delegate.setDocumentLocator(locator);
    }

    @Override
    public void characters(char[] ch, int start, int length)
            throws SAXException {
        delegate.characters(ch, start, length);
    }

    @Override
    public void ignorableWhitespace(char[] ch, int start, int length)
            throws SAXException {
        delegate.ignorableWhitespace(ch, start, length);
    }

    @Override
    public void startElement(String uri, String localName, String qName, Attributes attributes)
             throws SAXException {
        if (addSpaceBetweenElements) {
            delegate.characters(SPACE, 0, SPACE.length);
        }
    }

    @Override
    public void startDocument() throws SAXException {
        delegate.startDocument();
    }

    @Override
    public void endDocument() throws SAXException {
        delegate.endDocument();
    }

    @Override
    public String toString() {
        return delegate.toString();
    }

}
"
tika-core/src/main/java/org/apache/tika/sax/ToHTMLContentHandler.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax;

import java.io.OutputStream;
import java.io.UnsupportedEncodingException;
import java.util.Arrays;
import java.util.HashSet;
import java.util.Set;

import org.xml.sax.SAXException;

/**
 * SAX event handler that serializes the HTML document to a character stream.
 * The incoming SAX events are expected to be well-formed (properly nested,
 * etc.) and valid HTML.
 *
 * @since Apache Tika 0.10
 */
public class ToHTMLContentHandler extends ToXMLContentHandler {

    private static final Set<String> EMPTY_ELEMENTS =
        new HashSet<String>(Arrays.asList(
            "area", "base", "basefont", "br", "col", "frame", "hr",
            "img", "input", "isindex", "link", "meta", "param"));

    public ToHTMLContentHandler(OutputStream stream, String encoding)
            throws UnsupportedEncodingException {
        super(stream, encoding);
    }

    public ToHTMLContentHandler() {
        super();
    }

    @Override
    public void startDocument() throws SAXException {
    }

    @Override
    public void endElement(String uri, String localName, String qName)
            throws SAXException {
        if (inStartElement) {
            write('>');
            inStartElement = false;

            if (EMPTY_ELEMENTS.contains(localName)) {
                namespaces.clear();
                return;
            }
        }

        super.endElement(uri, localName, qName);
    }

}
"
tika-core/src/main/java/org/apache/tika/sax/ToTextContentHandler.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax;

import java.io.IOException;
import java.io.OutputStream;
import java.io.OutputStreamWriter;
import java.io.StringWriter;
import java.io.UnsupportedEncodingException;
import java.io.Writer;
import java.nio.charset.Charset;

import org.xml.sax.SAXException;
import org.xml.sax.helpers.DefaultHandler;

/**
 * SAX event handler that writes all character content out to a character
 * stream. No escaping or other transformations are made on the character
 * content.
 *
 * @since Apache Tika 0.10
 */
public class ToTextContentHandler extends DefaultHandler {

    /**
     * The character stream.
     */
    private final Writer writer;

    /**
     * Creates a content handler that writes character events to
     * the given writer.
     *
     * @param writer writer
     */
    public ToTextContentHandler(Writer writer) {
        this.writer = writer;
    }

    /**
     * Creates a content handler that writes character events to
     * the given output stream using the platform default encoding.
     *
     * @param stream output stream
     */
    public ToTextContentHandler(OutputStream stream) {
        this(new OutputStreamWriter(stream, Charset.defaultCharset()));
    }

    /**
     * Creates a content handler that writes character events to
     * the given output stream using the given encoding.
     *
     * @param stream output stream
     * @param encoding output encoding
     * @throws UnsupportedEncodingException if the encoding is unsupported
     */
    public ToTextContentHandler(OutputStream stream, String encoding)
            throws UnsupportedEncodingException {
        this(new OutputStreamWriter(stream, encoding));
    }

    /**
     * Creates a content handler that writes character events
     * to an internal string buffer. Use the {@link #toString()}
     * method to access the collected character content.
     */
    public ToTextContentHandler() {
        this(new StringWriter());
    }

    /**
     * Writes the given characters to the given character stream.
     */
    @Override
    public void characters(char[] ch, int start, int length)
            throws SAXException {
        try {
            writer.write(ch, start, length);
        } catch (IOException e) {
            throw new SAXException(
                    "Error writing: " + new String(ch, start, length), e);
        }
    }


    /**
     * Writes the given ignorable characters to the given character stream.
     * The default implementation simply forwards the call to the
     * {@link #characters(char[], int, int)} method.
     */
    @Override
    public void ignorableWhitespace(char[] ch, int start, int length)
            throws SAXException {
        characters(ch, start, length);
    }

    /**
     * Flushes the character stream so that no characters are forgotten
     * in internal buffers.
     *
     * @see <a href="https://issues.apache.org/jira/browse/TIKA-179">TIKA-179</a>
     * @throws SAXException if the stream can not be flushed
     */
    @Override
    public void endDocument() throws SAXException {
        try {
            writer.flush();
        } catch (IOException e) {
            throw new SAXException("Error flushing character output", e);
        }
    }

    /**
     * Returns the contents of the internal string buffer where
     * all the received characters have been collected. Only works
     * when this object was constructed using the empty default
     * constructor or by passing a {@link StringWriter} to the
     * other constructor.
     */
    @Override
    public String toString() {
        return writer.toString();
    }

}
"
tika-core/src/main/java/org/apache/tika/sax/ToXMLContentHandler.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax;

import java.io.OutputStream;
import java.io.UnsupportedEncodingException;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;

import org.xml.sax.Attributes;
import org.xml.sax.SAXException;

/**
 * SAX event handler that serializes the XML document to a character stream.
 * The incoming SAX events are expected to be well-formed (properly nested,
 * etc.) and to explicitly include namespace declaration attributes and
 * corresponding namespace prefixes in element and attribute names.
 *
 * @since Apache Tika 0.10
 */
public class ToXMLContentHandler extends ToTextContentHandler {

    private static class ElementInfo {

        private final ElementInfo parent;

        private final Map<String, String> namespaces;

        public ElementInfo(ElementInfo parent, Map<String, String> namespaces) {
            this.parent = parent;
            if (namespaces.isEmpty()) {
                this.namespaces = Collections.emptyMap();
            } else {
                this.namespaces = new HashMap<String, String>(namespaces);
            }
        }

        public String getPrefix(String uri) throws SAXException {
            String prefix = namespaces.get(uri);
            if (prefix != null) {
                return prefix;
            } else if (parent != null) {
                return parent.getPrefix(uri);
            } else if (uri == null || uri.length() == 0) {
                return "";
            } else {
                throw new SAXException("Namespace " + uri + " not declared");
            }
        }

        public String getQName(String uri, String localName)
                throws SAXException {
            String prefix = getPrefix(uri);
            if (prefix.length() > 0) {
                return prefix + ":" + localName;
            } else {
                return localName;
            }
        }

    }

    private final String encoding;

    protected boolean inStartElement = false;

    protected final Map<String, String> namespaces =
        new HashMap<String, String>();

    private ElementInfo currentElement;

    /**
     * Creates an XML serializer that writes to the given byte stream
     * using the given character encoding.
     *
     * @param stream output stream
     * @param encoding output encoding
     * @throws UnsupportedEncodingException if the encoding is unsupported
     */
    public ToXMLContentHandler(OutputStream stream, String encoding)
            throws UnsupportedEncodingException {
        super(stream, encoding);
        this.encoding = encoding;
    }

    public ToXMLContentHandler(String encoding) {
        super();
        this.encoding = encoding;
    }

    public ToXMLContentHandler() {
        super();
        this.encoding = null;
    }

    /**
     * Writes the XML prefix.
     */
    @Override
    public void startDocument() throws SAXException {
        if (encoding != null) {
            write("<?xml version=\"1.0\" encoding=\"");
            write(encoding);
            write("\"?>\n");
        }

        currentElement = null;
        namespaces.clear();
    }

    @Override
    public void startPrefixMapping(String prefix, String uri)
            throws SAXException {
        try {
            if (currentElement != null
                    && prefix.equals(currentElement.getPrefix(uri))) {
                return;
            }
        } catch (SAXException ignore) {
        }
        namespaces.put(uri, prefix);
    }

    @Override
    public void startElement(
            String uri, String localName, String qName, Attributes atts)
            throws SAXException {
        lazyCloseStartElement();

        currentElement = new ElementInfo(currentElement, namespaces);

        write('<');
        write(currentElement.getQName(uri, localName));

        for (int i = 0; i < atts.getLength(); i++) {
            write(' ');
            write(currentElement.getQName(atts.getURI(i), atts.getLocalName(i)));
            write('=');
            write('"');
            char[] ch = atts.getValue(i).toCharArray();
            writeEscaped(ch, 0, ch.length, true);
            write('"');
        }

        for (Map.Entry<String, String> entry : namespaces.entrySet()) {
            write(' ');
            write("xmlns");
            String prefix = entry.getValue();
            if (prefix.length() > 0) {
                write(':');
                write(prefix);
            }
            write('=');
            write('"');
            char[] ch = entry.getKey().toCharArray();
            writeEscaped(ch, 0, ch.length, true);
            write('"');
        }
        namespaces.clear();

        inStartElement = true;
    }

    @Override
    public void endElement(String uri, String localName, String qName)
            throws SAXException {
        if (inStartElement) {
            write(" />");
            inStartElement = false;
        } else {
            write("</");
            write(qName);
            write('>');
        }

        namespaces.clear();

        // Reset the position in the tree, to avoid endless stack overflow
        // chains (see TIKA-1070)
        currentElement = currentElement.parent;
    }

    @Override
    public void characters(char[] ch, int start, int length)
            throws SAXException {
        lazyCloseStartElement();
        writeEscaped(ch, start, start + length, false);
    }

    private void lazyCloseStartElement() throws SAXException {
        if (inStartElement) {
            write('>');
            inStartElement = false;
        }
    }

    /**
     * Writes the given character as-is.
     *
     * @param ch character to be written
     * @throws SAXException if the character could not be written
     */
    protected void write(char ch) throws SAXException {
        super.characters(new char[] { ch }, 0, 1);
    }

    /**
     * Writes the given string of character as-is.
     *
     * @param string string of character to be written
     * @throws SAXException if the character string could not be written
     */
    protected void write(String string) throws SAXException {
        super.characters(string.toCharArray(), 0, string.length());
    }

    /**
     * Writes the given characters as-is followed by the given entity.
     *
     * @param ch character array
     * @param from start position in the array
     * @param to end position in the array
     * @param entity entity code
     * @return next position in the array,
     *         after the characters plus one entity
     * @throws SAXException if the characters could not be written
     */
    private int writeCharsAndEntity(char[] ch, int from, int to, String entity)
            throws SAXException {
        super.characters(ch, from, to - from);
        write('&');
        write(entity);
        write(';');
        return to + 1;
    }

    /**
     * Writes the given characters with XML meta characters escaped.
     *
     * @param ch character array
     * @param from start position in the array
     * @param to end position in the array
     * @param attribute whether the characters should be escaped as
     *                  an attribute value or normal character content
     * @throws SAXException if the characters could not be written
     */
    private void writeEscaped(char[] ch, int from, int to, boolean attribute)
            throws SAXException {
        int pos = from;
        while (pos < to) {
            if (ch[pos] == '<') {
                from = pos = writeCharsAndEntity(ch, from, pos, "lt");
            } else if (ch[pos] == '>') {
                from = pos = writeCharsAndEntity(ch, from, pos, "gt");
            } else if (ch[pos] == '&') {
                from = pos = writeCharsAndEntity(ch, from, pos, "amp");
            } else if (attribute && ch[pos] == '"') {
                from = pos = writeCharsAndEntity(ch, from, pos, "quot");
            } else {
                pos++;
            }
        }
        super.characters(ch, from, to - from);
    }

}
"
tika-core/src/main/java/org/apache/tika/sax/WriteOutContentHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax;

import java.io.OutputStream;
import java.io.OutputStreamWriter;
import java.io.Serializable;
import java.io.StringWriter;
import java.io.Writer;
import java.nio.charset.Charset;
import java.util.UUID;

import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * SAX event handler that writes content up to an optional write
 * limit out to a character stream or other decorated handler.
 */
public class WriteOutContentHandler extends ContentHandlerDecorator {

    /**
     * The unique tag associated with exceptions from stream.
     */
    private final Serializable tag = UUID.randomUUID();

    /**
     * The maximum number of characters to write to the character stream.
     * Set to -1 for no limit.
     */
    private final int writeLimit;

    /**
     * Number of characters written so far.
     */
    private int writeCount = 0;

    /**
     * Creates a content handler that writes content up to the given
     * write limit to the given content handler.
     *
     * @since Apache Tika 0.10
     * @param handler content handler to be decorated
     * @param writeLimit write limit
     */
    public WriteOutContentHandler(ContentHandler handler, int writeLimit) {
        super(handler);
        this.writeLimit = writeLimit;
    }

    /**
     * Creates a content handler that writes content up to the given
     * write limit to the given character stream.
     *
     * @since Apache Tika 0.10
     * @param writer character stream
     * @param writeLimit write limit
     */
    public WriteOutContentHandler(Writer writer, int writeLimit) {
        this(new ToTextContentHandler(writer), writeLimit);
    }

    /**
     * Creates a content handler that writes character events to
     * the given writer.
     *
     * @param writer writer
     */
    public WriteOutContentHandler(Writer writer) {
        this(writer, -1);
    }

    /**
     * Creates a content handler that writes character events to
     * the given output stream using the default encoding.
     *
     * @param stream output stream
     */
    public WriteOutContentHandler(OutputStream stream) {
        this(new OutputStreamWriter(stream, Charset.defaultCharset()));
    }

    /**
     * Creates a content handler that writes character events
     * to an internal string buffer. Use the {@link #toString()}
     * method to access the collected character content.
     * <p>
     * The internal string buffer is bounded at the given number of characters.
     * If this write limit is reached, then a {@link SAXException} is thrown.
     * The {@link #isWriteLimitReached(Throwable)} method can be used to
     * detect this case.
     *
     * @since Apache Tika 0.7
     * @param writeLimit maximum number of characters to include in the string,
     *                   or -1 to disable the write limit
     */
    public WriteOutContentHandler(int writeLimit) {
        this(new StringWriter(), writeLimit);
    }

    /**
     * Creates a content handler that writes character events
     * to an internal string buffer. Use the {@link #toString()}
     * method to access the collected character content.
     * <p>
     * The internal string buffer is bounded at 100k characters. If this
     * write limit is reached, then a {@link SAXException} is thrown. The
     * {@link #isWriteLimitReached(Throwable)} method can be used to detect
     * this case.
     */
    public WriteOutContentHandler() {
        this(100 * 1000);
    }

    /**
     * Writes the given characters to the given character stream.
     */
    @Override
    public void characters(char[] ch, int start, int length)
            throws SAXException {
        if (writeLimit == -1 || writeCount + length <= writeLimit) {
            super.characters(ch, start, length);
            writeCount += length;
        } else {
            super.characters(ch, start, writeLimit - writeCount);
            writeCount = writeLimit;
            throw new WriteLimitReachedException(
                    "Your document contained more than " + writeLimit
                    + " characters, and so your requested limit has been"
                    + " reached. To receive the full text of the document,"
                    + " increase your limit. (Text up to the limit is"
                    + " however available).", tag);
        }
    }

    @Override
    public void ignorableWhitespace(char[] ch, int start, int length)
            throws SAXException {
        if (writeLimit == -1 || writeCount + length <= writeLimit) {
            super.ignorableWhitespace(ch, start, length);
            writeCount += length;
        } else {
            super.ignorableWhitespace(ch, start, writeLimit - writeCount);
            writeCount = writeLimit;
            throw new WriteLimitReachedException(
                    "Your document contained more than " + writeLimit
                    + " characters, and so your requested limit has been"
                    + " reached. To receive the full text of the document,"
                    + " increase your limit. (Text up to the limit is"
                    + " however available).", tag);
        }
    }

    /**
     * Checks whether the given exception (or any of it's root causes) was
     * thrown by this handler as a signal of reaching the write limit.
     *
     * @since Apache Tika 0.7
     * @param t throwable
     * @return <code>true</code> if the write limit was reached,
     *         <code>false</code> otherwise
     */
    public boolean isWriteLimitReached(Throwable t) {
        if (t instanceof WriteLimitReachedException) {
            return tag.equals(((WriteLimitReachedException) t).tag);
        } else {
            return t.getCause() != null && isWriteLimitReached(t.getCause());
        }
    }

    /**
     * The exception used as a signal when the write limit has been reached.
     */
    private static class WriteLimitReachedException extends SAXException {

        /** Serial version UID */
        private static final long serialVersionUID = -1850581945459429943L;

        /** Serializable tag of the handler that caused this exception */
        private final Serializable tag;

        public WriteLimitReachedException(String message, Serializable tag) {
           super(message);
           this.tag = tag;
        }

    }

}
"
tika-core/src/main/java/org/apache/tika/sax/XHTMLContentHandler.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax;

import java.util.Arrays;
import java.util.Collections;
import java.util.HashSet;
import java.util.Set;

import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.TikaCoreProperties;
import org.xml.sax.Attributes;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.AttributesImpl;

/**
 * Content handler decorator that simplifies the task of producing XHTML
 * events for Tika content parsers.
 */
public class XHTMLContentHandler extends SafeContentHandler {

    /**
     * The XHTML namespace URI
     */
    public static final String XHTML = "http://www.w3.org/1999/xhtml";

    /**
     * The newline character that gets inserted after block elements.
     */
    private static final char[] NL = new char[] { '\n' };

    /**
     * The tab character gets inserted before table cells and list items.
     */
    private static final char[] TAB = new char[] { '\t' };

    /**
     * The elements that are in the <head> section.
     */
    private static final Set<String> HEAD =
        unmodifiableSet("title", "link", "base", "meta");

    /**
     * The elements that are automatically emitted by lazyStartHead, so
     * skip them if they get sent to startElement/endElement by mistake.
     */
    private static final Set<String> AUTO =
        unmodifiableSet("html", "head", "body", "frameset");

    /**
     * The elements that get prepended with the {@link #TAB} character.
     */
    private static final Set<String> INDENT =
        unmodifiableSet("li", "dd", "dt", "td", "th", "frame");

    /**
     * The elements that get appended with the {@link #NL} character.
     */
    public static final Set<String> ENDLINE = unmodifiableSet(
            "p", "h1", "h2", "h3", "h4", "h5", "h6", "div", "ul", "ol", "dl",
            "pre", "hr", "blockquote", "address", "fieldset", "table", "form",
            "noscript", "li", "dt", "dd", "noframes", "br", "tr", "select", "option");

    private static final Attributes EMPTY_ATTRIBUTES = new AttributesImpl();

    private static Set<String> unmodifiableSet(String... elements) {
        return Collections.unmodifiableSet(
                new HashSet<String>(Arrays.asList(elements)));
    }

    /**
     * Metadata associated with the document. Used to fill in the
     * &lt;head/&gt; section.
     */
    private final Metadata metadata;

    /**
     * Flag to indicate whether the document has been started.
     */
    private boolean documentStarted = false;
    
    /**
     * Flags to indicate whether the document head element has been started/ended.
     */
    private boolean headStarted = false;
    private boolean headEnded = false;
    private boolean useFrameset = false;
    
    public XHTMLContentHandler(ContentHandler handler, Metadata metadata) {
        super(handler);
        this.metadata = metadata;
    }

    /**
     * Starts an XHTML document by setting up the namespace mappings 
     * when called for the first time.
     * The standard XHTML prefix is generated lazily when the first
     * element is started.
     */
    @Override
    public void startDocument() throws SAXException {
    	if(!documentStarted){
    		documentStarted = true;
            super.startDocument();
            startPrefixMapping("", XHTML);
    	}
    }

    /**
     * Generates the following XHTML prefix when called for the first time:
     * <pre>
     * &lt;html&gt;
     *   &lt;head&gt;
     *     &lt;title&gt;...&lt;/title&gt;
     *   &lt;/head&gt;
     *   &lt;body&gt;
     * </pre>
     */
    private void lazyStartHead() throws SAXException {
        if (!headStarted) {
            headStarted = true;
            
            // Call directly, so we don't go through our startElement(), which will
            // ignore these elements.
            super.startElement(XHTML, "html", "html", EMPTY_ATTRIBUTES);
            newline();
            super.startElement(XHTML, "head", "head", EMPTY_ATTRIBUTES);
            newline();
        }
    }

    /**
     * Generates the following XHTML prefix when called for the first time:
     * <pre>
     * &lt;html&gt;
     *   &lt;head&gt;
     *     &lt;title&gt;...&lt;/title&gt;
     *   &lt;/head&gt;
     *   &lt;body&gt; (or &lt;frameset&gt;
     * </pre>
     */
    private void lazyEndHead(boolean isFrameset) throws SAXException {
        lazyStartHead();
        
        if (!headEnded) {
            headEnded = true;
            useFrameset = isFrameset;
            
            // TIKA-478: Emit all metadata values (other than title). We have to call
            // startElement() and characters() directly to avoid recursive problems.
            for (String name : metadata.names()) {
                if (name.equals("title")) {
                    continue;
                }
                
                for (String value : metadata.getValues(name)) {
                    // Putting null values into attributes causes problems, but is
                    // allowed by Metadata, so guard against that.
                    if (value != null) {
                        AttributesImpl attributes = new AttributesImpl();
                        attributes.addAttribute("", "name", "name", "CDATA", name);
                        attributes.addAttribute("", "content", "content", "CDATA", value);
                        super.startElement(XHTML, "meta", "meta", attributes);
                        super.endElement(XHTML, "meta", "meta");
                        newline();
                    }
                }
            }
            
            super.startElement(XHTML, "title", "title", EMPTY_ATTRIBUTES);
            String title = metadata.get(TikaCoreProperties.TITLE);
            if (title != null && title.length() > 0) {
                char[] titleChars = title.toCharArray();
                super.characters(titleChars, 0, titleChars.length);
            } else {
                // TIKA-725: Prefer <title></title> over <title/>
                super.characters(new char[0], 0, 0);
            }
            super.endElement(XHTML, "title", "title");
            newline();
            
            super.endElement(XHTML, "head", "head");
            newline();
            
            if (useFrameset) {
                super.startElement(XHTML, "frameset", "frameset", EMPTY_ATTRIBUTES);
            } else {
                super.startElement(XHTML, "body", "body", EMPTY_ATTRIBUTES);
            }
        }
    }

    /**
     * Ends the XHTML document by writing the following footer and
     * clearing the namespace mappings:
     * <pre>
     *   &lt;/body&gt;
     * &lt;/html&gt;
     * </pre>
     */
    @Override
    public void endDocument() throws SAXException {
        lazyEndHead(useFrameset);
        
        if (useFrameset) {
            super.endElement(XHTML, "frameset", "frameset");
        } else {
            super.endElement(XHTML, "body", "body");
        }
        
        super.endElement(XHTML, "html", "html");
        
        endPrefixMapping("");
        super.endDocument();
    }

    /**
     * Starts the given element. Table cells and list items are automatically
     * indented by emitting a tab character as ignorable whitespace.
     */
    @Override
    public void startElement(
            String uri, String local, String name, Attributes attributes)
            throws SAXException {
        
        if (name.equals("frameset")) {
            lazyEndHead(true);
        } else if (!AUTO.contains(name)) {
            if (HEAD.contains(name)) {
                lazyStartHead();
            } else {
                lazyEndHead(false);
            }

            if (XHTML.equals(uri) && INDENT.contains(name)) {
                ignorableWhitespace(TAB, 0, TAB.length);
            }
            
            super.startElement(uri, local, name, attributes);
        }
    }

    /**
     * Ends the given element. Block elements are automatically followed
     * by a newline character.
     */
    @Override
    public void endElement(String uri, String local, String name) throws SAXException {
        if (!AUTO.contains(name)) {
            super.endElement(uri, local, name);
            if (XHTML.equals(uri) && ENDLINE.contains(name)) {
                newline();
            }
        }
    }

    /**
     * @see <a href="https://issues.apache.org/jira/browse/TIKA-210">TIKA-210</a>
     */
    @Override
    public void characters(char[] ch, int start, int length) throws SAXException {
        lazyEndHead(useFrameset);
        super.characters(ch, start, length);
    }

    //------------------------------------------< public convenience methods >

    public void startElement(String name) throws SAXException {
        startElement(XHTML, name, name, EMPTY_ATTRIBUTES);
    }

    public void startElement(String name, String attribute, String value)
            throws SAXException {
        AttributesImpl attributes = new AttributesImpl();
        attributes.addAttribute("", attribute, attribute, "CDATA", value);
        startElement(XHTML, name, name, attributes);
    }

    public void startElement(String name, AttributesImpl attributes)
            throws SAXException {
        startElement(XHTML, name, name, attributes);
    }

    public void endElement(String name) throws SAXException {
        endElement(XHTML, name, name);
    }

    public void characters(String characters) throws SAXException {
        if (characters != null && characters.length() > 0) {
            characters(characters.toCharArray(), 0, characters.length());
        }
    }

    public void newline() throws SAXException {
        ignorableWhitespace(NL, 0, NL.length);
    }

    /**
     * Emits an XHTML element with the given text content. If the given
     * text value is null or empty, then the element is not written.
     *
     * @param name XHTML element name
     * @param value element value, possibly <code>null</code>
     * @throws SAXException if the content element could not be written
     */
    public void element(String name, String value) throws SAXException {
        if (value != null && value.length() > 0) {
            startElement(name);
            characters(value);
            endElement(name);
        }
    }

}
"
tika-core/src/main/java/org/apache/tika/sax/XMPContentHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax;

import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.Property;
import org.xml.sax.Attributes;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.AttributesImpl;

/**
 * Content handler decorator that simplifies the task of producing XMP output.
 *
 * @since Apache Tika 1.0
 */
public class XMPContentHandler extends SafeContentHandler {

    /**
     * The RDF namespace URI
     */
    public static final String RDF =
            "http://www.w3.org/1999/02/22-rdf-syntax-ns#";

    /**
     * The XMP namespace URI
     */
    public static final String XMP =
            "http://ns.adobe.com/xap/1.0/";

    private static final Attributes EMPTY_ATTRIBUTES = new AttributesImpl();

    public XMPContentHandler(ContentHandler handler) {
        super(handler);
    }

    /**
     * Starts an XMP document by setting up the namespace mappings and
     * writing out the following header:
     * <pre>
     * &lt;rdf:RDF&gt;
     * </pre>
     */
    @Override
    public void startDocument() throws SAXException {
        super.startDocument();

        startPrefixMapping("rdf", RDF);
        startPrefixMapping("xmp", XMP);

        startElement(RDF, "RDF", "rdf:RDF", EMPTY_ATTRIBUTES);
    }

    /**
     * Ends the XMP document by writing the following footer and
     * clearing the namespace mappings:
     * <pre>
     * &lt;/rdf:RDF&gt;
     * </pre>
     */
    @Override
    public void endDocument() throws SAXException {
        endElement(RDF, "RDF", "rdf:RDF");

        endPrefixMapping("xmp");
        endPrefixMapping("rdf");

        super.endDocument();
    }

    //------------------------------------------< public convenience methods >

    private String prefix = null;

    private String uri = null;

    public void startDescription(String about, String prefix, String uri)
            throws SAXException {
        this.prefix = prefix;
        this.uri = uri;

        startPrefixMapping(prefix, uri);
        AttributesImpl attributes = new AttributesImpl();
        attributes.addAttribute(RDF, "about", "rdf:about", "CDATA", about);
        startElement(RDF, "Description", "rdf:Description", attributes);
    }

    public void endDescription() throws SAXException {
        endElement(RDF, "Description", "rdf:Description");
        endPrefixMapping(prefix);

        this.uri = null;
        this.prefix = null;
    }

    public void property(String name, String value) throws SAXException {
        String qname = prefix + ":" + name;
        startElement(uri, name, qname, EMPTY_ATTRIBUTES);
        characters(value.toCharArray(), 0, value.length());
        endElement(uri, name, qname);
    }

    public void metadata(Metadata metadata) throws SAXException {
        description(metadata, "xmp", XMP);
        description(metadata, "dc", "http://purl.org/dc/elements/1.1/");
        description(metadata, "xmpTPg", "http://ns.adobe.com/xap/1.0/t/pg/");
        description(metadata, "xmpRigths", "http://ns.adobe.com/xap/1.0/rights/");
        description(metadata, "xmpMM", "http://ns.adobe.com/xap/1.0/mm/");
        description(metadata, "xmpidq", "http://ns.adobe.com/xmp/identifier/qual/1.0/");
        description(metadata, "xmpBJ", "http://ns.adobe.com/xap/1.0/bj/");
        description(metadata, "xmpDM", "http://ns.adobe.com/xmp/1.0/DynamicMedia/");
        description(metadata, "pdf", "http://ns.adobe.com/pdf/1.3/");
        description(metadata, "photoshop", "s http://ns.adobe.com/photoshop/1.0/");
        description(metadata, "crs", "http://ns.adobe.com/camera-raw-settings/1.0/");
        description(metadata, "tiff", "http://ns.adobe.com/tiff/1.0/");
        description(metadata, "exif", "http://ns.adobe.com/exif/1.0/");
        description(metadata, "aux", "http://ns.adobe.com/exif/1.0/aux/");
    }

    private void description(Metadata metadata, String prefix, String uri)
            throws SAXException {
        int count = 0;
        for (Property property : Property.getProperties(prefix)) {
            String value = metadata.get(property);
            if (value != null) {
                if (count++ == 0) {
                    startDescription("", prefix, uri);
                }
                property(property.getName().substring(prefix.length() + 1), value);
            }
        }

        if (count > 0) {
            endDescription();
        }
    }

}
"
tika-core/src/main/java/org/apache/tika/sax/xpath/AttributeMatcher.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax.xpath;

/**
 * Final evaluation state of a <code>.../@*</code> XPath expression.
 * Matches all attributes of the current element.
 */
public class AttributeMatcher extends Matcher {

    public static final Matcher INSTANCE = new AttributeMatcher();

    public boolean matchesAttribute(String namespace, String name) {
        return true;
    }

}
"
tika-core/src/main/java/org/apache/tika/sax/xpath/ChildMatcher.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax.xpath;

/**
 * Intermediate evaluation state of a <code>.../*...</code> XPath expression.
 * Matches nothing, but specifies the evaluation state for all child elements.
 */
public class ChildMatcher extends Matcher {

    private final Matcher then;

    public ChildMatcher(Matcher then) {
        this.then = then;
    }

    public Matcher descend(String namespace, String name) {
        return then;
    }

}
"
tika-core/src/main/java/org/apache/tika/sax/xpath/CompositeMatcher.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax.xpath;

/**
 * Composite XPath evaluation state. Used when XPath evaluation results
 * in two or more branches of independent evaluation states.
 */
public class CompositeMatcher extends Matcher {

    private final Matcher a;

    private final Matcher b;

    public CompositeMatcher(Matcher a, Matcher b) {
        this.a = a;
        this.b = b;
    }

    public Matcher descend(String namespace, String name) {
        Matcher a = this.a.descend(namespace, name);
        Matcher b = this.b.descend(namespace, name);
        if (a == FAIL) {
            return b;
        } else if (b == FAIL) {
            return a;
        } else if (this.a == a && this.b == b) {
            return this;
        } else {
            return new CompositeMatcher(a, b);
        }
    }

    public boolean matchesElement() {
        return a.matchesElement() || b.matchesElement();
    }

    public boolean matchesAttribute(String namespace, String name) {
        return a.matchesAttribute(namespace, name)
            || b.matchesAttribute(namespace, name);
    }

    public boolean matchesText() {
        return a.matchesText() || b.matchesText();
    }

}
"
tika-core/src/main/java/org/apache/tika/sax/xpath/ElementMatcher.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax.xpath;

/**
 * Final evaluation state of an XPath expression that targets an element.
 * Matches the current element.
 */
public class ElementMatcher extends Matcher {

    public static final Matcher INSTANCE = new ElementMatcher();

    public boolean matchesElement() {
        return true;
    }

}
"
tika-core/src/main/java/org/apache/tika/sax/xpath/Matcher.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax.xpath;

/**
 * XPath element matcher. A matcher instance encapsulates a specific
 * state in XPath evaluation.
 */
public class Matcher {

    /**
     * State of a failed XPath evaluation, where nothing is matched.
     * This matcher instance is used as a sentinel object whenever an
     * XPath evaluation branch fails.
     */
    public static final Matcher FAIL = new Matcher();

    /**
     * Returns the XPath evaluation state that results from descending
     * to a child element with the given name.
     *
     * @param namespace element namespace or <code>null</code>
     * @param name element name
     * @return next XPath evaluation state
     */
    public Matcher descend(String namespace, String name) {
        return FAIL;
    }

    /**
     * Returns <code>true</code> if the XPath expression matches
     * the element associated with this evaluation state.
     *
     * @return XPath evaluation state for this element
     */
    public boolean matchesElement() {
        return false;
    }

    /**
     * Returns <code>true</code> if the XPath expression matches the named
     * attribute of the element associated with this evaluation state.
     *
     * @param namespace attribute namespace or <code>null</code>
     * @param name attribute name
     * @return XPath evaluation state for named attribute of this element
     */
    public boolean matchesAttribute(String namespace, String name) {
        return false;
    }

    /**
     * Returns <code>true</code> if the XPath expression matches all text
     * nodes whose parent is the element associated with this evaluation
     * state.
     *
     * @return XPath evaluation state for text children of this element
     */
    public boolean matchesText() {
        return false;
    }

}
"
tika-core/src/main/java/org/apache/tika/sax/xpath/MatchingContentHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax.xpath;

import java.util.LinkedList;

import org.apache.tika.sax.ContentHandlerDecorator;
import org.xml.sax.Attributes;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.AttributesImpl;

/**
 * Content handler decorator that only passes the elements, attributes,
 * and text nodes that match the given XPath expression.
 */
public class MatchingContentHandler extends ContentHandlerDecorator {

    private final LinkedList<Matcher> matchers = new LinkedList<Matcher>();

    private Matcher matcher;

    public MatchingContentHandler(ContentHandler delegate, Matcher matcher) {
        super(delegate);
        this.matcher = matcher;
    }

    public void startElement(
            String uri, String localName, String name, Attributes attributes)
            throws SAXException {
        matchers.addFirst(matcher);
        matcher = matcher.descend(uri, localName);

        AttributesImpl matches = new AttributesImpl();
        for (int i = 0; i < attributes.getLength(); i++) {
            String attributeURI = attributes.getURI(i);
            String attributeName = attributes.getLocalName(i);
            if (matcher.matchesAttribute(attributeURI, attributeName)) {
                matches.addAttribute(
                        attributeURI, attributeName, attributes.getQName(i),
                        attributes.getType(i), attributes.getValue(i));
            }
        }

        if (matcher.matchesElement() || matches.getLength() > 0) {
            super.startElement(uri, localName, name, matches);
            if (!matcher.matchesElement()) {
                // Force the matcher to match the current element, so the
                // endElement method knows to emit the correct event
                matcher =
                    new CompositeMatcher(matcher, ElementMatcher.INSTANCE);
            }
        }
    }

    public void endElement(String uri, String localName, String name)
            throws SAXException {
        if (matcher.matchesElement()) {
            super.endElement(uri, localName, name);
        }
        // Sometimes tagsoup returns double end tags, so the stack might
        // be empty! TODO: Remove this when the tagsoup problem is fixed.
        if (!matchers.isEmpty()) {
            matcher = matchers.removeFirst();
        }
    }

    public void characters(char[] ch, int start, int length)
            throws SAXException {
        if (matcher.matchesText()) {
            super.characters(ch, start, length);
        }
    }

    public void ignorableWhitespace(char[] ch, int start, int length)
            throws SAXException {
        if (matcher.matchesText()) {
            super.ignorableWhitespace(ch, start, length);
        }
    }

    public void processingInstruction(String target, String data) {
        // TODO: Support for matching processing instructions
    }

    public void skippedEntity(String name) throws SAXException {
        // TODO: Can skipped entities refer to more than text?
        if (matcher.matchesText()) {
            super.skippedEntity(name);
        }
    }

}
"
tika-core/src/main/java/org/apache/tika/sax/xpath/NamedAttributeMatcher.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax.xpath;

/**
 * Final evaluation state of a <code>.../@name</code> XPath expression.
 * Matches the named attributes of the current element.
 */
public class NamedAttributeMatcher extends Matcher {

    private final String namespace;

    private final String name;

    public NamedAttributeMatcher(String namespace, String name) {
        this.namespace = namespace;
        this.name = name;
    }

    public boolean matchesAttribute(String namespace, String name) {
        return equals(namespace, this.namespace) && name.equals(this.name);
    }

    private static boolean equals(String a, String b) {
        return (a == null) ? (b == null) : a.equals(b);
    }

}
"
tika-core/src/main/java/org/apache/tika/sax/xpath/NamedElementMatcher.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax.xpath;

/**
 * Intermediate evaluation state of a <code>.../name...</code> XPath
 * expression. Matches nothing, but specifies the evaluation state
 * for the child elements with the given name.
 */
public class NamedElementMatcher extends ChildMatcher {

    private final String namespace;

    private final String name;

    protected NamedElementMatcher(String namespace, String name, Matcher then) {
        super(then);
        this.namespace = namespace;
        this.name = name;
    }

    public Matcher descend(String namespace, String name) {
        if (equals(namespace, this.namespace) && name.equals(this.name)) {
            return super.descend(namespace, name);
        } else {
            return FAIL;
        }
    }

    private static boolean equals(String a, String b) {
        return (a == null) ? (b == null) : a.equals(b);
    }

}
"
tika-core/src/main/java/org/apache/tika/sax/xpath/NodeMatcher.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax.xpath;

/**
 * Final evaluation state of a <code>.../node()</code> XPath expression.
 * Matches all elements, attributes, and text.
 */
public class NodeMatcher extends Matcher {

    public static final Matcher INSTANCE = new NodeMatcher();

    @Override
    public boolean matchesElement() {
        return true;
    }

    @Override
    public boolean matchesAttribute(String namespace, String name) {
        return true;
    }

    @Override
    public boolean matchesText() {
        return true;
    }

}
"
tika-core/src/main/java/org/apache/tika/sax/xpath/package-info.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * XPath utilities
 */
@aQute.bnd.annotation.Version("1.0.0")
package org.apache.tika.sax.xpath;
"
tika-core/src/main/java/org/apache/tika/sax/xpath/SubtreeMatcher.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax.xpath;

/**
 * Evaluation state of a <code>...//...</code> XPath expression. Applies the
 * contained evaluation state to the current element and all its descendants.
 */
public class SubtreeMatcher extends Matcher {

    private final Matcher then;

    public SubtreeMatcher(Matcher then) {
        this.then = then;
    }

    @Override
    public Matcher descend(String namespace, String name) {
        Matcher next = then.descend(namespace, name);
        if (next == FAIL || next == then) {
            return this;
        } else {
            return new CompositeMatcher(next, this);
        }
    }

    @Override
    public boolean matchesElement() {
        return then.matchesElement();
    }

    @Override
    public boolean matchesAttribute(String namespace, String name) {
        return then.matchesAttribute(namespace, name);
    }

    @Override
    public boolean matchesText() {
        return then.matchesText();
    }

}
"
tika-core/src/main/java/org/apache/tika/sax/xpath/TextMatcher.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax.xpath;

/**
 * Final evaluation state of a <code>.../text()</code> XPath expression.
 * Matches all text children of the current element.
 */
public class TextMatcher extends Matcher {

    public static final Matcher INSTANCE = new TextMatcher();

    public boolean matchesText() {
        return true;
    }

}
"
tika-core/src/main/java/org/apache/tika/sax/xpath/XPathParser.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.sax.xpath;

import java.util.HashMap;
import java.util.Map;

/**
 * Parser for a very simple XPath subset. Only the following XPath constructs
 * (with namespaces) are supported:
 * <ul>
 *   <li><code>.../node()</code></li>
 *   <li><code>.../text()</code></li>
 *   <li><code>.../@*</code></li>
 *   <li><code>.../@name</code></li>
 *   <li><code>.../*...</code></li>
 *   <li><code>.../name...</code></li>
 *   <li><code>...//*...</code></li>
 *   <li><code>...//name...</code></li>
 * </ul>
 * <p>
 * In addition the non-abbreviated <code>.../descendant::node()</code>
 * construct can be used for cases where the descendant-or-self axis
 * used by the <code>...//node()</code> construct is not appropriate.
 */
public class XPathParser {

    private final Map<String, String> prefixes = new HashMap<String, String>();

    public XPathParser() {
    }

    public XPathParser(String prefix, String namespace) {
        addPrefix(prefix, namespace);
    }

    public void addPrefix(String prefix, String namespace) {
        prefixes.put(prefix, namespace);
    }

    /**
     * Parses the given simple XPath expression to an evaluation state
     * initialized at the document node. Invalid expressions are not flagged
     * as errors, they just result in a failing evaluation state.
     *
     * @param xpath simple XPath expression
     * @return XPath evaluation state
     */
    public Matcher parse(String xpath) {
        if (xpath.equals("/text()")) {
            return TextMatcher.INSTANCE;
        } else if (xpath.equals("/node()")) {
            return NodeMatcher.INSTANCE;
        } else if (xpath.equals("/descendant::node()")
                || xpath.equals("/descendant:node()")) { // for compatibility
            return new CompositeMatcher(
                    TextMatcher.INSTANCE,
                    new ChildMatcher(new SubtreeMatcher(NodeMatcher.INSTANCE)));
        } else if (xpath.equals("/@*")) {
            return AttributeMatcher.INSTANCE;
        } else if (xpath.length() == 0) {
            return ElementMatcher.INSTANCE;
        } else if (xpath.startsWith("/@")) {
            String name = xpath.substring(2);
            String prefix = null;
            int colon = name.indexOf(':');
            if (colon != -1) {
                prefix = name.substring(0, colon);
                name = name.substring(colon + 1);
            }
            if (prefixes.containsKey(prefix)) {
                return new NamedAttributeMatcher(prefixes.get(prefix), name);
            } else {
                return Matcher.FAIL;
            }
        } else if (xpath.startsWith("/*")) {
            return new ChildMatcher(parse(xpath.substring(2)));
        } else if (xpath.startsWith("///")) {
            return Matcher.FAIL;
        } else if (xpath.startsWith("//")) {
            return new SubtreeMatcher(parse(xpath.substring(1)));
        } else if (xpath.startsWith("/")) {
            int slash = xpath.indexOf('/', 1);
            if (slash == -1) {
                slash = xpath.length();
            }
            String name = xpath.substring(1, slash);
            String prefix = null;
            int colon = name.indexOf(':');
            if (colon != -1) {
                prefix = name.substring(0, colon);
                name = name.substring(colon + 1);
            }
            if (prefixes.containsKey(prefix)) {
                return new NamedElementMatcher(
                        prefixes.get(prefix), name,
                        parse(xpath.substring(slash)));
            } else {
                return Matcher.FAIL;
            }
        } else {
            return Matcher.FAIL;
        }
    }

}
"
tika-core/src/main/java/org/apache/tika/utils/CharsetUtils.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.utils;

import static java.util.Locale.ENGLISH;

import java.lang.reflect.Method;
import java.nio.charset.Charset;
import java.nio.charset.IllegalCharsetNameException;
import java.util.HashMap;
import java.util.Locale;
import java.util.Map;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

public class CharsetUtils {

    private static final Pattern CHARSET_NAME_PATTERN =
            Pattern.compile("[ \\\"]*([^ >,;\\\"]+).*");

    private static final Pattern ISO_NAME_PATTERN =
            Pattern.compile(".*8859-(\\d+)");

    private static final Pattern CP_NAME_PATTERN =
            Pattern.compile("cp-(\\d+)");

    private static final Pattern WIN_NAME_PATTERN =
            Pattern.compile("win-?(\\d+)");

    private static final Map<String, Charset> COMMON_CHARSETS =
            new HashMap<String, Charset>();

    private static Method getCharsetICU = null;
    private static Method isSupportedICU = null;

    private static Map<String, Charset> initCommonCharsets(String... names) {
        Map<String, Charset> charsets = new HashMap<String, Charset>();
        for (String name : names) {
            try {
                Charset charset = Charset.forName(name);
                COMMON_CHARSETS.put(name.toLowerCase(ENGLISH), charset);
                for (String alias : charset.aliases()) {
                    COMMON_CHARSETS.put(alias.toLowerCase(ENGLISH), charset);
                }
            } catch (Exception e) {
                // ignore
            }
        }
        return charsets;
    }

    static {
        initCommonCharsets(
                "Big5",
                "EUC-JP", "EUC-KR", "x-EUC-TW",
                "GB18030",
                "IBM855", "IBM866",
                "ISO-2022-CN", "ISO-2022-JP", "ISO-2022-KR",
                "ISO-8859-1", "ISO-8859-2", "ISO-8859-3", "ISO-8859-4",
                "ISO-8859-5", "ISO-8859-6", "ISO-8859-7", "ISO-8859-8",
                "ISO-8859-9", "ISO-8859-11", "ISO-8859-13", "ISO-8859-15",
                "KOI8-R",
                "x-MacCyrillic",
                "SHIFT_JIS",
                "UTF-8", "UTF-16BE", "UTF-16LE",
                "windows-1251", "windows-1252", "windows-1253", "windows-1255");

        // Common aliases/typos not included in standard charset definitions
        COMMON_CHARSETS.put("iso-8851-1", COMMON_CHARSETS.get("iso-8859-1"));
        COMMON_CHARSETS.put("windows", COMMON_CHARSETS.get("windows-1252"));
        COMMON_CHARSETS.put("koi8r", COMMON_CHARSETS.get("koi8-r"));

        // See if we can load the icu4j CharsetICU class
        Class<?> icuCharset = null;
        try  {
            icuCharset = CharsetUtils.class.getClassLoader().loadClass(
                    "com.ibm.icu.charset.CharsetICU");
        }  catch (ClassNotFoundException e) {
        }
        if (icuCharset != null) {
            try {
                getCharsetICU = icuCharset.getMethod("forNameICU", String.class);
            } catch (Throwable t) {
                throw new RuntimeException(t);
            }
            try {
                isSupportedICU = icuCharset.getMethod("isSupported", String.class);
            } catch (Throwable t) {
            }
            // TODO: would be nice to somehow log that we
            // successfully found ICU
        }
    }

    /**
     * Safely return whether <charsetName> is supported, without throwing exceptions
     * 
     * @param charsetName Name of charset (can be null)
     * @return true if the character set is supported
     */
    public static boolean isSupported(String charsetName) {
        try {
            if (isSupportedICU != null && ((Boolean) isSupportedICU.invoke(null, charsetName)).booleanValue()) {
                return true;
            }
            return Charset.isSupported(charsetName);
        } catch (IllegalCharsetNameException e) {
            return false;
        } catch (IllegalArgumentException e) {
            // null, for example
            return false;
        } catch (Exception e) {
            // Unexpected exception, what to do?
            return false;
        }
    }

    /**
     * Handle various common charset name errors, and return something
     * that will be considered valid (and is normalized)
     * 
     * @param charsetName name of charset to process
     * @return potentially remapped/cleaned up version of charset name
     */
    public static String clean(String charsetName) {
        try {
            return forName(charsetName).name();
        } catch (Exception e) {
            return null;
        }
    }

    /** Returns Charset impl, if one exists.  This method
     *  optionally uses ICU4J's CharsetICU.forNameICU,
     *  if it is found on the classpath, else only uses
     *  JDK's builtin Charset.forName. */
    public static Charset forName(String name) {
        if (name == null) {
            throw new IllegalArgumentException();
        }

        // Get rid of cruft around names, like <>, trailing commas, etc.
        Matcher m = CHARSET_NAME_PATTERN.matcher(name);
        if (!m.matches()) {
            throw new IllegalCharsetNameException(name);
        }
        name = m.group(1);

        String lower = name.toLowerCase(Locale.ENGLISH);
        Charset charset = COMMON_CHARSETS.get(lower);
        if (charset != null) {
            return charset;
        } else if ("none".equals(lower) || "no".equals(lower)) {
            throw new IllegalCharsetNameException(name);
        } else {
            Matcher iso = ISO_NAME_PATTERN.matcher(lower);
            Matcher cp = CP_NAME_PATTERN.matcher(lower);
            Matcher win = WIN_NAME_PATTERN.matcher(lower);
            if (iso.matches()) {
                // Handle "iso 8859-x" error
                name = "iso-8859-" + iso.group(1);
                charset = COMMON_CHARSETS.get(name);
            } else if (cp.matches()) {
                // Handle "cp-xxx" error
                name = "cp" + cp.group(1);
                charset = COMMON_CHARSETS.get(name);
            } else if (win.matches()) {
                // Handle "winxxx" and "win-xxx" errors
                name = "windows-" + win.group(1);
                charset = COMMON_CHARSETS.get(name);
            }
            if (charset != null) {
                return charset;
            }
        }

        if (getCharsetICU != null) {
            try {
                Charset cs = (Charset) getCharsetICU.invoke(null, name);
                if (cs != null) {
                    return cs;
                }
            } catch (Exception e) {
                // ignore
            }
        }

        return Charset.forName(name);
    }
}
"
tika-core/src/main/java/org/apache/tika/utils/DateUtils.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.utils;

import java.util.Calendar;
import java.util.Date;
import java.util.GregorianCalendar;
import java.util.Locale;
import java.util.TimeZone;

/**
 * Date related utility methods and constants
 */
public class DateUtils {
    /**
     * The UTC time zone. Not sure if {@link TimeZone#getTimeZone(String)}
     * understands "UTC" in all environments, but it'll fall back to GMT
     * in such cases, which is in practice equivalent to UTC.
     */
    public static final TimeZone UTC = TimeZone.getTimeZone("UTC");

    /**
     * Custom time zone used to interpret date values without a time
     * component in a way that most likely falls within the same day
     * regardless of in which time zone it is later interpreted. For
     * example, the "2012-02-17" date would map to "2012-02-17T12:00:00Z"
     * (instead of the default "2012-02-17T00:00:00Z"), which would still
     * map to "2012-02-17" if interpreted in say Pacific time (while the
     * default mapping would result in "2012-02-16" for UTC-8).
     */
    public static final TimeZone MIDDAY = TimeZone.getTimeZone("GMT-12:00");

    /**
     * Returns a ISO 8601 representation of the given date. This method 
     * is thread safe and non-blocking.
     *
     * @see <a href="https://issues.apache.org/jira/browse/TIKA-495">TIKA-495</a>
     * @param date given date
     * @return ISO 8601 date string, including timezone details
     */
    public static String formatDate(Date date) {
        Calendar calendar = GregorianCalendar.getInstance(UTC, Locale.US);
        calendar.setTime(date);
        return doFormatDate(calendar);
    }
    /**
     * Returns a ISO 8601 representation of the given date. This method 
     * is thread safe and non-blocking.
     *
     * @see <a href="https://issues.apache.org/jira/browse/TIKA-495">TIKA-495</a>
     * @param date given date
     * @return ISO 8601 date string, including timezone details
     */
    public static String formatDate(Calendar date) {
        // Explicitly switch it into UTC before formatting 
        date.setTimeZone(UTC);
        return doFormatDate(date);
    }
    /**
     * Returns a ISO 8601 representation of the given date, which is
     *  in an unknown timezone. This method is thread safe and non-blocking.
     *
     * @see <a href="https://issues.apache.org/jira/browse/TIKA-495">TIKA-495</a>
     * @param date given date
     * @return ISO 8601 date string, without timezone details
     */
    public static String formatDateUnknownTimezone(Date date) {
        // Create the Calendar object in the system timezone
        Calendar calendar = GregorianCalendar.getInstance(TimeZone.getDefault(), Locale.US);
        calendar.setTime(date);
        // Have it formatted
        String formatted = formatDate(calendar);
        // Strip the timezone details before returning
        return formatted.substring(0, formatted.length()-1);
    }
    private static String doFormatDate(Calendar calendar) {
        return String.format(
                Locale.ROOT,
                "%04d-%02d-%02dT%02d:%02d:%02dZ",
                calendar.get(Calendar.YEAR),
                calendar.get(Calendar.MONTH) + 1,
                calendar.get(Calendar.DAY_OF_MONTH),
                calendar.get(Calendar.HOUR_OF_DAY),
                calendar.get(Calendar.MINUTE),
                calendar.get(Calendar.SECOND));
    }
}
"
tika-core/src/main/java/org/apache/tika/utils/package-info.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Utilities.
 */
@aQute.bnd.annotation.Version("1.0.0")
package org.apache.tika.utils;
"
tika-core/src/main/java/org/apache/tika/utils/RegexUtils.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.utils;

import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

/**
 * Inspired from Nutch code class OutlinkExtractor. Apply regex to extract
 * content
 * 
 * 
 */
public class RegexUtils {

    /**
     * Regex pattern to get URLs within a plain text.
     * 
     * @see <a
     *      href="http://www.truerwords.net/articles/ut/urlactivation.html">http://www.truerwords.net/articles/ut/urlactivation.html
     *      </a>
     */
    private static final String LINKS_REGEX =
        "([A-Za-z][A-Za-z0-9+.-]{1,120}:"
        + "[A-Za-z0-9/](([A-Za-z0-9$_.+!*,;/?:@&~=-])|%[A-Fa-f0-9]{2}){1,333}"
        + "(#([a-zA-Z0-9][a-zA-Z0-9$_.+!*,;/?:@&~=%-]{0,1000}))?)";
    
    private static final Pattern LINKS_PATTERN = Pattern.compile(LINKS_REGEX, Pattern.CASE_INSENSITIVE + Pattern.MULTILINE);

    /**
     * Extract urls from plain text.
     *
     * @param content The plain text content to examine
     * @return List of urls within found in the plain text
     */
    public static List<String> extractLinks(String content) {
        if (content == null || content.length() == 0) {
            return Collections.emptyList();
        }

        List<String> extractions = new ArrayList<String>();
        final Matcher matcher = LINKS_PATTERN.matcher(content);
        while (matcher.find()) {
            extractions.add(matcher.group());
        }
        return extractions;

    }
}
"
tika-core/src/main/java/org/apache/tika/utils/RereadableInputStream.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.utils;

import java.io.BufferedInputStream;
import java.io.BufferedOutputStream;
import java.io.ByteArrayInputStream;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;


/**
 * Wraps an input stream, reading it only once, but making it available
 * for rereading an arbitrary number of times.  The stream's bytes are
 * stored in memory up to a user specified maximum, and then stored in a
 * temporary file which is deleted when this class' close() method is called.
 */
public class RereadableInputStream extends InputStream {


    /**
     * Input stream originally passed to the constructor.
     */
    private InputStream originalInputStream;

    /**
     * The inputStream currently being used by this object to read contents;
     * may be the original stream passed in, or a stream that reads
     * the saved copy.
     */
    private InputStream inputStream;

    /**
     * Maximum number of bytes that can be stored in memory before
     * storage will be moved to a temporary file.
     */
    private int maxBytesInMemory;

    /**
     * True when the original stream is being read; set to false when
     * reading is set to use the stored data instead.
     */
    private boolean firstPass = true;

    /**
     * Whether or not the stream's contents are being stored in a file
     * as opposed to memory.
     */
    private boolean bufferIsInFile;

    /**
     * The buffer used to store the stream's content; this storage is moved
     * to a file when the stored data's size exceeds maxBytesInMemory.
     */
    private byte[] byteBuffer;

    /**
     * The total number of bytes read from the original stream at the time.
     */
    private int size;

    /**
     * File used to store the stream's contents; is null until the stored
     * content's size exceeds maxBytesInMemory.
     */
    private File storeFile;

    /**
     * OutputStream used to save the content of the input stream in a
     * temporary file.
     */
    private OutputStream storeOutputStream;


    /**
     * Specifies whether or not to read to the end of stream on first
     * rewind.  This defaults to true.  If this is set to false,
     * then the first time when rewind() is called, only those bytes
     * already read from the original stream will be available from then on.
     */
    private boolean readToEndOfStreamOnFirstRewind = true;


    /**
     * Specifies whether or not to close the original input stream
     * when close() is called.  Defaults to true.
     */
    private boolean closeOriginalStreamOnClose = true;


    // TODO: At some point it would be better to replace the current approach
    // (specifying the above) with more automated behavior.  The stream could
    // keep the original stream open until EOF was reached.  For example, if:
    //
    // the original stream is 10 bytes, and
    // only 2 bytes are read on the first pass
    // rewind() is called
    // 5 bytes are read
    //
    // In this case, this instance gets the first 2 from its store,
    // and the next 3 from the original stream, saving those additional 3
    // bytes in the store.  In this way, only the maximum number of bytes
    // ever needed must be saved in the store; unused bytes are never read.
    // The original stream is closed when EOF is reached, or when close()
    // is called, whichever comes first.  Using this approach eliminates
    // the need to specify the flag (though makes implementation more complex).
    


    /**
     * Creates a rereadable input stream.
     *
     * @param inputStream stream containing the source of data
     * @param maxBytesInMemory maximum number of bytes to use to store
     *     the stream's contents in memory before switching to disk; note that
     *     the instance will preallocate a byte array whose size is
     *     maxBytesInMemory.  This byte array will be made available for
     *     garbage collection (i.e. its reference set to null) when the
     *     content size exceeds the array's size, when close() is called, or
     *     when there are no more references to the instance.
     * @param readToEndOfStreamOnFirstRewind Specifies whether or not to
     *     read to the end of stream on first rewind.  If this is set to false,
     *     then when rewind() is first called, only those bytes already read
     *     from the original stream will be available from then on.
     */
    public RereadableInputStream(InputStream inputStream, int maxBytesInMemory,
            boolean readToEndOfStreamOnFirstRewind,
            boolean closeOriginalStreamOnClose) {
        this.inputStream = inputStream;
        this.originalInputStream = inputStream;
        this.maxBytesInMemory = maxBytesInMemory;
        byteBuffer = new byte[maxBytesInMemory];
        this.readToEndOfStreamOnFirstRewind = readToEndOfStreamOnFirstRewind;
        this.closeOriginalStreamOnClose = closeOriginalStreamOnClose;
    }

    /**
     * Reads a byte from the stream, saving it in the store if it is being
     * read from the original stream.  Implements the abstract
     * InputStream.read().
     *
     * @return the read byte, or -1 on end of stream.
     * @throws IOException
     */
    public int read() throws IOException {
        int inputByte = inputStream.read();
        if (firstPass) {
            saveByte(inputByte);
        }
        return inputByte;
    }

    /**
     * "Rewinds" the stream to the beginning for rereading.
     * @throws IOException
     */
    public void rewind() throws IOException {

        if (firstPass && readToEndOfStreamOnFirstRewind) {
            // Force read to end of stream to fill store with any
            // remaining bytes from original stream.
            while(read() != -1) {
                // empty loop
            }
        }

        closeStream();
        if (storeOutputStream != null) {
            storeOutputStream.close();
            storeOutputStream = null;
        }
        firstPass = false;
        boolean newStreamIsInMemory = (size < maxBytesInMemory);
        inputStream = newStreamIsInMemory
                ? new ByteArrayInputStream(byteBuffer)
                : new BufferedInputStream(new FileInputStream(storeFile));
    }

    /**
     * Closes the input stream currently used for reading (may either be
     * the original stream or a memory or file stream after the first pass).
     *
     * @throws IOException
     */
    // Does anyone need/want for this to be public?
    private void closeStream() throws IOException {
        if (inputStream != null
                &&
                (inputStream != originalInputStream
                        || closeOriginalStreamOnClose)) {
            inputStream.close();
            inputStream = null;
        }
    }

    /**
     * Closes the input stream and removes the temporary file if one was
     * created.
     * 
     * @throws IOException
     */
    public void close() throws IOException {
        closeStream();
        super.close();
        if (storeFile != null) {
            storeFile.delete();
        }
    }

    /**
     * Returns the number of bytes read from the original stream.
     *
     * @return number of bytes read
     */
    public int getSize() {
        return size;
    }

    /**
     * Saves the byte read from the original stream to the store.
     *
     * @param inputByte byte read from original stream
     * @throws IOException
     */
    private void saveByte(int inputByte) throws IOException {

        if (!bufferIsInFile) {
            boolean switchToFile = (size == (maxBytesInMemory));
            if (switchToFile) {
                storeFile = File.createTempFile("TIKA_streamstore_", ".tmp");
                bufferIsInFile = true;
                storeOutputStream = new BufferedOutputStream(
                        new FileOutputStream(storeFile));
                storeOutputStream.write(byteBuffer, 0, size);
                storeOutputStream.write(inputByte);
                byteBuffer = null; // release for garbage collection
            } else {
                byteBuffer[size] = (byte) inputByte;
            }
        } else {
            storeOutputStream.write(inputByte);
        }
        ++size;
    }
}
"
tika-dotnet/src/main/java/Tika/Tika.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package Tika;

import java.io.File;
import java.io.IOException;
import java.net.URL;

import org.apache.tika.exception.TikaException;

public class Tika {

    private final org.apache.tika.Tika tika = new org.apache.tika.Tika();

    public cli.System.String detect(cli.System.String name) {
        return toCliString(tika.detect(toJvmString(name)));
    }

    public cli.System.String detect(cli.System.IO.FileInfo file)
            throws cli.System.IO.IOException {
        try {
            return toCliString(tika.detect(new File(file.get_FullName())));
        } catch (IOException e) {
            throw new cli.System.IO.IOException(e.getMessage(), e);
        }
    }

    public cli.System.String detect(cli.System.Uri uri)
            throws cli.System.IO.IOException {
        try {
            return toCliString(tika.detect(new URL(uri.get_AbsolutePath())));
        } catch (IOException e) {
            throw new cli.System.IO.IOException(e.getMessage(), e);
        }
    }

    public cli.System.String parseToString(cli.System.IO.FileInfo file)
            throws cli.System.IO.IOException, TikaException {
        try {
            return toCliString(tika.parseToString(new File(file.get_FullName())));
        } catch (IOException e) {
            throw new cli.System.IO.IOException(e.getMessage(), e);
        }
    }

    public cli.System.String parseToString(cli.System.Uri uri)
            throws cli.System.IO.IOException, TikaException {
        try {
            return toCliString(tika.parseToString(new URL(uri.get_AbsoluteUri())));
        } catch (IOException e) {
            throw new cli.System.IO.IOException(e.getMessage(), e);
        }
    }

    private static cli.System.String toCliString(String string) {
        return new cli.System.String(string.toCharArray());
    }

    private static String toJvmString(cli.System.String string) {
        return new String(string.ToCharArray());
    }

}
"
tika-example/src/main/java/org/apache/tika/example/ContentHandlerExample.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.example;

import java.io.IOException;
import java.io.InputStream;
import java.util.ArrayList;
import java.util.List;

import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.parser.AutoDetectParser;
import org.apache.tika.sax.BodyContentHandler;
import org.apache.tika.sax.ContentHandlerDecorator;
import org.apache.tika.sax.ToXMLContentHandler;
import org.apache.tika.sax.XHTMLContentHandler;
import org.apache.tika.sax.xpath.Matcher;
import org.apache.tika.sax.xpath.MatchingContentHandler;
import org.apache.tika.sax.xpath.XPathParser;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * Examples of using different Content Handlers to
 *  get different parts of the file's contents 
 */
public class ContentHandlerExample {
    /**
     * Example of extracting the plain text of the contents.
     * Will return only the "body" part of the document
     */
    public String parseToPlainText() throws IOException, SAXException, TikaException {
        BodyContentHandler handler = new BodyContentHandler();
        
        InputStream stream = ContentHandlerExample.class.getResourceAsStream("test.doc");
        AutoDetectParser parser = new AutoDetectParser();
        Metadata metadata = new Metadata();
        try {
            parser.parse(stream, handler, metadata);
            return handler.toString();
        } finally {
            stream.close();
        }
    }

    /**
     * Example of extracting the contents as HTML, as a string.
     */
    public String parseToHTML() throws IOException, SAXException, TikaException {
        ContentHandler handler = new ToXMLContentHandler();
        
        InputStream stream = ContentHandlerExample.class.getResourceAsStream("test.doc");
        AutoDetectParser parser = new AutoDetectParser();
        Metadata metadata = new Metadata();
        try {
            parser.parse(stream, handler, metadata);
            return handler.toString();
        } finally {
            stream.close();
        }
    }
    
    /**
     * Example of extracting just the body as HTML, without the
     *  head part, as a string
     */
    public String parseBodyToHTML() throws IOException, SAXException, TikaException {
        ContentHandler handler = new BodyContentHandler(
                new ToXMLContentHandler());
        
        InputStream stream = ContentHandlerExample.class.getResourceAsStream("test.doc");
        AutoDetectParser parser = new AutoDetectParser();
        Metadata metadata = new Metadata();
        try {
            parser.parse(stream, handler, metadata);
            return handler.toString();
        } finally {
            stream.close();
        }
    }
    
    /**
     * Example of extracting just one part of the document's body,
     *  as HTML as a string, excluding the rest
     */
    public String parseOnePartToHTML() throws IOException, SAXException, TikaException {
        // Only get things under html -> body -> div (class=header)
        XPathParser xhtmlParser = new XPathParser("xhtml", XHTMLContentHandler.XHTML);
        Matcher divContentMatcher = xhtmlParser.parse(
                "/xhtml:html/xhtml:body/xhtml:div/descendant::node()");        
        ContentHandler handler = new MatchingContentHandler(
                new ToXMLContentHandler(), divContentMatcher);
        
        InputStream stream = ContentHandlerExample.class.getResourceAsStream("test2.doc");
        AutoDetectParser parser = new AutoDetectParser();
        Metadata metadata = new Metadata();
        try {
            parser.parse(stream, handler, metadata);
            return handler.toString();
        } finally {
            stream.close();
        }
    }
    
    protected final int MAXIMUM_TEXT_CHUNK_SIZE = 40;
    /**
     * Example of extracting the plain text in chunks, with each chunk
     *  of no more than a certain maximum size
     */
    public List<String> parseToPlainTextChunks() throws IOException, SAXException, TikaException {
        final List<String> chunks = new ArrayList<String>();
        chunks.add("");
        ContentHandlerDecorator handler = new ContentHandlerDecorator() {
            @Override
            public void characters(char[] ch, int start, int length) {
                String lastChunk = chunks.get(chunks.size()-1);
                String thisStr = new String(ch, start, length);
                
                if (lastChunk.length()+length > MAXIMUM_TEXT_CHUNK_SIZE) {
                    chunks.add(thisStr);
                } else {
                    chunks.set(chunks.size()-1, lastChunk+thisStr);
                }
            }
        };
        
        InputStream stream = ContentHandlerExample.class.getResourceAsStream("test2.doc");
        AutoDetectParser parser = new AutoDetectParser();
        Metadata metadata = new Metadata();
        try {
            parser.parse(stream, handler, metadata);
            return chunks;
        } finally {
            stream.close();
        }
    }
}
"
tika-example/src/main/java/org/apache/tika/example/DumpTikaConfigExample.java,true,"package org.apache.tika.example;
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import org.apache.tika.config.TikaConfig;
import org.apache.tika.detect.DefaultDetector;
import org.apache.tika.detect.Detector;
import org.apache.tika.exception.TikaException;
import org.apache.tika.language.translate.DefaultTranslator;
import org.apache.tika.language.translate.Translator;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.CompositeParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.Parser;
import org.w3c.dom.Document;
import org.w3c.dom.Element;
import org.w3c.dom.Node;

import javax.xml.parsers.DocumentBuilder;
import javax.xml.parsers.DocumentBuilderFactory;
import javax.xml.transform.OutputKeys;
import javax.xml.transform.Transformer;
import javax.xml.transform.TransformerFactory;
import javax.xml.transform.dom.DOMSource;
import javax.xml.transform.stream.StreamResult;
import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.OutputStreamWriter;
import java.io.StringWriter;
import java.io.Writer;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.TreeMap;
import java.util.TreeSet;


/**
 * This class shows how to dump a TikaConfig object to a configuration file.
 * This allows users to easily dump the default TikaConfig as a base from which
 * to start if they want to modify the default configuration file.
 * <p>
 * For those who want to modify the mimes file, take a look at
 * tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml
 * for inspiration.  Consider adding org/apache/tika/mime/custom-mimetypes.xml
 * for your custom mime types.
 */
public class DumpTikaConfigExample {

    /**
     *
     * @param config config file to dump
     * @param writer writer to which to write
     * @throws Exception
     */
    public void dump(TikaConfig config, Writer writer, String encoding) throws Exception {
        DocumentBuilderFactory docFactory = DocumentBuilderFactory.newInstance();
        DocumentBuilder docBuilder = docFactory.newDocumentBuilder();
        // root elements
        Document doc = docBuilder.newDocument();
        Element rootElement = doc.createElement("properties");

        doc.appendChild(rootElement);
        addMimeComment(rootElement, doc);
        addTranslator(rootElement, doc, config);
        addDetectors(rootElement, doc, config);
        addParsers(rootElement, doc, config);


        //now write
        TransformerFactory transformerFactory = TransformerFactory.newInstance();
        Transformer transformer = transformerFactory.newTransformer();
        transformer.setOutputProperty(OutputKeys.INDENT, "yes");
        transformer.setOutputProperty("{http://xml.apache.org/xslt}indent-amount", "2");
        transformer.setOutputProperty(OutputKeys.ENCODING, encoding);
        DOMSource source = new DOMSource(doc);
        StreamResult result = new StreamResult(writer);

        transformer.transform(source, result);
    }

    private void addTranslator(Element rootElement, Document doc, TikaConfig config) {
        //TikaConfig only reads the first translator from the list,
        //but it looks like it expects a list
        Translator translator = config.getTranslator();
        if (translator instanceof DefaultTranslator) {
            Node mimeComment = doc.createComment(
                    "for example: "+
                            "<translator class=\"org.apache.tika.language.translate.GoogleTranslator\"/>");
            rootElement.appendChild(mimeComment);
        } else {
            Element translatorElement = doc.createElement("translator");
            translatorElement.setAttribute("class", translator.getClass().getCanonicalName());
            rootElement.appendChild(translatorElement);
        }
    }

    private void addMimeComment(Element rootElement, Document doc) {
        Node mimeComment = doc.createComment(
                "for example: <mimeTypeRepository resource=\"/org/apache/tika/mime/tika-mimetypes.xml\"/>");
        rootElement.appendChild(mimeComment);
    }

    private void addDetectors(Element rootElement, Document doc, TikaConfig config) throws Exception {
        Detector detector = config.getDetector();
        Element detectorsElement = doc.createElement("detectors");

        if (detector instanceof DefaultDetector) {
            List<Detector> children = ((DefaultDetector)detector).getDetectors();
            for (Detector d : children) {
                Element detectorElement = doc.createElement("detector");
                detectorElement.setAttribute("class", d.getClass().getCanonicalName());
                detectorsElement.appendChild(detectorElement);
            }
        }
        rootElement.appendChild(detectorsElement);
    }

    private void addParsers(Element rootElement, Document doc, TikaConfig config) throws Exception {
        Map<String, Parser> parsers = getConcreteParsers(config.getParser());

        Element parsersElement = doc.createElement("parsers");
        rootElement.appendChild(parsersElement);

        ParseContext context = new ParseContext();
        for (Map.Entry<String, Parser> e : parsers.entrySet()) {
            Element parserElement = doc.createElement("parser");
            Parser child = e.getValue();
            String className = e.getKey();
            parserElement.setAttribute("class", className);
            Set<MediaType> types = new TreeSet<MediaType>();
            types.addAll(child.getSupportedTypes(context));
            for (MediaType type : types){
                Element mimeElement = doc.createElement("mime");
                mimeElement.appendChild(doc.createTextNode(type.toString()));
                parserElement.appendChild(mimeElement);
            }
            parsersElement.appendChild(parserElement);
        }
        rootElement.appendChild(parsersElement);

    }

    private Map<String, Parser> getConcreteParsers(Parser parentParser)throws TikaException, IOException  {
        Map<String, Parser> parsers = new TreeMap<String, Parser>();
        if (parentParser instanceof CompositeParser) {
            addParsers((CompositeParser)parentParser, parsers);
        } else {
            addParser(parentParser, parsers);
        }
        return parsers;
    }

    private void addParsers(CompositeParser p, Map<String, Parser> parsers) {
        for (Parser child : p.getParsers().values()) {
            if (child instanceof CompositeParser) {
                addParsers((CompositeParser)child, parsers);
            } else {
                addParser(child, parsers);
            }
        }
    }

    private void addParser(Parser p, Map<String, Parser> parsers) {
        parsers.put(p.getClass().getCanonicalName(), p);
    }

    /**
     *
     * @param args outputFile, outputEncoding, if args is empty, this prints to console
     * @throws Exception
     */
    public static void main(String[] args) throws Exception {

        String encoding = "UTF-8";
        Writer writer = null;
        if (args.length > 0) {
            writer = new OutputStreamWriter(new FileOutputStream(new File(args[0])));
        } else {
            writer = new StringWriter();
        }

        if (args.length > 1) {
            encoding = args[1];
        }
        DumpTikaConfigExample ex = new DumpTikaConfigExample();
        ex.dump(TikaConfig.getDefaultConfig(), writer, encoding);

        writer.flush();

        if (writer instanceof StringWriter) {
            System.out.println(writer.toString());
        }
        writer.close();
    }
}
"
tika-example/src/main/java/org/apache/tika/example/GrabPhoneNumbersExample.java,false,"package org.apache.tika.example;
/**
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
import org.apache.tika.metadata.Metadata;
import org.apache.tika.parser.AutoDetectParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.Parser;
import org.apache.tika.sax.BodyContentHandler;
import org.apache.tika.sax.PhoneExtractingContentHandler;

import java.io.File;
import java.io.FileInputStream;
import java.io.InputStream;
import java.util.HashSet;

/**
 * Class to demonstrate how to use the {@link org.apache.tika.sax.PhoneExtractingContentHandler}
 * to get a list of all of the phone numbers from every file in a directory.
 *
 * You can run this main method by running
 * <code>
 *     mvn exec:java -Dexec.mainClass="org.apache.tika.example.GrabPhoneNumbersExample" -Dexec.args="/path/to/directory"
 * </code>
 * from the tika-example directory.
 */
public class GrabPhoneNumbersExample {
    private static HashSet<String> phoneNumbers = new HashSet<String>();
    private static int failedFiles, successfulFiles = 0;

    public static void main(String[] args){
        if (args.length != 1) {
            System.err.println("Usage `java GrabPhoneNumbers [corpus]");
            return;
        }
        final File folder = new File(args[0]);
        System.out.println("Searching " + folder.getAbsolutePath() + "...");
        processFolder(folder);
        System.out.println(phoneNumbers.toString());
        System.out.println("Parsed " + successfulFiles + "/" + (successfulFiles + failedFiles));
    }

    public static void processFolder(final File folder) {
        for (final File fileEntry : folder.listFiles()) {
            if (fileEntry.isDirectory()) {
                processFolder(fileEntry);
            } else {
                try {
                    process(fileEntry);
                    successfulFiles++;
                } catch (Exception e) {
                    failedFiles++;
                    // Ignore this file...
                }
            }
        }
    }

    public static void process(File file) throws Exception {
        Parser parser = new AutoDetectParser();
        Metadata metadata = new Metadata();
        // The PhoneExtractingContentHandler will examine any characters for phone numbers before passing them
        // to the underlying Handler.
        PhoneExtractingContentHandler handler = new PhoneExtractingContentHandler(new BodyContentHandler(), metadata);
        InputStream stream = new FileInputStream(file);
        try {
            parser.parse(stream, handler, metadata, new ParseContext());
        }
        finally {
            stream.close();
        }
        String[] numbers = metadata.getValues("phonenumbers");
        for (String number : numbers) {
            phoneNumbers.add(number);
        }
    }
}
"
tika-example/src/main/java/org/apache/tika/example/LanguageIdentifierExample.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.example;

import org.apache.tika.language.LanguageIdentifier;

public class LanguageIdentifierExample {
    public String identifyLanguage(String text) {
        LanguageIdentifier identifier = new LanguageIdentifier(text);
        return identifier.getLanguage();
    }
}
"
tika-example/src/main/java/org/apache/tika/example/ParsingExample.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.example;

import org.apache.tika.Tika;
import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.parser.AutoDetectParser;
import org.apache.tika.sax.BodyContentHandler;
import org.xml.sax.SAXException;

import java.io.IOException;
import java.io.InputStream;

public class ParsingExample {

    /**
     * Example of how to use Tika's parseToString method to parse the content of a file,
     * and return any text found.
     *
     * @return The content of a file.
     */
    public String parseToStringExample() throws IOException, SAXException, TikaException {
        InputStream stream = ParsingExample.class.getResourceAsStream("test.doc");
        Tika tika = new Tika();
        try {
            return tika.parseToString(stream);
        } finally {
            stream.close();
        }
    }

    /**
     * Example of how to use Tika to parse an file when you do not know its file type
     * ahead of time.
     *
     * AutoDetectParser attempts to discover the file's type automatically, then call
     * the exact Parser built for that file type.
     *
     * The stream to be parsed by the Parser. In this case, we get a file from the
     * resources folder of this project.
     *
     * Handlers are used to get the exact information you want out of the host of
     * information gathered by Parsers. The body content handler, intuitively, extracts
     * everything that would go between HTML body tags.
     *
     * The Metadata object will be filled by the Parser with Metadata discovered about
     * the file being parsed.
     *
     * @return The content of a file.
     */
    public String parseExample() throws IOException, SAXException, TikaException {
        InputStream stream = ParsingExample.class.getResourceAsStream("test.doc");
        AutoDetectParser parser = new AutoDetectParser();
        BodyContentHandler handler = new BodyContentHandler();
        Metadata metadata = new Metadata();
        try {
            parser.parse(stream, handler, metadata);
            return handler.toString();
        } finally {
            stream.close();
        }
    }
}
"
tika-example/src/main/java/org/apache/tika/example/TranslatorExample.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.example;

import org.apache.tika.language.translate.MicrosoftTranslator;

public class TranslatorExample {
    public String microsoftTranslateToFrench(String text) {
        MicrosoftTranslator translator = new MicrosoftTranslator();
        // Change the id and secret! See http://msdn.microsoft.com/en-us/library/hh454950.aspx.
        translator.setId("dummy-id");
        translator.setSecret("dummy-secret");
        try {
            return translator.translate(text, "fr");
        } catch (Exception e) {
            return "Error while translating.";
        }
    }
}
"
tika-java7/src/main/java/org/apache/tika/filetypedetector/package-info.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Tika Java-7 FileTypeDetector implementations.
 */
@aQute.bnd.annotation.Version("1.0.0")
package org.apache.tika.filetypedetector;"
tika-java7/src/main/java/org/apache/tika/filetypedetector/TikaFileTypeDetector.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.filetypedetector;

import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.spi.FileTypeDetector;

import org.apache.tika.Tika;
import org.apache.tika.mime.MimeTypes;

public class TikaFileTypeDetector extends FileTypeDetector {
    private final Tika tika = new Tika();
    
    public TikaFileTypeDetector() {
        super();
    }
    
    @Override
    public String probeContentType(Path path) throws IOException {
        // Try to detect based on the file name only for efficiency
        String fileNameDetect = tika.detect(path.toString());
        if(!fileNameDetect.equals(MimeTypes.OCTET_STREAM)) {
            return fileNameDetect;
        }
        
        // Then check the file content if necessary
        String fileContentDetect = tika.detect(path.toFile());
        if(!fileContentDetect.equals(MimeTypes.OCTET_STREAM)) {
            return fileContentDetect;
        }
        
        // Specification says to return null if we could not 
        // conclusively determine the file type
        return null;
    }
    
}
"
tika-parsers/src/main/java/org/apache/tika/parser/asm/ClassParser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.asm;

import java.io.IOException;
import java.io.InputStream;
import java.util.Collections;
import java.util.Set;

import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * Parser for Java .class files.
 */
public class ClassParser extends AbstractParser {

    /** Serial version UID */
    private static final long serialVersionUID = -3531388963354454357L;

    private static final Set<MediaType> SUPPORTED_TYPES =
        Collections.singleton(MediaType.application("java-vm"));

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }

    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        new XHTMLClassVisitor(handler, metadata).parse(stream);
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/asm/XHTMLClassVisitor.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.asm;

import java.io.IOException;
import java.io.InputStream;

import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.sax.XHTMLContentHandler;
import org.objectweb.asm.AnnotationVisitor;
import org.objectweb.asm.Attribute;
import org.objectweb.asm.ClassReader;
import org.objectweb.asm.ClassVisitor;
import org.objectweb.asm.FieldVisitor;
import org.objectweb.asm.MethodVisitor;
import org.objectweb.asm.Opcodes;
import org.objectweb.asm.Type;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * Class visitor that generates XHTML SAX events to describe the
 * contents of the visited class.
 */
class XHTMLClassVisitor extends ClassVisitor {

    private final XHTMLContentHandler xhtml;

    private final Metadata metadata;

    private Type type;

    private String packageName;

    public XHTMLClassVisitor(ContentHandler handler, Metadata metadata) {
        super(Opcodes.ASM4);
        this.xhtml = new XHTMLContentHandler(handler, metadata);
        this.metadata = metadata;
    }

    public void parse(InputStream stream)
            throws TikaException, SAXException, IOException {
        try {
            ClassReader reader = new ClassReader(stream);
            reader.accept(this, ClassReader.SKIP_FRAMES | ClassReader.SKIP_CODE);
        } catch (RuntimeException e) {
            if (e.getCause() instanceof SAXException) {
                throw (SAXException) e.getCause();
            } else {
                throw new TikaException("Failed to parse a Java class", e);
            }
        }
    }

    public void visit(
            int version, int access, String name, String signature,
            String superName, String[] interfaces) {
        type = Type.getObjectType(name);

        String className = type.getClassName();
        int dot = className.lastIndexOf('.');
        if (dot != -1) {
            packageName = className.substring(0, dot);
            className = className.substring(dot + 1);
        }

        metadata.set(TikaCoreProperties.TITLE, className);
        metadata.set(Metadata.RESOURCE_NAME_KEY, className + ".class");

        try {
            xhtml.startDocument();
            xhtml.startElement("pre");

            if (packageName != null) {
                writeKeyword("package");
                xhtml.characters(" " + packageName + ";\n");
            }

            writeAccess(access);
            if (isSet(access, Opcodes.ACC_INTERFACE)) {
                writeKeyword("interface");
                writeSpace();
                writeType(type);
                writeSpace();
                writeInterfaces("extends", interfaces);
            } else if (isSet(access, Opcodes.ACC_ENUM)) {
                writeKeyword("enum");
                writeSpace();
                writeType(type);
                writeSpace();
            } else {
                writeKeyword("class");
                writeSpace();
                writeType(type);
                writeSpace();
                if (superName != null) {
                    Type superType = Type.getObjectType(superName);
                    if (!superType.getClassName().equals("java.lang.Object")) {
                        writeKeyword("extends");
                        writeSpace();
                        writeType(superType);
                        writeSpace();
                    }
                }
                writeInterfaces("implements", interfaces);
            }
            xhtml.characters("{\n");
        } catch (SAXException e) {
            throw new RuntimeException(e);
        }
    }

    private void writeInterfaces(String keyword, String[] interfaces)
            throws SAXException {
        if (interfaces != null && interfaces.length > 0) {
            writeKeyword(keyword);
            String separator = " ";
            for (String iface : interfaces) {
                xhtml.characters(separator);
                writeType(Type.getObjectType(iface));
                separator = ", ";
            }
            writeSpace();
        }
    }

    public void visitEnd() {
        try {
            xhtml.characters("}\n");
            xhtml.endElement("pre");
            xhtml.endDocument();
        } catch (SAXException e) {
            throw new RuntimeException(e);
        }
    }

    /**
     * Ignored.
     */
    public void visitOuterClass(String owner, String name, String desc) {
    }

    /**
     * Ignored.
     */
    public void visitSource(String source, String debug) {
    }


    /**
     * Ignored.
     */
    public AnnotationVisitor visitAnnotation(String desc, boolean visible) {
        return null;
    }

    /**
     * Ignored.
     */
    public void visitAttribute(Attribute attr) {
    }

    /**
     * Ignored.
     */
    public void visitInnerClass(
            String name, String outerName, String innerName, int access) {
    }

    /**
     * Visits a field.
     */
    public FieldVisitor visitField(
            int access, String name, String desc, String signature,
            Object value) {
        if (!isSet(access, Opcodes.ACC_SYNTHETIC)) {
            try {
                xhtml.characters("    ");
                writeAccess(access);
                writeType(Type.getType(desc));
                writeSpace();
                writeIdentifier(name);

                if (isSet(access, Opcodes.ACC_STATIC) && value != null) {
                    xhtml.characters(" = ");
                    xhtml.characters(value.toString());
                }

                writeSemicolon();
                writeNewline();
            } catch (SAXException e) {
                throw new RuntimeException(e);
            }
        }

        return null;
    }

    /**
     * Visits a method.
     */
    public MethodVisitor visitMethod(
            int access, String name, String desc, String signature,
            String[] exceptions) {
        if (!isSet(access, Opcodes.ACC_SYNTHETIC)) {
            try {
                xhtml.characters("    ");
                writeAccess(access);
                writeType(Type.getReturnType(desc));
                writeSpace();
                if ("<init>".equals(name)) {
                    writeType(type);
                } else {
                    writeIdentifier(name);
                }

                xhtml.characters("(");
                String separator = "";
                for (Type arg : Type.getArgumentTypes(desc)) {
                    xhtml.characters(separator);
                    writeType(arg);
                    separator = ", ";
                }
                xhtml.characters(")");

                if (exceptions != null && exceptions.length > 0) {
                    writeSpace();
                    writeKeyword("throws");
                    separator = " ";
                    for (String exception : exceptions) {
                        xhtml.characters(separator);
                        writeType(Type.getObjectType(exception));
                        separator = ", ";
                    }
                }

                writeSemicolon();
                writeNewline();
            } catch (SAXException e) {
                throw new RuntimeException(e);
            }
        }

        return null;
    }

    private void writeIdentifier(String identifier) throws SAXException {
        xhtml.startElement("span", "class", "java-identifier");
        xhtml.characters(identifier);
        xhtml.endElement("span");
    }

    private void writeKeyword(String keyword) throws SAXException {
        xhtml.startElement("span", "class", "java-keyword");
        xhtml.characters(keyword);
        xhtml.endElement("span");
    }

    private void writeSemicolon() throws SAXException {
        xhtml.characters(";");
    }

    private void writeSpace() throws SAXException {
        xhtml.characters(" ");
    }

    private void writeNewline() throws SAXException {
        xhtml.characters("\n");
    }

    private void writeAccess(int access) throws SAXException {
        writeAccess(access, Opcodes.ACC_PRIVATE, "private");
        writeAccess(access, Opcodes.ACC_PROTECTED, "protected");
        writeAccess(access, Opcodes.ACC_PUBLIC, "public");
        writeAccess(access, Opcodes.ACC_STATIC, "static");
        writeAccess(access, Opcodes.ACC_FINAL, "final");
        writeAccess(access, Opcodes.ACC_ABSTRACT, "abstract");
        writeAccess(access, Opcodes.ACC_SYNCHRONIZED, "synchronized");
        writeAccess(access, Opcodes.ACC_TRANSIENT, "transient");
        writeAccess(access, Opcodes.ACC_VOLATILE, "volatile");
        writeAccess(access, Opcodes.ACC_NATIVE, "native");
    }

    private void writeAccess(int access, int code, String keyword)
            throws SAXException {
        if (isSet(access, code)) {
            writeKeyword(keyword);
            xhtml.characters(" ");
        }
    }

    private void writeType(Type type) throws SAXException {
        String name = type.getClassName();
        if (name.startsWith(packageName + ".")) {
            xhtml.characters(name.substring(packageName.length() + 1));
        } else if (name.startsWith("java.lang.")) {
            xhtml.characters(name.substring("java.lang.".length()));
        } else {
            xhtml.characters(name);
        }
    }

    private static boolean isSet(int value, int flag) {
        return (value & flag) != 0;
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/audio/AudioParser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.audio;

import java.io.BufferedInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.util.Arrays;
import java.util.Collections;
import java.util.HashSet;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Set;

import javax.sound.sampled.AudioFileFormat;
import javax.sound.sampled.AudioFileFormat.Type;
import javax.sound.sampled.AudioFormat;
import javax.sound.sampled.AudioSystem;
import javax.sound.sampled.UnsupportedAudioFileException;

import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.XMPDM;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

public class AudioParser extends AbstractParser {

    /** Serial version UID */
    private static final long serialVersionUID = -6015684081240882695L;

    private static final Set<MediaType> SUPPORTED_TYPES =
        Collections.unmodifiableSet(new HashSet<MediaType>(Arrays.asList(
                MediaType.audio("basic"),
                MediaType.audio("x-wav"),
                MediaType.audio("x-aiff"))));

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }

    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        // AudioSystem expects the stream to support the mark feature
        if (!stream.markSupported()) {
            stream = new BufferedInputStream(stream);
        }
        try {
            AudioFileFormat fileFormat = AudioSystem.getAudioFileFormat(stream);
            Type type = fileFormat.getType();
            if (type == Type.AIFC || type == Type.AIFF) {
                metadata.set(Metadata.CONTENT_TYPE, "audio/x-aiff");
            } else if (type == Type.AU || type == Type.SND) {
                metadata.set(Metadata.CONTENT_TYPE, "audio/basic");
            } else if (type == Type.WAVE) {
                metadata.set(Metadata.CONTENT_TYPE, "audio/x-wav");
            }

            AudioFormat audioFormat = fileFormat.getFormat();
            int channels = audioFormat.getChannels();
            if (channels != AudioSystem.NOT_SPECIFIED) {
                metadata.set("channels", String.valueOf(channels));
                // TODO: Use XMPDM.TRACKS? (see also frame rate in AudioFormat)
            }
            float rate = audioFormat.getSampleRate();
            if (rate != AudioSystem.NOT_SPECIFIED) {
                metadata.set("samplerate", String.valueOf(rate));
                metadata.set(
                        XMPDM.AUDIO_SAMPLE_RATE,
                        Integer.toString((int) rate));
            }
            int bits = audioFormat.getSampleSizeInBits();
            if (bits != AudioSystem.NOT_SPECIFIED) {
                metadata.set("bits", String.valueOf(bits));
                if (bits == 8) {
                    metadata.set(XMPDM.AUDIO_SAMPLE_TYPE, "8Int");
                } else if (bits == 16) {
                    metadata.set(XMPDM.AUDIO_SAMPLE_TYPE, "16Int");
                } else if (bits == 32) {
                    metadata.set(XMPDM.AUDIO_SAMPLE_TYPE, "32Int");
                }
            }
            metadata.set("encoding", audioFormat.getEncoding().toString());

            // Javadoc suggests that some of the following properties might
            // be available, but I had no success in finding any:

            // "duration" Long playback duration of the file in microseconds
            // "author" String name of the author of this file
            // "title" String title of this file
            // "copyright" String copyright message
            // "date" Date date of the recording or release
            // "comment" String an arbitrary text

            addMetadata(metadata, fileFormat.properties());
            addMetadata(metadata, audioFormat.properties());
        } catch (UnsupportedAudioFileException e) {
            // There is no way to know whether this exception was
            // caused by the document being corrupted or by the format
            // just being unsupported. So we do nothing.
        }

        XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);
        xhtml.startDocument();
        xhtml.endDocument();
    }

    private void addMetadata(Metadata metadata, Map<String, Object> properties) {
        if (properties != null) {
            for (Entry<String, Object> entry : properties.entrySet()) {
                Object value = entry.getValue();
                if (value != null) {
                    metadata.set(entry.getKey(), value.toString());
                }
            }
        }
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/audio/MidiParser.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.audio;

import java.io.BufferedInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.util.Arrays;
import java.util.Collections;
import java.util.HashSet;
import java.util.Set;

import javax.sound.midi.InvalidMidiDataException;
import javax.sound.midi.MetaMessage;
import javax.sound.midi.MidiMessage;
import javax.sound.midi.MidiSystem;
import javax.sound.midi.Patch;
import javax.sound.midi.Sequence;
import javax.sound.midi.Track;

import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

public class MidiParser extends AbstractParser {

    /** Serial version UID */
    private static final long serialVersionUID = 6343278584336189432L;

    private static final Set<MediaType> SUPPORTED_TYPES =
        Collections.unmodifiableSet(new HashSet<MediaType>(Arrays.asList(
                MediaType.application("x-midi"),
                MediaType.audio("midi"))));

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }

    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        metadata.set(Metadata.CONTENT_TYPE, "audio/midi");

        XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);
        xhtml.startDocument();

        // MidiSystem expects the stream to support the mark feature
        InputStream buffered = new BufferedInputStream(stream);
        try {
            Sequence sequence = MidiSystem.getSequence(buffered);

            Track[] tracks = sequence.getTracks();
            metadata.set("tracks", String.valueOf(tracks.length));
            // TODO: Use XMPDM.TRACKS?

            Patch[] patches = sequence.getPatchList();
            metadata.set("patches", String.valueOf(patches.length));

            float type = sequence.getDivisionType();
            if (type == Sequence.PPQ) {
                metadata.set("divisionType", "PPQ");
            } else if (type == Sequence.SMPTE_24) {
                metadata.set("divisionType", "SMPTE_24");
            } else if (type == Sequence.SMPTE_25) {
                metadata.set("divisionType", "SMPTE_25");
            } else if (type == Sequence.SMPTE_30) {
                metadata.set("divisionType", "SMPTE_30");
            } else if (type == Sequence.SMPTE_30DROP) {
                metadata.set("divisionType", "SMPTE_30DROP");
            } else if (type == Sequence.SMPTE_24) {
                metadata.set("divisionType", String.valueOf(type));
            }

            for (Track track : tracks) {
                xhtml.startElement("p");
                for (int i = 0; i < track.size(); i++) {
                    MidiMessage message = track.get(i).getMessage();
                    if (message instanceof MetaMessage) {
                        MetaMessage meta = (MetaMessage) message;
                        // Types 1-15 are reserved for text events
                        if (meta.getType() >= 1 && meta.getType() <= 15) {
                            // FIXME: What's the encoding?
                            xhtml.characters(
                                    new String(meta.getData(), "ISO-8859-1"));
                        }
                    }
                }
                xhtml.endElement("p");
            }
        } catch (InvalidMidiDataException ignore) {
            // There is no way to know whether this exception was
            // caused by the document being corrupted or by the format
            // just being unsupported. So we do nothing.
        }

        xhtml.endDocument();
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/chm/ChmParser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.chm;

import java.io.ByteArrayInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.util.Arrays;
import java.util.Collections;
import java.util.HashSet;
import java.util.Set;

import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.chm.accessor.DirectoryListingEntry;
import org.apache.tika.parser.chm.core.ChmExtractor;
import org.apache.tika.parser.html.HtmlParser;
import org.apache.tika.sax.BodyContentHandler;
import org.apache.tika.sax.EmbeddedContentHandler;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

public class ChmParser extends AbstractParser {

    /** Serial version UID */
    private static final long serialVersionUID = 5938777307516469802L;

    private static final Set<MediaType> SUPPORTED_TYPES =
            Collections.unmodifiableSet(new HashSet<MediaType>(Arrays.asList(
                    MediaType.application("vnd.ms-htmlhelp"),
                    MediaType.application("chm"),
                    MediaType.application("x-chm"))));

    @Override
    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }

    @Override
    public void parse(InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context) throws IOException,
            SAXException, TikaException {
        ChmExtractor chmExtractor = new ChmExtractor(stream);

        // metadata
        metadata.set(Metadata.CONTENT_TYPE, "application/vnd.ms-htmlhelp");

        // content
        XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);
        xhtml.startDocument();

        for (DirectoryListingEntry entry : chmExtractor.getChmDirList().getDirectoryListingEntryList()) {
            final String entryName = entry.getName();
            if (entryName.endsWith(".html") 
                    || entryName.endsWith(".htm")
            ) {
//                AttributesImpl attrs = new AttributesImpl();
//                attrs.addAttribute("", "name", "name", "String", entryName);
//                xhtml.startElement("", "document", "document", attrs);
                
                byte[] data = chmExtractor.extractChmEntry(entry);

                parsePage(data, xhtml);
                
//                xhtml.endElement("", "", "document");
            }
        }

        xhtml.endDocument();
    }


    private void parsePage(byte[] byteObject, ContentHandler xhtml) throws TikaException {// throws IOException
        InputStream stream = null;
        Metadata metadata = new Metadata();
        HtmlParser htmlParser = new HtmlParser();
        ContentHandler handler = new EmbeddedContentHandler(new BodyContentHandler(xhtml));// -1
        ParseContext parser = new ParseContext();
        try {
            stream = new ByteArrayInputStream(byteObject);
            htmlParser.parse(stream, handler, metadata, parser);
        } catch (SAXException e) {
            throw new RuntimeException(e);
        } catch (IOException e) {
            // Pushback overflow from tagsoup
        }
    }
    
}
"
tika-parsers/src/main/java/org/apache/tika/parser/chm/accessor/ChmAccessor.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.chm.accessor;

import java.io.Serializable;

import org.apache.tika.exception.TikaException;

/**
 * 
 * Defines an accessor interface
 * 
 * @param <T>
 */
public interface ChmAccessor<T> extends Serializable {
    /**
     * Parses chm accessor
     * 
     * @param data
     *            chm file
     * @param chmAccessor
     * @throws TikaException 
     */
    void parse(byte[] data, T chmAccessor) throws TikaException;
}
"
tika-parsers/src/main/java/org/apache/tika/parser/chm/accessor/ChmDirectoryListingSet.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.chm.accessor;

import java.io.UnsupportedEncodingException;
import java.math.BigInteger;
import java.util.ArrayList;
import java.util.List;
import org.apache.tika.exception.TikaException;
import org.apache.tika.parser.chm.core.ChmCommons;
import org.apache.tika.parser.chm.core.ChmConstants;
import org.apache.tika.parser.chm.exception.ChmParsingException;

/**
 * Holds chm listing entries
 */
public class ChmDirectoryListingSet {
    private List<DirectoryListingEntry> dlel;
    private byte[] data;
    private int placeHolder = -1;
    private long dataOffset = -1;
    private int controlDataIndex = -1;
    private int resetTableIndex = -1;

    private boolean isNotControlDataFound = true;
    private boolean isNotResetTableFound = true;

    /**
     * Constructs chm directory listing set
     * 
     * @param data
     *            byte[]
     * @param chmItsHeader
     * @param chmItspHeader
     * @throws TikaException 
     */
    public ChmDirectoryListingSet(byte[] data, ChmItsfHeader chmItsHeader,
            ChmItspHeader chmItspHeader) throws TikaException {
        setDirectoryListingEntryList(new ArrayList<DirectoryListingEntry>());
        ChmCommons.assertByteArrayNotNull(data);
        setData(data);
        enumerateChmDirectoryListingList(chmItsHeader, chmItspHeader);
    }

    public String toString() {
        StringBuilder sb = new StringBuilder();
        sb.append("list:=" + getDirectoryListingEntryList().toString()
                + System.getProperty("line.separator"));
        sb.append("number of list items:="
                + getDirectoryListingEntryList().size());
        return sb.toString();
    }

    /**
     * Returns control data index that located in List
     * 
     * @return control data index
     */
    public int getControlDataIndex() {
        return controlDataIndex;
    }

    /**
     * Sets control data index
     * 
     * @param controlDataIndex
     */
    protected void setControlDataIndex(int controlDataIndex) {
        this.controlDataIndex = controlDataIndex;
    }

    /**
     * Return index of reset table
     * 
     * @return reset table index
     */
    public int getResetTableIndex() {
        return resetTableIndex;
    }

    /**
     * Sets reset table index
     * 
     * @param resetTableIndex
     */
    protected void setResetTableIndex(int resetTableIndex) {
        this.resetTableIndex = resetTableIndex;
    }

    /**
     * Sets place holder
     * 
     * @param placeHolder
     */
    private void setPlaceHolder(int placeHolder) {
        this.placeHolder = placeHolder;
    }

    private ChmPmglHeader PMGLheader;
    /**
     * Enumerates chm directory listing entries
     * 
     * @param chmItsHeader
     *            chm itsf PMGLheader
     * @param chmItspHeader
     *            chm itsp PMGLheader
     */
    private void enumerateChmDirectoryListingList(ChmItsfHeader chmItsHeader,
            ChmItspHeader chmItspHeader) {
        try {
            int startPmgl = chmItspHeader.getIndex_head();
            int stopPmgl = chmItspHeader.getUnknown_0024();
            int dir_offset = (int) (chmItsHeader.getDirOffset() + chmItspHeader
                    .getHeader_len());
            setDataOffset(chmItsHeader.getDataOffset());

            /* loops over all pmgls */
            byte[] dir_chunk = null;
            for (int i = startPmgl; i>=0; ) {
                dir_chunk = new byte[(int) chmItspHeader.getBlock_len()];
                int start = i * (int) chmItspHeader.getBlock_len() + dir_offset;
                dir_chunk = ChmCommons
                        .copyOfRange(getData(), start,
                                start +(int) chmItspHeader.getBlock_len());

                PMGLheader = new ChmPmglHeader();
                PMGLheader.parse(dir_chunk, PMGLheader);
                enumerateOneSegment(dir_chunk);
                
                i=PMGLheader.getBlockNext();
                dir_chunk = null;
            }
        } catch (Exception e) {
            e.printStackTrace();
        } finally {
            setData(null);
        }
    }

    /**
     * Checks control data
     * 
     * @param dle
     *            chm directory listing entry
     */
    private void checkControlData(DirectoryListingEntry dle) {
        if (isNotControlDataFound) {
            if (dle.getName().contains(ChmConstants.CONTROL_DATA)) {
                setControlDataIndex(getDirectoryListingEntryList().size());
                isNotControlDataFound = false;
            }
        }
    }

    /**
     * Checks reset table
     * 
     * @param dle
     *            chm directory listing entry
     */
    private void checkResetTable(DirectoryListingEntry dle) {
        if (isNotResetTableFound) {
            if (dle.getName().contains(ChmConstants.RESET_TABLE)) {
                setResetTableIndex(getDirectoryListingEntryList().size());
                isNotResetTableFound = false;
            }
        }
    }

    public static final boolean startsWith(byte[] data, String prefix) {
        for (int i=0; i<prefix.length(); i++) {
            if (data[i]!=prefix.charAt(i)) {
                return false;
            }
        }
        
        return true;
    }
    /**
     * Enumerates chm directory listing entries in single chm segment
     * 
     * @param dir_chunk
     */
    private void enumerateOneSegment(byte[] dir_chunk) throws ChmParsingException {
//        try {
            if (dir_chunk != null) {
                int header_len;
                if (startsWith(dir_chunk, ChmConstants.CHM_PMGI_MARKER)) {
                    header_len = ChmConstants.CHM_PMGI_LEN;
                    return; //skip PMGI
                }
                else if (startsWith(dir_chunk, ChmConstants.PMGL)) {
                    header_len = ChmConstants.CHM_PMGL_LEN;
                }
                else {
                    throw new ChmParsingException("Bad dir entry block.");
                }

                placeHolder = header_len;
                //setPlaceHolder(header_len);
                while (placeHolder > 0 && placeHolder < dir_chunk.length - PMGLheader.getFreeSpace()
                        /*&& dir_chunk[placeHolder - 1] != 115*/) 
                {
                    //get entry name length
                    int strlen = 0;// = getEncint(data);
                    byte temp;
                    while ((temp=dir_chunk[placeHolder++]) >= 0x80)
                    {
                        strlen <<= 7;
                        strlen += temp & 0x7f;
                    }

                    strlen = (strlen << 7) + temp & 0x7f;
                    
                    if (strlen>dir_chunk.length) {
                        throw new ChmParsingException("Bad data of a string length.");
                    }
                    
                    DirectoryListingEntry dle = new DirectoryListingEntry();
                    dle.setNameLength(strlen);
                    try {
                        dle.setName(new String(ChmCommons.copyOfRange(
                                dir_chunk, placeHolder,
                                (placeHolder + dle.getNameLength())), "UTF-8"));
                    } catch (UnsupportedEncodingException ex) {
                        dle.setName(new String(dir_chunk, placeHolder, placeHolder + dle.getNameLength()));
                    }
                    checkControlData(dle);
                    checkResetTable(dle);
                    setPlaceHolder(placeHolder
                            + dle.getNameLength());

                    /* Sets entry type */
                    if (placeHolder < dir_chunk.length
                            && dir_chunk[placeHolder] == 0)
                        dle.setEntryType(ChmCommons.EntryType.UNCOMPRESSED);
                    else
                        dle.setEntryType(ChmCommons.EntryType.COMPRESSED);

                    setPlaceHolder(placeHolder + 1);
                    dle.setOffset(getEncint(dir_chunk));
                    dle.setLength(getEncint(dir_chunk));
                    getDirectoryListingEntryList().add(dle);
                }
                
//                int indexWorkData = ChmCommons.indexOf(dir_chunk,
//                        "::".getBytes("UTF-8"));
//                int indexUserData = ChmCommons.indexOf(dir_chunk,
//                        "/".getBytes("UTF-8"));
//
//                if (indexUserData>=0 && indexUserData < indexWorkData)
//                    setPlaceHolder(indexUserData);
//                else if (indexWorkData>=0) {
//                    setPlaceHolder(indexWorkData);
//                }
//                else {
//                    setPlaceHolder(indexUserData);
//                }
//
//                if (placeHolder > 0 && placeHolder < dir_chunk.length - PMGLheader.getFreeSpace()
//                        && dir_chunk[placeHolder - 1] != 115) {// #{
//                    do {
//                        if (dir_chunk[placeHolder - 1] > 0) {
//                            DirectoryListingEntry dle = new DirectoryListingEntry();
//
//                            // two cases: 1. when dir_chunk[placeHolder -
//                            // 1] == 0x73
//                            // 2. when dir_chunk[placeHolder + 1] == 0x2f
//                            doNameCheck(dir_chunk, dle);
//
//                            // dle.setName(new
//                            // String(Arrays.copyOfRange(dir_chunk,
//                            // placeHolder, (placeHolder +
//                            // dle.getNameLength()))));
//                            dle.setName(new String(ChmCommons.copyOfRange(
//                                    dir_chunk, placeHolder,
//                                    (placeHolder + dle.getNameLength())), "UTF-8"));
//                            checkControlData(dle);
//                            checkResetTable(dle);
//                            setPlaceHolder(placeHolder
//                                    + dle.getNameLength());
//
//                            /* Sets entry type */
//                            if (placeHolder < dir_chunk.length
//                                    && dir_chunk[placeHolder] == 0)
//                                dle.setEntryType(ChmCommons.EntryType.UNCOMPRESSED);
//                            else
//                                dle.setEntryType(ChmCommons.EntryType.COMPRESSED);
//
//                            setPlaceHolder(placeHolder + 1);
//                            dle.setOffset(getEncint(dir_chunk));
//                            dle.setLength(getEncint(dir_chunk));
//                            getDirectoryListingEntryList().add(dle);
//                        } else
//                            setPlaceHolder(placeHolder + 1);
//
//                    } while (nextEntry(dir_chunk));
//                }
            }

//        } catch (Exception e) {
//            e.printStackTrace();
//        }
    }


    /**
     * Returns encrypted integer
     * 
     * @param data_chunk
     * 
     * @return
     */
    private int getEncint(byte[] data_chunk) {
        byte ob;
        BigInteger bi = BigInteger.ZERO;
        byte[] nb = new byte[1];

        if (placeHolder < data_chunk.length) {
            while ((ob = data_chunk[placeHolder]) < 0) {
                nb[0] = (byte) ((ob & 0x7f));
                bi = bi.shiftLeft(7).add(new BigInteger(nb));
                setPlaceHolder(placeHolder + 1);
            }
            nb[0] = (byte) ((ob & 0x7f));
            bi = bi.shiftLeft(7).add(new BigInteger(nb));
            setPlaceHolder(placeHolder + 1);
        }
        return bi.intValue();
    }

    /**
     * Sets chm directory listing entry list
     * 
     * @param dlel
     *            chm directory listing entry list
     */
    public void setDirectoryListingEntryList(List<DirectoryListingEntry> dlel) {
        this.dlel = dlel;
    }

    /**
     * Returns chm directory listing entry list
     * 
     * @return List<DirectoryListingEntry>
     */
    public List<DirectoryListingEntry> getDirectoryListingEntryList() {
        return dlel;
    }

    /**
     * Sets data
     * 
     * @param data
     */
    private void setData(byte[] data) {
        this.data = data;
    }

    /**
     * Returns data
     * 
     * @return
     */
    private byte[] getData() {
        return data;
    }

    /**
     * Sets data offset
     * 
     * @param dataOffset
     */
    private void setDataOffset(long dataOffset) {
        this.dataOffset = dataOffset;
    }

    /**
     * Returns data offset
     * 
     * @return dataOffset
     */
    public long getDataOffset() {
        return dataOffset;
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/chm/accessor/ChmItsfHeader.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.chm.accessor;

import java.io.UnsupportedEncodingException;
import java.math.BigInteger;

import org.apache.tika.exception.TikaException;
import org.apache.tika.parser.chm.assertion.ChmAssert;
import org.apache.tika.parser.chm.core.ChmConstants;
import org.apache.tika.parser.chm.exception.ChmParsingException;

/**
 * The Header 0000: char[4] 'ITSF' 0004: DWORD 3 (Version number) 0008: DWORD
 * Total header length, including header section table and following data. 000C:
 * DWORD 1 (unknown) 0010: DWORD a timestamp 0014: DWORD Windows Language ID
 * 0018: GUID {7C01FD10-7BAA-11D0-9E0C-00A0-C922-E6EC} 0028: GUID
 * {7C01FD11-7BAA-11D0-9E0C-00A0-C922-E6EC} Note: a GUID is $10 bytes, arranged
 * as 1 DWORD, 2 WORDs, and 8 BYTEs. 0000: QWORD Offset of section from
 * beginning of file 0008: QWORD Length of section Following the header section
 * table is 8 bytes of additional header data. In Version 2 files, this data is
 * not there and the content section starts immediately after the directory.
 * 
 * {@link http
 * ://translated.by/you/microsoft-s-html-help-chm-format-incomplete/original
 * /?show-translation-form=1}
 * 
 */
/* structure of ITSF headers */
public class ChmItsfHeader implements ChmAccessor<ChmItsfHeader> {
    private static final long serialVersionUID = 2215291838533213826L;
    private byte[] signature;
    private int version; /* 4 */
    private int header_len; /* 8 */
    private int unknown_000c; /* c */
    private long last_modified; /* 10 */
    private long lang_id; /* 14 */
    private byte[] dir_uuid = new byte[ChmConstants.BYTE_ARRAY_LENGHT]; /* 18 */
    private byte[] stream_uuid = new byte[ChmConstants.BYTE_ARRAY_LENGHT]; /* 28 */
    private long unknown_offset; /* 38 */
    private long unknown_len; /* 40 */
    private long dir_offset; /* 48 */
    private long dir_len; /* 50 */
    private long data_offset; /* 58 (Not present before V3) */

    /* local usage */
    private int dataRemained;
    private int currentPlace = 0;

    public ChmItsfHeader() {
        try {
            signature = ChmConstants.ITSF.getBytes("UTF-8"); /* 0 (ITSF) */
        } catch (UnsupportedEncodingException e) {
            throw new AssertionError("UTF-8 not supported.");
        }
    }

    /**
     * Prints the values of ChmfHeader
     */
    public String toString() {
        StringBuilder sb = new StringBuilder();
        try {
            sb.append(new String(getSignature(), "UTF-8") + " ");
        } catch (UnsupportedEncodingException e) {
            throw new AssertionError("UTF-8 not supported.");
        }
        sb.append(getVersion() + " ");
        sb.append(getHeaderLen() + " ");
        sb.append(getUnknown_000c() + " ");
        sb.append(getLastModified() + " ");
        sb.append(getLangId() + " ");
        sb.append(getDir_uuid() + " ");
        sb.append(getStream_uuid() + " ");
        sb.append(getUnknownOffset() + " ");
        sb.append(getUnknownLen() + " ");
        sb.append(getDirOffset() + " ");
        sb.append(getDirLen() + " ");
        sb.append(getDataOffset() + " ");
        return sb.toString();
    }

    /**
     * Returns a signature of itsf header
     * 
     * @return itsf header
     */
    public byte[] getSignature() {
        return signature;
    }

    /**
     * Sets itsf header signature
     * 
     * @param signature
     */
    protected void setSignature(byte[] signature) {
        this.signature = signature;
    }

    /**
     * Returns itsf header version
     * 
     * @return itsf version
     */
    public int getVersion() {
        return version;
    }

    /**
     * Sets itsf version
     * 
     * @param version
     */
    protected void setVersion(int version) {
        this.version = version;
    }

    /**
     * Returns itsf header length
     * 
     * @return length
     */
    public int getHeaderLen() {
        return header_len;
    }

    /**
     * Sets itsf header length
     * 
     * @param header_len
     */
    protected void setHeaderLen(int header_len) {
        this.header_len = header_len;
    }

    /**
     * Returns unknown_00c value
     * 
     * @return unknown_00c
     */
    public int getUnknown_000c() {
        return unknown_000c;
    }

    /**
     * Sets unknown_00c
     * 
     * @param unknown_000c
     */
    protected void setUnknown_000c(int unknown_000c) {
        this.unknown_000c = unknown_000c;
    }

    /**
     * Returns last modified date of the chm file
     * 
     * @return last modified date as long
     */
    public long getLastModified() {
        return last_modified;
    }

    /**
     * Sets last modified date of the chm file
     * 
     * @param last_modified
     */
    protected void setLastModified(long last_modified) {
        this.last_modified = last_modified;
    }

    /**
     * Returns language ID
     * 
     * @return language_id
     */
    public long getLangId() {
        return lang_id;
    }

    /**
     * Sets language_id
     * 
     * @param lang_id
     */
    protected void setLangId(long lang_id) {
        this.lang_id = lang_id;
    }

    /**
     * Returns directory uuid
     * 
     * @return dir_uuid
     */
    public byte[] getDir_uuid() {
        return dir_uuid;
    }

    /**
     * Sets directory uuid
     * 
     * @param dir_uuid
     */
    protected void setDir_uuid(byte[] dir_uuid) {
        this.dir_uuid = dir_uuid;
    }

    /**
     * Returns stream uuid
     * 
     * @return stream_uuid
     */
    public byte[] getStream_uuid() {
        return stream_uuid;
    }

    /**
     * Sets stream uuid
     * 
     * @param stream_uuid
     */
    protected void setStream_uuid(byte[] stream_uuid) {
        this.stream_uuid = stream_uuid;
    }

    /**
     * Returns unknown offset
     * 
     * @return unknown_offset
     */
    public long getUnknownOffset() {
        return unknown_offset;
    }

    /**
     * Sets unknown offset
     * 
     * @param unknown_offset
     */
    protected void setUnknownOffset(long unknown_offset) {
        this.unknown_offset = unknown_offset;
    }

    /**
     * Returns unknown length
     * 
     * @return unknown_length
     */
    public long getUnknownLen() {
        return unknown_len;
    }

    /**
     * Sets unknown length
     * 
     * @param unknown_len
     */
    protected void setUnknownLen(long unknown_len) {
        this.unknown_len = unknown_len;
    }

    /**
     * Returns directory offset
     * 
     * @return directory_offset
     */
    public long getDirOffset() {
        return dir_offset;
    }

    /**
     * Sets directory offset
     * 
     * @param dir_offset
     */
    protected void setDirOffset(long dir_offset) {
        this.dir_offset = dir_offset;
    }

    /**
     * Returns directory length
     * 
     * @return directory_offset
     */
    public long getDirLen() {
        return dir_len;
    }

    /**
     * Sets directory length
     * 
     * @param dir_len
     */
    protected void setDirLen(long dir_len) {
        this.dir_len = dir_len;
    }

    /**
     * Returns data offset
     * 
     * @return data_offset
     */
    public long getDataOffset() {
        return data_offset;
    }

    /**
     * Sets data offset
     * 
     * @param data_offset
     */
    protected void setDataOffset(long data_offset) {
        this.data_offset = data_offset;
    }

    /**
     * Copies 4 first bytes of the byte[]
     * 
     * @param data
     * @param chmItsfHeader
     * @param count
     * @throws TikaException 
     */
    private void unmarshalCharArray(byte[] data, ChmItsfHeader chmItsfHeader,
            int count) throws TikaException {
        ChmAssert.assertChmAccessorParameters(data, chmItsfHeader, count);
        System.arraycopy(data, 0, chmItsfHeader.signature, 0, count);
        this.setCurrentPlace(this.getCurrentPlace() + count);
        this.setDataRemained(this.getDataRemained() - count);
    }

    /**
     * Copies X bytes of source byte[] to the dest byte[]
     * 
     * @param data
     * @param dest
     * @param count
     * @return
     */
    private byte[] unmarshalUuid(byte[] data, byte[] dest, int count) {
        System.arraycopy(data, this.getCurrentPlace(), dest, 0, count);
        this.setCurrentPlace(this.getCurrentPlace() + count);
        this.setDataRemained(this.getDataRemained() - count);
        return dest;
    }

    /**
     * Takes 8 bytes and reverses them
     * 
     * @param data
     * @param dest
     * @return
     * @throws TikaException 
     */
    private long unmarshalUint64(byte[] data, long dest) throws TikaException{
        byte[] temp = new byte[8];
        int i, j;

        if (8 > this.getDataRemained())
            throw new TikaException("8 > this.getDataRemained()");

        for (i = 8, j = 7; i > 0; i--) {
            temp[j--] = data[this.getCurrentPlace()];
            this.setCurrentPlace(this.getCurrentPlace() + 1);
        }

        dest = new BigInteger(temp).longValue();
        this.setDataRemained(this.getDataRemained() - 8);
        return dest;
    }

    private int unmarshalInt32(byte[] data, int dest) throws TikaException{
        ChmAssert.assertByteArrayNotNull(data);

        if (4 > this.getDataRemained())
            throw new TikaException("4 > dataLenght");
        dest = (data[this.getCurrentPlace()] & 0xff)
                | (data[this.getCurrentPlace() + 1] & 0xff) << 8
                | (data[this.getCurrentPlace() + 2] & 0xff) << 16
                | (data[this.getCurrentPlace() + 3] & 0xff) << 24;

        this.setCurrentPlace(this.getCurrentPlace() + 4);
        this.setDataRemained(this.getDataRemained() - 4);
        return dest;
    }

    private long unmarshalUInt32(byte[] data, long dest) throws TikaException{
        ChmAssert.assertByteArrayNotNull(data);
        if (4 > getDataRemained())
            throw new TikaException("4 > dataLenght");
        dest = data[this.getCurrentPlace()]
                | data[this.getCurrentPlace() + 1] << 8
                | data[this.getCurrentPlace() + 2] << 16
                | data[this.getCurrentPlace() + 3] << 24;

        setDataRemained(this.getDataRemained() - 4);
        this.setCurrentPlace(this.getCurrentPlace() + 4);
        return dest;
    }

    public static void main(String[] args) {
    }

    /**
     * Sets data remained to be processed
     * 
     * @param dataRemained
     */
    private void setDataRemained(int dataRemained) {
        this.dataRemained = dataRemained;
    }

    /**
     * Returns data remained
     * 
     * @return data_remainned
     */
    private int getDataRemained() {
        return dataRemained;
    }

    /**
     * Sets current place in the byte[]
     * 
     * @param currentPlace
     */
    private void setCurrentPlace(int currentPlace) {
        this.currentPlace = currentPlace;
    }

    /**
     * Returns current place in the byte[]
     * 
     * @return current place
     */
    private int getCurrentPlace() {
        return currentPlace;
    }

    // @Override
    public void parse(byte[] data, ChmItsfHeader chmItsfHeader) throws TikaException {
        if (data.length < ChmConstants.CHM_ITSF_V2_LEN
                || data.length > ChmConstants.CHM_ITSF_V3_LEN)
            throw new TikaException("we only know how to deal with the 0x58 and 0x60 byte structures");

        chmItsfHeader.setDataRemained(data.length);
        chmItsfHeader.unmarshalCharArray(data, chmItsfHeader, ChmConstants.CHM_SIGNATURE_LEN);
        chmItsfHeader.setVersion(chmItsfHeader.unmarshalInt32(data, chmItsfHeader.getVersion()));
        chmItsfHeader.setHeaderLen(chmItsfHeader.unmarshalInt32(data, chmItsfHeader.getHeaderLen()));
        chmItsfHeader.setUnknown_000c(chmItsfHeader.unmarshalInt32(data, chmItsfHeader.getUnknown_000c()));
        chmItsfHeader.setLastModified(chmItsfHeader.unmarshalUInt32(data, chmItsfHeader.getLastModified()));
        chmItsfHeader.setLangId(chmItsfHeader.unmarshalUInt32(data, chmItsfHeader.getLangId()));
        chmItsfHeader.setDir_uuid(chmItsfHeader.unmarshalUuid(data, chmItsfHeader.getDir_uuid(), 16));
        chmItsfHeader.setStream_uuid(chmItsfHeader.unmarshalUuid(data, chmItsfHeader.getStream_uuid(), 16));
        chmItsfHeader.setUnknownOffset(chmItsfHeader.unmarshalUint64(data, chmItsfHeader.getUnknownOffset()));
        chmItsfHeader.setUnknownLen(chmItsfHeader.unmarshalUint64(data, chmItsfHeader.getUnknownLen()));
        chmItsfHeader.setDirOffset(chmItsfHeader.unmarshalUint64(data, chmItsfHeader.getDirOffset()));
        chmItsfHeader.setDirLen(chmItsfHeader.unmarshalUint64(data, chmItsfHeader.getDirLen()));
        try {
            if (!new String(chmItsfHeader.getSignature(), "UTF-8").equals(ChmConstants.ITSF))
                throw new TikaException("seems not valid file");
        } catch (UnsupportedEncodingException e) {
            throw new AssertionError("UTF-8 not supported.");
        }
        if (chmItsfHeader.getVersion() == ChmConstants.CHM_VER_2) {
            if (chmItsfHeader.getHeaderLen() < ChmConstants.CHM_ITSF_V2_LEN)
                throw new TikaException("something wrong with header");
        } else if (chmItsfHeader.getVersion() == ChmConstants.CHM_VER_3) {
            if (chmItsfHeader.getHeaderLen() < ChmConstants.CHM_ITSF_V3_LEN)
                throw new TikaException("unknown v3 header lenght");
        } else
            throw new ChmParsingException("unsupported chm format");

        /*
         * now, if we have a V3 structure, unmarshal the rest, otherwise,
         * compute it
         */
        if (chmItsfHeader.getVersion() == ChmConstants.CHM_VER_3) {
            if (chmItsfHeader.getDataRemained() >= 0)
                chmItsfHeader.setDataOffset(chmItsfHeader.getDirOffset()
                        + chmItsfHeader.getDirLen());
            else
                throw new TikaException("cannot set data offset, no data remained");
        } else
            chmItsfHeader.setDataOffset(chmItsfHeader.getDirOffset()
                    + chmItsfHeader.getDirLen());
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/chm/accessor/ChmItspHeader.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.chm.accessor;

import org.apache.tika.exception.TikaException;
import org.apache.tika.parser.chm.assertion.ChmAssert;
import org.apache.tika.parser.chm.core.ChmCommons;
import org.apache.tika.parser.chm.core.ChmConstants;
import org.apache.tika.parser.chm.exception.ChmParsingException;

import java.io.UnsupportedEncodingException;

/**
 * Directory header The directory starts with a header; its format is as
 * follows: 0000: char[4] 'ITSP' 0004: DWORD Version number 1 0008: DWORD Length
 * of the directory header 000C: DWORD $0a (unknown) 0010: DWORD $1000 Directory
 * chunk size 0014: DWORD "Density" of quickref section, usually 2 0018: DWORD
 * Depth of the index tree - 1 there is no index, 2 if there is one level of
 * PMGI chunks 001C: DWORD Chunk number of root index chunk, -1 if there is none
 * (though at least one file has 0 despite there being no index chunk, probably
 * a bug) 0020: DWORD Chunk number of first PMGL (listing) chunk 0024: DWORD
 * Chunk number of last PMGL (listing) chunk 0028: DWORD -1 (unknown) 002C:
 * DWORD Number of directory chunks (total) 0030: DWORD Windows language ID
 * 0034: GUID {5D02926A-212E-11D0-9DF9-00A0C922E6EC} 0044: DWORD $54 (This is
 * the length again) 0048: DWORD -1 (unknown) 004C: DWORD -1 (unknown) 0050:
 * DWORD -1 (unknown)
 * 
 * {@link http
 * ://translated.by/you/microsoft-s-html-help-chm-format-incomplete/original
 * /?show-translation-form=1}
 * 
 */
public class ChmItspHeader implements ChmAccessor<ChmItspHeader> {
    // TODO: refactor all unmarshals
    private static final long serialVersionUID = 1962394421998181341L;
    private byte[] signature;
    private int version; /* 4 */
    private int header_len; /* 8 */
    private int unknown_000c; /* c */
    private long block_len; /* 10 */
    private int blockidx_intvl; /* 14 */
    private int index_depth; /* 18 */
    private int index_root; /* 1c */
    private int index_head; /* 20 */
    private int unknown_0024; /* 24 */
    private long num_blocks; /* 28 */
    private int unknown_002c; /* 2c */
    private long lang_id; /* 30 */
    private byte[] system_uuid = new byte[ChmConstants.BYTE_ARRAY_LENGHT]; /* 34 */
    private byte[] unknown_0044 = new byte[ChmConstants.BYTE_ARRAY_LENGHT]; /* 44 */

    /* local usage */
    private int dataRemained;
    private int currentPlace = 0;

    public ChmItspHeader() {
        try {
            signature = ChmConstants.ITSP.getBytes("UTF-8"); /*
                                                                          * 0
                                                                          * (ITSP
                                                                          * )
                                                                          */
        } catch (UnsupportedEncodingException e) {
            throw new AssertionError("UTF-8 not supported.");
        }
    }

    public String toString() {
        StringBuilder sb = new StringBuilder();
        try {
            sb.append("[ signature:=" + new String(getSignature(), "UTF-8")
                    + System.getProperty("line.separator"));
        } catch (UnsupportedEncodingException e) {
            throw new AssertionError("UTF-8 not supported.");
        }
        sb.append("version:=\t" + getVersion()
                + System.getProperty("line.separator"));
        sb.append("header_len:=\t" + getHeader_len()
                + System.getProperty("line.separator"));
        sb.append("unknown_00c:=\t" + getUnknown_000c()
                + System.getProperty("line.separator"));
        sb.append("block_len:=\t" + getBlock_len() + " [directory chunk size]"
                + System.getProperty("line.separator"));
        sb.append("blockidx_intvl:=" + getBlockidx_intvl()
                + ", density of quickref section, usually 2"
                + System.getProperty("line.separator"));
        sb.append("index_depth:=\t"
                + getIndex_depth()
                + ", depth of the index tree - 1 there is no index, 2 if there is one level of PMGI chunk"
                + System.getProperty("line.separator"));
        sb.append("index_root:=\t" + getIndex_root()
                + ", chunk number of root index chunk, -1 if there is none"
                + System.getProperty("line.separator"));
        sb.append("index_head:=\t" + getIndex_head()
                + ", chunk number of first PMGL (listing) chunk"
                + System.getProperty("line.separator"));
        sb.append("unknown_0024:=\t" + getUnknown_0024()
                + ", chunk number of last PMGL (listing) chunk"
                + System.getProperty("line.separator"));
        sb.append("num_blocks:=\t" + getNum_blocks() + ", -1 (unknown)"
                + System.getProperty("line.separator"));
        sb.append("unknown_002c:=\t" + getUnknown_002c()
                + ", number of directory chunks (total)"
                + System.getProperty("line.separator"));
        sb.append("lang_id:=\t" + getLang_id() + " - "
                + ChmCommons.getLanguage(getLang_id())
                + System.getProperty("line.separator"));
        sb.append("system_uuid:=" + getSystem_uuid()
                + System.getProperty("line.separator"));
        sb.append("unknown_0044:=" + getUnknown_0044() + " ]");
        return sb.toString();
    }

    /**
     * Copies 4 bits from data[]
     * 
     * @param data
     * @param chmItspHeader
     * @param count
     * @throws TikaException 
     */
    private void unmarshalCharArray(byte[] data, ChmItspHeader chmItspHeader,
            int count) throws TikaException {
        ChmAssert.assertByteArrayNotNull(data);
        ChmAssert.assertChmAccessorNotNull(chmItspHeader);
        this.setDataRemained(data.length);
        System.arraycopy(data, 0, chmItspHeader.signature, 0, count);
        this.setCurrentPlace(this.getCurrentPlace() + count);
        this.setDataRemained(this.getDataRemained() - count);
    }

    private int unmarshalInt32(byte[] data, int dataLenght, int dest) throws TikaException {
        ChmAssert.assertByteArrayNotNull(data);
        if (4 > this.getDataRemained())
            throw new TikaException("4 > dataLenght");
        dest = (data[this.getCurrentPlace()] & 0xff)
                | (data[this.getCurrentPlace() + 1] & 0xff) << 8
                | (data[this.getCurrentPlace() + 2] & 0xff) << 16
                | (data[this.getCurrentPlace() + 3] & 0xff) << 24;

        this.setCurrentPlace(this.getCurrentPlace() + 4);
        this.setDataRemained(this.getDataRemained() - 4);
        return dest;
    }

    private long unmarshalUInt32(byte[] data, int dataLenght, long dest) throws TikaException {
        ChmAssert.assertByteArrayNotNull(data);
        if (4 > dataLenght)
            throw new TikaException("4 > dataLenght");
        dest = (data[this.getCurrentPlace()] & 0xff)
                | (data[this.getCurrentPlace() + 1] & 0xff) << 8
                | (data[this.getCurrentPlace() + 2] & 0xff) << 16
                | (data[this.getCurrentPlace() + 3] & 0xff) << 24;

        setDataRemained(this.getDataRemained() - 4);
        this.setCurrentPlace(this.getCurrentPlace() + 4);
        return dest;
    }

    private byte[] unmarshalUuid(byte[] data, int dataLenght, byte[] dest,
            int count) {
        System.arraycopy(data, this.getCurrentPlace(), dest, 0, count);
        this.setCurrentPlace(this.getCurrentPlace() + count);
        this.setDataRemained(this.getDataRemained() - count);
        return dest;
    }

    /**
     * Returns how many bytes remained
     * 
     * @return int
     */
    private int getDataRemained() {
        return dataRemained;
    }

    /**
     * Sets how many bytes remained
     * 
     * @param dataRemained
     */
    private void setDataRemained(int dataRemained) {
        this.dataRemained = dataRemained;
    }

    /**
     * Returns a place holder
     * 
     * @return current place
     */
    private int getCurrentPlace() {
        return currentPlace;
    }

    /**
     * Sets current place
     * 
     * @param currentPlace
     */
    private void setCurrentPlace(int currentPlace) {
        this.currentPlace = currentPlace;
    }

    /**
     * Returns a signature of the header
     * 
     * @return itsp signature
     */
    public byte[] getSignature() {
        return signature;
    }

    /**
     * Sets itsp signature
     * 
     * @param signature
     */
    protected void setSignature(byte[] signature) {
        this.signature = signature;
    }

    /**
     * Returns version of itsp header
     * 
     * @return version
     */
    public int getVersion() {
        return version;
    }

    /**
     * Sets a version of itsp header
     * 
     * @param version
     */
    protected void setVersion(int version) {
        this.version = version;
    }

    /**
     * Returns header length
     * 
     * @return header length
     */
    public int getHeader_len() {
        return header_len;
    }

    /**
     * Sets itsp header length
     * 
     * @param header_len
     */
    protected void setHeader_len(int header_len) {
        this.header_len = header_len;
    }

    /**
     * Returns 000c unknown bytes
     */
    public int getUnknown_000c() {
        return unknown_000c;
    }

    /**
     * Sets 000c unknown bytes Unknown means here that those guys who cracked
     * the chm format do not know what's it purposes for
     * 
     * @param unknown_000c
     */
    protected void setUnknown_000c(int unknown_000c) {
        this.unknown_000c = unknown_000c;
    }

    /**
     * Returns block's length
     * 
     * @return block_length
     */
    public long getBlock_len() {
        return block_len;
    }

    /**
     * Sets block length
     * 
     * @param block_len
     */
    protected void setBlock_len(long block_len) {
        this.block_len = block_len;
    }

    /**
     * Returns block index interval
     * 
     * @return blockidx_intvl
     */
    public int getBlockidx_intvl() {
        return blockidx_intvl;
    }

    /**
     * Sets block index interval
     * 
     * @param blockidx_intvl
     */
    protected void setBlockidx_intvl(int blockidx_intvl) {
        this.blockidx_intvl = blockidx_intvl;
    }

    /**
     * Returns an index depth
     * 
     * @return index_depth
     */
    public int getIndex_depth() {
        return index_depth;
    }

    /**
     * Sets an index depth
     * 
     * @param index_depth
     */
    protected void setIndex_depth(int index_depth) {
        this.index_depth = index_depth;
    }

    /**
     * Returns index root
     * 
     * @return index_root
     */
    public int getIndex_root() {
        return index_root;
    }

    /**
     * Sets an index root
     * 
     * @param index_root
     */
    protected void setIndex_root(int index_root) {
        this.index_root = index_root;
    }

    /**
     * Returns an index head
     * 
     * @return index_head
     */
    public int getIndex_head() {
        return index_head;
    }

    /**
     * Sets an index head
     * 
     * @param index_head
     */
    protected void setIndex_head(int index_head) {
        this.index_head = index_head;
    }

    /**
     * Returns 0024 unknown bytes
     * 
     * @return unknown_0024
     */
    public int getUnknown_0024() {
        return unknown_0024;
    }

    /**
     * Sets 0024 unknown bytes
     * 
     * @param unknown_0024
     */
    protected void setUnknown_0024(int unknown_0024) {
        this.unknown_0024 = unknown_0024;
    }

    /**
     * Returns number of blocks
     * 
     * @return num_blocks
     */
    public long getNum_blocks() {
        return num_blocks;
    }

    /**
     * Sets number of blocks containing in the chm file
     * 
     * @param num_blocks
     */
    protected void setNum_blocks(long num_blocks) {
        this.num_blocks = num_blocks;
    }

    /**
     * Returns 002c unknown bytes
     * 
     * @return unknown_002c
     */
    public int getUnknown_002c() {
        return unknown_002c;
    }

    /**
     * Sets 002c unknown bytes
     * 
     * @param unknown_002c
     */
    protected void setUnknown_002c(int unknown_002c) {
        this.unknown_002c = unknown_002c;
    }

    /**
     * Returns language id
     * 
     * @return lang_id
     */
    public long getLang_id() {
        return lang_id;
    }

    /**
     * Sets language id
     * 
     * @param lang_id
     */
    protected void setLang_id(long lang_id) {
        this.lang_id = lang_id;
    }

    /**
     * Returns system uuid
     * 
     * @return system_uuid
     */
    public byte[] getSystem_uuid() {
        return system_uuid;
    }

    /**
     * Sets system uuid
     * 
     * @param system_uuid
     */
    protected void setSystem_uuid(byte[] system_uuid) {
        this.system_uuid = system_uuid;
    }

    /**
     * Returns 0044 unknown bytes
     * 
     * @return unknown_0044
     */
    public byte[] getUnknown_0044() {
        return unknown_0044;
    }

    /**
     * Sets 0044 unknown bytes
     * 
     * @param unknown_0044
     */
    protected void setUnknown_0044(byte[] unknown_0044) {
        this.unknown_0044 = unknown_0044;
    }

    // @Override
    public void parse(byte[] data, ChmItspHeader chmItspHeader) throws TikaException {
        /* we only know how to deal with the 0x58 and 0x60 byte structures */
        if (data.length != ChmConstants.CHM_ITSP_V1_LEN)
            throw new ChmParsingException("we only know how to deal with the 0x58 and 0x60 byte structures");

        /* unmarshal common fields */
        chmItspHeader.unmarshalCharArray(data, chmItspHeader, ChmConstants.CHM_SIGNATURE_LEN);
        // ChmCommons.unmarshalCharArray(data, chmItspHeader,
        // ChmConstants.CHM_SIGNATURE_LEN);
        chmItspHeader.setVersion(chmItspHeader.unmarshalInt32(data,
                chmItspHeader.getDataRemained(), chmItspHeader.getVersion()));
        chmItspHeader
                .setHeader_len(chmItspHeader.unmarshalInt32(data,
                        chmItspHeader.getDataRemained(),
                        chmItspHeader.getHeader_len()));
        chmItspHeader.setUnknown_000c(chmItspHeader.unmarshalInt32(data,
                chmItspHeader.getDataRemained(),
                chmItspHeader.getUnknown_000c()));
        chmItspHeader.setBlock_len(chmItspHeader.unmarshalUInt32(data,
                chmItspHeader.getDataRemained(), chmItspHeader.getBlock_len()));
        chmItspHeader.setBlockidx_intvl(chmItspHeader.unmarshalInt32(data,
                chmItspHeader.getDataRemained(),
                chmItspHeader.getBlockidx_intvl()));
        chmItspHeader
                .setIndex_depth(chmItspHeader.unmarshalInt32(data,
                        chmItspHeader.getDataRemained(),
                        chmItspHeader.getIndex_depth()));
        chmItspHeader
                .setIndex_root(chmItspHeader.unmarshalInt32(data,
                        chmItspHeader.getDataRemained(),
                        chmItspHeader.getIndex_root()));
        chmItspHeader
                .setIndex_head(chmItspHeader.unmarshalInt32(data,
                        chmItspHeader.getDataRemained(),
                        chmItspHeader.getIndex_head()));
        chmItspHeader.setUnknown_0024(chmItspHeader.unmarshalInt32(data,
                chmItspHeader.getDataRemained(),
                chmItspHeader.getUnknown_0024()));
        chmItspHeader
                .setNum_blocks(chmItspHeader.unmarshalUInt32(data,
                        chmItspHeader.getDataRemained(),
                        chmItspHeader.getNum_blocks()));
        chmItspHeader.setUnknown_002c((chmItspHeader.unmarshalInt32(data,
                chmItspHeader.getDataRemained(),
                chmItspHeader.getUnknown_002c())));
        chmItspHeader.setLang_id(chmItspHeader.unmarshalUInt32(data,
                chmItspHeader.getDataRemained(), chmItspHeader.getLang_id()));
        chmItspHeader
                .setSystem_uuid(chmItspHeader.unmarshalUuid(data,
                        chmItspHeader.getDataRemained(),
                        chmItspHeader.getSystem_uuid(),
                        ChmConstants.BYTE_ARRAY_LENGHT));
        chmItspHeader
                .setUnknown_0044(chmItspHeader.unmarshalUuid(data,
                        chmItspHeader.getDataRemained(),
                        chmItspHeader.getUnknown_0044(),
                        ChmConstants.BYTE_ARRAY_LENGHT));

        /* Checks validity of the itsp header */
        try {
            if (!new String(chmItspHeader.getSignature(), "UTF-8").equals(ChmConstants.ITSP))
                throw new ChmParsingException("seems not valid signature");
        } catch (UnsupportedEncodingException e) {
            throw new AssertionError("UTF-8 not supported.");
        }
        if (chmItspHeader.getVersion() != ChmConstants.CHM_VER_1)
            throw new ChmParsingException("!=ChmConstants.CHM_VER_1");

        if (chmItspHeader.getHeader_len() != ChmConstants.CHM_ITSP_V1_LEN)
            throw new ChmParsingException("!= ChmConstants.CHM_ITSP_V1_LEN");
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/chm/accessor/ChmLzxcControlData.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.chm.accessor;

import org.apache.tika.exception.TikaException;
import org.apache.tika.parser.chm.assertion.ChmAssert;
import org.apache.tika.parser.chm.core.ChmConstants;
import org.apache.tika.parser.chm.exception.ChmParsingException;

import java.io.UnsupportedEncodingException;

/**
 * 
 * ::DataSpace/Storage/<SectionName>/ControlData This file contains $20 bytes of
 * information on the compression. The information is partially known: 0000:
 * DWORD 6 (unknown) 0004: ASCII 'LZXC' Compression type identifier 0008: DWORD
 * 2 (Possibly numeric code for LZX) 000C: DWORD The Huffman reset interval in
 * $8000-byte blocks 0010: DWORD The window size in $8000-byte blocks 0014:
 * DWORD unknown (sometimes 2, sometimes 1, sometimes 0) 0018: DWORD 0 (unknown)
 * 001C: DWORD 0 (unknown)
 * 
 * {@link http
 * ://translated.by/you/microsoft-s-html-help-chm-format-incomplete/original
 * /?page=2 }
 * 
 */
public class ChmLzxcControlData implements ChmAccessor<ChmLzxcControlData> {
    private static final long serialVersionUID = -7897854774939631565L;
    /* class' members */
    private long size; /* 0 */
    private byte[] signature;
    private long version; /* 8 */
    private long resetInterval; /* c */
    private long windowSize; /* 10 */
    private long windowsPerReset; /* 14 */
    private long unknown_18; /* 18 */

    /* local usage */
    private int dataRemained;
    private int currentPlace = 0;

    public ChmLzxcControlData() {
        try {
            signature = ChmConstants.LZXC.getBytes("UTF-8"); /*
                                                              * 4
                                                              * (LZXC
                                                              * )
                                                              */
        } catch (UnsupportedEncodingException e) {
            throw new AssertionError("UTF-8 not supported.");
        }
    }

    /**
     * Returns a remained data
     * 
     * @return dataRemained
     */
    private int getDataRemained() {
        return dataRemained;
    }

    /**
     * Sets a remained data
     * 
     * @param dataRemained
     */
    private void setDataRemained(int dataRemained) {
        this.dataRemained = dataRemained;
    }

    /**
     * Returns a place holder
     * 
     * @return current_place
     */
    private int getCurrentPlace() {
        return currentPlace;
    }

    /**
     * Sets a place holder
     * 
     * @param current_place
     */
    private void setCurrentPlace(int currentPlace) {
        this.currentPlace = currentPlace;
    }

    /**
     * Returns a size of control data
     * 
     * @return size
     */
    public long getSize() {
        return size;
    }

    /**
     * Sets a size of control data
     * 
     * @param size
     */
    protected void setSize(long size) {
        this.size = size;
    }

    /**
     * Returns a signature of control data block
     * 
     * @return signature
     */
    public byte[] getSignature() {
        return signature;
    }

    /**
     * Sets a signature of control data block
     * 
     * @param signature
     */
    protected void setSignature(byte[] signature) {
        this.signature = signature;
    }

    /**
     * Returns a version of control data block
     * 
     * @return version
     */
    public long getVersion() {
        return version;
    }

    /**
     * Sets version of control data block
     * 
     * @param version
     */
    protected void setVersion(long version) {
        this.version = version;
    }

    /**
     * Returns reset interval
     * 
     * @return reset_interval
     */
    public long getResetInterval() {
        return resetInterval;
    }

    /**
     * Sets a reset interval
     * 
     * @param resetInterval
     */
    protected void setResetInterval(long resetInterval) {
        this.resetInterval = resetInterval;
    }

    /**
     * Returns a window size
     * 
     * @return window_size
     */
    public long getWindowSize() {
        return windowSize;
    }

    /**
     * Sets a window size
     * 
     * @param window_size
     */
    protected void setWindowSize(long windowSize) {
        this.windowSize = windowSize;
    }

    /**
     * Returns windows per reset
     * 
     * @return
     */
    public long getWindowsPerReset() {
        return windowsPerReset;
    }

    /**
     * Sets windows per reset
     * 
     * @param windows_per_reset
     */
    protected void setWindowsPerReset(long windowsPerReset) {
        this.windowsPerReset = windowsPerReset;
    }

    /**
     * Returns unknown 18 bytes
     * 
     * @return unknown_18
     */
    public long getUnknown_18() {
        return unknown_18;
    }

    /**
     * Sets unknown 18 bytes
     * 
     * @param unknown_18
     */
    protected void setUnknown_18(long unknown_18) {
        this.unknown_18 = unknown_18;
    }

    private long unmarshalUInt32(byte[] data, long dest) throws ChmParsingException {
        assert (data != null && data.length > 0);
        if (4 > getDataRemained())
            throw new ChmParsingException("4 > dataLenght");
        dest = data[this.getCurrentPlace()]
                | data[this.getCurrentPlace() + 1] << 8
                | data[this.getCurrentPlace() + 2] << 16
                | data[this.getCurrentPlace() + 3] << 24;

        setDataRemained(this.getDataRemained() - 4);
        this.setCurrentPlace(this.getCurrentPlace() + 4);
        return dest;
    }

    private void unmarshalCharArray(byte[] data,
            ChmLzxcControlData chmLzxcControlData, int count) throws TikaException {
        ChmAssert.assertByteArrayNotNull(data);
        ChmAssert.assertChmAccessorNotNull(chmLzxcControlData);
        ChmAssert.assertPositiveInt(count);
        System.arraycopy(data, 4, chmLzxcControlData.getSignature(), 0, count);
        this.setCurrentPlace(this.getCurrentPlace() + count);
        this.setDataRemained(this.getDataRemained() - count);
    }

    /**
     * Returns textual representation of ChmLzxcControlData
     */
    public String toString() {
        StringBuilder sb = new StringBuilder();
        sb.append("size(unknown):=" + this.getSize() + ", ");
        try {
            sb.append("signature(Compression type identifier):="
                    + new String(this.getSignature(), "UTF-8") + ", ");
        } catch (UnsupportedEncodingException e) {
            throw new AssertionError("UTF-8 not supported.");
        }
        sb.append("version(Possibly numeric code for LZX):="
                + this.getVersion() + System.getProperty("line.separator"));
        sb.append("resetInterval(The Huffman reset interval):="
                + this.getResetInterval() + ", ");
        sb.append("windowSize:=" + this.getWindowSize() + ", ");
        sb.append("windowsPerReset(unknown (sometimes 2, sometimes 1, sometimes 0):="
                + this.getWindowsPerReset() + ", ");
        sb.append("unknown_18:=" + this.getUnknown_18()
                + System.getProperty("line.separator"));
        return sb.toString();
    }

    // @Override
    public void parse(byte[] data, ChmLzxcControlData chmLzxcControlData) throws TikaException {
        if (data == null || (data.length < ChmConstants.CHM_LZXC_MIN_LEN))
            throw new ChmParsingException("we want at least 0x18 bytes");
        chmLzxcControlData.setDataRemained(data.length);
        chmLzxcControlData.setSize(unmarshalUInt32(data, chmLzxcControlData.getSize()));
        chmLzxcControlData.unmarshalCharArray(data, chmLzxcControlData,
                ChmConstants.CHM_SIGNATURE_LEN);
        chmLzxcControlData.setVersion(unmarshalUInt32(data,
                chmLzxcControlData.getVersion()));
        chmLzxcControlData.setResetInterval(unmarshalUInt32(data,
                chmLzxcControlData.getResetInterval()));
        chmLzxcControlData.setWindowSize(unmarshalUInt32(data,
                chmLzxcControlData.getWindowSize()));
        chmLzxcControlData.setWindowsPerReset(unmarshalUInt32(data,
                chmLzxcControlData.getWindowsPerReset()));

        if (data.length >= ChmConstants.CHM_LZXC_V2_LEN)
            chmLzxcControlData.setUnknown_18(unmarshalUInt32(data,
                    chmLzxcControlData.getUnknown_18()));
        else
            chmLzxcControlData.setUnknown_18(0);

        if (chmLzxcControlData.getVersion() == 2) {
            chmLzxcControlData.setWindowSize(getWindowSize()
                    * ChmConstants.CHM_WINDOW_SIZE_BLOCK);
        }

        if (chmLzxcControlData.getWindowSize() == 0
                || chmLzxcControlData.getResetInterval() == 0)
            throw new ChmParsingException(
                    "window size / resetInterval should be more than zero");

        if (chmLzxcControlData.getWindowSize() == 1)
            throw new ChmParsingException(
                    "window size / resetInterval should be more than 1");

        /* checks a signature */
        try {
            if (!new String(chmLzxcControlData.getSignature(), "UTF-8")
                    .equals(ChmConstants.LZXC))
                throw new ChmParsingException(
                        "the signature does not seem to be correct");
        } catch (UnsupportedEncodingException e) {
            throw new AssertionError("UTF-8 not supported.");
        }
    }

    /**
     * @param args
     */
    public static void main(String[] args) {
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/chm/accessor/ChmLzxcResetTable.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.chm.accessor;

import java.math.BigInteger;
import java.util.Arrays;

import org.apache.tika.exception.TikaException;
import org.apache.tika.parser.chm.assertion.ChmAssert;
import org.apache.tika.parser.chm.core.ChmConstants;
import org.apache.tika.parser.chm.exception.ChmParsingException;

/**
 * LZXC reset table For ensuring a decompression. Reads the block named
 * "::DataSpace/Storage/<SectionName>/Transform/{7FC28940-9D31-11D0-9B27-00A0C91E9C7C}/InstanceData/ResetTable"
 * .
 * 
 * {@link http
 * ://translated.by/you/microsoft-s-html-help-chm-format-incomplete/original
 * /?page=2 }
 * 
 */
public class ChmLzxcResetTable implements ChmAccessor<ChmLzxcResetTable> {
    private static final long serialVersionUID = -8209574429411707460L;
    /* class members */
    private long version; // 0000: DWORD 2 unknown (possibly a version number)
    private long block_count; // 0004: DWORD Number of entries in reset table
    private long unknown; // 0008: DWORD 8 unknown
    private long table_offset; // 000C: DWORD $28 Length of table header (area
                               // before table entries)
    private long uncompressed_len; // 0010: QWORD Uncompressed Length
    private long compressed_len; // 0018: QWORD Compressed Length
    private long block_len; // 0020: QWORD 0x8000 block size for locations below
    private long[] block_address;

    /* local usage */
    private int dataRemained;
    private int currentPlace = 0;

    private int getDataRemained() {
        return dataRemained;
    }

    private void setDataRemained(int dataRemained) {
        this.dataRemained = dataRemained;
    }

    /**
     * Returns block addresses
     * 
     * @return block addresses
     */
    public long[] getBlockAddress() {
        return block_address;
    }

    /**
     * Sets block addresses
     * 
     * @param block_address
     */
    public void setBlockAddress(long[] block_address) {
        this.block_address = block_address;
    }

    private int getCurrentPlace() {
        return currentPlace;
    }

    private void setCurrentPlace(int currentPlace) {
        this.currentPlace = currentPlace;
    }

    @Override
    public String toString() {
        StringBuilder sb = new StringBuilder();
        sb.append("version:=" + getVersion()
                + System.getProperty("line.separator"));
        sb.append("block_count:=" + getBlockCount()
                + System.getProperty("line.separator"));
        sb.append("unknown:=" + getUnknown()
                + System.getProperty("line.separator"));
        sb.append("table_offset:=" + getTableOffset()
                + System.getProperty("line.separator"));
        sb.append("uncompressed_len:=" + getUncompressedLen()
                + System.getProperty("line.separator"));
        sb.append("compressed_len:=" + getCompressedLen()
                + System.getProperty("line.separator"));
        sb.append("block_len:=" + getBlockLen()
                + System.getProperty("line.separator"));
        sb.append("block_addresses:=" + Arrays.toString(getBlockAddress()));
        return sb.toString();
    }

    /**
     * Enumerates chm block addresses
     * 
     * @param data
     * 
     * @return byte[] of addresses
     * @throws TikaException 
     */
    private long[] enumerateBlockAddresses(byte[] data) throws TikaException {
        ChmAssert.assertByteArrayNotNull(data);
        /* we have limit of number of blocks to be extracted */
        if (getBlockCount() > 5000)
            setBlockCount(5000);

        if (getBlockCount() < 0 && (getDataRemained() / 8) > 0)
            setBlockCount(getDataRemained() / 8);

        long[] addresses = new long[(int) getBlockCount()];
        int rem = getDataRemained() / 8;
        for (int i = 0; i < rem; i++) {
            long num = -1;

            try {
                addresses[i] = unmarshalUint64(data, num);
            } catch (Exception e) {
                throw new TikaException(e.getMessage());
            }
        }
        return addresses;
    }

    /**
     * Validates parameters such as byte[] and chm lzxc reset table
     * 
     * @param data
     * @param chmLzxcResetTable
     * 
     * @return boolean
     * @throws TikaException 
     */
    private boolean validateParamaters(byte[] data,
            ChmLzxcResetTable chmLzxcResetTable) throws TikaException {
        int goodParameter = 0;
        ChmAssert.assertByteArrayNotNull(data);
        ++goodParameter;
        ChmAssert.assertChmAccessorNotNull(chmLzxcResetTable);
        ++goodParameter;
        return (goodParameter == 2);
    }

    private long unmarshalUInt32(byte[] data, long dest) throws TikaException {
        ChmAssert.assertByteArrayNotNull(data);
        dest = (data[this.getCurrentPlace()] & 0xff)
                | (data[this.getCurrentPlace() + 1] & 0xff) << 8
                | (data[this.getCurrentPlace() + 2] & 0xff) << 16
                | (data[this.getCurrentPlace() + 3] & 0xff) << 24;

        setDataRemained(this.getDataRemained() - 4);
        this.setCurrentPlace(this.getCurrentPlace() + 4);
        return dest;
    }

    private long unmarshalUint64(byte[] data, long dest) throws TikaException {
        ChmAssert.assertByteArrayNotNull(data);
        byte[] temp = new byte[8];
        int i, j;// counters

        for (i = 8, j = 7; i > 0; i--) {
            if (data.length > this.getCurrentPlace()) {
                temp[j--] = data[this.getCurrentPlace()];
                this.setCurrentPlace(this.getCurrentPlace() + 1);
            } else
                throw new TikaException("data is too small to calculate address block");
        }
        dest = new BigInteger(temp).longValue();
        this.setDataRemained(this.getDataRemained() - 8);
        return dest;
    }

    /**
     * Returns the version
     * 
     * @return - long
     */
    public long getVersion() {
        return version;
    }

    /**
     * Sets the version
     * 
     * @param version
     *            - long
     */
    public void setVersion(long version) {
        this.version = version;
    }

    /**
     * Gets a block count
     * 
     * @return - int
     */
    public long getBlockCount() {
        return block_count;
    }

    /**
     * Sets a block count
     * 
     * @param block_count
     *            - long
     */
    public void setBlockCount(long block_count) {
        this.block_count = block_count;
    }

    /**
     * Gets unknown
     * 
     * @return - long
     */
    public long getUnknown() {
        return unknown;
    }

    /**
     * Sets an unknown
     * 
     * @param unknown
     *            - long
     */
    public void setUnknown(long unknown) {
        this.unknown = unknown;
    }

    /**
     * Gets a table offset
     * 
     * @return - long
     */
    public long getTableOffset() {
        return table_offset;
    }

    /**
     * Sets a table offset
     * 
     * @param table_offset
     *            - long
     */
    public void setTableOffset(long table_offset) {
        this.table_offset = table_offset;
    }

    /**
     * Gets uncompressed length
     * 
     * @return - {@link BigInteger }
     */
    public long getUncompressedLen() {
        return uncompressed_len;
    }

    /**
     * Sets uncompressed length
     * 
     * @param uncompressed_len
     *            - {@link BigInteger}
     */
    public void setUncompressedLen(long uncompressed_len) {
        this.uncompressed_len = uncompressed_len;
    }

    /**
     * Gets compressed length
     * 
     * @return - {@link BigInteger}
     */
    public long getCompressedLen() {
        return compressed_len;
    }

    /**
     * Sets compressed length
     * 
     * @param compressed_len
     *            - {@link BigInteger}
     */
    public void setCompressedLen(long compressed_len) {
        this.compressed_len = compressed_len;
    }

    /**
     * Gets a block length
     * 
     * @return - {@link BigInteger}
     */
    public long getBlockLen() {
        return block_len;
    }

    /**
     * Sets a block length
     * 
     * @param block_len
     *            - {@link BigInteger}
     */
    public void setBlockLlen(long block_len) {
        this.block_len = block_len;
    }

    // @Override
    public void parse(byte[] data, ChmLzxcResetTable chmLzxcResetTable) throws TikaException {
        setDataRemained(data.length);
        if (validateParamaters(data, chmLzxcResetTable)) {
            /* unmarshal fields */
            chmLzxcResetTable.setVersion(unmarshalUInt32(data, chmLzxcResetTable.getVersion()));
            chmLzxcResetTable.setBlockCount(unmarshalUInt32(data, chmLzxcResetTable.getBlockCount()));
            chmLzxcResetTable.setUnknown(unmarshalUInt32(data, chmLzxcResetTable.getUnknown()));
            chmLzxcResetTable.setTableOffset(unmarshalUInt32(data, chmLzxcResetTable.getTableOffset()));
            chmLzxcResetTable.setUncompressedLen(unmarshalUint64(data, chmLzxcResetTable.getUncompressedLen()));
            chmLzxcResetTable.setCompressedLen(unmarshalUint64(data, chmLzxcResetTable.getCompressedLen()));
            chmLzxcResetTable.setBlockLlen(unmarshalUint64(data, chmLzxcResetTable.getBlockLen()));
            chmLzxcResetTable.setBlockAddress(enumerateBlockAddresses(data));
        }

        /* checks chmLzxcResetTable */
        if (chmLzxcResetTable.getVersion() != ChmConstants.CHM_VER_2)
            throw new ChmParsingException(
                    "does not seem currect version of chmLzxcResetTable");
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/chm/accessor/ChmPmgiHeader.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.chm.accessor;

import java.io.UnsupportedEncodingException;
import java.util.Arrays;

import org.apache.tika.exception.TikaException;
import org.apache.tika.parser.chm.assertion.ChmAssert;
import org.apache.tika.parser.chm.core.ChmCommons;
import org.apache.tika.parser.chm.core.ChmConstants;
import org.apache.tika.parser.chm.exception.ChmParsingException;

/**
 * Description Note: not always exists An index chunk has the following format:
 * 0000: char[4] 'PMGI' 0004: DWORD Length of quickref/free area at end of
 * directory chunk 0008: Directory index entries (to quickref/free area) The
 * quickref area in an PMGI is the same as in an PMGL The format of a directory
 * index entry is as follows: BYTE: length of name BYTEs: name (UTF-8 encoded)
 * ENCINT: directory listing chunk which starts with name Encoded Integers aka
 * ENCINT An ENCINT is a variable-length integer. The high bit of each byte
 * indicates "continued to the next byte". Bytes are stored most significant to
 * least significant. So, for example, $EA $15 is (((0xEA&0x7F)<<7)|0x15) =
 * 0x3515.
 * 
 * <p>
 * Note: This class is not in use
 * 
 * {@link http://translated.by/you/microsoft-s-html-help-chm-format-incomplete/original/?show-translation-form=1 }
 * 
 * 
 */
public class ChmPmgiHeader implements ChmAccessor<ChmPmgiHeader> {
    private static final long serialVersionUID = -2092282339894303701L;
    private byte[] signature;
    private long free_space; /* 4 */

    /* local usage */
    private int dataRemained;
    private int currentPlace = 0;

    public ChmPmgiHeader() {
        try {
            signature = ChmConstants.CHM_PMGI_MARKER.getBytes("UTF-8"); /* 0 (PMGI) */
        } catch (UnsupportedEncodingException e) {
            throw new AssertionError("UTF-8 not supported.");
        }
    }

    private int getDataRemained() {
        return dataRemained;
    }

    private void setDataRemained(int dataRemained) {
        this.dataRemained = dataRemained;
    }

    private int getCurrentPlace() {
        return currentPlace;
    }

    private void setCurrentPlace(int currentPlace) {
        this.currentPlace = currentPlace;
    }

    private void unmarshalCharArray(byte[] data, ChmPmgiHeader chmPmgiHeader,
            int count) throws ChmParsingException {
        int index = -1;
        ChmAssert.assertByteArrayNotNull(data);
        ChmAssert.assertChmAccessorNotNull(chmPmgiHeader);
        ChmAssert.assertPositiveInt(count);
        this.setDataRemained(data.length);
        try {
            index = ChmCommons.indexOf(data,
                    ChmConstants.CHM_PMGI_MARKER.getBytes("UTF-8"));
        } catch (UnsupportedEncodingException e) {
            throw new AssertionError("UTF-8 not supported.");
        }
        if (index >= 0)
            System.arraycopy(data, index, chmPmgiHeader.getSignature(), 0, count);
        else{
            //Some chm documents (actually most of them) do not contain
            //PMGI header, in this case, we just notice about it.
        }
        this.setCurrentPlace(this.getCurrentPlace() + count);
        this.setDataRemained(this.getDataRemained() - count);
    }

    private long unmarshalUInt32(byte[] data, long dest) throws ChmParsingException {
        ChmAssert.assertByteArrayNotNull(data);

        if (4 > getDataRemained())
            throw new ChmParsingException("4 > dataLenght");
        dest = (data[this.getCurrentPlace()] & 0xff)
                | (data[this.getCurrentPlace() + 1] & 0xff) << 8
                | (data[this.getCurrentPlace() + 2] & 0xff) << 16
                | (data[this.getCurrentPlace() + 3] & 0xff) << 24;

        setDataRemained(this.getDataRemained() - 4);
        this.setCurrentPlace(this.getCurrentPlace() + 4);
        return dest;
    }

    /**
     * Returns pmgi signature if exists
     * 
     * @return signature
     */
    public byte[] getSignature() {
        return signature;
    }

    /**
     * Sets pmgi signature
     * 
     * @param signature
     */
    protected void setSignature(byte[] signature) {
        this.signature = signature;
    }

    /**
     * Returns pmgi free space
     * 
     * @return free_space
     */
    public long getFreeSpace() {
        return free_space;
    }

    /**
     * Sets pmgi free space
     * 
     * @param free_space
     */
    protected void setFreeSpace(long free_space) {
        this.free_space = free_space;
    }

    /**
     * Returns textual representation of the pmgi header
     */
    public String toString() {
        StringBuilder sb = new StringBuilder();
        try {
            sb.append("signature:=" + new String(getSignature(), "UTF-8") + ", ");
        } catch (UnsupportedEncodingException e) {
            throw new AssertionError("UTF-8 not supported.");
        }
        sb.append("free space:=" + getFreeSpace()
                + System.getProperty("line.separator"));
        return sb.toString();
    }

    // @Override
    public void parse(byte[] data, ChmPmgiHeader chmPmgiHeader) throws TikaException {
        /* we only know how to deal with a 0x8 byte structures */
        if (data.length < ChmConstants.CHM_PMGI_LEN)
            throw new TikaException("we only know how to deal with a 0x8 byte structures");

        /* unmarshal fields */
        chmPmgiHeader.unmarshalCharArray(data, chmPmgiHeader, ChmConstants.CHM_SIGNATURE_LEN);
        chmPmgiHeader.setFreeSpace(chmPmgiHeader.unmarshalUInt32(data, chmPmgiHeader.getFreeSpace()));

        /* check structure */
        try {
            if (!Arrays.equals(chmPmgiHeader.getSignature(),
                    ChmConstants.CHM_PMGI_MARKER.getBytes("UTF-8")))
                throw new TikaException(
                        "it does not seem to be valid a PMGI signature, check ChmItsp index_root if it was -1, means no PMGI, use PMGL insted");
        } catch (UnsupportedEncodingException e) {
            throw new AssertionError("UTF-8 not supported.");
        }

    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/chm/accessor/ChmPmglHeader.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.chm.accessor;

import java.io.UnsupportedEncodingException;

import org.apache.tika.exception.TikaException;
import org.apache.tika.parser.chm.assertion.ChmAssert;
import org.apache.tika.parser.chm.core.ChmConstants;
import org.apache.tika.parser.chm.exception.ChmParsingException;

/**
 * Description There are two types of directory chunks -- index chunks, and
 * listing chunks. The index chunk will be omitted if there is only one listing
 * chunk. A listing chunk has the following format: 0000: char[4] 'PMGL' 0004:
 * DWORD Length of free space and/or quickref area at end of directory chunk
 * 0008: DWORD Always 0 000C: DWORD Chunk number of previous listing chunk when
 * reading directory in sequence (-1 if this is the first listing chunk) 0010:
 * DWORD Chunk number of next listing chunk when reading directory in sequence
 * (-1 if this is the last listing chunk) 0014: Directory listing entries (to
 * quickref area) Sorted by filename; the sort is case-insensitive The quickref
 * area is written backwards from the end of the chunk. One quickref entry
 * exists for every n entries in the file, where n is calculated as 1 + (1 <<
 * quickref density). So for density = 2, n = 5 Chunklen-0002: WORD Number of
 * entries in the chunk Chunklen-0004: WORD Offset of entry n from entry 0
 * Chunklen-0008: WORD Offset of entry 2n from entry 0 Chunklen-000C: WORD
 * Offset of entry 3n from entry 0 ... The format of a directory listing entry
 * is as follows BYTE: length of name BYTEs: name (UTF-8 encoded) ENCINT:
 * content section ENCINT: offset ENCINT: length The offset is from the
 * beginning of the content section the file is in, after the section has been
 * decompressed (if appropriate). The length also refers to length of the file
 * in the section after decompression. There are two kinds of file represented
 * in the directory: user data and format related files. The files which are
 * format-related have names which begin with '::', the user data files have
 * names which begin with "/".
 * 
 * {@link http
 * ://translated.by/you/microsoft-s-html-help-chm-format-incomplete/original
 * /?show-translation-form=1 }
 * 
 * @author olegt
 * 
 */
public class ChmPmglHeader implements ChmAccessor<ChmPmglHeader> {
    private static final long serialVersionUID = -6139486487475923593L;
    private byte[] signature;
    private long free_space; /* 4 */
    private long unknown_0008; /* 8 */
    private int block_prev; /* c */
    private int block_next; /* 10 */

    /* local usage */
    private int dataRemained;
    private int currentPlace = 0;

    public ChmPmglHeader() {
        try {
            signature = ChmConstants.PMGL.getBytes("UTF-8"); /*
                                                                          * 0
                                                                          * (PMGL
                                                                          * )
                                                                          */
        } catch (UnsupportedEncodingException e) {
            throw new AssertionError("UTF-8 not supported.");
        }
    }

    private int getDataRemained() {
        return dataRemained;
    }

    private void setDataRemained(int dataRemained) {
        this.dataRemained = dataRemained;
    }

    private int getCurrentPlace() {
        return currentPlace;
    }

    private void setCurrentPlace(int currentPlace) {
        this.currentPlace = currentPlace;
    }

    public long getFreeSpace() {
        return free_space;
    }

    public void setFreeSpace(long free_space) throws TikaException {
        if (free_space < 0) {
            throw new TikaException("Bad PMGLheader.FreeSpace="+free_space);
        }
        this.free_space = free_space;
    }

    public String toString() {
        StringBuilder sb = new StringBuilder();
        try {
            sb.append("signatute:=" + new String(getSignature(), "UTF-8") + ", ");
        } catch (UnsupportedEncodingException e) {
            throw new AssertionError("UTF-8 not supported.");
        }
        sb.append("free space:=" + getFreeSpace() + ", ");
        sb.append("unknown0008:=" + getUnknown0008() + ", ");
        sb.append("prev block:=" + getBlockPrev() + ", ");
        sb.append("next block:=" + getBlockNext()
                + System.getProperty("line.separator"));
        return sb.toString();
    }

    protected void unmarshalCharArray(byte[] data, ChmPmglHeader chmPmglHeader,
            int count) throws TikaException {
        ChmAssert.assertByteArrayNotNull(data);
        this.setDataRemained(data.length);
        System.arraycopy(data, 0, chmPmglHeader.signature, 0, count);
        this.setCurrentPlace(this.getCurrentPlace() + count);
        this.setDataRemained(this.getDataRemained() - count);
    }

    private int unmarshalInt32(byte[] data) throws TikaException {
        ChmAssert.assertByteArrayNotNull(data);
        int dest;
        if (4 > this.getDataRemained())
            throw new TikaException("4 > dataLenght");
        dest = (data[this.getCurrentPlace()] & 0xff)
                | (data[this.getCurrentPlace() + 1] & 0xff) << 8
                | (data[this.getCurrentPlace() + 2] & 0xff) << 16
                | (data[this.getCurrentPlace() + 3] & 0xff) << 24;

        this.setCurrentPlace(this.getCurrentPlace() + 4);
        this.setDataRemained(this.getDataRemained() - 4);
        return dest;
    }

    private long unmarshalUInt32(byte[] data) throws ChmParsingException {
        ChmAssert.assertByteArrayNotNull(data);
        long dest;
        if (4 > getDataRemained())
            throw new ChmParsingException("4 > dataLenght");
        dest = (data[this.getCurrentPlace()] & 0xff)
                | (data[this.getCurrentPlace() + 1] & 0xff) << 8
                | (data[this.getCurrentPlace() + 2] & 0xff) << 16
                | (data[this.getCurrentPlace() + 3] & 0xff) << 24;

        setDataRemained(this.getDataRemained() - 4);
        this.setCurrentPlace(this.getCurrentPlace() + 4);
        return dest;
    }

    // @Override
    public void parse(byte[] data, ChmPmglHeader chmPmglHeader) throws TikaException {
        if (data.length < ChmConstants.CHM_PMGL_LEN)
            throw new TikaException(ChmPmglHeader.class.getName()
                    + " we only know how to deal with a 0x14 byte structures");

        /* unmarshal fields */
        chmPmglHeader.unmarshalCharArray(data, chmPmglHeader,
                ChmConstants.CHM_SIGNATURE_LEN);
        chmPmglHeader.setFreeSpace(chmPmglHeader.unmarshalUInt32(data));
        chmPmglHeader.setUnknown0008(chmPmglHeader.unmarshalUInt32(data));
        chmPmglHeader.setBlockPrev(chmPmglHeader.unmarshalInt32(data));
        chmPmglHeader.setBlockNext(chmPmglHeader.unmarshalInt32(data));

        /* check structure */
        try {
            if (!new String(chmPmglHeader.getSignature(), "UTF-8").equals(ChmConstants.PMGL))
                throw new ChmParsingException(ChmPmglHeader.class.getName()
                        + " pmgl != pmgl.signature");
        } catch (UnsupportedEncodingException e) {
            throw new AssertionError("UTF-8 not supported.");
        }
    }

    public byte[] getSignature() {
        return signature;
    }

    protected void setSignature(byte[] signature) {
        this.signature = signature;
    }

    public long getUnknown0008() {
        return unknown_0008;
    }

    protected void setUnknown0008(long unknown_0008) {
        this.unknown_0008 = unknown_0008;
    }

    public int getBlockPrev() {
        return block_prev;
    }

    protected void setBlockPrev(int block_prev) {
        this.block_prev = block_prev;
    }

    public int getBlockNext() {
        return block_next;
    }

    protected void setBlockNext(int block_next) {
        this.block_next = block_next;
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/chm/accessor/DirectoryListingEntry.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.chm.accessor;

import org.apache.tika.exception.TikaException;
import org.apache.tika.parser.chm.assertion.ChmAssert;
import org.apache.tika.parser.chm.core.ChmCommons;

/**
 * The format of a directory listing entry is as follows: BYTE: length of name
 * BYTEs: name (UTF-8 encoded) ENCINT: content section ENCINT: offset ENCINT:
 * length The offset is from the beginning of the content section the file is
 * in, after the section has been decompressed (if appropriate). The length also
 * refers to length of the file in the section after decompression. There are
 * two kinds of file represented in the directory: user data and format related
 * files. The files which are format-related have names which begin with '::',
 * the user data files have names which begin with "/".
 * 
 */
public class DirectoryListingEntry {
    /* Length of the entry name */
    private int name_length;
    /* Entry name or directory name */
    private String name;
    /* Entry type */
    private ChmCommons.EntryType entryType;
    /* Entry offset */
    private int offset;
    /* Entry size */
    private int length;

    public DirectoryListingEntry() {

    }

    /**
     * Constructs directoryListingEntry
     * 
     * @param name_length
     *            int
     * @param name
     *            String
     * @param isCompressed
     *            ChmCommons.EntryType
     * @param offset
     *            int
     * @param length
     *            int
     * @throws TikaException 
     */
    public DirectoryListingEntry(int name_length, String name,
            ChmCommons.EntryType isCompressed, int offset, int length) throws TikaException {
        ChmAssert.assertDirectoryListingEntry(name_length, name, isCompressed, offset, length);
        setNameLength(name_length);
        setName(name);
        setEntryType(isCompressed);
        setOffset(offset);
        setLength(length);
    }

    public String toString() {
        StringBuilder sb = new StringBuilder();
        sb.append("name_length:=" + getNameLength() + System.getProperty("line.separator"));
        sb.append("name:=" + getName() + System.getProperty("line.separator"));
        sb.append("entryType:=" + getEntryType() + System.getProperty("line.separator"));
        sb.append("offset:=" + getOffset() + System.getProperty("line.separator"));
        sb.append("length:=" + getLength());
        return sb.toString();
    }
    
    /**
     * Returns an entry name length
     * 
     * @return int
     */
    public int getNameLength() {
        return name_length;
    }

    /**
     * Sets an entry name length
     * 
     * @param name_length
     *            int
     */
    protected void setNameLength(int name_length) {
        this.name_length = name_length;
    }

    /**
     * Returns an entry name
     * 
     * @return String
     */
    public String getName() {
        return name;
    }

    /**
     * Sets entry name
     * 
     * @param name
     *            String
     */
    protected void setName(String name) {
        this.name = name;
    }

    /**
     * Returns ChmCommons.EntryType (COMPRESSED or UNCOMPRESSED)
     * 
     * @return ChmCommons.EntryType
     */
    public ChmCommons.EntryType getEntryType() {
        return entryType;
    }

    protected void setEntryType(ChmCommons.EntryType entryType) {
        this.entryType = entryType;
    }

    public int getOffset() {
        return offset;
    }

    protected void setOffset(int offset) {
        this.offset = offset;
    }

    public int getLength() {
        return length;
    }

    protected void setLength(int length) {
        this.length = length;
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/chm/assertion/ChmAssert.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.chm.assertion;

import java.io.IOException;
import java.io.InputStream;

import org.apache.tika.exception.TikaException;
import org.apache.tika.parser.chm.accessor.ChmAccessor;
import org.apache.tika.parser.chm.accessor.ChmLzxcResetTable;
import org.apache.tika.parser.chm.core.ChmCommons;
import org.apache.tika.parser.chm.exception.ChmParsingException;

/**
 * Contains chm extractor assertions
 */
public class ChmAssert {
    /**
     * Checks a validity of the chmBlockSegment parameters
     * 
     * @param data
     *            byte[]
     * @param resetTable
     *            ChmLzxcResetTable
     * @param blockNumber
     *            int
     * @param lzxcBlockOffset
     *            int
     * @param lzxcBlockLength
     *            int
     * @throws TikaException 
     */
    public static final void assertChmBlockSegment(byte[] data,
            ChmLzxcResetTable resetTable, int blockNumber, int lzxcBlockOffset,
            int lzxcBlockLength) throws TikaException {
        if ((data == null))
            throw new TikaException("data[] is null");

        if ((data.length <= 0))
            throw new TikaException("data[] length should be greater than zero");

        if (resetTable == null)
            throw new TikaException("resetTable is null");

        if (resetTable.getBlockAddress().length <= 1)
            throw new TikaException("resetTable.getBlockAddress().length should be greater than zero");

        if (blockNumber < 0)
            throw new TikaException("blockNumber should be positive number");

        if (lzxcBlockOffset < 0)
            throw new TikaException("lzxcBlockOffset should be positive number");

        if (lzxcBlockLength < 0)
            throw new TikaException("lzxcBlockLength should be positive number");
    }

    /**
     * Checks if InputStream is not null
     * 
     * @param is
     *            InputStream
     * @throws ChmParsingException 
     * @throws IOException 
     */
    public static final void assertInputStreamNotNull(InputStream is) throws IOException {
        if (is == null)
            throw new IOException("input sream is null");
    }

    /**
     * Checks validity of ChmAccessor parameters
     * 
     * @param data
     * @param chmItsfHeader
     * @param count
     * @throws ChmParsingException 
     */
    public static final void assertChmAccessorParameters(byte[] data,
            ChmAccessor<?> chmAccessor, int count) throws ChmParsingException {
        assertByteArrayNotNull(data);
        assertChmAccessorNotNull(chmAccessor);
    }

    /**
     * Checks if byte[] is not null
     * 
     * @param data
     * @throws ChmParsingException 
     */
    public static final void assertByteArrayNotNull(byte[] data) throws ChmParsingException {
        if (data == null)
            throw new ChmParsingException("byte[] data is null");
    }

    /**
     * Checks if ChmAccessor is not null In case of null throws exception
     * 
     * @param ChmAccessor
     * @throws ChmParsingException 
     */
    public static final void assertChmAccessorNotNull(ChmAccessor<?> chmAccessor) throws ChmParsingException {
        if (chmAccessor == null)
            throw new ChmParsingException("chm header is null");
    }

    /**
     * Checks validity of the DirectoryListingEntry's parameters In case of
     * invalid parameter(s) throws an exception
     * 
     * @param name_length
     *            length of the chm entry name
     * @param name
     *            chm entry name
     * @param entryType
     *            EntryType
     * @param offset
     * @param length
     * @throws ChmParsingException 
     */
    public static final void assertDirectoryListingEntry(int name_length,
            String name, ChmCommons.EntryType entryType, int offset, int length) throws ChmParsingException {
        if (name_length < 0)
            throw new ChmParsingException("invalid name length");
        if (name == null)
            throw new ChmParsingException("invalid name");

        if ((entryType != ChmCommons.EntryType.COMPRESSED)
                && (entryType != ChmCommons.EntryType.UNCOMPRESSED))
            throw new ChmParsingException("invalid compressed type, should be EntryType.COMPRESSED | EntryType.UNCOMPRESSED");

        if (offset < 0)
            throw new ChmParsingException("invalid offset");

        if (length < 0)
            throw new ChmParsingException("invalid length");
    }

    public static void assertCopyingDataIndex(int index, int dataLength) throws ChmParsingException {
        if (index >= dataLength)
            throw new ChmParsingException("cannot parse chm file index > data.length");
    }

    /**
     * Checks if int param is greater than zero In case param <=0 throws an
     * exception
     * 
     * @param param
     * @throws ChmParsingException 
     */
    public static void assertPositiveInt(int param) throws ChmParsingException {
        if (param <= 0)
            throw new ChmParsingException("resetTable.getBlockAddress().length should be greater than zero");
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/chm/core/ChmCommons.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.chm.core;

import java.io.FileNotFoundException;
import java.io.FileOutputStream;
import java.io.IOException;
import java.util.List;

import org.apache.tika.exception.TikaException;
import org.apache.tika.parser.chm.accessor.ChmLzxcResetTable;
import org.apache.tika.parser.chm.accessor.DirectoryListingEntry;
import org.apache.tika.parser.chm.assertion.ChmAssert;
import org.apache.tika.parser.chm.exception.ChmParsingException;

public class ChmCommons {
    /* Prevents initialization */
    private ChmCommons() {
    }

    public static void assertByteArrayNotNull(byte[] data) throws TikaException {
        if (data == null)
            throw new TikaException("byte[] is null");
    }

    /**
     * Represents entry types: uncompressed, compressed
     */
    public enum EntryType {
        UNCOMPRESSED, COMPRESSED
    }

    /**
     * Represents lzx states: started decoding, not started decoding
     */
    public enum LzxState {
        STARTED_DECODING, NOT_STARTED_DECODING
    }

    /**
     * Represents intel file states during decompression
     */
    public enum IntelState {
        STARTED, NOT_STARTED
    }

    /**
     * Represents lzx block types in order to decompress differently
     */
    public final static int UNDEFINED = 0;
    public final static int VERBATIM = 1;
    public final static int ALIGNED_OFFSET = 2;
    public final static int UNCOMPRESSED = 3;

    /**
     * LZX supports window sizes of 2^15 (32Kb) through 2^21 (2Mb) Returns X,
     * i.e 2^X
     * 
     * @param window
     *            chmLzxControlData.getWindowSize()
     * 
     * @return window size
     */
    public static int getWindowSize(int window) {
        int win = 0;
        while (window > 1) {
            window >>>= 1;
            win++;
        }
        return win;
    }

    public static byte[] getChmBlockSegment(byte[] data,
            ChmLzxcResetTable resetTable, int blockNumber, int lzxcBlockOffset,
            int lzxcBlockLength) throws TikaException {
        ChmAssert.assertChmBlockSegment(data, resetTable, blockNumber,
                lzxcBlockOffset, lzxcBlockLength);
        int blockLength = -1;
        // TODO add int_max_value checking
        if (blockNumber < (resetTable.getBlockAddress().length - 1)) {
            blockLength = (int) (resetTable.getBlockAddress()[blockNumber + 1] - resetTable
                    .getBlockAddress()[blockNumber]);
        } else {
            /* new code */
            if (blockNumber >= resetTable.getBlockAddress().length)
                blockLength = 0;
            else
                /* end new code */
                blockLength = (int) (lzxcBlockLength - resetTable
                        .getBlockAddress()[blockNumber]);
        }
        byte[] t = ChmCommons
                .copyOfRange(
                        data,
                        (int) (lzxcBlockOffset + resetTable.getBlockAddress()[blockNumber]),
                        (int) (lzxcBlockOffset
                                + resetTable.getBlockAddress()[blockNumber] + blockLength));
        return (t != null) ? t : new byte[1];
    }

    /**
     * Returns textual representation of LangID
     * 
     * @param langID
     * 
     * @return language name
     */
    public static String getLanguage(long langID) {
        /* Potential problem with casting */
        switch ((int) langID) {
        case 1025:
            return "Arabic";
        case 1069:
            return "Basque";
        case 1027:
            return "Catalan";
        case 2052:
            return "Chinese (Simplified)";
        case 1028:
            return "Chinese (Traditional)";
        case 1029:
            return "Czech";
        case 1030:
            return "Danish";
        case 1043:
            return "Dutch";
        case 1033:
            return "English (United States)";
        case 1035:
            return "Finnish";
        case 1036:
            return "French";
        case 1031:
            return "German";
        case 1032:
            return "Greek";
        case 1037:
            return "Hebrew";
        case 1038:
            return "Hungarian";
        case 1040:
            return "Italian";
        case 1041:
            return "Japanese";
        case 1042:
            return "Korean";
        case 1044:
            return "Norwegian";
        case 1045:
            return "Polish";
        case 2070:
            return "Portuguese";
        case 1046:
            return "Portuguese (Brazil)";
        case 1049:
            return "Russian";
        case 1051:
            return "Slovakian";
        case 1060:
            return "Slovenian";
        case 3082:
            return "Spanish";
        case 1053:
            return "Swedish";
        case 1055:
            return "Turkish";
        default:
            return "unknown - http://msdn.microsoft.com/en-us/library/bb165625%28VS.80%29.aspx";
        }
    }

    /**
     * Checks skippable patterns
     * 
     * @param directoryListingEntry
     * 
     * @return boolean
     */
    public static boolean hasSkip(DirectoryListingEntry directoryListingEntry) {
        return (directoryListingEntry.getName().startsWith("/$")
                || directoryListingEntry.getName().startsWith("/#") || directoryListingEntry
                .getName().startsWith("::")) ? true : false;
    }

    /**
     * Writes byte[][] to the file
     * 
     * @param buffer
     * @param fileToBeSaved
     *            file name
     * @throws TikaException 
     */
    public static void writeFile(byte[][] buffer, String fileToBeSaved) throws TikaException {
        FileOutputStream output = null;
        if (buffer != null && fileToBeSaved != null
                && !ChmCommons.isEmpty(fileToBeSaved)) {
            try {
                output = new FileOutputStream(fileToBeSaved);
                for (byte[] bufferEntry : buffer) {
                    output.write(bufferEntry);
                }
            } catch (FileNotFoundException e) {
                throw new TikaException(e.getMessage());
            } catch (IOException e) {
                e.printStackTrace();
            } finally {
                if (output != null)
                    try {
                        output.flush();
                        output.close();
                    } catch (IOException e) {
                        e.printStackTrace();
                    }
            }
        }
    }

    /**
     * Reverses the order of given array
     * 
     * @param array
     */
    public static void reverse(byte[] array) {
        if (array == null) {
            return;
        }
        int i = 0;
        int j = array.length - 1;
        byte tmp;
        while (j > i) {
            tmp = array[j];
            array[j] = array[i];
            array[i] = tmp;
            j--;
            i++;
        }
    }

    /**
     * Returns an index of the reset table
     * 
     * @param text
     * @param pattern
     * @return index of the reset table
     * @throws ChmParsingException 
     */
    public static final int indexOfResetTableBlock(byte[] text, byte[] pattern) throws ChmParsingException {
        return (indexOf(text, pattern)) - 4;
    }

    /**
     * Searches some pattern in byte[]
     * 
     * @param text
     *            byte[]
     * @param pattern
     *            byte[]
     * @return an index, if nothing found returns -1
     * @throws ChmParsingException 
     */
    public static int indexOf(byte[] text, byte[] pattern) throws ChmParsingException {
        int[] next = null;
        int i = 0, j = -1;

        /* Preprocessing */
        if (pattern != null && text != null) {
            next = new int[pattern.length];
            next[0] = -1;
        } else
            throw new ChmParsingException("pattern and/or text should not be null");

        /* Computes a failure function */
        while (i < pattern.length - 1) {
            if (j == -1 || pattern[i] == pattern[j]) {
                i++;
                j++;
                if (pattern[i] != pattern[j])
                    next[i] = j;
                else
                    next[i] = next[j];
            } else
                j = next[j];
        }

        /* Reinitializes local variables */
        i = j = 0;

        /* Matching */
        while (i < text.length && j < pattern.length) {
            if (j == -1 || pattern[j] == text[i]) {
                i++;
                j++;
            } else
                j = next[j];
        }
        if (j == pattern.length)
            return (i - j); // match found at offset i - M
        else
            return -1; // not found
    }

    /**
     * Searches for some pattern in the directory listing entry list
     * 
     * @param list
     * @param pattern
     * @return an index, if nothing found returns -1
     */
    public static int indexOf(List<DirectoryListingEntry> list, String pattern) {
        int place = 0;
        for (DirectoryListingEntry directoryListingEntry : list) {
            if (directoryListingEntry.toString().contains(pattern)) return place;
            ++place;
        }
        return -1;// not found
    }

    /*
     * This method is added because of supporting of Java 5
     */
    public static byte[] copyOfRange(byte[] original, int from, int to) {
        checkCopyOfRangeParams(original, from, to);
        int newLength = to - from;
        if (newLength < 0)
            throw new IllegalArgumentException(from + " > " + to);
        byte[] copy = new byte[newLength];
        System.arraycopy(original, from, copy, 0, Math.min(original.length - from, newLength));
        return copy;
    }

    private static void checkCopyOfRangeParams(byte[] original, int from, int to) {
        if (original == null)
            throw new NullPointerException("array is null");
        if (from < 0)
            throw new IllegalArgumentException(from + " should be > 0");
        if (to < 0)
            throw new IllegalArgumentException(to + " should be > 0");
    }

    /*
     * This method is added because of supporting of Java 5
     */
    public static boolean isEmpty(String str) {
        return str == null || str.length() == 0;
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/chm/core/ChmConstants.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.chm.core;

public class ChmConstants {
    /* Prevents instantiation */
    private ChmConstants() {
    }

    public static final String DEFAULT_CHARSET = "UTF-8";
    public static final String ITSF = "ITSF";
    public static final String ITSP = "ITSP";
    public static final String PMGL = "PMGL";
    public static final String LZXC = "LZXC";
    public static final String CHM_PMGI_MARKER = "PMGI";
    public static final int BYTE_ARRAY_LENGHT = 16;
    public static final int CHM_ITSF_V2_LEN = 0x58;
    public static final int CHM_ITSF_V3_LEN = 0x60;
    public static final int CHM_ITSP_V1_LEN = 0x54;
    public static final int CHM_PMGL_LEN = 0x14;
    public static final int CHM_PMGI_LEN = 0x08;
    public static final int CHM_LZXC_RESETTABLE_V1_LEN = 0x28;
    public static final int CHM_LZXC_MIN_LEN = 0x18;
    public static final int CHM_LZXC_V2_LEN = 0x1c;
    public static final int CHM_SIGNATURE_LEN = 4;
    public static final int CHM_VER_2 = 2;
    public static final int CHM_VER_3 = 3;
    public static final int CHM_VER_1 = 1;
    public static final int CHM_WINDOW_SIZE_BLOCK = 0x8000;

    /* my hacking */
    public static final int START_PMGL = 0xCC;
    public static final String CONTROL_DATA = "ControlData";
    public static final String RESET_TABLE = "ResetTable";
    public static final String CONTENT = "Content";

    /* some constants defined by the LZX specification */
    public static final int LZX_MIN_MATCH = 2;
    public static final int LZX_MAX_MATCH = 257;
    public static final int LZX_NUM_CHARS = 256;
    public static final int LZX_BLOCKTYPE_INVALID = 0; /*
                                                        * also blocktypes 4-7
                                                        * invalid
                                                        */
    public static final int LZX_BLOCKTYPE_VERBATIM = 1;
    public static final int LZX_BLOCKTYPE_ALIGNED = 2;
    public static final int LZX_BLOCKTYPE_UNCOMPRESSED = 3;
    public static final int LZX_PRETREE_NUM_ELEMENTS_BITS = 4; /* ??? */
    public static final int LZX_PRETREE_NUM_ELEMENTS = 20;
    public static final int LZX_ALIGNED_NUM_ELEMENTS = 8; /*
                                                           * aligned offset tree
                                                           * #elements
                                                           */
    public static final int LZX_NUM_PRIMARY_LENGTHS = 7; /*
                                                          * this one missing
                                                          * from spec!
                                                          */
    public static final int LZX_NUM_SECONDARY_LENGTHS = 249; /*
                                                              * length tree
                                                              * #elements
                                                              */

    /* LZX huffman defines: tweak tablebits as desired */
    public static final int LZX_PRETREE_MAXSYMBOLS = LZX_PRETREE_NUM_ELEMENTS;
    public static final int LZX_PRETREE_TABLEBITS = 6;
    public static final int LZX_MAINTREE_MAXSYMBOLS = LZX_NUM_CHARS + 50 * 8;
    public static final int LZX_MAIN_MAXSYMBOLS = LZX_NUM_CHARS * 2;
    public static final int LZX_MAINTREE_TABLEBITS = 12;
    public static final int LZX_LENGTH_MAXSYMBOLS = LZX_NUM_SECONDARY_LENGTHS + 1;
    public static final int LZX_LENGTH_TABLEBITS = 12;
    public static final int LZX_ALIGNED_MAXSYMBOLS = LZX_ALIGNED_NUM_ELEMENTS;
    public static final int LZX_ALIGNED_TABLEBITS = 7;
    public static final int LZX_LENTABLE_SAFETY = 64;

    public static short[] EXTRA_BITS = { 0, 0, 0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5,
            5, 6, 6, 7, 7, 8, 8, 9, 9, 10, 10, 11, 11, 12, 12, 13, 13, 14, 14,
            15, 15, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,
            17, 17 };

    public static int[] POSITION_BASE = { 0, 1, 2, 3, 4, 6, 8, 12, 16, 24, 32,
            48, 64, 96, 128, 192, 256, 384, 512, 768, 1024, 1536, 2048, 3072,
            4096, 6144, 8192, 12288, 16384, 24576, 32768, 49152, 65536, 98304,
            131072, 196608, 262144, 393216, 524288, 655360, 786432, 917504,
            1048576, 1179648, 1310720, 1441792, 1572864, 1703936, 1835008,
            1966080, 2097152 };
}
"
tika-parsers/src/main/java/org/apache/tika/parser/chm/core/ChmExtractor.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.chm.core;

import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.util.ArrayList;
import java.util.List;
import org.apache.tika.exception.TikaException;
import org.apache.tika.io.IOUtils;
import org.apache.tika.parser.chm.accessor.ChmDirectoryListingSet;
import org.apache.tika.parser.chm.accessor.ChmItsfHeader;
import org.apache.tika.parser.chm.accessor.ChmItspHeader;
import org.apache.tika.parser.chm.accessor.ChmLzxcControlData;
import org.apache.tika.parser.chm.accessor.ChmLzxcResetTable;
import org.apache.tika.parser.chm.accessor.DirectoryListingEntry;
import org.apache.tika.parser.chm.assertion.ChmAssert;
import org.apache.tika.parser.chm.core.ChmCommons.EntryType;
import org.apache.tika.parser.chm.lzx.ChmBlockInfo;
import org.apache.tika.parser.chm.lzx.ChmLzxBlock;

/**
 * Extracts text from chm file. Enumerates chm entries.
 */
public class ChmExtractor {
    private List<ChmLzxBlock> lzxBlocksCache = null;
    private ChmDirectoryListingSet chmDirList = null;
    private ChmItsfHeader chmItsfHeader = null;
    private ChmItspHeader chmItspHeader = null;
    private ChmLzxcResetTable chmLzxcResetTable = null;
    private ChmLzxcControlData chmLzxcControlData = null;
    private byte[] data = null;
    private int indexOfContent;
    private long lzxBlockOffset;
    private long lzxBlockLength;

    /**
     * Returns lzxc control data.
     * 
     * @return ChmLzxcControlData
     */
    private ChmLzxcControlData getChmLzxcControlData() {
        return chmLzxcControlData;
    }

    /**
     * Sets lzxc control data
     * 
     * @param chmLzxcControlData
     */
    private void setChmLzxcControlData(ChmLzxcControlData chmLzxcControlData) {
        this.chmLzxcControlData = chmLzxcControlData;
    }

    private ChmItspHeader getChmItspHeader() {
        return chmItspHeader;
    }

    private void setChmItspHeader(ChmItspHeader chmItspHeader) {
        this.chmItspHeader = chmItspHeader;
    }

    /**
     * Returns lzxc reset table
     * 
     * @return ChmLzxcResetTable
     */
    private ChmLzxcResetTable getChmLzxcResetTable() {
        return chmLzxcResetTable;
    }

    /**
     * Sets lzxc reset table
     * 
     * @param chmLzxcResetTable
     */
    private void setChmLzxcResetTable(ChmLzxcResetTable chmLzxcResetTable) {
        this.chmLzxcResetTable = chmLzxcResetTable;
    }

    /**
     * Returns lzxc hit_cache length
     * 
     * @return lzxBlockLength
     */
    private long getLzxBlockLength() {
        return lzxBlockLength;
    }

    /**
     * Sets lzxc hit_cache length
     * 
     * @param lzxBlockLength
     */
    private void setLzxBlockLength(long lzxBlockLength) {
        this.lzxBlockLength = lzxBlockLength;
    }

    /**
     * Returns lzxc hit_cache offset
     * 
     * @return lzxBlockOffset
     */
    private long getLzxBlockOffset() {
        return lzxBlockOffset;
    }

    /**
     * Sets lzxc hit_cache offset
     */
    private void setLzxBlockOffset(long lzxBlockOffset) {
        this.lzxBlockOffset = lzxBlockOffset;
    }

    private int getIndexOfContent() {
        return indexOfContent;
    }

    private void setIndexOfContent(int indexOfContent) {
        this.indexOfContent = indexOfContent;
    }

    private byte[] getData() {
        return data;
    }

    private void setData(byte[] data) {
        this.data = data;
    }

    public ChmExtractor(InputStream is) throws TikaException, IOException {
        ChmAssert.assertInputStreamNotNull(is);
        try {
            setData(IOUtils.toByteArray(is));

            /* Creates and parses chm itsf header */
            setChmItsfHeader(new ChmItsfHeader());
            // getChmItsfHeader().parse(Arrays.copyOfRange(getData(), 0,
            // ChmConstants.CHM_ITSF_V3_LEN - 1), getChmItsfHeader());
            getChmItsfHeader().parse(ChmCommons.copyOfRange(getData(), 0,
                            ChmConstants.CHM_ITSF_V3_LEN - 1), getChmItsfHeader());

            /* Creates and parses chm itsp header */
            setChmItspHeader(new ChmItspHeader());
            // getChmItspHeader().parse(Arrays.copyOfRange( getData(), (int)
            // getChmItsfHeader().getDirOffset(),
            // (int) getChmItsfHeader().getDirOffset() +
            // ChmConstants.CHM_ITSP_V1_LEN), getChmItspHeader());
            getChmItspHeader().parse(
                    ChmCommons.copyOfRange(getData(), (int) getChmItsfHeader()
                            .getDirOffset(), (int) getChmItsfHeader().getDirOffset() + 
                            ChmConstants.CHM_ITSP_V1_LEN), getChmItspHeader());

            /* Creates instance of ChmDirListingContainer */
            setChmDirList(new ChmDirectoryListingSet(getData(),
                    getChmItsfHeader(), getChmItspHeader()));

            int indexOfControlData = getChmDirList().getControlDataIndex();
            int indexOfResetData = ChmCommons.indexOfResetTableBlock(getData(),
                    ChmConstants.LZXC.getBytes("UTF-8"));
            byte[] dir_chunk = null;
            if (indexOfResetData > 0)
                dir_chunk = ChmCommons.copyOfRange( getData(), indexOfResetData, indexOfResetData  
                        + getChmDirList().getDirectoryListingEntryList().get(indexOfControlData).getLength());
            // dir_chunk = Arrays.copyOfRange(getData(), indexOfResetData,
            // indexOfResetData
            // +
            // getChmDirList().getDirectoryListingEntryList().get(indexOfControlData).getLength());

            /* Creates and parses chm control data */
            setChmLzxcControlData(new ChmLzxcControlData());
            getChmLzxcControlData().parse(dir_chunk, getChmLzxcControlData());

            int indexOfResetTable = getChmDirList().getResetTableIndex();
            setChmLzxcResetTable(new ChmLzxcResetTable());

            int startIndex = (int) getChmDirList().getDataOffset()
                    + getChmDirList().getDirectoryListingEntryList()
                            .get(indexOfResetTable).getOffset();

            // assert startIndex < data.length
            ChmAssert.assertCopyingDataIndex(startIndex, getData().length);

            // dir_chunk = Arrays.copyOfRange(getData(), startIndex, startIndex
            // +
            // getChmDirList().getDirectoryListingEntryList().get(indexOfResetTable).getLength());
            dir_chunk = ChmCommons.copyOfRange(getData(), startIndex, startIndex
                            + getChmDirList().getDirectoryListingEntryList().get(indexOfResetTable).getLength());

            getChmLzxcResetTable().parse(dir_chunk, getChmLzxcResetTable());

            setIndexOfContent(ChmCommons.indexOf(getChmDirList().getDirectoryListingEntryList(), 
                    ChmConstants.CONTENT));
            setLzxBlockOffset((getChmDirList().getDirectoryListingEntryList().get(getIndexOfContent()).getOffset() 
                    + getChmItsfHeader().getDataOffset()));
            setLzxBlockLength(getChmDirList().getDirectoryListingEntryList().get(getIndexOfContent()).getLength());

            setLzxBlocksCache(new ArrayList<ChmLzxBlock>());

        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    /**
     * Enumerates chm entities
     * 
     * @return list of chm entities
     */
    public List<String> enumerateChm() {
        List<String> listOfEntries = new ArrayList<String>();
        for (DirectoryListingEntry directoryListingEntry : getChmDirList().getDirectoryListingEntryList()) {
            listOfEntries.add(directoryListingEntry.getName());
        }
        return listOfEntries;
    }

    /**
     * Decompresses a chm entry
     * 
     * @param directoryListingEntry
     * 
     * @return decompressed data
     * @throws TikaException 
     */
    public byte[] extractChmEntry(DirectoryListingEntry directoryListingEntry) throws TikaException {
        ByteArrayOutputStream buffer = new ByteArrayOutputStream();
        ChmLzxBlock lzxBlock = null;
        try {
            /* UNCOMPRESSED type is easiest one */
            if (directoryListingEntry.getEntryType() == EntryType.UNCOMPRESSED
                    && directoryListingEntry.getLength() > 0
                    && !ChmCommons.hasSkip(directoryListingEntry)) {
                int dataOffset = (int) (getChmItsfHeader().getDataOffset() + directoryListingEntry
                        .getOffset());
                // dataSegment = Arrays.copyOfRange(getData(), dataOffset,
                // dataOffset + directoryListingEntry.getLength());
                buffer.write(ChmCommons.copyOfRange(
                        getData(), dataOffset,
                        dataOffset + directoryListingEntry.getLength()));
            } else if (directoryListingEntry.getEntryType() == EntryType.COMPRESSED
                    && !ChmCommons.hasSkip(directoryListingEntry)) {
                /* Gets a chm hit_cache info */
                ChmBlockInfo bb = ChmBlockInfo.getChmBlockInfoInstance(
                        directoryListingEntry, (int) getChmLzxcResetTable()
                                .getBlockLen(), getChmLzxcControlData());

                int i = 0, start = 0, hit_cache = 0;

                if ((getLzxBlockLength() < Integer.MAX_VALUE)
                        && (getLzxBlockOffset() < Integer.MAX_VALUE)) {
                    // TODO: Improve the caching
                    // caching ... = O(n^2) - depends on startBlock and endBlock
                    start = -1;
                    if (!getLzxBlocksCache().isEmpty()) {
                        for (i = 0; i < getLzxBlocksCache().size(); i++) {
                            //lzxBlock = getLzxBlocksCache().get(i);
                            int bn = getLzxBlocksCache().get(i).getBlockNumber();
                            for (int j = bb.getIniBlock(); j <= bb.getStartBlock(); j++) {
                                if (bn == j) {
                                    if (j > start) {
                                        start = j;
                                        hit_cache = i;
                                    }
                                }
                            }
                            if (start == bb.getStartBlock())
                                break;
                        }
                    }

//                    if (i == getLzxBlocksCache().size() && i == 0) {
                    if (start<0) {
                        start = bb.getIniBlock();

                        byte[] dataSegment = ChmCommons.getChmBlockSegment(
                                getData(),
                                getChmLzxcResetTable(), start,
                                (int) getLzxBlockOffset(),
                                (int) getLzxBlockLength());

                        lzxBlock = new ChmLzxBlock(start, dataSegment,
                                getChmLzxcResetTable().getBlockLen(), null);

                        getLzxBlocksCache().add(lzxBlock);
                    } else {
                        lzxBlock = getLzxBlocksCache().get(hit_cache);
                    }

                    for (i = start; i <= bb.getEndBlock();) {
                        if (i == bb.getStartBlock() && i == bb.getEndBlock()) {
                            buffer.write(lzxBlock.getContent(
                                    bb.getStartOffset(), bb.getEndOffset()));
                            break;
                        }

                        if (i == bb.getStartBlock()) {
                            buffer.write(lzxBlock.getContent(
                                    bb.getStartOffset()));
                        }

                        if (i > bb.getStartBlock() && i < bb.getEndBlock()) {
                            buffer.write(lzxBlock.getContent());
                        }

                        if (i == bb.getEndBlock()) {
                            buffer.write(lzxBlock.getContent(
                                    0, bb.getEndOffset()));
                            break;
                        }

                        i++;

                        if (i % getChmLzxcControlData().getResetInterval() == 0) {
                            lzxBlock = new ChmLzxBlock(i,
                                    ChmCommons.getChmBlockSegment(getData(),
                                            getChmLzxcResetTable(), i,
                                            (int) getLzxBlockOffset(),
                                            (int) getLzxBlockLength()),
                                    getChmLzxcResetTable().getBlockLen(), null);
                        } else {
                            lzxBlock = new ChmLzxBlock(i,
                                    ChmCommons.getChmBlockSegment(getData(),
                                            getChmLzxcResetTable(), i,
                                            (int) getLzxBlockOffset(),
                                            (int) getLzxBlockLength()),
                                    getChmLzxcResetTable().getBlockLen(),
                                    lzxBlock);
                        }

                        getLzxBlocksCache().add(lzxBlock);
                    }

                    if (getLzxBlocksCache().size() > getChmLzxcResetTable()
                            .getBlockCount()) {
                        getLzxBlocksCache().clear();
                    }
                } //end of if
                
                if (buffer.size() != directoryListingEntry.getLength()) {
                    throw new TikaException("CHM file extract error: extracted Length is wrong.");
                }
            } //end of if compressed
        } catch (Exception e) {
            throw new TikaException(e.getMessage());
        }

        return buffer.toByteArray();
    }

    private void setLzxBlocksCache(List<ChmLzxBlock> lzxBlocksCache) {
        this.lzxBlocksCache = lzxBlocksCache;
    }

    private List<ChmLzxBlock> getLzxBlocksCache() {
        return lzxBlocksCache;
    }

    private void setChmDirList(ChmDirectoryListingSet chmDirList) {
        this.chmDirList = chmDirList;
    }

    public ChmDirectoryListingSet getChmDirList() {
        return chmDirList;
    }

    private void setChmItsfHeader(ChmItsfHeader chmItsfHeader) {
        this.chmItsfHeader = chmItsfHeader;
    }

    private ChmItsfHeader getChmItsfHeader() {
        return chmItsfHeader;
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/chm/core/ChmWrapper.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.parser.chm.core;

import java.util.List;

import org.apache.tika.parser.chm.accessor.ChmDirectoryListingSet;
import org.apache.tika.parser.chm.accessor.ChmItsfHeader;
import org.apache.tika.parser.chm.accessor.ChmItspHeader;
import org.apache.tika.parser.chm.accessor.ChmLzxcControlData;
import org.apache.tika.parser.chm.accessor.ChmLzxcResetTable;
import org.apache.tika.parser.chm.lzx.ChmLzxBlock;

public class ChmWrapper {
    private List<ChmLzxBlock> lzxBlocksCache = null;
    private ChmDirectoryListingSet chmDirList = null;
    private ChmItsfHeader chmItsfHeader = null;
    private ChmItspHeader chmItspHeader = null;
    private ChmLzxcResetTable chmLzxcResetTable = null;
    private ChmLzxcControlData chmLzxcControlData = null;
    private byte[] data = null;
    private int indexOfContent;
    private long lzxBlockOffset;
    private long lzxBlockLength;
    private int indexOfResetData;
    private int indexOfResetTable;
    private int startIndex;

    protected int getStartIndex() {
        return startIndex;
    }

    protected void setStartIndex(int startIndex) {
        this.startIndex = startIndex;
    }

    protected int getIndexOfResetTable() {
        return indexOfResetTable;
    }

    protected void setIndexOfResetTable(int indexOfResetTable) {
        this.indexOfResetTable = indexOfResetTable;
    }

    protected List<ChmLzxBlock> getLzxBlocksCache() {
        return lzxBlocksCache;
    }

    protected void setLzxBlocksCache(List<ChmLzxBlock> lzxBlocksCache) {
        this.lzxBlocksCache = lzxBlocksCache;
    }

    protected ChmDirectoryListingSet getChmDirList() {
        return chmDirList;
    }

    protected void setChmDirList(ChmDirectoryListingSet chmDirList) {
        this.chmDirList = chmDirList;
    }

    protected ChmItsfHeader getChmItsfHeader() {
        return chmItsfHeader;
    }

    protected void setChmItsfHeader(ChmItsfHeader chmItsfHeader) {
        this.chmItsfHeader = chmItsfHeader;
    }

    protected ChmLzxcResetTable getChmLzxcResetTable() {
        return chmLzxcResetTable;
    }

    protected void setChmLzxcResetTable(ChmLzxcResetTable chmLzxcResetTable) {
        this.chmLzxcResetTable = chmLzxcResetTable;
    }

    protected ChmLzxcControlData getChmLzxcControlData() {
        return chmLzxcControlData;
    }

    protected void setChmLzxcControlData(ChmLzxcControlData chmLzxcControlData) {
        this.chmLzxcControlData = chmLzxcControlData;
    }

    protected byte[] getData() {
        return data;
    }

    protected void setData(byte[] data) {
        this.data = data;
    }

    protected int getIndexOfContent() {
        return indexOfContent;
    }

    protected void setIndexOfContent(int indexOfContent) {
        this.indexOfContent = indexOfContent;
    }

    protected long getLzxBlockOffset() {
        return lzxBlockOffset;
    }

    protected void setLzxBlockOffset(long lzxBlockOffset) {
        this.lzxBlockOffset = lzxBlockOffset;
    }

    protected long getLzxBlockLength() {
        return lzxBlockLength;
    }

    protected void setLzxBlockLength(long lzxBlockLength) {
        this.lzxBlockLength = lzxBlockLength;
    }

    protected void setChmItspHeader(ChmItspHeader chmItspHeader) {
        this.chmItspHeader = chmItspHeader;
    }

    protected ChmItspHeader getChmItspHeader() {
        return chmItspHeader;
    }

    protected void setIndexOfResetData(int indexOfResetData) {
        this.indexOfResetData = indexOfResetData;
    }

    protected int getIndexOfResetData() {
        return indexOfResetData;
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/chm/exception/ChmParsingException.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.chm.exception;

import org.apache.tika.exception.TikaException;

public class ChmParsingException extends TikaException {
    private static final long serialVersionUID = 6497936044733665210L;

    public ChmParsingException(String description) {
        super(description);
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/chm/lzx/ChmBlockInfo.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.chm.lzx;

import org.apache.tika.exception.TikaException;
import org.apache.tika.parser.chm.accessor.ChmLzxcControlData;
import org.apache.tika.parser.chm.accessor.DirectoryListingEntry;
import org.apache.tika.parser.chm.exception.ChmParsingException;

/**
 * A container that contains chm block information such as: i. initial block is
 * using to reset main tree ii. start block is using for knowing where to start
 * iii. end block is using for knowing where to stop iv. start offset is using
 * for knowing where to start reading v. end offset is using for knowing where
 * to stop reading
 * 
 */
public class ChmBlockInfo {
    /* class members */
    private int iniBlock;
    private int startBlock;
    private int endBlock;
    private int startOffset;
    private int endOffset;

    private static ChmBlockInfo chmBlockInfo = null;

    private ChmBlockInfo() {

    }

    /**
     * Returns an information related to the chmBlockInfo
     * 
     * @param dle
     *            - DirectoryListingEntry
     * @param bytesPerBlock
     *            - int, = chmLzxcResetTable.block_length
     * @param clcd
     *            - ChmLzxcControlData
     * @param chmBlockInfo
     *            - ChmBlockInfo
     * 
     * @return ChmBlockInfo
     * @throws TikaException 
     */
    protected ChmBlockInfo getChmBlockInfo(DirectoryListingEntry dle,
            int bytesPerBlock, ChmLzxcControlData clcd,
            ChmBlockInfo chmBlockInfo) throws TikaException {
        if (!validateParameters(dle, bytesPerBlock, clcd, chmBlockInfo))
            throw new ChmParsingException("Please check you parameters");

        chmBlockInfo.setStartBlock(dle.getOffset() / bytesPerBlock);
        chmBlockInfo.setEndBlock((dle.getOffset() + dle.getLength())
                / bytesPerBlock);
        chmBlockInfo.setStartOffset(dle.getOffset() % bytesPerBlock);
        chmBlockInfo.setEndOffset((dle.getOffset() + dle.getLength())
                % bytesPerBlock);
        // potential problem with casting long to int
        chmBlockInfo
                .setIniBlock(chmBlockInfo.startBlock - 
                        chmBlockInfo.startBlock % (int) clcd.getResetInterval());
//                .setIniBlock((chmBlockInfo.startBlock - chmBlockInfo.startBlock)
//                        % (int) clcd.getResetInterval());
        return chmBlockInfo;
    }

    public static ChmBlockInfo getChmBlockInfoInstance(
            DirectoryListingEntry dle, int bytesPerBlock,
            ChmLzxcControlData clcd) {
        setChmBlockInfo(new ChmBlockInfo());
        getChmBlockInfo().setStartBlock(dle.getOffset() / bytesPerBlock);
        getChmBlockInfo().setEndBlock(
                (dle.getOffset() + dle.getLength()) / bytesPerBlock);
        getChmBlockInfo().setStartOffset(dle.getOffset() % bytesPerBlock);
        getChmBlockInfo().setEndOffset(
                (dle.getOffset() + dle.getLength()) % bytesPerBlock);
        // potential problem with casting long to int
        getChmBlockInfo().setIniBlock(
                getChmBlockInfo().startBlock - getChmBlockInfo().startBlock
                        % (int) clcd.getResetInterval());
//                (getChmBlockInfo().startBlock - getChmBlockInfo().startBlock)
//                        % (int) clcd.getResetInterval());
        return getChmBlockInfo();
    }

    /**
     * Returns textual representation of ChmBlockInfo
     */
    public String toString() {
        StringBuilder sb = new StringBuilder();
        sb.append("iniBlock:=" + getIniBlock() + ", ");
        sb.append("startBlock:=" + getStartBlock() + ", ");
        sb.append("endBlock:=" + getEndBlock() + ", ");
        sb.append("startOffset:=" + getStartOffset() + ", ");
        sb.append("endOffset:=" + getEndOffset()
                + System.getProperty("line.separator"));
        return sb.toString();
    }

    private boolean validateParameters(DirectoryListingEntry dle,
            int bytesPerBlock, ChmLzxcControlData clcd,
            ChmBlockInfo chmBlockInfo) {
        int goodParameter = 0;
        if (dle != null)
            ++goodParameter;
        if (bytesPerBlock > 0)
            ++goodParameter;
        if (clcd != null)
            ++goodParameter;
        if (chmBlockInfo != null)
            ++goodParameter;
        return (goodParameter == 4);
    }

    public static void main(String[] args) {
    }

    /**
     * Returns an initial block index
     * 
     * @return int
     */
    public int getIniBlock() {
        return iniBlock;
    }

    /**
     * Sets the initial block index
     * 
     * @param iniBlock
     *            - int
     */
    private void setIniBlock(int iniBlock) {
        this.iniBlock = iniBlock;
    }

    /**
     * Returns the start block index
     * 
     * @return int
     */
    public int getStartBlock() {
        return startBlock;
    }

    /**
     * Sets the start block index
     * 
     * @param startBlock
     *            - int
     */
    private void setStartBlock(int startBlock) {
        this.startBlock = startBlock;
    }

    /**
     * Returns the end block index
     * 
     * @return - int
     */
    public int getEndBlock() {
        return endBlock;
    }

    /**
     * Sets the end block index
     * 
     * @param endBlock
     *            - int
     */
    private void setEndBlock(int endBlock) {
        this.endBlock = endBlock;
    }

    /**
     * Returns the start offset index
     * 
     * @return - int
     */
    public int getStartOffset() {
        return startOffset;
    }

    /**
     * Sets the start offset index
     * 
     * @param startOffset
     *            - int
     */
    private void setStartOffset(int startOffset) {
        this.startOffset = startOffset;
    }

    /**
     * Returns the end offset index
     * 
     * @return - int
     */
    public int getEndOffset() {
        return endOffset;
    }

    /**
     * Sets the end offset index
     * 
     * @param endOffset
     *            - int
     */
    private void setEndOffset(int endOffset) {
        this.endOffset = endOffset;
    }

    public static void setChmBlockInfo(ChmBlockInfo chmBlockInfo) {
        ChmBlockInfo.chmBlockInfo = chmBlockInfo;
    }

    public static ChmBlockInfo getChmBlockInfo() {
        return chmBlockInfo;
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/chm/lzx/ChmLzxBlock.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.chm.lzx;

import java.math.BigInteger;

import org.apache.tika.exception.TikaException;
import org.apache.tika.parser.chm.core.ChmCommons;
import org.apache.tika.parser.chm.core.ChmCommons.IntelState;
import org.apache.tika.parser.chm.core.ChmCommons.LzxState;
import org.apache.tika.parser.chm.core.ChmConstants;
import org.apache.tika.parser.chm.exception.ChmParsingException;

/**
 * Decompresses a chm block. Depending on chm block type chooses most relevant
 * decompressing method. A chm block type can be as follows:</br> <li>UNDEFINED
 * - no action taken, i.e. skipping the block <li>VERBATIM <li>ALIGNED_OFFSET
 * <li>UNCOMPRESSED the most simplest In addition there are unknown types (4-7).
 * Currently relying on previous chm block these types changing according to the
 * previous chm block type. We need to invent more appropriate way to handle
 * such types.
 * 
 */
public class ChmLzxBlock {
    private int block_number;
    private long block_length;
    private ChmLzxState state;
    private byte[] content = null;
    private ChmSection chmSection = null;
    private int contentLength = 0;

    // trying to find solution for bad blocks ...
    private int previousBlockType = -1;

    public ChmLzxBlock(int blockNumber, byte[] dataSegment, long blockLength,
            ChmLzxBlock prevBlock) throws TikaException {
        try {
            if (validateConstructorParams(blockNumber, dataSegment, blockLength)) {
                setBlockNumber(blockNumber);

                if (prevBlock != null
                        && prevBlock.getState().getBlockLength() > prevBlock
                                .getState().getBlockRemaining())
                    setChmSection(new ChmSection(dataSegment, prevBlock.getContent()));
                else
                    setChmSection(new ChmSection(dataSegment));

                setBlockLength(blockLength);

                // ============================================
                // we need to take care of previous context
                // ============================================
                checkLzxBlock(prevBlock);
                if (prevBlock == null
                        || blockLength < (int) getBlockLength()) {
                    setContent((int) getBlockLength());
                }
                else {
                    setContent((int) blockLength);
                }

                if (prevBlock != null && prevBlock.getState() != null)
                    previousBlockType = prevBlock.getState().getBlockType();

                extractContent();
            } else
                throw new TikaException("Check your chm lzx block parameters");
        } catch (TikaException e) {
            throw e;
        }
    }

    protected int getContentLength() {
        return contentLength;
    }

    protected void setContentLength(int contentLength) {
        this.contentLength = contentLength;
    }

    private ChmSection getChmSection() {
        return chmSection;
    }

    private void setChmSection(ChmSection chmSection) {
        this.chmSection = chmSection;
    }

    private void assertStateNotNull() throws TikaException {
        if (getState() == null)
            throw new ChmParsingException("state is null");
    }

    private void extractContent() throws TikaException {
        assertStateNotNull();
        if (getChmSection().getData() != null) {
            boolean continueLoop = true;
            while (continueLoop && getContentLength() < getBlockLength()) {
                if (getState() != null && getState().getBlockRemaining() == 0) {
                    if (getState().getHadStarted() == LzxState.NOT_STARTED_DECODING) {
                        getState().setHadStarted(LzxState.STARTED_DECODING);
                        if (getChmSection().getSyncBits(1) == 1) {
                            int intelSizeTemp = (getChmSection()
                                    .getSyncBits(16) << 16)
                                    + getChmSection().getSyncBits(16);
                            if (intelSizeTemp >= 0)
                                getState().setIntelFileSize(intelSizeTemp);
                            else
                                getState().setIntelFileSize(0);
                        }
                    }
                    getState().setBlockType(getChmSection().getSyncBits(3));
                    getState().setBlockLength(
                            (getChmSection().getSyncBits(16) << 8)
                                    + getChmSection().getSyncBits(8));
                    getState().setBlockRemaining(getState().getBlockLength());

                    // ----------------------------------------
                    // Trying to handle 3 - 7 block types
                    // ----------------------------------------
                    if (getState().getBlockType() > 3) {
                        if (previousBlockType >= 0 && previousBlockType < 3)
                            getState().setBlockType(previousBlockType);
                    }

                    switch (getState().getBlockType()) {
                        case ChmCommons.ALIGNED_OFFSET:
                            createAlignedTreeTable();
                            //fall through
                        case ChmCommons.VERBATIM:
                            /* Creates mainTreeTable */
                            createMainTreeTable();
                            createLengthTreeTable();
                            if (getState().getMainTreeLengtsTable()[0xe8] != 0)
                                getState().setIntelState(IntelState.STARTED);
                            break;
                        case ChmCommons.UNCOMPRESSED:
                            getState().setIntelState(IntelState.STARTED);
                            if (getChmSection().getTotal() > 16)
                                getChmSection().setSwath(
                                        getChmSection().getSwath() - 1);
                            getState().setR0(
                                    (new BigInteger(getChmSection()
                                            .reverseByteOrder(
                                                    getChmSection().unmarshalBytes(
                                                            4))).longValue()));
                            getState().setR1(
                                    (new BigInteger(getChmSection()
                                            .reverseByteOrder(
                                                    getChmSection().unmarshalBytes(
                                                            4))).longValue()));
                            getState().setR2(
                                    (new BigInteger(getChmSection()
                                            .reverseByteOrder(
                                                    getChmSection().unmarshalBytes(
                                                            4))).longValue()));
                            break;
                        default:
                            break;
                    }
                } //end of if BlockRemaining == 0

                int tempLen;

                if (getContentLength() + getState().getBlockRemaining() > getBlockLength()) {
                    getState().setBlockRemaining(
                            getContentLength() + getState().getBlockRemaining()
                                    - (int) getBlockLength());
                    tempLen = (int) getBlockLength();
                } else {
                    tempLen = getContentLength()
                            + getState().getBlockRemaining();
                    getState().setBlockRemaining(0);
                }

                int lastLength = getContentLength();
                switch (getState().getBlockType()) {
                case ChmCommons.ALIGNED_OFFSET:
                    // if(prevblock.lzxState.length>prevblock.lzxState.remaining)
                    decompressAlignedBlock(tempLen, getChmSection().getPrevContent() == null ? getChmSection().getData() : getChmSection().getPrevContent());// prevcontext
                    break;
                case ChmCommons.VERBATIM:
                    decompressVerbatimBlock(tempLen, getChmSection().getPrevContent() == null ? getChmSection().getData() : getChmSection().getPrevContent());
                    break;
                case ChmCommons.UNCOMPRESSED:
                    decompressUncompressedBlock(tempLen, getChmSection().getPrevContent() == null ? getChmSection().getData() : getChmSection().getPrevContent());
                    break;
                }
                getState().increaseFramesRead();
                if ((getState().getFramesRead() < 32768)
                        && getState().getIntelFileSize() != 0)
                    intelE8Decoding();

                continueLoop = getContentLength() > lastLength;
            }
        }
    }

    protected void intelE8Decoding() {
        if (getBlockLength() <= ChmConstants.LZX_PRETREE_TABLEBITS
                || (getState().getIntelState() == IntelState.NOT_STARTED)) {
            getState().setBlockRemaining(
                    getState().getBlockRemaining() - (int) getBlockLength());
        } else {
            long curpos = getState().getBlockRemaining();
            getState().setBlockRemaining(
                    getState().getBlockRemaining() - (int) getBlockLength());
            int i = 0;
            while (i < getBlockLength() - 10) {
                if (content[i] != 0xe8) {
                    i++;
                    continue;
                }
                byte[] b = new byte[4];
                b[0] = getContent()[i + 3];
                b[1] = getContent()[i + 2];
                b[2] = getContent()[i + 1];
                b[3] = getContent()[i + 0];
                long absoff = (new BigInteger(b)).longValue();
                if ((absoff >= -curpos)
                        && (absoff < getState().getIntelFileSize())) {
                    long reloff = (absoff >= 0) ? absoff - curpos : absoff
                            + getState().getIntelFileSize();
                    getContent()[i + 0] = (byte) reloff;
                    getContent()[i + 1] = (byte) (reloff >>> 8);
                    getContent()[i + 2] = (byte) (reloff >>> 16);
                    getContent()[i + 3] = (byte) (reloff >>> 24);
                }
                i += 4;
                curpos += 5;
            }
        }
    }

    private short[] createPreLenTable() {
        short[] tmp = new short[ChmConstants.LZX_PRETREE_MAXSYMBOLS];
        for (int i = 0; i < ChmConstants.LZX_PRETREE_MAXSYMBOLS; i++) {
            tmp[i] = (short) getChmSection().getSyncBits(
                    ChmConstants.LZX_PRETREE_NUM_ELEMENTS_BITS);
        }
        return tmp;
    }

    private void createLengthTreeTable() throws TikaException {
        //Read Pre Tree Table
        short[] prelentable = createPreLenTable();

        if (prelentable == null) {
            throw new ChmParsingException("pretreetable is null");
        }

        short[] pretreetable = createTreeTable2(prelentable,
                (1 << ChmConstants.LZX_PRETREE_TABLEBITS)
                        + (ChmConstants.LZX_PRETREE_MAXSYMBOLS << 1),
                ChmConstants.LZX_PRETREE_TABLEBITS,
                ChmConstants.LZX_PRETREE_MAXSYMBOLS);

        if (pretreetable == null) {
            throw new ChmParsingException("pretreetable is null");
        }

        //Build Length Tree
        createLengthTreeLenTable(0, ChmConstants.LZX_NUM_SECONDARY_LENGTHS,
                pretreetable, prelentable);

        getState().setLengthTreeTable(
                createTreeTable2(getState().getLengthTreeLengtsTable(),
                        (1 << ChmConstants.LZX_LENGTH_TABLEBITS)
                                + (ChmConstants.LZX_LENGTH_MAXSYMBOLS << 1),
                        ChmConstants.LZX_LENGTH_TABLEBITS,
                        ChmConstants.LZX_NUM_SECONDARY_LENGTHS));
    }

    private void decompressUncompressedBlock(int len, byte[] prevcontent) {
        if (getContentLength() + getState().getBlockRemaining() <= getBlockLength()) {
            for (int i = getContentLength(); i < (getContentLength() + getState()
                    .getBlockRemaining()); i++)
                content[i] = getChmSection().getByte();

            setContentLength(getContentLength()
                    + getState().getBlockRemaining());
            getState().setBlockRemaining(0);
        } else {
            for (int i = getContentLength(); i < getBlockLength(); i++)
                content[i] = getChmSection().getByte();
            getState().setBlockRemaining(
                    (int) getBlockLength() - getContentLength());// = blockLen -
                                                                 // contentlen;
            setContentLength((int) getBlockLength());
        }
    }

    private void decompressAlignedBlock(int len, byte[] prevcontent) throws TikaException {

        if ((getChmSection() == null) || (getState() == null)
                || (getState().getMainTreeTable() == null))
            throw new ChmParsingException("chm section is null");

        short s;
        int x, i, border;
        int matchlen = 0, matchfooter = 0, extra, rundest, runsrc;
        int matchoffset = 0;
        for (i = getContentLength(); i < len; i++) {
            /* new code */
            //read huffman tree from main tree
            border = getChmSection().peekBits(
                    ChmConstants.LZX_MAINTREE_TABLEBITS);
            if (border >= getState().mainTreeTable.length)
                throw new ChmParsingException("error decompressing aligned block.");
                //break;
            /* end new code */
            s = getState().mainTreeTable[getChmSection().peekBits(
                    ChmConstants.LZX_MAINTREE_TABLEBITS)];
            if (s >= getState().getMainTreeElements()) {
                x = ChmConstants.LZX_MAINTREE_TABLEBITS;
                do {
                    x++;
                    s <<= 1;
                    s += getChmSection().checkBit(x);
                } while ((s = getState().mainTreeTable[s]) >= getState()
                        .getMainTreeElements());
            }
            //System.out.printf("%d,", s);
            //?getChmSection().getSyncBits(getState().mainTreeTable[s]);
            getChmSection().getSyncBits(getState().getMainTreeLengtsTable()[s]);
            if (s < ChmConstants.LZX_NUM_CHARS) {
                content[i] = (byte) s;
            } else {
                s -= ChmConstants.LZX_NUM_CHARS;
                matchlen = s & ChmConstants.LZX_NUM_PRIMARY_LENGTHS;
                if (matchlen == ChmConstants.LZX_NUM_PRIMARY_LENGTHS) {
                    matchfooter = getState().lengthTreeTable[getChmSection()
                            .peekBits(ChmConstants.LZX_LENGTH_TABLEBITS)];//.LZX_MAINTREE_TABLEBITS)];
                    if (matchfooter >= ChmConstants.LZX_LENGTH_MAXSYMBOLS/*?LZX_LENGTH_TABLEBITS*/) {
                        x = ChmConstants.LZX_LENGTH_TABLEBITS;
                        do {
                            x++;
                            matchfooter <<= 1;
                            matchfooter += getChmSection().checkBit(x);
                        } while ((matchfooter = getState().lengthTreeTable[matchfooter]) >= ChmConstants.LZX_NUM_SECONDARY_LENGTHS);
                    }
                    getChmSection().getSyncBits(
                            getState().lengthTreeLengtsTable[matchfooter]);
                    matchlen += matchfooter;
                }
                matchlen += ChmConstants.LZX_MIN_MATCH;
                matchoffset = s >>> 3;
                if (matchoffset > 2) {
                    extra = ChmConstants.EXTRA_BITS[matchoffset];
                    matchoffset = (ChmConstants.POSITION_BASE[matchoffset] - 2);
                    if (extra > 3) {
                        extra -= 3;
                        long verbatim_bits = getChmSection().getSyncBits(extra);
                        matchoffset += (verbatim_bits << 3);
                        //READ HUFF SYM in Aligned Tree
                        int aligned_bits = getChmSection().peekBits(
                                ChmConstants.LZX_NUM_PRIMARY_LENGTHS);
                        int t = getState().getAlignedTreeTable()[aligned_bits];
                        if (t >= getState().getMainTreeElements()) {
                            x = ChmConstants.LZX_ALIGNED_TABLEBITS; //?LZX_MAINTREE_TABLEBITS; //?LZX_ALIGNED_TABLEBITS
                            do {
                                x++;
                                t <<= 1;
                                t += getChmSection().checkBit(x);
                            } while ((t = getState().getAlignedTreeTable()[t]) >= getState()
                                    .getMainTreeElements());
                        }
                        getChmSection().getSyncBits(
                                getState().getAlignedLenTable()[t]);
                        matchoffset += t;
                    } else if (extra == 3) {
                        int g = getChmSection().peekBits(
                                ChmConstants.LZX_NUM_PRIMARY_LENGTHS);
                        int t = getState().getAlignedTreeTable()[g];
                        if (t >= getState().getMainTreeElements()) {
                            x = ChmConstants.LZX_ALIGNED_TABLEBITS; //?LZX_MAINTREE_TABLEBITS;
                            do {
                                x++;
                                t <<= 1;
                                t += getChmSection().checkBit(x);
                            } while ((t = getState().getAlignedTreeTable()[t]) >= getState()
                                    .getMainTreeElements());
                        }
                        getChmSection().getSyncBits(
                                getState().getAlignedLenTable()[t]);
                        matchoffset += t;
                    } else if (extra > 0) {
                        long l = getChmSection().getSyncBits(extra);
                        matchoffset += l;
                    } else
                        matchoffset = 1;
                    getState().setR2(getState().getR1());
                    getState().setR1(getState().getR0());
                    getState().setR0(matchoffset);
                } else if (matchoffset == 0) {
                    matchoffset = (int) getState().getR0();
                } else if (matchoffset == 1) {
                    matchoffset = (int) getState().getR1();
                    getState().setR1(getState().getR0());
                    getState().setR0(matchoffset);
                } else /** match_offset == 2 */
                {
                    matchoffset = (int) getState().getR2();
                    getState().setR2(getState().getR0());
                    getState().setR0(matchoffset);
                }
                rundest = i;
                runsrc = rundest - matchoffset;
                i += (matchlen - 1);
                if (i > len)
                    break;

                if (runsrc < 0) {
                    if (matchlen + runsrc <= 0) {
                        runsrc = prevcontent.length + runsrc;
                        while (matchlen-- > 0)
                            content[rundest++] = prevcontent[runsrc++];
                    } else {
                        runsrc = prevcontent.length + runsrc;
                        while (runsrc < prevcontent.length)
                            content[rundest++] = prevcontent[runsrc++];
                        matchlen = matchlen + runsrc - prevcontent.length;
                        runsrc = 0;
                        while (matchlen-- > 0)
                            content[rundest++] = content[runsrc++];
                    }

                } else {
                    /* copies any wrappes around source data */
                    while ((runsrc < 0) && (matchlen-- > 0)) {
                        content[rundest++] = content[(int) (runsrc + getBlockLength())];
                        runsrc++;
                    }
                    /* copies match data - no worries about destination wraps */
                    while (matchlen-- > 0)
                        content[rundest++] = content[runsrc++];
                }
            }
        }
        setContentLength(len);
    }

    private void assertShortArrayNotNull(short[] array) throws TikaException {
        if (array == null)
            throw new ChmParsingException("short[] is null");
    }

    private void decompressVerbatimBlock(int len, byte[] prevcontent) throws TikaException {
        short s;
        int x, i;
        int matchlen = 0, matchfooter = 0, extra, rundest, runsrc;
        int matchoffset = 0;
        for (i = getContentLength(); i < len; i++) {
            int f = getChmSection().peekBits(
                    ChmConstants.LZX_MAINTREE_TABLEBITS);
            assertShortArrayNotNull(getState().getMainTreeTable());
            s = getState().getMainTreeTable()[f];
            if (s >= ChmConstants.LZX_MAIN_MAXSYMBOLS) {
                x = ChmConstants.LZX_MAINTREE_TABLEBITS;
                do {
                    x++;
                    s <<= 1;
                    s += getChmSection().checkBit(x);
                } while ((s = getState().getMainTreeTable()[s]) >= ChmConstants.LZX_MAIN_MAXSYMBOLS);
            }
            getChmSection().getSyncBits(getState().getMainTreeLengtsTable()[s]);
            if (s < ChmConstants.LZX_NUM_CHARS) {
                content[i] = (byte) s;
            } else {
                s -= ChmConstants.LZX_NUM_CHARS;
                matchlen = s & ChmConstants.LZX_NUM_PRIMARY_LENGTHS;
                if (matchlen == ChmConstants.LZX_NUM_PRIMARY_LENGTHS) {
                    matchfooter = getState().getLengthTreeTable()[getChmSection()
                            .peekBits(ChmConstants.LZX_LENGTH_TABLEBITS)];
                    if (matchfooter >= ChmConstants.LZX_NUM_SECONDARY_LENGTHS) {
                        x = ChmConstants.LZX_LENGTH_TABLEBITS;
                        do {
                            x++;
                            matchfooter <<= 1;
                            matchfooter += getChmSection().checkBit(x);
                        } while ((matchfooter = getState().getLengthTreeTable()[matchfooter]) >= ChmConstants.LZX_NUM_SECONDARY_LENGTHS);
                    }
                    getChmSection().getSyncBits(
                            getState().getLengthTreeLengtsTable()[matchfooter]);
                    matchlen += matchfooter;
                }
                matchlen += ChmConstants.LZX_MIN_MATCH;
                // shorter than 2
                matchoffset = s >>> 3;
                if (matchoffset > 2) {
                    if (matchoffset != 3) { // should get other bits to retrieve
                                            // offset
                        extra = ChmConstants.EXTRA_BITS[matchoffset];
                        long l = getChmSection().getSyncBits(extra);
                        matchoffset = (int) (ChmConstants.POSITION_BASE[matchoffset] - 2 + l);
                    } else {
                        matchoffset = 1;
                    }
                    getState().setR2(getState().getR1());
                    getState().setR1(getState().getR0());
                    getState().setR0(matchoffset);
                } else if (matchoffset == 0) {
                    matchoffset = (int) getState().getR0();
                } else if (matchoffset == 1) {
                    matchoffset = (int) getState().getR1();
                    getState().setR1(getState().getR0());
                    getState().setR0(matchoffset);
                } else /* match_offset == 2 */
                {
                    matchoffset = (int) getState().getR2();
                    getState().setR2(getState().getR0());
                    getState().setR0(matchoffset);
                }
                rundest = i;
                runsrc = rundest - matchoffset;
                i += (matchlen - 1);
                if (i > len)
                    break;
                if (runsrc < 0) {
                    if (matchlen + runsrc <= 0) {
                        runsrc = prevcontent.length + runsrc;
                        while ((matchlen-- > 0) && (prevcontent != null)
                                && ((runsrc + 1) > 0))
                            if ((rundest < content.length)
                                    && (runsrc < content.length))
                                content[rundest++] = prevcontent[runsrc++];
                    } else {
                        runsrc = prevcontent.length + runsrc;
                        while (runsrc < prevcontent.length)
                            if ((rundest < content.length)
                                    && (runsrc < content.length))
                                content[rundest++] = prevcontent[runsrc++];
                        matchlen = matchlen + runsrc - prevcontent.length;
                        runsrc = 0;
                        while (matchlen-- > 0)
                            content[rundest++] = content[runsrc++];
                    }

                } else {
                    /* copies any wrapped source data */
                    while ((runsrc < 0) && (matchlen-- > 0)) {
                        content[rundest++] = content[(int) (runsrc + getBlockLength())];
                        runsrc++;
                    }
                    /* copies match data - no worries about destination wraps */
                    while (matchlen-- > 0) {
                        if ((rundest < content.length)
                                && (runsrc < content.length))
                            content[rundest++] = content[runsrc++];
                    }
                }
            }
        }
        setContentLength(len);
    }

    private void createLengthTreeLenTable(int offset, int tablelen,
            short[] pretreetable, short[] prelentable) throws TikaException {
        if (prelentable == null || getChmSection() == null
                || pretreetable == null || prelentable == null)
            throw new ChmParsingException("is null");

        int i = offset; // represents offset
        int z, y, x;// local counters
        while (i < tablelen) {
            //Read HUFF sym to z
            z = pretreetable[getChmSection().peekBits(
                    ChmConstants.LZX_PRETREE_TABLEBITS)];
            if (z >= ChmConstants.LZX_PRETREE_NUM_ELEMENTS) {// 1 bug, should be
                                                             // 20
                x = ChmConstants.LZX_PRETREE_TABLEBITS;
                do {
                    x++;
                    z <<= 1;
                    z += getChmSection().checkBit(x);
                } while ((z = pretreetable[z]) >= ChmConstants.LZX_PRETREE_NUM_ELEMENTS);
            }
            getChmSection().getSyncBits(prelentable[z]);
            
            if (z < 17) {
                z = getState().getLengthTreeLengtsTable()[i] - z;
                if (z < 0)
                    z = z + 17;
                getState().getLengthTreeLengtsTable()[i] = (short) z;
                i++;
            } else if (z == 17) {
                y = getChmSection().getSyncBits(4);
                y += 4;
                for (int j = 0; j < y; j++)
                    if (i < getState().getLengthTreeLengtsTable().length)
                        getState().getLengthTreeLengtsTable()[i++] = 0;
            } else if (z == 18) {
                y = getChmSection().getSyncBits(5);
                y += 20;
                for (int j = 0; j < y; j++)
                    //no tolerate //if (i < getState().getLengthTreeLengtsTable().length)
                        getState().getLengthTreeLengtsTable()[i++] = 0;
            } else if (z == 19) {
                y = getChmSection().getSyncBits(1);
                y += 4;
                z = pretreetable[getChmSection().peekBits(
                        ChmConstants.LZX_PRETREE_TABLEBITS)];
                if (z >= ChmConstants.LZX_PRETREE_NUM_ELEMENTS) {// 20
                    x = ChmConstants.LZX_PRETREE_TABLEBITS;// 6
                    do {
                        x++;
                        z <<= 1;
                        z += getChmSection().checkBit(x);
                    } while ((z = pretreetable[z]) >= ChmConstants.LZX_PRETREE_NUM_ELEMENTS);//LZX_MAINTREE_TABLEBITS);
                }
                getChmSection().getSyncBits(prelentable[z]);
                z = getState().getLengthTreeLengtsTable()[i] - z;
                if (z < 0)
                    z = z + 17;
                for (int j = 0; j < y; j++)
                    getState().getLengthTreeLengtsTable()[i++] = (short) z;
            }
        }
    }

    private void createMainTreeTable() throws TikaException {
        //Read Pre Tree Table
        short[] prelentable = createPreLenTable();
        short[] pretreetable = createTreeTable2(prelentable,
                (1 << ChmConstants.LZX_PRETREE_TABLEBITS)
                        + (ChmConstants.LZX_PRETREE_MAXSYMBOLS << 1),
                ChmConstants.LZX_PRETREE_TABLEBITS,
                ChmConstants.LZX_PRETREE_MAXSYMBOLS);

        createMainTreeLenTable(0, ChmConstants.LZX_NUM_CHARS, pretreetable,
                prelentable);
        
        //Read Pre Tree Table
        prelentable = createPreLenTable();
        pretreetable = createTreeTable2(prelentable,
                (1 << ChmConstants.LZX_PRETREE_TABLEBITS)
                        + (ChmConstants.LZX_PRETREE_MAXSYMBOLS << 1),
                ChmConstants.LZX_PRETREE_TABLEBITS,
                ChmConstants.LZX_PRETREE_MAXSYMBOLS);

        createMainTreeLenTable(ChmConstants.LZX_NUM_CHARS,
                getState().mainTreeLengtsTable.length, pretreetable,
                prelentable);

        getState().setMainTreeTable(
                createTreeTable2(getState().mainTreeLengtsTable,
                        (1 << ChmConstants.LZX_MAINTREE_TABLEBITS)
                                + (ChmConstants.LZX_MAINTREE_MAXSYMBOLS << 1),
                        ChmConstants.LZX_MAINTREE_TABLEBITS, getState()
                                .getMainTreeElements()));
    }

    private void createMainTreeLenTable(int offset, int tablelen,
            short[] pretreetable, short[] prelentable) throws TikaException {
        if (pretreetable == null)
            throw new ChmParsingException("pretreetable is null");
        int i = offset;
        int z, y, x;
        while (i < tablelen) {
            int f = getChmSection().peekBits(
                    ChmConstants.LZX_PRETREE_TABLEBITS);
            z = pretreetable[f];
            if (z >= ChmConstants.LZX_PRETREE_MAXSYMBOLS) {
                x = ChmConstants.LZX_PRETREE_TABLEBITS;
                do {
                    x++;
                    z <<= 1;
                    z += getChmSection().checkBit(x);
                } while ((z = pretreetable[z]) >= ChmConstants.LZX_PRETREE_MAXSYMBOLS);
            }
            getChmSection().getSyncBits(prelentable[z]);
            if (z < 17) {
                z = getState().getMainTreeLengtsTable()[i] - z;
                if (z < 0)
                    z = z + 17;
                getState().mainTreeLengtsTable[i] = (short) z;
                i++;
            } else if (z == 17) {
                y = getChmSection().getSyncBits(4);
                y += 4;
                for (int j = 0; j < y; j++) {
                    assertInRange(getState().getMainTreeLengtsTable(), i);
                    getState().mainTreeLengtsTable[i++] = 0;
                }
            } else if (z == 18) {
                y = getChmSection().getSyncBits(5);
                y += 20;
                for (int j = 0; j < y; j++) {
                    assertInRange(getState().getMainTreeLengtsTable(), i);
                    getState().mainTreeLengtsTable[i++] = 0;
                }
            } else if (z == 19) {
                y = getChmSection().getSyncBits(1);
                y += 4;
                z = pretreetable[getChmSection().peekBits(
                        ChmConstants.LZX_PRETREE_TABLEBITS)];
                if (z >= ChmConstants.LZX_PRETREE_MAXSYMBOLS) {
                    x = ChmConstants.LZX_PRETREE_TABLEBITS;
                    do {
                        x++;
                        z <<= 1;
                        z += getChmSection().checkBit(x);
                    } while ((z = pretreetable[z]) >= ChmConstants.LZX_PRETREE_MAXSYMBOLS);
                }
                getChmSection().getSyncBits(prelentable[z]);
                z = getState().mainTreeLengtsTable[i] - z;
                if (z < 0)
                    z = z + 17;
                for (int j = 0; j < y; j++)
                    if (i < getState().getMainTreeLengtsTable().length)
                        getState().mainTreeLengtsTable[i++] = (short) z;
            }
        }
    }

    private void assertInRange(short[] array, int index) throws ChmParsingException {
        if (index >= array.length)
            throw new ChmParsingException(index + " is bigger than "
                    + array.length);
    }

    private short[] createAlignedLenTable() {
        int tablelen = ChmConstants.LZX_ALIGNED_NUM_ELEMENTS;//LZX_BLOCKTYPE_UNCOMPRESSED;//
        int bits = ChmConstants.LZX_BLOCKTYPE_UNCOMPRESSED;
        short[] tmp = new short[tablelen];
        for (int i = 0; i < tablelen; i++) {
            tmp[i] = (short) getChmSection().getSyncBits(bits);
        }
        return tmp;
    }

    private void createAlignedTreeTable() throws ChmParsingException {
        getState().setAlignedLenTable(createAlignedLenTable());
        getState().setAlignedTreeTable(//setAlignedLenTable(
                createTreeTable2(getState().getAlignedLenTable(),
                        (1 << ChmConstants.LZX_NUM_PRIMARY_LENGTHS)
                                + (ChmConstants.LZX_ALIGNED_MAXSYMBOLS << 1),
                        ChmConstants.LZX_NUM_PRIMARY_LENGTHS,
                        ChmConstants.LZX_ALIGNED_MAXSYMBOLS));
    }

    private short[] createTreeTable2(short[] lentable, int tablelen, int bits,
            int maxsymbol) throws ChmParsingException {
        short[] tmp = new short[tablelen];
        short sym;
        int leaf;
        int bit_num = 1;
        long fill;
        int pos = 0;
        /* the current position in the decode table */
        long table_mask = (1 << bits);
        long bit_mask = (table_mask >> 1);
        long next_symbol = bit_mask;

        /* fills entries for short codes for a direct mapping */
        while (bit_num <= bits) {
            for (sym = 0; sym < maxsymbol; sym++) {
                if (lentable.length > sym && lentable[sym] == bit_num) {
                    leaf = pos;

                    if ((pos += bit_mask) > table_mask) {
                        /* table overflow */
                        throw new ChmParsingException("Table overflow");
                    }

                    fill = bit_mask;
                    while (fill-- > 0)
                        tmp[leaf++] = sym;
                }
            }
            bit_mask >>= 1;
            bit_num++;
        }

        /* if there are any codes longer than nbits */
        if (pos != table_mask) {
            /* clears the remainder of the table */
            for (leaf = pos; leaf < table_mask; leaf++)
                tmp[leaf] = 0;

            /* gives ourselves room for codes to grow by up to 16 more bits */
            pos <<= 16;
            table_mask <<= 16;
            bit_mask = 1 << 15;

            while (bit_num <= 16) {
                for (sym = 0; sym < maxsymbol; sym++) {
                    if ((lentable.length > sym) && (lentable[sym] == bit_num)) {
                        leaf = pos >> 16;
                        for (fill = 0; fill < bit_num - bits; fill++) {
                            /*
                             * if this path hasn't been taken yet, 'allocate'
                             * two entries
                             */
                            if (tmp[leaf] == 0) {
                                if (((next_symbol << 1) + 1) < tmp.length) {
                                    tmp[(int) (next_symbol << 1)] = 0;
                                    tmp[(int) (next_symbol << 1) + 1] = 0;
                                    tmp[leaf] = (short) next_symbol++;
                                }

                            }
                            /*
                             * follows the path and select either left or right
                             * for next bit
                             */
                            leaf = tmp[leaf] << 1;
                            if (((pos >> (15 - fill)) & 1) != 0)
                                leaf++;
                        }
                        tmp[leaf] = sym;

                        if ((pos += bit_mask) > table_mask) {
                            /* table overflow */
                            throw new ChmParsingException("Table overflow");
                        }
                    }
                }
                bit_mask >>= 1;
                bit_num++;
            }
        }

        /* is it full table? */
        if (pos == table_mask)
            return tmp;

        return tmp;
    }

    public byte[] getContent() {
        return content;
    }

    public byte[] getContent(int startOffset, int endOffset) {
        return (getContent() != null) ? ChmCommons.copyOfRange(getContent(),
                startOffset, endOffset) : new byte[1];
    }

    public byte[] getContent(int start) {
        return (getContent() != null) ? ChmCommons.copyOfRange(getContent(),
                start, getContent().length) : new byte[1];
    }

    private void setContent(int contentLength) {
        this.content = new byte[contentLength];
    }

    private void checkLzxBlock(ChmLzxBlock chmPrevLzxBlock) throws TikaException {
        if (chmPrevLzxBlock == null && getBlockLength() < Integer.MAX_VALUE)
            setState(new ChmLzxState((int) getBlockLength()));
        else
            //use clone to avoid changing a cached or to be cached block
            setState(chmPrevLzxBlock.getState().clone()); 
    }

    private boolean validateConstructorParams(int blockNumber,
            byte[] dataSegment, long blockLength) throws TikaException {
        int goodParameter = 0;
        if (blockNumber >= 0)
            ++goodParameter;
        else
            throw new ChmParsingException("block number should be possitive");
        if (dataSegment != null && dataSegment.length > 0)
            ++goodParameter;
        else
            throw new ChmParsingException("data segment should not be null");
        if (blockLength > 0)
            ++goodParameter;
        else
            throw new ChmParsingException(
                    "block length should be more than zero");
        return (goodParameter == 3);
    }

    public int getBlockNumber() {
        return block_number;
    }

    private void setBlockNumber(int block_number) {
        this.block_number = block_number;
    }

    private long getBlockLength() {
        return block_length;
    }

    private void setBlockLength(long block_length) {
        this.block_length = block_length;
    }

    public ChmLzxState getState() {
        return state;
    }

    private void setState(ChmLzxState state) {
        this.state = state;
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/chm/lzx/ChmLzxState.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.chm.lzx;

import java.util.concurrent.CancellationException;
import org.apache.tika.exception.TikaException;
import org.apache.tika.parser.chm.core.ChmCommons;
import org.apache.tika.parser.chm.core.ChmCommons.IntelState;
import org.apache.tika.parser.chm.core.ChmCommons.LzxState;
import org.apache.tika.parser.chm.core.ChmConstants;
import org.apache.tika.parser.chm.exception.ChmParsingException;

public class ChmLzxState implements Cloneable {
    /* Class' members */
    private int window; /* the actual decoding window */
    private long window_size; /* window size (32Kb through 2Mb) */
    private int window_position; /* current offset within the window */
    private int main_tree_elements; /* number of main tree elements */
    private LzxState hadStarted; /* have we started decoding at all yet? */
    private int block_type; /* type of this block */
    private int block_length; /* uncompressed length of this block */
    private int block_remaining; /* uncompressed bytes still left to decode */
    private int frames_read; /* the number of CFDATA blocks processed */
    private int intel_file_size; /* magic header value used for transform */
    private long intel_current_possition; /* current offset in transform space */
    private IntelState intel_state; /* have we seen any translatable data yet? */
    private long R0; /* for the LRU offset system */
    private long R1; /* for the LRU offset system */
    private long R2; /* for the LRU offset system */

    // Trees - PRETREE, MAINTREE, LENGTH, ALIGNED
    protected short[] mainTreeLengtsTable;
    protected short[] mainTreeTable;

    protected short[] lengthTreeTable;
    protected short[] lengthTreeLengtsTable;

    protected short[] alignedLenTable;
    protected short[] alignedTreeTable;

    @Override
    public ChmLzxState clone() {
        try {
          ChmLzxState clone = (ChmLzxState)super.clone();
          clone.mainTreeLengtsTable = arrayClone(mainTreeLengtsTable);
          clone.mainTreeTable = arrayClone(mainTreeTable);
          clone.lengthTreeTable = arrayClone(lengthTreeTable);
          clone.lengthTreeLengtsTable = arrayClone(lengthTreeLengtsTable);
          clone.alignedLenTable = arrayClone(alignedLenTable);
          clone.alignedTreeTable = arrayClone(alignedTreeTable);
          return clone;
        } catch (CloneNotSupportedException ex) {
           return null;
        }
    }
    
    protected short[] getMainTreeTable() {
        return mainTreeTable;
    }

    protected short[] getAlignedTreeTable() {
        return alignedTreeTable;
    }

    protected void setAlignedTreeTable(short[] alignedTreeTable) {
        this.alignedTreeTable = alignedTreeTable;
    }

    protected short[] getLengthTreeTable() throws TikaException {
        if (lengthTreeTable != null)
            return this.lengthTreeTable;
        else
            throw new ChmParsingException("lengthTreeTable is null");
    }

    protected void setLengthTreeTable(short[] lengthTreeTable) {
        this.lengthTreeTable = lengthTreeTable;
    }

    protected void setMainTreeTable(short[] mainTreeTable) {
        this.mainTreeTable = mainTreeTable;
    }

    protected short[] getAlignedLenTable() {
        return this.alignedLenTable;
    }

    protected void setAlignedLenTable(short[] alignedLenTable) {
        this.alignedLenTable = alignedLenTable;
    }

    /**
     * It suits for informative outlook
     */
    public String toString() {
        StringBuilder sb = new StringBuilder();
        sb.append("actual decoding window:=" + getWindow()
                + System.getProperty("line.separator"));
        sb.append("window size (32Kb through 2Mb):=" + getWindowSize()
                + System.getProperty("line.separator"));
        sb.append("current offset within the window:=" + getWindowPosition()
                + System.getProperty("line.separator"));
        sb.append("number of main tree elements:=" + getMainTreeElements()
                + System.getProperty("line.separator"));
        sb.append("have we started decoding at all yet?:=" + getHadStarted()
                + System.getProperty("line.separator"));
        sb.append("type of this block:=" + getBlockType()
                + System.getProperty("line.separator"));
        sb.append("uncompressed length of this block:=" + getBlockLength()
                + System.getProperty("line.separator"));
        sb.append("uncompressed bytes still left to decode:="
                + getBlockRemaining() + System.getProperty("line.separator"));
        sb.append("the number of CFDATA blocks processed:=" + getFramesRead()
                + System.getProperty("line.separator"));
        sb.append("magic header value used for transform:="
                + getIntelFileSize() + System.getProperty("line.separator"));
        sb.append("current offset in transform space:="
                + getIntelCurrentPossition()
                + System.getProperty("line.separator"));
        sb.append("have we seen any translatable data yet?:=" + getIntelState()
                + System.getProperty("line.separator"));
        sb.append("R0 for the LRU offset system:=" + getR0()
                + System.getProperty("line.separator"));
        sb.append("R1 for the LRU offset system:=" + getR1()
                + System.getProperty("line.separator"));
        sb.append("R2 for the LRU offset system:=" + getR2()
                + System.getProperty("line.separator"));
        sb.append("main tree length:=" + getMainTreeLengtsTable().length
                + System.getProperty("line.separator"));
        sb.append("secondary tree length:=" + getLengthTreeLengtsTable().length
                + System.getProperty("line.separator"));
        return sb.toString();
    }

    public ChmLzxState(int window) throws TikaException {
        if (window >= 0) {
            int position_slots;
            int win = ChmCommons.getWindowSize(window);
            setWindowSize(1 << win);
            /* LZX supports window sizes of 2^15 (32Kb) through 2^21 (2Mb) */
            if (win < 15 || win > 21)
                throw new ChmParsingException("window less than 15 or window greater than 21");

            /* Calculates required position slots */
            if (win == 20)
                position_slots = 42;
            else if (win == 21)
                position_slots = 50;
            else
                position_slots = win << 1;
            //TODO: position_slots is not used ?
            setR0(1);
            setR1(1);
            setR2(1);
            setMainTreeElements(512);
            setHadStarted(LzxState.NOT_STARTED_DECODING);
            setFramesRead(0);
            setBlockRemaining(0);
            setBlockType(ChmConstants.LZX_BLOCKTYPE_INVALID);
            setIntelCurrentPossition(0);
            setIntelState(IntelState.NOT_STARTED);
            setWindowPosition(0);
            setMainTreeLengtsTable(new short[getMainTreeElements()]);
            setLengthTreeLengtsTable(new short[ChmConstants.LZX_NUM_SECONDARY_LENGTHS]);
        } else
            throw new CancellationException(
                    "window size should be more than zero");
    }

    protected void setWindow(int window) {
        this.window = window;
    }

    protected int getWindow() {
        return window;
    }

    protected void setWindowSize(long window_size) {
        this.window_size = window_size;
    }

    protected long getWindowSize() {
        return window_size;
    }

    protected void setWindowPosition(int window_position) {
        this.window_position = window_position;
    }

    protected int getWindowPosition() {
        return window_position;
    }

    protected void setMainTreeElements(int main_tree_elements) {
        this.main_tree_elements = main_tree_elements;
    }

    protected int getMainTreeElements() {
        return main_tree_elements;
    }

    protected void setHadStarted(LzxState hadStarted) {
        this.hadStarted = hadStarted;
    }

    protected LzxState getHadStarted() {
        return hadStarted;
    }

    protected void setBlockType(int block_type) {
        this.block_type = block_type;
    }

    public int getBlockType() {
        return block_type;
    }

    protected void setBlockLength(int block_length) {
        this.block_length = block_length;
    }

    protected int getBlockLength() {
        return block_length;
    }

    protected void setBlockRemaining(int block_remaining) {
        this.block_remaining = block_remaining;
    }

    protected int getBlockRemaining() {
        return block_remaining;
    }

    protected void setFramesRead(int frames_read) {
        this.frames_read = frames_read;
    }

    protected void increaseFramesRead() {
        this.frames_read = getFramesRead() + 1;
    }

    protected int getFramesRead() {
        return frames_read;
    }

    protected void setIntelFileSize(int intel_file_size) {
        this.intel_file_size = intel_file_size;
    }

    protected int getIntelFileSize() {
        return intel_file_size;
    }

    protected void setIntelCurrentPossition(long intel_current_possition) {
        this.intel_current_possition = intel_current_possition;
    }

    protected long getIntelCurrentPossition() {
        return intel_current_possition;
    }

    protected void setIntelState(IntelState intel_state) {
        this.intel_state = intel_state;
    }

    protected IntelState getIntelState() {
        return intel_state;
    }

    protected void setR0(long r0) {
        R0 = r0;
    }

    protected long getR0() {
        return R0;
    }

    protected void setR1(long r1) {
        R1 = r1;
    }

    protected long getR1() {
        return R1;
    }

    protected void setR2(long r2) {
        R2 = r2;
    }

    protected long getR2() {
        return R2;
    }

    public void setMainTreeLengtsTable(short[] mainTreeLengtsTable) {
        this.mainTreeLengtsTable = mainTreeLengtsTable;
    }

    public short[] getMainTreeLengtsTable() {
        return mainTreeLengtsTable;
    }

    public void setLengthTreeLengtsTable(short[] lengthTreeLengtsTable) {
        this.lengthTreeLengtsTable = lengthTreeLengtsTable;
    }

    public short[] getLengthTreeLengtsTable() {
        return lengthTreeLengtsTable;
    }
    
    private static short[] arrayClone(short[] a) {
        return a==null ? null : (short[]) a.clone();
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/chm/lzx/ChmSection.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.chm.lzx;

import java.math.BigInteger;
import java.util.Arrays;

import org.apache.tika.exception.TikaException;
import org.apache.tika.parser.chm.core.ChmCommons;

public class ChmSection {
    final private byte[] data;
    final private byte[] prevcontent;
    private int swath;// kiks
    private int total;// remains
    private int buffer;// val

    public ChmSection(byte[] data) throws TikaException {
        this(data, null);
    }

    public ChmSection(byte[] data, byte[] prevconent) throws TikaException {
        ChmCommons.assertByteArrayNotNull(data);
        this.data = data;
        this.prevcontent = prevconent;
        //setData(data);
    }
    
    /* Utilities */
    public byte[] reverseByteOrder(byte[] toBeReversed) throws TikaException {
        ChmCommons.assertByteArrayNotNull(toBeReversed);
        ChmCommons.reverse(toBeReversed);
        return toBeReversed;
    }

    public int checkBit(int i) {
        return ((getBuffer() & (1 << (getTotal() - i))) == 0) ? 0 : 1;
    }

    public int getSyncBits(int bit) {
        return getDesyncBits(bit, bit);
    }

    public int peekBits(int bit) {
        return getDesyncBits(bit, 0);
    }
    
    private int getDesyncBits(int bit, int removeBit) {
        while (getTotal() < 16) {
            setBuffer((getBuffer() << 16) + unmarshalUByte()
                    + (unmarshalUByte() << 8));
            setTotal(getTotal() + 16);
        }
        int tmp = (getBuffer() >>> (getTotal() - bit));
        setTotal(getTotal() - removeBit);
        setBuffer(getBuffer() - ((getBuffer() >>> getTotal()) << getTotal()));
        return tmp;
    }

    public int unmarshalUByte() {
        return getByte() & 255;
    }

    public byte getByte() {
        if (getSwath() < getData().length) {
            setSwath(getSwath() + 1);
            return getData()[getSwath() - 1];
        } else
            return 0;
    }

    public int getLeft() {
        return (getData().length - getSwath());
    }

    public byte[] getData() {
        return data;
    }

    public byte[] getPrevContent() {
        return prevcontent;
    }
    
    public BigInteger getBigInteger(int i) {
        if (getData() == null)
            return BigInteger.ZERO;
        if (getData().length - getSwath() < i)
            i = getData().length - getSwath();
        byte[] tmp = new byte[i];
        for (int j = i - 1; j >= 0; j--) {
            tmp[i - j - 1] = getData()[getSwath() + j];
        }
        setSwath(getSwath() + i);
        return new BigInteger(tmp);
    }

    public byte[] stringToAsciiBytes(String s) {
        char[] c = s.toCharArray();
        byte[] byteval = new byte[c.length];
        for (int i = 0; i < c.length; i++)
            byteval[i] = (byte) c[i];
        return byteval;
    }

    public BigInteger unmarshalUlong() {
        return getBigInteger(8);
    }

    public long unmarshalUInt() {
        return getBigInteger(4).longValue();
    }

    public int unmarshalInt() {
        return getBigInteger(4).intValue();
    }

    public byte[] unmarshalBytes(int i) {
        if (i == 0)
            return new byte[1];
        byte[] t = new byte[i];
        for (int j = 0; j < i; j++)
            t[j] = getData()[j + getSwath()];
        setSwath(getSwath() + i);
        return t;
    }

    public BigInteger getEncint() {
        byte ob;
        BigInteger bi = BigInteger.ZERO;
        byte[] nb = new byte[1];
        while ((ob = this.getByte()) < 0) {
            nb[0] = (byte) ((ob & 0x7f));
            bi = bi.shiftLeft(7).add(new BigInteger(nb));
        }
        nb[0] = (byte) ((ob & 0x7f));
        bi = bi.shiftLeft(7).add(new BigInteger(nb));
        return bi;
    }

    public char unmarshalUtfChar() {
        byte ob;
        int i = 1;
        byte[] ba;
        ob = this.getByte();
        if (ob < 0) {
            i = 2;
            while ((ob << (24 + i)) < 0)
                i++;
        }
        ba = new byte[i];
        ba[0] = ob;
        int j = 1;
        while (j < i) {
            ba[j] = this.getByte();
            j++;
        }
        i = ba.length;
        if (i == 1)
            return (char) ba[0];
        else {
            int n;
            n = ba[0] & 15; // 00001111b, gets last 4 bits
            j = 1;
            while (j < i)
                n = (n << 6) + (ba[j++] & 63);// 00111111b,gets last 6 bits
            return (char) n;
        }
    }

//    private void setData(byte[] data) {
//        this.data = data;
//    }

    public int getSwath() {
        return swath;
    }

    public void setSwath(int swath) {
        this.swath = swath;
    }

    public int getTotal() {
        return total;
    }

    public void setTotal(int total) {
        this.total = total;
    }

    private int getBuffer() {
        return buffer;
    }

    private void setBuffer(int buffer) {
        this.buffer = buffer;
    }

    /**
     * @param args
     * @throws TikaException 
     */
    public static void main(String[] args) throws TikaException {
        byte[] array = { 4, 78, -67, 90, 1, -33 };
        ChmSection chmSection = new ChmSection(array);
        System.out.println("before " + Arrays.toString(array));
        System.out.println("after " + Arrays.toString(chmSection.reverseByteOrder(array)));
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/code/SourceCodeParser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.code;

import static com.uwyn.jhighlight.renderer.XhtmlRendererFactory.CPP;
import static com.uwyn.jhighlight.renderer.XhtmlRendererFactory.GROOVY;
import static com.uwyn.jhighlight.renderer.XhtmlRendererFactory.JAVA;

import java.io.IOException;
import java.io.InputStream;
import java.io.StringReader;
import java.nio.charset.Charset;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import org.apache.tika.config.ServiceLoader;
import org.apache.tika.detect.AutoDetectReader;
import org.apache.tika.exception.TikaException;
import org.apache.tika.io.CloseShieldInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.Parser;
import org.ccil.cowan.tagsoup.HTMLSchema;
import org.ccil.cowan.tagsoup.Schema;
import org.xml.sax.ContentHandler;
import org.xml.sax.InputSource;
import org.xml.sax.SAXException;

import com.uwyn.jhighlight.renderer.Renderer;
import com.uwyn.jhighlight.renderer.XhtmlRendererFactory;
/**
 * Generic Source code parser for Java, Groovy, C++
 *
 * @author Hong-Thai.Nguyen
 * @since 1.6
 */
public class SourceCodeParser implements Parser {

  private static final long serialVersionUID = -4543476498190054160L;

  private static final Pattern authorPattern = Pattern.compile("(?im)@author (.*) *$");

  private static final Map<MediaType, String> TYPES_TO_RENDERER = new HashMap<MediaType, String>() {
    private static final long serialVersionUID = -741976157563751152L;
    {
      put(MediaType.text("x-c++src"), CPP);
      put(MediaType.text("x-java-source"), JAVA);
      put(MediaType.text("x-groovy"), GROOVY);
    }
  };

  private static final ServiceLoader LOADER = new ServiceLoader(SourceCodeParser.class.getClassLoader());
  
  //Parse the HTML document
  private static final Schema HTML_SCHEMA = new HTMLSchema();
  
  @Override
  public Set<MediaType> getSupportedTypes(ParseContext context) {
    return TYPES_TO_RENDERER.keySet();
  }

  @Override
  public void parse(InputStream stream, ContentHandler handler, Metadata metadata, ParseContext context)
      throws IOException, SAXException, TikaException {

    AutoDetectReader reader = new AutoDetectReader(new CloseShieldInputStream(stream), metadata, context.get(ServiceLoader.class, LOADER));

    try {
      Charset charset = reader.getCharset();
      String mediaType = metadata.get(Metadata.CONTENT_TYPE);
      String name = metadata.get(Metadata.RESOURCE_NAME_KEY);
      if (mediaType != null && name != null) {
        MediaType type = MediaType.parse(mediaType);
        metadata.set(Metadata.CONTENT_TYPE, type.toString());
        metadata.set(Metadata.CONTENT_ENCODING, charset.name());

        StringBuilder out = new StringBuilder();
        String line;
        int nbLines =  0;
        while ((line = reader.readLine()) != null) {
            out.append(line + System.getProperty("line.separator"));
            String author = parserAuthor(line);
            if (author != null) {
              metadata.add(TikaCoreProperties.CREATOR, author);
            }
            nbLines ++;
        }
        metadata.set("LoC", String.valueOf(nbLines));
        Renderer renderer = getRenderer(type.toString());
        
        String codeAsHtml = renderer.highlight(name, out.toString(), charset.name(), false);
        
        Schema schema = context.get(Schema.class, HTML_SCHEMA);

        org.ccil.cowan.tagsoup.Parser parser = new org.ccil.cowan.tagsoup.Parser();
        parser.setProperty(org.ccil.cowan.tagsoup.Parser.schemaProperty, schema);
        parser.setContentHandler(handler);
        parser.parse(new InputSource(new StringReader(codeAsHtml)));
      }
    } finally {
      reader.close();
    }

  }

  private Renderer getRenderer(String mimeType) {
    MediaType mt = MediaType.parse(mimeType);
    String type = TYPES_TO_RENDERER.get(mt);
    if (type == null) {
      throw new RuntimeException("unparseable content type " + mimeType);
    }
    return XhtmlRendererFactory.getRenderer(type);
  }


  private String parserAuthor(String line) {
    Matcher m = authorPattern.matcher(line);
    if (m.find()) {
      return m.group(1).trim();
    }

    return null;
  }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/crypto/Pkcs7Parser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.crypto;

import java.io.IOException;
import java.io.InputStream;
import java.util.Set;

import org.apache.tika.exception.TikaException;
import org.apache.tika.io.CloseShieldInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.EmptyParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.Parser;
import org.bouncycastle.cms.CMSException;
import org.bouncycastle.cms.CMSSignedDataParser;
import org.bouncycastle.cms.CMSTypedStream;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * Basic parser for PKCS7 data.
 */
public class Pkcs7Parser extends AbstractParser {

    /** Serial version UID */
    private static final long serialVersionUID = -7310531559075115044L;

    private static final MediaType PKCS7_MIME =
            MediaType.application("pkcs7-mime");

    private static final MediaType PKCS7_SIGNATURE =
            MediaType.application("pkcs7-signature");

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return MediaType.set(PKCS7_MIME, PKCS7_SIGNATURE);
    }

    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        try {
            CMSSignedDataParser parser =
                    new CMSSignedDataParser(new CloseShieldInputStream(stream));
            try {
                CMSTypedStream content = parser.getSignedContent();     
                if (content == null) {
                  throw new TikaException("cannot parse detached pkcs7 signature (no signed data to parse)");
                }
                InputStream input = content.getContentStream();
                try {
                    Parser delegate =
                            context.get(Parser.class, EmptyParser.INSTANCE);
                    delegate.parse(input, handler, metadata, context);
                } finally {
                    input.close();
                }
            } finally {
                parser.close();
            }
        } catch (CMSException e) {
            throw new TikaException("Unable to parse pkcs7 signed data", e);
        }
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/dwg/DWGParser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.dwg;

import java.io.IOException;
import java.io.InputStream;
import java.util.Collections;
import java.util.Set;

import org.apache.poi.util.IOUtils;
import org.apache.poi.util.StringUtil;
import org.apache.tika.exception.TikaException;
import org.apache.tika.io.EndianUtils;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.Property;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * DWG (CAD Drawing) parser. This is a very basic parser, which just
 *  looks for bits of the headers.
 * Note that we use Apache POI for various parts of the processing, as
 *  lots of the low level string/int/short concepts are the same.
 */
public class DWGParser extends AbstractParser {

    /** Serial version UID */
    private static final long serialVersionUID = -7744232583079169119L;

    private static MediaType TYPE = MediaType.image("vnd.dwg");

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return Collections.singleton(TYPE);
    }

    /** The order of the fields in the header */
    private static final Property[] HEADER_PROPERTIES_ENTRIES = {
        TikaCoreProperties.TITLE, 
        TikaCoreProperties.TRANSITION_SUBJECT_TO_DC_DESCRIPTION,
        TikaCoreProperties.CREATOR,
        TikaCoreProperties.TRANSITION_KEYWORDS_TO_DC_SUBJECT,
        TikaCoreProperties.COMMENTS,
        TikaCoreProperties.MODIFIER,
        null, // Unknown?
        TikaCoreProperties.RELATION, // Hyperlink
    };

    /** For the 2000 file, they're indexed */
    private static final Property[] HEADER_2000_PROPERTIES_ENTRIES = {
       null, 
       TikaCoreProperties.RELATION, // 0x01
       TikaCoreProperties.TITLE,    // 0x02
       TikaCoreProperties.TRANSITION_SUBJECT_TO_DC_DESCRIPTION,  // 0x03
       TikaCoreProperties.CREATOR,   // 0x04
       null,
       TikaCoreProperties.COMMENTS,// 0x06 
       TikaCoreProperties.TRANSITION_KEYWORDS_TO_DC_SUBJECT,    // 0x07
       TikaCoreProperties.MODIFIER, // 0x08
   };

    private static final String HEADER_2000_PROPERTIES_MARKER_STR =
            "DWGPROPS COOKIE";

    private static final byte[] HEADER_2000_PROPERTIES_MARKER =
            new byte[HEADER_2000_PROPERTIES_MARKER_STR.length()];

    static {
        StringUtil.putCompressedUnicode(
                HEADER_2000_PROPERTIES_MARKER_STR,
                HEADER_2000_PROPERTIES_MARKER, 0);
    }

    /** 
     * How far to skip after the last standard property, before
     *  we find any custom properties that might be there.
     */
    private static final int CUSTOM_PROPERTIES_SKIP = 20;
    
    /** 
     * The value of padding bytes other than 0 in some DWG files.
     */
    private static final int[] CUSTOM_PROPERTIES_ALT_PADDING_VALUES = new int[] {0x2, 0, 0, 0};

    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, TikaException, SAXException {
        // First up, which version of the format are we handling?
        byte[] header = new byte[128];
        IOUtils.readFully(stream, header);
        String version = new String(header, 0, 6, "US-ASCII");

        XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);
        xhtml.startDocument();

        if (version.equals("AC1015")) {
            metadata.set(Metadata.CONTENT_TYPE, TYPE.toString());
            if (skipTo2000PropertyInfoSection(stream, header)) {
                get2000Props(stream,metadata,xhtml);
            }
        } else if (version.equals("AC1018")) {
            metadata.set(Metadata.CONTENT_TYPE, TYPE.toString());
            if (skipToPropertyInfoSection(stream, header)) {
                get2004Props(stream,metadata,xhtml);
            }
        } else if (version.equals("AC1021") || version.equals("AC1024")) {
            metadata.set(Metadata.CONTENT_TYPE, TYPE.toString());
            if (skipToPropertyInfoSection(stream, header)) {
                get2007and2010Props(stream,metadata,xhtml);
            }
        } else {
            throw new TikaException(
                    "Unsupported AutoCAD drawing version: " + version);
        }

        xhtml.endDocument();
    }

    /**
     * Stored as US-ASCII
     */
    private void get2004Props(
            InputStream stream, Metadata metadata, XHTMLContentHandler xhtml)
            throws IOException, TikaException, SAXException {
       // Standard properties
        for (int i = 0; i < HEADER_PROPERTIES_ENTRIES.length; i++) {
            String headerValue = read2004String(stream);
            handleHeader(i, headerValue, metadata, xhtml);
        }

        // Custom properties
        int customCount = skipToCustomProperties(stream);
        for (int i = 0; i < customCount; i++) {
           String propName = read2004String(stream);
           String propValue = read2004String(stream);
           if(propName.length() > 0 && propValue.length() > 0) {
              metadata.add(propName, propValue);
           }
        }
    }

    private String read2004String(InputStream stream) throws IOException, TikaException {
       int stringLen = EndianUtils.readUShortLE(stream);

       byte[] stringData = new byte[stringLen];
       IOUtils.readFully(stream, stringData);

       // Often but not always null terminated
       if (stringData[stringLen-1] == 0) {
           stringLen--;
       }
       String value = StringUtil.getFromCompressedUnicode(stringData, 0, stringLen);
       return value;
    }

    /**
     * Stored as UCS2, so 16 bit "unicode"
     */
    private void get2007and2010Props(
            InputStream stream, Metadata metadata, XHTMLContentHandler xhtml)
            throws IOException, TikaException, SAXException {
        // Standard properties
        for (int i = 0; i < HEADER_PROPERTIES_ENTRIES.length; i++) {
            String headerValue = read2007and2010String(stream);
            handleHeader(i, headerValue, metadata, xhtml);
        }

        // Custom properties
        int customCount = skipToCustomProperties(stream);
        for (int i = 0; i < customCount; i++) {
           String propName = read2007and2010String(stream);
           String propValue = read2007and2010String(stream);
           if(propName.length() > 0 && propValue.length() > 0) {
              metadata.add(propName, propValue);
           }
        }
    }

    private String read2007and2010String(InputStream stream) throws IOException, TikaException {
       int stringLen = EndianUtils.readUShortLE(stream);

       byte[] stringData = new byte[stringLen * 2];
       IOUtils.readFully(stream, stringData);
       String value = StringUtil.getFromUnicodeLE(stringData);

       // Some strings are null terminated
       if(value.charAt(value.length()-1) == 0) {
           value = value.substring(0, value.length()-1);
       }

       return value;
    }

    private void get2000Props(
            InputStream stream, Metadata metadata, XHTMLContentHandler xhtml)
            throws IOException, TikaException, SAXException {
        int propCount = 0;
        while(propCount < 30) {
            int propIdx = EndianUtils.readUShortLE(stream);
            int length = EndianUtils.readUShortLE(stream);
            int valueType = stream.read();
            
            if(propIdx == 0x28) {
               // This one seems not to follow the pattern
               length = 0x19;
            } else if(propIdx == 90) {
               // We think this means the end of properties
               break;
            }

            byte[] value = new byte[length];
            IOUtils.readFully(stream, value);
            if(valueType == 0x1e) {
                // Normal string, good
                String val = StringUtil.getFromCompressedUnicode(value, 0, length);
                
                // Is it one we can look up by index?
                if(propIdx < HEADER_2000_PROPERTIES_ENTRIES.length) {
                   metadata.add(HEADER_2000_PROPERTIES_ENTRIES[propIdx], val);
                   xhtml.element("p", val);
                } else if(propIdx == 0x012c) {
                   int splitAt = val.indexOf('='); 
                   if(splitAt > -1) {
                      String propName = val.substring(0, splitAt);
                      String propVal = val.substring(splitAt+1);
                      metadata.add(propName, propVal);
                   }
                }
            } else {
                // No idea...
            }
            
            propCount++;
        }
    }

    private void handleHeader(
            int headerNumber, String value, Metadata metadata,
            XHTMLContentHandler xhtml) throws SAXException {
        if(value == null || value.length() == 0) {
            return;
        }

        Property headerProp = HEADER_PROPERTIES_ENTRIES[headerNumber];
        if(headerProp != null) {
            metadata.set(headerProp, value);
        }

        xhtml.element("p", value);
    }

    /**
     * Grab the offset, then skip there
     */
    private boolean skipToPropertyInfoSection(InputStream stream, byte[] header)
            throws IOException, TikaException {
        // The offset is stored in the header from 0x20 onwards
        long offsetToSection = EndianUtils.getLongLE(header, 0x20);
        
        // Sanity check the offset. Some files seem to use a different format,
        //  and the offset isn't available at 0x20. Until we can work out how
        //  to find the offset in those files, skip them if detected
        if (offsetToSection > 0xa00000l) {
           // Header should never be more than 10mb into the file, something is wrong
           offsetToSection = 0;
        }
        
        // Work out how far to skip, and sanity check
        long toSkip = offsetToSection - header.length;
        if(offsetToSection == 0){
            return false;
        }        
        while (toSkip > 0) {
            byte[] skip = new byte[Math.min((int) toSkip, 0x4000)];
            IOUtils.readFully(stream, skip);
            toSkip -= skip.length;
        }
        return true;
    }

    /**
     * We think it can be anywhere...
     */
    private boolean skipTo2000PropertyInfoSection(InputStream stream, byte[] header)
            throws IOException {
       int val = 0;
       while(val != -1) {
          val = stream.read();
          if(val == HEADER_2000_PROPERTIES_MARKER[0]) {
             boolean going = true;
             for(int i=1; i<HEADER_2000_PROPERTIES_MARKER.length && going; i++) {
                val = stream.read();
                if(val != HEADER_2000_PROPERTIES_MARKER[i]) going = false;
             }
             if(going) {
                // Bingo, found it
                return true;
             }
          }
       }
       return false;
    }

    private int skipToCustomProperties(InputStream stream) 
            throws IOException, TikaException {
       // There should be 4 zero bytes or CUSTOM_PROPERTIES_ALT_PADDING_VALUES next
       byte[] padding = new byte[4];
       IOUtils.readFully(stream, padding);
       if((padding[0] == 0 && padding[1] == 0 &&
             padding[2] == 0 && padding[3] == 0) ||
             (padding[0] == CUSTOM_PROPERTIES_ALT_PADDING_VALUES[0] && 
               padding[1] == CUSTOM_PROPERTIES_ALT_PADDING_VALUES[1] &&
               padding[2] == CUSTOM_PROPERTIES_ALT_PADDING_VALUES[2] &&
               padding[3] == CUSTOM_PROPERTIES_ALT_PADDING_VALUES[3])) {
           
          // Looks hopeful, skip on
          padding = new byte[CUSTOM_PROPERTIES_SKIP];
          IOUtils.readFully(stream, padding);
          
          // We should now have the count
          int count = EndianUtils.readUShortLE(stream);
          
          // Sanity check it
          if(count > 0 && count < 0x7f) {
             // Looks plausible
             return count;
          } else {
             // No properties / count is too high to trust
             return 0;
          }
       } else {
          // No padding. That probably means no custom props
          return 0;
       }
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/envi/EnviHeaderParser.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * 
 */
package org.apache.tika.parser.envi;

import java.io.IOException;
import java.io.InputStream;
import java.util.Collections;
import java.util.Set;
import java.nio.charset.Charset;

import org.apache.tika.detect.AutoDetectReader;
import org.apache.tika.io.CloseShieldInputStream;
import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.sax.XHTMLContentHandler;

import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

public class EnviHeaderParser extends AbstractParser {

    private static final long serialVersionUID = -1479368523072408091L;

    public static final String ENVI_MIME_TYPE = "application/envi.hdr";

    private static final Set<MediaType> SUPPORTED_TYPES = Collections
            .singleton(MediaType.application("envi.hdr"));

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }

    public void parse(InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context) throws IOException,
            SAXException, TikaException {

        // Only outputting the MIME type as metadata
        metadata.set(Metadata.CONTENT_TYPE, ENVI_MIME_TYPE);

        // The following code was taken from the TXTParser
        // Automatically detect the character encoding
        AutoDetectReader reader = new AutoDetectReader(
                new CloseShieldInputStream(stream), metadata);

        try {
            Charset charset = reader.getCharset();
            MediaType type = new MediaType(MediaType.TEXT_PLAIN, charset);
            // deprecated, see TIKA-431
            metadata.set(Metadata.CONTENT_ENCODING, charset.name());

            XHTMLContentHandler xhtml = new XHTMLContentHandler(handler,
                    metadata);

            xhtml.startDocument();

            // text contents of the xhtml
            String line;
            while ((line = reader.readLine()) != null) {
                xhtml.startElement("p");
                xhtml.characters(line);
                xhtml.endElement("p");
            }
            
            xhtml.endDocument();
        } finally {
            reader.close();
        }
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/epub/EpubContentParser.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.epub;

import java.io.IOException;
import java.io.InputStream;
import java.util.Collections;
import java.util.Set;

import javax.xml.XMLConstants;
import javax.xml.parsers.ParserConfigurationException;
import javax.xml.parsers.SAXParser;
import javax.xml.parsers.SAXParserFactory;

import org.apache.tika.exception.TikaException;
import org.apache.tika.io.CloseShieldInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.OfflineContentHandler;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;
import org.xml.sax.SAXNotRecognizedException;

/**
 * Parser for EPUB OPS <code>*.html</code> files.
 *
 * For the time being, assume XHTML (TODO: DTBook)
 */
public class EpubContentParser extends AbstractParser {

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return Collections.emptySet(); // not a top-level parser
    }

    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        final XHTMLContentHandler xhtml =
            new XHTMLContentHandler(handler,metadata);

        try {
            SAXParserFactory factory = SAXParserFactory.newInstance();
            factory.setValidating(false);
            factory.setNamespaceAware(true);
            try {
                factory.setFeature(XMLConstants.FEATURE_SECURE_PROCESSING, true);
            } catch (SAXNotRecognizedException e) {
                // TIKA-329: Some XML parsers do not support the secure-processing
                // feature, even though it's required by JAXP in Java 5. Ignoring
                // the exception is fine here, deployments without this feature
                // are inherently vulnerable to XML denial-of-service attacks.
            }
            SAXParser parser = factory.newSAXParser();
            parser.parse(
                    new CloseShieldInputStream(stream),
                    new OfflineContentHandler(xhtml));
        } catch (ParserConfigurationException e) {
            throw new TikaException("XML parser configuration error", e);
        }
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/epub/EpubParser.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.epub;

import java.io.IOException;
import java.io.InputStream;
import java.util.Arrays;
import java.util.Collections;
import java.util.HashSet;
import java.util.Set;
import java.util.zip.ZipEntry;
import java.util.zip.ZipInputStream;

import org.apache.tika.exception.TikaException;
import org.apache.tika.io.IOUtils;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.Parser;
import org.apache.tika.parser.xml.DcXMLParser;
import org.apache.tika.sax.BodyContentHandler;
import org.apache.tika.sax.EmbeddedContentHandler;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.DefaultHandler;

/**
 * Epub parser
 */
public class EpubParser extends AbstractParser {

    /** Serial version UID */
    private static final long serialVersionUID = 215176772484050550L;

    private static final Set<MediaType> SUPPORTED_TYPES =
            Collections.unmodifiableSet(new HashSet<MediaType>(Arrays.asList(
            		MediaType.application("epub+zip"),
                  MediaType.application("x-ibooks+zip")
            )));

    private Parser meta = new DcXMLParser();

    private Parser content = new EpubContentParser();

    public Parser getMetaParser() {
        return meta;
    }

    public void setMetaParser(Parser meta) {
        this.meta = meta;
    }

    public Parser getContentParser() {
        return content;
    }

    public void setContentParser(Parser content) {
        this.content = content;
    }

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }

    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        // Because an EPub file is often made up of multiple XHTML files,
        //  we need explicit control over the start and end of the document
        XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);
        xhtml.startDocument();
        ContentHandler childHandler = new EmbeddedContentHandler(
              new BodyContentHandler(xhtml));
       
        ZipInputStream zip = new ZipInputStream(stream);
        ZipEntry entry = zip.getNextEntry();
        while (entry != null) {
            if (entry.getName().equals("mimetype")) {
                String type = IOUtils.toString(zip, "UTF-8");
                metadata.set(Metadata.CONTENT_TYPE, type);
            } else if (entry.getName().equals("metadata.xml")) {
                meta.parse(zip, new DefaultHandler(), metadata, context);
            } else if (entry.getName().endsWith(".opf")) {
                meta.parse(zip, new DefaultHandler(), metadata, context);
            } else if (entry.getName().endsWith(".html") || 
            		   entry.getName().endsWith(".xhtml")) {
                content.parse(zip, childHandler, metadata, context);
            }
            entry = zip.getNextEntry();
        }
        
        // Finish everything
        xhtml.endDocument();
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/executable/ExecutableParser.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.executable;

import java.io.IOException;
import java.io.InputStream;
import java.sql.Date;
import java.util.Arrays;
import java.util.Collections;
import java.util.HashSet;
import java.util.Set;

import org.apache.poi.util.IOUtils;
import org.apache.poi.util.LittleEndian;
import org.apache.tika.exception.TikaException;
import org.apache.tika.io.EndianUtils;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * Parser for executable files. Currently supports ELF and PE
 */
public class ExecutableParser extends AbstractParser implements MachineMetadata {
    /** Serial version UID */
    private static final long serialVersionUID = 32128791892482l;

    private static final MediaType PE_EXE = MediaType.application("x-msdownload");
    private static final MediaType ELF_GENERAL = MediaType.application("x-elf");
    private static final MediaType ELF_OBJECT = MediaType.application("x-object");
    private static final MediaType ELF_EXECUTABLE = MediaType.application("x-executable");
    private static final MediaType ELF_SHAREDLIB = MediaType.application("x-sharedlib");
    private static final MediaType ELF_COREDUMP = MediaType.application("x-coredump");
    private static final Set<MediaType> SUPPORTED_TYPES =
            Collections.unmodifiableSet(new HashSet<MediaType>(Arrays.asList(
            		PE_EXE,
                  ELF_GENERAL,
                  ELF_OBJECT, ELF_EXECUTABLE, ELF_SHAREDLIB, ELF_COREDUMP
            )));
    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }

    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        // We only do metadata, for now
        XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);

        // What kind is it?
        byte[] first4 = new byte[4];
        IOUtils.readFully(stream, first4);
        
        if (first4[0] == (byte)'M' && first4[1] == (byte)'Z') {
           parsePE(xhtml, metadata, stream, first4);
        } else if (first4[0] == (byte)0x7f && first4[1] == (byte)'E' &&
                   first4[2] == (byte)'L' && first4[3] == (byte)'F') {
           parseELF(xhtml, metadata, stream, first4);
        }
        
        
        // Finish everything
        xhtml.endDocument();
    }

    /**
     * Parses a DOS or Windows PE file
     */
    public void parsePE(XHTMLContentHandler xhtml, Metadata metadata,
          InputStream stream, byte[] first4) throws TikaException, IOException {
       metadata.add(Metadata.CONTENT_TYPE, PE_EXE.toString());
       metadata.set(PLATFORM, PLATFORM_WINDOWS);
       
       // Skip over the MS-DOS bit
       byte[] msdosSection = new byte[0x3c-4];
       IOUtils.readFully(stream, msdosSection);
       
       // Grab the PE header offset
       int peOffset = LittleEndian.readInt(stream);
       
       // Sanity check - while it may go anywhere, it's normally in the first few kb
       if (peOffset > 4096 || peOffset < 0x3f) return;
       
       // Skip the rest of the MS-DOS stub (if PE), until we reach what should
       //  be the PE header (if this is a PE executable)
       stream.skip(peOffset - 0x40);
       
       // Read the PE header
       byte[] pe = new byte[24];
       IOUtils.readFully(stream, pe);
       
       // Check it really is a PE header
       if (pe[0] == (byte)'P' && pe[1] == (byte)'E' && pe[2]==0 && pe[3]==0) {
          // Good, has a valid PE signature
       } else {
          // Old style MS-DOS
          return;
       }
       
       // Read the header values
       int machine    = LittleEndian.getUShort(pe, 4);
       int numSectors = LittleEndian.getUShort(pe, 6);
       long createdAt = LittleEndian.getInt(pe, 8);
       long symbolTableOffset = LittleEndian.getInt(pe, 12);
       long numSymbols = LittleEndian.getInt(pe, 16);
       int sizeOptHdrs = LittleEndian.getUShort(pe, 20);
       int characteristcs = LittleEndian.getUShort(pe, 22);
       
       // Turn this into helpful metadata
       Date createdAtD = new Date(createdAt*1000l);
       metadata.set(Metadata.CREATION_DATE, createdAtD);
       
       switch(machine) {
         case 0x14c:
            metadata.set(MACHINE_TYPE, MACHINE_x86_32);
            metadata.set(ENDIAN, Endian.LITTLE.getName());
            metadata.set(ARCHITECTURE_BITS, "32");
            break;
         case 0x8664:
            metadata.set(MACHINE_TYPE, MACHINE_x86_32);
            metadata.set(ENDIAN, Endian.LITTLE.getName());
            metadata.set(ARCHITECTURE_BITS, "64");
            break;
         case 0x200:
            metadata.set(MACHINE_TYPE, MACHINE_IA_64);
            metadata.set(ENDIAN, Endian.LITTLE.getName());
            metadata.set(ARCHITECTURE_BITS, "64");
            break;
            
         case 0x184:
            metadata.set(MACHINE_TYPE, MACHINE_ALPHA);
            metadata.set(ENDIAN, Endian.LITTLE.getName());
            metadata.set(ARCHITECTURE_BITS, "32");
            break;
         case 0x284:
            metadata.set(MACHINE_TYPE, MACHINE_ALPHA);
            metadata.set(ENDIAN, Endian.LITTLE.getName());
            metadata.set(ARCHITECTURE_BITS, "64");
            break;
            
         case 0x1c0:
         case 0x1c4:
            metadata.set(MACHINE_TYPE, MACHINE_ARM);
            metadata.set(ENDIAN, Endian.LITTLE.getName());
            metadata.set(ARCHITECTURE_BITS, "32");
            break;

         case 0x268:
            metadata.set(MACHINE_TYPE, MACHINE_M68K);
            metadata.set(ENDIAN, Endian.BIG.getName());
            metadata.set(ARCHITECTURE_BITS, "32");
            break;

         case 0x266:
         case 0x366:
         case 0x466:
            metadata.set(MACHINE_TYPE, MACHINE_MIPS);
            metadata.set(ENDIAN, Endian.BIG.getName());
            metadata.set(ARCHITECTURE_BITS, "16");
            break;
         case 0x162:
         case 0x166:
         case 0x168:
         case 0x169:
            metadata.set(MACHINE_TYPE, MACHINE_MIPS);
            metadata.set(ENDIAN, Endian.LITTLE.getName());
            metadata.set(ARCHITECTURE_BITS, "16");
            break;
            
         case 0x1f0:
         case 0x1f1:
            metadata.set(MACHINE_TYPE, MACHINE_PPC);
            metadata.set(ENDIAN, Endian.LITTLE.getName());
            metadata.set(ARCHITECTURE_BITS, "32");
            break;
            
         case 0x1a2:
         case 0x1a3:
            metadata.set(MACHINE_TYPE, MACHINE_SH3);
            metadata.set(ENDIAN, Endian.BIG.getName());
            metadata.set(ARCHITECTURE_BITS, "32");
            break;
         case 0x1a6:
            metadata.set(MACHINE_TYPE, MACHINE_SH4);
            metadata.set(ENDIAN, Endian.BIG.getName());
            metadata.set(ARCHITECTURE_BITS, "32");
            break;
         case 0x1a8:
            metadata.set(MACHINE_TYPE, MACHINE_SH3);
            metadata.set(ENDIAN, Endian.BIG.getName());
            metadata.set(ARCHITECTURE_BITS, "32");
            break;

         case 0x9041:
            metadata.set(MACHINE_TYPE, MACHINE_M32R);
            metadata.set(ENDIAN, Endian.BIG.getName());
            metadata.set(ARCHITECTURE_BITS, "32");
            break;

         case 0xebc:
            metadata.set(MACHINE_TYPE, MACHINE_EFI);
            break;

         default:
            metadata.set(MACHINE_TYPE, MACHINE_UNKNOWN);
            break;
       }
    }

    /**
     * Parses a Unix ELF file
     */
    public void parseELF(XHTMLContentHandler xhtml, Metadata metadata,
          InputStream stream, byte[] first4) throws TikaException, IOException {
       // Byte 5 is the architecture
       int architecture = stream.read();
       if (architecture == 1) {
          metadata.set(ARCHITECTURE_BITS, "32");
       } else if (architecture == 2) {
          metadata.set(ARCHITECTURE_BITS, "64");          
       }
       
       // Byte 6 is the endian-ness
       int endian = stream.read();
       if (endian == 1) {
          metadata.set(ENDIAN, Endian.LITTLE.getName());
       } else if (endian == 2) {
          metadata.set(ENDIAN, Endian.BIG.getName());
       }
       
       // Byte 7 is the elf version
       int elfVer = stream.read();
       
       // Byte 8 is the OS, if set (lots of compilers don't)
       // Byte 9 is the OS (specific) ABI version
       int os = stream.read();
       int osVer = stream.read();
       if (os > 0 || osVer > 0)
       {
          switch (os) {
          case 0:
             metadata.set(PLATFORM, PLATFORM_SYSV);
             break;

          case 1:
             metadata.set(PLATFORM, PLATFORM_HPUX);
             break;

          case 2:
             metadata.set(PLATFORM, PLATFORM_NETBSD);
             break;

          case 3:
             metadata.set(PLATFORM, PLATFORM_LINUX);
             break;

          case 6:
             metadata.set(PLATFORM, PLATFORM_SOLARIS);
             break;

          case 7:
             metadata.set(PLATFORM, PLATFORM_AIX);
             break;

          case 8:
             metadata.set(PLATFORM, PLATFORM_IRIX);
             break;

          case 9:
             metadata.set(PLATFORM, PLATFORM_FREEBSD);
             break;

          case 10:
             metadata.set(PLATFORM, PLATFORM_TRU64);
             break;

          case 12:
             metadata.set(PLATFORM, PLATFORM_FREEBSD);
             break;

          case 64:
          case 97:
             metadata.set(PLATFORM, PLATFORM_ARM);
             break;

          case 255:
             metadata.set(PLATFORM, PLATFORM_EMBEDDED);
             break;
          }
       }
       
       // Bytes 10-16 are padding and lengths
       byte[] padLength = new byte[7];
       IOUtils.readFully(stream, padLength);
       
       // Bytes 16-17 are the object type (LE/BE)
       int type;
       if (endian == 1) {
          type = EndianUtils.readUShortLE(stream);
       } else {
          type = EndianUtils.readUShortBE(stream);
       }
       switch(type) {
         case 1:
            metadata.add(Metadata.CONTENT_TYPE, ELF_OBJECT.toString());
            break;
            
         case 2:
            metadata.add(Metadata.CONTENT_TYPE, ELF_EXECUTABLE.toString());
            break;
            
         case 3:
            metadata.add(Metadata.CONTENT_TYPE, ELF_SHAREDLIB.toString());
            break;
            
         case 4:
            metadata.add(Metadata.CONTENT_TYPE, ELF_COREDUMP.toString());
            break;
            
         default:
            metadata.add(Metadata.CONTENT_TYPE, ELF_GENERAL.toString());
            break;
       }
                 
       // Bytes 18-19 are the machine (EM_*)
       int machine;
       if (endian == 1) {
          machine = EndianUtils.readUShortLE(stream);
       } else {
          machine = EndianUtils.readUShortBE(stream);
       }
       switch(machine) {
         case 2:
         case 18:
         case 43:
            metadata.set(MACHINE_TYPE, MACHINE_SPARC);
            break;
         case 3:
            metadata.set(MACHINE_TYPE, MACHINE_x86_32);
            break;
         case 4:
            metadata.set(MACHINE_TYPE, MACHINE_M68K);
            break;
         case 5:
            metadata.set(MACHINE_TYPE, MACHINE_M88K);
            break;
         case 8:
         case 10:
            metadata.set(MACHINE_TYPE, MACHINE_MIPS);
            break;
         case 7:
            metadata.set(MACHINE_TYPE, MACHINE_S370);
            break;
         case 20:
         case 21:
            metadata.set(MACHINE_TYPE, MACHINE_PPC);
            break;
         case 22:
            metadata.set(MACHINE_TYPE, MACHINE_S390);
            break;
         case 40:
            metadata.set(MACHINE_TYPE, MACHINE_ARM);
            break;
         case 41:
         case 0x9026:
            metadata.set(MACHINE_TYPE, MACHINE_ALPHA);
            break;
         case 50:
            metadata.set(MACHINE_TYPE, MACHINE_IA_64);
            break;
         case 62:
            metadata.set(MACHINE_TYPE, MACHINE_x86_64);
            break;
         case 75:
            metadata.set(MACHINE_TYPE, MACHINE_VAX);
            break;
         case 88:
            metadata.set(MACHINE_TYPE, MACHINE_M32R);
            break;
       }
       
       
       
       // Bytes 20-23 are the version
       // TODO
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/executable/MachineMetadata.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.executable;

import org.apache.tika.metadata.Property;

/**
 * Metadata for describing machines, such as their
 *  architecture, type and endian-ness
 */
public interface MachineMetadata {
    public static final String PREFIX = "machine:";
   
    public static Property ARCHITECTURE_BITS = Property.internalClosedChoise(PREFIX+"architectureBits", 
         "8", "16", "32", "64");

    public static final String PLATFORM_SYSV    = "System V";
    public static final String PLATFORM_HPUX    = "HP-UX";
    public static final String PLATFORM_NETBSD  = "NetBSD";
    public static final String PLATFORM_LINUX   = "Linux";
    public static final String PLATFORM_SOLARIS = "Solaris";
    public static final String PLATFORM_AIX     = "AIX";
    public static final String PLATFORM_IRIX    = "IRIX";
    public static final String PLATFORM_FREEBSD = "FreeBSD";
    public static final String PLATFORM_TRU64   = "Tru64";
    public static final String PLATFORM_ARM     = "ARM"; // ARM architecture ABI
    public static final String PLATFORM_EMBEDDED = "Embedded"; // Stand-alone (embedded) ABI
    public static final String PLATFORM_WINDOWS = "Windows";
    
    public static Property PLATFORM = Property.internalClosedChoise(PREFIX+"platform", 
          PLATFORM_SYSV, PLATFORM_HPUX, PLATFORM_NETBSD, PLATFORM_LINUX,
                         PLATFORM_SOLARIS, PLATFORM_AIX, PLATFORM_IRIX, PLATFORM_FREEBSD, PLATFORM_TRU64,
                         PLATFORM_ARM, PLATFORM_EMBEDDED, PLATFORM_WINDOWS);
    
    public static final String MACHINE_x86_32 = "x86-32";
    public static final String MACHINE_x86_64 = "x86-64";
    public static final String MACHINE_IA_64  = "IA-64";
    public static final String MACHINE_SPARC  = "SPARC";
    public static final String MACHINE_M68K   = "Motorola-68000";
    public static final String MACHINE_M88K   = "Motorola-88000";
    public static final String MACHINE_MIPS   = "MIPS";
    public static final String MACHINE_PPC    = "PPC";
    public static final String MACHINE_S370   = "S370";
    public static final String MACHINE_S390   = "S390";
    public static final String MACHINE_ARM    = "ARM";
    public static final String MACHINE_VAX    = "Vax";
    public static final String MACHINE_ALPHA  = "Alpha";
    public static final String MACHINE_EFI    = "EFI"; // EFI ByteCode
    public static final String MACHINE_M32R   = "M32R";
    public static final String MACHINE_SH3    = "SH3";
    public static final String MACHINE_SH4    = "SH4";
    public static final String MACHINE_SH5    = "SH5";
    public static final String MACHINE_UNKNOWN = "Unknown";
    
    public static Property MACHINE_TYPE = Property.internalClosedChoise(PREFIX+"machineType", 
           MACHINE_x86_32, MACHINE_x86_64, MACHINE_IA_64, MACHINE_SPARC,
           MACHINE_M68K, MACHINE_M88K, MACHINE_MIPS, MACHINE_PPC,
           MACHINE_S370, MACHINE_S390,
           MACHINE_ARM, MACHINE_VAX, MACHINE_ALPHA, MACHINE_EFI, MACHINE_M32R,
           MACHINE_SH3, MACHINE_SH4, MACHINE_SH5, MACHINE_UNKNOWN);
    
    public static final class Endian {
        private String name;
        private boolean msb;
        public String getName() { return name; }
        @SuppressWarnings("unused")
        public boolean isMSB() { return msb; }
        @SuppressWarnings("unused")
        public String getMSB() { if(msb) { return "MSB"; } else { return "LSB"; } }
        private Endian(String name, boolean msb) { this.name = name; this.msb = msb; }
       
        public static final Endian LITTLE = new Endian("Little", false);
        public static final Endian BIG = new Endian("Big", true);
    }
    public static Property ENDIAN = Property.internalClosedChoise(PREFIX+"endian", 
          Endian.LITTLE.name, Endian.BIG.name);
}
"
tika-parsers/src/main/java/org/apache/tika/parser/feed/FeedParser.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.feed;

import java.io.IOException;
import java.io.InputStream;
import java.util.Arrays;
import java.util.Collections;
import java.util.HashSet;
import java.util.Set;

import org.apache.tika.exception.TikaException;
import org.apache.tika.io.CloseShieldInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.InputSource;
import org.xml.sax.SAXException;

import com.sun.syndication.feed.synd.SyndContent;
import com.sun.syndication.feed.synd.SyndEntry;
import com.sun.syndication.feed.synd.SyndFeed;
import com.sun.syndication.io.FeedException;
import com.sun.syndication.io.SyndFeedInput;

/**
 * Feed parser.
 * <p>
 * Uses Rome for parsing the feeds. A feed description is put in a paragraph
 * with its link and title in an anchor.
 */
public class FeedParser extends AbstractParser {

    /** Serial version UID */
    private static final long serialVersionUID = -3785361933034525186L;

    private static final Set<MediaType> SUPPORTED_TYPES =
            Collections.unmodifiableSet(new HashSet<MediaType>(Arrays.asList(
                    MediaType.application("rss+xml"),
                    MediaType.application("atom+xml"))));

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }

    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        // set the encoding?
        try {
            SyndFeed feed = new SyndFeedInput().build(
                    new InputSource(new CloseShieldInputStream(stream)));

            String title = stripTags(feed.getTitleEx());
            String description = stripTags(feed.getDescriptionEx());

            metadata.set(TikaCoreProperties.TITLE, title);
            metadata.set(TikaCoreProperties.DESCRIPTION, description);
            // store the other fields in the metadata

            XHTMLContentHandler xhtml =
                new XHTMLContentHandler(handler, metadata);
            xhtml.startDocument();

            xhtml.element("h1", title);
            xhtml.element("p", description);

            xhtml.startElement("ul");
            for (Object e : feed.getEntries()) {
                SyndEntry entry = (SyndEntry) e;
                String link = entry.getLink();
                if (link != null) {
                    xhtml.startElement("li");
                    xhtml.startElement("a", "href", link);
                    xhtml.characters(stripTags(entry.getTitleEx()));
                    xhtml.endElement("a");
                    SyndContent content = entry.getDescription();
                    if (content != null) {
                        xhtml.newline();
                        xhtml.characters(stripTags(content));
                    }
                    xhtml.endElement("li");
                }
            }
            xhtml.endElement("ul");

            xhtml.endDocument();
        } catch (FeedException e) {
            throw new TikaException("RSS parse error", e);
        }

    }

    private static String stripTags(SyndContent c) {
        if (c == null)
            return "";

        String value = c.getValue();

        String[] parts = value.split("<[^>]*>");
        StringBuffer buf = new StringBuffer();

        for (String part : parts)
            buf.append(part);

        return buf.toString().trim();
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/font/AdobeFontMetricParser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.font;

import java.io.IOException;
import java.io.InputStream;
import java.util.Collections;
import java.util.List;
import java.util.Set;

import org.apache.fontbox.afm.AFMParser;
import org.apache.fontbox.afm.FontMetric;
import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.Property;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * Parser for AFM Font Files
 */
public class AdobeFontMetricParser extends AbstractParser { 
    /** Serial version UID */
    private static final long serialVersionUID = -4820306522217196835L;

    private static final MediaType AFM_TYPE =
         MediaType.application( "x-font-adobe-metric" );

    private static final Set<MediaType> SUPPORTED_TYPES = Collections.singleton(AFM_TYPE);

    // TIKA-1325 Replace these with properties, from a well known standard
    static final String MET_AVG_CHAR_WIDTH = "AvgCharacterWidth";
    static final String MET_DOC_VERSION = "DocVersion";
    static final String MET_PS_NAME = "PSName";
    static final String MET_FONT_NAME = "FontName";
    static final String MET_FONT_FULL_NAME = "FontFullName";
    static final String MET_FONT_FAMILY_NAME = "FontFamilyName";
    static final String MET_FONT_SUB_FAMILY_NAME = "FontSubFamilyName";
    static final String MET_FONT_VERSION = "FontVersion";
    static final String MET_FONT_WEIGHT = "FontWeight";
    static final String MET_FONT_NOTICE = "FontNotice";
    static final String MET_FONT_UNDERLINE_THICKNESS = "FontUnderlineThickness";
    
    public Set<MediaType> getSupportedTypes( ParseContext context ) { 
       return SUPPORTED_TYPES;
    }

    public void parse(InputStream stream, ContentHandler handler,
                      Metadata metadata, ParseContext context)
                      throws IOException, SAXException, TikaException { 
       FontMetric fontMetrics;
       AFMParser  parser      = new AFMParser( stream );

       // Have FontBox process the file
       parser.parse();
       fontMetrics = parser.getResult();

       // Get the comments in the file to display in xhtml
       List<String> comments = fontMetrics.getComments();

       // Get the creation date
       extractCreationDate( metadata, comments );

       metadata.set( Metadata.CONTENT_TYPE, AFM_TYPE.toString() );
       metadata.set( TikaCoreProperties.TITLE, fontMetrics.getFullName() );

       // Add metadata associated with the font type
       addMetadataByString( metadata, MET_AVG_CHAR_WIDTH, Float.toString( fontMetrics.getAverageCharacterWidth() ) );
       addMetadataByString( metadata, MET_DOC_VERSION, Float.toString( fontMetrics.getAFMVersion() ) );
       addMetadataByString( metadata, MET_FONT_NAME, fontMetrics.getFontName() );
       addMetadataByString( metadata, MET_FONT_FULL_NAME, fontMetrics.getFullName() );
       addMetadataByString( metadata, MET_FONT_FAMILY_NAME, fontMetrics.getFamilyName() );
       addMetadataByString( metadata, MET_FONT_VERSION, fontMetrics.getFontVersion() );
       addMetadataByString( metadata, MET_FONT_WEIGHT, fontMetrics.getWeight() );
       addMetadataByString( metadata, MET_FONT_NOTICE, fontMetrics.getNotice() );
       addMetadataByString( metadata, MET_FONT_UNDERLINE_THICKNESS, Float.toString( fontMetrics.getUnderlineThickness() ) );

       // Output the remaining comments as text
       XHTMLContentHandler xhtml = new XHTMLContentHandler( handler, metadata );
       xhtml.startDocument();

       // Display the comments
       if (comments.size() > 0) {
          xhtml.element( "h1", "Comments" );
          xhtml.startElement("div", "class", "comments");
          for (String comment : comments) {
              xhtml.element( "p", comment );
          }
          xhtml.endElement("div");
       }

       xhtml.endDocument();
    }

    private void addMetadataByString( Metadata metadata, String name, String value ) { 
       // Add metadata if an appropriate value is passed 
       if (value != null) { 
          metadata.add( name, value );
       }
    }

    private void addMetadataByProperty( Metadata metadata, Property property, String value ) { 
       // Add metadata if an appropriate value is passed 
       if (value != null) 
       {
          metadata.set( property, value );
       }
    }


    private void extractCreationDate( Metadata metadata, List<String> comments ) {
       String   date = null;

       for (String value : comments) {
          // Look for the creation date
          if( value.matches( ".*Creation\\sDate.*" ) ) {
             date = value.substring( value.indexOf( ":" ) + 2 );
             comments.remove( value );

             break;
          }
       }

       // If appropriate date then store as metadata
       if( date != null ) {
          addMetadataByProperty( metadata, Metadata.CREATION_DATE, date );
       }
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/font/TrueTypeParser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.font;

import java.io.IOException;
import java.io.InputStream;
import java.util.Collections;
import java.util.Set;

import org.apache.fontbox.ttf.NameRecord;
import org.apache.fontbox.ttf.NamingTable;
import org.apache.fontbox.ttf.TTFParser;
import org.apache.fontbox.ttf.TrueTypeFont;
import org.apache.tika.exception.TikaException;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * Parser for TrueType font files (TTF).
 */
public class TrueTypeParser extends AbstractParser {

    /** Serial version UID */
    private static final long serialVersionUID = 44788554612243032L;

    private static final MediaType TYPE =
        MediaType.application("x-font-ttf");

    private static final Set<MediaType> SUPPORTED_TYPES =
        Collections.singleton(TYPE);

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }

    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        TikaInputStream tis = TikaInputStream.cast(stream);
        
        // Ask FontBox to parse the file for us
        TrueTypeFont font;
        TTFParser parser = new TTFParser();
        if (tis != null && tis.hasFile()) {
            font = parser.parseTTF(tis.getFile());
        } else {
            font = parser.parseTTF(stream);
        }

        // Report the details of the font
        metadata.set(Metadata.CONTENT_TYPE, TYPE.toString());
        metadata.set(TikaCoreProperties.CREATED, 
                font.getHeader().getCreated());
        metadata.set(TikaCoreProperties.MODIFIED,
                font.getHeader().getModified());
        metadata.set(AdobeFontMetricParser.MET_DOC_VERSION,
                Float.toString(font.getHeader().getVersion()));
        
        // Pull out the naming info
        NamingTable fontNaming = font.getNaming();
        for (NameRecord nr : fontNaming.getNameRecords()) {
            if (nr.getNameId() == NameRecord.NAME_FONT_FAMILY_NAME) {
                metadata.set(AdobeFontMetricParser.MET_FONT_FAMILY_NAME, nr.getString());
            }
            if (nr.getNameId() == NameRecord.NAME_FONT_SUB_FAMILY_NAME) {
                metadata.set(AdobeFontMetricParser.MET_FONT_SUB_FAMILY_NAME, nr.getString());
            }
            if (nr.getNameId() == NameRecord.NAME_FULL_FONT_NAME) {
                metadata.set(AdobeFontMetricParser.MET_FONT_NAME, nr.getString());
                metadata.set(TikaCoreProperties.TITLE, nr.getString());
            }
            if (nr.getNameId() == NameRecord.NAME_POSTSCRIPT_NAME) {
                metadata.set(AdobeFontMetricParser.MET_PS_NAME, nr.getString());
            }
            if (nr.getNameId() == NameRecord.NAME_COPYRIGHT) {
                metadata.set("Copyright", nr.getString());
            }
            if (nr.getNameId() == NameRecord.NAME_TRADEMARK) {
                metadata.set("Trademark", nr.getString());
            }
        }
        
        // For now, we only output metadata, no textual contents
        XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);
        xhtml.startDocument();
        xhtml.endDocument();
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/gdal/GDALParser.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.parser.gdal;

//JDK imports
import java.io.ByteArrayInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.io.Reader;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Map;
import java.util.Scanner;
import java.util.Set;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

//Tika imports
import org.apache.tika.exception.TikaException;
import org.apache.tika.io.TemporaryResources;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.external.ExternalParser;
import org.apache.tika.sax.XHTMLContentHandler;

import static org.apache.tika.parser.external.ExternalParser.INPUT_FILE_TOKEN;

//SAX imports
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * Wraps execution of the <a href="http//gdal.org/">Geospatial Data Abstraction
 * Library (GDAL)</a> <code>gdalinfo</code> tool used to extract geospatial
 * information out of hundreds of geo file formats.
 * <p/>
 * The parser requires the installation of GDAL and for <code>gdalinfo</code> to
 * be located on the path.
 * <p/>
 * Basic information (Size, Coordinate System, Bounding Box, Driver, and
 * resource info) are extracted as metadata, and the remaining metadata patterns
 * are extracted and added.
 * <p/>
 * The output of the command is available from the provided
 * {@link ContentHandler} in the
 * {@link #parse(InputStream, ContentHandler, Metadata, ParseContext)} method.
 */
public class GDALParser extends AbstractParser {

    private static final long serialVersionUID = -3869130527323941401L;

    private String command;

    public GDALParser() {
        setCommand("gdalinfo ${INPUT}");
    }

    public void setCommand(String command) {
        this.command = command;
    }

    public String getCommand() {
        return this.command;
    }

    public String processCommand(InputStream stream) {
        TikaInputStream tis = (TikaInputStream) stream;
        String pCommand = this.command;
        try {
            if (this.command.indexOf(INPUT_FILE_TOKEN) != -1) {
                pCommand = this.command.replace(INPUT_FILE_TOKEN, tis.getFile()
                        .getPath());
            }
        } catch (Exception e) {
            e.printStackTrace();
        }

        return pCommand;
    }

    @Override
    public Set<MediaType> getSupportedTypes(ParseContext context) {
        Set<MediaType> types = new HashSet<MediaType>();
        types.add(MediaType.application("x-netcdf"));
        types.add(MediaType.application("vrt"));
        types.add(MediaType.image("geotiff"));
        types.add(MediaType.image("ntif"));
        types.add(MediaType.application("x-rpf-toc"));
        types.add(MediaType.application("x-ecrg-toc"));
        types.add(MediaType.image("hfa"));
        types.add(MediaType.image("sar-ceos"));
        types.add(MediaType.image("ceos"));
        types.add(MediaType.application("jaxa-pal-sar"));
        types.add(MediaType.application("gff"));
        types.add(MediaType.application("elas"));
        types.add(MediaType.application("aig"));
        types.add(MediaType.application("aaigrid"));
        types.add(MediaType.application("grass-ascii-grid"));
        types.add(MediaType.application("sdts-raster"));
        types.add(MediaType.application("dted"));
        types.add(MediaType.image("png"));
        types.add(MediaType.image("jpeg"));
        types.add(MediaType.image("raster"));
        types.add(MediaType.application("jdem"));
        types.add(MediaType.image("gif"));
        types.add(MediaType.image("big-gif"));
        types.add(MediaType.image("envisat"));
        types.add(MediaType.image("fits"));
        types.add(MediaType.application("fits"));
        types.add(MediaType.image("bsb"));
        types.add(MediaType.application("xpm"));
        types.add(MediaType.image("bmp"));
        types.add(MediaType.image("x-dimap"));
        types.add(MediaType.image("x-airsar"));
        types.add(MediaType.application("x-rs2"));
        types.add(MediaType.application("x-pcidsk"));
        types.add(MediaType.application("pcisdk"));
        types.add(MediaType.image("x-pcraster"));
        types.add(MediaType.image("ilwis"));
        types.add(MediaType.image("sgi"));
        types.add(MediaType.application("x-srtmhgt"));
        types.add(MediaType.application("leveller"));
        types.add(MediaType.application("terragen"));
        types.add(MediaType.application("x-gmt"));
        types.add(MediaType.application("x-isis3"));
        types.add(MediaType.application("x-isis2"));
        types.add(MediaType.application("x-pds"));
        types.add(MediaType.application("x-til"));
        types.add(MediaType.application("x-ers"));
        types.add(MediaType.application("x-l1b"));
        types.add(MediaType.image("fit"));
        types.add(MediaType.application("x-grib"));
        types.add(MediaType.image("jp2"));
        types.add(MediaType.application("x-rmf"));
        types.add(MediaType.application("x-wcs"));
        types.add(MediaType.application("x-wms"));
        types.add(MediaType.application("x-msgn"));
        types.add(MediaType.application("x-wms"));
        types.add(MediaType.application("x-wms"));
        types.add(MediaType.application("x-rst"));
        types.add(MediaType.application("x-ingr"));
        types.add(MediaType.application("x-gsag"));
        types.add(MediaType.application("x-gsbg"));
        types.add(MediaType.application("x-gs7bg"));
        types.add(MediaType.application("x-cosar"));
        types.add(MediaType.application("x-tsx"));
        types.add(MediaType.application("x-coasp"));
        types.add(MediaType.application("x-r"));
        types.add(MediaType.application("x-map"));
        types.add(MediaType.application("x-pnm"));
        types.add(MediaType.application("x-doq1"));
        types.add(MediaType.application("x-doq2"));
        types.add(MediaType.application("x-envi"));
        types.add(MediaType.application("x-envi-hdr"));
        types.add(MediaType.application("x-generic-bin"));
        types.add(MediaType.application("x-p-aux"));
        types.add(MediaType.image("x-mff"));
        types.add(MediaType.image("x-mff2"));
        types.add(MediaType.image("x-fujibas"));
        types.add(MediaType.application("x-gsc"));
        types.add(MediaType.application("x-fast"));
        types.add(MediaType.application("x-bt"));
        types.add(MediaType.application("x-lan"));
        types.add(MediaType.application("x-cpg"));
        types.add(MediaType.image("ida"));
        types.add(MediaType.application("x-ndf"));
        types.add(MediaType.image("eir"));
        types.add(MediaType.application("x-dipex"));
        types.add(MediaType.application("x-lcp"));
        types.add(MediaType.application("x-gtx"));
        types.add(MediaType.application("x-los-las"));
        types.add(MediaType.application("x-ntv2"));
        types.add(MediaType.application("x-ctable2"));
        types.add(MediaType.application("x-ace2"));
        types.add(MediaType.application("x-snodas"));
        types.add(MediaType.application("x-kro"));
        types.add(MediaType.image("arg"));
        types.add(MediaType.application("x-rik"));
        types.add(MediaType.application("x-usgs-dem"));
        types.add(MediaType.application("x-gxf"));
        types.add(MediaType.application("x-dods"));
        types.add(MediaType.application("x-http"));
        types.add(MediaType.application("x-bag"));
        types.add(MediaType.application("x-hdf"));
        types.add(MediaType.image("x-hdf5-image"));
        types.add(MediaType.application("x-nwt-grd"));
        types.add(MediaType.application("x-nwt-grc"));
        types.add(MediaType.image("adrg"));
        types.add(MediaType.image("x-srp"));
        types.add(MediaType.application("x-blx"));
        types.add(MediaType.application("x-rasterlite"));
        types.add(MediaType.application("x-epsilon"));
        types.add(MediaType.application("x-sdat"));
        types.add(MediaType.application("x-kml"));
        types.add(MediaType.application("x-xyz"));
        types.add(MediaType.application("x-geo-pdf"));
        types.add(MediaType.image("x-ozi"));
        types.add(MediaType.application("x-ctg"));
        types.add(MediaType.application("x-e00-grid"));
        types.add(MediaType.application("x-zmap"));
        types.add(MediaType.application("x-webp"));
        types.add(MediaType.application("x-ngs-geoid"));
        types.add(MediaType.application("x-mbtiles"));
        types.add(MediaType.application("x-ppi"));
        types.add(MediaType.application("x-cappi"));
        return types;
    }

    @Override
    public void parse(InputStream stream, ContentHandler handler,
                      Metadata metadata, ParseContext context) throws IOException,
            SAXException, TikaException {

        if (!ExternalParser.check("gdalinfo")) {
            return;
        }

        // first set up and run GDAL
        // process the command
        TemporaryResources tmp = new TemporaryResources();
        TikaInputStream tis = TikaInputStream.get(stream, tmp);

        String runCommand = processCommand(tis);
        String output = execCommand(new String[]{runCommand});

        // now extract the actual metadata params
        // from the GDAL output in the content stream
        // to do this, we need to literally process the output
        // from the invoked command b/c we can't read metadata and
        // output text from the handler in ExternalParser
        // at the same time, so for now, we can't use the
        // ExternalParser to do this and I've had to bring some of
        // that functionality directly into this class
        // TODO: investigate a way to do both using ExternalParser

        extractMetFromOutput(output, metadata);
        applyPatternsToOutput(output, metadata, getPatterns());

        // make the content handler and provide output there
        // now that we have metadata
        processOutput(handler, metadata, output);
    }

    private Map<Pattern, String> getPatterns() {
        Map<Pattern, String> patterns = new HashMap<Pattern, String>();
        this.addPatternWithColon("Driver", patterns);
        this.addPatternWithColon("Files", patterns);
        this.addPatternWithIs("Size", patterns);
        this.addPatternWithIs("Coordinate System", patterns);
        this.addBoundingBoxPattern("Upper Left", patterns);
        this.addBoundingBoxPattern("Lower Left", patterns);
        this.addBoundingBoxPattern("Upper Right", patterns);
        this.addBoundingBoxPattern("Lower Right", patterns);
        return patterns;
    }

    private void addPatternWithColon(String name, Map<Pattern, String> patterns) {
        patterns.put(
                Pattern.compile(name + "\\:\\s*([A-Za-z0-9/ _\\-\\.]+)\\s*"),
                name);
    }

    private void addPatternWithIs(String name, Map<Pattern, String> patterns) {
        patterns.put(Pattern.compile(name + " is ([A-Za-z0-9\\.,\\s`']+)"),
                name);
    }

    private void addBoundingBoxPattern(String name,
                                       Map<Pattern, String> patterns) {
        patterns.put(
                Pattern.compile(name
                        + "\\s*\\(\\s*([0-9]+\\.[0-9]+\\s*,\\s*[0-9]+\\.[0-9]+\\s*)\\)\\s*"),
                name);
    }

    private void extractMetFromOutput(String output, Metadata met) {
        Scanner scanner = new Scanner(output);
        String currentKey = null;
        String[] headings = {"Subdatasets", "Corner Coordinates"};
        StringBuilder metVal = new StringBuilder();
        while (scanner.hasNextLine()) {
            String line = scanner.nextLine();
            if (line.contains("=") || hasHeadings(line, headings)) {
                if (currentKey != null) {
                    // time to flush this key and met val
                    met.add(currentKey, metVal.toString());
                }
                metVal.setLength(0);

                String[] lineToks = line.split("=");
                currentKey = lineToks[0].trim();
                if (lineToks.length == 2) {
                    metVal.append(lineToks[1]);
                } else {
                    metVal.append("");
                }
            } else {
                metVal.append(line);
            }

        }
    }

    private boolean hasHeadings(String line, String[] headings) {
        if (headings != null && headings.length > 0) {
            for (String heading : headings) {
                if (line.contains(heading)) {
                    return true;
                }
            }
            return false;
        } else return false;
    }

    private void applyPatternsToOutput(String output, Metadata metadata,
                                       Map<Pattern, String> metadataPatterns) {
        Scanner scanner = new Scanner(output);
        while (scanner.hasNextLine()) {
            String line = scanner.nextLine();
            for (Pattern p : metadataPatterns.keySet()) {
                Matcher m = p.matcher(line);
                if (m.find()) {
                    if (metadataPatterns.get(p) != null
                            && !metadataPatterns.get(p).equals("")) {
                        metadata.add(metadataPatterns.get(p), m.group(1));
                    } else {
                        metadata.add(m.group(1), m.group(2));
                    }
                }
            }
        }

    }

    private String execCommand(String[] cmd) throws IOException {
        // Execute
        Process process;
        String output = null;
        if (cmd.length == 1) {
            process = Runtime.getRuntime().exec(cmd[0]);
        } else {
            process = Runtime.getRuntime().exec(cmd);
        }

        try {
            InputStream out = process.getInputStream();

            try {
                output = extractOutput(out);
            } catch (Exception e) {
                e.printStackTrace();
                output = "";
            }

        } finally {
            try {
                process.waitFor();
            } catch (InterruptedException ignore) {
            }

            return output;
        }

    }

    private String extractOutput(InputStream stream) throws SAXException,
            IOException {
        StringBuffer sb = new StringBuffer();
        Reader reader = new InputStreamReader(stream, "UTF-8");
        try {
            char[] buffer = new char[1024];
            for (int n = reader.read(buffer); n != -1; n = reader.read(buffer)) {
                sb.append(buffer, 0, n);
            }
        } finally {
            reader.close();
            return sb.toString();
        }
    }

    private void processOutput(ContentHandler handler, Metadata metadata,
                               String output) throws SAXException, IOException {
        XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);
        InputStream stream = new ByteArrayInputStream(output.getBytes("UTF-8"));
        Reader reader = new InputStreamReader(stream, "UTF-8");
        try {
            xhtml.startDocument();
            xhtml.startElement("p");
            char[] buffer = new char[1024];
            for (int n = reader.read(buffer); n != -1; n = reader.read(buffer)) {
                xhtml.characters(buffer, 0, n);
            }
            xhtml.endElement("p");

        } finally {
            reader.close();
            xhtml.endDocument();
        }

    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/hdf/HDFParser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.parser.hdf;

//JDK imports
import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.util.Collections;
import java.util.Set;

import org.apache.tika.exception.TikaException;
import org.apache.tika.io.IOUtils;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.netcdf.NetCDFParser;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

import ucar.nc2.Attribute;
import ucar.nc2.Group;
import ucar.nc2.NetcdfFile;

/**
 * 
 * Since the {@link NetCDFParser} depends on the <a
 * href="http://www.unidata.ucar.edu/software/netcdf-java" >NetCDF-Java</a> API,
 * we are able to use it to parse HDF files as well. See <a href=
 * "http://www.unidata.ucar.edu/software/netcdf-java/formats/FileTypes.html"
 * >this link</a> for more information.
 */
public class HDFParser extends AbstractParser {

    /** Serial version UID */
    private static final long serialVersionUID = 1091208208003437549L;

    private static final Set<MediaType> SUPPORTED_TYPES =
        Collections.singleton(MediaType.application("x-hdf"));

    /*
     * (non-Javadoc)
     * 
     * @see
     * org.apache.tika.parser.netcdf.NetCDFParser#getSupportedTypes(org.apache
     * .tika.parser.ParseContext)
     */
    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }

    /*
     * (non-Javadoc)
     * 
     * @see
     * org.apache.tika.parser.netcdf.NetCDFParser#parse(java.io.InputStream,
     * org.xml.sax.ContentHandler, org.apache.tika.metadata.Metadata,
     * org.apache.tika.parser.ParseContext)
     */
    public void parse(InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context) throws IOException,
            SAXException, TikaException {
        ByteArrayOutputStream os = new ByteArrayOutputStream();
        IOUtils.copy(stream, os);

        String name = metadata.get(Metadata.RESOURCE_NAME_KEY);
        if (name == null) {
            name = "";
        }
        try {
            NetcdfFile ncFile = NetcdfFile.openInMemory(name, os.toByteArray());
            unravelStringMet(ncFile, null, metadata);
        } catch (IOException e) {
            throw new TikaException("HDF parse error", e);
        }

        XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);
        xhtml.startDocument();
        xhtml.endDocument();
    }

    protected void unravelStringMet(NetcdfFile ncFile, Group group, Metadata met) {
        if (group == null) {
            group = ncFile.getRootGroup();
        }

        // unravel its string attrs
        for (Attribute attribute : group.getAttributes()) {
            if (attribute.isString()) {
                met.add(attribute.getName(), attribute.getStringValue());
            } else {
                // try and cast its value to a string
                met.add(attribute.getName(), String.valueOf(attribute
                        .getNumericValue()));
            }
        }

        for (Group g : group.getGroups()) {
            unravelStringMet(ncFile, g, met);
        }
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/html/BoilerpipeContentHandler.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.html;

import java.io.Writer;
import java.util.ArrayList;
import java.util.BitSet;
import java.util.List;
import java.util.Locale;

import org.apache.tika.metadata.Metadata;
import org.apache.tika.sax.WriteOutContentHandler;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.Attributes;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.AttributesImpl;

import de.l3s.boilerpipe.BoilerpipeExtractor;
import de.l3s.boilerpipe.BoilerpipeProcessingException;
import de.l3s.boilerpipe.document.TextBlock;
import de.l3s.boilerpipe.document.TextDocument;
import de.l3s.boilerpipe.extractors.ArticleExtractor;
import de.l3s.boilerpipe.extractors.DefaultExtractor;
import de.l3s.boilerpipe.sax.BoilerpipeHTMLContentHandler;

/**
 * Uses the <a href="http://code.google.com/p/boilerpipe/">boilerpipe</a>
 * library to automatically extract the main content from a web page.
 *
 * Use this as a {@link ContentHandler} object passed to
 * {@link HtmlParser#parse(java.io.InputStream, ContentHandler, Metadata, org.apache.tika.parser.ParseContext)}
 */
public class BoilerpipeContentHandler extends BoilerpipeHTMLContentHandler {

    private static class RecordedElement {
        public enum ElementType {
            START,
            END,
            CONTINUE
        }

        private String uri;
        private String localName;
        private String qName;
        private Attributes attrs;
        private List<char[]> characters;
        private ElementType elementType;

        public RecordedElement(String uri, String localName, String qName, Attributes attrs) {
            this(uri, localName, qName, attrs, ElementType.START);
        }

        public RecordedElement(String uri, String localName, String qName) {
            this(uri, localName, qName, null, ElementType.END);
        }

        public RecordedElement() {
            this(null, null, null, null, ElementType.CONTINUE);
        }

        protected RecordedElement(String uri, String localName, String qName, Attributes attrs, RecordedElement.ElementType elementType) {
            this.uri = uri;
            this.localName = localName;
            this.qName = qName;
            this.attrs = attrs;
            this.elementType = elementType;
            this.characters = new ArrayList<char[]>();
        }

        @Override
        public String toString() {
            return String.format(Locale.ROOT, "<%s> of type %s", localName, elementType);
        }

        public String getUri() {
            return uri;
        }

        public String getLocalName() {
            return localName;
        }

        public String getQName() {
            return qName;
        }

        public Attributes getAttrs() {
            return attrs;
        }

        public List<char[]> getCharacters() {
            return characters;
        }

        public RecordedElement.ElementType getElementType() {
            return elementType;
        }
    }

    /**
     * The newline character that gets inserted after block elements.
     */
    private static final char[] NL = new char[] { '\n' };

    private ContentHandler delegate;
    private BoilerpipeExtractor extractor;

    private boolean includeMarkup;
    private boolean inHeader;
    private boolean inFooter;
    private int headerCharOffset;
    private List<RecordedElement> elements;
    private TextDocument td;

    /**
     * Creates a new boilerpipe-based content extractor, using the
     * {@link DefaultExtractor} extraction rules and "delegate" as the content handler.
     *
     * @param delegate
     *            The {@link ContentHandler} object
     */
    public BoilerpipeContentHandler(ContentHandler delegate) {
        this(delegate, DefaultExtractor.INSTANCE);
    }

    /**
     * Creates a content handler that writes XHTML body character events to
     * the given writer.
     *
     * @param writer writer
     */
    public BoilerpipeContentHandler(Writer writer) {
        this(new WriteOutContentHandler(writer));
    }

    /**
     * Creates a new boilerpipe-based content extractor, using the given
     * extraction rules. The extracted main content will be passed to the
     * <delegate> content handler.
     *
     * @param delegate
     *            The {@link ContentHandler} object
     * @param extractor
     *            Extraction rules to use, e.g. {@link ArticleExtractor}
     */
    public BoilerpipeContentHandler(ContentHandler delegate, BoilerpipeExtractor extractor) {
        this.td = null;
        this.delegate = delegate;
        this.extractor = extractor;
    }

    public void setIncludeMarkup(boolean includeMarkup) {
        this.includeMarkup = includeMarkup;
    }

    public boolean isIncludeMarkup() {
        return includeMarkup;
    }

    /**
     * Retrieves the built TextDocument
     *
     * @return TextDocument
     */
    public TextDocument getTextDocument() {
        return td;
    }

    @Override
    public void startDocument() throws SAXException {
        super.startDocument();

        delegate.startDocument();

        inHeader = true;
        inFooter = false;
        headerCharOffset = 0;

        if (includeMarkup) {
            elements = new ArrayList<RecordedElement>();
        }
    };

    @Override
    public void startPrefixMapping(String prefix, String uri) throws SAXException {
        super.startPrefixMapping(prefix, uri);
        delegate.startPrefixMapping(prefix, uri);
    };

    @Override
    public void startElement(String uri, String localName, String qName, Attributes atts) throws SAXException {
        super.startElement(uri, localName, qName, atts);

        if (inHeader) {
            delegate.startElement(uri, localName, qName, atts);
        } else if (inFooter) {
            // Do nothing
        } else if (includeMarkup) {
            elements.add(new RecordedElement(uri, localName, qName, atts));
        } else {
            // This happens for the <body> element, if we're not doing markup.
            delegate.startElement(uri, localName, qName, atts);
        }
    };

    @Override
    public void characters(char[] chars, int offset, int length) throws SAXException {
        super.characters(chars, offset, length);

        if (inHeader) {
            delegate.characters(chars, offset, length);
            headerCharOffset++;
        } else if (inFooter) {
            // Do nothing
        } else if (includeMarkup) {
            RecordedElement element = elements.get(elements.size() - 1);

            char[] characters = new char[length];
            System.arraycopy(chars, offset, characters, 0, length);
            element.getCharacters().add(characters);
        }
    };

    @Override
    public void endElement(String uri, String localName, String qName) throws SAXException {
        super.endElement(uri, localName, qName);

        if (inHeader) {
            delegate.endElement(uri, localName, qName);
            inHeader = !localName.equals("head");
        } else if (inFooter) {
            // Do nothing
        } else if (localName.equals("body")) {
            inFooter = true;
        } else if (includeMarkup) {
            // Add the end element, and the continuation from the previous element
            elements.add(new RecordedElement(uri, localName, qName));
            elements.add(new RecordedElement());
        }
    };

    @Override
    public void endDocument() throws SAXException {
        super.endDocument();

        td = toTextDocument();
        try {
            extractor.process(td);
        } catch (BoilerpipeProcessingException e) {
            throw new SAXException(e);
        }

        Attributes emptyAttrs = new AttributesImpl();

        // At this point we have all the information we need to either emit N paragraphs
        // of plain text (if not including markup), or we have to replay our recorded elements
        // and only emit character runs that passed the boilerpipe filters.
        if (includeMarkup) {
            BitSet validCharacterRuns = new BitSet();
            for (TextBlock block : td.getTextBlocks()) {
                if (block.isContent()) {
                    BitSet bs = block.getContainedTextElements();
                    if (bs != null) {
                        validCharacterRuns.or(bs);
                    }
                }
            }

            // Now have bits set for all valid character runs. Replay our recorded elements,
            // but only emit character runs flagged as valid.
            int curCharsIndex = headerCharOffset;

            for (RecordedElement element : elements) {
                switch (element.getElementType()) {
                    case START:
                        delegate.startElement(element.getUri(), element.getLocalName(), element.getQName(), element.getAttrs());
                        // Fall through

                    case CONTINUE:
                        // Now emit characters that are valid. Note that boilerpipe pre-increments the character index, so
                        // we have to follow suit.
                        for (char[] chars : element.getCharacters()) {
                            curCharsIndex++;

                            if (validCharacterRuns.get(curCharsIndex)) {
                                delegate.characters(chars, 0, chars.length);

                                // https://issues.apache.org/jira/browse/TIKA-961
                                if (!Character.isWhitespace(chars[chars.length - 1])) {
                                    // Only add whitespace for certain elements
                                    if (XHTMLContentHandler.ENDLINE.contains(element.getLocalName())) {
                                        delegate.ignorableWhitespace(NL, 0, NL.length);
                                    }
                                }
                            }
                        }
                        break;

                    case END:
                        delegate.endElement(element.getUri(), element.getLocalName(), element.getQName());
                        break;

                    default:
                        throw new RuntimeException("Unhandled element type: " + element.getElementType());
                }


            }
        } else {
            for (TextBlock block : td.getTextBlocks()) {
                if (block.isContent()) {
                    delegate.startElement(XHTMLContentHandler.XHTML, "p", "p", emptyAttrs);
                    char[] chars = block.getText().toCharArray();
                    delegate.characters(chars, 0, chars.length);
                    delegate.endElement(XHTMLContentHandler.XHTML, "p", "p");
                    delegate.ignorableWhitespace(NL, 0, NL.length);
                }
            }
        }

        delegate.endElement(XHTMLContentHandler.XHTML, "body", "body");
        delegate.endElement(XHTMLContentHandler.XHTML, "html", "html");

        // We defer ending any prefix mapping until here, which is why we don't pass this
        // through to the delegate in an overridden method.
        delegate.endPrefixMapping("");

        delegate.endDocument();
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/html/DefaultHtmlMapper.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.html;

import java.util.HashMap;
import java.util.HashSet;
import java.util.Map;
import java.util.Set;

/**
 * The default HTML mapping rules in Tika.
 *
 * @since Apache Tika 0.6
 */
@SuppressWarnings("serial")
public class DefaultHtmlMapper implements HtmlMapper {

    // Based on http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd
    private static final Map<String, String> SAFE_ELEMENTS = new HashMap<String, String>() {{
        put("H1", "h1");
        put("H2", "h2");
        put("H3", "h3");
        put("H4", "h4");
        put("H5", "h5");
        put("H6", "h6");

        put("P", "p");
        put("PRE", "pre");
        put("BLOCKQUOTE", "blockquote");
        put("Q", "q");
        
        put("UL", "ul");
        put("OL", "ol");
        put("MENU", "ul");
        put("LI", "li");
        put("DL", "dl");
        put("DT", "dt");
        put("DD", "dd");

        put("TABLE", "table");
        put("THEAD", "thead");
        put("TBODY", "tbody");
        put("TR", "tr");
        put("TH", "th");
        put("TD", "td");

        put("ADDRESS", "address");
        
        // TIKA-460 - add anchors
        put("A", "a");
        
        // TIKA-463 - add additional elements that contain URLs (and their sub-elements)
        put("MAP", "map");
        put("AREA", "area");
        put("IMG", "img");
        put("FRAMESET", "frameset");
        put("FRAME", "frame");
        put("IFRAME", "iframe");
        put("OBJECT", "object");
        put("PARAM", "param");
        put("INS", "ins");
        put("DEL", "del");
    }};
    
    private static final Set<String> DISCARDABLE_ELEMENTS = new HashSet<String>() {{
        add("STYLE");
        add("SCRIPT");
    }};

    // For information on tags & attributes, see:
    // http://www.w3.org/TR/2002/REC-xhtml1-20020801/dtds.html#a_dtd_XHTML-1.0-Strict
    // http://www.w3schools.com/TAGS/
    private static final Map<String, Set<String>> SAFE_ATTRIBUTES = new HashMap<String, Set<String>>() {{
        put("a", attrSet("charset", "type", "name", "href", "hreflang", "rel", "rev", "shape", "coords"));
        put("img", attrSet("src", "alt", "longdesc", "height", "width", "usemap", "ismap"));
        put("frame", attrSet("longdesc", "name", "src", "frameborder", "marginwidth", "marginheight", "noresize", "scrolling"));
        put("iframe", attrSet("longdesc", "name", "src", "frameborder", "marginwidth", "marginheight", "scrolling", "align", "height", "width"));
        put("link", attrSet("charset", "href", "hreflang", "type", "rel", "rev", "media"));
        put("map", attrSet("id", "class", "style", "title", "name"));
        put("area", attrSet("shape", "coords", "href", "nohref", "alt"));
        put("object", attrSet("declare", "classid", "codebase", "data", "type", "codetype", "archive", "standby", "height", 
                "width", "usemap", "name", "tabindex", "align", "border", "hspace", "vspace"));
        put("param", attrSet("id", "name", "value", "valuetype", "type"));
        put("blockquote", attrSet("cite"));
        put("ins", attrSet("cite", "datetime"));
        put("del", attrSet("cite", "datetime"));
        put("q", attrSet("cite"));
        
        // TODO - fill out this set. Include core, i18n, etc sets where appropriate.
    }};
    
    private static Set<String> attrSet(String... attrs) {
        Set<String> result = new HashSet<String>();
        for (String attr : attrs) {
            result.add(attr);
        }
        return result;
    }
    
    /**
     * @since Apache Tika 0.8
     */
    public static final HtmlMapper INSTANCE = new DefaultHtmlMapper();

    public String mapSafeElement(String name) {
        return SAFE_ELEMENTS.get(name);
    }

    /** Normalizes an attribute name. Assumes that the element name 
     * is valid and normalized 
     */
    public String mapSafeAttribute(String elementName, String attributeName) {
        Set<String> safeAttrs = SAFE_ATTRIBUTES.get(elementName);
        if ((safeAttrs != null) && safeAttrs.contains(attributeName)) {
            return attributeName;
        } else {
            return null;
        }
    }
    
    public boolean isDiscardElement(String name) {
        return DISCARDABLE_ELEMENTS.contains(name);
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/html/HtmlEncodingDetector.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.html;

import java.io.IOException;
import java.io.InputStream;
import java.nio.ByteBuffer;
import java.nio.charset.Charset;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import org.apache.tika.detect.EncodingDetector;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.utils.CharsetUtils;

/**
 * Character encoding detector for determining the character encoding of a
 * HTML document based on the potential charset parameter found in a
 * Content-Type http-equiv meta tag somewhere near the beginning. Especially
 * useful for determining the type among multiple closely related encodings
 * (ISO-8859-*) for which other types of encoding detection are unreliable.
 *
 * @since Apache Tika 1.2
 */
public class HtmlEncodingDetector implements EncodingDetector {

    // TIKA-357 - use bigger buffer for meta tag sniffing (was 4K)
    private static final int META_TAG_BUFFER_SIZE = 8192;

  
    private static final Pattern HTTP_META_PATTERN = Pattern.compile(
          "(?is)<\\s*meta\\s+([^<>]+)"
          );
    
    //this should match both the older:
    //<meta http-equiv="content-type" content="text/html; charset=xyz"/>
    //and 
    //html5 <meta charset="xyz">
    //See http://webdesign.about.com/od/metatags/qt/meta-charset.htm
    //for the noisiness that one might encounter in charset attrs.
    //Chose to go with strict ([-_:\\.a-z0-9]+) to match encodings
    //following http://docs.oracle.com/javase/7/docs/api/java/nio/charset/Charset.html
    //For a more general "not" matcher, try:
    //("(?is)charset\\s*=\\s*['\\\"]?\\s*([^<>\\s'\\\";]+)")
    private static final Pattern FLEXIBLE_CHARSET_ATTR_PATTERN = Pattern.compile(
          ("(?is)charset\\s*=\\s*(?:['\\\"]\\s*)?([-_:\\.a-z0-9]+)")
          );
    
    private static final Charset ASCII = Charset.forName("US-ASCII");

    public Charset detect(InputStream input, Metadata metadata)
            throws IOException {
        if (input == null) {
            return null;
        }

        // Read enough of the text stream to capture possible meta tags
        input.mark(META_TAG_BUFFER_SIZE);
        byte[] buffer = new byte[META_TAG_BUFFER_SIZE];
        int n = 0;
        int m = input.read(buffer);
        while (m != -1 && n < buffer.length) {
            n += m;
            m = input.read(buffer, n, buffer.length - n);
        }
        input.reset();

        // Interpret the head as ASCII and try to spot a meta tag with
        // a possible character encoding hint
        
        String head = ASCII.decode(ByteBuffer.wrap(buffer, 0, n)).toString();

        Matcher equiv = HTTP_META_PATTERN.matcher(head);
        Matcher charsetMatcher = FLEXIBLE_CHARSET_ATTR_PATTERN.matcher("");
        //iterate through meta tags
        while (equiv.find()) {
           String attrs = equiv.group(1);
           charsetMatcher.reset(attrs);
           //iterate through charset= and return the first match
           //that is valid
           while (charsetMatcher.find()){
              String candCharset = charsetMatcher.group(1);
              if (CharsetUtils.isSupported(candCharset)){
                 try{
                    return CharsetUtils.forName(candCharset);
                 } catch (Exception e){
                    //ignore
                 }
              }
           }
        }
        return null;
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/html/HtmlHandler.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.html;

import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.mime.MediaType;
import org.apache.tika.sax.TextContentHandler;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.Attributes;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.AttributesImpl;

import java.net.MalformedURLException;
import java.net.URL;
import java.util.Arrays;
import java.util.HashSet;
import java.util.Locale;
import java.util.Set;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

class HtmlHandler extends TextContentHandler {

    // List of attributes that need to be resolved.
    private static final Set<String> URI_ATTRIBUTES =
        new HashSet<String>(Arrays.asList("src", "href", "longdesc", "cite"));

    private final HtmlMapper mapper;

    private final XHTMLContentHandler xhtml;

    private final Metadata metadata;

    private int bodyLevel = 0;

    private int discardLevel = 0;

    private int titleLevel = 0;
    
    private boolean isTitleSetToMetadata = false; 

    private final StringBuilder title = new StringBuilder();

    private HtmlHandler(
            HtmlMapper mapper, XHTMLContentHandler xhtml, Metadata metadata) {
        super(xhtml);
        this.mapper = mapper;
        this.xhtml = xhtml;
        this.metadata = metadata;

        // Try to determine the default base URL, if one has not been given
        if (metadata.get(Metadata.CONTENT_LOCATION) == null) {
            String name = metadata.get(Metadata.RESOURCE_NAME_KEY);
            if (name != null) {
                name = name.trim();
                try {
                    new URL(name); // test URL format
                    metadata.set(Metadata.CONTENT_LOCATION, name);
                } catch (MalformedURLException e) {
                    // The resource name is not a valid URL, ignore it
                }
            }
        }
    }

    public HtmlHandler(
            HtmlMapper mapper, ContentHandler handler, Metadata metadata) {
        this(mapper, new XHTMLContentHandler(handler, metadata), metadata);
    }

    @Override
    public void startElement(
            String uri, String local, String name, Attributes atts)
            throws SAXException {
        if ("TITLE".equals(name) || titleLevel > 0) {
            titleLevel++;
        }
        if ("BODY".equals(name) || ("FRAMESET".equals(name)) || bodyLevel > 0) {
            bodyLevel++;
        }
        if (mapper.isDiscardElement(name) || discardLevel > 0) {
            discardLevel++;
        }

        if (bodyLevel == 0 && discardLevel == 0) {
            if ("META".equals(name) && atts.getValue("content") != null) {
                // TIKA-478: For cases where we have either a name or
                // "http-equiv", assume that XHTMLContentHandler will emit
                // these in the <head>, thus passing them through safely.
                if (atts.getValue("http-equiv") != null) {
                    addHtmlMetadata(
                            atts.getValue("http-equiv"),
                            atts.getValue("content"));
                } else if (atts.getValue("name") != null) {
                    // Record the meta tag in the metadata
                    addHtmlMetadata(
                            atts.getValue("name"),
                            atts.getValue("content"));
                } else if (atts.getValue("property") != null) {
                    // TIKA-983: Handle <meta property="og:xxx" content="yyy" /> tags
                    metadata.add(
                            atts.getValue("property"),
                            atts.getValue("content"));
                }
            } else if ("BASE".equals(name) && atts.getValue("href") != null) {
                startElementWithSafeAttributes("base", atts);
                xhtml.endElement("base");
                metadata.set(
                        Metadata.CONTENT_LOCATION,
                        resolve(atts.getValue("href")));
            } else if ("LINK".equals(name)) {
                startElementWithSafeAttributes("link", atts);
                xhtml.endElement("link");
            }
        }

        if (bodyLevel > 0 && discardLevel == 0) {
            String safe = mapper.mapSafeElement(name);
            if (safe != null) {
                startElementWithSafeAttributes(safe, atts);
            }
        }

        title.setLength(0);
    }

    private static final Pattern ICBM =
        Pattern.compile("\\s*(-?\\d+\\.\\d+)[,\\s]+(-?\\d+\\.\\d+)\\s*");

    /**
     * Adds a metadata setting from the HTML <head/> to the Tika metadata
     * object. The name and value are normalized where possible.
     */
    private void addHtmlMetadata(String name, String value) {
        if (name == null || value == null) {
            // ignore
        } else if (name.equalsIgnoreCase("ICBM")) {
            Matcher m = ICBM.matcher(value);
            if (m.matches()) {
                metadata.set("ICBM", m.group(1) + ", " + m.group(2));
                metadata.set(Metadata.LATITUDE, m.group(1));
                metadata.set(Metadata.LONGITUDE, m.group(2));
            } else {
                metadata.set("ICBM", value);
            }
        } else if (name.equalsIgnoreCase(Metadata.CONTENT_TYPE)){
            MediaType type = MediaType.parse(value);
            if (type != null) {
                metadata.set(Metadata.CONTENT_TYPE, type.toString());
            } else {
                metadata.set(Metadata.CONTENT_TYPE, value);
            }
        } else {
            metadata.add(name, value);
        }
    }

    private void startElementWithSafeAttributes(String name, Attributes atts) throws SAXException {
        if (atts.getLength() == 0) {
            xhtml.startElement(name);
            return;
        }

        boolean isObject = name.equals("object");
        String codebase = null;
        if (isObject) {
            codebase = atts.getValue("", "codebase");
            if (codebase != null) {
                codebase = resolve(codebase);
            } else {
                codebase = metadata.get(Metadata.CONTENT_LOCATION);
            }
        }

        AttributesImpl newAttributes = new AttributesImpl(atts);
        for (int att = 0; att < newAttributes.getLength(); att++) {
            String attrName = newAttributes.getLocalName(att);
            String normAttrName = mapper.mapSafeAttribute(name, attrName);
            if (normAttrName == null) {
                newAttributes.removeAttribute(att);
                att--;
            } else {
                // We have a remapped attribute name, so set it as it might have changed.
                newAttributes.setLocalName(att, normAttrName);

                // And resolve relative links. Eventually this should be pushed
                // into the HtmlMapper code.
                if (URI_ATTRIBUTES.contains(normAttrName)) {
                    newAttributes.setValue(att, resolve(newAttributes.getValue(att)));
                } else if (isObject && "codebase".equals(normAttrName)) {
                    newAttributes.setValue(att, codebase);
                } else if (isObject
                        && ("data".equals(normAttrName)
                                || "classid".equals(normAttrName))) {
                    newAttributes.setValue(
                            att,
                            resolve(codebase, newAttributes.getValue(att)));
                }
            }
        }

        if ("img".equals(name) && newAttributes.getValue("", "alt") == null) {
            newAttributes.addAttribute("", "alt", "alt", "CDATA", "");
        }

        xhtml.startElement(name, newAttributes);
    }

    @Override
    public void endElement(
            String uri, String local, String name) throws SAXException {
        if (bodyLevel > 0 && discardLevel == 0) {
            String safe = mapper.mapSafeElement(name);
            if (safe != null) {
                xhtml.endElement(safe);
            } else if (XHTMLContentHandler.ENDLINE.contains(
                    name.toLowerCase(Locale.ENGLISH))) {
                // TIKA-343: Replace closing block tags (and <br/>) with a
                // newline unless the HtmlMapper above has already mapped
                // them to something else
                xhtml.newline();
            }
        }

        if (titleLevel > 0) {
            titleLevel--;
            if (titleLevel == 0 && !isTitleSetToMetadata) {            	
                metadata.set(TikaCoreProperties.TITLE, title.toString().trim());
                isTitleSetToMetadata = true;
            }
        }
        if (bodyLevel > 0) {
            bodyLevel--;
        }
        if (discardLevel > 0) {
            discardLevel--;
        }
    }

    @Override
    public void characters(char[] ch, int start, int length)
            throws SAXException {
        if (titleLevel > 0 && bodyLevel == 0) {
            title.append(ch, start, length);
        }
        if (bodyLevel > 0 && discardLevel == 0) {
            super.characters(ch, start, length);
        }
    }

    @Override
    public void ignorableWhitespace(char[] ch, int start, int length)
            throws SAXException {
        if (bodyLevel > 0 && discardLevel == 0) {
            super.ignorableWhitespace(ch, start, length);
        }
    }

    private String resolve(String url) {
        return resolve(metadata.get(Metadata.CONTENT_LOCATION), url);
    }

    private String resolve(String base, String url) {
        url = url.trim();

        // Return the URL as-is if no base URL is available or if the URL
        // matches a common non-hierarchical or pseudo URI prefix
        String lower = url.toLowerCase(Locale.ENGLISH);
        if (base == null
                || lower.startsWith("urn:")
                || lower.startsWith("mailto:")
                || lower.startsWith("tel:")
                || lower.startsWith("data:")
                || lower.startsWith("javascript:")
                || lower.startsWith("about:")) {
            return url;
        }

        try {
            URL baseURL = new URL(base.trim());

            // We need to handle one special case, where the relativeUrl is
            // just a query string (like "?pid=1"), and the baseUrl doesn't
            // end with a '/'. In that case, the URL class removes the last
            // portion of the path, which we don't want.
            String path = baseURL.getPath();
            if (url.startsWith("?") && path.length() > 0 && !path.endsWith("/")) {
                return new URL(
                        baseURL.getProtocol(),
                        baseURL.getHost(), baseURL.getPort(),
                        baseURL.getPath() + url).toExternalForm();
            } else {
                return new URL(baseURL, url).toExternalForm();
            }
        } catch (MalformedURLException e) {
            // Unknown or broken format; just return the URL as received.
            return url;
        }
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/html/HtmlMapper.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.html;

/**
 * HTML mapper used to make incoming HTML documents easier to handle by
 * Tika clients. The {@link HtmlParser} looks up an optional HTML mapper from
 * the parse context and uses it to map parsed HTML to "safe" XHTML. A client
 * that wants to customize this mapping can place a custom HtmlMapper instance
 * into the parse context.
 *
 * @since Apache Tika 0.6
 */
public interface HtmlMapper {

    /**
     * Maps "safe" HTML element names to semantic XHTML equivalents. If the
     * given element is unknown or deemed unsafe for inclusion in the parse
     * output, then this method returns <code>null</code> and the element
     * will be ignored but the content inside it is still processed. See
     * the {@link #isDiscardElement(String)} method for a way to discard
     * the entire contents of an element.
     *
     * @param name HTML element name (upper case)
     * @return XHTML element name (lower case), or
     *         <code>null</code> if the element is unsafe 
     */
    String mapSafeElement(String name);

    /**
     * Checks whether all content within the given HTML element should be
     * discarded instead of including it in the parse output.
     *
     * @param name HTML element name (upper case)
     * @return <code>true</code> if content inside the named element
     *         should be ignored, <code>false</code> otherwise
     */
    boolean isDiscardElement(String name);
    
    
    /**
     * Maps "safe" HTML attribute names to semantic XHTML equivalents. If the
     * given attribute is unknown or deemed unsafe for inclusion in the parse
     * output, then this method returns <code>null</code> and the attribute
     * will be ignored. This method assumes that the element name 
     * is valid and normalised.
     *
     * @param elementName HTML element name (lower case)
     * @param attributeName HTML attribute name (lower case)
     * @return XHTML attribute name (lower case), or
     *         <code>null</code> if the element is unsafe 
     */
    String mapSafeAttribute(String elementName, String attributeName);

}
"
tika-parsers/src/main/java/org/apache/tika/parser/html/HtmlParser.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.html;

import java.io.IOException;
import java.io.InputStream;
import java.nio.charset.Charset;
import java.util.Arrays;
import java.util.Collections;
import java.util.HashSet;
import java.util.Set;

import org.apache.tika.config.ServiceLoader;
import org.apache.tika.detect.AutoDetectReader;
import org.apache.tika.exception.TikaException;
import org.apache.tika.io.CloseShieldInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.ccil.cowan.tagsoup.HTMLSchema;
import org.ccil.cowan.tagsoup.Schema;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * HTML parser. Uses TagSoup to turn the input document to HTML SAX events,
 * and post-processes the events to produce XHTML and metadata expected by
 * Tika clients.
 */
public class HtmlParser extends AbstractParser {

    /** Serial version UID */
    private static final long serialVersionUID = 7895315240498733128L;

    private static final Set<MediaType> SUPPORTED_TYPES =
        Collections.unmodifiableSet(new HashSet<MediaType>(Arrays.asList(
                MediaType.text("html"),
                MediaType.application("xhtml+xml"),
                MediaType.application("vnd.wap.xhtml+xml"),
                MediaType.application("x-asp"))));

    private static final ServiceLoader LOADER =
            new ServiceLoader(HtmlParser.class.getClassLoader());

    /**
     * HTML schema singleton used to amortise the heavy instantiation time.
     */
    private static final Schema HTML_SCHEMA = new HTMLSchema();

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }

    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        // Automatically detect the character encoding
        AutoDetectReader reader = new AutoDetectReader(
                new CloseShieldInputStream(stream), metadata,
                context.get(ServiceLoader.class, LOADER));
        try {
            Charset charset = reader.getCharset();
            String previous = metadata.get(Metadata.CONTENT_TYPE);
            if (previous == null || previous.startsWith("text/html")) {
                MediaType type = new MediaType(MediaType.TEXT_HTML, charset);
                metadata.set(Metadata.CONTENT_TYPE, type.toString());
            }
            // deprecated, see TIKA-431
            metadata.set(Metadata.CONTENT_ENCODING, charset.name());

            // Get the HTML mapper from the parse context
            HtmlMapper mapper =
                    context.get(HtmlMapper.class, new HtmlParserMapper());

            // Parse the HTML document
            org.ccil.cowan.tagsoup.Parser parser =
                    new org.ccil.cowan.tagsoup.Parser();

            // Use schema from context or default
            Schema schema = context.get(Schema.class, HTML_SCHEMA);

            // TIKA-528: Reuse share schema to avoid heavy instantiation
            parser.setProperty(
                    org.ccil.cowan.tagsoup.Parser.schemaProperty, schema);
            // TIKA-599: Shared schema is thread-safe only if bogons are ignored
            parser.setFeature(
                    org.ccil.cowan.tagsoup.Parser.ignoreBogonsFeature, true);

            parser.setContentHandler(new XHTMLDowngradeHandler(
                    new HtmlHandler(mapper, handler, metadata)));

            parser.parse(reader.asInputSource());
        } finally {
            reader.close();
        }
    }

    /**
     * Maps "safe" HTML element names to semantic XHTML equivalents. If the
     * given element is unknown or deemed unsafe for inclusion in the parse
     * output, then this method returns <code>null</code> and the element
     * will be ignored but the content inside it is still processed. See
     * the {@link #isDiscardElement(String)} method for a way to discard
     * the entire contents of an element.
     * <p>
     * Subclasses can override this method to customize the default mapping.
     *
     * @deprecated Use the {@link HtmlMapper} mechanism to customize
     *             the HTML mapping. This method will be removed in Tika 1.0.
     * @since Apache Tika 0.5
     * @param name HTML element name (upper case)
     * @return XHTML element name (lower case), or
     *         <code>null</code> if the element is unsafe 
     */
    protected String mapSafeElement(String name) {
        return DefaultHtmlMapper.INSTANCE.mapSafeElement(name);
    }

    /**
     * Checks whether all content within the given HTML element should be
     * discarded instead of including it in the parse output. Subclasses
     * can override this method to customize the set of discarded elements.
     *
     * @deprecated Use the {@link HtmlMapper} mechanism to customize
     *             the HTML mapping. This method will be removed in Tika 1.0.
     * @since Apache Tika 0.5
     * @param name HTML element name (upper case)
     * @return <code>true</code> if content inside the named element
     *         should be ignored, <code>false</code> otherwise
     */
    protected boolean isDiscardElement(String name) {
        return DefaultHtmlMapper.INSTANCE.isDiscardElement(name);
    }

    /**
    * @deprecated Use the {@link HtmlMapper} mechanism to customize
    *             the HTML mapping. This method will be removed in Tika 1.0.
    **/
    public String mapSafeAttribute(String elementName, String attributeName) {
        return DefaultHtmlMapper.INSTANCE.mapSafeAttribute(elementName,attributeName) ;
    }    
    
    /**
     * Adapter class that maintains backwards compatibility with the
     * protected HtmlParser methods. Making HtmlParser implement HtmlMapper
     * directly would require those methods to be public, which would break
     * backwards compatibility with subclasses.
     *
     * @deprecated Use the {@link HtmlMapper} mechanism to customize
     *             the HTML mapping. This class will be removed in Tika 1.0.
     */
    private class HtmlParserMapper implements HtmlMapper {
        public String mapSafeElement(String name) {
            return HtmlParser.this.mapSafeElement(name);
        }
        public boolean isDiscardElement(String name) {
            return HtmlParser.this.isDiscardElement(name);
        }
        public String mapSafeAttribute(String elementName, String attributeName){
            return HtmlParser.this.mapSafeAttribute(elementName,attributeName);
        }
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/html/IdentityHtmlMapper.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.html;

import java.util.Locale;

/**
 * Alternative HTML mapping rules that pass the input HTML as-is without any
 * modifications.
 * 
 * @since Apache Tika 0.8
 */
public class IdentityHtmlMapper implements HtmlMapper {

    public static final HtmlMapper INSTANCE = new IdentityHtmlMapper();

    public boolean isDiscardElement(String name) {
        return false;
    }

    public String mapSafeAttribute(String elementName, String attributeName) {
        return attributeName.toLowerCase(Locale.ENGLISH);
    }

    public String mapSafeElement(String name) {
        return name.toLowerCase(Locale.ENGLISH);
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/html/XHTMLDowngradeHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.html;

import java.util.Locale;

import javax.xml.XMLConstants;

import org.apache.tika.sax.ContentHandlerDecorator;
import org.xml.sax.Attributes;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.AttributesImpl;

/**
 * Content handler decorator that downgrades XHTML elements to
 * old-style HTML elements before passing them on to the decorated
 * content handler. This downgrading consists of dropping all namespaces
 * (and namespaced attributes) and uppercasing all element names.
 * Used by the {@link HtmlParser} to make all incoming HTML look the same.
 */
class XHTMLDowngradeHandler extends ContentHandlerDecorator {

    public XHTMLDowngradeHandler(ContentHandler handler) {
        super(handler);
    }

    @Override
    public void startElement(
            String uri, String localName, String name, Attributes atts)
            throws SAXException {
        String upper = localName.toUpperCase(Locale.ENGLISH);

        AttributesImpl attributes = new AttributesImpl();
        for (int i = 0; i < atts.getLength(); i++) {
            String auri = atts.getURI(i);
            String local = atts.getLocalName(i);
            String qname = atts.getQName(i);
            if (XMLConstants.NULL_NS_URI.equals(auri)
                    && !local.equals(XMLConstants.XMLNS_ATTRIBUTE)
                    && !qname.startsWith(XMLConstants.XMLNS_ATTRIBUTE + ":")) {
                attributes.addAttribute(
                        auri, local, qname, atts.getType(i), atts.getValue(i));
            }
        }

        super.startElement(XMLConstants.NULL_NS_URI, upper, upper, attributes);
    }

    @Override
    public void endElement(String uri, String localName, String name)
            throws SAXException {
        String upper = localName.toUpperCase(Locale.ENGLISH);
        super.endElement(XMLConstants.NULL_NS_URI, upper, upper);
    }

    @Override
    public void startPrefixMapping(String prefix, String uri) {
    }

    @Override
    public void endPrefixMapping(String prefix) {
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/image/BPGParser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.image;

import java.io.IOException;
import java.io.InputStream;
import java.util.Arrays;
import java.util.Collections;
import java.util.HashSet;
import java.util.Set;

import org.apache.poi.util.IOUtils;
import org.apache.tika.exception.TikaException;
import org.apache.tika.io.EndianUtils;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.Photoshop;
import org.apache.tika.metadata.TIFF;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * Parser for the Better Portable Graphics )BPG) File Format.
 * 
 * Documentation on the file format is available from
 * http://bellard.org/bpg/bpg_spec.txt
 */
public class BPGParser extends AbstractParser {
    private static final long serialVersionUID = -161736541253892772L;
    
    private static final Set<MediaType> SUPPORTED_TYPES =
        Collections.unmodifiableSet(new HashSet<MediaType>(Arrays.asList(
                MediaType.image("x-bpg"), MediaType.image("bpg"))));

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }
    
    protected static final int EXTENSION_TAG_EXIF = 1;
    protected static final int EXTENSION_TAG_ICC_PROFILE = 2;
    protected static final int EXTENSION_TAG_XMP = 3;
    protected static final int EXTENSION_TAG_THUMBNAIL = 4;

    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        // Check for the magic header signature
        byte[] signature = new byte[4];
        IOUtils.readFully(stream, signature);
        if(signature[0] == (byte)'B' && signature[1] == (byte)'P' &&
           signature[2] == (byte)'G' && signature[3] == (byte)0xfb) {
           // Good, signature found
        } else {
           throw new TikaException("BPG magic signature invalid");
        }
        
        // Grab and decode the first byte
        int pdf = stream.read();
        
        // Pixel format: Greyscale / 4:2:0 / 4:2:2 / 4:4:4
        int pixelFormat = pdf & 0x7;
        // TODO Identify a suitable metadata key for this

        // Is there an alpha plane as well as a colour plane?
        boolean hasAlphaPlane1 = (pdf & 0x8) == 0x8;
        // TODO Identify a suitable metadata key for this+hasAlphaPlane2
        
        // Bit depth minus 8
        int bitDepth = (pdf >> 4) + 8;
        metadata.set(TIFF.BITS_PER_SAMPLE, Integer.toString(bitDepth));
        
        // Grab and decode the second byte
        int cer = stream.read();
        
        // Colour Space: YCbCr / RGB / YCgCo / YCbCrK / CMYK
        int colourSpace = cer & 0x15;
        switch (colourSpace) {
            case 0:
                metadata.set(Photoshop.COLOR_MODE, "YCbCr Colour");
                break;
            case 1:
                metadata.set(Photoshop.COLOR_MODE, "RGB Colour");
                break;
            case 2:
                metadata.set(Photoshop.COLOR_MODE, "YCgCo Colour");
                break;
            case 3:
                metadata.set(Photoshop.COLOR_MODE, "YCbCrK Colour");
                break;
            case 4:
                metadata.set(Photoshop.COLOR_MODE, "CMYK Colour");
                break;
        }
        
        // Are there extensions or not?
        boolean hasExtensions = (cer & 16) == 16;

        // Is the Alpha Plane 2 flag set?
        boolean hasAlphaPlane2 = (cer & 32) == 32;

        // cer then holds 2 more booleans - limited range, reserved
        
        // Width and height next
        int width  = (int)EndianUtils.readUE7(stream);
        int height = (int)EndianUtils.readUE7(stream);
        metadata.set(TIFF.IMAGE_LENGTH, height);
        metadata.set(TIFF.IMAGE_WIDTH, width);
        
        // Picture Data length
        EndianUtils.readUE7(stream);
        
        // Extension Data Length, if extensions present
        long extensionDataLength = 0;
        if (hasExtensions)
            extensionDataLength = EndianUtils.readUE7(stream);
        
        // Alpha Data Length, if alpha used
        long alphaDataLength = 0;
        if (hasAlphaPlane1 || hasAlphaPlane2)
            alphaDataLength = EndianUtils.readUE7(stream);
        
        // Extension Data
        if (hasExtensions) {
            long extensionsDataSeen = 0;
            ImageMetadataExtractor metadataExtractor = 
                    new ImageMetadataExtractor(metadata);
            
            while (extensionsDataSeen < extensionDataLength) {
                int extensionType = (int)EndianUtils.readUE7(stream);
                int extensionLength = (int)EndianUtils.readUE7(stream);
                switch (extensionType) {
                    case EXTENSION_TAG_EXIF:
                        metadataExtractor.parseRawExif(stream, extensionLength, true);
                        break;
                    case EXTENSION_TAG_XMP:
                        handleXMP(stream, extensionLength, metadataExtractor);
                        break;
                    default:
                        stream.skip(extensionLength);
                }
                extensionsDataSeen += extensionLength;
            }
        }
        
        // HEVC Header + Data
        // Alpha HEVC Header + Data
        // We can't do anything with these parts
        
        // We don't have any helpful text, sorry...
        XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);
        xhtml.startDocument();
        xhtml.endDocument();
    }
    
    protected void handleXMP(InputStream stream, int xmpLength, 
            ImageMetadataExtractor extractor) throws IOException, TikaException, SAXException {
        byte[] xmp = new byte[xmpLength];
        IOUtils.readFully(stream, xmp);
        extractor.parseRawXMP(xmp);
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/image/ImageMetadataExtractor.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.image;

import java.io.File;
import java.io.IOException;
import java.io.InputStream;
import java.text.DecimalFormat;
import java.text.DecimalFormatSymbols;
import java.text.SimpleDateFormat;
import java.util.Date;
import java.util.Iterator;
import java.util.Locale;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import org.apache.poi.util.IOUtils;
import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.IPTC;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.Property;
import org.apache.tika.metadata.TikaCoreProperties;
import org.xml.sax.SAXException;

import com.drew.imaging.jpeg.JpegMetadataReader;
import com.drew.imaging.jpeg.JpegProcessingException;
import com.drew.imaging.tiff.TiffMetadataReader;
import com.drew.lang.BufferReader;
import com.drew.lang.ByteArrayReader;
import com.drew.lang.GeoLocation;
import com.drew.lang.Rational;
import com.drew.metadata.Directory;
import com.drew.metadata.MetadataException;
import com.drew.metadata.Tag;
import com.drew.metadata.exif.ExifIFD0Directory;
import com.drew.metadata.exif.ExifReader;
import com.drew.metadata.exif.ExifSubIFDDirectory;
import com.drew.metadata.exif.ExifThumbnailDirectory;
import com.drew.metadata.exif.GpsDirectory;
import com.drew.metadata.iptc.IptcDirectory;
import com.drew.metadata.jpeg.JpegCommentDirectory;
import com.drew.metadata.jpeg.JpegDirectory;
import com.drew.metadata.xmp.XmpReader;

/**
 * Uses the <a href="http://www.drewnoakes.com/code/exif/">Metadata Extractor</a> library
 * to read EXIF and IPTC image metadata and map to Tika fields.
 * 
 * As of 2.4.0 the library supports jpeg and tiff.
 */
public class ImageMetadataExtractor {

    private final Metadata metadata;
    private DirectoryHandler[] handlers;
    private static final String GEO_DECIMAL_FORMAT_STRING = "#.######"; // 6 dp seems to be reasonable

    /**
     * @param metadata to extract to, using default directory handlers
     */
    public ImageMetadataExtractor(Metadata metadata) {
        this(metadata,
            new CopyUnknownFieldsHandler(),
            new JpegCommentHandler(),
            new ExifHandler(),
            new DimensionsHandler(),
            new GeotagHandler(),
            new IptcHandler()
        );
    }
    
    /**
     * @param metadata to extract to
     * @param handlers handlers in order, note that handlers may override values from earlier handlers
     */
    public ImageMetadataExtractor(Metadata metadata, DirectoryHandler... handlers) {
        this.metadata = metadata;
        this.handlers = handlers;
    }

    public void parseJpeg(File file)
            throws IOException, SAXException, TikaException {
        try {
            com.drew.metadata.Metadata jpegMetadata = JpegMetadataReader.readMetadata(file);
            handle(jpegMetadata);
        } catch (JpegProcessingException e) {
            throw new TikaException("Can't read JPEG metadata", e);
        } catch (MetadataException e) {
            throw new TikaException("Can't read JPEG metadata", e);
        }
    }

    public void parseTiff(File file)
            throws IOException, SAXException, TikaException {
        try {
            com.drew.metadata.Metadata tiffMetadata = TiffMetadataReader.readMetadata(file);
            handle(tiffMetadata);
        } catch (MetadataException e) {
            throw new TikaException("Can't read TIFF metadata", e);
        }
    }
    
    public void parseRawExif(InputStream stream, int length, boolean needsExifHeader)
            throws IOException, SAXException, TikaException {
        byte[] exif;
        if (needsExifHeader) {
            exif = new byte[length+6];
            exif[0] = (byte)'E';
            exif[1] = (byte)'x';
            exif[2] = (byte)'i';
            exif[3] = (byte)'f';
            IOUtils.readFully(stream, exif, 6, length);
        } else {
            exif = new byte[length];
            IOUtils.readFully(stream, exif, 0, length);
        }
        parseRawExif(exif);
    }
    public void parseRawExif(byte[] exifData)
            throws IOException, SAXException, TikaException {
        BufferReader exifReader = new ByteArrayReader(exifData);
        
        com.drew.metadata.Metadata metadata = new com.drew.metadata.Metadata();
        ExifReader reader = new ExifReader();
        reader.extract(exifReader, metadata);
        
        try {
            handle(metadata);
        } catch (MetadataException e) {
            throw new TikaException("Can't process the EXIF Data", e);
        }
    }
    public void parseRawXMP(byte[] xmpData)
            throws IOException, SAXException, TikaException {
        BufferReader xmpReader = new ByteArrayReader(xmpData);
        
        com.drew.metadata.Metadata metadata = new com.drew.metadata.Metadata();
        XmpReader reader = new XmpReader();
        reader.extract(xmpReader, metadata);
        
        try {
            handle(metadata);
        } catch (MetadataException e) {
            throw new TikaException("Can't process the XMP Data", e);
        }
    }

    /**
     * Copies extracted tags to tika metadata using registered handlers.
     * @param metadataExtractor Tag directories from a Metadata Extractor "reader"
     * @throws MetadataException This method does not handle exceptions from Metadata Extractor
     */
    protected void handle(com.drew.metadata.Metadata metadataExtractor) 
            throws MetadataException {
        handle(metadataExtractor.getDirectories().iterator());
    }

    /**
     * Copies extracted tags to tika metadata using registered handlers.
     * @param directories Metadata Extractor {@link com.drew.metadata.Directory} instances.
     * @throws MetadataException This method does not handle exceptions from Metadata Extractor
     */    
    protected void handle(Iterator<Directory> directories) throws MetadataException {
        while (directories.hasNext()) {
            Directory directory = directories.next();
            for (DirectoryHandler handler : handlers) {
                if (handler.supports(directory.getClass())) {
                    handler.handle(directory, metadata);
                }
            }
        }
    }

    /**
     * Reads one or more type of Metadata Extractor fields.
     */
    static interface DirectoryHandler {
        /**
         * @param directoryType A Metadata Extractor directory class
         * @return true if the directory type is supported by this handler
         */
        boolean supports(Class<? extends Directory> directoryType);
        /**
         * @param directory extracted tags
         * @param metadata current tika metadata
         * @throws MetadataException typically field extraction error, aborts all further extraction
         */
        void handle(Directory directory, Metadata metadata) 
                throws MetadataException;
    }

    /**
     * Mimics the behavior from TIKA-314 of copying all extracted tags
     * to tika metadata using field names from Metadata Extractor.
     */
    static class CopyAllFieldsHandler implements DirectoryHandler {
        public boolean supports(Class<? extends Directory> directoryType) {
            return true;
        }
        public void handle(Directory directory, Metadata metadata)
                throws MetadataException {
            if (directory.getTags() != null) {
                for (Tag tag : directory.getTags()) {
                    metadata.set(tag.getTagName(), tag.getDescription());
                }
            }
        }
    }    
    
    /**
     * Copies all fields regardless of directory, if the tag name
     * is not identical to a known Metadata field name.
     * This leads to more predictable behavior than {@link CopyAllFieldsHandler}.
     */
    static class CopyUnknownFieldsHandler implements DirectoryHandler {
        public boolean supports(Class<? extends Directory> directoryType) {
            return true;
        }
        public void handle(Directory directory, Metadata metadata)
                throws MetadataException {
            if (directory.getTags() != null) {
                for (Tag tag : directory.getTags()) {
                    String name = tag.getTagName();
                    if (!MetadataFields.isMetadataField(name) && tag.getDescription() != null) {
                          String value = tag.getDescription().trim();
                          if (Boolean.TRUE.toString().equalsIgnoreCase(value)) {
                              value = Boolean.TRUE.toString();
                          } else if (Boolean.FALSE.toString().equalsIgnoreCase(value)) {
                              value = Boolean.FALSE.toString();
                          }
                          metadata.set(name, value);
                    }
                }
            }
        }
    }
    
    /**
     * Basic image properties for TIFF and JPEG, at least.
     */
    static class DimensionsHandler implements DirectoryHandler {
        private final Pattern LEADING_NUMBERS = Pattern.compile("(\\d+)\\s*.*");
        public boolean supports(Class<? extends Directory> directoryType) {
            return directoryType == JpegDirectory.class || 
                        directoryType == ExifSubIFDDirectory.class ||
                        directoryType == ExifThumbnailDirectory.class ||
                        directoryType == ExifIFD0Directory.class;
        }
        public void handle(Directory directory, Metadata metadata) throws MetadataException {
            // The test TIFF has width and height stored as follows according to exiv2
            //Exif.Image.ImageWidth                        Short       1  100
            //Exif.Image.ImageLength                       Short       1  75
            // and the values are found in "Thumbnail Image Width" (and Height) from Metadata Extractor
            set(directory, metadata, ExifThumbnailDirectory.TAG_THUMBNAIL_IMAGE_WIDTH, Metadata.IMAGE_WIDTH);
            set(directory, metadata, JpegDirectory.TAG_JPEG_IMAGE_WIDTH, Metadata.IMAGE_WIDTH);
            set(directory, metadata, ExifThumbnailDirectory.TAG_THUMBNAIL_IMAGE_HEIGHT, Metadata.IMAGE_LENGTH);
            set(directory, metadata, JpegDirectory.TAG_JPEG_IMAGE_HEIGHT, Metadata.IMAGE_LENGTH);
            // Bits per sample, two methods of extracting, exif overrides jpeg
            set(directory, metadata, JpegDirectory.TAG_JPEG_DATA_PRECISION, Metadata.BITS_PER_SAMPLE);
            set(directory, metadata, ExifSubIFDDirectory.TAG_BITS_PER_SAMPLE, Metadata.BITS_PER_SAMPLE);
            // Straightforward
            set(directory, metadata, ExifSubIFDDirectory.TAG_SAMPLES_PER_PIXEL, Metadata.SAMPLES_PER_PIXEL);
        }
        private void set(Directory directory, Metadata metadata, int extractTag, Property metadataField) {
            if (directory.containsTag(extractTag)) {
                Matcher m = LEADING_NUMBERS.matcher(directory.getString(extractTag));
                if(m.matches()) {
                    metadata.set(metadataField, m.group(1));
                }
            }
        }
    }
    
    static class JpegCommentHandler implements DirectoryHandler {
        public boolean supports(Class<? extends Directory> directoryType) {
            return directoryType == JpegCommentDirectory.class;
        }
        public void handle(Directory directory, Metadata metadata) throws MetadataException {
            if (directory.containsTag(JpegCommentDirectory.TAG_JPEG_COMMENT)) {
                metadata.add(TikaCoreProperties.COMMENTS, directory.getString(JpegCommentDirectory.TAG_JPEG_COMMENT));
            }
        }
    }
    
    static class ExifHandler implements DirectoryHandler {
        // There's a new ExifHandler for each file processed, so this is thread safe
	private static final ThreadLocal<SimpleDateFormat> DATE_UNSPECIFIED_TZ = new ThreadLocal<SimpleDateFormat>(){
            @Override
            protected SimpleDateFormat initialValue()
            {
               return new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss");
            }
        };

        public boolean supports(Class<? extends Directory> directoryType) {
            return directoryType == ExifIFD0Directory.class || 
                    directoryType == ExifSubIFDDirectory.class;
        }
        public void handle(Directory directory, Metadata metadata) {
            try {
                handleDateTags(directory, metadata);
                handlePhotoTags(directory, metadata);
                handleCommentTags(directory, metadata);
            } catch (MetadataException e) {
                // ignore date parse errors and proceed with other tags
            }
        }
        /**
         * EXIF may contain image description, although with undefined encoding.
         * Use IPTC for other annotation fields, and XMP for unicode support.
         */
        public void handleCommentTags(Directory directory, Metadata metadata) {
            if (metadata.get(TikaCoreProperties.DESCRIPTION) == null &&
                    directory.containsTag(ExifIFD0Directory.TAG_IMAGE_DESCRIPTION)) {
                metadata.set(TikaCoreProperties.DESCRIPTION, 
                        directory.getString(ExifIFD0Directory.TAG_IMAGE_DESCRIPTION));
            }
        }
        /**
         * Maps common TIFF and EXIF tags onto the Tika
         *  TIFF image metadata namespace.
         */       
        public void handlePhotoTags(Directory directory, Metadata metadata) {
            if(directory.containsTag(ExifSubIFDDirectory.TAG_EXPOSURE_TIME)) {
               Object exposure = directory.getObject(ExifSubIFDDirectory.TAG_EXPOSURE_TIME);
               if(exposure instanceof Rational) {
                  metadata.set(Metadata.EXPOSURE_TIME, ((Rational)exposure).doubleValue());
               } else {
                  metadata.set(Metadata.EXPOSURE_TIME, directory.getString(ExifSubIFDDirectory.TAG_EXPOSURE_TIME));
               }
            }
            
            if(directory.containsTag(ExifSubIFDDirectory.TAG_FLASH)) {
               String flash = directory.getDescription(ExifSubIFDDirectory.TAG_FLASH);
               if(flash.contains("Flash fired")) {
                  metadata.set(Metadata.FLASH_FIRED, Boolean.TRUE.toString());
               }
               else if(flash.contains("Flash did not fire")) {
                  metadata.set(Metadata.FLASH_FIRED, Boolean.FALSE.toString());
               }
               else {
                  metadata.set(Metadata.FLASH_FIRED, flash);
               }
            }

            if(directory.containsTag(ExifSubIFDDirectory.TAG_FNUMBER)) {
               Object fnumber = directory.getObject(ExifSubIFDDirectory.TAG_FNUMBER);
               if(fnumber instanceof Rational) {
                  metadata.set(Metadata.F_NUMBER, ((Rational)fnumber).doubleValue());
               } else {
                  metadata.set(Metadata.F_NUMBER, directory.getString(ExifSubIFDDirectory.TAG_FNUMBER));
               }
            }
            
            if(directory.containsTag(ExifSubIFDDirectory.TAG_FOCAL_LENGTH)) {
               Object length = directory.getObject(ExifSubIFDDirectory.TAG_FOCAL_LENGTH);
               if(length instanceof Rational) {
                  metadata.set(Metadata.FOCAL_LENGTH, ((Rational)length).doubleValue());
               } else {
                  metadata.set(Metadata.FOCAL_LENGTH, directory.getString(ExifSubIFDDirectory.TAG_FOCAL_LENGTH));
               }
            }
            
            if(directory.containsTag(ExifSubIFDDirectory.TAG_ISO_EQUIVALENT)) {
               metadata.set(Metadata.ISO_SPEED_RATINGS, directory.getString(ExifSubIFDDirectory.TAG_ISO_EQUIVALENT));
            }
          
            if(directory.containsTag(ExifIFD0Directory.TAG_MAKE)) {
               metadata.set(Metadata.EQUIPMENT_MAKE, directory.getString(ExifIFD0Directory.TAG_MAKE));
            }
            if(directory.containsTag(ExifIFD0Directory.TAG_MODEL)) {
               metadata.set(Metadata.EQUIPMENT_MODEL, directory.getString(ExifIFD0Directory.TAG_MODEL));
            }
          
            if(directory.containsTag(ExifIFD0Directory.TAG_ORIENTATION)) {
               Object length = directory.getObject(ExifIFD0Directory.TAG_ORIENTATION);
               if(length instanceof Integer) {
                  metadata.set(Metadata.ORIENTATION, Integer.toString((Integer)length));
               } else {
                  metadata.set(Metadata.ORIENTATION, directory.getString(ExifIFD0Directory.TAG_ORIENTATION));
               }
            }
            
            if(directory.containsTag(ExifIFD0Directory.TAG_SOFTWARE)) {
               metadata.set(Metadata.SOFTWARE, directory.getString(ExifIFD0Directory.TAG_SOFTWARE));
            }
            
            if(directory.containsTag(ExifIFD0Directory.TAG_X_RESOLUTION)) {
               Object resolution = directory.getObject(ExifIFD0Directory.TAG_X_RESOLUTION);
               if(resolution instanceof Rational) {
                  metadata.set(Metadata.RESOLUTION_HORIZONTAL, ((Rational)resolution).doubleValue());
               } else {
                  metadata.set(Metadata.RESOLUTION_HORIZONTAL, directory.getString(ExifIFD0Directory.TAG_X_RESOLUTION));
               }
            }
            if(directory.containsTag(ExifIFD0Directory.TAG_Y_RESOLUTION)) {
               Object resolution = directory.getObject(ExifIFD0Directory.TAG_Y_RESOLUTION);
               if(resolution instanceof Rational) {
                  metadata.set(Metadata.RESOLUTION_VERTICAL, ((Rational)resolution).doubleValue());
               } else {
                  metadata.set(Metadata.RESOLUTION_VERTICAL, directory.getString(ExifIFD0Directory.TAG_Y_RESOLUTION));
               }
            }
            if(directory.containsTag(ExifIFD0Directory.TAG_RESOLUTION_UNIT)) {
               metadata.set(Metadata.RESOLUTION_UNIT, directory.getDescription(ExifIFD0Directory.TAG_RESOLUTION_UNIT));
            }
            if(directory.containsTag(ExifThumbnailDirectory.TAG_THUMBNAIL_IMAGE_WIDTH)) {
                metadata.set(Metadata.IMAGE_WIDTH, directory.getDescription(ExifThumbnailDirectory.TAG_THUMBNAIL_IMAGE_WIDTH));
            }
            if(directory.containsTag(ExifThumbnailDirectory.TAG_THUMBNAIL_IMAGE_HEIGHT)) {
                metadata.set(Metadata.IMAGE_LENGTH, directory.getDescription(ExifThumbnailDirectory.TAG_THUMBNAIL_IMAGE_HEIGHT));
            }
        }
        /**
         * Maps exif dates to metadata fields.
         */
        public void handleDateTags(Directory directory, Metadata metadata)
                throws MetadataException {
            // Date/Time Original overrides value from ExifDirectory.TAG_DATETIME
            Date original = null;
            if (directory.containsTag(ExifSubIFDDirectory.TAG_DATETIME_ORIGINAL)) {
                original = directory.getDate(ExifSubIFDDirectory.TAG_DATETIME_ORIGINAL);
                // Unless we have GPS time we don't know the time zone so date must be set
                // as ISO 8601 datetime without timezone suffix (no Z or +/-)
                if (original != null) {
		    String datetimeNoTimeZone = DATE_UNSPECIFIED_TZ.get().format(original); // Same time zone as Metadata Extractor uses
                    metadata.set(TikaCoreProperties.CREATED, datetimeNoTimeZone);
                    metadata.set(Metadata.ORIGINAL_DATE, datetimeNoTimeZone);
                }
            }
            if (directory.containsTag(ExifIFD0Directory.TAG_DATETIME)) {
                Date datetime = directory.getDate(ExifIFD0Directory.TAG_DATETIME);
                if (datetime != null) {
		    String datetimeNoTimeZone = DATE_UNSPECIFIED_TZ.get().format(datetime);
                    metadata.set(TikaCoreProperties.MODIFIED, datetimeNoTimeZone);
                    // If Date/Time Original does not exist this might be creation date
                    if (metadata.get(TikaCoreProperties.CREATED) == null) {
                        metadata.set(TikaCoreProperties.CREATED, datetimeNoTimeZone);
                    }
                }
            }
        }
    }
    
    /**
     * Reads image comments, originally TIKA-472.
     * Metadata Extractor does not read XMP so we need to use the values from Iptc or EXIF
     */
    static class IptcHandler implements DirectoryHandler {
        public boolean supports(Class<? extends Directory> directoryType) {
            return directoryType == IptcDirectory.class;
        }
        public void handle(Directory directory, Metadata metadata)
                throws MetadataException {
            if (directory.containsTag(IptcDirectory.TAG_KEYWORDS)) {
                String[] keywords = directory.getStringArray(IptcDirectory.TAG_KEYWORDS);
                for (String k : keywords) {
                    metadata.add(TikaCoreProperties.KEYWORDS, k);
                }
            }
            if (directory.containsTag(IptcDirectory.TAG_HEADLINE)) {
                metadata.set(TikaCoreProperties.TITLE, directory.getString(IptcDirectory.TAG_HEADLINE));
            } else if (directory.containsTag(IptcDirectory.TAG_OBJECT_NAME)) {
                metadata.set(TikaCoreProperties.TITLE, directory.getString(IptcDirectory.TAG_OBJECT_NAME));
            }
            if (directory.containsTag(IptcDirectory.TAG_BY_LINE)) {
                metadata.set(TikaCoreProperties.CREATOR, directory.getString(IptcDirectory.TAG_BY_LINE));
                metadata.set(IPTC.CREATOR, directory.getString(IptcDirectory.TAG_BY_LINE));
            }
            if (directory.containsTag(IptcDirectory.TAG_CAPTION)) {
                metadata.set(TikaCoreProperties.DESCRIPTION,
                        // Looks like metadata extractor returns IPTC newlines as a single carriage return,
                        // but the exiv2 command does not so we change to line feed here because that is less surprising to users                        
                        directory.getString(IptcDirectory.TAG_CAPTION).replaceAll("\r\n?", "\n"));
            }
        }
    }

    /**
     * Maps EXIF Geo Tags onto the Tika Geo metadata namespace.
     */
    static class GeotagHandler implements DirectoryHandler {
        public boolean supports(Class<? extends Directory> directoryType) {
            return directoryType == GpsDirectory.class;
        }
        public void handle(Directory directory, Metadata metadata) throws MetadataException {
            GeoLocation geoLocation = ((GpsDirectory) directory).getGeoLocation();
            if (geoLocation != null) {
                DecimalFormat geoDecimalFormat = new DecimalFormat(GEO_DECIMAL_FORMAT_STRING,
                        new DecimalFormatSymbols(Locale.ENGLISH));
                metadata.set(TikaCoreProperties.LATITUDE, geoDecimalFormat.format(geoLocation.getLatitude()));
                metadata.set(TikaCoreProperties.LONGITUDE, geoDecimalFormat.format(geoLocation.getLongitude()));
            }
        }
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/image/ImageParser.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.image;

import java.io.IOException;
import java.io.InputStream;
import java.util.Arrays;
import java.util.Collections;
import java.util.HashSet;
import java.util.Iterator;
import java.util.Set;

import javax.imageio.IIOException;
import javax.imageio.ImageIO;
import javax.imageio.ImageReader;
import javax.imageio.metadata.IIOMetadata;
import javax.imageio.stream.ImageInputStream;

import org.apache.tika.exception.TikaException;
import org.apache.tika.io.CloseShieldInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.Property;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.XHTMLContentHandler;
import org.w3c.dom.NamedNodeMap;
import org.w3c.dom.Node;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

public class ImageParser extends AbstractParser {

    /** Serial version UID */
    private static final long serialVersionUID = 7852529269245520335L;

    private static final MediaType CANONICAL_BMP_TYPE = MediaType.image("x-ms-bmp");
    private static final MediaType JAVA_BMP_TYPE = MediaType.image("bmp");
    
    private static final Set<MediaType> SUPPORTED_TYPES =
        Collections.unmodifiableSet(new HashSet<MediaType>(Arrays.asList(
                CANONICAL_BMP_TYPE,
                JAVA_BMP_TYPE,
                MediaType.image("gif"),
                MediaType.image("png"),
                MediaType.image("vnd.wap.wbmp"),
                MediaType.image("x-icon"),
                MediaType.image("x-xcf"))));

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }

    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        String type = metadata.get(Metadata.CONTENT_TYPE);
        if (type != null) {
            // Java has a different idea of the BMP mime type to
            //  what the canonical one is, fix this up.
            if (CANONICAL_BMP_TYPE.toString().equals(type)) {
               type = JAVA_BMP_TYPE.toString();
            }
           
            try {
                Iterator<ImageReader> iterator =
                    ImageIO.getImageReadersByMIMEType(type);
                if (iterator.hasNext()) {
                    ImageReader reader = iterator.next();
                    try {
                        ImageInputStream imageStream = ImageIO.createImageInputStream(
                                new CloseShieldInputStream(stream));
                        try {
                            reader.setInput(imageStream);
                            
                            metadata.set(Metadata.IMAGE_WIDTH, Integer.toString(reader.getWidth(0)));
                            metadata.set(Metadata.IMAGE_LENGTH, Integer.toString(reader.getHeight(0)));
                            metadata.set("height", Integer.toString(reader.getHeight(0)));
                            metadata.set("width", Integer.toString(reader.getWidth(0)));

                            loadMetadata(reader.getImageMetadata(0), metadata);
                        } finally {
                            imageStream.close();
                        }
                    } finally {
                        reader.dispose();
                    }
                }
                
                // Translate certain Metadata tags from the ImageIO
                //  specific namespace into the general Tika one
                setIfPresent(metadata, "CommentExtensions CommentExtension", TikaCoreProperties.COMMENTS);
                setIfPresent(metadata, "markerSequence com", TikaCoreProperties.COMMENTS);
                setIfPresent(metadata, "Data BitsPerSample", Metadata.BITS_PER_SAMPLE);
            } catch (IIOException e) {
                // TIKA-619: There is a known bug in the Sun API when dealing with GIF images
                //  which Tika will just ignore.
                if (!(e.getMessage().equals("Unexpected block type 0!") && type.equals("image/gif"))) {
                    throw new TikaException(type + " parse error", e);
                }
            }
        }

        XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);
        xhtml.startDocument();
        xhtml.endDocument();
    }


    private static void setIfPresent(Metadata metadata, String imageIOkey, String tikaKey) {
	if(metadata.get(imageIOkey) != null) {
	    metadata.set(tikaKey, metadata.get(imageIOkey));
	}
    }
    private static void setIfPresent(Metadata metadata, String imageIOkey, Property tikaProp) {
	if(metadata.get(imageIOkey) != null) {
	    String v = metadata.get(imageIOkey);
	    if(v.endsWith(" ")) {
		v = v.substring(0, v.lastIndexOf(' '));
	    }
	    metadata.set(tikaProp, v);
	}
    }

    private static void loadMetadata(IIOMetadata imageMetadata, Metadata metadata) {
        String[] names = imageMetadata.getMetadataFormatNames();
        if (names == null) {
            return;
        }
        for (String name : names) {
            loadNode(metadata, imageMetadata.getAsTree(name), "", false);
        }
    }

    private static void loadNode(
            Metadata metadata, Node node, String parents,
            boolean addThisNodeName) {
        if (addThisNodeName) {
            if (parents.length() > 0) {
                parents += " ";
            }
            parents += node.getNodeName();
        }
        NamedNodeMap map = node.getAttributes();
        if (map != null) {

            int length = map.getLength();
            if (length == 1) {
                metadata.add(parents, normalize(map.item(0).getNodeValue()));
            } else if (length > 1) {
                StringBuilder value = new StringBuilder();
                for (int i = 0; i < length; i++) {
                    if (i > 0) {
                        value.append(", ");
                    }
                    Node attr = map.item(i);
                    value.append(attr.getNodeName());
                    value.append("=");
                    value.append(normalize(attr.getNodeValue()));
                }
                metadata.add(parents, value.toString());
            }
        }

        Node child = node.getFirstChild();
        while (child != null) {
            // print children recursively
            loadNode(metadata, child, parents, true);
            child = child.getNextSibling();
        }
    }

    private static String normalize(String value) {
        if (value != null) {
            value = value.trim();
        } else {
            value = "";
        }
        if (Boolean.TRUE.toString().equalsIgnoreCase(value)) {
            return Boolean.TRUE.toString();
        } else if (Boolean.FALSE.toString().equalsIgnoreCase(value)) {
            return Boolean.FALSE.toString();
        }
        return value;
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/image/MetadataFields.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.image;

import java.lang.reflect.Field;
import java.lang.reflect.Modifier;
import java.util.HashSet;

import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.Property;
import org.apache.tika.metadata.TikaCoreProperties;

/**
 * Knowns about all declared {@link Metadata} fields.
 * Didn't find this functionality anywhere so it was added for
 * ImageMetadataExtractor, but it can be generalized.
 */
public abstract class MetadataFields {
    
    private static HashSet<String> known;
    
    private static void setKnownForClass(Class<?> clazz) {
        Field[] fields = clazz.getFields();
        for (Field f : fields) {
            int mod = f.getModifiers();
            if (Modifier.isPublic(mod) && Modifier.isStatic(mod) && Modifier.isFinal(mod)) {
                Class<?> c = f.getType();
                if (String.class.equals(c)) {
                    try {
                        String p = (String) f.get(null);
                        if (p != null) {
                            known.add(p);
                        }
                    } catch (IllegalArgumentException e) {
                        e.printStackTrace();
                    } catch (IllegalAccessException e) {
                        e.printStackTrace();
                    }
                }
                if (Property.class.isAssignableFrom(c)) {
                    try {
                        Property p = (Property) f.get(null);
                        if (p != null) {
                            known.add(p.getName());
                        }
                    } catch (IllegalArgumentException e) {
                        e.printStackTrace();
                    } catch (IllegalAccessException e) {
                        e.printStackTrace();
                    }
                }
            }
        }
    }
    
    static {
        known = new HashSet<String>();
        setKnownForClass(TikaCoreProperties.class);
        setKnownForClass(Metadata.class);
    }
    
    public static boolean isMetadataField(String name) {
        return known.contains(name);
    }
    
    public static boolean isMetadataField(Property property) {
        return known.contains(property.getName());
    }
    
}
"
tika-parsers/src/main/java/org/apache/tika/parser/image/PSDParser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.image;

import java.io.IOException;
import java.io.InputStream;
import java.io.UnsupportedEncodingException;
import java.util.Arrays;
import java.util.Collections;
import java.util.HashSet;
import java.util.Set;

import org.apache.poi.util.IOUtils;
import org.apache.tika.exception.TikaException;
import org.apache.tika.io.EndianUtils;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.Photoshop;
import org.apache.tika.metadata.TIFF;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * Parser for the Adobe Photoshop PSD File Format.
 * 
 * Documentation on the file format is available from
 * http://www.adobe.com/devnet-apps/photoshop/fileformatashtml/PhotoshopFileFormats.htm
 */
public class PSDParser extends AbstractParser {

    /** Serial version UID */
    private static final long serialVersionUID = 883387734607994914L;

    private static final Set<MediaType> SUPPORTED_TYPES =
        Collections.unmodifiableSet(new HashSet<MediaType>(Arrays.asList(
                MediaType.image("vnd.adobe.photoshop"))));

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }

    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        // Check for the magic header signature
        byte[] signature = new byte[4];
        IOUtils.readFully(stream, signature);
        if(signature[0] == (byte)'8' && signature[1] == (byte)'B' &&
           signature[2] == (byte)'P' && signature[3] == (byte)'S') {
           // Good, signature found
        } else {
           throw new TikaException("PSD/PSB magic signature invalid");
        }
        
        // Check the version
        int version = EndianUtils.readUShortBE(stream);
        if(version == 1 || version == 2) {
           // Good, we support these two
        } else {
           throw new TikaException("Invalid PSD/PSB version " + version);
        }
        
        // Skip the reserved block
        IOUtils.readFully(stream, new byte[6]);
        
        // Number of channels in the image
        int numChannels = EndianUtils.readUShortBE(stream);
        // TODO Identify a suitable metadata key for this

        // Width and Height
        int height = EndianUtils.readIntBE(stream);
        int width = EndianUtils.readIntBE(stream);
        metadata.set(TIFF.IMAGE_LENGTH, height);
        metadata.set(TIFF.IMAGE_WIDTH, width);
        
        // Depth (bits per channel)
        int depth = EndianUtils.readUShortBE(stream);
        metadata.set(TIFF.BITS_PER_SAMPLE, Integer.toString(depth));
        
        // Colour mode, eg Bitmap or RGB
        int colorMode = EndianUtils.readUShortBE(stream);
        metadata.set(Photoshop.COLOR_MODE, Photoshop._COLOR_MODE_CHOICES_INDEXED[colorMode]);
        
        // Next is the Color Mode section
        // We don't care about this bit
        long colorModeSectionSize = EndianUtils.readIntBE(stream);
        stream.skip(colorModeSectionSize);

        // Next is the Image Resources section
        // Check for certain interesting keys here
        long imageResourcesSectionSize = EndianUtils.readIntBE(stream);
        long read = 0;
        while(read < imageResourcesSectionSize) {
           ResourceBlock rb = new ResourceBlock(stream);
           read += rb.totalLength;
           
           // Is it one we can do something useful with?
           if(rb.id == ResourceBlock.ID_CAPTION) {
              metadata.add(TikaCoreProperties.DESCRIPTION, rb.getDataAsString()); 
           } else if(rb.id == ResourceBlock.ID_EXIF_1) {
              // TODO Parse the EXIF info via ImageMetadataExtractor
           } else if(rb.id == ResourceBlock.ID_EXIF_3) {
              // TODO Parse the EXIF info via ImageMetadataExtractor
           } else if(rb.id == ResourceBlock.ID_XMP) {
              // TODO Parse the XMP info via ImageMetadataExtractor
           }
        }
        
        // Next is the Layer and Mask Info
        // Finally we have Image Data
        // We can't do anything with these parts
        
        // We don't have any helpful text, sorry...
        XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);
        xhtml.startDocument();
        xhtml.endDocument();
    }
    
    private static class ResourceBlock {
       private static final long SIGNATURE = 0x3842494d; // 8BIM
       private static final int ID_CAPTION = 0x03F0;
       private static final int ID_URL = 0x040B;
       private static final int ID_EXIF_1 = 0x0422;
       private static final int ID_EXIF_3 = 0x0423;
       private static final int ID_XMP = 0x0424;
       
       private int id;
       private String name;
       private byte[] data;
       private int totalLength;
       private ResourceBlock(InputStream stream) throws IOException, TikaException {
          // Verify the signature
          long sig = EndianUtils.readIntBE(stream);
          if(sig != SIGNATURE) {
             throw new TikaException("Invalid Image Resource Block Signature Found, got " +
                   sig + " 0x" + Long.toHexString(sig) + " but the spec defines " + SIGNATURE);
          }
          
          // Read the block
          id = EndianUtils.readUShortBE(stream);
          
          StringBuffer nameB = new StringBuffer();
          int nameLen = 0;
          while(true) {
             int v = stream.read();
             nameLen++;
             
             if(v == 0) {
                // The name length is padded to be even
                if(nameLen % 2 == 1) {
                   stream.read();
                   nameLen++;
                }
                break;
             } else {
                nameB.append((char)v);
             }
             name = nameB.toString();
          }
          
          int dataLen = EndianUtils.readIntBE(stream);
          if(dataLen %2 == 1) {
              // Data Length is even padded
              dataLen = dataLen + 1;
          }
          totalLength = 4 + 2 + nameLen + 4 + dataLen;
          
          data = new byte[dataLen];
          IOUtils.readFully(stream, data);
       }
       
       private String getDataAsString() {
          // Will be null padded
          try {
             return new String(data, 0, data.length-1, "ASCII");
          } catch(UnsupportedEncodingException e) {
             throw new RuntimeException("Something is very broken in your JVM!");
          }
       }
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/image/TiffParser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.image;

import java.io.IOException;
import java.io.InputStream;
import java.util.Collections;
import java.util.Set;

import org.apache.tika.exception.TikaException;
import org.apache.tika.io.TemporaryResources;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.image.xmp.JempboxExtractor;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

public class TiffParser extends AbstractParser {

    /** Serial version UID */
    private static final long serialVersionUID = -3941143576535464926L;

    private static final Set<MediaType> SUPPORTED_TYPES =
        Collections.singleton(MediaType.image("tiff"));

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }

    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        TemporaryResources tmp = new TemporaryResources();
        try {
            TikaInputStream tis = TikaInputStream.get(stream, tmp);
            new ImageMetadataExtractor(metadata).parseTiff(tis.getFile());
            new JempboxExtractor(metadata).parse(tis);
        } finally {
            tmp.dispose();
        }

        XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);
        xhtml.startDocument();
        xhtml.endDocument();
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/image/xmp/JempboxExtractor.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.image.xmp;

import java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.io.Reader;
import java.util.Iterator;
import java.util.List;

import org.apache.jempbox.xmp.XMPMetadata;
import org.apache.jempbox.xmp.XMPSchemaDublinCore;
import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.TikaCoreProperties;
import org.xml.sax.InputSource;

public class JempboxExtractor {

    private XMPPacketScanner scanner = new XMPPacketScanner();
    
    private Metadata metadata;
    
    // The XMP spec says it must be unicode, but for most file formats it specifies "must be encoded in UTF-8"
    private static final String DEFAULT_XMP_CHARSET = "UTF-8";

    public JempboxExtractor(Metadata metadata) {
        this.metadata = metadata;
    }

    public void parse(InputStream file) throws IOException, TikaException {
        ByteArrayOutputStream xmpraw = new ByteArrayOutputStream();
        if (!scanner.parse(file, xmpraw)) {
            return;
        }

        Reader decoded = new InputStreamReader(
                new ByteArrayInputStream(xmpraw.toByteArray()),
                DEFAULT_XMP_CHARSET);
        try {
            XMPMetadata xmp = XMPMetadata.load(new InputSource(decoded));
            XMPSchemaDublinCore dc = xmp.getDublinCoreSchema();
            if (dc != null) {
                if (dc.getTitle() != null) {
                    metadata.set(TikaCoreProperties.TITLE, dc.getTitle());
                }
                if (dc.getDescription() != null) {
                    metadata.set(TikaCoreProperties.DESCRIPTION, dc.getDescription());
                }
                if (dc.getCreators() != null && dc.getCreators().size() > 0) {
                    metadata.set(TikaCoreProperties.CREATOR, joinCreators(dc.getCreators()));
                }
                if (dc.getSubjects() != null && dc.getSubjects().size() > 0) {
                    for (String keyword : dc.getSubjects()) {
                        metadata.add(TikaCoreProperties.KEYWORDS, keyword);
                    }
                    // TODO should we set KEYWORDS too?
                    // All tested photo managers set the same in Iptc.Application2.Keywords and Xmp.dc.subject
                }
            }
        } catch (IOException e) {
            // Could not parse embedded XMP metadata. That's not a serious
            // problem, so we'll just ignore the issue for now.
            // TODO: Make error handling like this configurable.
        }
    }

    protected String joinCreators(List<String> creators) {
        if (creators == null || creators.size() == 0) {
            return "";
        }
        if (creators.size() == 1) {
            return creators.get(0);
        }
        StringBuffer c = new StringBuffer();
        for (String s : creators) {
            c.append(", ").append(s);
        }
        return c.substring(2);
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/image/xmp/XMPPacketScanner.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/* $Id: XMPPacketParser.java 750418 2009-03-05 11:03:54Z vhennebert $ */

package org.apache.tika.parser.image.xmp;

import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.io.UnsupportedEncodingException;

/**
 * This class is a parser for XMP packets. By default, it tries to locate the first XMP packet
 * it finds and parses it.
 * <p>
 * Important: Before you use this class to look for an XMP packet in some random file, please read
 * the chapter on "Scanning Files for XMP Packets" in the XMP specification!
 * <p>
 * Thic class was branched from http://xmlgraphics.apache.org/ XMPPacketParser.
 * See also org.semanticdesktop.aperture.extractor.xmp.XMPExtractor, a variant.
 */
public class XMPPacketScanner {

    private static final byte[] PACKET_HEADER;
    private static final byte[] PACKET_HEADER_END;
    private static final byte[] PACKET_TRAILER;

    static {
        try {
            PACKET_HEADER = "<?xpacket begin=".getBytes("US-ASCII");
            PACKET_HEADER_END = "?>".getBytes("US-ASCII");
            PACKET_TRAILER = "<?xpacket".getBytes("US-ASCII");
        } catch (UnsupportedEncodingException e) {
            throw new RuntimeException("Incompatible JVM! US-ASCII encoding not supported.");
        }
    }

    /**
     * Locates an XMP packet in a stream, parses it and returns the XMP metadata. If no
     * XMP packet is found until the stream ends, null is returned. Note: This method
     * only finds the first XMP packet in a stream. And it cannot determine whether it
     * has found the right XMP packet if there are multiple packets.
     * 
     * Does <em>not</em> close the stream.
     * If XMP block was found reading can continue below the block.
     * 
     * @param in the InputStream to search
     * @param xmlOut to write the XMP packet to
     * @return true if XMP packet is found, false otherwise
     * @throws IOException if an I/O error occurs
     * @throws TransformerException if an error occurs while parsing the XMP packet
     */
    public boolean parse(InputStream in, OutputStream xmlOut) throws IOException {
        if (!in.markSupported()) {
            in = new java.io.BufferedInputStream(in);
        }
        boolean foundXMP = skipAfter(in, PACKET_HEADER);
        if (!foundXMP) {
            return false;
        }
        //TODO Inspect "begin" attribute!
        if (!skipAfter(in, PACKET_HEADER_END)) {
            throw new IOException("Invalid XMP packet header!");
        }
        //TODO Do with TeeInputStream when Commons IO 1.4 is available
        if (!skipAfter(in, PACKET_TRAILER, xmlOut)) {
            throw new IOException("XMP packet not properly terminated!");
        }
        return true;
    }

    private static boolean skipAfter(InputStream in, byte[] match) throws IOException {
        return skipAfter(in, match, null);
    }

    private static boolean skipAfter(InputStream in, byte[] match, OutputStream out)
            throws IOException {
        int found = 0;
        int len = match.length;
        int b;
        while ((b = in.read()) >= 0) {
            if (b == match[found]) {
                found++;
                if (found == len) {
                    return true;
                }
            } else {
                if (out != null) {
                    if (found > 0) {
                        out.write(match, 0, found);
                    }
                    out.write(b);
                }
                found = 0;
            }
        }
        return false;
    }

}

"
tika-parsers/src/main/java/org/apache/tika/parser/internal/Activator.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.internal;

import java.util.Properties;

import org.apache.tika.detect.DefaultDetector;
import org.apache.tika.detect.Detector;
import org.apache.tika.fork.ForkParser;
import org.apache.tika.parser.DefaultParser;
import org.apache.tika.parser.Parser;
import org.osgi.framework.BundleActivator;
import org.osgi.framework.BundleContext;
import org.osgi.framework.ServiceRegistration;

public class Activator implements BundleActivator {

    private ServiceRegistration detectorService;

    private ServiceRegistration parserService;

    private ServiceRegistration forkParserService;

    public void start(BundleContext context) throws Exception {
        detectorService = context.registerService(
                Detector.class.getName(),
                new DefaultDetector(Activator.class.getClassLoader()),
                new Properties());
        Parser parser = new DefaultParser(Activator.class.getClassLoader());
        parserService = context.registerService(
                Parser.class.getName(),
                parser,
                new Properties());
        forkParserService = context.registerService(
                ForkParser.class.getName(),
                new ForkParser(Activator.class.getClassLoader(), parser),
                new Properties());
    }

    public void stop(BundleContext context) throws Exception {
        parserService.unregister();
        detectorService.unregister();
        forkParserService.unregister();
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/iptc/IptcAnpaParser.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.iptc;

import java.io.IOException;
import java.io.InputStream;
import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.Collections;
import java.util.Date;
import java.util.HashMap;
import java.util.Locale;
import java.util.Set;
import java.util.TimeZone;

import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.Parser;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * Parser for IPTC ANPA New Wire Feeds
 */
public class IptcAnpaParser implements Parser {
    /** Serial version UID */
    private static final long serialVersionUID = -6062820170212879115L;

    private static final MediaType TYPE =
        MediaType.text("vnd.iptc.anpa");

    private static final Set<MediaType> SUPPORTED_TYPES =
        Collections.singleton(TYPE);

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }

    public void parse(
           InputStream stream, ContentHandler handler,
           Metadata metadata, ParseContext context)
           throws IOException, SAXException, TikaException {

        HashMap<String,String> properties = this.loadProperties(stream);
        this.setMetadata(metadata, properties);

        XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);
        xhtml.startDocument();
        // TODO: put body content here
        xhtml.startElement("p");
        String body = clean(properties.get("body"));
        if (body != null)
           xhtml.characters(body);
        xhtml.endElement("p");
        xhtml.endDocument();
    }

    /**
     * @deprecated This method will be removed in Apache Tika 1.0.
     */
    public void parse(
            InputStream stream, ContentHandler handler, Metadata metadata)
            throws IOException, SAXException, TikaException {
        parse(stream, handler, metadata, new ParseContext());
    }


   private int FMT_ANPA_1312    = 0x00;   // "NAA 89-3 (ANPA 1312)"
   private int FMT_ANPA_UPI     = 0x01;   // "United Press International ANPA 1312 variant"
   private int FMT_ANPA_UPI_DL  = 0x02;   // "United Press International Down-Load Message"
   private int FMT_IPTC_7901    = 0x03;   // "IPTC7901 Recommended Message Format"
   private int FMT_IPTC_PHOTO   = 0x04;   // "IPTC-NAA Digital Newsphoto Parameter Record"
   private int FMT_IPTC_CHAR    = 0x05;   // "IPTC Unstructured Character Oriented File Format (UCOFF)"
   private int FMT_NITF         = 0x06;   // "News Industry Text Format (NITF)"
   private int FMT_NITF_TT      = 0x07;   // "Tidningarnas Telegrambyra NITF version (TTNITF DTD)"
   private int FMT_NITF_RB      = 0x08;   // "Ritzaus Bureau NITF version (RBNITF DTD)"
   private int FMT_IPTC_AP      = 0x09;   // "Associated Press news wire format"
   private int FMT_IPTC_BLM     = 0x0A;   // "Bloomberg News news wire format"
   private int FMT_IPTC_NYT     = 0x0B;   // "New York Times news wire format"
   private int FMT_IPTC_RTR     = 0x0C;   // "Reuters news wire format"

   private int FORMAT = FMT_ANPA_1312;    // assume the default format to be ANPA-1312

   private final static char SOH = 0x01;    // start of header (ctrl-a)
   private final static char STX = 0x02;    // start of text (ctrl-b)
   private final static char ETX = 0x03;    // end of text (ctrl-c)
   private final static char EOT = 0x04;    // the tab character (ctrl-d)
   private final static char SYN = 0x16;    // synchronous idle (ctrl-v)

   private final static char BS = 0x08;    // the backspace character (used for diacriticals)
   private final static char TB = 0x09;    // the tab character
   private final static char LF = 0x0A;    // line feed
   private final static char FF = 0x0C;    // form feed
   private final static char CR = 0x0D;    // carriage return
   private final static char XQ = 0x11;    // device control (ctrl-q)
   private final static char XS = 0x13;    // device control (ctrl-s)
   private final static char FS = 0x1F;    // a field delimiter

   private final static char HY = 0x2D;    // hyphen
   private final static char SP = 0x20;    // the blank space
   private final static char LT = 0x3C;    // less than
   private final static char EQ = 0x3D;    // less than
   private final static char CT = 0x5E;    // carat

   private final static char SL = 0x91;    // single-quote left
   private final static char SR = 0x92;    // single-quote right
   private final static char DL = 0x93;    // double-quote left
   private final static char DR = 0x94;    // double-quote right


   /**
    * scan the news messsage and store the metadata and data into a map
    */
   private HashMap<String,String> loadProperties(InputStream is) {
      
      HashMap<String,String> properties = new HashMap<String,String>();

      FORMAT = this.scanFormat(is);

      byte[] residual = this.getSection(is,"residual");

      byte[] header = this.getSection(is,"header");
      parseHeader(header, properties);

      byte[] body = this.getSection(is,"body");
      parseBody(body, properties);

      byte[] footer = this.getSection(is,"footer");
      parseFooter(footer, properties);
       
      return (properties);
   }


   private int scanFormat(InputStream is) {
      int format    = this.FORMAT;
      int  maxsize  = 524288;     //  512K

      byte[] buf = new byte[maxsize];
      try {
         if (is.markSupported()) {
            is.mark(maxsize);
         }
         int msgsize = is.read(buf);                // read in at least the full data

         String message = (new String(buf, "UTF-8")).toLowerCase(Locale.ROOT);
         // these are not if-then-else, because we want to go from most common
         // and fall through to least.  this is imperfect, as these tags could
         // show up in other agency stories, but i can't find a spec or any
         // explicit codes to identify the wire source in the message itself

         if (message.contains("ap-wf")) {
            format = this.FMT_IPTC_AP;
         }
         if (message.contains("reuters")) {
            format = this.FMT_IPTC_RTR;
         }
         if (message.contains("new york times")) {
            format = this.FMT_IPTC_NYT;
         }
         if (message.contains("bloomberg news")) {
            format = this.FMT_IPTC_BLM;
         }
      }
      catch (IOException eio) {
         // we are in an unstable state
      }

      try {
         if (is.markSupported()) {
            is.reset();
         }
      }
      catch (IOException eio) {
         // we are in an unstable state
      }
      return (format);
   }


   private void setFormat(int format) {
      this.FORMAT = format;
   }


   private String getFormatName() {
      
      String name = "";
      
      if (FORMAT == this.FMT_IPTC_AP) {
         name = "Associated Press";
      }
      
      else if(FORMAT == this.FMT_IPTC_BLM) {
         name = "Bloomberg";
      }

      else if(FORMAT == this.FMT_IPTC_NYT) {
         name = "New York Times";
      }

      else if(FORMAT == this.FMT_IPTC_RTR) {
         name = "Reuters";
      }

      return (name);
   }


   private byte[] getSection(InputStream is, String name) {

      byte[] value = new byte[0];

      if (name.equals("residual")) {
         // the header shouldn't be more than 1k, but just being generous here
         int  maxsize  = 8192;     //  8K
         byte bstart   = SYN;     // check for SYN [0x16 : ctrl-v] (may have leftover residue from preceding message)
         byte bfinish  = SOH;     // check for SOH [0x01 : ctrl-a] (typically follows a pair of SYN [0x16 : ctrl-v])
         value = getSection(is, maxsize, bstart, bfinish, true);
      }

      else if(name.equals("header")) {
         // the header shouldn't be more than 1k, but just being generous here
         int  maxsize  = 8192;     //  8K
         byte bstart   = SOH;     // check for SOH [0x01 : ctrl-a] (typically follows a pair of SYN [0x16 : ctrl-v])
         byte bfinish  = STX;     // check for STX [0x02 : ctrl-b] (marks end of header, beginning of message)
         value = getSection(is, maxsize, bstart, bfinish, true);
      }

      else if (name.equals("body")) {
         // the message shouldn't be more than 16k (?), leaving plenty of space
         int  maxsize  = 524288;     //  512K
         byte bstart   = STX;     // check for STX [0x02 : ctrl-b] (marks end of header, beginning of message)
         byte bfinish  = ETX;     // check for ETX [0x03 : ctrl-c] (marks end of message, beginning of footer)
         value = getSection(is, maxsize, bstart, bfinish, true);
      }

      else if (name.equals("footer")) {
         // the footer shouldn't be more than 1k , leaving plenty of space
         int maxsize   = 8192;     //  8K
         byte bstart   = ETX;     // check for ETX [0x03 : ctrl-c] (marks end of message, beginning of footer)
         byte bfinish  = EOT;     // check for EOT [0x04 : ctrl-d] (marks end of transmission)
         value = getSection(is, maxsize, bstart, bfinish, true);
      }

      return (value);
   }


   private byte[] getSection(InputStream is, int maxsize, byte bstart, byte bfinish, boolean ifincomplete) {
      byte[] value  = new byte[0];

      try {
         boolean started = false;                   // check if we have found the start flag
         boolean finished = false;                  // check if we have found the finish flag
         int read = 0;                              // the number of bytes we read
         int start = 0;                             // the position after the start flag

         // TODO: this only pulls back 8K of data on a read, regardless of buffer size
         //       more nefariously, it caps at a total 8K, through all sections
         int streammax = is.available();
         maxsize = Math.min(maxsize, streammax);

         is.mark(maxsize);
         byte[] buf = new byte[maxsize];
         int totsize = 0;
         int remainder = maxsize - totsize;
         while (remainder > 0) {
            int msgsize = is.read(buf, maxsize-remainder, maxsize);    // read in at least the full data
            if (msgsize == -1) {
               remainder = msgsize = 0;
            }
            remainder -= msgsize;
            totsize   += msgsize;
         }

         // scan through the provided input stream
         for (read=0; read < totsize; read++) {
            byte b = buf[read];

            if (!started) {
               started = (b == bstart);
               start = read + 1;
               continue;
            }

            if (finished = (b == bfinish)) {
/*
               is.reset();
               long skipped = is.skip((long)read);
               if (skipped != read) {
                  // we are in an unstable state
               }
               is.mark(1);
 */
               break;
            }

            // load from the stream until we run out of characters, or hit the termination byte
            continue;
         }

         // move the input stream back to where it was initially
         is.reset();

         if (finished) {
            // now, we want to reset the stream to be sitting right on top of the finish marker
            is.skip(read);
            value = new byte[read-start];
            System.arraycopy(buf, start, value, 0, read-start);
         }
         else {
            if (ifincomplete && started) {
               // the caller wants anything that was read, and we finished the stream or buffer
               value = new byte[read-start];
               System.arraycopy(buf, start, value, 0, read-start);
            }
         }
      }
      catch (IOException eio) {
         // something invalid occurred, return an empty string
      }

      return (value);
   }


   private boolean parseHeader(byte[] value, HashMap<String,String> properties) {
      boolean added = false;

      String env_serviceid = "";
      String env_category = "";
      String env_urgency = "";
      String hdr_edcode = "";
      String hdr_subject = "";
      String hdr_date = "";
      String hdr_time = "";

      int read = 0;

      while (read < value.length) {

         // pull apart the envelope, getting the service id  (....\x1f)
         while (read < value.length) {
            byte val_next = value[read++];
            if (val_next != FS) {
               env_serviceid += (char)(val_next & 0xff);  // convert the byte to an unsigned int
            }
            else {
               break;
            }
         }

         // pull apart the envelope, getting the category  (....\x13\x11)
         while (read < value.length) {
            byte val_next = value[read++];
            if (val_next != XS) {   // the end of the envelope is marked (\x13)
               env_category += (char)(val_next & 0xff);  // convert the byte to an unsigned int
            }
            else {
               val_next = value[read];  // get the remaining byte (\x11)
               if (val_next == XQ) {
                  read++;
               }
               break;
            }
         }

         // pull apart the envelope, getting the subject heading
         while (read < value.length) {
            boolean subject = true;
            byte val_next = value[read++];
            while ((subject) && (val_next != SP) && (val_next != 0x00)) {  // ignore the envelope subject
               hdr_subject += (char)(val_next & 0xff);  // convert the byte to an unsigned int
               val_next =  (read < value.length) ? value[read++] : 0x00;
               while (val_next == SP) {  // consume all the spaces
                  subject = false;
                  val_next =  (read < value.length) ? value[read++] : 0x00;
                  if (val_next != SP) {
                     --read;  // otherwise we eat into the next section
                  }
               }
            }
            if (!subject) {
               break;
            }
         }

         // pull apart the envelope, getting the date and time
         while (read < value.length) {
            byte val_next = value[read++];
            if (hdr_date.length() == 0) {
               while (((val_next >= (byte)0x30) && (val_next <= (byte)0x39))  // consume all numerics and hyphens
                  ||   (val_next == HY)) {
                  hdr_date += (char)(val_next & 0xff);  // convert the byte to an unsigned int
                  val_next =  (read < value.length) ? value[read++] : 0x00;
               }
            }
            else if (val_next == SP) {
               while (val_next == SP) {  // consume all the spaces
                  val_next =  (read < value.length) ? value[read++] : 0x00;
               }
               continue;
            }
            else {
               while (((val_next >= (byte)0x30) && (val_next <= (byte)0x39))  // consume all numerics and hyphens
                  ||   (val_next == HY)) {
                  hdr_time += (char)(val_next & 0xff);  // convert the byte to an unsigned int
                  val_next =  (read < value.length) ? value[read++] : 0x00;
               }
            }
         }
         break; // don't let this run back through and start thrashing metadata
      }

      // if we were saving any of these values, we would set the properties map here

      added = (env_serviceid.length() + env_category.length() + hdr_subject.length() + 
               hdr_date.length() + hdr_time.length()) > 0; 
      return added;
   }

   private boolean parseBody(byte[] value, HashMap<String,String> properties) {
      boolean added = false;

      String bdy_heading = "";
      String bdy_title = "";
      String bdy_source = "";
      String bdy_author = "";
      String bdy_body = "";

      int read = 0;
      boolean done = false;

      while (!done && (read < value.length)) {

         // pull apart the body, getting the heading (^....\x0d\x0a)
         while (read < value.length) {
            byte val_next = value[read++];
            if (val_next == CT) {      //  start of a new section , first is the heading
               val_next =  (read < value.length) ? value[read++] : 0x00;
               // AP, NYT, and Bloomberg end with < , Reuters with EOL
               while ((val_next != LT) && (val_next != CR) && (val_next != LF)) {   // less than delimiter (\x3c) and not EOL
                  bdy_heading += (char)(val_next & 0xff);  // convert the byte to an unsigned int
                  val_next =  (read < value.length) ? value[read++] : 0x00;
                  if (read > value.length) { break; }  // shouldn't ever hit this, but save a NPE
               }
               if (val_next == LT) {
                  // hit the delimiter, carry on
                  val_next =  (read < value.length) ? value[read++] : 0x00;
               }
               while (bdy_heading.length() > 0 && ((val_next == CR) || (val_next == LF))) {
                  val_next =  (read < value.length) ? value[read++] : 0x00;  // skip the new lines
                  if ((val_next != CR) && (val_next != LF)) {
                     --read;
                  }
               }
            }
            else {
               // this will only be hit on poorly-formed files

               // for reuters, the heading does not start with the ^, so we push one back into the stream
               if (FORMAT == this.FMT_IPTC_RTR) {
                  if (val_next != CT) {
                     // for any non-whitespace, we need to go back an additional step to non destroy the data
                     if ((val_next != SP) && (val_next != TB) && (val_next != CR) && (val_next != LF)) {
                        // if the very first byte is data, we have to shift the whole array, and stuff in a carat
                        if (read == 1) {
                           byte[] resize = new byte[value.length + 1];
                           System.arraycopy(value, 0, resize, 1, value.length);
                           value = resize;
                        }
                     }
                     value[--read] = CT;
                     continue;
                  }
               }
            }
            break;
         }

         // pull apart the body, getting the title (^....\x0d\x0a)
         while (read < value.length) {
            byte val_next = value[read++];
            if (val_next == CT) {      //  start of a new section , first is the heading
               val_next =  (read < value.length) ? value[read++] : 0x00;
               // AP, NYT, and Bloomberg end with < , Reuters with EOL
               while ((val_next != LT) && (val_next != CT) && (val_next != CR) && (val_next != LF)) {   // less than delimiter (\x3c), or carat (\x5e) and not EOL
                  bdy_title += (char)(val_next & 0xff);  // convert the byte to an unsigned int
                  val_next =  (read < value.length) ? value[read++] : 0x00;
                  if (read > value.length) { break; }  // shouldn't ever hit this, but save a NPE
               }

               if (val_next == CT) {      //  start of a new section , when first didn't finish cleanly
                   --read;
               }

               if (val_next == LT) {
                  // hit the delimiter, carry on
                  val_next =  (read < value.length) ? value[read++] : 0x00;
               }

               while (bdy_title.length() > 0 && ((val_next == CR) || (val_next == LF))) {
                  val_next =  (read < value.length) ? value[read++] : 0x00;  // skip the new lines
                  if ((val_next != CR) && (val_next != LF)) {
                     --read;
                  }
               }
            }
            else {
               // this will only be hit on poorly-formed files

               // for bloomberg, the title does not start with the ^, so we push one back into the stream
               if (FORMAT == this.FMT_IPTC_BLM) {
                  if (val_next == TB) {
                     value[--read] = CT;
                     continue;
                  }
               }

               // for reuters, the title does not start with the ^, so we push one back into the stream
               if (FORMAT == this.FMT_IPTC_RTR) {
                  if (val_next != CT) {
                     // for any non-whitespace, we need to go back an additional step to non destroy the data
                     if ((val_next != SP) && (val_next != TB) && (val_next != CR) && (val_next != LF)) {
                        --read;
                     }
                     value[--read] = CT;
                     continue;
                  }
               }
            }
            break;
         }


         // at this point, we have a variable number of metadata lines, with various orders
         // we scan the start of each line for the special character, and run to the end character
         // pull apart the body, getting the title (^....\x0d\x0a)
         boolean metastarted = false;
         String longline = "";
         String longkey = "";
         while (read < value.length) {
            byte val_next = value[read++];

            // eat up whitespace before committing to the next section
            if ((val_next == SP) || (val_next == TB) || (val_next == CR) || (val_next == LF)) {
               continue;
            }

            if (val_next == CT) {      //  start of a new section , could be authors, sources, etc
               val_next =  (read < value.length) ? value[read++] : 0x00;
               String tmp_line = "";
               while ((val_next != LT) && (val_next != CT) && (val_next != CR) && (val_next != LF) && (val_next != 0))  {
                  // less than delimiter (\x3c), maybe also badly formed with just new line
                  tmp_line += (char)(val_next & 0xff);  // convert the byte to an unsigned int
                  val_next =  (read < value.length) ? value[read++] : 0x00;
                  if (read > value.length) { break; }  // shouldn't ever hit this, but save a NPE
               }

               if (val_next == CT) {      //  start of a new section , when first didn't finish cleanly
                   --read;
               }

               if (val_next == LT) {
                  // hit the delimiter, carry on
                  val_next =  (read < value.length) ? value[read++] : 0x00;
               }

               while ((val_next == CR) || (val_next == LF)) {
                  val_next =  (read < value.length) ? value[read++] : 0x00;  // skip the new lines
                  if ((val_next != CR) && (val_next != LF)) {
                     --read;
                  }
               }
               if (tmp_line.toLowerCase(Locale.ROOT).startsWith("by") || longline.equals("bdy_author")) {
                  longkey = "bdy_author";

                  // prepend a space to subsequent line, so it gets parsed consistent with the lead line
                  tmp_line = (longline.equals(longkey) ? " " : "") + tmp_line;

                  // we have an author candidate
                  int term = tmp_line.length();
                  term = Math.min(term, (tmp_line.contains("<") ? tmp_line.indexOf("<")  : term));
                  term = Math.min(term, (tmp_line.contains("=") ? tmp_line.indexOf("=")  : term));
                  term = Math.min(term, (tmp_line.contains("\n") ? tmp_line.indexOf("\n") : term));
                  term = (term > 0 ) ? term : tmp_line.length();
                  bdy_author += tmp_line.substring(tmp_line.indexOf(" "), term);
                  metastarted = true;
                  longline = ((tmp_line.contains("=")) && (!longline.equals(longkey)) ? longkey : "");
               }
               else if (FORMAT == this.FMT_IPTC_BLM) {
                  String byline = "   by ";
                  if (tmp_line.toLowerCase(Locale.ROOT).contains(byline)) {
                     longkey = "bdy_author";

                     int term = tmp_line.length();
                     term = Math.min(term, (tmp_line.contains("<") ? tmp_line.indexOf("<")  : term));
                     term = Math.min(term, (tmp_line.contains("=") ? tmp_line.indexOf("=")  : term));
                     term = Math.min(term, (tmp_line.contains("\n") ? tmp_line.indexOf("\n") : term));
                     term = (term > 0 ) ? term : tmp_line.length();
                     // for bloomberg, the author line sits below their copyright statement
                     bdy_author += tmp_line.substring(tmp_line.toLowerCase(Locale.ROOT).indexOf(byline) + byline.length(), term) + " ";
                     metastarted = true;
                     longline = ((tmp_line.contains("=")) && (!longline.equals(longkey)) ? longkey : "");
                  }
                  else if(tmp_line.toLowerCase(Locale.ROOT).startsWith("c.")) {
                     // the author line for bloomberg is a multiline starting with c.2011 Bloomberg News
                     // then containing the author info on the next line
                     if (val_next == TB) {
                        value[--read] = CT;
                        continue;
                     }
                  }
                  else if(tmp_line.toLowerCase(Locale.ROOT).trim().startsWith("(") && tmp_line.toLowerCase(Locale.ROOT).trim().endsWith(")")) {
                     // the author line may have one or more comment lines between the copyright
                     // statement, and the By AUTHORNAME line
                     if (val_next == TB) {
                        value[--read] = CT;
                        continue;
                     }
                  }
               }

               else if (tmp_line.toLowerCase(Locale.ROOT).startsWith("eds") || longline.equals("bdy_source")) {
                  longkey = "bdy_source";
                  // prepend a space to subsequent line, so it gets parsed consistent with the lead line
                  tmp_line = (longline.equals(longkey) ? " " : "") + tmp_line;

                  // we have a source candidate
                  int term = tmp_line.length();
                  term = Math.min(term, (tmp_line.contains("<") ? tmp_line.indexOf("<")  : term));
                  term = Math.min(term, (tmp_line.contains("=") ? tmp_line.indexOf("=")  : term));
//                  term = Math.min(term, (tmp_line.indexOf("\n") > -1 ? tmp_line.indexOf("\n") : term));
                  term = (term > 0 ) ? term : tmp_line.length();
                  bdy_source += tmp_line.substring(tmp_line.indexOf(" ") + 1, term) + " ";
                  metastarted = true;
                  longline = (!longline.equals(longkey) ? longkey  : "");
               }
               else {
                  // this has fallen all the way through.  trap it as part of the subject,
                  // rather than just losing it
                  if (!metastarted) {
                     bdy_title += " , " + tmp_line;     //  not sure where else to put this but in the title
                  }
                  else {
                     // what to do with stuff that is metadata, which falls after metadata lines started?
                     bdy_body += " " + tmp_line + " , ";     //  not sure where else to put this but in the title
                  }
               }
            }
            else {  // we're on to the main body
               while ((read < value.length) && (val_next != 0))  {
                  // read until the train runs out of tracks
                  bdy_body += (char)(val_next & 0xff);  // convert the byte to an unsigned int
                  val_next =  (read < value.length) ? value[read++] : 0x00;
                  if (read > value.length) { break; }  // shouldn't ever hit this, but save a NPE
               }

            }
            // we would normally break here, but just let this read out to the end
         }
         done = true; // don't let this run back through and start thrashing metadata
      }
      properties.put("body", bdy_body);
      properties.put("title", bdy_title);
      properties.put("subject", bdy_heading);
      properties.put("author", bdy_author);
      properties.put("source", bdy_source);

      added = (bdy_body.length() + bdy_title.length() + bdy_heading.length() + bdy_author.length() +
               bdy_source.length()) > 0;
      return added;
   }


   private boolean parseFooter(byte[] value, HashMap<String,String> properties) {
      boolean added = false;

      String ftr_source = "";
      String ftr_datetime = "";

      int read = 0;
      boolean done = false;

      while (!done && (read < value.length)) {

         // pull apart the footer, getting the news feed source (^....\x0d\x0a)
         byte val_next = value[read++];
         byte val_peek =  (read < value.length) ? value[read+1] : 0x00;  // skip the new lines

         while (((val_next < (byte)0x30) || (val_next > (byte)0x39)) && (val_next != 0)) {  // consume all non-numerics first
            ftr_source += (char)(val_next & 0xff);  // convert the byte to an unsigned int
            val_next =  (read < value.length) ? value[read] : 0x00;  // attempt to read until end of stream
            read++;
            if (read > value.length) { break; }  // shouldn't ever hit this, but save a NPE
         }

         while ((val_next != LT) && (val_next != CR) && (val_next != LF) && (val_next != 0))  {  // get as much timedate as possible
            // this is an american format, so arrives as mm-dd-yy HHiizzz
            ftr_datetime += (char)(val_next & 0xff);  // convert the byte to an unsigned int
            val_next =  (read < value.length) ? value[read++] : 0x00;  // skip the new lines
            if (read > value.length) { break; }  // shouldn't ever hit this, but save a NPE
         }
         if (val_next == LT) {
            // hit the delimiter, carry on
            val_next =  (read < value.length) ? value[read++] : 0x00;
         }

         if (ftr_datetime.length() > 0) {
            // we want to pass this back in a more friendly format
            String format_out = "yyyy-MM-dd'T'HH:mm:ss'Z'";
            Date dateunix = new Date();
            try {
               // standard ap format
               String format_in = "MM-dd-yy HHmmzzz";

               if (FORMAT == this.FMT_IPTC_RTR) {
                  // standard reuters format
                  format_in = "HH:mm MM-dd-yy";
               }
               SimpleDateFormat dfi = new SimpleDateFormat(format_in, Locale.ROOT);
               dfi.setTimeZone(TimeZone.getTimeZone("UTC"));
               dateunix = dfi.parse(ftr_datetime);
            }
            catch (ParseException ep) {
               // failed, but this will just fall through to setting the date to now
            }
            SimpleDateFormat dfo = new SimpleDateFormat(format_out, Locale.ROOT);
            dfo.setTimeZone(TimeZone.getTimeZone("UTC"));
            ftr_datetime = dfo.format(dateunix);
         }
         while ((val_next == CR) || (val_next == LF)) {
            val_next =  (read < value.length) ? value[read++] : 0x00;  // skip the new lines
            if ((val_next != CR) && (val_next != LF)) {
               --read;
            }
         }
         done = true; // don't let this run back through and start thrashing metadata
      }

      properties.put("publisher", ftr_source);
      properties.put("created", ftr_datetime);
      properties.put("modified", ftr_datetime);

      added = (ftr_source.length() + ftr_datetime.length()) > 0; 
      return added;
   }


   private void setMetadata(Metadata metadata, HashMap<String,String> properties) {

      // every property that gets set must be non-null, or it will cause NPE
      // in other consuming applications, like Lucene
      metadata.set(Metadata.CONTENT_TYPE,  clean("text/anpa-1312"));
      metadata.set(TikaCoreProperties.TITLE,         clean(properties.get("title")));
      metadata.set(TikaCoreProperties.KEYWORDS,       clean(properties.get("subject")));
      metadata.set(TikaCoreProperties.CREATOR,        clean(properties.get("author")));
      metadata.set(TikaCoreProperties.CREATED, clean(properties.get("created")));
      metadata.set(TikaCoreProperties.MODIFIED,      clean(properties.get("modified")));
      metadata.set(TikaCoreProperties.SOURCE,      clean(properties.get("source")));
//      metadata.set(TikaCoreProperties.PUBLISHER,     clean(properties.get("publisher")));
      metadata.set(TikaCoreProperties.PUBLISHER,     clean(this.getFormatName()));

/*
        metadata.set(TikaCoreProperties.DATE, font.getHeader().getCreated().getTime());
        metadata.set(
                Property.internalDate(TikaCoreProperties.MODIFIED),
                font.getHeader().getModified().getTime());
*/
   }

   private String clean(String value) {
      if (value == null) {
         value = "";
      }

      value = value.replaceAll("``", "`");
      value = value.replaceAll("''", "'");
      value = value.replaceAll(new String(new char[] {SL}), "'");
      value = value.replaceAll(new String(new char[] {SR}), "'");
      value = value.replaceAll(new String(new char[] {DL}), "\"");
      value = value.replaceAll(new String(new char[] {DR}), "\"");
      value = value.trim();

      return (value);
   }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/iwork/AutoPageNumberUtils.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.iwork;

import java.util.Locale;

/**
 * Utility class to allow for conversion from an integer to Roman numerals
 * or alpha-numeric symbols in line with Pages auto numbering formats.
 */
 class AutoPageNumberUtils {
	
	private static final String ALPHABET[] = { "A", "B", "C", "D", "E", "F", "G",
		"H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T",
		"U", "V", "W", "X", "Y", "Z" };
	
	private static final int MAX = 26; 

	public static String asAlphaNumeric(int i) {
		StringBuffer sbuff = new StringBuffer();
		int index = i % MAX;
		int ratio = i / MAX;
		
		if (index == 0) {
			ratio--;
			index = MAX;
		}
		
		for(int j = 0; j <= ratio; j++) {
			sbuff.append(ALPHABET[index - 1]);		}
		return sbuff.toString();
	}
	
	public static String asAlphaNumericLower(int i) {
		return asAlphaNumeric(i).toLowerCase(Locale.ROOT);
	}
	
	/*
	 * Code copied from jena.apache.org.
	 * @see com.hp.hpl.jena.sparql.util.RomanNumeral
	 */
    public static String asRomanNumerals(int i) {
        if ( i <= 0 )
            throw new NumberFormatException("Roman numerals are 1-3999 ("+i+")") ;
        if ( i > 3999 )
            throw new NumberFormatException("Roman numerals are 1-3999 ("+i+")") ;
        StringBuffer sbuff = new StringBuffer() ;
        
        i = i2r(sbuff, i, "M", 1000, "CM", 900, "D", 500, "CD", 400 ) ;
        i = i2r(sbuff, i, "C", 100,  "XC", 90,  "L", 50,  "XL", 40 ) ;
        i = i2r(sbuff, i, "X", 10,   "IX", 9,   "V", 5,   "IV", 4) ;
        
        while ( i >= 1 )
        {
            sbuff.append("I") ;
            i -= 1 ;
        }
        return sbuff.toString() ;
            
        
    }
    
	public static String asRomanNumeralsLower(int i) {
		return asRomanNumerals(i).toLowerCase(Locale.ROOT);
	}
    
    private static int i2r(StringBuffer sbuff, int i,
                           String tens,  int iTens, 
                           String nines, int iNines,
                           String fives, int iFives,
                           String fours, int iFours)
    {
        while ( i >= iTens )
        {
            sbuff.append(tens) ;
            i -= iTens ;
        }
        
        if ( i >= iNines )
        {
            sbuff.append(nines) ;
            i -= iNines;
        }

        if ( i >= iFives )
        {
            sbuff.append(fives) ;
            i -= iFives ;
        }
        if ( i >= iFours )
        {
            sbuff.append(fours) ;
            i -= iFours ;
        }
        return i ;
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/iwork/IWorkPackageParser.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.iwork;

import java.io.BufferedInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.util.Arrays;
import java.util.Collections;
import java.util.HashSet;
import java.util.Set;

import javax.xml.namespace.QName;

import org.apache.commons.compress.archivers.zip.UnsupportedZipFeatureException;
import org.apache.commons.compress.archivers.zip.ZipArchiveEntry;
import org.apache.commons.compress.archivers.zip.ZipArchiveInputStream;
import org.apache.commons.compress.archivers.zip.ZipFile;
import org.apache.tika.detect.XmlRootExtractor;
import org.apache.tika.exception.TikaException;
import org.apache.tika.io.CloseShieldInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.OfflineContentHandler;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * A parser for the IWork container files. This includes *.key, *.pages and *.numbers files.
 * This parser delegates the relevant entries to a {@link ContentHandler} that parsers the content.
 * 
 * Currently supported formats:
 * <ol>
 * <li>Keynote format version 2.x. Currently only tested with Keynote version 5.x
 * <li>Pages format version 1.x. Currently only tested with Pages version 4.0.x
 * <li>Numbers format version 1.x. Currently only tested with Numbers version 2.0.x
 * </ol>
 */
public class IWorkPackageParser extends AbstractParser {

    /** Serial version UID */
    private static final long serialVersionUID = -2160322853809682372L;

    /**
     * Which files within an iWork file contain the actual content?
     */
    public final static Set<String> IWORK_CONTENT_ENTRIES = Collections.unmodifiableSet(
            new HashSet<String>(Arrays.asList("index.apxl", "index.xml", "presentation.apxl"))
    );
    /**
     * All iWork files contain one of these, so we can detect based on it
     */
    public final static String IWORK_COMMON_ENTRY = "buildVersionHistory.plist";
    
    public enum IWORKDocumentType {
       KEYNOTE("http://developer.apple.com/namespaces/keynote2", "presentation", MediaType.application("vnd.apple.keynote")),
       NUMBERS("http://developer.apple.com/namespaces/ls", "document", MediaType.application("vnd.apple.numbers")),
       PAGES("http://developer.apple.com/namespaces/sl", "document", MediaType.application("vnd.apple.pages")),
       ENCRYPTED(null, null, MediaType.application("x-tika-iworks-protected"));
       
       private final String namespace;
       private final String part;
       private final MediaType type;
       
       IWORKDocumentType(String namespace, String part, MediaType type) {
          this.namespace = namespace;
          this.part = part;
          this.type = type;
       }
       
       public String getNamespace() {
          return namespace;
       }

       public String getPart() {
          return part;
       }

       public MediaType getType() {
          return type;
       }

       public static IWORKDocumentType detectType(ZipArchiveEntry entry, ZipFile zip) {
          try {
             if (entry == null) {
                 return null;
             }

             InputStream stream = zip.getInputStream(entry);
             try {
                return detectType(stream);
             } finally {
                 stream.close();
             }
          } catch (IOException e) {
             return null;
          }
       }
       
       public static IWORKDocumentType detectType(ZipArchiveEntry entry, ZipArchiveInputStream zip) {
          if (entry == null) {
              return null;
          }

          return detectType(zip);
       }
       
       private static IWORKDocumentType detectType(InputStream stream) {
          QName qname = new XmlRootExtractor().extractRootElement(stream);
          if (qname != null) {
             String uri = qname.getNamespaceURI();
             String local = qname.getLocalPart();
            
             for (IWORKDocumentType type : values()) {
                if(type.getNamespace().equals(uri) && 
                   type.getPart().equals(local)) {
                   return type;
                }
             }
          } else {
             // There was a problem with extracting the root type
             // Password Protected iWorks files are funny, but we can usually
             //  spot them because they encrypt part of the zip stream 
             try {
                stream.read();
             } catch(UnsupportedZipFeatureException e) {
                // Compression field was likely encrypted
                return ENCRYPTED;
             } catch(Exception ignored) {
             }
          }
          return null;
       }
    }

    /**
     * This parser handles all iWorks formats.
     */
    private final static Set<MediaType> supportedTypes =
         Collections.unmodifiableSet(new HashSet<MediaType>(Arrays.asList(
                MediaType.application("vnd.apple.iwork"),
                IWORKDocumentType.KEYNOTE.getType(),
                IWORKDocumentType.NUMBERS.getType(),
                IWORKDocumentType.PAGES.getType()
         )));

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return supportedTypes;
    }

    public void parse(InputStream stream, ContentHandler handler, Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        ZipArchiveInputStream zip = new ZipArchiveInputStream(stream);
        ZipArchiveEntry entry = zip.getNextZipEntry();

        while (entry != null) {
            if (!IWORK_CONTENT_ENTRIES.contains(entry.getName())) {
                entry = zip.getNextZipEntry();
                continue;
            }

            InputStream entryStream = new BufferedInputStream(zip, 4096);
            entryStream.mark(4096);
            IWORKDocumentType type = IWORKDocumentType.detectType(entryStream);
            entryStream.reset();
            
            if(type != null) {
               XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);
               ContentHandler contentHandler;
               
               switch(type) {
               case KEYNOTE:
                  contentHandler = new KeynoteContentHandler(xhtml, metadata);
                  break;
               case NUMBERS:
                  contentHandler = new NumbersContentHandler(xhtml, metadata);
                  break;
               case PAGES:
                  contentHandler = new PagesContentHandler(xhtml, metadata);
                  break;
               case ENCRYPTED:
                   // We can't do anything for the file right now
                   contentHandler = null;
                   break;
               default:
                  throw new TikaException("Unhandled iWorks file " + type);
               }

               metadata.add(Metadata.CONTENT_TYPE, type.getType().toString());
               xhtml.startDocument();
               if (contentHandler != null) {
                  context.getSAXParser().parse(
                          new CloseShieldInputStream(entryStream),
                          new OfflineContentHandler(contentHandler)
                  );
               }
               xhtml.endDocument();
            }
            
            entry = zip.getNextZipEntry();
        }
        zip.close();
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/iwork/KeynoteContentHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.iwork;

import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.Attributes;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.DefaultHandler;

class KeynoteContentHandler extends DefaultHandler {

    public final static String PRESENTATION_WIDTH = "slides-width";
    public final static String PRESENTATION_HEIGHT = "slides-height";

    private final XHTMLContentHandler xhtml;
    private final Metadata metadata;

    private boolean inSlide = false;
    private boolean inTheme = false;
    private boolean inTitle = false;
    private boolean inBody = false;
    private String tableId;
    private Integer numberOfColumns = null;
    private Integer currentColumn = null;

    private boolean inMetadata = false;
    private boolean inMetaDataTitle = false;
    private boolean inMetaDataAuthors = false;

    private boolean inParsableText = false;

    private int numberOfSlides = 0;

    KeynoteContentHandler(XHTMLContentHandler xhtml, Metadata metadata) {
        this.xhtml = xhtml;
        this.metadata = metadata;
    }

    @Override
    public void endDocument() throws SAXException {
        metadata.set(Metadata.SLIDE_COUNT, String.valueOf(numberOfSlides));
    }

    @Override
    public void startElement(
            String uri, String localName, String qName, Attributes attributes)
            throws SAXException {
        if ("key:theme".equals(qName)) {
            inTheme = true;
        } else if ("key:slide".equals(qName)) {
            inSlide = true;
            numberOfSlides++;
            xhtml.startElement("div");
        } else if ("key:master-slide".equals(qName)) {
            inSlide = true;
            xhtml.startElement("div");
        } else if ("key:title-placeholder".equals(qName) && inSlide) {
            inTitle = true;
            xhtml.startElement("h1");
        } else if ("sf:sticky-note".equals(qName) && inSlide) {
            xhtml.startElement("p");
        } else if ("key:notes".equals(qName) && inSlide) {
            xhtml.startElement("p");
        } else if ("key:body-placeholder".equals(qName) && inSlide) {
            xhtml.startElement("p");
            inBody = true;
        } else if ("key:size".equals(qName) && !inTheme) {
            String width = attributes.getValue("sfa:w");
            String height = attributes.getValue("sfa:h");
            metadata.set(PRESENTATION_WIDTH, width);
            metadata.set(PRESENTATION_HEIGHT, height);
        } else if ("sf:text-body".equals(qName)) {
            inParsableText = true;
        } else if ("key:metadata".equals(qName)) {
            inMetadata = true;
        } else if (inMetadata && "key:title".equals(qName)) {
            inMetaDataTitle = true;
        } else if (inMetadata && "key:authors".equals(qName)) {
            inMetaDataAuthors = true;
        } else if (inMetaDataTitle && "key:string".equals(qName)) {
            metadata.set(TikaCoreProperties.TITLE, attributes.getValue("sfa:string"));
        } else if (inMetaDataAuthors && "key:string".equals(qName)) {
            metadata.add(TikaCoreProperties.CREATOR, attributes.getValue("sfa:string"));
        } else if (inSlide && "sf:tabular-model".equals(qName)) {
            tableId = attributes.getValue("sfa:ID");
            xhtml.startElement("table");
        } else if (tableId != null && "sf:columns".equals(qName)) {
            numberOfColumns = Integer.parseInt(attributes.getValue("sf:count"));
            currentColumn = 0;
        } else if (tableId != null && "sf:ct".equals(qName)) {
            parseTableData(attributes.getValue("sfa:s"));
        } else if (tableId != null && "sf:n".equals(qName)) {
            parseTableData(attributes.getValue("sf:v"));
        } else if ("sf:p".equals(qName)) {
            xhtml.startElement("p");
        }
    }

    @Override
    public void endElement(String uri, String localName, String qName)
            throws SAXException {
        if ("key:theme".equals(qName)) {
            inTheme = false;
        } else if ("key:slide".equals(qName)) {
            inSlide = false;
            xhtml.endElement("div");
        } else if ("key:master-slide".equals(qName)) {
            inSlide = false;
            xhtml.endElement("div");
        } else if ("key:title-placeholder".equals(qName) && inSlide) {
            inTitle = false;
            xhtml.endElement("h1");
        } else if ("sf:sticky-note".equals(qName) && inSlide) {
            xhtml.endElement("p");
        } else if ("key:notes".equals(qName) && inSlide) {
            xhtml.endElement("p");
        } else if ("key:body-placeholder".equals(qName) && inSlide) {
            xhtml.endElement("p");
            inBody = false;
        } else if ("sf:text-body".equals(qName)) {
            inParsableText = false;
        } else if ("key:metadata".equals(qName)) {
            inMetadata = false;
        } else if (inMetadata && "key:title".equals(qName)) {
            inMetaDataTitle = false;
        } else if (inMetadata && "key:authors".equals(qName)) {
            inMetaDataAuthors = false;
        } else if (inSlide && "sf:tabular-model".equals(qName)) {
            xhtml.endElement("table");
            tableId = null;
            numberOfColumns = null;
            currentColumn = null;
        } else if ("sf:p".equals(qName)) {
            xhtml.endElement("p");
        }
    }

    @Override
    public void characters(char[] ch, int start, int length)
            throws SAXException {
        if (inParsableText && inSlide && length != 0) {
            xhtml.characters(ch, start, length);
        }
    }

    private void parseTableData(String value) throws SAXException {
      if (currentColumn == 0) {
          xhtml.startElement("tr");
      }

      xhtml.element("td", value);

      if (currentColumn.equals(numberOfColumns)) {
          xhtml.endElement("tr");
      }
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/iwork/NumbersContentHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.iwork;

import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.Property;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.Attributes;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.DefaultHandler;

import java.util.HashMap;
import java.util.Map;

class NumbersContentHandler extends DefaultHandler {

    private final XHTMLContentHandler xhtml;
    private final Metadata metadata;

    private boolean inSheet = false;

    private boolean inText = false;
    private boolean parseText = false;

    private boolean inMetadata = false;
    private Property metadataKey;
    private String metadataPropertyQName;

    private boolean inTable = false;
    private int numberOfSheets = 0;
    private int numberOfColumns = -1;
    private int currentColumn = 0;

    private Map<String, String> menuItems = new HashMap<String, String>();
    private String currentMenuItemId;

    NumbersContentHandler(XHTMLContentHandler xhtml, Metadata metadata) {
        this.xhtml = xhtml;
        this.metadata = metadata;
    }

    @Override
    public void endDocument() throws SAXException {
        metadata.set(Metadata.PAGE_COUNT, String.valueOf(numberOfSheets));
    }

    @Override
    public void startElement(String uri, String localName, String qName, Attributes attributes) throws SAXException {
        if ("ls:workspace".equals(qName)) {
            inSheet = true;
            numberOfSheets++;
            xhtml.startElement("div");
            String sheetName = attributes.getValue("ls:workspace-name");
            metadata.add("sheetNames", sheetName);
        }

        if ("sf:text".equals(qName)) {
            inText = true;
            xhtml.startElement("p");
        }

        if ("sf:p".equals(qName)) {
            parseText = true;
        }

        if ("sf:metadata".equals(qName)) {
            inMetadata = true;
            return;
        }

        if (inMetadata && metadataKey == null) {
            metadataKey = resolveMetadataKey(localName);
            metadataPropertyQName = qName;
        }

        if (inMetadata && metadataKey != null && "sf:string".equals(qName)) {
            metadata.add(metadataKey, attributes.getValue("sfa:string"));
        }

        if (!inSheet) {
            return;
        }

        if ("sf:tabular-model".equals(qName)) {
            String tableName = attributes.getValue("sf:name");
            xhtml.startElement("div");
            xhtml.characters(tableName);
            xhtml.endElement("div");
            inTable = true;
            xhtml.startElement("table");
            xhtml.startElement("tr");
            currentColumn = 0;
        }

        if ("sf:menu-choices".equals(qName)) {
            menuItems = new HashMap<String, String>();
        }

        if (inTable && "sf:grid".equals(qName)) {
            numberOfColumns = Integer.parseInt(attributes.getValue("sf:numcols"));
        }

        if (menuItems != null && "sf:t".equals(qName)) {
            currentMenuItemId = attributes.getValue("sfa:ID");
        }

        if (currentMenuItemId != null && "sf:ct".equals(qName)) {
            menuItems.put(currentMenuItemId, attributes.getValue("sfa:s"));
        }

        if (inTable && "sf:ct".equals(qName)) {
            if (currentColumn >= numberOfColumns) {
                currentColumn = 0;
                xhtml.endElement("tr");
                xhtml.startElement("tr");
            }

            xhtml.element("td", attributes.getValue("sfa:s"));
            currentColumn++;
        }

        if (inTable && ("sf:n".equals(qName) || "sf:rn".equals(qName))) {
            if (currentColumn >= numberOfColumns) {
                currentColumn = 0;
                xhtml.endElement("tr");
                xhtml.startElement("tr");
            }

            xhtml.element("td", attributes.getValue("sf:v"));
            currentColumn++;
        }

        if (inTable && "sf:proxied-cell-ref".equals(qName)) {
            if (currentColumn >= numberOfColumns) {
                currentColumn = 0;
                xhtml.endElement("tr");
                xhtml.startElement("tr");
            }

            xhtml.element("td", menuItems.get(attributes.getValue("sfa:IDREF")));
            currentColumn++;
        }

        if ("sf:chart-name".equals(qName)) {
            // Extract chart name:
            xhtml.startElement("div", "class", "chart");
            xhtml.startElement("h1");
            xhtml.characters(attributes.getValue("sfa:string"));
            xhtml.endElement("h1");
            xhtml.endElement("div");
        }
    }

    @Override
    public void characters(char[] ch, int start, int length) throws SAXException {
        if (parseText && length > 0) {
            xhtml.characters(ch, start, length);
        }
    }

    @Override
    public void endElement(String uri, String localName, String qName) throws SAXException {
        if ("ls:workspace".equals(qName)) {
            inSheet = false;
            xhtml.endElement("div");
        }

        if ("sf:text".equals(qName)) {
            inText = false;
            xhtml.endElement("p");
        }

        if ("sf:p".equals(qName)) {
            parseText = false;
        }

        if ("sf:metadata".equals(qName)) {
            inMetadata = false;
        }

        if (inMetadata && qName.equals(metadataPropertyQName)) {
            metadataPropertyQName = null;
            metadataKey = null;
        }

        if (!inSheet) {
            return;
        }

        if ("sf:menu-choices".equals(qName)) {
        }

        if ("sf:tabular-model".equals(qName)) {
            inTable = false;
            xhtml.endElement("tr");
            xhtml.endElement("table");
        }

        if (currentMenuItemId != null && "sf:t".equals(qName)) {
            currentMenuItemId = null;
        }
    }

    private Property resolveMetadataKey(String localName) {
        if ("authors".equals(localName)) {
            return TikaCoreProperties.CREATOR;
        }
        if ("title".equals(localName)) {
            return TikaCoreProperties.TITLE;
        }
        if ("comment".equals(localName)) {
            return TikaCoreProperties.COMMENTS;
        }
        return Property.internalText(localName);
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/iwork/PagesContentHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.iwork;

import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.Property;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.Attributes;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.DefaultHandler;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

class PagesContentHandler extends DefaultHandler {

    private final XHTMLContentHandler xhtml;
    private final Metadata metadata;

    /** The (interesting) part of the document we're in. Should be more structured... */
    private enum DocumentPart {
       METADATA, PARSABLE_TEXT, 
       HEADERS, HEADER_ODD, HEADER_EVEN, HEADER_FIRST,
       FOOTERS, FOOTER_ODD, FOOTER_EVEN, FOOTER_FIRST,
       FOOTNOTES, ANNOTATIONS;
    }
    private DocumentPart inPart = null;
    private boolean ghostText;
    
    private static String alphabet = "ABCDEFGHIJKLMNOPQRSTUVWXYZ";

    private boolean parseProperty = false;
    private int pageCount = 0;
    private int slPageCount = 0;

    private HeaderFooter headers = null;
    private HeaderFooter footers = null;
    private Footnotes footnotes = null; 
    private Annotations annotations = null; 
    
    private Map<String, List<List<String>>> tableData =
        new HashMap<String, List<List<String>>>();
    private String activeTableId;
    private int numberOfColumns = 0;
    private List<String> activeRow = new ArrayList<String>();

    private String metaDataLocalName;
    private String metaDataQName;

    PagesContentHandler(XHTMLContentHandler xhtml, Metadata metadata) {
        this.xhtml = xhtml;
        this.metadata = metadata;
    }

    @Override
    public void endDocument() throws SAXException {
        metadata.set(Metadata.PAGE_COUNT, String.valueOf(pageCount));
        if (pageCount > 0) {
            doFooter();
            xhtml.endElement("div");
        }
    }

    @Override
    public void startElement(
            String uri, String localName, String qName, Attributes attributes)
            throws SAXException {
        if (parseProperty) {
            String value = parsePrimitiveElementValue(qName, attributes);
            if (value != null) {
                Object metaDataKey = resolveMetaDataKey(metaDataLocalName);
                if(metaDataKey instanceof Property) {
                    metadata.set((Property)metaDataKey, value);
                } else {
                    metadata.add((String)metaDataKey, value);
                }
            }
        }

        if ("sl:publication-info".equals(qName)) {
            inPart = DocumentPart.METADATA;
        } else if ("sf:metadata".equals(qName)) {
           inPart = DocumentPart.METADATA;
        } else if ("sf:page-start".equals(qName) || "sl:page-group".equals(qName)) {
            if (pageCount > 0) {
                doFooter();
                xhtml.endElement("div");
            }
            xhtml.startElement("div");
            if ("sl:page-group".equals(qName)) {
                slPageCount++;
            } else {
                pageCount++;
            }
            doHeader();
        } else if ("sf:p".equals(qName)) {
          if (pageCount+slPageCount > 0) {
            inPart = DocumentPart.PARSABLE_TEXT;
            xhtml.startElement("p");
          }
        } else if ("sf:attachment".equals(qName)) {
            String kind = attributes.getValue("sf:kind");
            if ("tabular-attachment".equals(kind)) {
                activeTableId = attributes.getValue("sfa:ID");
                tableData.put(activeTableId, new ArrayList<List<String>>());
            }
        } else if ("sf:attachment-ref".equals(qName)) {
            String idRef = attributes.getValue("sfa:IDREF");
            outputTable(idRef);
        } else if ("sf:headers".equals(qName)) {
            headers = new HeaderFooter(qName);
            inPart = DocumentPart.HEADERS;
        } else if ("sf:footers".equals(qName)) {
           footers = new HeaderFooter(qName);
           inPart = DocumentPart.FOOTERS;
        } else if ("sf:header".equals(qName)) {
            inPart = headers.identifyPart(attributes.getValue("sf:name"));
        } else if ("sf:footer".equals(qName)) {
           inPart = footers.identifyPart(attributes.getValue("sf:name"));
        } else if ("sf:page-number".equals(qName)) {	
        	if (inPart == DocumentPart.FOOTER_ODD
        		|| inPart == DocumentPart.FOOTER_FIRST
        		|| inPart == DocumentPart.FOOTER_EVEN) {
        		// We are in a footer
        		footers.hasAutoPageNumber = true;
        		footers.autoPageNumberFormat = attributes.getValue("sf:format");   
        	} else {
        		headers.hasAutoPageNumber = true;
        		headers.autoPageNumberFormat = attributes.getValue("sf:format");   
        	}

        	xhtml.characters(Integer.toString(this.pageCount));
        } else if ("sf:footnotes".equals(qName)) {
           footnotes = new Footnotes();
           inPart = DocumentPart.FOOTNOTES;
        } else if ("sf:footnote-mark".equals(qName)) {
           footnotes.recordMark(attributes.getValue("sf:mark"));
        } else if ("sf:footnote".equals(qName) && inPart == DocumentPart.PARSABLE_TEXT) {
           // What about non auto-numbered?
           String footnoteMark = attributes.getValue("sf:autonumber");
           if (footnotes != null) {
              String footnoteText = footnotes.footnotes.get(footnoteMark);
              if (footnoteText != null) {
                 xhtml.startElement("div", "style", "footnote");
                 xhtml.characters("Footnote:" ); // As shown in Pages
                 xhtml.characters(footnoteText);
                 xhtml.endElement("div");
              }
           }
        } else if ("sf:annotations".equals(qName)) {
           annotations = new Annotations();
           inPart = DocumentPart.ANNOTATIONS;
        } else if ("sf:annotation".equals(qName) && inPart == DocumentPart.ANNOTATIONS) {
           annotations.start(attributes.getValue("sf:target"));
        } else if ("sf:annotation-field".equals(qName) && inPart == DocumentPart.PARSABLE_TEXT) {
           xhtml.startElement("div", "style", "annotated");
           
           String annotationText = annotations.annotations.get(attributes.getValue("sfa:ID"));
           if (annotationText != null) {
              xhtml.startElement("div", "style", "annotation");
              xhtml.characters(annotationText);
              xhtml.endElement("div");
           }
        } else if ("sf:ghost-text".equals(qName)) {
            ghostText = true;
        }

        if (activeTableId != null) {
            parseTableData(qName, attributes);
        }

        if (inPart == DocumentPart.METADATA) {
            metaDataLocalName = localName;
            metaDataQName = qName;
            parseProperty = true;
        }
    }

    @Override
    public void endElement(String uri, String localName, String qName)
            throws SAXException {
        if (metaDataLocalName != null && metaDataLocalName.equals(localName)) {
            metaDataLocalName = null;
            parseProperty = false;
        }

        if ("sl:publication-info".equals(qName)) {
            inPart = null;
        } else if ("sf:metadata".equals(qName)) {
            inPart = null;
        } else if ("sf:p".equals(qName) && (pageCount+slPageCount) > 0) {
            inPart = null;
            xhtml.endElement("p");
        } else if ("sf:attachment".equals(qName)) {
            activeTableId = null;
        } else if ("sf:annotation".equals(qName) && inPart == DocumentPart.ANNOTATIONS) {
            annotations.end();
        } else if ("sf:annotation-field".equals(qName) && inPart == DocumentPart.PARSABLE_TEXT) {
            xhtml.endElement("div");
        } else if ("sf:ghost-text".equals(qName)) {
            ghostText = false;
        }
    }

    @Override
    public void characters(char[] ch, int start, int length) throws SAXException {
        if (length > 0) {
           if (inPart == DocumentPart.PARSABLE_TEXT) {
               if (!ghostText) {
                   xhtml.characters(ch, start, length);
               }
          } else if(inPart != null) {
              String str = new String(ch, start, length);
              if (inPart == DocumentPart.HEADER_FIRST) headers.defaultFirst = str;
              if (inPart == DocumentPart.HEADER_EVEN)  headers.defaultEven = str;
              if (inPart == DocumentPart.HEADER_ODD)   headers.defaultOdd = str;
              if (inPart == DocumentPart.FOOTER_FIRST) footers.defaultFirst = str;
              if (inPart == DocumentPart.FOOTER_EVEN)  footers.defaultEven = str;
              if (inPart == DocumentPart.FOOTER_ODD)   footers.defaultOdd = str;
              if (inPart == DocumentPart.FOOTNOTES)    footnotes.text(str);
              if (inPart == DocumentPart.ANNOTATIONS)  annotations.text(str);
          }
        }
    }

    private void parseTableData(String qName, Attributes attributes) {
        if ("sf:grid".equals(qName)) {
            String numberOfColumns = attributes.getValue("sf:numcols");
            this.numberOfColumns = Integer.parseInt(numberOfColumns);
        } else if ("sf:ct".equals(qName)) {
            activeRow.add(attributes.getValue("sfa:s"));

            if (activeRow.size() >= 3) {
                tableData.get(activeTableId).add(activeRow);
                activeRow = new ArrayList<String>();
            }
        }
    }

    private void outputTable(String idRef) throws SAXException {
        List<List<String>> tableData = this.tableData.get(idRef);
        if (tableData != null) {
            xhtml.startElement("table");
            for (List<String> row : tableData) {
                xhtml.startElement("tr");
                for (String cell : row) {
                    xhtml.element("td", cell);
                }
                xhtml.endElement("tr");
            }
            xhtml.endElement("table");
        }
    }

    /**
     * Returns a resolved key that is common in other document types or
     * returns the specified metaDataLocalName if no common key could be found.
     * The key could be a simple String key, or could be a {@link Property}
     *
     * @param metaDataLocalName The localname of the element containing metadata
     * @return a resolved key that is common in other document types
     */
    private Object resolveMetaDataKey(String metaDataLocalName) {
        Object metaDataKey = metaDataLocalName;
        if ("sf:authors".equals(metaDataQName)) {
            metaDataKey = TikaCoreProperties.CREATOR;
        } else if ("sf:title".equals(metaDataQName)) {
            metaDataKey = TikaCoreProperties.TITLE;
        } else if ("sl:SLCreationDateProperty".equals(metaDataQName)) {
            metaDataKey = TikaCoreProperties.CREATED;
        } else if ("sl:SLLastModifiedDateProperty".equals(metaDataQName)) {
            metaDataKey = Metadata.LAST_MODIFIED;
        } else if ("sl:language".equals(metaDataQName)) {
            metaDataKey = TikaCoreProperties.LANGUAGE;
        }
        return metaDataKey;
    }

    /**
     * Returns the value of a primitive element e.g.:
     * &lt;sl:number sfa:number="0" sfa:type="f"/&gt; - the number attribute
     * &lt;sl:string sfa:string="en"/&gt; = the string attribute
     * <p>
     * Returns <code>null</code> if the value could not be extracted from
     * the list of attributes.
     *
     * @param qName      The fully qualified name of the element containing
     *                   the value to extract
     * @param attributes The list of attributes of which one contains the
     *                   value to be extracted
     * @return the value of a primitive element
     */
    private String parsePrimitiveElementValue(
            String qName, Attributes attributes) {
        if ("sl:string".equals(qName) || "sf:string".equals(qName)) {
            return attributes.getValue("sfa:string");
        } else if ("sl:number".equals(qName)) {
            return attributes.getValue("sfa:number");
        } else if ("sl:date".equals(qName)) {
            return attributes.getValue("sf:val");
        }

        return null;
    }
    
    private void doHeader() throws SAXException {
       if (headers != null) {
          headers.output("header");
       }
    }
    private void doFooter() throws SAXException {
       if (footers != null) {
          footers.output("footer");
       }
    }

    /**
     * Represents the Headers or Footers in a document
     */
    private class HeaderFooter {
       private String type; // sf:headers or sf:footers
       private String defaultOdd;
       private String defaultEven;
       private String defaultFirst;
       private boolean hasAutoPageNumber;
       private String autoPageNumberFormat;
       // TODO Can there be custom ones?
       
       private HeaderFooter(String type) {
          this.type = type; 
       }
       private DocumentPart identifyPart(String name) {
          if("SFWPDefaultOddHeaderIdentifier".equals(name))
             return DocumentPart.HEADER_ODD;
          if("SFWPDefaultEvenHeaderIdentifier".equals(name))
             return DocumentPart.HEADER_EVEN;
          if("SFWPDefaultFirstHeaderIdentifier".equals(name))
             return DocumentPart.HEADER_FIRST;
          
          if("SFWPDefaultOddFooterIdentifier".equals(name))
             return DocumentPart.FOOTER_ODD;
          if("SFWPDefaultEvenFooterIdentifier".equals(name))
             return DocumentPart.FOOTER_EVEN;
          if("SFWPDefaultFirstFooterIdentifier".equals(name))
             return DocumentPart.FOOTER_FIRST;
          
          return null;
       }
       private void output(String what) throws SAXException {
          String text = null;
          if (pageCount == 1 && defaultFirst != null) {
             text = defaultFirst;
          } else if (pageCount % 2 == 0 && defaultEven != null) {
             text = defaultEven;
          } else {
             text = defaultOdd;
          }
          
          if (text != null) {
             xhtml.startElement("div", "class", "header");
             xhtml.characters(text);
             if (hasAutoPageNumber) {
            	 if (autoPageNumberFormat == null) { // raw number
            		 xhtml.characters("\t" + pageCount);
            	 } else if (autoPageNumberFormat.equals("upper-roman")){
            		 xhtml.characters("\t" + AutoPageNumberUtils.asRomanNumerals(pageCount));
            	 } else if (autoPageNumberFormat.equals("lower-roman")){
            		 xhtml.characters("\t" + AutoPageNumberUtils.asRomanNumeralsLower(pageCount));
            	 } else if (autoPageNumberFormat.equals("upper-alpha")){
            		 xhtml.characters("\t" + AutoPageNumberUtils.asAlphaNumeric(pageCount));
            	 } else if (autoPageNumberFormat.equals("lower-alpha")){
            		 xhtml.characters("\t" + AutoPageNumberUtils.asAlphaNumericLower(pageCount));
            	 }
             }
             xhtml.endElement("div");
          }
       }
    }
    /**
     * Represents Footnotes in a document. The way these work
     *  in the file format isn't very clean...
     */
    private static class Footnotes {
       /** Mark -> Text */
       Map<String,String> footnotes = new HashMap<String, String>();
       String lastSeenMark = null;
       
       /**
        * Normally happens before the text of the mark
        */
       private void recordMark(String mark) {
          lastSeenMark = mark;
       }
       private void text(String text) {
          if (lastSeenMark != null) {
             if (footnotes.containsKey(lastSeenMark)) {
                text = footnotes.get(lastSeenMark) + text;
             }
             footnotes.put(lastSeenMark, text);
          }
       }
    }
    /**
     * Represents Annotations in a document. We currently
     *  just grab all the sf:p text in each one 
     */
    private class Annotations {
       /** ID -> Text */
       Map<String,String> annotations = new HashMap<String, String>();
       String currentID = null;
       StringBuffer currentText = null;
       
       private void start(String id) {
          currentID = id;
          currentText = new StringBuffer();
       }
       private void text(String text) {
          if (text != null && text.length() > 0 && currentText != null) {
             currentText.append(text);
          }
       }
       private void end() {
          if (currentText.length() > 0) {
             annotations.put(currentID, currentText.toString());
             currentID = null;
             currentText = null;
          }
       }
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/jpeg/JpegParser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.jpeg;

import java.io.IOException;
import java.io.InputStream;
import java.util.Collections;
import java.util.Set;

import org.apache.tika.exception.TikaException;
import org.apache.tika.io.TemporaryResources;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.image.ImageMetadataExtractor;
import org.apache.tika.parser.image.xmp.JempboxExtractor;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

public class JpegParser extends AbstractParser {

    /** Serial version UID */
    private static final long serialVersionUID = -1355028253756234603L;

    private static final Set<MediaType> SUPPORTED_TYPES =
        Collections.singleton(MediaType.image("jpeg"));

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }

    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        TemporaryResources tmp = new TemporaryResources();
        try {
            TikaInputStream tis = TikaInputStream.get(stream, tmp);
            new ImageMetadataExtractor(metadata).parseJpeg(tis.getFile());
            new JempboxExtractor(metadata).parse(tis);
        } finally {
            tmp.dispose();
        }

        XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);
        xhtml.startDocument();
        xhtml.endDocument();
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/mail/MailContentHandler.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.mail;

import java.io.IOException;
import java.io.InputStream;

import org.apache.james.mime4j.MimeException;
import org.apache.james.mime4j.codec.DecodeMonitor;
import org.apache.james.mime4j.codec.DecoderUtil;
import org.apache.james.mime4j.dom.address.Address;
import org.apache.james.mime4j.dom.address.AddressList;
import org.apache.james.mime4j.dom.address.Mailbox;
import org.apache.james.mime4j.dom.address.MailboxList;
import org.apache.james.mime4j.dom.field.AddressListField;
import org.apache.james.mime4j.dom.field.DateTimeField;
import org.apache.james.mime4j.dom.field.MailboxListField;
import org.apache.james.mime4j.dom.field.ParsedField;
import org.apache.james.mime4j.dom.field.UnstructuredField;
import org.apache.james.mime4j.field.LenientFieldParser;
import org.apache.james.mime4j.parser.ContentHandler;
import org.apache.james.mime4j.stream.BodyDescriptor;
import org.apache.james.mime4j.stream.Field;
import org.apache.tika.config.TikaConfig;
import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.parser.AutoDetectParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.Parser;
import org.apache.tika.sax.BodyContentHandler;
import org.apache.tika.sax.EmbeddedContentHandler;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.SAXException;

/**
 * Bridge between mime4j's content handler and the generic Sax content handler
 * used by Tika. See
 * http://james.apache.org/mime4j/apidocs/org/apache/james/mime4j/parser/ContentHandler.html
 */
class MailContentHandler implements ContentHandler {

    private boolean strictParsing = false;

    private XHTMLContentHandler handler;
    private ParseContext context;
    private Metadata metadata;
    private TikaConfig tikaConfig = null;

    private boolean inPart = false;
    
    MailContentHandler(XHTMLContentHandler xhtml, Metadata metadata, ParseContext context, boolean strictParsing) {
        this.handler = xhtml;
        this.context = context;
        this.metadata = metadata;
        this.strictParsing = strictParsing;
    }

    public void body(BodyDescriptor body, InputStream is) throws MimeException,
            IOException {
        // Work out the best underlying parser for the part
        // Check first for a specified AutoDetectParser (which may have a
        //  specific Config), then a recursing parser, and finally the default
        Parser parser = context.get(AutoDetectParser.class);
        if (parser == null) {
           parser = context.get(Parser.class);
        }
        if (parser == null) {
           if (tikaConfig == null) {
              tikaConfig = context.get(TikaConfig.class);
              if (tikaConfig == null) {
                 tikaConfig = TikaConfig.getDefaultConfig();
              }
           }
           parser = tikaConfig.getParser();
        }

        // use a different metadata object
        // in order to specify the mime type of the
        // sub part without damaging the main metadata

        Metadata submd = new Metadata();
        submd.set(Metadata.CONTENT_TYPE, body.getMimeType());
        submd.set(Metadata.CONTENT_ENCODING, body.getCharset());

        try {
            BodyContentHandler bch = new BodyContentHandler(handler);
            parser.parse(is, new EmbeddedContentHandler(bch), submd, context);
        } catch (SAXException e) {
            throw new MimeException(e);
        } catch (TikaException e) {
            throw new MimeException(e);
        }
    }

    public void endBodyPart() throws MimeException {
        try {
            handler.endElement("p");
            handler.endElement("div");
        } catch (SAXException e) {
            throw new MimeException(e);
        }
    }

    public void endHeader() throws MimeException {
    }

    public void startMessage() throws MimeException {
        try {
            handler.startDocument();
        } catch (SAXException e) {
            throw new MimeException(e);
        }
    }

    public void endMessage() throws MimeException {
        try {
            handler.endDocument();
        } catch (SAXException e) {
            throw new MimeException(e);
        }
    }

    public void endMultipart() throws MimeException {
        inPart = false;
    }

    public void epilogue(InputStream is) throws MimeException, IOException {
    }

    /**
     * Header for the whole message or its parts
     * 
     * @see http://james.apache.org/mime4j/apidocs/org/apache/james/mime4j/parser/
     *      Field.html
     **/
    public void field(Field field) throws MimeException {
        // inPart indicates whether these metadata correspond to the
        // whole message or its parts
        if (inPart) {
            return;
        }

        try {
            String fieldname = field.getName();
            ParsedField parsedField = LenientFieldParser.getParser().parse(
                    field, DecodeMonitor.SILENT);
            if (fieldname.equalsIgnoreCase("From")) {
                MailboxListField fromField = (MailboxListField) parsedField;
                MailboxList mailboxList = fromField.getMailboxList();
                if (fromField.isValidField() && mailboxList != null) {
                    for (Address address : mailboxList) {
                        String from = getDisplayString(address);
                        metadata.add(Metadata.MESSAGE_FROM, from);
                        metadata.add(TikaCoreProperties.CREATOR, from);
                    }
                } else {
                    String from = stripOutFieldPrefix(field, "From:");
                    if (from.startsWith("<")) {
                        from = from.substring(1);
                    }
                    if (from.endsWith(">")) {
                        from = from.substring(0, from.length() - 1);
                    }
                    metadata.add(Metadata.MESSAGE_FROM, from);
                    metadata.add(TikaCoreProperties.CREATOR, from);
                }
            } else if (fieldname.equalsIgnoreCase("Subject")) {
                metadata.add(TikaCoreProperties.TRANSITION_SUBJECT_TO_DC_TITLE,
                        ((UnstructuredField) parsedField).getValue());
            } else if (fieldname.equalsIgnoreCase("To")) {
                processAddressList(parsedField, "To:", Metadata.MESSAGE_TO);
            } else if (fieldname.equalsIgnoreCase("CC")) {
                processAddressList(parsedField, "Cc:", Metadata.MESSAGE_CC);
            } else if (fieldname.equalsIgnoreCase("BCC")) {
                processAddressList(parsedField, "Bcc:", Metadata.MESSAGE_BCC);
            } else if (fieldname.equalsIgnoreCase("Date")) {
                DateTimeField dateField = (DateTimeField) parsedField;
                metadata.set(TikaCoreProperties.CREATED, dateField.getDate());
            }
        } catch (RuntimeException me) {
            if (strictParsing) {
                throw me;
            }
        }
    }

    private void processAddressList(ParsedField field, String addressListType,
            String metadataField) throws MimeException {
        AddressListField toField = (AddressListField) field;
        if (toField.isValidField()) {
            AddressList addressList = toField.getAddressList();
            for (Address address : addressList) {
                metadata.add(metadataField, getDisplayString(address));
            }
        } else {
            String to = stripOutFieldPrefix(field,
                    addressListType);
            for (String eachTo : to.split(",")) {
                metadata.add(metadataField, eachTo.trim());
            }
        }
    }

    private String getDisplayString(Address address) {
        if (address instanceof Mailbox) {
            Mailbox mailbox = (Mailbox) address;
            String name = mailbox.getName();
            if (name != null && name.length() > 0) {
                name = DecoderUtil.decodeEncodedWords(name, DecodeMonitor.SILENT);
                return name + " <" + mailbox.getAddress() + ">";
            } else {
                return mailbox.getAddress();
            }
        } else {
            return address.toString();
        }
    }

    public void preamble(InputStream is) throws MimeException, IOException {
    }

    public void raw(InputStream is) throws MimeException, IOException {
    }

    public void startBodyPart() throws MimeException {
        try {
            handler.startElement("div", "class", "email-entry");
            handler.startElement("p");
        } catch (SAXException e) {
            throw new MimeException(e);
        }
    }

    public void startHeader() throws MimeException {
        // TODO Auto-generated method stub

    }

    public void startMultipart(BodyDescriptor descr) throws MimeException {
        inPart = true;
    }

    private String stripOutFieldPrefix(Field field, String fieldname) {
        String temp = field.getRaw().toString();
        int loc = fieldname.length();
        while (temp.charAt(loc) ==' ') {
            loc++;
        }
        return temp.substring(loc);
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/mail/RFC822Parser.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.mail;

import java.io.IOException;
import java.io.InputStream;
import java.util.Collections;
import java.util.Set;

import org.apache.james.mime4j.MimeException;
import org.apache.james.mime4j.parser.MimeStreamParser;
import org.apache.james.mime4j.stream.MimeConfig;
import org.apache.tika.exception.TikaException;
import org.apache.tika.io.TaggedInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * Uses apache-mime4j to parse emails. Each part is treated with the
 * corresponding parser and displayed within elements.
 * <p>
 * A {@link MimeEntityConfig} object can be passed in the parsing context
 * to better control the parsing process.
 *
 * @author jnioche@digitalpebble.com
 */
public class RFC822Parser extends AbstractParser {

    /** Serial version UID */
    private static final long serialVersionUID = -5504243905998074168L;

    private static final Set<MediaType> SUPPORTED_TYPES = Collections
            .singleton(MediaType.parse("message/rfc822"));

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }

    public void parse(InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context) throws IOException,
            SAXException, TikaException {
        // Get the mime4j configuration, or use a default one
        MimeConfig config = new MimeConfig();
        config.setMaxLineLen(100000);
        config.setMaxHeaderLen(100000); // max length of any individual header
        config = context.get(MimeConfig.class, config);

        MimeStreamParser parser = new MimeStreamParser(config);
        XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);

        MailContentHandler mch = new MailContentHandler(
                xhtml, metadata, context, config.isStrictParsing());
        parser.setContentHandler(mch);
        parser.setContentDecoding(true);
        TaggedInputStream tagged = TaggedInputStream.get(stream);
        try {
            parser.parse(tagged);
        } catch (IOException e) {
            tagged.throwIfCauseOf(e);
            throw new TikaException("Failed to parse an email message", e);
        } catch (MimeException e) {
            // Unwrap the exception in case it was not thrown by mime4j
            Throwable cause = e.getCause();
            if (cause instanceof TikaException) {
                throw (TikaException) cause;
            } else if (cause instanceof SAXException) {
                throw (SAXException) cause;
            } else {
                throw new TikaException("Failed to parse an email message", e);
            }
        }
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/mat/MatParser.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.mat;

//JDK imports
import java.io.IOException;
import java.io.InputStream;
import java.util.Collections;
import java.util.Set;
import java.util.Map;

import org.apache.tika.exception.TikaException;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.mime.MediaType;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

//JMatIO imports
import com.jmatio.io.MatFileHeader;
import com.jmatio.io.MatFileReader;
import com.jmatio.types.MLArray;
import com.jmatio.types.MLStructure;


public class MatParser extends AbstractParser {

    public static final String MATLAB_MIME_TYPE =
            "application/x-matlab-data";

    private final Set<MediaType> SUPPORTED_TYPES =
            Collections.singleton(MediaType.application("x-matlab-data"));

    public Set<MediaType> getSupportedTypes(ParseContext context){
        return SUPPORTED_TYPES;
    }

    public void parse(InputStream stream, ContentHandler handler, Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {

        //Set MIME type as Matlab
        metadata.set(Metadata.CONTENT_TYPE, MATLAB_MIME_TYPE);

        try {
            // Use TIS so we can spool a temp file for parsing.
            TikaInputStream tis = TikaInputStream.get(stream);

            //Extract information from header file
            MatFileReader mfr = new MatFileReader(tis.getFile()); //input .mat file
            MatFileHeader hdr = mfr.getMatFileHeader(); //.mat header information

            // Example header: "MATLAB 5.0 MAT-file, Platform: MACI64, Created on: Sun Mar  2 23:41:57 2014"
            String[] parts = hdr.getDescription().split(","); // Break header information into its parts

            if (parts[2].contains("Created")) {
                int lastIndex1 = parts[2].lastIndexOf("Created on:");
                String dateCreated = parts[2].substring(lastIndex1 + "Created on:".length()).trim();
                metadata.set("createdOn", dateCreated);
            }

            if (parts[1].contains("Platform")) {
                int lastIndex2 = parts[1].lastIndexOf("Platform:");
                String platform = parts[1].substring(lastIndex2 + "Platform:".length()).trim();
                metadata.set("platform" , platform);
            }

            if (parts[0].contains("MATLAB")) {
                metadata.set("fileType", parts[0]);
            }

            // Get endian indicator from header file
            String endianBytes = new String(hdr.getEndianIndicator(), "UTF-8"); // Retrieve endian bytes and convert to string
            String endianCode = String.valueOf(endianBytes.toCharArray()); // Convert bytes to characters to string
            metadata.set("endian", endianCode);

            //Text output	
            XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);
            xhtml.startDocument();
            xhtml.newline();
            //Loop through each variable
            for (Map.Entry<String, MLArray> entry : mfr.getContent().entrySet()) {
                String varName = entry.getKey();
                MLArray varData = entry.getValue();

                xhtml.element("p", varName + ":" + String.valueOf(varData));

                // If the variable is a structure, extract variable info from structure
                if (varData.isStruct()){
                    MLStructure mlStructure = (MLStructure) mfr.getMLArray(varName);
                    xhtml.startElement("ul");
                    xhtml.newline();
                    for (MLArray element : mlStructure.getAllFields()){
                        xhtml.startElement("li");
                        xhtml.characters(String.valueOf(element));

                        // If there is an embedded structure, extract variable info.
                        if (element.isStruct()){
                            xhtml.startElement("ul");
                            // Should this actually be a recursive call?
                            xhtml.element("li", element.contentToString());
                            xhtml.endElement("ul");
                        }

                        xhtml.endElement("li");
                    }
                    xhtml.endElement("ul");
                }
            }
            xhtml.endDocument();
        } catch (IOException e) {
            throw new TikaException("Error parsing Matlab file with MatParser", e);
        }
    }
}"
tika-parsers/src/main/java/org/apache/tika/parser/mbox/MboxParser.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.mbox;

import java.io.BufferedReader;
import java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.Collections;
import java.util.Date;
import java.util.HashMap;
import java.util.LinkedList;
import java.util.Locale;
import java.util.Map;
import java.util.Queue;
import java.util.Set;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import org.apache.tika.exception.TikaException;
import org.apache.tika.extractor.EmbeddedDocumentExtractor;
import org.apache.tika.extractor.ParsingEmbeddedDocumentExtractor;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * Mbox (mailbox) parser. This version extracts each mail from Mbox and uses the
 * DelegatingParser to process each mail.
 */
public class MboxParser extends AbstractParser {

  /** Serial version UID */
  private static final long serialVersionUID = -1762689436731160661L;

  private static final Set<MediaType> SUPPORTED_TYPES = Collections.singleton(MediaType.application("mbox"));

  public static final String MBOX_MIME_TYPE = "application/mbox";
  public static final String MBOX_RECORD_DIVIDER = "From ";
  public static final int MAIL_MAX_SIZE = 50000000;

  private static final Pattern EMAIL_HEADER_PATTERN = Pattern.compile("([^ ]+):[ \t]*(.*)");
  private static final Pattern EMAIL_ADDRESS_PATTERN = Pattern.compile("<(.*@.*)>");

  private static final String EMAIL_HEADER_METADATA_PREFIX = "MboxParser-";
  private static final String EMAIL_FROMLINE_METADATA = EMAIL_HEADER_METADATA_PREFIX + "from";

  private boolean tracking = false;
  private final Map<Integer, Metadata> trackingMetadata = new HashMap<Integer, Metadata>();

  public Set<MediaType> getSupportedTypes(ParseContext context) {
    return SUPPORTED_TYPES;
  }

  public void parse(InputStream stream, ContentHandler handler, Metadata metadata, ParseContext context)
      throws IOException, TikaException, SAXException {

    EmbeddedDocumentExtractor extractor = context.get(EmbeddedDocumentExtractor.class,
        new ParsingEmbeddedDocumentExtractor(context));

    String charsetName = "windows-1252";

    metadata.set(Metadata.CONTENT_TYPE, MBOX_MIME_TYPE);
    metadata.set(Metadata.CONTENT_ENCODING, charsetName);

    XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);
    xhtml.startDocument();

    InputStreamReader isr = new InputStreamReader(stream, charsetName);
    BufferedReader reader = new BufferedReader(isr);
    try {
      String curLine = reader.readLine();
      int mailItem = 0;
      do {
        if (curLine.startsWith(MBOX_RECORD_DIVIDER)) {
          Metadata mailMetadata = new Metadata();
          Queue<String> multiline = new LinkedList<String>();
          mailMetadata.add(EMAIL_FROMLINE_METADATA, curLine.substring(MBOX_RECORD_DIVIDER.length()));
          mailMetadata.set(Metadata.CONTENT_TYPE, "message/rfc822");
          curLine = reader.readLine();

          ByteArrayOutputStream message = new ByteArrayOutputStream(100000);
          do {
            if (curLine.startsWith(" ") || curLine.startsWith("\t")) {
              String latestLine = multiline.poll();
              latestLine += " " + curLine.trim();
              multiline.add(latestLine);
            } else {
              multiline.add(curLine);
            }

            message.write(curLine.getBytes(charsetName));
            message.write(0x0A);
            curLine = reader.readLine();
          } while (curLine != null && !curLine.startsWith(MBOX_RECORD_DIVIDER) && message.size() < MAIL_MAX_SIZE);

          for (String item : multiline) {
            saveHeaderInMetadata(mailMetadata, item);
          }

          ByteArrayInputStream messageStream = new ByteArrayInputStream(message.toByteArray());
          message = null;

          if (extractor.shouldParseEmbedded(mailMetadata)) {
            extractor.parseEmbedded(messageStream, xhtml, mailMetadata, true);
          }

          if (tracking) {
            getTrackingMetadata().put(mailItem++, mailMetadata);
          }
        } else {
          curLine = reader.readLine();
        }

      } while (curLine != null && !Thread.currentThread().isInterrupted());

    } finally {
      reader.close();
    }

    xhtml.endDocument();
  }

  public static Date parseDate(String headerContent) throws ParseException {
    SimpleDateFormat dateFormat = new SimpleDateFormat("EEE, d MMM yyyy HH:mm:ss Z", Locale.US);
    return dateFormat.parse(headerContent);
  }

  public boolean isTracking() {
    return tracking;
  }

  public void setTracking(boolean tracking) {
    this.tracking = tracking;
  }

  public Map<Integer, Metadata> getTrackingMetadata() {
    return trackingMetadata;
  }

  private void saveHeaderInMetadata(Metadata metadata, String curLine) {
    Matcher headerMatcher = EMAIL_HEADER_PATTERN.matcher(curLine);
    if (!headerMatcher.matches()) {
      return; // ignore malformed header lines
    }

    String headerTag = headerMatcher.group(1).toLowerCase(Locale.ROOT);
    String headerContent = headerMatcher.group(2);

    if (headerTag.equalsIgnoreCase("From")) {
      metadata.set(TikaCoreProperties.CREATOR, headerContent);
    } else if (headerTag.equalsIgnoreCase("To") || headerTag.equalsIgnoreCase("Cc")
        || headerTag.equalsIgnoreCase("Bcc")) {
      Matcher address = EMAIL_ADDRESS_PATTERN.matcher(headerContent);
      if (address.find()) {
        metadata.add(Metadata.MESSAGE_RECIPIENT_ADDRESS, address.group(1));
      } else if (headerContent.indexOf('@') > -1) {
        metadata.add(Metadata.MESSAGE_RECIPIENT_ADDRESS, headerContent);
      }

      String property = Metadata.MESSAGE_TO;
      if (headerTag.equalsIgnoreCase("Cc")) {
        property = Metadata.MESSAGE_CC;
      } else if (headerTag.equalsIgnoreCase("Bcc")) {
        property = Metadata.MESSAGE_BCC;
      }
      metadata.add(property, headerContent);
    } else if (headerTag.equalsIgnoreCase("Subject")) {
      metadata.add(Metadata.SUBJECT, headerContent);
    } else if (headerTag.equalsIgnoreCase("Date")) {
      try {
        Date date = parseDate(headerContent);
        metadata.set(TikaCoreProperties.CREATED, date);
      } catch (ParseException e) {
        // ignoring date because format was not understood
      }
    } else if (headerTag.equalsIgnoreCase("Message-Id")) {
      metadata.set(TikaCoreProperties.IDENTIFIER, headerContent);
    } else if (headerTag.equalsIgnoreCase("In-Reply-To")) {
      metadata.set(TikaCoreProperties.RELATION, headerContent);
    } else if (headerTag.equalsIgnoreCase("Content-Type")) {
      // TODO - key off content-type in headers to
      // set mapping to use for content and convert if necessary.

      metadata.add(Metadata.CONTENT_TYPE, headerContent);
      metadata.set(TikaCoreProperties.FORMAT, headerContent);
    } else {
      metadata.add(EMAIL_HEADER_METADATA_PREFIX + headerTag, headerContent);
    }
  }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/mbox/OutlookPSTParser.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.mbox;

import static java.lang.String.valueOf;
import static java.util.Collections.singleton;

import java.io.ByteArrayInputStream;
import java.io.File;
import java.io.IOException;
import java.io.InputStream;
import java.util.Set;

import org.apache.tika.exception.TikaException;
import org.apache.tika.extractor.EmbeddedDocumentExtractor;
import org.apache.tika.extractor.ParsingEmbeddedDocumentExtractor;
import org.apache.tika.io.TemporaryResources;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.AttributesImpl;

import com.pff.PSTAttachment;
import com.pff.PSTFile;
import com.pff.PSTFolder;
import com.pff.PSTMessage;

/**
 * @author Tran Nam Quang
 * @author hong-thai.nguyen
 *
 */
public class OutlookPSTParser extends AbstractParser {

  private static final long serialVersionUID = 620998217748364063L;

  private static final MediaType MS_OUTLOOK_PST_MIMETYPE = MediaType.application("vnd.ms-outlook-pst");
  private static final Set<MediaType> SUPPORTED_TYPES = singleton(MS_OUTLOOK_PST_MIMETYPE);

  public Set<MediaType> getSupportedTypes(ParseContext context) {
    return SUPPORTED_TYPES;
  }

  public void parse(InputStream stream, ContentHandler handler, Metadata metadata, ParseContext context)
      throws IOException, SAXException, TikaException {

    // Use the delegate parser to parse the contained document
    EmbeddedDocumentExtractor embeddedExtractor = context.get(EmbeddedDocumentExtractor.class,
        new ParsingEmbeddedDocumentExtractor(context));

    metadata.set(Metadata.CONTENT_TYPE, MS_OUTLOOK_PST_MIMETYPE.toString());

    XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);
    xhtml.startDocument();

    TikaInputStream in = TikaInputStream.get(stream);
    PSTFile pstFile = null;
    try {
      pstFile = new PSTFile(in.getFile().getPath());
      metadata.set(Metadata.CONTENT_LENGTH, valueOf(pstFile.getFileHandle().length()));
      boolean isValid = pstFile.getFileHandle().getFD().valid();
      metadata.set("isValid", valueOf(isValid));
      if (isValid) {
        parseFolder(xhtml, pstFile.getRootFolder(), embeddedExtractor);
      }
    } catch (Exception e) {
      throw new TikaException(e.getMessage(), e);
    } finally {
      if (pstFile != null && pstFile.getFileHandle() != null) {
        try{
          pstFile.getFileHandle().close();
        } catch (IOException e) {
          //swallow closing exception
        }
      }
    }

    xhtml.endDocument();
  }

  private void parseFolder(XHTMLContentHandler handler, PSTFolder pstFolder, EmbeddedDocumentExtractor embeddedExtractor)
      throws Exception {
    if (pstFolder.getContentCount() > 0) {
      PSTMessage pstMail = (PSTMessage) pstFolder.getNextChild();
      while (pstMail != null) {
        AttributesImpl attributes = new AttributesImpl();
        attributes.addAttribute("", "class", "class", "CDATA", "embedded");
        attributes.addAttribute("", "id", "id", "CDATA", pstMail.getInternetMessageId());
        handler.startElement("div", attributes);
        handler.element("h1", pstMail.getSubject());

        parserMailItem(handler, pstMail, embeddedExtractor);
        parseMailAttachments(handler, pstMail, embeddedExtractor);

        handler.endElement("div");

        pstMail = (PSTMessage) pstFolder.getNextChild();
      }
    }

    if (pstFolder.hasSubfolders()) {
      for (PSTFolder pstSubFolder : pstFolder.getSubFolders()) {
        handler.startElement("div", createAttribute("class", "email-folder"));
        handler.element("h1", pstSubFolder.getDisplayName());
        parseFolder(handler, pstSubFolder, embeddedExtractor);
        handler.endElement("div");
      }
    }
  }

  private void parserMailItem(XHTMLContentHandler handler, PSTMessage pstMail, EmbeddedDocumentExtractor embeddedExtractor) throws SAXException, IOException {
    Metadata mailMetadata = new Metadata();
    mailMetadata.set(Metadata.RESOURCE_NAME_KEY, pstMail.getInternetMessageId());
    mailMetadata.set(Metadata.EMBEDDED_RELATIONSHIP_ID, pstMail.getInternetMessageId());
    mailMetadata.set(TikaCoreProperties.IDENTIFIER, pstMail.getInternetMessageId());
    mailMetadata.set(TikaCoreProperties.TITLE, pstMail.getSubject());
    mailMetadata.set(Metadata.MESSAGE_FROM, pstMail.getSenderName());
    mailMetadata.set(TikaCoreProperties.CREATOR, pstMail.getSenderName());
    mailMetadata.set(TikaCoreProperties.CREATED, pstMail.getCreationTime());
    mailMetadata.set(TikaCoreProperties.MODIFIED, pstMail.getLastModificationTime());
    mailMetadata.set(TikaCoreProperties.COMMENTS, pstMail.getComment());
    mailMetadata.set("descriptorNodeId", valueOf(pstMail.getDescriptorNodeId()));
    mailMetadata.set("senderEmailAddress", pstMail.getSenderEmailAddress());
    mailMetadata.set("recipients", pstMail.getRecipientsString());
    mailMetadata.set("displayTo", pstMail.getDisplayTo());
    mailMetadata.set("displayCC", pstMail.getDisplayCC());
    mailMetadata.set("displayBCC", pstMail.getDisplayBCC());
    mailMetadata.set("importance", valueOf(pstMail.getImportance()));
    mailMetadata.set("priority", valueOf(pstMail.getPriority()));
    mailMetadata.set("flagged", valueOf(pstMail.isFlagged()));

    byte[] mailContent = pstMail.getBody().getBytes("UTF-8");
    embeddedExtractor.parseEmbedded(new ByteArrayInputStream(mailContent), handler, mailMetadata, true);
  }


  private static AttributesImpl createAttribute(String attName, String attValue) {
    AttributesImpl attributes = new AttributesImpl();
    attributes.addAttribute("", attName, attName, "CDATA", attValue);
    return attributes;
  }

  private void parseMailAttachments(XHTMLContentHandler xhtml, PSTMessage email, EmbeddedDocumentExtractor embeddedExtractor)
      throws TikaException {
    int numberOfAttachments = email.getNumberOfAttachments();
    for (int i = 0; i < numberOfAttachments; i++) {
      File tempFile = null;
      try {
        PSTAttachment attach = email.getAttachment(i);

        // Get the filename; both long and short filenames can be used for attachments
        String filename = attach.getLongFilename();
        if (filename.isEmpty()) {
          filename = attach.getFilename();
        }

        xhtml.element("p", filename);

        Metadata attachMeta = new Metadata();
        attachMeta.set(Metadata.RESOURCE_NAME_KEY, filename);
        attachMeta.set(Metadata.EMBEDDED_RELATIONSHIP_ID, filename);
        AttributesImpl attributes = new AttributesImpl();
        attributes.addAttribute("", "class", "class", "CDATA", "embedded");
        attributes.addAttribute("", "id", "id", "CDATA", filename);
        xhtml.startElement("div", attributes);
        if (embeddedExtractor.shouldParseEmbedded(attachMeta)) {
          TemporaryResources tmp = new TemporaryResources();
          try {
            TikaInputStream tis = TikaInputStream.get(attach.getFileInputStream(), tmp);
            embeddedExtractor.parseEmbedded(tis, xhtml, attachMeta, true);
          } finally {
            tmp.dispose();
          }
        }
        xhtml.endElement("div");

      } catch (Exception e) {
        throw new TikaException("Unable to unpack document stream", e);
      } finally {
        if (tempFile != null)
          tempFile.delete();
      }
    }
  }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/microsoft/AbstractPOIFSExtractor.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.microsoft;

import java.io.FileNotFoundException;
import java.io.IOException;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.poi.poifs.filesystem.DirectoryEntry;
import org.apache.poi.poifs.filesystem.DirectoryNode;
import org.apache.poi.poifs.filesystem.DocumentEntry;
import org.apache.poi.poifs.filesystem.DocumentInputStream;
import org.apache.poi.poifs.filesystem.Entry;
import org.apache.poi.poifs.filesystem.Ole10Native;
import org.apache.poi.poifs.filesystem.Ole10NativeException;
import org.apache.tika.config.TikaConfig;
import org.apache.tika.detect.Detector;
import org.apache.tika.exception.TikaException;
import org.apache.tika.extractor.EmbeddedDocumentExtractor;
import org.apache.tika.extractor.ParsingEmbeddedDocumentExtractor;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.mime.MimeType;
import org.apache.tika.mime.MimeTypeException;
import org.apache.tika.mime.MimeTypes;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.PasswordProvider;
import org.apache.tika.parser.microsoft.OfficeParser.POIFSDocumentType;
import org.apache.tika.parser.pkg.ZipContainerDetector;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.SAXException;

abstract class AbstractPOIFSExtractor {
    private final EmbeddedDocumentExtractor extractor;
    private PasswordProvider passwordProvider;
    private TikaConfig tikaConfig;
    private MimeTypes mimeTypes;
    private Detector detector;
    private Metadata metadata;
    private static final Log logger = LogFactory.getLog(AbstractPOIFSExtractor.class);

    protected AbstractPOIFSExtractor(ParseContext context) {
        this(context, null);
    }
    protected AbstractPOIFSExtractor(ParseContext context, Metadata metadata) {
        EmbeddedDocumentExtractor ex = context.get(EmbeddedDocumentExtractor.class);

        if (ex==null) {
            this.extractor = new ParsingEmbeddedDocumentExtractor(context);
        } else {
            this.extractor = ex;
        }
        
        this.passwordProvider = context.get(PasswordProvider.class);
        this.tikaConfig = context.get(TikaConfig.class);
        this.mimeTypes = context.get(MimeTypes.class);
        this.detector = context.get(Detector.class);
        this.metadata = metadata;
    }
    
    // Note - these cache, but avoid creating the default TikaConfig if not needed
    protected TikaConfig getTikaConfig() {
       if (tikaConfig == null) {
          tikaConfig = TikaConfig.getDefaultConfig();
       }
       return tikaConfig;
    }
    protected Detector getDetector() {
       if (detector != null) return detector;
       
       detector = getTikaConfig().getDetector();
       return detector;
    }
    protected MimeTypes getMimeTypes() {
       if (mimeTypes != null) return mimeTypes;
       
       mimeTypes = getTikaConfig().getMimeRepository();
       return mimeTypes;
    }
    
    /**
     * Returns the password to be used for this file, or null
     *  if no / default password should be used
     */
    protected String getPassword() {
        if (passwordProvider != null) {
            return passwordProvider.getPassword(metadata);
        }
        return null;
    }
    
    protected void handleEmbeddedResource(TikaInputStream resource, String filename,
                                          String relationshipID, String mediaType, XHTMLContentHandler xhtml,
                                          boolean outputHtml)
          throws IOException, SAXException, TikaException {
       try {
           Metadata metadata = new Metadata();
           if(filename != null) {
               metadata.set(Metadata.TIKA_MIME_FILE, filename);
               metadata.set(Metadata.RESOURCE_NAME_KEY, filename);
           }
           if (relationshipID != null) {
               metadata.set(Metadata.EMBEDDED_RELATIONSHIP_ID, relationshipID);
           }
           if(mediaType != null) {
               metadata.set(Metadata.CONTENT_TYPE, mediaType);
           }

           if (extractor.shouldParseEmbedded(metadata)) {
               extractor.parseEmbedded(resource, xhtml, metadata, outputHtml);
           }
       } finally {
           resource.close();
       }
    }

    /**
     * Handle an office document that's embedded at the POIFS level
     */
    protected void handleEmbeddedOfficeDoc(
            DirectoryEntry dir, XHTMLContentHandler xhtml)
            throws IOException, SAXException, TikaException {

        // Is it an embedded OLE2 document, or an embedded OOXML document?

        if (dir.hasEntry("Package")) {
            // It's OOXML (has a ZipFile):
            Entry ooxml = dir.getEntry("Package");

            TikaInputStream stream = TikaInputStream.get(
                    new DocumentInputStream((DocumentEntry) ooxml));
            try {
                ZipContainerDetector detector = new ZipContainerDetector();
                MediaType type = detector.detect(stream, new Metadata());
                handleEmbeddedResource(stream, null, dir.getName(), type.toString(), xhtml, true);
                return;
            } finally {
                stream.close();
            }
        }

        // It's regular OLE2:

        // What kind of document is it?
        Metadata metadata = new Metadata();
        metadata.set(Metadata.EMBEDDED_RELATIONSHIP_ID, dir.getName());
        POIFSDocumentType type = POIFSDocumentType.detectType(dir);
        TikaInputStream embedded = null;

        try {
            if (type == POIFSDocumentType.OLE10_NATIVE) {
                try {
                    // Try to un-wrap the OLE10Native record:
                    Ole10Native ole = Ole10Native.createFromEmbeddedOleObject((DirectoryNode)dir);
                    if (ole.getLabel() != null) {
                        metadata.set(Metadata.RESOURCE_NAME_KEY, dir.getName() + '/' + ole.getLabel());
                    }
                    byte[] data = ole.getDataBuffer();
                    embedded = TikaInputStream.get(data);
                } catch (Ole10NativeException ex) {
                    // Not a valid OLE10Native record, skip it
                } catch (Exception e) {
                    logger.warn("Ignoring unexpected exception while parsing possible OLE10_NATIVE embedded document " + dir.getName(), e);
                }
            } else if (type == POIFSDocumentType.COMP_OBJ) {
                try {
                   // Grab the contents and process
                   DocumentEntry contentsEntry;
                   try {
                     contentsEntry = (DocumentEntry)dir.getEntry("CONTENTS");
                   } catch (FileNotFoundException ioe) {
                     contentsEntry = (DocumentEntry)dir.getEntry("Contents");
                   }
                   DocumentInputStream inp = new DocumentInputStream(contentsEntry);
                   byte[] contents = new byte[contentsEntry.getSize()];
                   inp.readFully(contents);
                   embedded = TikaInputStream.get(contents);
                   
                   // Try to work out what it is
                   MediaType mediaType = getDetector().detect(embedded, new Metadata());
                   String extension = type.getExtension();
                   try {
                      MimeType mimeType = getMimeTypes().forName(mediaType.toString());
                      extension = mimeType.getExtension();
                   } catch(MimeTypeException mte) {
                      // No details on this type are known
                   }
                   
                   // Record what we can do about it
                   metadata.set(Metadata.CONTENT_TYPE, mediaType.getType().toString());
                   metadata.set(Metadata.RESOURCE_NAME_KEY, dir.getName() + extension);
                } catch(Exception e) {
                   throw new TikaException("Invalid embedded resource", e);
                }
            } else {
                metadata.set(Metadata.CONTENT_TYPE, type.getType().toString());
                metadata.set(Metadata.RESOURCE_NAME_KEY, dir.getName() + '.' + type.getExtension());
            }

            // Should we parse it?
            if (extractor.shouldParseEmbedded(metadata)) {
                if (embedded == null) {
                    // Make a TikaInputStream that just
                    // passes the root directory of the
                    // embedded document, and is otherwise
                    // empty (byte[0]):
                    embedded = TikaInputStream.get(new byte[0]);
                    embedded.setOpenContainer(dir);
                }
                extractor.parseEmbedded(embedded, xhtml, metadata, true);
            }
        } finally {
            if (embedded != null) {
                embedded.close();
            }
        }
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/microsoft/Cell.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.microsoft;

import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.SAXException;

/**
 * Cell of content. Classes that implement this interface are used by
 * Tika parsers (currently just the MS Excel parser) to keep track of
 * individual pieces of content before they are rendered to the XHTML
 * SAX event stream.
 */
public interface Cell {

    /**
     * Renders the content to the given XHTML SAX event stream.
     *
     * @param handler
     * @throws SAXException
     */
    void render(XHTMLContentHandler handler) throws SAXException;

}
"
tika-parsers/src/main/java/org/apache/tika/parser/microsoft/CellDecorator.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.microsoft;

import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.SAXException;

/**
 * Cell decorator.
 */
public class CellDecorator implements Cell {

    private final Cell cell;

    public CellDecorator(Cell cell) {
        this.cell = cell;
    }

    public void render(XHTMLContentHandler handler) throws SAXException {
        cell.render(handler);
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ExcelExtractor.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.microsoft;

import java.awt.Point;
import java.io.IOException;
import java.text.NumberFormat;
import java.util.ArrayList;
import java.util.Comparator;
import java.util.List;
import java.util.Locale;
import java.util.Map;
import java.util.SortedMap;
import java.util.TreeMap;

import org.apache.poi.ddf.EscherBSERecord;
import org.apache.poi.ddf.EscherBlipRecord;
import org.apache.poi.ddf.EscherRecord;
import org.apache.poi.hssf.eventusermodel.FormatTrackingHSSFListener;
import org.apache.poi.hssf.eventusermodel.HSSFEventFactory;
import org.apache.poi.hssf.eventusermodel.HSSFListener;
import org.apache.poi.hssf.eventusermodel.HSSFRequest;
import org.apache.poi.hssf.extractor.OldExcelExtractor;
import org.apache.poi.hssf.record.BOFRecord;
import org.apache.poi.hssf.record.BoundSheetRecord;
import org.apache.poi.hssf.record.CellValueRecordInterface;
import org.apache.poi.hssf.record.CountryRecord;
import org.apache.poi.hssf.record.DateWindow1904Record;
import org.apache.poi.hssf.record.DrawingGroupRecord;
import org.apache.poi.hssf.record.EOFRecord;
import org.apache.poi.hssf.record.ExtendedFormatRecord;
import org.apache.poi.hssf.record.FormatRecord;
import org.apache.poi.hssf.record.FormulaRecord;
import org.apache.poi.hssf.record.HyperlinkRecord;
import org.apache.poi.hssf.record.LabelRecord;
import org.apache.poi.hssf.record.LabelSSTRecord;
import org.apache.poi.hssf.record.NumberRecord;
import org.apache.poi.hssf.record.RKRecord;
import org.apache.poi.hssf.record.Record;
import org.apache.poi.hssf.record.SSTRecord;
import org.apache.poi.hssf.record.StringRecord;
import org.apache.poi.hssf.record.TextObjectRecord;
import org.apache.poi.hssf.record.chart.SeriesTextRecord;
import org.apache.poi.hssf.record.common.UnicodeString;
import org.apache.poi.hssf.record.crypto.Biff8EncryptionKey;
import org.apache.poi.hssf.usermodel.HSSFPictureData;
import org.apache.poi.poifs.filesystem.DirectoryEntry;
import org.apache.poi.poifs.filesystem.DirectoryNode;
import org.apache.poi.poifs.filesystem.DocumentInputStream;
import org.apache.poi.poifs.filesystem.Entry;
import org.apache.poi.poifs.filesystem.NPOIFSFileSystem;
import org.apache.tika.exception.EncryptedDocumentException;
import org.apache.tika.exception.TikaException;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.SAXException;

/**
 * Excel parser implementation which uses POI's Event API
 * to handle the contents of a Workbook.
 * <p>
 * The Event API uses a much smaller memory footprint than
 * <code>HSSFWorkbook</code> when processing excel files
 * but at the cost of more complexity.
 * <p>
 * With the Event API a <i>listener</i> is registered for
 * specific record types and those records are created,
 * fired off to the listener and then discarded as the stream
 * is being processed.
 *
 * @see org.apache.poi.hssf.eventusermodel.HSSFListener
 * @see <a href="http://poi.apache.org/hssf/how-to.html#event_api">
 * POI Event API How To</a>
 */
public class ExcelExtractor extends AbstractPOIFSExtractor {

    /**
     * <code>true</code> if the HSSFListener should be registered
     * to listen for all records or <code>false</code> (the default)
     * if the listener should be configured to only receive specified
     * records.
     */
    private boolean listenForAllRecords = false;
    
    private static final String WORKBOOK_ENTRY = "Workbook";
    private static final String BOOK_ENTRY = "Book";

    public ExcelExtractor(ParseContext context, Metadata metadata) {
        super(context, metadata);
    }

    /**
     * Returns <code>true</code> if this parser is configured to listen
     * for all records instead of just the specified few.
     */
    public boolean isListenForAllRecords() {
        return listenForAllRecords;
    }

    /**
     * Specifies whether this parser should to listen for all
     * records or just for the specified few.
     * <p>
     * <strong>Note:</strong> Under normal operation this setting should
     * be <code>false</code> (the default), but you can experiment with
     * this setting for testing and debugging purposes.
     *
     * @param listenForAllRecords <code>true</code> if the HSSFListener
     * should be registered to listen for all records or <code>false</code>
     * if the listener should be configured to only receive specified records.
     */
    public void setListenForAllRecords(boolean listenForAllRecords) {
        this.listenForAllRecords = listenForAllRecords;
    }

    /**
     * Extracts text from an Excel Workbook writing the extracted content
     * to the specified {@link Appendable}.
     *
     * @param filesystem POI file system
     * @throws IOException if an error occurs processing the workbook
     * or writing the extracted content
     */
    protected void parse(
            NPOIFSFileSystem filesystem, XHTMLContentHandler xhtml,
            Locale locale) throws IOException, SAXException, TikaException {
        parse(filesystem.getRoot(), xhtml, locale);
    }

    protected void parse(
            DirectoryNode root, XHTMLContentHandler xhtml,
            Locale locale) throws IOException, SAXException, TikaException {
        if (! root.hasEntry(WORKBOOK_ENTRY)) {
            if (root.hasEntry(BOOK_ENTRY)) {
                // Excel 5 / Excel 95 file
                // Records are in a different structure so needs a
                //  different parser to process them
                OldExcelExtractor extractor = new OldExcelExtractor(root);
                OldExcelParser.parse(extractor, xhtml);
                return;
            } else {
               // Corrupt file / very old file, just skip text extraction
               return;
            }
        }
        
        // If a password was supplied, use it, otherwise the default
        Biff8EncryptionKey.setCurrentUserPassword(getPassword());
       
        // Have the file processed in event mode
        TikaHSSFListener listener = new TikaHSSFListener(xhtml, locale, this);
        listener.processFile(root, isListenForAllRecords());
        listener.throwStoredException();

        for (Entry entry : root) {
            if (entry.getName().startsWith("MBD")
                    && entry instanceof DirectoryEntry) {
                try {
                    handleEmbeddedOfficeDoc((DirectoryEntry) entry, xhtml);
                } catch (TikaException e) {
                    // ignore parse errors from embedded documents
                }
            }
         }
    }

    // ======================================================================

    /**
     * HSSF Listener implementation which processes the HSSF records.
     */
    private static class TikaHSSFListener implements HSSFListener {

        /**
         * XHTML content handler to which the document content is rendered.
         */
        private final XHTMLContentHandler handler;
        
        /**
         * The POIFS Extractor, used for embeded resources.
         */
        private final AbstractPOIFSExtractor extractor;

        /**
         * Potential exception thrown by the content handler. When set to
         * non-<code>null</code>, causes all subsequent HSSF records to be
         * ignored and the stored exception to be thrown when
         * {@link #throwStoredException()} is invoked.
         */
        private Exception exception = null;

        private SSTRecord sstRecord;
        private FormulaRecord stringFormulaRecord;
        
        private short previousSid;

        /**
         * Internal <code>FormatTrackingHSSFListener</code> to handle cell
         * formatting within the extraction.
         */
        private FormatTrackingHSSFListener formatListener;

        /**
         * List of worksheet names.
         */
        private List<String> sheetNames = new ArrayList<String>();

        /**
         * Index of the current worksheet within the workbook.
         * Used to find the worksheet name in the {@link #sheetNames} list.
         */
        private short currentSheetIndex;

        /**
         * Content of the current worksheet, or <code>null</code> if no
         * worksheet is currently active.
         */
        private SortedMap<Point, Cell> currentSheet = null;
        
        /**
         * Extra text or cells that crops up, typically as part of a
         *  worksheet but not always.
         */
        private List<Cell> extraTextCells = new ArrayList<Cell>();

        /**
         * Format for rendering numbers in the worksheet. Currently we just
         * use the platform default formatting.
         *
         * @see <a href="https://issues.apache.org/jira/browse/TIKA-103">TIKA-103</a>
         */
        private final NumberFormat format;
        
        /**
         * These aren't complete when we first see them, as the
         *  depend on continue records that aren't always
         *  contiguous. Collect them for later processing.
         */
        private List<DrawingGroupRecord> drawingGroups = new ArrayList<DrawingGroupRecord>();

        /**
         * Construct a new listener instance outputting parsed data to
         * the specified XHTML content handler.
         *
         * @param handler Destination to write the parsed output to
         */
        private TikaHSSFListener(XHTMLContentHandler handler, Locale locale, AbstractPOIFSExtractor extractor) {
            this.handler = handler;
            this.extractor = extractor;
            this.format = NumberFormat.getInstance(locale);
            this.formatListener = new FormatTrackingHSSFListener(this, locale);
        }

        /**
         * Entry point to listener to start the processing of a file.
         *
         * @param filesystem POI file system.
         * @param listenForAllRecords sets whether the listener is configured to listen
         * for all records types or not.
         * @throws IOException on any IO errors.
         * @throws SAXException on any SAX parsing errors.
         */
    	public void processFile(NPOIFSFileSystem filesystem, boolean listenForAllRecords)
    		throws IOException, SAXException, TikaException {
            processFile(filesystem.getRoot(), listenForAllRecords);
        }

    	public void processFile(DirectoryNode root, boolean listenForAllRecords)
    		throws IOException, SAXException, TikaException {

    		// Set up listener and register the records we want to process
            HSSFRequest hssfRequest = new HSSFRequest();
            if (listenForAllRecords) {
                hssfRequest.addListenerForAllRecords(formatListener);
            } else {
                hssfRequest.addListener(formatListener, BOFRecord.sid);
                hssfRequest.addListener(formatListener, EOFRecord.sid);
                hssfRequest.addListener(formatListener, DateWindow1904Record.sid);
                hssfRequest.addListener(formatListener, CountryRecord.sid);
                hssfRequest.addListener(formatListener, BoundSheetRecord.sid);
                hssfRequest.addListener(formatListener, SSTRecord.sid);
                hssfRequest.addListener(formatListener, FormulaRecord.sid);
                hssfRequest.addListener(formatListener, LabelRecord.sid);
                hssfRequest.addListener(formatListener, LabelSSTRecord.sid);
                hssfRequest.addListener(formatListener, NumberRecord.sid);
                hssfRequest.addListener(formatListener, RKRecord.sid);
                hssfRequest.addListener(formatListener, StringRecord.sid);
                hssfRequest.addListener(formatListener, HyperlinkRecord.sid);
                hssfRequest.addListener(formatListener, TextObjectRecord.sid);
                hssfRequest.addListener(formatListener, SeriesTextRecord.sid);
                hssfRequest.addListener(formatListener, FormatRecord.sid);
                hssfRequest.addListener(formatListener, ExtendedFormatRecord.sid);
                hssfRequest.addListener(formatListener, DrawingGroupRecord.sid);
            }

            // Create event factory and process Workbook (fire events)
            DocumentInputStream documentInputStream = root.createDocumentInputStream(WORKBOOK_ENTRY);
            HSSFEventFactory eventFactory = new HSSFEventFactory();
            try {
                eventFactory.processEvents(hssfRequest, documentInputStream);
            } catch (org.apache.poi.EncryptedDocumentException e) {
                throw new EncryptedDocumentException(e);
            }
            
            // Output any extra text that came after all the sheets
            processExtraText(); 
            
            // Look for embeded images, now that the drawing records
            //  have been fully matched with their continue data
            for(DrawingGroupRecord dgr : drawingGroups) {
               dgr.decode();
               findPictures(dgr.getEscherRecords());
            }
    	}

        /**
         * Process a HSSF record.
         *
         * @param record HSSF Record
         */
        public void processRecord(Record record) {
            if (exception == null) {
                try {
                    internalProcessRecord(record);
                } catch (TikaException te) {
                   exception = te;
                } catch (IOException ie) {
                    exception = ie;
                } catch (SAXException se) {
                    exception = se;
                }
            }
        }

        public void throwStoredException() throws TikaException, SAXException, IOException {
            if (exception != null) {
                if(exception instanceof IOException)
                   throw (IOException)exception;
                if(exception instanceof SAXException)
                   throw (SAXException)exception;
                if(exception instanceof TikaException)
                   throw (TikaException)exception;
                throw new TikaException(exception.getMessage());
            }
        }

        private void internalProcessRecord(Record record) throws SAXException, TikaException, IOException {
            switch (record.getSid()) {
            case BOFRecord.sid: // start of workbook, worksheet etc. records
                BOFRecord bof = (BOFRecord) record;
                if (bof.getType() == BOFRecord.TYPE_WORKBOOK) {
                    currentSheetIndex = -1;
                } else if (bof.getType() == BOFRecord.TYPE_CHART) {
                    if(previousSid == EOFRecord.sid) {
                        // This is a sheet which contains only a chart
                        newSheet();
                    } else {
                        // This is a chart within a normal sheet
                        // Handling of this is a bit hacky...
                        if (currentSheet != null) {
                            processSheet();
                            currentSheetIndex--;
                            newSheet();
                        }
                    }
                } else if (bof.getType() == BOFRecord.TYPE_WORKSHEET) {
                    newSheet();
                }
                break;

            case EOFRecord.sid: // end of workbook, worksheet etc. records
                if (currentSheet != null) {
                    processSheet();
                }
                currentSheet = null;
                break;

            case BoundSheetRecord.sid: // Worksheet index record
                BoundSheetRecord boundSheetRecord = (BoundSheetRecord) record;
                sheetNames.add(boundSheetRecord.getSheetname());
                break;

            case SSTRecord.sid: // holds all the strings for LabelSSTRecords
                sstRecord = (SSTRecord) record;
                break;

            case FormulaRecord.sid: // Cell value from a formula
                FormulaRecord formula = (FormulaRecord) record;
                if (formula.hasCachedResultString()) {
                   // The String itself should be the next record
                   stringFormulaRecord = formula;
                } else {
                   addTextCell(record, formatListener.formatNumberDateCell(formula));
                }
                break;
                
            case StringRecord.sid:
                if (previousSid == FormulaRecord.sid) {
                   // Cached string value of a string formula
                   StringRecord sr = (StringRecord) record;
                   addTextCell(stringFormulaRecord, sr.getString());
                } else {
                   // Some other string not associated with a cell, skip
                }
                break;

            case LabelRecord.sid: // strings stored directly in the cell
                LabelRecord label = (LabelRecord) record;
                addTextCell(record, label.getValue());
                break;

            case LabelSSTRecord.sid: // Ref. a string in the shared string table
                LabelSSTRecord sst = (LabelSSTRecord) record;
                UnicodeString unicode = sstRecord.getString(sst.getSSTIndex());
                addTextCell(record, unicode.getString());
                break;

            case NumberRecord.sid: // Contains a numeric cell value
                NumberRecord number = (NumberRecord) record;
                addTextCell(record, formatListener.formatNumberDateCell(number));
                break;

            case RKRecord.sid: // Excel internal number record
                RKRecord rk = (RKRecord) record;
                addCell(record, new NumberCell(rk.getRKNumber(), format));
                break;

            case HyperlinkRecord.sid: // holds a URL associated with a cell
                if (currentSheet != null) {
                    HyperlinkRecord link = (HyperlinkRecord) record;
                    Point point =
                        new Point(link.getFirstColumn(), link.getFirstRow());
                    Cell cell = currentSheet.get(point);
                    if (cell != null) {
                        String address = link.getAddress();
                        if (address != null) {
                            addCell(record, new LinkedCell(cell, address));
                        } else {
                            addCell(record, cell);
                        }
                    }
                }
                break;

            case TextObjectRecord.sid:
                TextObjectRecord tor = (TextObjectRecord) record;
                addTextCell(record, tor.getStr().getString());
                break;

            case SeriesTextRecord.sid: // Chart label or title
                SeriesTextRecord str = (SeriesTextRecord) record;
                addTextCell(record, str.getText());
                break;

            case DrawingGroupRecord.sid:
               // Collect this now, we'll process later when all
               //  the continue records are in
               drawingGroups.add( (DrawingGroupRecord)record );
               break;

            }

            previousSid = record.getSid();
            
            if (stringFormulaRecord != record) {
               stringFormulaRecord = null;
            }
        }

        private void processExtraText() throws SAXException {
            if(extraTextCells.size() > 0) {
                for(Cell cell : extraTextCells) {
                    handler.startElement("div", "class", "outside");
                    cell.render(handler);
                    handler.endElement("div");
                }

                // Reset
                extraTextCells.clear();
            }
        }

        /**
         * Adds the given cell (unless <code>null</code>) to the current
         * worksheet (if any) at the position (if any) of the given record.
         *
         * @param record record that holds the cell value
         * @param cell cell value (or <code>null</code>)
         */
        private void addCell(Record record, Cell cell) throws SAXException {
            if (cell == null) {
                // Ignore empty cells
            } else if (currentSheet != null
                    && record instanceof CellValueRecordInterface) {
                // Normal cell inside a worksheet
                CellValueRecordInterface value =
                    (CellValueRecordInterface) record;
                Point point = new Point(value.getColumn(), value.getRow());
                currentSheet.put(point, cell);
            } else {
                // Cell outside the worksheets
                extraTextCells.add(cell);
            }
        }

        /**
         * Adds a text cell with the given text comment. The given text
         * is trimmed, and ignored if <code>null</code> or empty.
         *
         * @param record record that holds the text value
         * @param text text content, may be <code>null</code>
         * @throws SAXException
         */
        private void addTextCell(Record record, String text) throws SAXException {
            if (text != null) {
                text = text.trim();
                if (text.length() > 0) {
                    addCell(record, new TextCell(text));
                }
            }
        }

        private void newSheet() {
            currentSheetIndex++;
            currentSheet = new TreeMap<Point, Cell>(new PointComparator());
        }

        /**
         * Process an excel sheet.
         *
         * @throws SAXException if an error occurs
         */
        private void processSheet() throws SAXException {
            // Sheet Start
            handler.startElement("div", "class", "page");
            if (currentSheetIndex < sheetNames.size()) {
                handler.element("h1", sheetNames.get(currentSheetIndex));
            }
            handler.startElement("table");
            handler.startElement("tbody");

            // Process Rows
            int currentRow = 0;
            int currentColumn = 0;
            handler.startElement("tr");
            handler.startElement("td");
            for (Map.Entry<Point, Cell> entry : currentSheet.entrySet()) {
                while (currentRow < entry.getKey().y) {
                    handler.endElement("td");
                    handler.endElement("tr");
                    handler.startElement("tr");
                    handler.startElement("td");
                    currentRow++;
                    currentColumn = 0;
                }

                while (currentColumn < entry.getKey().x) {
                    handler.endElement("td");
                    handler.startElement("td");
                    currentColumn++;
                }

                entry.getValue().render(handler);
            }
            handler.endElement("td");
            handler.endElement("tr");

            // Sheet End
            handler.endElement("tbody");
            handler.endElement("table");
            
            // Finish up
            processExtraText();
            handler.endElement("div");
        }

        private void findPictures(List<EscherRecord> records) throws IOException, SAXException, TikaException {
           for(EscherRecord escherRecord : records) {
              if (escherRecord instanceof EscherBSERecord) {
                 EscherBlipRecord blip = ((EscherBSERecord) escherRecord).getBlipRecord();
                 if (blip != null) {
                    HSSFPictureData picture = new HSSFPictureData(blip);
                    String mimeType = picture.getMimeType();
                    TikaInputStream stream = TikaInputStream.get(picture.getData());
                    
                    // Handle the embeded resource
                    extractor.handleEmbeddedResource(
                          stream, null, null, mimeType,
                          handler, true
                    );
                 }
              }

              // Recursive call.
              findPictures(escherRecord.getChildRecords());
           }
        }
    }

    /**
     * Utility comparator for points.
     */
    private static class PointComparator implements Comparator<Point> {

        public int compare(Point a, Point b) {
            int diff = a.y - b.y;
            if (diff == 0) {
                diff = a.x - b.x;
            }
            return diff;
        }

    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/microsoft/HSLFExtractor.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.microsoft;

import java.io.IOException;
import java.util.HashSet;

import org.apache.poi.hslf.HSLFSlideShow;
import org.apache.poi.hslf.model.Comment;
import org.apache.poi.hslf.model.HeadersFooters;
import org.apache.poi.hslf.model.MasterSheet;
import org.apache.poi.hslf.model.Notes;
import org.apache.poi.hslf.model.OLEShape;
import org.apache.poi.hslf.model.Picture;
import org.apache.poi.hslf.model.Shape;
import org.apache.poi.hslf.model.Slide;
import org.apache.poi.hslf.model.Table;
import org.apache.poi.hslf.model.TableCell;
import org.apache.poi.hslf.model.TextRun;
import org.apache.poi.hslf.model.TextShape;
import org.apache.poi.hslf.usermodel.ObjectData;
import org.apache.poi.hslf.usermodel.PictureData;
import org.apache.poi.hslf.usermodel.SlideShow;
import org.apache.poi.poifs.filesystem.DirectoryNode;
import org.apache.poi.poifs.filesystem.NPOIFSFileSystem;
import org.apache.tika.exception.TikaException;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.AttributesImpl;

public class HSLFExtractor extends AbstractPOIFSExtractor {
   public HSLFExtractor(ParseContext context) {
      super(context);
   }
	
   protected void parse(
         NPOIFSFileSystem filesystem, XHTMLContentHandler xhtml)
         throws IOException, SAXException, TikaException {
       parse(filesystem.getRoot(), xhtml);
   }
    
   protected void parse(
         DirectoryNode root, XHTMLContentHandler xhtml)
         throws IOException, SAXException, TikaException {
      HSLFSlideShow ss = new HSLFSlideShow(root);
      SlideShow _show = new SlideShow(ss);
      Slide[] _slides = _show.getSlides();

      xhtml.startElement("div", "class", "slideShow");

      /* Iterate over slides and extract text */
      for( Slide slide : _slides ) {
         xhtml.startElement("div", "class", "slide");

         // Slide header, if present
         HeadersFooters hf = slide.getHeadersFooters();
         if (hf != null && hf.isHeaderVisible() && hf.getHeaderText() != null) {
            xhtml.startElement("p", "class", "slide-header");

            xhtml.characters( hf.getHeaderText() );

            xhtml.endElement("p");
         }

         // Slide master, if present
         extractMaster(xhtml, slide.getMasterSheet());

         // Slide text
         {
            xhtml.startElement("p", "class", "slide-content");

            textRunsToText(xhtml, slide.getTextRuns());

            xhtml.endElement("p");
         }

         // Table text
         for (Shape shape: slide.getShapes()){
            if (shape instanceof Table){
               extractTableText(xhtml, (Table)shape);
            }
         }

         // Slide footer, if present
         if (hf != null && hf.isFooterVisible() && hf.getFooterText() != null) {
            xhtml.startElement("p", "class", "slide-footer");

            xhtml.characters( hf.getFooterText() );

            xhtml.endElement("p");
         }

         // Comments, if present
         for( Comment comment : slide.getComments() ) {
            xhtml.startElement("p", "class", "slide-comment");
            if (comment.getAuthor() != null) {
               xhtml.startElement("b");
               xhtml.characters( comment.getAuthor() );
               xhtml.endElement("b");
               
               if (comment.getText() != null) {
                  xhtml.characters( " - ");
               }
            }
            if (comment.getText() != null) {
               xhtml.characters( comment.getText() );
            }
            xhtml.endElement("p");
         }

         // Now any embedded resources
         handleSlideEmbeddedResources(slide, xhtml);

         // TODO Find the Notes for this slide and extract inline

         // Slide complete
         xhtml.endElement("div");
      }

      // All slides done
      xhtml.endElement("div");

      /* notes */
      xhtml.startElement("div", "class", "slideNotes");
      HashSet<Integer> seenNotes = new HashSet<Integer>();
      HeadersFooters hf = _show.getNotesHeadersFooters();

      for (Slide slide : _slides) {
         Notes notes = slide.getNotesSheet();
         if (notes == null) {
            continue;
         }
         Integer id = notes._getSheetNumber();
         if (seenNotes.contains(id)) {
            continue;
         }
         seenNotes.add(id);

         // Repeat the Notes header, if set
         if (hf != null && hf.isHeaderVisible() && hf.getHeaderText() != null) {
            xhtml.startElement("p", "class", "slide-note-header");
            xhtml.characters( hf.getHeaderText() );
            xhtml.endElement("p");
         }

         // Notes text
         textRunsToText(xhtml, notes.getTextRuns());

         // Repeat the notes footer, if set
         if (hf != null && hf.isFooterVisible() && hf.getFooterText() != null) {
            xhtml.startElement("p", "class", "slide-note-footer");
            xhtml.characters( hf.getFooterText() );
            xhtml.endElement("p");
         }
      }

      handleSlideEmbeddedPictures(_show, xhtml);

      xhtml.endElement("div");
   }

   private void extractMaster(XHTMLContentHandler xhtml, MasterSheet master) throws SAXException {
      if (master == null){
         return;
      }
      Shape[] shapes = master.getShapes();
      if (shapes == null || shapes.length == 0){
         return;
      }

      xhtml.startElement("div", "class", "slide-master-content");
      for (Shape shape : shapes){
         if (shape != null && ! MasterSheet.isPlaceholder(shape)){
            if (shape instanceof TextShape){
               TextShape tsh = (TextShape)shape;
               String text = tsh.getText();
               if (text != null){
                  xhtml.element("p", text);
               }
            }
         }
      }
      xhtml.endElement("div");
   }

   private void extractTableText(XHTMLContentHandler xhtml, Table shape) throws SAXException {
      xhtml.startElement("table");
      for (int row = 0; row < shape.getNumberOfRows(); row++){
         xhtml.startElement("tr");
         for (int col = 0; col < shape.getNumberOfColumns(); col++){
            TableCell cell = shape.getCell(row, col);
            //insert empty string for empty cell if cell is null
            String txt = "";
            if (cell != null){
               txt = cell.getText();
            }
            xhtml.element("td", txt);
         }
         xhtml.endElement("tr");
      }
      xhtml.endElement("table");   
   }

   private void textRunsToText(XHTMLContentHandler xhtml, TextRun[] runs) throws SAXException {
      if (runs==null) {
         return;
      }

      for (TextRun run : runs) {
         if (run != null) {
           // Leaving in wisdom from TIKA-712 for easy revert.
           // Avoid boiler-plate text on the master slide (0
           // = TextHeaderAtom.TITLE_TYPE, 1 = TextHeaderAtom.BODY_TYPE):
           //if (!isMaster || (run.getRunType() != 0 && run.getRunType() != 1)) {
           String txt = run.getText();
           if (txt != null){
               xhtml.characters(txt);
               xhtml.startElement("br");
               xhtml.endElement("br");
           }
         }
      }
   }

    private void handleSlideEmbeddedPictures(SlideShow slideshow, XHTMLContentHandler xhtml)
            throws TikaException, SAXException, IOException {
        for (PictureData pic : slideshow.getPictureData()) {
            String mediaType = null;

            switch (pic.getType()) {
                case Picture.EMF:
                    mediaType = "application/x-emf";
                    break;
                case Picture.JPEG:
                    mediaType = "image/jpeg";
                    break;
                case Picture.PNG:
                    mediaType = "image/png";
                    break;
                case Picture.WMF:
                    mediaType = "application/x-msmetafile";
                    break;
                case Picture.DIB:
                    mediaType = "image/bmp";
                    break;
            }

            handleEmbeddedResource(
                  TikaInputStream.get(pic.getData()), null, null,
                  mediaType, xhtml, false);
        }
    }

    private void handleSlideEmbeddedResources(Slide slide, XHTMLContentHandler xhtml)
                throws TikaException, SAXException, IOException {
      Shape[] shapes;
      try {
         shapes = slide.getShapes();
      } catch(NullPointerException e) {
         // Sometimes HSLF hits problems
         // Please open POI bugs for any you come across!
         return;
      }
      
      for( Shape shape : shapes ) {
         if( shape instanceof OLEShape ) {
            OLEShape oleShape = (OLEShape)shape;
            ObjectData data = null;
            try {
                data = oleShape.getObjectData();
            } catch( NullPointerException e ) { 
                /* getObjectData throws NPE some times. */
            }
 
            if (data != null) {
               String objID = Integer.toString(oleShape.getObjectID());

               // Embedded Object: add a <div
               // class="embedded" id="X"/> so consumer can see where
               // in the main text each embedded document
               // occurred:
               AttributesImpl attributes = new AttributesImpl();
               attributes.addAttribute("", "class", "class", "CDATA", "embedded");
               attributes.addAttribute("", "id", "id", "CDATA", objID);
               xhtml.startElement("div", attributes);
               xhtml.endElement("div");

               TikaInputStream stream =
                    TikaInputStream.get(data.getData());
               try {
                  String mediaType = null;
                  if ("Excel.Chart.8".equals(oleShape.getProgID())) {
                     mediaType = "application/vnd.ms-excel";
                  }
                  handleEmbeddedResource(
                        stream, objID, objID,
                        mediaType, xhtml, false);
               } finally {
                  stream.close();
               }
            }
         }
      }
   }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/microsoft/LinkedCell.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.microsoft;

import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.SAXException;

/**
 * Linked cell. This class decorates another content cell with a hyperlink.
 */
public class LinkedCell extends CellDecorator {

    private final String link;

    public LinkedCell(Cell cell, String link) {
        super(cell);
        assert link != null;
        this.link = link;
    }

    public void render(XHTMLContentHandler handler) throws SAXException {
        handler.startElement("a", "href", link);
        super.render(handler);
        handler.endElement("a");
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/microsoft/NumberCell.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.microsoft;

import java.text.NumberFormat;

import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.SAXException;

/**
 * Number cell.
 */
public class NumberCell implements Cell {

    private final double number;

    private final NumberFormat format;

    public NumberCell(double number, NumberFormat format) {
        this.number = number;
        this.format = format;
    }

    public void render(XHTMLContentHandler handler) throws SAXException {
        handler.characters(format.format(number));
    }

    public String toString() {
        return "Numeric Cell: " + format.format(number);
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/microsoft/OfficeParser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.microsoft;

import java.io.IOException;
import java.io.InputStream;
import java.security.GeneralSecurityException;
import java.util.Arrays;
import java.util.Collections;
import java.util.HashSet;
import java.util.Locale;
import java.util.Set;

import org.apache.poi.hdgf.extractor.VisioTextExtractor;
import org.apache.poi.hpbf.extractor.PublisherTextExtractor;
import org.apache.poi.poifs.crypt.Decryptor;
import org.apache.poi.poifs.crypt.EncryptionInfo;
import org.apache.poi.poifs.filesystem.DirectoryEntry;
import org.apache.poi.poifs.filesystem.DirectoryNode;
import org.apache.poi.poifs.filesystem.Entry;
import org.apache.poi.poifs.filesystem.NPOIFSFileSystem;
import org.apache.poi.poifs.filesystem.POIFSFileSystem;
import org.apache.tika.exception.EncryptedDocumentException;
import org.apache.tika.exception.TikaException;
import org.apache.tika.io.CloseShieldInputStream;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.PasswordProvider;
import org.apache.tika.parser.microsoft.ooxml.OOXMLParser;
import org.apache.tika.sax.BodyContentHandler;
import org.apache.tika.sax.EmbeddedContentHandler;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * Defines a Microsoft document content extractor.
 */
public class OfficeParser extends AbstractParser {

    /** Serial version UID */
    private static final long serialVersionUID = 7393462244028653479L;

    private static final Set<MediaType> SUPPORTED_TYPES =
            Collections.unmodifiableSet(new HashSet<MediaType>(Arrays.asList(
                    POIFSDocumentType.WORKBOOK.type,
                    POIFSDocumentType.OLE10_NATIVE.type,
                    POIFSDocumentType.WORDDOCUMENT.type,
                    POIFSDocumentType.UNKNOWN.type,
                    POIFSDocumentType.ENCRYPTED.type,
                    POIFSDocumentType.POWERPOINT.type,
                    POIFSDocumentType.PUBLISHER.type,
                    POIFSDocumentType.PROJECT.type,
                    POIFSDocumentType.VISIO.type,
                    // Works isn't supported
                    POIFSDocumentType.XLR.type, // but Works 7.0 Spreadsheet is
                    POIFSDocumentType.OUTLOOK.type,
                    POIFSDocumentType.SOLIDWORKS_PART.type,
                    POIFSDocumentType.SOLIDWORKS_ASSEMBLY.type,
                    POIFSDocumentType.SOLIDWORKS_DRAWING.type
                    )));

    public enum POIFSDocumentType {
        WORKBOOK("xls", MediaType.application("vnd.ms-excel")),
        OLE10_NATIVE("ole", POIFSContainerDetector.OLE10_NATIVE),
        COMP_OBJ("ole", POIFSContainerDetector.COMP_OBJ),
        WORDDOCUMENT("doc", MediaType.application("msword")),
        UNKNOWN("unknown", MediaType.application("x-tika-msoffice")),
        ENCRYPTED("ole", MediaType.application("x-tika-ooxml-protected")),
        POWERPOINT("ppt", MediaType.application("vnd.ms-powerpoint")),
        PUBLISHER("pub", MediaType.application("x-mspublisher")),
        PROJECT("mpp", MediaType.application("vnd.ms-project")),
        VISIO("vsd", MediaType.application("vnd.visio")),
        WORKS("wps", MediaType.application("vnd.ms-works")),
        XLR("xlr", MediaType.application("x-tika-msworks-spreadsheet")),
        OUTLOOK("msg", MediaType.application("vnd.ms-outlook")),
        SOLIDWORKS_PART("sldprt", MediaType.application("sldworks")),
        SOLIDWORKS_ASSEMBLY("sldasm", MediaType.application("sldworks")),
        SOLIDWORKS_DRAWING("slddrw", MediaType.application("sldworks"));

        private final String extension;
        private final MediaType type;

        POIFSDocumentType(String extension, MediaType type) {
            this.extension = extension;
            this.type = type;
        }

        public String getExtension() {
            return extension;
        }

        public MediaType getType() {
            return type;
        }

        public static POIFSDocumentType detectType(POIFSFileSystem fs) {
            return detectType(fs.getRoot());
        }

        public static POIFSDocumentType detectType(NPOIFSFileSystem fs) {
           return detectType(fs.getRoot());
       }

        public static POIFSDocumentType detectType(DirectoryEntry node) {
            Set<String> names = new HashSet<String>();
            for (Entry entry : node) {
                names.add(entry.getName());
            }
            MediaType type = POIFSContainerDetector.detect(names, node);
            for (POIFSDocumentType poifsType : values()) {
               if (type.equals(poifsType.type)) {
                  return poifsType;
               }
            }
            return UNKNOWN;
        }
    }

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }

    /**
     * Extracts properties and text from an MS Document input stream
     */
    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);
        xhtml.startDocument();

        final DirectoryNode root;
        TikaInputStream tstream = TikaInputStream.cast(stream);
        if (tstream == null) {
            root = new NPOIFSFileSystem(new CloseShieldInputStream(stream)).getRoot();
        } else {
            final Object container = tstream.getOpenContainer();
            if (container instanceof NPOIFSFileSystem) {
                root = ((NPOIFSFileSystem) container).getRoot();
            } else if (container instanceof DirectoryNode) {
                root = (DirectoryNode) container;
            } else {
                NPOIFSFileSystem fs;
                if (tstream.hasFile()) {
                    fs = new NPOIFSFileSystem(tstream.getFile(), true);
                } else {
                    fs = new NPOIFSFileSystem(new CloseShieldInputStream(tstream));
                }
                tstream.setOpenContainer(fs);
                root = fs.getRoot();
            }
        }
        parse(root, context, metadata, xhtml);
        xhtml.endDocument();
    }

    protected void parse(
            DirectoryNode root, ParseContext context, Metadata metadata, XHTMLContentHandler xhtml)
            throws IOException, SAXException, TikaException {

        // Parse summary entries first, to make metadata available early
        new SummaryExtractor(metadata).parseSummaries(root);

        // Parse remaining document entries
        POIFSDocumentType type = POIFSDocumentType.detectType(root);

        if (type!=POIFSDocumentType.UNKNOWN) {
            setType(metadata, type.getType());
        }

        switch (type) {
        case SOLIDWORKS_PART:
        case SOLIDWORKS_ASSEMBLY:
        case SOLIDWORKS_DRAWING:
        	break;
        case PUBLISHER:
           PublisherTextExtractor publisherTextExtractor =
              new PublisherTextExtractor(root);
           xhtml.element("p", publisherTextExtractor.getText());
           break;
        case WORDDOCUMENT:
           new WordExtractor(context).parse(root, xhtml);
           break;
        case POWERPOINT:
           new HSLFExtractor(context).parse(root, xhtml);
           break;
        case WORKBOOK:
        case XLR:
           Locale locale = context.get(Locale.class, Locale.getDefault());
           new ExcelExtractor(context, metadata).parse(root, xhtml, locale);
           break;
        case PROJECT:
           // We currently can't do anything beyond the metadata
           break;
        case VISIO:
           VisioTextExtractor visioTextExtractor =
              new VisioTextExtractor(root);
           for (String text : visioTextExtractor.getAllText()) {
              xhtml.element("p", text);
           }
           break;
        case OUTLOOK:
           OutlookExtractor extractor =
                 new OutlookExtractor(root, context);

           extractor.parse(xhtml, metadata);
           break;
        case ENCRYPTED:
           EncryptionInfo info = new EncryptionInfo(root);
           Decryptor d = Decryptor.getInstance(info);

           try {
              // By default, use the default Office Password
              String password = Decryptor.DEFAULT_PASSWORD;
              
              // If they supplied a Password Provider, ask that for the password,
              //  and use the provider given one if available (stick with default if not)
              PasswordProvider passwordProvider = context.get(PasswordProvider.class);
              if (passwordProvider != null) {
                 String suppliedPassword = passwordProvider.getPassword(metadata);
                 if (suppliedPassword != null) {
                     password = suppliedPassword;
                 }
              }
              
              // Check if we've the right password or not
              if (!d.verifyPassword(password)) {
                 throw new EncryptedDocumentException();
              }

              // Decrypt the OLE2 stream, and delegate the resulting OOXML
              //  file to the regular OOXML parser for normal handling
              OOXMLParser parser = new OOXMLParser();

              parser.parse(d.getDataStream(root), new EmbeddedContentHandler(
                    new BodyContentHandler(xhtml)),
                    metadata, context);
           } catch (GeneralSecurityException ex) {
              throw new EncryptedDocumentException(ex);
           }
        default:
            // For unsupported / unhandled types, just the metadata
            //  is extracted, which happened above
            break;
        }
    }

    private void setType(Metadata metadata, MediaType type) {
        metadata.set(Metadata.CONTENT_TYPE, type.toString());
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/microsoft/OldExcelParser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.microsoft;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStream;
import java.io.StringReader;
import java.util.Arrays;
import java.util.Collections;
import java.util.HashSet;
import java.util.Set;

import org.apache.poi.hssf.extractor.OldExcelExtractor;
import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * A POI-powered Tika Parser for very old versions of Excel, from
 *  pre-OLE2 days, such as Excel 4.
 */
public class OldExcelParser extends AbstractParser {
   private static final long serialVersionUID = 4611820730372823452L;
   
   private static final Set<MediaType> SUPPORTED_TYPES =
        Collections.unmodifiableSet(new HashSet<MediaType>(Arrays.asList(
              MediaType.application("vnd.ms-excel.sheet.4"),
              MediaType.application("vnd.ms-excel.workspace.4"),
              MediaType.application("vnd.ms-excel.sheet.3"),
              MediaType.application("vnd.ms-excel.workspace.3"),
              MediaType.application("vnd.ms-excel.sheet.2")
         )));

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }

    /**
     * Extracts properties and text from an MS Document input stream
     */
    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
       // Open the POI provided extractor
       OldExcelExtractor extractor = new OldExcelExtractor(stream);
       
       // We can't do anything about metadata, as these old formats
       //  didn't have any stored with them
       
       // Set the content type
       // TODO Get the version and type, to set as the Content Type
       
       // Have the text extracted and given to our Content Handler
       XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);
       parse(extractor, xhtml);
    }
    
    protected static void parse(OldExcelExtractor extractor, 
            XHTMLContentHandler xhtml) throws TikaException, IOException, SAXException {
        // Get the whole text, as a single string
        String text = extractor.getText();
        
        // Split and output
        xhtml.startDocument();
        
        String line;
        BufferedReader reader = new BufferedReader(new StringReader(text));
        while ((line = reader.readLine()) != null) {
            xhtml.startElement("p");
            xhtml.characters(line);
            xhtml.endElement("p");
        }
        
        xhtml.endDocument();
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/microsoft/OutlookExtractor.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.microsoft;

import java.io.ByteArrayInputStream;
import java.io.IOException;
import java.text.ParseException;
import java.util.Date;
import java.util.Locale;

import org.apache.poi.hmef.attribute.MAPIRtfAttribute;
import org.apache.poi.hsmf.MAPIMessage;
import org.apache.poi.hsmf.datatypes.AttachmentChunks;
import org.apache.poi.hsmf.datatypes.ByteChunk;
import org.apache.poi.hsmf.datatypes.Chunk;
import org.apache.poi.hsmf.datatypes.MAPIProperty;
import org.apache.poi.hsmf.datatypes.StringChunk;
import org.apache.poi.hsmf.datatypes.Types;
import org.apache.poi.hsmf.exceptions.ChunkNotFoundException;
import org.apache.poi.poifs.filesystem.DirectoryNode;
import org.apache.poi.poifs.filesystem.NPOIFSFileSystem;
import org.apache.tika.exception.TikaException;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.html.HtmlParser;
import org.apache.tika.parser.mbox.MboxParser;
import org.apache.tika.parser.rtf.RTFParser;
import org.apache.tika.parser.txt.CharsetDetector;
import org.apache.tika.parser.txt.CharsetMatch;
import org.apache.tika.sax.BodyContentHandler;
import org.apache.tika.sax.EmbeddedContentHandler;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.SAXException;

/**
 * Outlook Message Parser.
 */
public class OutlookExtractor extends AbstractPOIFSExtractor {
    private final MAPIMessage msg;

    public OutlookExtractor(NPOIFSFileSystem filesystem, ParseContext context) throws TikaException {
        this(filesystem.getRoot(), context);
    }

    public OutlookExtractor(DirectoryNode root, ParseContext context) throws TikaException {
        super(context);
        
        try {
            this.msg = new MAPIMessage(root);
        } catch (IOException e) {
            throw new TikaException("Failed to parse Outlook message", e);
        }
    }

    public void parse(XHTMLContentHandler xhtml, Metadata metadata)
            throws TikaException, SAXException, IOException {
        try {
           msg.setReturnNullOnMissingChunk(true);
           
           // If the message contains strings that aren't stored
           //  as Unicode, try to sort out an encoding for them
           if(msg.has7BitEncodingStrings()) {
              if(msg.getHeaders() != null) {
                 // There's normally something in the headers
                 msg.guess7BitEncoding();
              } else {
                 // Nothing in the header, try encoding detection
                 //  on the message body
                 StringChunk text = msg.getMainChunks().textBodyChunk; 
                 if(text != null) {
                    CharsetDetector detector = new CharsetDetector();
                    detector.setText( text.getRawValue() );
                    CharsetMatch match = detector.detect();
                    if(match.getConfidence() > 35) {
                       msg.set7BitEncoding( match.getName() );
                    }
                 }
              }
           }
           
           // Start with the metadata
           String subject = msg.getSubject();
           String from = msg.getDisplayFrom();
   
           metadata.set(TikaCoreProperties.CREATOR, from);
           metadata.set(Metadata.MESSAGE_FROM, from);
           metadata.set(Metadata.MESSAGE_TO, msg.getDisplayTo());
           metadata.set(Metadata.MESSAGE_CC, msg.getDisplayCC());
           metadata.set(Metadata.MESSAGE_BCC, msg.getDisplayBCC());
           
           metadata.set(TikaCoreProperties.TITLE, subject);
           // TODO: Move to description in Tika 2.0
           metadata.set(TikaCoreProperties.TRANSITION_SUBJECT_TO_DC_DESCRIPTION, 
                   msg.getConversationTopic());
           
           try {
           for(String recipientAddress : msg.getRecipientEmailAddressList()) {
               if(recipientAddress != null)
        	   metadata.add(Metadata.MESSAGE_RECIPIENT_ADDRESS, recipientAddress);
           }
           } catch(ChunkNotFoundException he) {} // Will be fixed in POI 3.7 Final
           
           // Date - try two ways to find it
           // First try via the proper chunk
           if(msg.getMessageDate() != null) {
              metadata.set(TikaCoreProperties.CREATED, msg.getMessageDate().getTime());
              metadata.set(TikaCoreProperties.MODIFIED, msg.getMessageDate().getTime());
           } else {
              try {
                 // Failing that try via the raw headers 
                 String[] headers = msg.getHeaders();
                 if(headers != null && headers.length > 0) {
                     for(String header: headers) {
                        if(header.toLowerCase(Locale.ROOT).startsWith("date:")) {
                            String date = header.substring(header.indexOf(':')+1).trim();
                            
                            // See if we can parse it as a normal mail date
                            try {
                               Date d = MboxParser.parseDate(date);
                               metadata.set(TikaCoreProperties.CREATED, d);
                               metadata.set(TikaCoreProperties.MODIFIED, d);
                            } catch(ParseException e) {
                               // Store it as-is, and hope for the best...
                               metadata.set(TikaCoreProperties.CREATED, date);
                               metadata.set(TikaCoreProperties.MODIFIED, date);
                            }
                            break;
                        }
                     }
                 }
              } catch(ChunkNotFoundException he) {
                 // We can't find the date, sorry...
              }
           }
           
   
           xhtml.element("h1", subject);
   
           // Output the from and to details in text, as you
           //  often want them in text form for searching
           xhtml.startElement("dl");
           if (from!=null) {
               header(xhtml, "From", from);
           }
           header(xhtml, "To", msg.getDisplayTo());
           header(xhtml, "Cc", msg.getDisplayCC());
           header(xhtml, "Bcc", msg.getDisplayBCC());
           try {
               header(xhtml, "Recipients", msg.getRecipientEmailAddress());
           } catch(ChunkNotFoundException e) {}
           xhtml.endElement("dl");
   
           // Get the message body. Preference order is: html, rtf, text
           Chunk htmlChunk = null;
           Chunk rtfChunk = null;
           Chunk textChunk = null;
           for(Chunk chunk : msg.getMainChunks().getChunks()) {
              if(chunk.getChunkId() == MAPIProperty.BODY_HTML.id) {
                 htmlChunk = chunk;
              }
              if(chunk.getChunkId() == MAPIProperty.RTF_COMPRESSED.id) {
                 rtfChunk = chunk;
              }
              if(chunk.getChunkId() == MAPIProperty.BODY.id) {
                 textChunk = chunk;
              }
           }
           
           boolean doneBody = false;
           xhtml.startElement("div", "class", "message-body");
           if(htmlChunk != null) {
              byte[] data = null;
              if(htmlChunk instanceof ByteChunk) {
                 data = ((ByteChunk)htmlChunk).getValue();
              } else if(htmlChunk instanceof StringChunk) {
                 data = ((StringChunk)htmlChunk).getRawValue();
              }
              if(data != null) {
                 HtmlParser htmlParser = new HtmlParser();
                 htmlParser.parse(
                       new ByteArrayInputStream(data),
                       new EmbeddedContentHandler(new BodyContentHandler(xhtml)), 
                       new Metadata(), new ParseContext()
                 );
                 doneBody = true;
              }
           }
           if(rtfChunk != null && !doneBody) {
              ByteChunk chunk = (ByteChunk)rtfChunk;
              MAPIRtfAttribute rtf = new MAPIRtfAttribute(
                    MAPIProperty.RTF_COMPRESSED, Types.BINARY.getId(), chunk.getValue()
              );
              RTFParser rtfParser = new RTFParser();
              rtfParser.parse(
                              new ByteArrayInputStream(rtf.getData()),
                              new EmbeddedContentHandler(new BodyContentHandler(xhtml)),
                              new Metadata(), new ParseContext());
              doneBody = true;
           }
           if(textChunk != null && !doneBody) {
              xhtml.element("p", ((StringChunk)textChunk).getValue());
           }
           xhtml.endElement("div");
           
           // Process the attachments
           for (AttachmentChunks attachment : msg.getAttachmentFiles()) {
               xhtml.startElement("div", "class", "attachment-entry");
               
               String filename = null;
               if (attachment.attachLongFileName != null) {
                  filename = attachment.attachLongFileName.getValue();
               } else if (attachment.attachFileName != null) {
                  filename = attachment.attachFileName.getValue();
               }
               if (filename != null && filename.length() > 0) {
                   xhtml.element("h1", filename);
               }
               
               if(attachment.attachData != null) {
                  handleEmbeddedResource(
                        TikaInputStream.get(attachment.attachData.getValue()),
                        filename, null,
                        null, xhtml, true
                  );
               }
               if(attachment.attachmentDirectory != null) {
                  handleEmbeddedOfficeDoc(
                        attachment.attachmentDirectory.getDirectory(),
                        xhtml
                  );
               }

               xhtml.endElement("div");
           }
        } catch(ChunkNotFoundException e) {
           throw new TikaException("POI MAPIMessage broken - didn't return null on missing chunk", e);
        }
    }

    private void header(XHTMLContentHandler xhtml, String key, String value)
            throws SAXException {
        if (value != null && value.length() > 0) {
            xhtml.element("dt", key);
            xhtml.element("dd", value);
        }
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/microsoft/POIFSContainerDetector.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.microsoft;

import static org.apache.tika.mime.MediaType.application;

import java.io.File;
import java.io.IOException;
import java.io.InputStream;
import java.util.Collections;
import java.util.HashSet;
import java.util.Set;
import java.util.regex.Pattern;

import org.apache.poi.poifs.filesystem.DirectoryEntry;
import org.apache.poi.poifs.filesystem.DirectoryNode;
import org.apache.poi.poifs.filesystem.DocumentInputStream;
import org.apache.poi.poifs.filesystem.DocumentNode;
import org.apache.poi.poifs.filesystem.Entry;
import org.apache.poi.poifs.filesystem.NPOIFSFileSystem;
import org.apache.tika.detect.Detector;
import org.apache.tika.io.IOUtils;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;

/**
 * A detector that works on a POIFS OLE2 document
 *  to figure out exactly what the file is.
 * This should work for all OLE2 documents, whether
 *  they are ones supported by POI or not.
 */
public class POIFSContainerDetector implements Detector {

    /** Serial version UID */
    private static final long serialVersionUID = -3028021741663605293L;
    
    /** An ASCII String "StarImpress" */
    private static final byte [] STAR_IMPRESS = new byte [] {
        0x53, 0x74, 0x61, 0x72, 0x49, 0x6d, 0x70, 0x72, 0x65, 0x73, 0x73
    };
    
    /** An ASCII String "StarDraw" */
    private static final byte [] STAR_DRAW = new byte [] {
        0x53, 0x74, 0x61, 0x72, 0x44, 0x72, 0x61, 0x77
    };
    
    /** An ASCII String "Quill96" for Works Files */
    private static final byte [] WORKS_QUILL96 = new byte[] {
        0x51, 0x75, 0x69, 0x6c, 0x6c, 0x39, 0x36
    };

    /** The OLE base file format */
    public static final MediaType OLE = application("x-tika-msoffice");
    
    /** The protected OOXML base file format */
    public static final MediaType OOXML_PROTECTED = application("x-tika-ooxml-protected");
    
    /** General embedded document type within an OLE2 container */
    public static final MediaType GENERAL_EMBEDDED = application("x-tika-msoffice-embedded");
    
    /** An OLE10 Native embedded document within another OLE2 document */
    public static final MediaType OLE10_NATIVE =
            new MediaType(GENERAL_EMBEDDED, "format", "ole10_native");
    
    /** Some other kind of embedded document, in a CompObj container within another OLE2 document */
    public static final MediaType COMP_OBJ =
            new MediaType(GENERAL_EMBEDDED, "format", "comp_obj");

    /** Microsoft Excel */
    public static final MediaType XLS = application("vnd.ms-excel");

    /** Microsoft Word */
    public static final MediaType DOC = application("msword");

    /** Microsoft PowerPoint */
    public static final MediaType PPT = application("vnd.ms-powerpoint");

    /** Microsoft Publisher */
    public static final MediaType PUB = application("x-mspublisher");

    /** Microsoft Visio */
    public static final MediaType VSD = application("vnd.visio");

    /** Microsoft Works */
    public static final MediaType WPS = application("vnd.ms-works");
    
    /** Microsoft Works Spreadsheet 7.0 */
    public static final MediaType XLR = application("x-tika-msworks-spreadsheet");

    /** Microsoft Outlook */
    public static final MediaType MSG = application("vnd.ms-outlook");
    
    /** Microsoft Project */
    public static final MediaType MPP = application("vnd.ms-project");
    
    /** StarOffice Calc */
    public static final MediaType SDC = application("vnd.stardivision.calc");
    
    /** StarOffice Draw */
    public static final MediaType SDA = application("vnd.stardivision.draw");
    
    /** StarOffice Impress */
    public static final MediaType SDD = application("vnd.stardivision.impress");
    
    /** StarOffice Writer */
    public static final MediaType SDW = application("vnd.stardivision.writer");

    /** SolidWorks CAD file */
    public static final MediaType SLDWORKS = application("sldworks");

    /** Regexp for matching the MPP Project Data stream */
    private static final Pattern mppDataMatch = Pattern.compile("\\s\\s\\s\\d+");

    public MediaType detect(InputStream input, Metadata metadata)
             throws IOException {
        // Check if we have access to the document
        if (input == null) {
            return MediaType.OCTET_STREAM;
        }

        // If this is a TikaInputStream wrapping an already
        // parsed NPOIFileSystem/DirectoryNode, just get the
        // names from the root:
        TikaInputStream tis = TikaInputStream.cast(input);
        Set<String> names = null;
        if (tis != null) {
            Object container = tis.getOpenContainer();
            if (container instanceof NPOIFSFileSystem) {
                names = getTopLevelNames(((NPOIFSFileSystem) container).getRoot());
            } else if (container instanceof DirectoryNode) {
                names = getTopLevelNames((DirectoryNode) container);
            }
        }

        if (names == null) {
            // Check if the document starts with the OLE header
            input.mark(8);
            try {
                if (input.read() != 0xd0 || input.read() != 0xcf
                    || input.read() != 0x11 || input.read() != 0xe0
                    || input.read() != 0xa1 || input.read() != 0xb1
                    || input.read() != 0x1a || input.read() != 0xe1) {
                    return MediaType.OCTET_STREAM;
                }
            } finally {
                input.reset();
            }
        }

        // We can only detect the exact type when given a TikaInputStream
        if (names == null && tis != null) {
            // Look for known top level entry names to detect the document type
            names = getTopLevelNames(tis);
        }
        
        // Detect based on the names (as available)
        if (tis != null && 
            tis.getOpenContainer() != null && 
            tis.getOpenContainer() instanceof NPOIFSFileSystem) {
            return detect(names, ((NPOIFSFileSystem)tis.getOpenContainer()).getRoot());
        } else {
            return detect(names, null);
        }
    }

    /**
     * Internal detection of the specific kind of OLE2 document, based on the
     * names of the top level streams within the file.
     * 
     * @deprecated Use {@link #detect(Set, DirectoryEntry)} and pass the root
     *             entry of the filesystem whose type is to be detected, as a
     *             second argument.
     */
    protected static MediaType detect(Set<String> names) {
        return detect(names, null);
    }
    
    /**
     * Internal detection of the specific kind of OLE2 document, based on the
     * names of the top-level streams within the file. In some cases the
     * detection may need access to the root {@link DirectoryEntry} of that file
     * for best results. The entry can be given as a second, optional argument.
     * 
     * @param names
     * @param root
     * @return
     */
    protected static MediaType detect(Set<String> names, DirectoryEntry root) {
        if (names != null) {
            if (names.contains("SwDocContentMgr") && names.contains("SwDocMgrTempStorage")) {
                return SLDWORKS;
            } else if (names.contains("StarCalcDocument")) {
                // Star Office Calc
                return SDC;
            } else if (names.contains("StarWriterDocument")) {
                return SDW;
            } else if (names.contains("StarDrawDocument3")) {
                if (root == null) {
                    /*
                     * This is either StarOfficeDraw or StarOfficeImpress, we have
                     * to consult the CompObj to distinguish them, if this method is
                     * called in "legacy mode", without the root, just return
                     * x-tika-msoffice. The one-argument method is only for backward
                     * compatibility, if someone calls old API he/she can get the
                     * old result.
                     */
                    return OLE;
                } else {
                    return processCompObjFormatType(root);
                }
            } else if (names.contains("WksSSWorkBook")) {
                // This check has to be before names.contains("Workbook")
                // Works 7.0 spreadsheet files contain both
                // we want to avoid classifying this as Excel
                return XLR; 
            } else if (names.contains("Workbook") || names.contains("WORKBOOK")) {
                return XLS;
            } else if (names.contains("Book")) {
               // Excel 95 or older, we won't be able to parse this....
               return XLS;
            } else if (names.contains("EncryptedPackage") && 
                    names.contains("EncryptionInfo") &&
                    names.contains("\u0006DataSpaces")) {
                // This is a protected OOXML document, which is an OLE2 file
                //  with an Encrypted Stream which holds the OOXML data
                // Without decrypting the stream, we can't tell what kind of
                //  OOXML file we have. Return a general OOXML Protected type,
                //  and hope the name based detection can guess the rest! 
                return OOXML_PROTECTED;
            } else if (names.contains("EncryptedPackage")) {
                return OLE;
            } else if (names.contains("WordDocument")) {
                return DOC;
            } else if (names.contains("Quill")) {
                return PUB;
            } else if (names.contains("PowerPoint Document")) {
                return PPT;
            } else if (names.contains("VisioDocument")) {
                return VSD;
            } else if (names.contains("\u0001Ole10Native")) {
                return OLE10_NATIVE;
            } else if (names.contains("MatOST")) {
                // this occurs on older Works Word Processor files (versions 3.0 and 4.0)
                return WPS;
            } else if (names.contains("CONTENTS") && names.contains("SPELLING")) {
                // Newer Works files
                return WPS;
            } else if (names.contains("Contents") && names.contains("\u0003ObjInfo")) {
                return COMP_OBJ;
            } else if (names.contains("CONTENTS") && names.contains("\u0001CompObj")) {
               // CompObj is a general kind of OLE2 embedding, but this may be an old Works file
               // If we have the Directory, check
               if (root != null) {
                  MediaType type = processCompObjFormatType(root);
                  if (type == WPS) {
                     return WPS;
                  } else {
                     // Assume it's a general CompObj embedded resource
                     return COMP_OBJ;
                  }
               } else {
                  // Assume it's a general CompObj embedded resource
                  return COMP_OBJ;
               }
            } else if (names.contains("CONTENTS")) {
               // CONTENTS without SPELLING nor CompObj normally means some sort
               //  of embedded non-office file inside an OLE2 document
               // This is most commonly triggered on nested directories
               return OLE;
            } else if (names.contains("\u0001CompObj") &&
                  (names.contains("Props") || names.contains("Props9") || names.contains("Props12"))) {
               // Could be Project, look for common name patterns
               for (String name : names) {
                  if (mppDataMatch.matcher(name).matches()) {
                     return MPP;
                  }
               }
            } else if (names.contains("PerfectOffice_MAIN")) {
                if (names.contains("SlideShow")) {
                    return MediaType.application("x-corelpresentations"); // .shw
                } else if (names.contains("PerfectOffice_OBJECTS")) {
                    return MediaType.application("x-quattro-pro"); // .wb?
                }
            } else if (names.contains("NativeContent_MAIN")) {
                return MediaType.application("x-quattro-pro"); // .qpw
            } else {
                for (String name : names) {
                    if (name.startsWith("__substg1.0_")) {
                        return MSG;
                    }
                }
            }
        }

        // Couldn't detect a more specific type
        return OLE;
    }

    /**
     * Is this one of the kinds of formats which uses CompObj to
     *  store all of their data, eg Star Draw, Star Impress or
     *  (older) Works?
     * If not, it's likely an embedded resource
     */
    private static MediaType processCompObjFormatType(DirectoryEntry root) {
        try {
            Entry e = root.getEntry("\u0001CompObj");
            if (e != null && e.isDocumentEntry()) {
                DocumentNode dn = (DocumentNode)e;
                DocumentInputStream stream = new DocumentInputStream(dn);
                byte [] bytes = IOUtils.toByteArray(stream);
                /*
                 * This array contains a string with a normal ASCII name of the
                 * application used to create this file. We want to search for that
                 * name.
                 */
                if ( arrayContains(bytes, STAR_DRAW) ) {
                    return SDA;
                } else if (arrayContains(bytes, STAR_IMPRESS)) {
                    return SDD;
                } else if (arrayContains(bytes, WORKS_QUILL96)) {
                   return WPS;
                }
            } 
        } catch (Exception e) {
            /*
             * "root.getEntry" can throw FileNotFoundException. The code inside
             * "if" can throw IOExceptions. Theoretically. Practically no
             * exceptions will likely ever appear.
             * 
             * Swallow all of them. If any occur, we just assume that we can't
             * distinguish between Draw and Impress and return something safe:
             * x-tika-msoffice
             */
        }
        return OLE;
    }
    
    // poor man's search for byte arrays, replace with some library call if
    // you know one without adding new dependencies
    private static boolean arrayContains(byte [] larger, byte [] smaller) {
        int largerCounter = 0;
        int smallerCounter = 0;
        while (largerCounter < larger.length) {
            if (larger[largerCounter] == smaller[smallerCounter]) {
                largerCounter++;
                smallerCounter++;
                if (smallerCounter == smaller.length) {
                    return true;
                }
            } else {
                largerCounter = largerCounter - smallerCounter + 1;
                smallerCounter=0;
            }
        }
        return false;
    }

    private static Set<String> getTopLevelNames(TikaInputStream stream)
            throws IOException {
        // Force the document stream to a (possibly temporary) file
        // so we don't modify the current position of the stream
        File file = stream.getFile();

        try {
            NPOIFSFileSystem fs = new NPOIFSFileSystem(file, true);

            // Optimize a possible later parsing process by keeping
            // a reference to the already opened POI file system
            stream.setOpenContainer(fs);

            return getTopLevelNames(fs.getRoot());
        } catch (IOException e) {
            // Parse error in POI, so we don't know the file type
            return Collections.emptySet();
        } catch (RuntimeException e) {
            // Another problem in POI
            return Collections.emptySet();
        }
    }

    private static Set<String> getTopLevelNames(DirectoryNode root) {
        Set<String> names = new HashSet<String>();
        for (Entry entry : root) {
            names.add(entry.getName());
        }
        return names;
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/microsoft/SummaryExtractor.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.microsoft;

import java.io.FileNotFoundException;
import java.io.IOException;
import java.util.Date;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.poi.hpsf.CustomProperties;
import org.apache.poi.hpsf.DocumentSummaryInformation;
import org.apache.poi.hpsf.MarkUnsupportedException;
import org.apache.poi.hpsf.NoPropertySetStreamException;
import org.apache.poi.hpsf.PropertySet;
import org.apache.poi.hpsf.SummaryInformation;
import org.apache.poi.hpsf.UnexpectedPropertySetTypeException;
import org.apache.poi.poifs.filesystem.DirectoryNode;
import org.apache.poi.poifs.filesystem.DocumentEntry;
import org.apache.poi.poifs.filesystem.DocumentInputStream;
import org.apache.poi.poifs.filesystem.NPOIFSFileSystem;
import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.MSOffice;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.Office;
import org.apache.tika.metadata.OfficeOpenXMLCore;
import org.apache.tika.metadata.OfficeOpenXMLExtended;
import org.apache.tika.metadata.PagedText;
import org.apache.tika.metadata.Property;
import org.apache.tika.metadata.TikaCoreProperties;

/**
 * Extractor for Common OLE2 (HPSF) metadata
 */
public class SummaryExtractor {
    private static final Log logger = LogFactory.getLog(AbstractPOIFSExtractor.class);

    private static final String SUMMARY_INFORMATION =
        SummaryInformation.DEFAULT_STREAM_NAME;

    private static final String DOCUMENT_SUMMARY_INFORMATION =
        DocumentSummaryInformation.DEFAULT_STREAM_NAME;

    private final Metadata metadata;

    public SummaryExtractor(Metadata metadata) {
        this.metadata = metadata;
    }

    public void parseSummaries(NPOIFSFileSystem filesystem)
            throws IOException, TikaException {
        parseSummaries(filesystem.getRoot());
    }

    public void parseSummaries(DirectoryNode root)
            throws IOException, TikaException {
        parseSummaryEntryIfExists(root, SUMMARY_INFORMATION);
        parseSummaryEntryIfExists(root, DOCUMENT_SUMMARY_INFORMATION);
    }

    private void parseSummaryEntryIfExists(
            DirectoryNode root, String entryName)
            throws IOException, TikaException {
        try {
            DocumentEntry entry =
                (DocumentEntry) root.getEntry(entryName);
            PropertySet properties =
                new PropertySet(new DocumentInputStream(entry));
            if (properties.isSummaryInformation()) {
                parse(new SummaryInformation(properties));
            }
            if (properties.isDocumentSummaryInformation()) {
                parse(new DocumentSummaryInformation(properties));
            }
        } catch (FileNotFoundException e) {
            // entry does not exist, just skip it
        } catch (NoPropertySetStreamException e) {
            // no property stream, just skip it
        } catch (UnexpectedPropertySetTypeException e) {
            throw new TikaException("Unexpected HPSF document", e);
        } catch (MarkUnsupportedException e) {
            throw new TikaException("Invalid DocumentInputStream", e);
        } catch (Exception e) {
            logger.warn("Ignoring unexpected exception while parsing summary entry " + entryName, e);
        }
    }

    private void parse(SummaryInformation summary) {
        set(TikaCoreProperties.TITLE, summary.getTitle());
        set(TikaCoreProperties.CREATOR, summary.getAuthor());
        set(TikaCoreProperties.KEYWORDS, summary.getKeywords());
        // TODO Move to OO subject in Tika 2.0
        set(TikaCoreProperties.TRANSITION_SUBJECT_TO_OO_SUBJECT, summary.getSubject());
        set(TikaCoreProperties.MODIFIER, summary.getLastAuthor());
        set(TikaCoreProperties.COMMENTS, summary.getComments());
        set(OfficeOpenXMLExtended.TEMPLATE, summary.getTemplate());
        set(OfficeOpenXMLExtended.APPLICATION, summary.getApplicationName());
        set(OfficeOpenXMLCore.REVISION, summary.getRevNumber());
        set(TikaCoreProperties.CREATED, summary.getCreateDateTime());
        set(TikaCoreProperties.MODIFIED, summary.getLastSaveDateTime());
        set(TikaCoreProperties.PRINT_DATE, summary.getLastPrinted());
        set(Metadata.EDIT_TIME, summary.getEditTime());
        set(OfficeOpenXMLExtended.DOC_SECURITY, summary.getSecurity());
        
        // New style counts
        set(Office.WORD_COUNT, summary.getWordCount());
        set(Office.CHARACTER_COUNT, summary.getCharCount());
        set(Office.PAGE_COUNT, summary.getPageCount());
        if (summary.getPageCount() > 0) {
            metadata.set(PagedText.N_PAGES, summary.getPageCount());
        }
        
        // Old style, Tika 1.0 properties
        // TODO Remove these in Tika 2.0
        set(Metadata.TEMPLATE, summary.getTemplate());
        set(Metadata.APPLICATION_NAME, summary.getApplicationName());
        set(Metadata.REVISION_NUMBER, summary.getRevNumber());
        set(Metadata.SECURITY, summary.getSecurity());
        set(MSOffice.WORD_COUNT, summary.getWordCount());
        set(MSOffice.CHARACTER_COUNT, summary.getCharCount());
        set(MSOffice.PAGE_COUNT, summary.getPageCount());
    }

    private void parse(DocumentSummaryInformation summary) {
        set(OfficeOpenXMLExtended.COMPANY, summary.getCompany());
        set(OfficeOpenXMLExtended.MANAGER, summary.getManager());
        set(TikaCoreProperties.LANGUAGE, getLanguage(summary));
        set(OfficeOpenXMLCore.CATEGORY, summary.getCategory());
        
        // New style counts
        set(Office.SLIDE_COUNT, summary.getSlideCount());
        if (summary.getSlideCount() > 0) {
            metadata.set(PagedText.N_PAGES, summary.getSlideCount());
        }
        // Old style, Tika 1.0 counts
        // TODO Remove these in Tika 2.0
        set(Metadata.COMPANY, summary.getCompany());
        set(Metadata.MANAGER, summary.getManager());
        set(MSOffice.SLIDE_COUNT, summary.getSlideCount());
        set(Metadata.CATEGORY, summary.getCategory());
        
        parse(summary.getCustomProperties());
    }

    private String getLanguage(DocumentSummaryInformation summary) {
        CustomProperties customProperties = summary.getCustomProperties();
        if (customProperties != null) {
            Object value = customProperties.get("Language");
            if (value instanceof String) {
                return (String) value;
            }
        }
        return null;
    }

    /**
     * Attempt to parse custom document properties and add to the collection of metadata
     * @param customProperties
     */
    private void parse(CustomProperties customProperties) {
        if (customProperties != null) {
            for (String name : customProperties.nameSet()) {
                // Apply the custom prefix
                String key = Metadata.USER_DEFINED_METADATA_NAME_PREFIX + name;

                // Get, convert and save property value
                Object value = customProperties.get(name);
                if (value instanceof String){
                    set(key, (String)value);
                } else if (value instanceof Date) {
                    Property prop = Property.externalDate(key);
                    metadata.set(prop, (Date)value);
                } else if (value instanceof Boolean) {
                    Property prop = Property.externalBoolean(key);
                    metadata.set(prop, value.toString());
                } else if (value instanceof Long) {
                    Property prop = Property.externalInteger(key);
                    metadata.set(prop, ((Long)value).intValue());
                } else if (value instanceof Double) {
                    Property prop = Property.externalReal(key);
                    metadata.set(prop, (Double)value);
                } else if (value instanceof Integer) {
                    Property prop = Property.externalInteger(key);
                    metadata.set(prop, ((Integer)value).intValue());
                }
            }
        }
    }

    private void set(String name, String value) {
        if (value != null) {
            metadata.set(name, value);
        }
    }
    
    private void set(Property property, String value) {
        if (value != null) {
            metadata.set(property, value);
        }
    }

    private void set(Property property, Date value) {
        if (value != null) {
            metadata.set(property, value);
        }
    }

    private void set(Property property, int value) {
        if (value > 0) {
            metadata.set(property, value);
        }
    }

    private void set(String name, long value) {
        if (value > 0) {
            metadata.set(name, Long.toString(value));
        }
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/microsoft/TextCell.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.microsoft;

import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.SAXException;

/**
 * Text cell.
 */
public class TextCell implements Cell {

    private final String text;

    public TextCell(String text) {
        this.text = text;
    }

    public void render(XHTMLContentHandler handler) throws SAXException {
        handler.characters(text);
    }

    public String toString() {
        return "Text Cell: \"" + text + "\"";
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/microsoft/TNEFParser.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.microsoft;

import java.io.IOException;
import java.io.InputStream;
import java.util.Arrays;
import java.util.Collections;
import java.util.HashSet;
import java.util.Set;

import org.apache.poi.hmef.Attachment;
import org.apache.poi.hmef.HMEFMessage;
import org.apache.poi.hmef.attribute.MAPIAttribute;
import org.apache.poi.hmef.attribute.MAPIRtfAttribute;
import org.apache.poi.hsmf.datatypes.MAPIProperty;
import org.apache.tika.exception.TikaException;
import org.apache.tika.extractor.EmbeddedDocumentExtractor;
import org.apache.tika.extractor.ParsingEmbeddedDocumentExtractor;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.EmbeddedContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * A POI-powered Tika Parser for TNEF (Transport Neutral
 *  Encoding Format) messages, aka winmail.dat
 */
public class TNEFParser extends AbstractParser {
   private static final long serialVersionUID = 4611820730372823452L;
   
   private static final Set<MediaType> SUPPORTED_TYPES =
        Collections.unmodifiableSet(new HashSet<MediaType>(Arrays.asList(
              MediaType.application("vnd.ms-tnef"),
              MediaType.application("ms-tnef"),
              MediaType.application("x-tnef")
         )));

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }

    /**
     * Extracts properties and text from an MS Document input stream
     */
    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
       
       // We work by recursing, so get the appropriate bits 
       EmbeddedDocumentExtractor ex = context.get(EmbeddedDocumentExtractor.class);
       EmbeddedDocumentExtractor embeddedExtractor;
       if (ex==null) {
           embeddedExtractor = new ParsingEmbeddedDocumentExtractor(context);
       } else {
           embeddedExtractor = ex;
       }
       
       // Ask POI to process the file for us
       HMEFMessage msg = new HMEFMessage(stream);
       
       // Set the message subject if known
       String subject = msg.getSubject();
       if(subject != null && subject.length() > 0) {
          // TODO: Move to title in Tika 2.0
          metadata.set(TikaCoreProperties.TRANSITION_SUBJECT_TO_DC_TITLE, subject);
       }
       
       // Recurse into the message body RTF
       MAPIAttribute attr = msg.getMessageMAPIAttribute(MAPIProperty.RTF_COMPRESSED);
       if(attr != null && attr instanceof MAPIRtfAttribute) {
          MAPIRtfAttribute rtf = (MAPIRtfAttribute)attr;
          handleEmbedded(
                "message.rtf", "application/rtf",
                rtf.getData(),
                embeddedExtractor, handler
          );
       }
       
       // Recurse into each attachment in turn
       for(Attachment attachment : msg.getAttachments()) {
          String name = attachment.getLongFilename();
          if(name == null || name.length() == 0) {
             name = attachment.getFilename();
          }
          if(name == null || name.length() == 0) {
             String ext = attachment.getExtension();
             if(ext != null) {
                name = "unknown" + ext;
             }
          }
          handleEmbedded(
                name, null, attachment.getContents(),
                embeddedExtractor, handler
          );
       }
    }
    
    private void handleEmbedded(String name, String type, byte[] contents,
          EmbeddedDocumentExtractor embeddedExtractor, ContentHandler handler)
          throws IOException, SAXException, TikaException {
       Metadata metadata = new Metadata();
       if(name != null)
          metadata.set(Metadata.RESOURCE_NAME_KEY, name);
       if(type != null)
          metadata.set(Metadata.CONTENT_TYPE, type);

       if (embeddedExtractor.shouldParseEmbedded(metadata)) {
         embeddedExtractor.parseEmbedded(
                 TikaInputStream.get(contents),
                 new EmbeddedContentHandler(handler),
                 metadata, false);
       }
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/microsoft/WordExtractor.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.microsoft;

import java.io.FileNotFoundException;
import java.io.IOException;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Locale;
import java.util.Map;
import java.util.Set;

import org.apache.poi.hwpf.HWPFDocument;
import org.apache.poi.hwpf.HWPFOldDocument;
import org.apache.poi.hwpf.OldWordFileFormatException;
import org.apache.poi.hwpf.extractor.Word6Extractor;
import org.apache.poi.hwpf.model.FieldsDocumentPart;
import org.apache.poi.hwpf.model.PicturesTable;
import org.apache.poi.hwpf.model.StyleDescription;
import org.apache.poi.hwpf.usermodel.CharacterRun;
import org.apache.poi.hwpf.usermodel.Field;
import org.apache.poi.hwpf.usermodel.HeaderStories;
import org.apache.poi.hwpf.usermodel.Paragraph;
import org.apache.poi.hwpf.usermodel.Picture;
import org.apache.poi.hwpf.usermodel.Range;
import org.apache.poi.hwpf.usermodel.Table;
import org.apache.poi.hwpf.usermodel.TableCell;
import org.apache.poi.hwpf.usermodel.TableRow;
import org.apache.poi.poifs.filesystem.DirectoryEntry;
import org.apache.poi.poifs.filesystem.DirectoryNode;
import org.apache.poi.poifs.filesystem.Entry;
import org.apache.poi.poifs.filesystem.NPOIFSFileSystem;
import org.apache.tika.exception.TikaException;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.AttributesImpl;

public class WordExtractor extends AbstractPOIFSExtractor {

    private static final char UNICODECHAR_NONBREAKING_HYPHEN = '\u2011';
    private static final char UNICODECHAR_ZERO_WIDTH_SPACE = '\u200b';

    public WordExtractor(ParseContext context) {
        super(context);
    }

    // True if we are currently in the named style tag:
    private boolean curStrikeThrough;
    private boolean curBold;
    private boolean curItalic;

    protected void parse(
            NPOIFSFileSystem filesystem, XHTMLContentHandler xhtml)
            throws IOException, SAXException, TikaException {
        parse(filesystem.getRoot(), xhtml);
    }

    protected void parse(
            DirectoryNode root, XHTMLContentHandler xhtml)
            throws IOException, SAXException, TikaException {
        HWPFDocument document;
        try {
            document = new HWPFDocument(root);
        } catch(OldWordFileFormatException e) {
            parseWord6(root, xhtml);
            return;
        }
        org.apache.poi.hwpf.extractor.WordExtractor wordExtractor =
            new org.apache.poi.hwpf.extractor.WordExtractor(document);
        HeaderStories headerFooter = new HeaderStories(document);

        // Grab the list of pictures. As far as we can tell,
        //  the pictures should be in order, and may be directly
        //  placed or referenced from an anchor
        PicturesTable pictureTable = document.getPicturesTable();
        PicturesSource pictures = new PicturesSource(document);

        // Do any headers, if present
        Range[] headers = new Range[] { headerFooter.getFirstHeaderSubrange(),
                headerFooter.getEvenHeaderSubrange(), headerFooter.getOddHeaderSubrange() };
        handleHeaderFooter(headers, "header", document, pictures, pictureTable, xhtml);

        // Do the main paragraph text
        Range r = document.getRange();
        for(int i=0; i<r.numParagraphs(); i++) {
           Paragraph p = r.getParagraph(i);
           i += handleParagraph(p, 0, r, document, FieldsDocumentPart.MAIN, pictures, pictureTable, xhtml);
        }

        // Do everything else
        for (String paragraph: wordExtractor.getMainTextboxText()) {
            xhtml.element("p", paragraph);
        }

	for (String paragraph : wordExtractor.getFootnoteText()) {
            xhtml.element("p", paragraph);
        }

        for (String paragraph : wordExtractor.getCommentsText()) {
            xhtml.element("p", paragraph);
        }

        for (String paragraph : wordExtractor.getEndnoteText()) {
            xhtml.element("p", paragraph);
        }

        // Do any footers, if present
        Range[] footers = new Range[] { headerFooter.getFirstFooterSubrange(),
                headerFooter.getEvenFooterSubrange(), headerFooter.getOddFooterSubrange() };
        handleHeaderFooter(footers, "footer", document, pictures, pictureTable, xhtml);

        // Handle any pictures that we haven't output yet
        for(Picture p = pictures.nextUnclaimed(); p != null; ) {
           handlePictureCharacterRun(
                 null, p, pictures, xhtml
           );
           p = pictures.nextUnclaimed();
        }

        // Handle any embeded office documents
        try {
            DirectoryEntry op = (DirectoryEntry) root.getEntry("ObjectPool");
            for (Entry entry : op) {
                if (entry.getName().startsWith("_")
                        && entry instanceof DirectoryEntry) {
                    handleEmbeddedOfficeDoc((DirectoryEntry) entry, xhtml);
                }
            }
        } catch(FileNotFoundException e) {
        }
    }

    private static int countParagraphs(Range... ranges) {
        int count = 0;
        for (Range r : ranges) {
            if (r != null) { count += r.numParagraphs(); }
        }
        return count;
    }

    private void handleHeaderFooter(Range[] ranges, String type, HWPFDocument document,
          PicturesSource pictures, PicturesTable pictureTable, XHTMLContentHandler xhtml)
          throws SAXException, IOException, TikaException {
        if (countParagraphs(ranges) > 0) {
            xhtml.startElement("div", "class", type);
            for (Range r : ranges) {
                if (r != null) {
                    for(int i=0; i<r.numParagraphs(); i++) {
                        Paragraph p = r.getParagraph(i);

                        i += handleParagraph(p, 0, r, document,
                                FieldsDocumentPart.HEADER, pictures, pictureTable, xhtml);
                     }
                }
            }
            xhtml.endElement("div");
        }
    }

    private int handleParagraph(Paragraph p, int parentTableLevel, Range r, HWPFDocument document,
          FieldsDocumentPart docPart, PicturesSource pictures, PicturesTable pictureTable,
          XHTMLContentHandler xhtml) throws SAXException, IOException, TikaException {
       // Note - a poi bug means we can't currently properly recurse
       //  into nested tables, so currently we don't
       if(p.isInTable() && p.getTableLevel() > parentTableLevel && parentTableLevel==0) {
          Table t = r.getTable(p);
          xhtml.startElement("table");
          xhtml.startElement("tbody");
          for(int rn=0; rn<t.numRows(); rn++) {
             TableRow row = t.getRow(rn);
             xhtml.startElement("tr");
             for(int cn=0; cn<row.numCells(); cn++) {
                TableCell cell = row.getCell(cn);
                xhtml.startElement("td");

                for(int pn=0; pn<cell.numParagraphs(); pn++) {
                   Paragraph cellP = cell.getParagraph(pn);
                   handleParagraph(cellP, p.getTableLevel(), cell, document, docPart, pictures, pictureTable, xhtml);
                }
                xhtml.endElement("td");
             }
             xhtml.endElement("tr");
          }
          xhtml.endElement("tbody");
          xhtml.endElement("table");
          return (t.numParagraphs()-1);
       }

       String text = p.text();
       if (text.replaceAll("[\\r\\n\\s]+", "").isEmpty()) {
            // Skip empty paragraphs
            return 0;
       }

       TagAndStyle tas;

       if (document.getStyleSheet().numStyles()>p.getStyleIndex()) {
           StyleDescription style =
              document.getStyleSheet().getStyleDescription(p.getStyleIndex());
           if (style != null && style.getName() != null && style.getName().length() > 0) {
               tas = buildParagraphTagAndStyle(style.getName(), (parentTableLevel>0));
           } else {
               tas = new TagAndStyle("p", null);
           }
       } else {
           tas = new TagAndStyle("p", null);
       }

       if(tas.getStyleClass() != null) {
           xhtml.startElement(tas.getTag(), "class", tas.getStyleClass());
       } else {
           xhtml.startElement(tas.getTag());
       }

       for(int j=0; j<p.numCharacterRuns(); j++) {
          CharacterRun cr = p.getCharacterRun(j);

          // FIELD_BEGIN_MARK:
          if (cr.text().getBytes("UTF-8")[0] == 0x13) {
             Field field = document.getFields().getFieldByStartOffset(docPart, cr.getStartOffset());
             // 58 is an embedded document
             // 56 is a document link
             if (field != null && (field.getType() == 58 || field.getType() == 56)) {
               // Embedded Object: add a <div
               // class="embedded" id="_X"/> so consumer can see where
               // in the main text each embedded document
               // occurred:
               String id = "_" + field.getMarkSeparatorCharacterRun(r).getPicOffset();
               AttributesImpl attributes = new AttributesImpl();
               attributes.addAttribute("", "class", "class", "CDATA", "embedded");
               attributes.addAttribute("", "id", "id", "CDATA", id);
               xhtml.startElement("div", attributes);
               xhtml.endElement("div");
             }
          }

          if(cr.text().equals("\u0013")) {
             j += handleSpecialCharacterRuns(p, j, tas.isHeading(), pictures, xhtml);
          } else if(cr.text().startsWith("\u0008")) {
             // Floating Picture(s)
             for(int pn=0; pn<cr.text().length(); pn++) {
                // Assume they're in the order from the unclaimed list...
                Picture picture = pictures.nextUnclaimed();

                // Output
                handlePictureCharacterRun(cr, picture, pictures, xhtml);
             }
          } else if(pictureTable.hasPicture(cr)) {
             // Inline Picture
             Picture picture = pictures.getFor(cr);
             handlePictureCharacterRun(cr, picture, pictures, xhtml);
          } else {
             handleCharacterRun(cr, tas.isHeading(), xhtml);
          }
       }

       // Close any still open style tags
       if (curStrikeThrough) {
         xhtml.endElement("s");
         curStrikeThrough = false;
       }
       if (curItalic) {
         xhtml.endElement("i");
         curItalic = false;
       }
       if (curBold) {
         xhtml.endElement("b");
         curBold = false;
       }

       xhtml.endElement(tas.getTag());

       return 0;
    }

    private void handleCharacterRun(CharacterRun cr, boolean skipStyling, XHTMLContentHandler xhtml)
          throws SAXException {
       // Skip trailing newlines
       if(!isRendered(cr) || cr.text().equals("\r"))
          return;

       if(!skipStyling) {
         if (cr.isBold() != curBold) {
           // Enforce nesting -- must close s and i tags
           if (curStrikeThrough) {
             xhtml.endElement("s");
             curStrikeThrough = false;
           }
           if (curItalic) {
             xhtml.endElement("i");
             curItalic = false;
           }
           if (cr.isBold()) {
             xhtml.startElement("b");
           } else {
             xhtml.endElement("b");
           }
           curBold = cr.isBold();
         }

         if (cr.isItalic() != curItalic) {
           // Enforce nesting -- must close s tag
           if (curStrikeThrough) {
             xhtml.endElement("s");
             curStrikeThrough = false;
           }
           if (cr.isItalic()) {
             xhtml.startElement("i");
           } else {
             xhtml.endElement("i");
           }
           curItalic = cr.isItalic();
         }

         if (cr.isStrikeThrough() != curStrikeThrough) {
           if (cr.isStrikeThrough()) {
             xhtml.startElement("s");
           } else {
             xhtml.endElement("s");
           }
           curStrikeThrough = cr.isStrikeThrough();
         }
       }

       // Clean up the text
       String text = cr.text();
       text = text.replace('\r', '\n');
       if(text.endsWith("\u0007")) {
          // Strip the table cell end marker
          text = text.substring(0, text.length()-1);
       }

       // Copied from POI's org/apache/poi/hwpf/converter/AbstractWordConverter.processCharacters:

       // Non-breaking hyphens are returned as char 30
       text = text.replace((char) 30, UNICODECHAR_NONBREAKING_HYPHEN);

       // Non-required hyphens to zero-width space
       text = text.replace((char) 31, UNICODECHAR_ZERO_WIDTH_SPACE);

       // Control characters as line break
       text = text.replaceAll("[\u0000-\u001f]", "\n");
       xhtml.characters(text);
    }
    /**
     * Can be \13..text..\15 or \13..control..\14..text..\15 .
     * Nesting is allowed
     */
    private int handleSpecialCharacterRuns(Paragraph p, int index, boolean skipStyling,
          PicturesSource pictures, XHTMLContentHandler xhtml) throws SAXException, TikaException, IOException {
       List<CharacterRun> controls = new ArrayList<CharacterRun>();
       List<CharacterRun> texts = new ArrayList<CharacterRun>();
       boolean has14 = false;

       // Split it into before and after the 14
       int i;
       for(i=index+1; i<p.numCharacterRuns(); i++) {
          CharacterRun cr = p.getCharacterRun(i);
          if(cr.text().equals("\u0013")) {
             // Nested, oh joy...
             int increment = handleSpecialCharacterRuns(p, i+1, skipStyling, pictures, xhtml);
             i += increment;
          } else if(cr.text().equals("\u0014")) {
             has14 = true;
          } else if(cr.text().equals("\u0015")) {
             if(!has14) {
                texts = controls;
                controls = new ArrayList<CharacterRun>();
             }
             break;
          } else {
             if(has14) {
                texts.add(cr);
             } else {
                controls.add(cr);
             }
          }
       }

       // Do we need to do something special with this?
       if(controls.size() > 0) {
          String text = controls.get(0).text();
          for(int j=1; j<controls.size(); j++) {
             text += controls.get(j).text();
          }

          if((text.startsWith("HYPERLINK") || text.startsWith(" HYPERLINK"))
                 && text.indexOf('"') > -1) {
             String url = text.substring(
                   text.indexOf('"') + 1,
                   text.lastIndexOf('"')
             );
             xhtml.startElement("a", "href", url);
             for(CharacterRun cr : texts) {
                handleCharacterRun(cr, skipStyling, xhtml);
             }
             xhtml.endElement("a");
          } else {
             // Just output the text ones
             for(CharacterRun cr : texts) {
                if(pictures.hasPicture(cr)) {
                   Picture picture = pictures.getFor(cr);
                   handlePictureCharacterRun(cr, picture, pictures, xhtml);
                } else {
                   handleCharacterRun(cr, skipStyling, xhtml);
                }
             }
          }
       } else {
          // We only had text
          // Output as-is
          for(CharacterRun cr : texts) {
             handleCharacterRun(cr, skipStyling, xhtml);
          }
       }

       // Tell them how many to skip over
       return i-index;
    }

    private void handlePictureCharacterRun(CharacterRun cr, Picture picture, PicturesSource pictures, XHTMLContentHandler xhtml)
          throws SAXException, IOException, TikaException {
       if(!isRendered(cr) || picture == null) {
          // Oh dear, we've run out...
          // Probably caused by multiple \u0008 images referencing
          //  the same real image
          return;
       }

       // Which one is it?
       String extension = picture.suggestFileExtension();
       int pictureNumber = pictures.pictureNumber(picture);

       // Make up a name for the picture
       // There isn't one in the file, but we need to be able to reference
       //  the picture from the img tag and the embedded resource
       String filename = "image"+pictureNumber+(extension.length()>0 ? "."+extension : "");

       // Grab the mime type for the picture
       String mimeType = picture.getMimeType();

       // Output the img tag
       AttributesImpl attr = new AttributesImpl();
       attr.addAttribute("", "src", "src", "CDATA", "embedded:" + filename);
       attr.addAttribute("", "alt", "alt", "CDATA", filename);
       xhtml.startElement("img", attr);
       xhtml.endElement("img");

       // Have we already output this one?
       // (Only expose each individual image once)
       if(! pictures.hasOutput(picture)) {
          TikaInputStream stream = TikaInputStream.get(picture.getContent());
          handleEmbeddedResource(stream, filename, null, mimeType, xhtml, false);
          pictures.recordOutput(picture);
       }
    }

    /**
     * Outputs a section of text if the given text is non-empty.
     *
     * @param xhtml XHTML content handler
     * @param section the class of the &lt;div/&gt; section emitted
     * @param text text to be emitted, if any
     * @throws SAXException if an error occurs
     */
    private void addTextIfAny(
            XHTMLContentHandler xhtml, String section, String text)
            throws SAXException {
        if (text != null && text.length() > 0) {
            xhtml.startElement("div", "class", section);
            xhtml.element("p", text);
            xhtml.endElement("div");
        }
    }

    protected void parseWord6(
            NPOIFSFileSystem filesystem, XHTMLContentHandler xhtml)
            throws IOException, SAXException, TikaException {
        parseWord6(filesystem.getRoot(), xhtml);
    }

    protected void parseWord6(
            DirectoryNode root, XHTMLContentHandler xhtml)
            throws IOException, SAXException, TikaException {
        HWPFOldDocument doc = new HWPFOldDocument(root);
        Word6Extractor extractor = new Word6Extractor(doc);

        for(String p : extractor.getParagraphText()) {
            xhtml.element("p", p);
        }
    }

    private static final Map<String,TagAndStyle> fixedParagraphStyles = new HashMap<String,TagAndStyle>();
    private static final TagAndStyle defaultParagraphStyle = new TagAndStyle("p", null);
    static {
        fixedParagraphStyles.put("Default", defaultParagraphStyle);
        fixedParagraphStyles.put("Normal", defaultParagraphStyle);
        fixedParagraphStyles.put("heading", new TagAndStyle("h1", null));
        fixedParagraphStyles.put("Heading", new TagAndStyle("h1", null));
        fixedParagraphStyles.put("Title", new TagAndStyle("h1", "title"));
        fixedParagraphStyles.put("Subtitle", new TagAndStyle("h2", "subtitle"));
        fixedParagraphStyles.put("HTML Preformatted", new TagAndStyle("pre", null));
    }

    /**
     * Given a style name, return what tag should be used, and
     *  what style should be applied to it.
     */
    public static TagAndStyle buildParagraphTagAndStyle(String styleName, boolean isTable) {
       TagAndStyle tagAndStyle = fixedParagraphStyles.get(styleName);
       if (tagAndStyle != null) {
           return tagAndStyle;
       }

       if (styleName.equals("Table Contents") && isTable) {
           return defaultParagraphStyle;
       }

       String tag = "p";
       String styleClass = null;

       if(styleName.startsWith("heading") || styleName.startsWith("Heading")) {
           // "Heading 3" or "Heading2" or "heading 4"
           int num = 1;
           try {
               num = Integer.parseInt(
                                      styleName.substring(styleName.length()-1)
                                      );
           } catch(NumberFormatException e) {}
           // Turn it into a H1 - H6 (H7+ isn't valid!)
           tag = "h" + Math.min(num, 6);
       } else {
           styleClass = styleName.replace(' ', '_');
           styleClass = styleClass.substring(0,1).toLowerCase(Locale.ROOT) +
               styleClass.substring(1);
       }

       return new TagAndStyle(tag,styleClass);
    }

    public static class TagAndStyle {
       private String tag;
       private String styleClass;
       public TagAndStyle(String tag, String styleClass) {
         this.tag = tag;
         this.styleClass = styleClass;
       }
       public String getTag() {
         return tag;
       }
       public String getStyleClass() {
         return styleClass;
       }
       public boolean isHeading() {
          return tag.length()==2 && tag.startsWith("h");
       }
    }

    /**
     * Determines if character run should be included in the extraction.
     *
     * @param cr character run.
     * @return true if character run should be included in extraction.
     */
    private boolean isRendered(final CharacterRun cr) {
 	   return cr == null || !cr.isMarkedDeleted();
    }


    /**
     * Provides access to the pictures both by offset, iteration
     *  over the un-claimed, and peeking forward
     */
    private static class PicturesSource {
       private PicturesTable picturesTable;
       private Set<Picture> output = new HashSet<Picture>();
       private Map<Integer,Picture> lookup;
       private List<Picture> nonU1based;
       private List<Picture> all;
       private int pn = 0;

       private PicturesSource(HWPFDocument doc) {
          picturesTable = doc.getPicturesTable();
          all = picturesTable.getAllPictures();

          // Build the Offset-Picture lookup map
          lookup = new HashMap<Integer, Picture>();
          for(Picture p : all) {
             lookup.put(p.getStartOffset(), p);
          }

          // Work out which Pictures aren't referenced by
          //  a \u0001 in the main text
          // These are \u0008 escher floating ones, ones
          //  found outside the normal text, and who
          //  knows what else...
          nonU1based = new ArrayList<Picture>();
          nonU1based.addAll(all);
          Range r = doc.getRange();
          for(int i=0; i<r.numCharacterRuns(); i++) {
             CharacterRun cr = r.getCharacterRun(i);
             if(picturesTable.hasPicture(cr)) {
                Picture p = getFor(cr);
                int at = nonU1based.indexOf(p);
                nonU1based.set(at, null);
             }
          }
       }

       private boolean hasPicture(CharacterRun cr) {
          return picturesTable.hasPicture(cr);
       }

       private void recordOutput(Picture picture) {
          output.add(picture);
       }
       private boolean hasOutput(Picture picture) {
          return output.contains(picture);
       }

       private int pictureNumber(Picture picture) {
          return all.indexOf(picture) + 1;
       }

       private Picture getFor(CharacterRun cr) {
          return lookup.get(cr.getPicOffset());
       }

       /**
        * Return the next unclaimed one, used towards
        *  the end
        */
       private Picture nextUnclaimed() {
          Picture p = null;
          while(pn < nonU1based.size()) {
             p = nonU1based.get(pn);
             pn++;
             if(p != null) return p;
          }
          return null;
       }
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/AbstractOOXMLExtractor.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.microsoft.ooxml;

import static org.apache.tika.sax.XHTMLContentHandler.XHTML;

import java.io.FileNotFoundException;
import java.io.IOException;
import java.io.InputStream;
import java.net.URI;
import java.util.List;

import org.apache.poi.POIXMLDocument;
import org.apache.poi.POIXMLTextExtractor;
import org.apache.poi.openxml4j.exceptions.InvalidFormatException;
import org.apache.poi.openxml4j.opc.OPCPackage;
import org.apache.poi.openxml4j.opc.PackagePart;
import org.apache.poi.openxml4j.opc.PackageRelationship;
import org.apache.poi.openxml4j.opc.PackageRelationshipTypes;
import org.apache.poi.openxml4j.opc.TargetMode;
import org.apache.poi.poifs.filesystem.DirectoryNode;
import org.apache.poi.poifs.filesystem.Ole10Native;
import org.apache.poi.poifs.filesystem.Ole10NativeException;
import org.apache.poi.poifs.filesystem.POIFSFileSystem;
import org.apache.tika.exception.TikaException;
import org.apache.tika.extractor.EmbeddedDocumentExtractor;
import org.apache.tika.extractor.ParsingEmbeddedDocumentExtractor;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.microsoft.OfficeParser.POIFSDocumentType;
import org.apache.tika.sax.EmbeddedContentHandler;
import org.apache.tika.sax.XHTMLContentHandler;
import org.apache.xmlbeans.XmlException;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.AttributesImpl;

/**
 * Base class for all Tika OOXML extractors.
 * 
 * Tika extractors decorate POI extractors so that the parsed content of
 * documents is returned as a sequence of XHTML SAX events. Subclasses must
 * implement the buildXHTML method {@link #buildXHTML(XHTMLContentHandler)} that
 * populates the {@link XHTMLContentHandler} object received as parameter.
 */
public abstract class AbstractOOXMLExtractor implements OOXMLExtractor {
    static final String RELATION_AUDIO = "http://schemas.openxmlformats.org/officeDocument/2006/relationships/audio";
    static final String RELATION_IMAGE = "http://schemas.openxmlformats.org/officeDocument/2006/relationships/image";
    static final String RELATION_OLE_OBJECT = "http://schemas.openxmlformats.org/officeDocument/2006/relationships/oleObject";
    static final String RELATION_PACKAGE = "http://schemas.openxmlformats.org/officeDocument/2006/relationships/package";

    private static final String TYPE_OLE_OBJECT =
            "application/vnd.openxmlformats-officedocument.oleObject";

    protected POIXMLTextExtractor extractor;

    private final EmbeddedDocumentExtractor embeddedExtractor;

    public AbstractOOXMLExtractor(ParseContext context, POIXMLTextExtractor extractor) {
        this.extractor = extractor;

        EmbeddedDocumentExtractor ex = context.get(EmbeddedDocumentExtractor.class);

        if (ex==null) {
            embeddedExtractor = new ParsingEmbeddedDocumentExtractor(context);
        } else {
            embeddedExtractor = ex;
        }

    }

    /**
     * @see org.apache.tika.parser.microsoft.ooxml.OOXMLExtractor#getDocument()
     */
    public POIXMLDocument getDocument() {
        return extractor.getDocument();
    }

    /**
     * @see org.apache.tika.parser.microsoft.ooxml.OOXMLExtractor#getMetadataExtractor()
     */
    public MetadataExtractor getMetadataExtractor() {
        return new MetadataExtractor(extractor);
    }

    /**
     * @see org.apache.tika.parser.microsoft.ooxml.OOXMLExtractor#getXHTML(org.xml.sax.ContentHandler,
     *      org.apache.tika.metadata.Metadata)
     */
    public void getXHTML(
            ContentHandler handler, Metadata metadata, ParseContext context)
            throws SAXException, XmlException, IOException, TikaException {
        XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);
        xhtml.startDocument();

        buildXHTML(xhtml);

        // Now do any embedded parts
        handleEmbeddedParts(handler);
        
        // thumbnail
        handleThumbnail(handler);

        xhtml.endDocument();
    }
  
    protected String getJustFileName(String desc) {
      int idx = desc.lastIndexOf('/');
      if (idx != -1) {
        desc = desc.substring(idx+1);
      }
      idx = desc.lastIndexOf('.');
      if (idx != -1) {
        desc = desc.substring(0, idx);
      }

      return desc;
    }
    
    private void handleThumbnail( ContentHandler handler ) {
        try {
            OPCPackage opcPackage = extractor.getPackage();
            for (PackageRelationship rel : opcPackage.getRelationshipsByType( PackageRelationshipTypes.THUMBNAIL )) {
                PackagePart tPart = opcPackage.getPart(rel);
                InputStream tStream = tPart.getInputStream();
                Metadata thumbnailMetadata = new Metadata();                
                String thumbName = tPart.getPartName().getName();
                thumbnailMetadata.set(Metadata.RESOURCE_NAME_KEY, thumbName);
                
                AttributesImpl attributes = new AttributesImpl();
                attributes.addAttribute(XHTML, "class", "class", "CDATA", "embedded");
                attributes.addAttribute(XHTML, "id", "id", "CDATA", thumbName);
                handler.startElement(XHTML, "div", "div", attributes);
                handler.endElement(XHTML, "div", "div");
                
                thumbnailMetadata.set(Metadata.EMBEDDED_RELATIONSHIP_ID, thumbName);
                thumbnailMetadata.set(Metadata.CONTENT_TYPE, tPart.getContentType());
                thumbnailMetadata.set(TikaCoreProperties.TITLE, tPart.getPartName().getName());
                
                if (embeddedExtractor.shouldParseEmbedded(thumbnailMetadata)) {
                    embeddedExtractor.parseEmbedded(TikaInputStream.get(tStream), new EmbeddedContentHandler(handler), thumbnailMetadata, false);
                }
                
                tStream.close();
            }
         } catch (Exception ex) {
             
         }
    }

    private void handleEmbeddedParts(ContentHandler handler)
            throws TikaException, IOException, SAXException {
        try {
            for (PackagePart source : getMainDocumentParts()) {
                for (PackageRelationship rel : source.getRelationships()) {

                    URI sourceURI = rel.getSourceURI();
                    String sourceDesc;
                    if (sourceURI != null) {
                        sourceDesc = getJustFileName(sourceURI.getPath());
                        if (sourceDesc.startsWith("slide")) {
                          sourceDesc += "_";
                        } else {
                          sourceDesc = "";
                        }
                    } else {
                        sourceDesc = "";
                    }
                    if (rel.getTargetMode() == TargetMode.INTERNAL) {
                        PackagePart target;

                        try {
                            target = source.getRelatedPart(rel);
                        } catch (IllegalArgumentException ex) {
                            continue;
                        }

                        String type = rel.getRelationshipType();
                        if (RELATION_OLE_OBJECT.equals(type)
                                && TYPE_OLE_OBJECT.equals(target.getContentType())) {
                            handleEmbeddedOLE(target, handler, sourceDesc + rel.getId());
                        } else if (RELATION_AUDIO.equals(type)
                                || RELATION_IMAGE.equals(type)
                                || RELATION_PACKAGE.equals(type)
                                || RELATION_OLE_OBJECT.equals(type)) {
                            handleEmbeddedFile(target, handler, sourceDesc + rel.getId());
                        }
                    }
                }
            }
        } catch (InvalidFormatException e) {
            throw new TikaException("Broken OOXML file", e);
        }
    }

    /**
     * Handles an embedded OLE object in the document
     */
    private void handleEmbeddedOLE(PackagePart part, ContentHandler handler, String rel)
            throws IOException, SAXException {
        // A POIFSFileSystem needs to be at least 3 blocks big to be valid
        if (part.getSize() >= 0 && part.getSize() < 512*3) {
           // Too small, skip
           return;
        }
       
        // Open the POIFS (OLE2) structure and process
        POIFSFileSystem fs = new POIFSFileSystem(part.getInputStream());
        try {
            Metadata metadata = new Metadata();
            TikaInputStream stream = null;
            metadata.set(Metadata.EMBEDDED_RELATIONSHIP_ID, rel);

            DirectoryNode root = fs.getRoot();
            POIFSDocumentType type = POIFSDocumentType.detectType(root);
            
            if (root.hasEntry("CONTENTS")
                  && root.hasEntry("\u0001Ole")
                  && root.hasEntry("\u0001CompObj")
                  && root.hasEntry("\u0003ObjInfo")) {
               // TIKA-704: OLE 2.0 embedded non-Office document?
               stream = TikaInputStream.get(
                     fs.createDocumentInputStream("CONTENTS"));
               if (embeddedExtractor.shouldParseEmbedded(metadata)) {
                  embeddedExtractor.parseEmbedded(
                        stream, new EmbeddedContentHandler(handler),
                        metadata, false);
               }
            } else if (POIFSDocumentType.OLE10_NATIVE == type) {
                // TIKA-704: OLE 1.0 embedded document
                Ole10Native ole =
                        Ole10Native.createFromEmbeddedOleObject(fs);
                if (ole.getLabel() != null) {
                    metadata.set(Metadata.RESOURCE_NAME_KEY, ole.getLabel());
                }
                byte[] data = ole.getDataBuffer();
                if (data != null) {
                    stream = TikaInputStream.get(data);
                }

                if (stream != null
                        && embeddedExtractor.shouldParseEmbedded(metadata)) {
                    embeddedExtractor.parseEmbedded(
                            stream, new EmbeddedContentHandler(handler),
                            metadata, false);
                }
            } else {
                handleEmbeddedFile(part, handler, rel);
            }
        } catch (FileNotFoundException e) {
            // There was no CONTENTS entry, so skip this part
        } catch (Ole10NativeException e) {
            // Could not process an OLE 1.0 entry, so skip this part
        }
    }

    /**
     * Handles an embedded file in the document
     */
    protected void handleEmbeddedFile(PackagePart part, ContentHandler handler, String rel)
            throws SAXException, IOException {
        Metadata metadata = new Metadata();
        metadata.set(Metadata.EMBEDDED_RELATIONSHIP_ID, rel);

        // Get the name
        String name = part.getPartName().getName();
        metadata.set(
                Metadata.RESOURCE_NAME_KEY,
                name.substring(name.lastIndexOf('/') + 1));

        // Get the content type
        metadata.set(
                Metadata.CONTENT_TYPE, part.getContentType());

        // Call the recursing handler
        if (embeddedExtractor.shouldParseEmbedded(metadata)) {
            embeddedExtractor.parseEmbedded(
                    TikaInputStream.get(part.getInputStream()),
                    new EmbeddedContentHandler(handler),
                    metadata, false);
        }
    }

    /**
     * Populates the {@link XHTMLContentHandler} object received as parameter.
     */
    protected abstract void buildXHTML(XHTMLContentHandler xhtml)
            throws SAXException, XmlException, IOException;
    
    /**
     * Return a list of the main parts of the document, used
     *  when searching for embedded resources.
     * This should be all the parts of the document that end
     *  up with things embedded into them.
     */
    protected abstract List<PackagePart> getMainDocumentParts()
            throws TikaException;
}
"
tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/MetadataExtractor.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.microsoft.ooxml;

import java.math.BigDecimal;
import java.util.Date;

import org.apache.poi.POIXMLTextExtractor;
import org.apache.poi.POIXMLProperties.CoreProperties;
import org.apache.poi.POIXMLProperties.CustomProperties;
import org.apache.poi.POIXMLProperties.ExtendedProperties;
import org.apache.poi.openxml4j.opc.internal.PackagePropertiesPart;
import org.apache.poi.openxml4j.util.Nullable;
import org.apache.poi.xssf.extractor.XSSFEventBasedExcelExtractor;
import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.MSOffice;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.Office;
import org.apache.tika.metadata.OfficeOpenXMLCore;
import org.apache.tika.metadata.OfficeOpenXMLExtended;
import org.apache.tika.metadata.PagedText;
import org.apache.tika.metadata.Property;
import org.apache.tika.metadata.TikaCoreProperties;
import org.openxmlformats.schemas.officeDocument.x2006.customProperties.CTProperty;
import org.openxmlformats.schemas.officeDocument.x2006.extendedProperties.CTProperties;

/**
 * OOXML metadata extractor.
 * 
 * Currently POI doesn't support metadata extraction for OOXML.
 * 
 * @see OOXMLExtractor#getMetadataExtractor()
 */
public class MetadataExtractor {

    private final POIXMLTextExtractor extractor;

    public MetadataExtractor(POIXMLTextExtractor extractor) {
        this.extractor = extractor;
    }

    public void extract(Metadata metadata) throws TikaException {
        if (extractor.getDocument() != null ||
              (extractor instanceof XSSFEventBasedExcelExtractor && 
               extractor.getPackage() != null)) {
            extractMetadata(extractor.getCoreProperties(), metadata);
            extractMetadata(extractor.getExtendedProperties(), metadata);
            extractMetadata(extractor.getCustomProperties(), metadata);
        }
    }

    private void extractMetadata(CoreProperties properties, Metadata metadata) {
        PackagePropertiesPart propsHolder = properties
                .getUnderlyingProperties();

        addProperty(metadata, OfficeOpenXMLCore.CATEGORY, propsHolder.getCategoryProperty());
        addProperty(metadata, OfficeOpenXMLCore.CONTENT_STATUS, propsHolder
                .getContentStatusProperty());
        addProperty(metadata, TikaCoreProperties.CREATED, propsHolder
                .getCreatedProperty());
        addProperty(metadata, TikaCoreProperties.CREATOR, propsHolder
                .getCreatorProperty());
        addProperty(metadata, TikaCoreProperties.DESCRIPTION, propsHolder
                .getDescriptionProperty());
        addProperty(metadata, TikaCoreProperties.IDENTIFIER, propsHolder
                .getIdentifierProperty());
        addProperty(metadata, TikaCoreProperties.KEYWORDS, propsHolder
                .getKeywordsProperty());
        addProperty(metadata, TikaCoreProperties.LANGUAGE, propsHolder
                .getLanguageProperty());
        addProperty(metadata, TikaCoreProperties.MODIFIER, propsHolder
                .getLastModifiedByProperty());
        addProperty(metadata, TikaCoreProperties.PRINT_DATE, propsHolder
                .getLastPrintedProperty());
        addProperty(metadata, Metadata.LAST_MODIFIED, propsHolder
                .getModifiedProperty());
        addProperty(metadata, TikaCoreProperties.MODIFIED, propsHolder
              .getModifiedProperty());
        addProperty(metadata, OfficeOpenXMLCore.REVISION, propsHolder
                .getRevisionProperty());
        // TODO: Move to OO subject in Tika 2.0
        addProperty(metadata, TikaCoreProperties.TRANSITION_SUBJECT_TO_OO_SUBJECT, 
                propsHolder.getSubjectProperty());
        addProperty(metadata, TikaCoreProperties.TITLE, propsHolder.getTitleProperty());
        addProperty(metadata, OfficeOpenXMLCore.VERSION, propsHolder.getVersionProperty());
        
        // Legacy Tika-1.0 style stats
        // TODO Remove these in Tika 2.0
        addProperty(metadata, Metadata.CATEGORY, propsHolder.getCategoryProperty());
        addProperty(metadata, Metadata.CONTENT_STATUS, propsHolder
                .getContentStatusProperty());
        addProperty(metadata, Metadata.REVISION_NUMBER, propsHolder
                .getRevisionProperty());
        addProperty(metadata, Metadata.VERSION, propsHolder.getVersionProperty());
    }

    private void extractMetadata(ExtendedProperties properties,
            Metadata metadata) {
        CTProperties propsHolder = properties.getUnderlyingProperties();

        addProperty(metadata, OfficeOpenXMLExtended.APPLICATION, propsHolder.getApplication());
        addProperty(metadata, OfficeOpenXMLExtended.APP_VERSION, propsHolder.getAppVersion());
        addProperty(metadata, TikaCoreProperties.PUBLISHER, propsHolder.getCompany());
        addProperty(metadata, OfficeOpenXMLExtended.COMPANY, propsHolder.getCompany());
        addProperty(metadata, OfficeOpenXMLExtended.MANAGER, propsHolder.getManager());
        addProperty(metadata, OfficeOpenXMLExtended.NOTES, propsHolder.getNotes());
        addProperty(metadata, OfficeOpenXMLExtended.PRESENTATION_FORMAT, propsHolder.getPresentationFormat());
        addProperty(metadata, OfficeOpenXMLExtended.TEMPLATE, propsHolder.getTemplate());
        addProperty(metadata, OfficeOpenXMLExtended.TOTAL_TIME, propsHolder.getTotalTime());

        if (propsHolder.getPages() > 0) {
           metadata.set(PagedText.N_PAGES, propsHolder.getPages());
        } else if (propsHolder.getSlides() > 0) {
           metadata.set(PagedText.N_PAGES, propsHolder.getSlides());
        }

        // Process the document statistics
        addProperty(metadata, Office.PAGE_COUNT, propsHolder.getPages());
        addProperty(metadata, Office.SLIDE_COUNT, propsHolder.getSlides());
        addProperty(metadata, Office.PARAGRAPH_COUNT, propsHolder.getParagraphs());
        addProperty(metadata, Office.LINE_COUNT, propsHolder.getLines());
        addProperty(metadata, Office.WORD_COUNT, propsHolder.getWords());
        addProperty(metadata, Office.CHARACTER_COUNT, propsHolder.getCharacters());
        addProperty(metadata, Office.CHARACTER_COUNT_WITH_SPACES, propsHolder.getCharactersWithSpaces());
        
        // Legacy Tika-1.0 style stats
        // TODO Remove these in Tika 2.0
        addProperty(metadata, Metadata.APPLICATION_NAME, propsHolder.getApplication());
        addProperty(metadata, Metadata.APPLICATION_VERSION, propsHolder.getAppVersion());
        addProperty(metadata, Metadata.MANAGER, propsHolder.getManager());
        addProperty(metadata, Metadata.NOTES, propsHolder.getNotes());
        addProperty(metadata, Metadata.PRESENTATION_FORMAT, propsHolder.getPresentationFormat());
        addProperty(metadata, Metadata.TEMPLATE, propsHolder.getTemplate());
        addProperty(metadata, Metadata.TOTAL_TIME, propsHolder.getTotalTime());
        addProperty(metadata, MSOffice.PAGE_COUNT, propsHolder.getPages());
        addProperty(metadata, MSOffice.SLIDE_COUNT, propsHolder.getSlides());
        addProperty(metadata, MSOffice.PARAGRAPH_COUNT, propsHolder.getParagraphs());
        addProperty(metadata, MSOffice.LINE_COUNT, propsHolder.getLines());
        addProperty(metadata, MSOffice.WORD_COUNT, propsHolder.getWords());
        addProperty(metadata, MSOffice.CHARACTER_COUNT, propsHolder.getCharacters());
        addProperty(metadata, MSOffice.CHARACTER_COUNT_WITH_SPACES, propsHolder.getCharactersWithSpaces());
    }

    private void extractMetadata(CustomProperties properties,
          Metadata metadata) {
       org.openxmlformats.schemas.officeDocument.x2006.customProperties.CTProperties
           props = properties.getUnderlyingProperties();
       for (int i = 0; i < props.sizeOfPropertyArray(); i++) {
          CTProperty property = props.getPropertyArray(i);
          String val = null;
          Date date = null;

          if (property.isSetLpwstr()) {
             val = property.getLpwstr(); 
          }
          else if (property.isSetLpstr()) {
             val = property.getLpstr(); 
          }
          else if (property.isSetDate()) {
             date = property.getDate().getTime(); 
          }
          else if (property.isSetFiletime()) {
             date = property.getFiletime().getTime(); 
          }

          else if (property.isSetBool()) {
             val = Boolean.toString( property.getBool() );
          }

          // Integers
          else if (property.isSetI1()) {
             val = Integer.toString(property.getI1()); 
          }
          else if (property.isSetI2()) {
             val = Integer.toString(property.getI2()); 
          }
          else if (property.isSetI4()) {
             val = Integer.toString(property.getI4()); 
          }
          else if (property.isSetI8()) {
             val = Long.toString(property.getI8()); 
          }
          else if (property.isSetInt()) {
             val = Integer.toString( property.getInt() ); 
          }

          // Unsigned Integers
          else if (property.isSetUi1()) {
             val = Integer.toString(property.getUi1()); 
          }
          else if (property.isSetUi2()) {
             val = Integer.toString(property.getUi2()); 
          }
          else if (property.isSetUi4()) {
             val = Long.toString(property.getUi4()); 
          }
          else if (property.isSetUi8()) {
             val = property.getUi8().toString(); 
          }
          else if (property.isSetUint()) {
             val = Long.toString(property.getUint()); 
          }

          // Reals
          else if (property.isSetR4()) {
             val = Float.toString( property.getR4() ); 
          }
          else if (property.isSetR8()) {
             val = Double.toString( property.getR8() ); 
          }
          else if (property.isSetDecimal()) {
             BigDecimal d = property.getDecimal();
             if (d == null) {
                val = null;
             } else {
                val = d.toPlainString();
             }
          }

          else if (property.isSetArray()) {
             // TODO Fetch the array values and output
          }
          else if (property.isSetVector()) {
             // TODO Fetch the vector values and output
          }

          else if (property.isSetBlob() || property.isSetOblob()) {
             // TODO Decode, if possible
          }
          else if (property.isSetStream() || property.isSetOstream() ||
                   property.isSetVstream()) {
             // TODO Decode, if possible
          }
          else if (property.isSetStorage() || property.isSetOstorage()) {
             // TODO Decode, if possible
          }
          
          else {
             // This type isn't currently supported yet, skip the property
          }
          
          String propName = "custom:" + property.getName();
          if (date != null) {
             Property tikaProp = Property.externalDate(propName);
             metadata.set(tikaProp, date);
          } else if (val != null) {
             metadata.set(propName, val);
          }
       }
    }
    
    private <T> void addProperty(Metadata metadata, Property property, Nullable<T> nullableValue) {
        T value = nullableValue.getValue();
        if (value != null) {
            if (value instanceof Date) {
                metadata.set(property, (Date) value);
            } else if (value instanceof String) {
                metadata.set(property, (String) value);
            } else if (value instanceof Integer) {
                metadata.set(property, (Integer) value);
            } else if (value instanceof Double) {
                metadata.set(property, (Double) value);
            }
        }
    }

    private void addProperty(Metadata metadata, String name, Nullable<?> value) {
        if (value.getValue() != null) {
            addProperty(metadata, name, value.getValue().toString());
        }
    }
    
    private void addProperty(Metadata metadata, Property property, String value) {
        if (value != null) {
            metadata.set(property, value);
        }
    }

    private void addProperty(Metadata metadata, String name, String value) {
        if (value != null) {
            metadata.set(name, value);
        }
    }

    private void addProperty(Metadata metadata, Property property, int value) {
       if (value > 0) {
           metadata.set(property, value);
       }
    }
    
    private void addProperty(Metadata metadata, String name, int value) {
        if (value > 0) {
            metadata.set(name, Integer.toString(value));
        }
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/OOXMLExtractor.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.microsoft.ooxml;

import java.io.IOException;

import org.apache.poi.POIXMLDocument;
import org.apache.poi.POIXMLTextExtractor;
import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.parser.ParseContext;
import org.apache.xmlbeans.XmlException;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * Interface implemented by all Tika OOXML extractors.
 * 
 * @see org.apache.poi.POIXMLTextExtractor
 */
public interface OOXMLExtractor {

    /**
     * Returns the opened document.
     * 
     * @see POIXMLTextExtractor#getDocument()
     */
    POIXMLDocument getDocument();

    /**
     * {@link POIXMLTextExtractor#getMetadataTextExtractor()} not yet supported
     * for OOXML by POI.
     */
    MetadataExtractor getMetadataExtractor();

    /**
     * Parses the document into a sequence of XHTML SAX events sent to the
     * given content handler.
     */
    void getXHTML(ContentHandler handler, Metadata metadata, ParseContext context)
            throws SAXException, XmlException, IOException, TikaException;
}
"
tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/OOXMLExtractorFactory.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.microsoft.ooxml;

import java.io.IOException;
import java.io.InputStream;
import java.util.Locale;

import org.apache.poi.POIXMLDocument;
import org.apache.poi.POIXMLTextExtractor;
import org.apache.poi.extractor.ExtractorFactory;
import org.apache.poi.openxml4j.exceptions.InvalidFormatException;
import org.apache.poi.openxml4j.exceptions.OpenXML4JException;
import org.apache.poi.openxml4j.opc.OPCPackage;
import org.apache.poi.openxml4j.opc.PackageAccess;
import org.apache.poi.xslf.extractor.XSLFPowerPointExtractor;
import org.apache.poi.xslf.usermodel.XMLSlideShow;
import org.apache.poi.xssf.extractor.XSSFEventBasedExcelExtractor;
import org.apache.poi.xwpf.extractor.XWPFWordExtractor;
import org.apache.poi.xwpf.usermodel.XWPFDocument;
import org.apache.tika.exception.TikaException;
import org.apache.tika.io.CloseShieldInputStream;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.EmptyParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.pkg.ZipContainerDetector;
import org.apache.xmlbeans.XmlException;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * Figures out the correct {@link OOXMLExtractor} for the supplied document and
 * returns it.
 */
public class OOXMLExtractorFactory {

    public static void parse(
            InputStream stream, ContentHandler baseHandler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        Locale locale = context.get(Locale.class, Locale.getDefault());
        ExtractorFactory.setThreadPrefersEventExtractors(true);
        
        try {
            OOXMLExtractor extractor;
            OPCPackage pkg;

            // Locate or Open the OPCPackage for the file
            TikaInputStream tis = TikaInputStream.cast(stream);
            if (tis != null && tis.getOpenContainer() instanceof OPCPackage) {
                pkg = (OPCPackage) tis.getOpenContainer();
            } else if (tis != null && tis.hasFile()) {
                pkg = OPCPackage.open( tis.getFile().getPath(), PackageAccess.READ );
                tis.setOpenContainer(pkg);
            } else {
                InputStream shield = new CloseShieldInputStream(stream);
                pkg = OPCPackage.open(shield); 
            }
            
            // Get the type, and ensure it's one we handle
            MediaType type = ZipContainerDetector.detectOfficeOpenXML(pkg);
            if (type == null || OOXMLParser.UNSUPPORTED_OOXML_TYPES.contains(type)) {
               // Not a supported type, delegate to Empty Parser 
               EmptyParser.INSTANCE.parse(stream, baseHandler, metadata, context);
               return;
            }
            metadata.set(Metadata.CONTENT_TYPE, type.toString());

            // Have the appropriate OOXML text extractor picked
            POIXMLTextExtractor poiExtractor = ExtractorFactory.createExtractor(pkg);
            
            POIXMLDocument document = poiExtractor.getDocument();
            if (poiExtractor instanceof XSSFEventBasedExcelExtractor) {
               extractor = new XSSFExcelExtractorDecorator(
                   context, (XSSFEventBasedExcelExtractor)poiExtractor, locale);
            } else if (document == null) {
               throw new TikaException(
                     "Expecting UserModel based POI OOXML extractor with a document, but none found. " +
                     "The extractor returned was a " + poiExtractor
               );
            } else if (document instanceof XMLSlideShow) {
                extractor = new XSLFPowerPointExtractorDecorator(
                        context, (XSLFPowerPointExtractor) poiExtractor);
            } else if (document instanceof XWPFDocument) {
                extractor = new XWPFWordExtractorDecorator(
                        context, (XWPFWordExtractor) poiExtractor);
            } else {
                extractor = new POIXMLTextExtractorDecorator(context, poiExtractor);
            }
            
            // Get the bulk of the metadata first, so that it's accessible during
            //  parsing if desired by the client (see TIKA-1109)
            extractor.getMetadataExtractor().extract(metadata);
            
            // Extract the text, along with any in-document metadata
            extractor.getXHTML(baseHandler, metadata, context);
        } catch (IllegalArgumentException e) {
            if (e.getMessage().startsWith("No supported documents found")) {
                throw new TikaException(
                        "TIKA-418: RuntimeException while getting content"
                        + " for thmx and xps file types", e);
            } else {
                throw new TikaException("Error creating OOXML extractor", e);
            }
        } catch (InvalidFormatException e) {
            throw new TikaException("Error creating OOXML extractor", e);
        } catch (OpenXML4JException e) {
            throw new TikaException("Error creating OOXML extractor", e);
        } catch (XmlException e) {
            throw new TikaException("Error creating OOXML extractor", e);

        }
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/OOXMLParser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.microsoft.ooxml;

import java.io.IOException;
import java.io.InputStream;
import java.util.Arrays;
import java.util.Collections;
import java.util.HashSet;
import java.util.Set;

import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * Office Open XML (OOXML) parser.
 */
public class OOXMLParser extends AbstractParser {

    /** Serial version UID */
    private static final long serialVersionUID = 6535995710857776481L;
   
    protected static final Set<MediaType> SUPPORTED_TYPES =
        Collections.unmodifiableSet(new HashSet<MediaType>(Arrays.asList(
                MediaType.application("x-tika-ooxml"),
                MediaType.application("vnd.openxmlformats-officedocument.presentationml.presentation"),
                MediaType.application("vnd.ms-powerpoint.presentation.macroenabled.12"),
                MediaType.application("vnd.openxmlformats-officedocument.presentationml.template"),
                MediaType.application("vnd.openxmlformats-officedocument.presentationml.slideshow"),
                MediaType.application("vnd.ms-powerpoint.slideshow.macroenabled.12"),
                MediaType.application("vnd.ms-powerpoint.addin.macroenabled.12"),
                MediaType.application("vnd.openxmlformats-officedocument.spreadsheetml.sheet"),
                MediaType.application("vnd.ms-excel.sheet.macroenabled.12"),
                MediaType.application("vnd.openxmlformats-officedocument.spreadsheetml.template"),
                MediaType.application("vnd.ms-excel.template.macroenabled.12"),
                MediaType.application("vnd.ms-excel.addin.macroenabled.12"),
                MediaType.application("vnd.openxmlformats-officedocument.wordprocessingml.document"),
                MediaType.application("vnd.ms-word.document.macroenabled.12"),
                MediaType.application("vnd.openxmlformats-officedocument.wordprocessingml.template"),
                MediaType.application("vnd.ms-word.template.macroenabled.12"))));
    
    /**
     * We claim to support all OOXML files, but we actually don't support a small
     *  number of them.
     * This list is used to decline certain formats that are not yet supported
     *  by Tika and/or POI.
     */
    protected static final Set<MediaType> UNSUPPORTED_OOXML_TYPES = 
       Collections.unmodifiableSet(new HashSet<MediaType>(Arrays.asList(
                MediaType.application("vnd.ms-excel.sheet.binary.macroenabled.12"),
                MediaType.application("vnd.ms-xpsdocument")
       )));

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }

    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        // Have the OOXML file processed
        OOXMLExtractorFactory.parse(stream, handler, metadata, context);
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/POIXMLTextExtractorDecorator.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.microsoft.ooxml;

import java.util.ArrayList;
import java.util.List;

import org.apache.poi.POIXMLTextExtractor;
import org.apache.poi.openxml4j.opc.PackagePart;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.SAXException;

public class POIXMLTextExtractorDecorator extends AbstractOOXMLExtractor {

    public POIXMLTextExtractorDecorator(ParseContext context, POIXMLTextExtractor extractor) {
        super(context, extractor);
    }

    @Override
    protected void buildXHTML(XHTMLContentHandler xhtml) throws SAXException {
        // extract document content as a single string (not structured)
        xhtml.element("p", extractor.getText());
    }

    @Override
    protected List<PackagePart> getMainDocumentParts() {
       return new ArrayList<PackagePart>();
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/XSLFPowerPointExtractorDecorator.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.microsoft.ooxml;

import javax.xml.namespace.QName;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

import org.apache.poi.openxml4j.exceptions.InvalidFormatException;
import org.apache.poi.openxml4j.opc.PackagePart;
import org.apache.poi.openxml4j.opc.PackagePartName;
import org.apache.poi.openxml4j.opc.PackageRelationship;
import org.apache.poi.openxml4j.opc.PackagingURIHelper;
import org.apache.poi.openxml4j.opc.TargetMode;
import org.apache.poi.xslf.XSLFSlideShow;
import org.apache.poi.xslf.extractor.XSLFPowerPointExtractor;
import org.apache.poi.xslf.usermodel.Placeholder;
import org.apache.poi.xslf.usermodel.XMLSlideShow;
import org.apache.poi.xslf.usermodel.XSLFComments;
import org.apache.poi.xslf.usermodel.XSLFGraphicFrame;
import org.apache.poi.xslf.usermodel.XSLFGroupShape;
import org.apache.poi.xslf.usermodel.XSLFPictureShape;
import org.apache.poi.xslf.usermodel.XSLFRelation;
import org.apache.poi.xslf.usermodel.XSLFShape;
import org.apache.poi.xslf.usermodel.XSLFSheet;
import org.apache.poi.xslf.usermodel.XSLFSlide;
import org.apache.poi.xslf.usermodel.XSLFTable;
import org.apache.poi.xslf.usermodel.XSLFTableCell;
import org.apache.poi.xslf.usermodel.XSLFTableRow;
import org.apache.poi.xslf.usermodel.XSLFTextShape;
import org.apache.tika.exception.TikaException;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.XHTMLContentHandler;
import org.apache.xmlbeans.XmlException;
import org.apache.xmlbeans.XmlObject;
import org.openxmlformats.schemas.presentationml.x2006.main.CTComment;
import org.openxmlformats.schemas.presentationml.x2006.main.CTPicture;
import org.openxmlformats.schemas.presentationml.x2006.main.CTSlideIdList;
import org.openxmlformats.schemas.presentationml.x2006.main.CTSlideIdListEntry;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.AttributesImpl;

public class XSLFPowerPointExtractorDecorator extends AbstractOOXMLExtractor {
    public XSLFPowerPointExtractorDecorator(ParseContext context, XSLFPowerPointExtractor extractor) {
        super(context, extractor);
    }

    /**
     * @see org.apache.poi.xslf.extractor.XSLFPowerPointExtractor#getText()
     */
    protected void buildXHTML(XHTMLContentHandler xhtml) throws SAXException, IOException {
        XMLSlideShow slideShow = (XMLSlideShow) extractor.getDocument();

        XSLFSlide[] slides = slideShow.getSlides();
        for (XSLFSlide slide : slides) {
            String slideDesc;
            if (slide.getPackagePart() != null && slide.getPackagePart().getPartName() != null) {
              slideDesc = getJustFileName(slide.getPackagePart().getPartName().toString());
              slideDesc += "_";
            } else {
              slideDesc = null;
            }

            // slide
            extractContent(slide.getShapes(), false, xhtml, slideDesc);

            // slide layout which is the master sheet for this slide
            XSLFSheet slideLayout = slide.getMasterSheet();
            extractContent(slideLayout.getShapes(), true, xhtml, null);

            // slide master which is the master sheet for all text layouts
            XSLFSheet slideMaster = slideLayout.getMasterSheet();
            extractContent(slideMaster.getShapes(), true, xhtml, null);

            // notes (if present)
            XSLFSheet slideNotes = slide.getNotes();
            if (slideNotes != null) {
                extractContent(slideNotes.getShapes(), false, xhtml, slideDesc);

                // master sheet for this notes
                XSLFSheet notesMaster = slideNotes.getMasterSheet();
                extractContent(notesMaster.getShapes(), true, xhtml, null);
            }

            // comments (if present)
            XSLFComments comments = slide.getComments();
            if (comments != null) {
                for (int i = 0; i < comments.getNumberOfComments(); i++) {
                    CTComment comment = comments.getCommentAt(i);
                    xhtml.element("p", comment.getText());
                }
            }
        }
    }

    private void extractContent(XSLFShape[] shapes, boolean skipPlaceholders, XHTMLContentHandler xhtml, String slideDesc)
            throws SAXException {
        for (XSLFShape sh : shapes) {
            if (sh instanceof XSLFTextShape) {
                XSLFTextShape txt = (XSLFTextShape) sh;
                Placeholder ph = txt.getTextType();
                if (skipPlaceholders && ph != null) {
                    continue;
                }
                xhtml.element("p", txt.getText());
            } else if (sh instanceof XSLFGroupShape){
                // recurse into groups of shapes
                XSLFGroupShape group = (XSLFGroupShape)sh;
                extractContent(group.getShapes(), skipPlaceholders, xhtml, slideDesc);
            } else if (sh instanceof XSLFTable) {
                XSLFTable tbl = (XSLFTable)sh;
                for(XSLFTableRow row : tbl){
                    List<XSLFTableCell> cells = row.getCells();
                    extractContent(cells.toArray(new XSLFTableCell[cells.size()]), skipPlaceholders, xhtml, slideDesc);
                }
            } else if (sh instanceof XSLFGraphicFrame) {
                XSLFGraphicFrame frame = (XSLFGraphicFrame) sh;
                XmlObject[] sp = frame.getXmlObject().selectPath(
                                   "declare namespace p='http://schemas.openxmlformats.org/presentationml/2006/main' .//*/p:oleObj");
                if (sp != null) {
                    for(XmlObject emb : sp) {
                        XmlObject relIDAtt = emb.selectAttribute(new QName("http://schemas.openxmlformats.org/officeDocument/2006/relationships", "id"));
                        if (relIDAtt != null) {
                            String relID = relIDAtt.getDomNode().getNodeValue();
                            if (slideDesc != null) {
                              relID = slideDesc + relID;
                            }
                            AttributesImpl attributes = new AttributesImpl();
                            attributes.addAttribute("", "class", "class", "CDATA", "embedded");
                            attributes.addAttribute("", "id", "id", "CDATA", relID);
                            xhtml.startElement("div", attributes);
                            xhtml.endElement("div");
                        }
                    }
                }
            } else if (sh instanceof XSLFPictureShape) {
                if (!skipPlaceholders && (sh.getXmlObject() instanceof CTPicture)) {
                    CTPicture ctPic = ((CTPicture) sh.getXmlObject());
                    if (ctPic.getBlipFill() != null && ctPic.getBlipFill().getBlip() != null) {
                        String relID = ctPic.getBlipFill().getBlip().getEmbed();
                        if (relID != null) {
                            if (slideDesc != null) {
                              relID = slideDesc + relID;
                            }
                            AttributesImpl attributes = new AttributesImpl();
                            attributes.addAttribute("", "class", "class", "CDATA", "embedded");
                            attributes.addAttribute("", "id", "id", "CDATA", relID);
                            xhtml.startElement("div", attributes);
                            xhtml.endElement("div");
                        }
                    }
                }
            }
        }
    }
    
    /**
     * In PowerPoint files, slides have things embedded in them,
     *  and slide drawings which have the images
     */
    @Override
    protected List<PackagePart> getMainDocumentParts() throws TikaException {
       List<PackagePart> parts = new ArrayList<PackagePart>();
       XMLSlideShow slideShow = (XMLSlideShow) extractor.getDocument();
       XSLFSlideShow document = null;
       try {
          document = slideShow._getXSLFSlideShow(); // TODO Avoid this in future
       } catch(Exception e) {
          throw new TikaException(e.getMessage()); // Shouldn't happen
       }

       CTSlideIdList ctSlideIdList = document.getSlideReferences();
       if (ctSlideIdList != null) {
           for (int i = 0; i < ctSlideIdList.sizeOfSldIdArray(); i++) {
               CTSlideIdListEntry ctSlide = ctSlideIdList.getSldIdArray(i);
               // Add the slide
               PackagePart slidePart;
               try {
                   slidePart = document.getSlidePart(ctSlide);
               } catch (IOException e) {
                   throw new TikaException("Broken OOXML file", e);
               } catch (XmlException xe) {
                   throw new TikaException("Broken OOXML file", xe);
               }
               parts.add(slidePart);

               // If it has drawings, return those too
               try {
                   for (PackageRelationship rel : slidePart.getRelationshipsByType(XSLFRelation.VML_DRAWING.getRelation())) {
                       if (rel.getTargetMode() == TargetMode.INTERNAL) {
                           PackagePartName relName = PackagingURIHelper.createPartName(rel.getTargetURI());
                           parts.add(rel.getPackage().getPart(relName));
                       }
                   }
               } catch (InvalidFormatException e) {
                   throw new TikaException("Broken OOXML file", e);
               }
           }
       }
       return parts;
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/XSSFExcelExtractorDecorator.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.microsoft.ooxml;

import java.io.IOException;
import java.io.InputStream;
import java.util.ArrayList;
import java.util.List;
import java.util.Locale;

import javax.xml.parsers.ParserConfigurationException;
import javax.xml.parsers.SAXParser;
import javax.xml.parsers.SAXParserFactory;

import org.apache.poi.hssf.extractor.ExcelExtractor;
import org.apache.poi.openxml4j.exceptions.InvalidFormatException;
import org.apache.poi.openxml4j.exceptions.OpenXML4JException;
import org.apache.poi.openxml4j.opc.OPCPackage;
import org.apache.poi.openxml4j.opc.PackagePart;
import org.apache.poi.openxml4j.opc.PackagePartName;
import org.apache.poi.openxml4j.opc.PackageRelationship;
import org.apache.poi.openxml4j.opc.PackagingURIHelper;
import org.apache.poi.openxml4j.opc.TargetMode;
import org.apache.poi.ss.usermodel.DataFormatter;
import org.apache.poi.ss.usermodel.HeaderFooter;
import org.apache.poi.xssf.eventusermodel.ReadOnlySharedStringsTable;
import org.apache.poi.xssf.eventusermodel.XSSFReader;
import org.apache.poi.xssf.eventusermodel.XSSFSheetXMLHandler;
import org.apache.poi.xssf.eventusermodel.XSSFSheetXMLHandler.SheetContentsHandler;
import org.apache.poi.xssf.extractor.XSSFEventBasedExcelExtractor;
import org.apache.poi.xssf.model.CommentsTable;
import org.apache.poi.xssf.model.StylesTable;
import org.apache.poi.xssf.usermodel.XSSFComment;
import org.apache.poi.xssf.usermodel.XSSFRelation;
import org.apache.poi.xssf.usermodel.XSSFShape;
import org.apache.poi.xssf.usermodel.XSSFSimpleShape;
import org.apache.poi.xssf.usermodel.helpers.HeaderFooterHelper;
import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.TikaMetadataKeys;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.XHTMLContentHandler;
import org.apache.xmlbeans.XmlException;
import org.xml.sax.Attributes;
import org.xml.sax.ContentHandler;
import org.xml.sax.InputSource;
import org.xml.sax.Locator;
import org.xml.sax.SAXException;
import org.xml.sax.XMLReader;

public class XSSFExcelExtractorDecorator extends AbstractOOXMLExtractor {
    private final XSSFEventBasedExcelExtractor extractor;
    private final DataFormatter formatter;
    private final List<PackagePart> sheetParts = new ArrayList<PackagePart>();
    private Metadata metadata;

    public XSSFExcelExtractorDecorator(
            ParseContext context, XSSFEventBasedExcelExtractor extractor, Locale locale) {
        super(context, extractor);

        this.extractor = extractor;
        extractor.setFormulasNotResults(false);
        extractor.setLocale(locale);
        
        if(locale == null) {
           formatter = new DataFormatter();
        } else  {
           formatter = new DataFormatter(locale);
        }
    }

    @Override
    public void getXHTML(
            ContentHandler handler, Metadata metadata, ParseContext context)
            throws SAXException, XmlException, IOException, TikaException {

	this.metadata = metadata;
	metadata.set(TikaMetadataKeys.PROTECTED, "false");

	super.getXHTML(handler, metadata, context);
    }

    /**
     * @see org.apache.poi.xssf.extractor.XSSFExcelExtractor#getText()
     */
    @Override
    protected void buildXHTML(XHTMLContentHandler xhtml) throws SAXException,
            XmlException, IOException {
       OPCPackage container = extractor.getPackage();
       
       ReadOnlySharedStringsTable strings;
       XSSFReader.SheetIterator iter;
       XSSFReader xssfReader;
       StylesTable styles;
       try {
          xssfReader = new XSSFReader(container);
          styles = xssfReader.getStylesTable();
          iter = (XSSFReader.SheetIterator) xssfReader.getSheetsData();
          strings = new ReadOnlySharedStringsTable(container);
       } catch(InvalidFormatException e) {
          throw new XmlException(e);
       } catch (OpenXML4JException oe) {
          throw new XmlException(oe);
       }

       while (iter.hasNext()) {
           InputStream stream = iter.next();
           sheetParts.add(iter.getSheetPart());
           
           SheetTextAsHTML sheetExtractor = new SheetTextAsHTML(xhtml);
           CommentsTable comments = iter.getSheetComments();

           // Start, and output the sheet name
           xhtml.startElement("div");
           xhtml.element("h1", iter.getSheetName());
           
           // Extract the main sheet contents
           xhtml.startElement("table");
           xhtml.startElement("tbody");
           
           processSheet(sheetExtractor, comments, styles, strings, stream);

           xhtml.endElement("tbody");
           xhtml.endElement("table");
           
           // Output any headers and footers
           // (Need to process the sheet to get them, so we can't
           //  do the headers before the contents)
           for(String header : sheetExtractor.headers) {
              extractHeaderFooter(header, xhtml);
           }
           for(String footer : sheetExtractor.footers) {
              extractHeaderFooter(footer, xhtml);
           }
           processShapes(iter.getShapes(), xhtml);
           // All done with this sheet
           xhtml.endElement("div");
       }
    }

    private void extractHeaderFooter(String hf, XHTMLContentHandler xhtml)
            throws SAXException {
        String content = ExcelExtractor._extractHeaderFooter(
              new HeaderFooterFromString(hf));
        if (content.length() > 0) {
            xhtml.element("p", content);
        }
    }
    
    private void processShapes(List<XSSFShape> shapes, XHTMLContentHandler xhtml) throws SAXException {
       if (shapes == null){
           return;
       }
       for (XSSFShape shape : shapes){
           if (shape instanceof XSSFSimpleShape){
               String sText = ((XSSFSimpleShape)shape).getText();
               if (sText != null && sText.length() > 0){
                   xhtml.element("p", sText);
               }
           }
       }
   }
    
    public void processSheet(
          SheetContentsHandler sheetContentsExtractor,
          CommentsTable comments,
          StylesTable styles,
          ReadOnlySharedStringsTable strings,
          InputStream sheetInputStream)
          throws IOException, SAXException {
      InputSource sheetSource = new InputSource(sheetInputStream);
      SAXParserFactory saxFactory = SAXParserFactory.newInstance();
      try {
         SAXParser saxParser = saxFactory.newSAXParser();
         XMLReader sheetParser = saxParser.getXMLReader();
         XSSFSheetInterestingPartsCapturer handler =  
            new XSSFSheetInterestingPartsCapturer(new XSSFSheetXMLHandler(
               styles, comments, strings, sheetContentsExtractor, formatter, false));
         sheetParser.setContentHandler(handler);
         sheetParser.parse(sheetSource);
         sheetInputStream.close();
         
         if (handler.hasProtection) {
	     metadata.set(TikaMetadataKeys.PROTECTED, "true");
	 }
      } catch(ParserConfigurationException e) {
         throw new RuntimeException("SAX parser appears to be broken - " + e.getMessage());
      }
    }
     
    /**
     * Turns formatted sheet events into HTML
     */
    protected static class SheetTextAsHTML implements SheetContentsHandler {
       private XHTMLContentHandler xhtml;
       private List<String> headers;
       private List<String> footers;
       
       protected SheetTextAsHTML(XHTMLContentHandler xhtml) {
          this.xhtml = xhtml;
          headers = new ArrayList<String>();
          footers = new ArrayList<String>();
       }
       
       public void startRow(int rowNum) {
          try {
             xhtml.startElement("tr");
          } catch(SAXException e) {}
       }
       
       public void endRow(int rowNum) {
          try {
             xhtml.endElement("tr");
          } catch(SAXException e) {}
       }

       public void cell(String cellRef, String formattedValue, XSSFComment comment) {
          try {
             xhtml.startElement("td");

             // Main cell contents
             if (formattedValue != null) {
                 xhtml.characters(formattedValue);
             }

             // Comments
             if(comment != null) {
                xhtml.startElement("br");
                xhtml.endElement("br");
                xhtml.characters(comment.getAuthor());
                xhtml.characters(": ");
                xhtml.characters(comment.getString().getString());
             }

             xhtml.endElement("td");
          } catch(SAXException e) {}
       }
       
       public void headerFooter(String text, boolean isHeader, String tagName) {
          if(isHeader) {
             headers.add(text);
          } else {
             footers.add(text);
          }
       }
    }
    
    /**
     * Allows access to headers/footers from raw xml strings
     */
    private static HeaderFooterHelper hfHelper = new HeaderFooterHelper();
    protected static class HeaderFooterFromString implements HeaderFooter {
      private String text;
      protected HeaderFooterFromString(String text) {
         this.text = text;
      }

      public String getCenter() {
         return hfHelper.getCenterSection(text);
      }
      public String getLeft() {
         return hfHelper.getLeftSection(text);
      }
      public String getRight() {
         return hfHelper.getRightSection(text);
      }

      public void setCenter(String paramString) {}
      public void setLeft(String paramString) {}
      public void setRight(String paramString) {}
    }
    
    /**
     * Captures information on interesting tags, whilst
     *  delegating the main work to the formatting handler
     */
    protected static class XSSFSheetInterestingPartsCapturer implements ContentHandler {
      private ContentHandler delegate;
      private boolean hasProtection = false;
      
      protected XSSFSheetInterestingPartsCapturer(ContentHandler delegate) {
         this.delegate = delegate;
      }
      
      public void startElement(String uri, String localName, String qName,
            Attributes atts) throws SAXException {
         if("sheetProtection".equals(qName)) {
            hasProtection = true;
         }
         delegate.startElement(uri, localName, qName, atts);
      }

      public void characters(char[] ch, int start, int length)
            throws SAXException {
         delegate.characters(ch, start, length);
      }
      public void endDocument() throws SAXException {
         delegate.endDocument();
      }
      public void endElement(String uri, String localName, String qName)
            throws SAXException {
         delegate.endElement(uri, localName, qName);
      }
      public void endPrefixMapping(String prefix) throws SAXException {
         delegate.endPrefixMapping(prefix);
      }
      public void ignorableWhitespace(char[] ch, int start, int length)
            throws SAXException {
         delegate.ignorableWhitespace(ch, start, length);
      }
      public void processingInstruction(String target, String data)
            throws SAXException {
         delegate.processingInstruction(target, data);
      }
      public void setDocumentLocator(Locator locator) {
         delegate.setDocumentLocator(locator);
      }
      public void skippedEntity(String name) throws SAXException {
         delegate.skippedEntity(name);
      }
      public void startDocument() throws SAXException {
         delegate.startDocument();
      }
      public void startPrefixMapping(String prefix, String uri)
            throws SAXException {
         delegate.startPrefixMapping(prefix, uri);
      }
    }
    
    /**
     * In Excel files, sheets have things embedded in them,
     *  and sheet drawings which have the images
     */
    @Override
    protected List<PackagePart> getMainDocumentParts() throws TikaException {
       List<PackagePart> parts = new ArrayList<PackagePart>();
       for(PackagePart part : sheetParts) {
          // Add the sheet
          parts.add(part);
          
          // If it has drawings, return those too
          try {
             for(PackageRelationship rel : part.getRelationshipsByType(XSSFRelation.DRAWINGS.getRelation())) {
                if(rel.getTargetMode() == TargetMode.INTERNAL) {
                   PackagePartName relName = PackagingURIHelper.createPartName(rel.getTargetURI());
                   parts.add( rel.getPackage().getPart(relName) );
                }
             }
             for(PackageRelationship rel : part.getRelationshipsByType(XSSFRelation.VML_DRAWINGS.getRelation())) {
                if(rel.getTargetMode() == TargetMode.INTERNAL) {
                   PackagePartName relName = PackagingURIHelper.createPartName(rel.getTargetURI());
                   parts.add( rel.getPackage().getPart(relName) );
                }
             }
          } catch(InvalidFormatException e) {
             throw new TikaException("Broken OOXML file", e);
          }
       }

       return parts;
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/XWPFWordExtractorDecorator.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.microsoft.ooxml;

import javax.xml.namespace.QName;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

import org.apache.poi.openxml4j.opc.PackagePart;
import org.apache.poi.xwpf.extractor.XWPFWordExtractor;
import org.apache.poi.xwpf.model.XWPFCommentsDecorator;
import org.apache.poi.xwpf.model.XWPFHeaderFooterPolicy;
import org.apache.poi.xwpf.usermodel.BodyType;
import org.apache.poi.xwpf.usermodel.IBody;
import org.apache.poi.xwpf.usermodel.IBodyElement;
import org.apache.poi.xwpf.usermodel.ICell;
import org.apache.poi.xwpf.usermodel.IRunElement;
import org.apache.poi.xwpf.usermodel.ISDTContent;
import org.apache.poi.xwpf.usermodel.XWPFDocument;
import org.apache.poi.xwpf.usermodel.XWPFHeaderFooter;
import org.apache.poi.xwpf.usermodel.XWPFHyperlink;
import org.apache.poi.xwpf.usermodel.XWPFHyperlinkRun;
import org.apache.poi.xwpf.usermodel.XWPFParagraph;
import org.apache.poi.xwpf.usermodel.XWPFPicture;
import org.apache.poi.xwpf.usermodel.XWPFPictureData;
import org.apache.poi.xwpf.usermodel.XWPFRun;
import org.apache.poi.xwpf.usermodel.XWPFSDT;
import org.apache.poi.xwpf.usermodel.XWPFSDTCell;
import org.apache.poi.xwpf.usermodel.XWPFStyle;
import org.apache.poi.xwpf.usermodel.XWPFStyles;
import org.apache.poi.xwpf.usermodel.XWPFTable;
import org.apache.poi.xwpf.usermodel.XWPFTableCell;
import org.apache.poi.xwpf.usermodel.XWPFTableRow;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.microsoft.WordExtractor;
import org.apache.tika.parser.microsoft.WordExtractor.TagAndStyle;
import org.apache.tika.sax.XHTMLContentHandler;
import org.apache.xmlbeans.XmlCursor;
import org.apache.xmlbeans.XmlException;
import org.apache.xmlbeans.XmlObject;
import org.openxmlformats.schemas.wordprocessingml.x2006.main.CTBookmark;
import org.openxmlformats.schemas.wordprocessingml.x2006.main.CTObject;
import org.openxmlformats.schemas.wordprocessingml.x2006.main.CTP;
import org.openxmlformats.schemas.wordprocessingml.x2006.main.CTSectPr;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.AttributesImpl;

public class XWPFWordExtractorDecorator extends AbstractOOXMLExtractor {
    private XWPFDocument document;
    private XWPFStyles styles;

    public XWPFWordExtractorDecorator(ParseContext context, XWPFWordExtractor extractor) {
        super(context, extractor);
        
        document = (XWPFDocument) extractor.getDocument();
        styles = document.getStyles();
    }

    /**
     * @see org.apache.poi.xwpf.extractor.XWPFWordExtractor#getText()
     */
    @Override
    protected void buildXHTML(XHTMLContentHandler xhtml)
            throws SAXException, XmlException, IOException {
        XWPFHeaderFooterPolicy hfPolicy = document.getHeaderFooterPolicy();

        // headers
        if (hfPolicy!=null) {
            extractHeaders(xhtml, hfPolicy);
        }

        // process text in the order that it occurs in
        extractIBodyText(document, xhtml);

        // then all document tables
        if (hfPolicy!=null) {
            extractFooters(xhtml, hfPolicy);
        }
    }

    private void extractIBodyText(IBody bodyElement, XHTMLContentHandler xhtml)
            throws SAXException, XmlException, IOException {
       for(IBodyElement element : bodyElement.getBodyElements()) {
          if(element instanceof XWPFParagraph) {
             XWPFParagraph paragraph = (XWPFParagraph)element;
             extractParagraph(paragraph, xhtml);
          }
          if(element instanceof XWPFTable) {
             XWPFTable table = (XWPFTable)element;
             extractTable(table, xhtml);
          }
          if (element instanceof XWPFSDT){
             extractSDT((XWPFSDT) element, xhtml);
          }

      }
    }
    
    private void extractSDT(XWPFSDT element, XHTMLContentHandler xhtml) throws SAXException, 
    XmlException, IOException {
       ISDTContent content = element.getContent();
       String tag = "p";
       xhtml.startElement(tag);
       xhtml.characters(content.getText());
       xhtml.endElement(tag);
    }
    
    private void extractParagraph(XWPFParagraph paragraph, XHTMLContentHandler xhtml)
            throws SAXException, XmlException, IOException {
       // If this paragraph is actually a whole new section, then
       //  it could have its own headers and footers
       // Check and handle if so
       XWPFHeaderFooterPolicy headerFooterPolicy = null;
       if (paragraph.getCTP().getPPr() != null) {
           CTSectPr ctSectPr = paragraph.getCTP().getPPr().getSectPr();
           if(ctSectPr != null) {
              headerFooterPolicy =
                  new XWPFHeaderFooterPolicy(document, ctSectPr);
              extractHeaders(xhtml, headerFooterPolicy);
           }
       }
       
       // Is this a paragraph, or a heading?
       String tag = "p";
       String styleClass = null;
       if(paragraph.getStyleID() != null) {
          XWPFStyle style = styles.getStyle(
                paragraph.getStyleID()
          );

          if (style != null && style.getName() != null) {
             TagAndStyle tas = WordExtractor.buildParagraphTagAndStyle(
                   style.getName(), paragraph.getPartType() == BodyType.TABLECELL
             );
             tag = tas.getTag();
             styleClass = tas.getStyleClass();
          }
       }
       
       if(styleClass == null) {
          xhtml.startElement(tag);
       } else {
          xhtml.startElement(tag, "class", styleClass);
       }

       // Output placeholder for any embedded docs:

       // TODO: replace w/ XPath/XQuery:
       for(XWPFRun run : paragraph.getRuns()) {
          XmlCursor c = run.getCTR().newCursor();
          c.selectPath("./*");
          while (c.toNextSelection()) {
             XmlObject o = c.getObject();
             if (o instanceof CTObject) {
                XmlCursor c2 = o.newCursor();
                c2.selectPath("./*");
                while (c2.toNextSelection()) {
                   XmlObject o2 = c2.getObject();

                   XmlObject embedAtt = o2.selectAttribute(new QName("Type"));
                   if (embedAtt != null && embedAtt.getDomNode().getNodeValue().equals("Embed")) {
                      // Type is "Embed"
                      XmlObject relIDAtt = o2.selectAttribute(new QName("http://schemas.openxmlformats.org/officeDocument/2006/relationships", "id"));
                      if (relIDAtt != null) {
                         String relID = relIDAtt.getDomNode().getNodeValue();
                         AttributesImpl attributes = new AttributesImpl();
                         attributes.addAttribute("", "class", "class", "CDATA", "embedded");
                         attributes.addAttribute("", "id", "id", "CDATA", relID);
                         xhtml.startElement("div", attributes);
                         xhtml.endElement("div");
                      }
                   }
                }
                c2.dispose();
             }
          }

          c.dispose();
       }
       
       // Attach bookmarks for the paragraph
       // (In future, we might put them in the right place, for now
       //  we just put them in the correct paragraph)
       for (int i = 0; i < paragraph.getCTP().sizeOfBookmarkStartArray(); i++) {
          CTBookmark bookmark = paragraph.getCTP().getBookmarkStartArray(i);
          xhtml.startElement("a", "name", bookmark.getName());
          xhtml.endElement("a");
       }
       
       TmpFormatting fmtg = new TmpFormatting(false, false);
       
       // Do the iruns
       for(IRunElement run : paragraph.getIRuns()) {
          if (run instanceof XWPFSDT){
             fmtg = closeStyleTags(xhtml, fmtg);
             processSDTRun((XWPFSDT)run, xhtml);
             //for now, we're ignoring formatting in sdt
             //if you hit an sdt reset to false
             fmtg.setBold(false);
             fmtg.setItalic(false);
          } else {
             fmtg = processRun((XWPFRun)run, paragraph, xhtml, fmtg);
          }
       }
       closeStyleTags(xhtml, fmtg);
       
       
       // Now do any comments for the paragraph
       XWPFCommentsDecorator comments = new XWPFCommentsDecorator(paragraph, null);
       String commentText = comments.getCommentText();
       if(commentText != null && commentText.length() > 0) {
          xhtml.characters(commentText);
       }

       String footnameText = paragraph.getFootnoteText();
       if(footnameText != null && footnameText.length() > 0) {
          xhtml.characters(footnameText + "\n");
       }

       // Also extract any paragraphs embedded in text boxes:
       for (XmlObject embeddedParagraph : paragraph.getCTP().selectPath("declare namespace w='http://schemas.openxmlformats.org/wordprocessingml/2006/main' declare namespace wps='http://schemas.microsoft.com/office/word/2010/wordprocessingShape' .//*/wps:txbx/w:txbxContent/w:p")) {
           extractParagraph(new XWPFParagraph(CTP.Factory.parse(embeddedParagraph.xmlText()), paragraph.getBody()), xhtml);
       }

       // Finish this paragraph
       xhtml.endElement(tag);

       if (headerFooterPolicy != null) {
           extractFooters(xhtml, headerFooterPolicy);
       }
    }

    private TmpFormatting closeStyleTags(XHTMLContentHandler xhtml,
          TmpFormatting fmtg) throws SAXException {
       // Close any still open style tags
       if (fmtg.isItalic()) {
          xhtml.endElement("i");
          fmtg.setItalic(false);
       }
       if (fmtg.isBold()) {
          xhtml.endElement("b");
          fmtg.setBold(false);
       }
       return fmtg;
    }

    private TmpFormatting processRun(XWPFRun run, XWPFParagraph paragraph, 
          XHTMLContentHandler xhtml, TmpFormatting tfmtg) 
          throws SAXException, XmlException, IOException{
       // True if we are currently in the named style tag:
       if (run.isBold() != tfmtg.isBold()) {
          if (tfmtg.isItalic()) {
             xhtml.endElement("i");
             tfmtg.setItalic(false);
          }
          if (run.isBold()) {
             xhtml.startElement("b");
          } else {
             xhtml.endElement("b");
          }
          tfmtg.setBold(run.isBold());
       }

       if (run.isItalic() != tfmtg.isItalic()) {
          if (run.isItalic()) {
             xhtml.startElement("i");
          } else {
             xhtml.endElement("i");
          }
          tfmtg.setItalic(run.isItalic());
       }

       boolean addedHREF = false;
       if(run instanceof XWPFHyperlinkRun) {
          XWPFHyperlinkRun linkRun = (XWPFHyperlinkRun)run;
          XWPFHyperlink link = linkRun.getHyperlink(document);
          if(link != null && link.getURL() != null) {
             xhtml.startElement("a", "href", link.getURL());
             addedHREF = true;
          } else if(linkRun.getAnchor() != null && linkRun.getAnchor().length() > 0) {
             xhtml.startElement("a", "href", "#" + linkRun.getAnchor());
             addedHREF = true;
          }
       }

       xhtml.characters(run.toString());

       // If we have any pictures, output them
       for(XWPFPicture picture : run.getEmbeddedPictures()) {
          if(paragraph.getDocument() != null) {
             XWPFPictureData data = picture.getPictureData();
             if(data != null) {
                AttributesImpl attr = new AttributesImpl();

                attr.addAttribute("", "src", "src", "CDATA", "embedded:" + data.getFileName());
                attr.addAttribute("", "alt", "alt", "CDATA", picture.getDescription());

                xhtml.startElement("img", attr);
                xhtml.endElement("img");
             }
          }
       }

       if (addedHREF) {
          xhtml.endElement("a");
       }

       return tfmtg;
    }

    private void processSDTRun(XWPFSDT run, XHTMLContentHandler xhtml)
          throws SAXException, XmlException, IOException{
       xhtml.characters(run.getContent().getText());
    }

    private void extractTable(XWPFTable table, XHTMLContentHandler xhtml)
            throws SAXException, XmlException, IOException {
       xhtml.startElement("table");
       xhtml.startElement("tbody");
       for(XWPFTableRow row : table.getRows()) {
          xhtml.startElement("tr");
          for(ICell cell : row.getTableICells()){
              xhtml.startElement("td");
              if (cell instanceof XWPFTableCell) {
                  extractIBodyText((XWPFTableCell)cell, xhtml);
              } else if (cell instanceof XWPFSDTCell) {
                  xhtml.characters(((XWPFSDTCell)cell).getContent().getText());
              }
              xhtml.endElement("td");
          }
          xhtml.endElement("tr");
       }
       xhtml.endElement("tbody");
       xhtml.endElement("table");
    }
    
    private void extractFooters(
            XHTMLContentHandler xhtml, XWPFHeaderFooterPolicy hfPolicy)
            throws SAXException, XmlException, IOException {
        // footers
        if (hfPolicy.getFirstPageFooter() != null) {
            extractHeaderText(xhtml, hfPolicy.getFirstPageFooter());
        }
        if (hfPolicy.getEvenPageFooter() != null) {
            extractHeaderText(xhtml, hfPolicy.getEvenPageFooter());
        }
        if (hfPolicy.getDefaultFooter() != null) {
            extractHeaderText(xhtml, hfPolicy.getDefaultFooter());
        }
    }

    private void extractHeaders(
            XHTMLContentHandler xhtml, XWPFHeaderFooterPolicy hfPolicy)
            throws SAXException, XmlException, IOException {
        if (hfPolicy == null) return;
       
        if (hfPolicy.getFirstPageHeader() != null) {
            extractHeaderText(xhtml, hfPolicy.getFirstPageHeader());
        }

        if (hfPolicy.getEvenPageHeader() != null) {
            extractHeaderText(xhtml, hfPolicy.getEvenPageHeader());
        }

        if (hfPolicy.getDefaultHeader() != null) {
            extractHeaderText(xhtml, hfPolicy.getDefaultHeader());
        }
    }

    private void extractHeaderText(XHTMLContentHandler xhtml, XWPFHeaderFooter header) throws SAXException, XmlException, IOException {

        for (IBodyElement e : header.getBodyElements()){
           if (e instanceof XWPFParagraph){
              extractParagraph((XWPFParagraph)e, xhtml);
           } else if (e instanceof XWPFTable){
              extractTable((XWPFTable)e, xhtml);
           } else if (e instanceof XWPFSDT){
              extractSDT((XWPFSDT)e, xhtml);
           }
        }
    }

    /**
     * Word documents are simple, they only have the one
     *  main part
     */
    @Override
    protected List<PackagePart> getMainDocumentParts() {
       List<PackagePart> parts = new ArrayList<PackagePart>();
       parts.add( document.getPackagePart() );
       return parts;
    }
    
    private class TmpFormatting{
       private boolean bold = false;
       private boolean italic = false;
       private TmpFormatting(boolean bold, boolean italic){
          this.bold = bold;
          this.italic = italic;
       }
       public boolean isBold() {
          return bold;
       }
       public void setBold(boolean bold) {
          this.bold = bold;
       }
       public boolean isItalic() {
          return italic;
       }
       public void setItalic(boolean italic) {
          this.italic = italic;
       }
       
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/mp3/AudioFrame.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.mp3;

import java.io.IOException;
import java.io.InputStream;

import org.apache.tika.exception.TikaException;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * An Audio Frame in an MP3 file. These come after the ID3v2 tags in the file.
 * Currently, only the header is processed, not the raw audio data.
 */
public class AudioFrame implements MP3Frame {
    /** Constant for the MPEG version 1. */
    public static final int MPEG_V1 = 3;

    /** Constant for the MPEG version 2. */
    public static final int MPEG_V2 = 2;

    /** Constant for the MPEG version 2.5. */
    public static final int MPEG_V2_5 = 0;

    /** Constant for audio layer 1. */
    public static final int LAYER_1 = 3;
    
    /** Constant for audio layer 2. */
    public static final int LAYER_2 = 2;
    
    /** Constant for audio layer 3. */
    public static final int LAYER_3 = 1;
    
    private final String version;
    private final int versionCode;
    private final int layer;
    private final int sampleRate;
    private final int channels;
    private final int bitRate;
    private final int length;
    private final float duration;

    public String getVersion() {
        return version;
    }

    /**
     * Get the sampling rate, in Hz
     */
    public int getSampleRate() {
        return sampleRate;
    }

    /**
     * Get the number of channels (1=mono, 2=stereo)
     */
    public int getChannels() {
        return channels;
    }

    /**
     * Get the version code.
     * @return the version code (one of the {@code MPEG} constants)
     */
    public int getVersionCode()
    {
        return versionCode;
    }

    /**
     * Get the audio layer code.
     * @return the audio layer (one of the {@code LAYER} constants)
     */
    public int getLayer()
    {
        return layer;
    }

    /**
     * Get the bit rate in bit per second.
     * @return the bit rate
     */
    public int getBitRate()
    {
        return bitRate;
    }

    /**
     * Returns the frame length in bytes.
     * @return the frame length
     */
    public int getLength()
    {
        return length;
    }

    /**
     * Returns the duration in milliseconds.
     * @return the duration
     */
    public float getDuration()
    {
        return duration;
    }

    /**
     * Does this appear to be a 4 byte audio frame header?
     */
    public static boolean isAudioHeader(int h1, int h2, int h3, int h4) {
        if (h1 == -1 || h2 == -1 || h3 == -1 || h4 == -1) {
            return false;
        }
        // Check for the magic 11 bits set at the start
        // Note - doesn't do a CRC check
        if (h1 == 0xff && (h2 & 0x60) == 0x60) {
            return true;
        }
        return false;
    }

    /**
     * @deprecated Use the constructor which is passed all values directly.
     */
    @Deprecated
    public AudioFrame(InputStream stream, ContentHandler handler)
            throws IOException, SAXException, TikaException {
        this(-2, -2, -2, -2, stream);
    }

    /**
     * @deprecated Use the constructor which is passed all values directly.
     */
    @Deprecated
    public AudioFrame(int h1, int h2, int h3, int h4, InputStream in)
            throws IOException {
        if (h1 == -2 && h2 == -2 && h3 == -2 && h4 == -2) {
            h1 = in.read();
            h2 = in.read();
            h3 = in.read();
            h4 = in.read();
        }

        if (isAudioHeader(h1, h2, h3, h4)) {
            layer = (h2 >> 1) & 0x03;
            versionCode = (h2 >> 3) & 0x03;
            version = generateVersionStr(versionCode, layer);

            int rateCode = (h3 >> 2) & 0x03;
            int rate;
            switch (rateCode) {
            case 0:
                rate = 11025;
                break;
            case 1:
                rate = 12000;
                break;
            default:
                rate = 8000;
            }
            if (versionCode == MPEG_V2) {
                rate *= 2;
            } else if(versionCode == MPEG_V1) {
                rate *= 4;
            }
            sampleRate = rate;

            int chans = h4 & 0x192;
            if (chans < 3) {
                // Stereo, joint stereo, dual channel
                channels = 2;
            } else {
                channels = 1;
            }
            bitRate = 0;
            duration = 0;
            length = 0;
        } else {
            throw new IllegalArgumentException("Magic Audio Frame Header not found");
        }
    }
    
    /**
     * 
     * Creates a new instance of {@code AudioFrame} and initializes all properties.
     * @param mpegVersion the code for the MPEG version
     * @param layer the code for the layer
     * @param bitRate the bit rate (in bps)
     * @param sampleRate the sample rate (in samples per second)
     * @param channels the number of channels
     * @param length the frame length (in bytes)
     * @param duration the duration of this frame (in milliseconds)
     */
    public AudioFrame(int mpegVersion, int layer, int bitRate, int sampleRate,
            int channels, int length, float duration) {
        versionCode = mpegVersion;
        this.layer = layer;
        this.bitRate = bitRate;
        this.sampleRate = sampleRate;
        this.channels = channels;
        this.length = length;
        this.duration = duration;
        version = generateVersionStr(mpegVersion, layer);
    }

    /**
     * Generates a string for the version of this audio frame.
     * @param version the code for the MPEG version
     * @param layer the code for the layer
     * @return a string for the version
     */
    private static String generateVersionStr(int version, int layer) {
        StringBuilder buf = new StringBuilder(64);
        buf.append("MPEG 3 Layer ");
        if (layer == LAYER_3) {
            buf.append("III");
        } else if (layer == LAYER_2) {
            buf.append("II");
        } else if (layer == LAYER_1) {
            buf.append("I");
        } else {
            buf.append("(reserved)");
        }

        buf.append(" Version ");
        if (version == MPEG_V2_5) {
            buf.append("2.5");
        } else if(version == MPEG_V2) {
            buf.append("2");
        } else if(version == MPEG_V1) {
            buf.append("1");
        } else {
            buf.append("(reseved)");
        }
        
        return buf.toString();
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/mp3/CompositeTagHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.mp3;

import java.util.Collections;
import java.util.List;

/**
 * Takes an array of {@link ID3Tags} in preference order, and when asked for
 * a given tag, will return it from the first {@link ID3Tags} that has it.
 */
public class CompositeTagHandler implements ID3Tags {

    private ID3Tags[] tags;

    public CompositeTagHandler(ID3Tags[] tags) {
        this.tags = tags;
    }

    public boolean getTagsPresent() {
        for (ID3Tags tag : tags) {
            if (tag.getTagsPresent()) {
                return true;
            }
        }
        return false;
    }

    public String getTitle() {
        for (ID3Tags tag : tags) {
            if (tag.getTitle() != null) {
                return tag.getTitle();
            }
        }
        return null;
    }

    public String getArtist() {
        for (ID3Tags tag : tags) {
            if (tag.getArtist() != null) {
                return tag.getArtist();
            }
        }
        return null;
    }

    public String getAlbum() {
        for (ID3Tags tag : tags) {
            if (tag.getAlbum() != null) {
                return tag.getAlbum();
            }
        }
        return null;
    }

    public String getComposer() {
        for (ID3Tags tag : tags) {
            if (tag.getComposer() != null) {
                return tag.getComposer();
            }
        }
        return null;
    }

    public String getYear() {
        for (ID3Tags tag : tags) {
            if (tag.getYear() != null) {
                return tag.getYear();
            }
        }
        return null;
    }

    public List<ID3Comment> getComments() {
        for (ID3Tags tag : tags) {
            List<ID3Comment> comments = tag.getComments();
            if (comments != null && comments.size() > 0) {
                return comments;
            }
        }
        return Collections.emptyList();
    }

    public String getGenre() {
        for (ID3Tags tag : tags) {
            if (tag.getGenre() != null) {
                return tag.getGenre();
            }
        }
        return null;
    }

    public String getTrackNumber() {
        for (ID3Tags tag : tags) {
            if (tag.getTrackNumber() != null) {
                return tag.getTrackNumber();
            }
        }
        return null;
    }

    public String getAlbumArtist() {
        for (ID3Tags tag : tags) {
            if (tag.getAlbumArtist() != null) {
                return tag.getAlbumArtist();
            }
        }
        return null;
    }

    public String getDisc() {
        for (ID3Tags tag : tags) {
            if (tag.getDisc() != null) {
                return tag.getDisc();
            }
        }
        return null;
    }

    public String getCompilation() {
        for (ID3Tags tag : tags) {
            if (tag.getCompilation() != null) {
                return tag.getCompilation();
            }
        }
        return null;
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/mp3/ID3Tags.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.mp3;

import java.util.List;

/**
 * Interface that defines the common interface for ID3 tag parsers,
 *  such as ID3v1 and ID3v2.3.
 * Implementations should return NULL if the file lacks a given
 *  tag, or if the tag isn't defined for the version.
 *  
 * Note that so far, only the ID3v1 core tags are listed here. In
 *  future, we may wish to add more to cover the extra tags that
 *  our ID3v2 handlers can produce.
 */
public interface ID3Tags {
    /**
     * List of predefined genres.
     *
     * @see http://www.id3.org/id3v2-00
     */
    String[] GENRES = new String[] {
        /*  0 */ "Blues",
        /*  1 */ "Classic Rock",
        /*  2 */ "Country",
        /*  3 */ "Dance",
        /*  4 */ "Disco",
        /*  5 */ "Funk",
        /*  6 */ "Grunge",
        /*  7 */ "Hip-Hop",
        /*  8 */ "Jazz",
        /*  9 */ "Metal",
        /* 10 */ "New Age",
        /* 11 */ "Oldies",
        /* 12 */ "Other",
        /* 13 */ "Pop",
        /* 14 */ "R&B",
        /* 15 */ "Rap",
        /* 16 */ "Reggae",
        /* 17 */ "Rock",
        /* 18 */ "Techno",
        /* 19 */ "Industrial",
        /* 20 */ "Alternative",
        /* 21 */ "Ska",
        /* 22 */ "Death Metal",
        /* 23 */ "Pranks",
        /* 24 */ "Soundtrack",
        /* 25 */ "Euro-Techno",
        /* 26 */ "Ambient",
        /* 27 */ "Trip-Hop",
        /* 28 */ "Vocal",
        /* 29 */ "Jazz+Funk",
        /* 30 */ "Fusion",
        /* 31 */ "Trance",
        /* 32 */ "Classical",
        /* 33 */ "Instrumental",
        /* 34 */ "Acid",
        /* 35 */ "House",
        /* 36 */ "Game",
        /* 37 */ "Sound Clip",
        /* 38 */ "Gospel",
        /* 39 */ "Noise",
        /* 40 */ "AlternRock",
        /* 41 */ "Bass",
        /* 42 */ "Soul",
        /* 43 */ "Punk",
        /* 44 */ "Space",
        /* 45 */ "Meditative",
        /* 46 */ "Instrumental Pop",
        /* 47 */ "Instrumental Rock",
        /* 48 */ "Ethnic",
        /* 49 */ "Gothic",
        /* 50 */ "Darkwave",
        /* 51 */ "Techno-Industrial",
        /* 52 */ "Electronic",
        /* 53 */ "Pop-Folk",
        /* 54 */ "Eurodance",
        /* 55 */ "Dream",
        /* 56 */ "Southern Rock",
        /* 57 */ "Comedy",
        /* 58 */ "Cult",
        /* 59 */ "Gangsta",
        /* 60 */ "Top 40",
        /* 61 */ "Christian Rap",
        /* 62 */ "Pop/Funk",
        /* 63 */ "Jungle",
        /* 64 */ "Native American",
        /* 65 */ "Cabaret",
        /* 66 */ "New Wave",
        /* 67 */ "Psychadelic",
        /* 68 */ "Rave",
        /* 69 */ "Showtunes",
        /* 70 */ "Trailer",
        /* 71 */ "Lo-Fi",
        /* 72 */ "Tribal",
        /* 73 */ "Acid Punk",
        /* 74 */ "Acid Jazz",
        /* 75 */ "Polka",
        /* 76 */ "Retro",
        /* 77 */ "Musical",
        /* 78 */ "Rock & Roll",
        /* 79 */ "Hard Rock",
        /* 80 */ "Folk",
        /* 81 */ "Folk-Rock",
        /* 82 */ "National Folk",
        /* 83 */ "Swing",
        /* 84 */ "Fast Fusion",
        /* 85 */ "Bebob",
        /* 86 */ "Latin",
        /* 87 */ "Revival",
        /* 88 */ "Celtic",
        /* 89 */ "Bluegrass",
        /* 90 */ "Avantgarde",
        /* 91 */ "Gothic Rock",
        /* 92 */ "Progressive Rock",
        /* 93 */ "Psychedelic Rock",
        /* 94 */ "Symphonic Rock",
        /* 95 */ "Slow Rock",
        /* 96 */ "Big Band",
        /* 97 */ "Chorus",
        /* 98 */ "Easy Listening",
        /* 99 */ "Acoustic",
        /* 100 */ "Humour",
        /* 101 */ "Speech",
        /* 102 */ "Chanson",
        /* 103 */ "Opera",
        /* 104 */ "Chamber Music",
        /* 105 */ "Sonata",
        /* 106 */ "Symphony",
        /* 107 */ "Booty Bass",
        /* 108 */ "Primus",
        /* 109 */ "Porn Groove",
        /* 110 */ "Satire",
        /* 111 */ "Slow Jam",
        /* 112 */ "Club",
        /* 113 */ "Tango",
        /* 114 */ "Samba",
        /* 115 */ "Folklore",
        /* 116 */ "Ballad",
        /* 117 */ "Power Ballad",
        /* 118 */ "Rhythmic Soul",
        /* 119 */ "Freestyle",
        /* 120 */ "Duet",
        /* 121 */ "Punk Rock",
        /* 122 */ "Drum Solo",
        /* 123 */ "A capella",
        /* 124 */ "Euro-House",
        /* 125 */ "Dance Hall",
        /* sentinel */ ""
    };

    /**
     * Does the file contain this kind of tags?
     */
    boolean getTagsPresent();

    String getTitle();

    /**
     * The Artist for the track
     */
    String getArtist();

    /**
     * The Artist for the overall album / compilation of albums
     */
    String getAlbumArtist();

    String getAlbum();
    
    String getComposer();

    String getCompilation();
    
    /**
     * Retrieves the comments, if any.
     * Files may have more than one comment, but normally only 
     *  one with any language/description pair.
     */
    List<ID3Comment> getComments();

    String getGenre();

    String getYear();

    /**
     * The number of the track within the album / recording
     */
    String getTrackNumber();

    /**
     * The number of the disc this belongs to, within the set
     */
    String getDisc();

    /**
     * Represents a comments in ID3 (especially ID3 v2), where are 
     *  made up of several parts
     */
    public static class ID3Comment {
        private String language;
        private String description;
        private String text;
        
        /**
         * Creates an ID3 v1 style comment tag
         */
        public ID3Comment(String id3v1Text) {
           this.text = id3v1Text;
        }
        /**
         * Creates an ID3 v2 style comment tag
         */
        public ID3Comment(String language, String description, String text) {
            this.language = language;
            this.description = description;
            this.text = text;
        }

        /**
         * Gets the language, if present
         */
        public String getLanguage() {
           return language;
        }
        /**
         * Gets the description, if present
         */
        public String getDescription() {
           return description;
        }
        /**
         * Gets the text, if present
         */
        public String getText() {
           return text;
        }
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/mp3/ID3v1Handler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.mp3;

import java.io.IOException;
import java.io.InputStream;
import java.io.UnsupportedEncodingException;
import java.util.Arrays;
import java.util.List;

import org.apache.tika.exception.TikaException;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * This is used to parse ID3 Version 1 Tag information from an MP3 file, 
 * if available.
 *
 * @see <a href="http://www.id3.org/ID3v1">MP3 ID3 Version 1 specification</a>
 */
public class ID3v1Handler implements ID3Tags {
    private String title;
    private String artist;
    private String album;
    private String year;
    private ID3Comment comment;
    private String genre;
    private String trackNumber;

    boolean found = false;

    public ID3v1Handler(InputStream stream, ContentHandler handler)
            throws IOException, SAXException, TikaException {
        this(LyricsHandler.getSuffix(stream, 128));
    }

    /**
     * Creates from the last 128 bytes of a stream.
     * @param tagData Must be the last 128 bytes 
     */
    protected ID3v1Handler(byte[] tagData)
            throws IOException, SAXException, TikaException {
        if (tagData.length == 128
                && tagData[0] == 'T' && tagData[1] == 'A' && tagData[2] == 'G') {
            found = true;

            title = getString(tagData, 3, 33);
            artist = getString(tagData, 33, 63);
            album = getString(tagData, 63, 93);
            year = getString(tagData, 93, 97);
            
            String commentStr = getString(tagData, 97, 127);
            comment = new ID3Comment(commentStr);

            int genreID = (int) tagData[127] & 0xff; // unsigned byte
            genre = GENRES[Math.min(genreID, GENRES.length - 1)];

            // ID3v1.1 Track addition
            // If the last two bytes of the comment field are zero and
            // non-zero, then the last byte is the track number
            if (tagData[125] == 0 && tagData[126] != 0) {
                int trackNum = (int) tagData[126] & 0xff;
                trackNumber = Integer.toString(trackNum);
            }
        }
    }


    public boolean getTagsPresent() {
        return found;
    }

    public String getTitle() {
        return title;
    }

    public String getArtist() {
        return artist;
    }

    public String getAlbum() {
        return album;
    }

    public String getYear() {
        return year;
    }

    public List<ID3Comment> getComments() {
       return Arrays.asList(comment);
    }

    public String getGenre() {
        return genre;
    }

    public String getTrackNumber() {
        return trackNumber;
    }
    
    /**
     * ID3v1 doesn't have composers,
     *  so returns null;
     */
    public String getComposer() {
        return null;
    }

    /**
     * ID3v1 doesn't have album-wide artists,
     *  so returns null;
     */
    public String getAlbumArtist() {
        return null;
    }

    /**
     * ID3v1 doesn't have disc numbers,
     *  so returns null;
     */
    public String getDisc() {
        return null;
    }

    /**
     * ID3v1 doesn't have compilations,
     *  so returns null;
     */
    public String getCompilation() {
        return null;
    }

    /**
     * Returns the identified ISO-8859-1 substring from the given byte buffer.
     * The return value is the zero-terminated substring retrieved from
     * between the given start and end positions in the given byte buffer.
     * Extra whitespace (and control characters) from the beginning and the
     * end of the substring is removed.
     *
     * @param buffer byte buffer
     * @param start start index of the substring
     * @param end end index of the substring
     * @return the identified substring
     * @throws TikaException if the ISO-8859-1 encoding is not available
     */
    private static String getString(byte[] buffer, int start, int end)
            throws TikaException {
        // Find the zero byte that marks the end of the string
        int zero = start;
        while (zero < end && buffer[zero] != 0) {
            zero++;
        }

        // Skip trailing whitespace
        end = zero;
        while (start < end && buffer[end - 1] <= ' ') {
            end--;
        }

        // Skip leading whitespace
        while (start < end && buffer[start] <= ' ') {
            start++;
        }

        // Return the remaining substring
        try {
            return new String(buffer, start, end - start, "ISO-8859-1");
        } catch (UnsupportedEncodingException e) {
            throw new TikaException("ISO-8859-1 encoding is not available", e);
        }
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/mp3/ID3v22Handler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.mp3;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

import org.apache.tika.exception.TikaException;
import org.apache.tika.parser.mp3.ID3v2Frame.RawTag;
import org.apache.tika.parser.mp3.ID3v2Frame.RawTagIterator;
import org.xml.sax.SAXException;

/**
 * This is used to parse ID3 Version 2.2 Tag information from an MP3 file,
 * if available.
 *
 * @see <a href="http://id3lib.sourceforge.net/id3/id3v2-00.txt">MP3 ID3 Version 2.2 specification</a>
 */
public class ID3v22Handler implements ID3Tags {
    private String title;
    private String artist;
    private String album;
    private String year;
    private String composer;
    private String genre;
    private String trackNumber;
    private String albumArtist;
    private String disc;
    private List<ID3Comment> comments = new ArrayList<ID3Comment>();

    public ID3v22Handler(ID3v2Frame frame)
            throws IOException, SAXException, TikaException {
        RawTagIterator tags = new RawV22TagIterator(frame);
        while (tags.hasNext()) {
            RawTag tag = tags.next();
            if (tag.name.equals("TT2")) {
                title = getTagString(tag.data, 0, tag.data.length); 
            } else if (tag.name.equals("TP1")) {
                artist = getTagString(tag.data, 0, tag.data.length); 
            } else if (tag.name.equals("TP2")) {
                albumArtist = getTagString(tag.data, 0, tag.data.length); 
            } else if (tag.name.equals("TAL")) {
                album = getTagString(tag.data, 0, tag.data.length); 
            } else if (tag.name.equals("TYE")) {
                year = getTagString(tag.data, 0, tag.data.length); 
            } else if (tag.name.equals("TCM")) {
                composer = getTagString(tag.data, 0, tag.data.length); 
            } else if (tag.name.equals("COM")) {
                comments.add( getComment(tag.data, 0, tag.data.length) ); 
            } else if (tag.name.equals("TRK")) {
                trackNumber = getTagString(tag.data, 0, tag.data.length); 
            } else if (tag.name.equals("TPA")) {
                disc = getTagString(tag.data, 0, tag.data.length); 
            } else if (tag.name.equals("TCO")) {
                genre = extractGenre( getTagString(tag.data, 0, tag.data.length) );
            }
        }
    }

    private String getTagString(byte[] data, int offset, int length) {
        return ID3v2Frame.getTagString(data, offset, length);
    }
    private ID3Comment getComment(byte[] data, int offset, int length) {
        return ID3v2Frame.getComment(data, offset, length);
    }
    
    protected static String extractGenre(String rawGenre) {
       int open = rawGenre.indexOf("(");
       int close = rawGenre.indexOf(")");
       if (open == -1 && close == -1) {
          return rawGenre;
       } else if (open < close) {
           String genreStr = rawGenre.substring(0, open).trim();
           try {
               int genreID = Integer.parseInt(rawGenre.substring(open+1, close));
               return ID3Tags.GENRES[genreID];
           } catch(ArrayIndexOutOfBoundsException invalidNum) {
              return genreStr;
           } catch(NumberFormatException notANum) {
              return genreStr;
           }
       } else {
          return null;
       }
    }

    public boolean getTagsPresent() {
        return true;
    }

    public String getTitle() {
        return title;
    }

    public String getArtist() {
        return artist;
    }

    public String getAlbum() {
        return album;
    }

    public String getYear() {
        return year;
    }
    
    public String getComposer() {
        return composer;
    }

    public List<ID3Comment> getComments() {
        return comments;
    }

    public String getGenre() {
        return genre;
    }

    public String getTrackNumber() {
        return trackNumber;
    }

    public String getAlbumArtist() {
        return albumArtist;
    }

    public String getDisc() {
        return disc;
    }

    /**
     * ID3v22 doesn't have compilations,
     *  so returns null;
     */
    public String getCompilation() {
        return null;
    }

    private class RawV22TagIterator extends RawTagIterator {
        private RawV22TagIterator(ID3v2Frame frame) {
            frame.super(3, 3, 1, 0);
        }
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/mp3/ID3v23Handler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.mp3;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

import org.apache.tika.exception.TikaException;
import org.apache.tika.parser.mp3.ID3v2Frame.RawTag;
import org.apache.tika.parser.mp3.ID3v2Frame.RawTagIterator;
import org.xml.sax.SAXException;

/**
 * This is used to parse ID3 Version 2.3 Tag information from an MP3 file,
 * if available.
 *
 * @see <a href="http://id3lib.sourceforge.net/id3/id3v2.3.0.html">MP3 ID3 Version 2.3 specification</a>
 */
public class ID3v23Handler implements ID3Tags {
    private String title;
    private String artist;
    private String album;
    private String year;
    private String composer;
    private String genre;
    private String trackNumber;
    private String albumArtist;
    private String disc;
    private String compilation;
    private List<ID3Comment> comments = new ArrayList<ID3Comment>();

    public ID3v23Handler(ID3v2Frame frame)
            throws IOException, SAXException, TikaException {
        RawTagIterator tags = new RawV23TagIterator(frame);
        while (tags.hasNext()) {
            RawTag tag = tags.next();
            if (tag.name.equals("TIT2")) {
                title = getTagString(tag.data, 0, tag.data.length); 
            } else if (tag.name.equals("TPE1")) {
                artist = getTagString(tag.data, 0, tag.data.length); 
            } else if (tag.name.equals("TPE2")) {
                albumArtist = getTagString(tag.data, 0, tag.data.length); 
            } else if (tag.name.equals("TALB")) {
                album = getTagString(tag.data, 0, tag.data.length); 
            } else if (tag.name.equals("TYER")) {
                year = getTagString(tag.data, 0, tag.data.length); 
            } else if (tag.name.equals("TCOM")) {
                composer = getTagString(tag.data, 0, tag.data.length); 
            } else if (tag.name.equals("COMM")) {
                comments.add( getComment(tag.data, 0, tag.data.length) ); 
            } else if (tag.name.equals("TRCK")) {
                trackNumber = getTagString(tag.data, 0, tag.data.length); 
            } else if (tag.name.equals("TPOS")) {
                disc = getTagString(tag.data, 0, tag.data.length); 
            } else if (tag.name.equals("TCMP")) {
                compilation = getTagString(tag.data, 0, tag.data.length); 
            } else if (tag.name.equals("TCON")) {
                genre = ID3v22Handler.extractGenre( getTagString(tag.data, 0, tag.data.length) );
            }
        }
    }

    private String getTagString(byte[] data, int offset, int length) {
        return ID3v2Frame.getTagString(data, offset, length);
    }
    private ID3Comment getComment(byte[] data, int offset, int length) {
       return ID3v2Frame.getComment(data, offset, length);
    }

    public boolean getTagsPresent() {
        return true;
    }

    public String getTitle() {
        return title;
    }

    public String getArtist() {
        return artist;
    }

    public String getAlbum() {
        return album;
    }

    public String getYear() {
        return year;
    }

    public String getComposer() {
        return composer;
    }

    public List<ID3Comment> getComments() {
        return comments;
    }

    public String getGenre() {
        return genre;
    }

    public String getTrackNumber() {
        return trackNumber;
    }

    public String getAlbumArtist() {
        return albumArtist;
    }

    public String getDisc() {
        return disc;
    }

    public String getCompilation() {
        return compilation;
    }

    private class RawV23TagIterator extends RawTagIterator {
        private RawV23TagIterator(ID3v2Frame frame) {
            frame.super(4, 4, 1, 2);
        }
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/mp3/ID3v24Handler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.mp3;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

import org.apache.tika.exception.TikaException;
import org.apache.tika.parser.mp3.ID3v2Frame.RawTag;
import org.apache.tika.parser.mp3.ID3v2Frame.RawTagIterator;
import org.xml.sax.SAXException;

/**
 * This is used to parse ID3 Version 2.4 Tag information from an MP3 file,
 * if available.
 *
 * @see <a href="http://www.id3.org/id3v2.4.0-structure">MP3 ID3 Version 2.4 specification</a>
 * @see <a href="http://www.id3.org/id3v2.4.0-frames">MP3 ID3 Version 2.4 frames/tags</a>
 */
public class ID3v24Handler implements ID3Tags {
    private String title;
    private String artist;
    private String album;
    private String year;
    private String composer;
    private String genre;
    private String trackNumber;
    private String albumArtist;
    private String disc;
    private String compilation;
    private List<ID3Comment> comments = new ArrayList<ID3Comment>();

    public ID3v24Handler(ID3v2Frame frame)
            throws IOException, SAXException, TikaException {
        RawTagIterator tags = new RawV24TagIterator(frame);
        while (tags.hasNext()) {
            RawTag tag = tags.next();
            if (tag.name.equals("TIT2")) {
                title = getTagString(tag.data, 0, tag.data.length); 
            } else if (tag.name.equals("TPE1")) {
                artist = getTagString(tag.data, 0, tag.data.length); 
            } else if (tag.name.equals("TPE2")) {
                albumArtist = getTagString(tag.data, 0, tag.data.length); 
            } else if (tag.name.equals("TALB")) {
                album = getTagString(tag.data, 0, tag.data.length); 
            } else if (tag.name.equals("TYER")) {
                year = getTagString(tag.data, 0, tag.data.length); 
            } else if (tag.name.equals("TDRC")) {
               if(year == null) {
                  year = getTagString(tag.data, 0, tag.data.length);
               }
            } else if (tag.name.equals("TCOM")) {
                composer = getTagString(tag.data, 0, tag.data.length); 
            } else if (tag.name.equals("COMM")) {
                comments.add( getComment(tag.data, 0, tag.data.length) ); 
            } else if (tag.name.equals("TRCK")) {
                trackNumber = getTagString(tag.data, 0, tag.data.length); 
            } else if (tag.name.equals("TPOS")) {
                disc = getTagString(tag.data, 0, tag.data.length); 
            } else if (tag.name.equals("TCMP")) {
                compilation = getTagString(tag.data, 0, tag.data.length); 
            } else if (tag.name.equals("TCON")) {
               genre = ID3v22Handler.extractGenre( getTagString(tag.data, 0, tag.data.length) );
            }
        }
    }

    private String getTagString(byte[] data, int offset, int length) {
        return ID3v2Frame.getTagString(data, offset, length);
    }
    private ID3Comment getComment(byte[] data, int offset, int length) {
        return ID3v2Frame.getComment(data, offset, length);
    }

    public boolean getTagsPresent() {
        return true;
    }

    public String getTitle() {
        return title;
    }

    public String getArtist() {
        return artist;
    }

    public String getAlbum() {
        return album;
    }

    public String getYear() {
        return year;
    }

    public String getComposer() {
        return composer;
    }

    public List<ID3Comment> getComments() {
        return comments;
    }

    public String getGenre() {
        return genre;
    }

    public String getTrackNumber() {
        return trackNumber;
    }

    public String getAlbumArtist() {
        return albumArtist;
    }

    public String getDisc() {
        return disc;
    }

    public String getCompilation() {
        return compilation;
    }

    private class RawV24TagIterator extends RawTagIterator {
        private RawV24TagIterator(ID3v2Frame frame) {
            frame.super(4, 4, 1, 2);
        }
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/mp3/ID3v2Frame.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.mp3;

import java.io.IOException;
import java.io.InputStream;
import java.io.PushbackInputStream;
import java.io.UnsupportedEncodingException;
import java.util.Iterator;

import org.apache.tika.parser.mp3.ID3Tags.ID3Comment;

/**
 * A frame of ID3v2 data, which is then passed to a handler to 
 * be turned into useful data.
 */
public class ID3v2Frame implements MP3Frame {
    private int majorVersion;
    private int minorVersion;
    private int flags;
    private int length;
    /** Excludes the header size part */
    private byte[] extendedHeader;
    private byte[] data;

    public int getMajorVersion() {
        return majorVersion;
    }

    public int getMinorVersion() {
        return minorVersion;
    }

    public int getFlags() {
        return flags;
    }

    public int getLength() {
        return length;
    }

    public byte[] getExtendedHeader() {
        return extendedHeader;
    }

    public byte[] getData() {
        return data;
    }

    /**
     * Returns the next ID3v2 Frame in
     *  the file, or null if the next batch of data
     *  doesn't correspond to either an ID3v2 header.
     * If no ID3v2 frame could be detected and the passed in input stream is a
     * {@code PushbackInputStream}, the bytes read so far are pushed back so
     * that they can be read again.
     * ID3v2 Frames should come before all Audio ones.
     */
    public static MP3Frame createFrameIfPresent(InputStream inp)
            throws IOException {
        int h1 = inp.read();
        int h2 = inp.read();
        int h3 = inp.read();
        
        // Is it an ID3v2 Frame? 
        if (h1 == (int)'I' && h2 == (int)'D' && h3 == (int)'3') {
            int majorVersion = inp.read();
            int minorVersion = inp.read();
            if (majorVersion == -1 || minorVersion == -1) {
                pushBack(inp, h1, h2, h3, majorVersion, minorVersion);
                return null;
            }
            return new ID3v2Frame(majorVersion, minorVersion, inp);
        }

        // Not a frame header
        pushBack(inp, h1, h2, h3);
        return null;
    }

    /**
     * Pushes bytes back into the stream if possible. This method is called if
     * no ID3v2 header could be found at the current stream position.
     * 
     * @param inp the input stream
     * @param bytes the bytes to be pushed back
     * @throws IOException if an error occurs
     */
    private static void pushBack(InputStream inp, int... bytes)
            throws IOException
    {
        if (inp instanceof PushbackInputStream)
        {
            byte[] buf = new byte[bytes.length];
            for (int i = 0; i < bytes.length; i++)
            {
                buf[i] = (byte) bytes[i];
            }
            ((PushbackInputStream) inp).unread(buf);
        }
    }

    private ID3v2Frame(int majorVersion, int minorVersion, InputStream inp)
            throws IOException {
        this.majorVersion = majorVersion;
        this.minorVersion = minorVersion;

        // Get the flags and the length
        flags = inp.read();
        length = get7BitsInt(readFully(inp, 4), 0);

        // Do we have an extended header?
        if ((flags & 0x02) == 0x02) {
            int size = getInt(readFully(inp, 4));
            extendedHeader = readFully(inp, size);
        }

        // Get the frame's data, or at least as much
        //  of it as we could do
        data = readFully(inp, length, false);
    }

    protected static int getInt(byte[] data) {
        return getInt(data, 0);
    }

    protected static int getInt(byte[] data, int offset) {
        int b0 = data[offset+0] & 0xFF;
        int b1 = data[offset+1] & 0xFF;
        int b2 = data[offset+2] & 0xFF;
        int b3 = data[offset+3] & 0xFF;
        return (b0 << 24) + (b1 << 16) + (b2 << 8) + (b3 << 0);
    }

    protected static int getInt3(byte[] data, int offset) {
        int b0 = data[offset+0] & 0xFF;
        int b1 = data[offset+1] & 0xFF;
        int b2 = data[offset+2] & 0xFF;
        return (b0 << 16) + (b1 << 8) + (b2 << 0);
    }

    protected static int getInt2(byte[] data, int offset) {
        int b0 = data[offset+0] & 0xFF;
        int b1 = data[offset+1] & 0xFF;
        return (b0 << 8) + (b1 << 0);
    }

    /**
     * AKA a Synchsafe integer.
     * 4 bytes hold a 28 bit number. The highest
     *  bit in each byte is always 0 and always ignored.
     */
    protected static int get7BitsInt(byte[] data, int offset) {
        int b0 = data[offset+0] & 0x7F;
        int b1 = data[offset+1] & 0x7F;
        int b2 = data[offset+2] & 0x7F;
        int b3 = data[offset+3] & 0x7F;
        return (b0 << 21) + (b1 << 14) + (b2 << 7) + (b3 << 0);
    }

    protected static byte[] readFully(InputStream inp, int length)
            throws IOException {
       return readFully(inp, length, true);
    }
    protected static byte[] readFully(InputStream inp, int length, boolean shortDataIsFatal)
            throws IOException {
        byte[] b = new byte[length];

        int pos = 0;
        int read;
        while (pos < length) {
            read = inp.read(b, pos, length-pos);
            if (read == -1) {
                if(shortDataIsFatal) {
                   throw new IOException("Tried to read " + length + " bytes, but only " + pos + " bytes present");
                } else {
                   // Give them what we found
                   // TODO Log the short read
                   return b;
                }
            }
            pos += read;
        }

        return b;
    }
    
    protected static class TextEncoding {
       public final boolean doubleByte;
       public final String encoding;
       private TextEncoding(String encoding, boolean doubleByte) {
          this.doubleByte = doubleByte;
          this.encoding = encoding;
       }
    }
    protected static final TextEncoding[] encodings = new TextEncoding[] {
          new TextEncoding("ISO-8859-1", false),
          new TextEncoding("UTF-16", true), // With BOM
          new TextEncoding("UTF-16BE", true), // Without BOM
          new TextEncoding("UTF-8", false)
    };

    /**
     * Returns the (possibly null padded) String at the given offset and
     * length. String encoding is held in the first byte; 
     */
    protected static String getTagString(byte[] data, int offset, int length) {
        int actualLength = length;
        if (actualLength == 0) {
            return "";
        }
        if (actualLength == 1 && data[offset] == 0) {
            return "";
        }

        // Does it have an encoding flag?
        // Detect by the first byte being sub 0x20
        TextEncoding encoding = encodings[0];
        byte maybeEncodingFlag = data[offset];
        if (maybeEncodingFlag >= 0 && maybeEncodingFlag < encodings.length) {
            offset++;
            actualLength--;
            encoding = encodings[maybeEncodingFlag];
        }
        
        // Trim off null termination / padding (as present) 
        while (encoding.doubleByte && actualLength >= 2 && data[offset+actualLength-1] == 0 && data[offset+actualLength-2] == 0) {
           actualLength -= 2;
        } 
        while (!encoding.doubleByte && actualLength >= 1 && data[offset+actualLength-1] == 0) {
           actualLength--;
        }
        if (actualLength == 0) {
           return "";
        }

        // TIKA-1024: If it's UTF-16 (with BOM) and all we
        // have is a naked BOM then short-circuit here
        // (return empty string), because new String(..)
        // gives different results on different JVMs
        if (encoding.encoding.equals("UTF-16") && actualLength == 2 &&
            ((data[offset] == (byte) 0xff && data[offset+1] == (byte) 0xfe) ||
             (data[offset] == (byte) 0xfe && data[offset+1] == (byte) 0xff))) {
          return "";
        }

        try {
            // Build the base string
            return new String(data, offset, actualLength, encoding.encoding);
        } catch (UnsupportedEncodingException e) {
            throw new RuntimeException(
                    "Core encoding " + encoding.encoding + " is not available", e);
        }
    }
    /**
     * Builds up the ID3 comment, by parsing and extracting
     *  the comment string parts from the given data. 
     */
    protected static ID3Comment getComment(byte[] data, int offset, int length) {
       // Comments must have an encoding
       int encodingFlag = data[offset];
       if (encodingFlag >= 0 && encodingFlag < encodings.length) {
          // Good, valid flag
       } else {
          // Invalid string
          return null;
       }
       
       TextEncoding encoding = encodings[encodingFlag];
       
       // First is a 3 byte language
       String lang = getString(data, offset+1, 3);
       
       // After that we have [Desc]\0(\0)[Text]
       int descStart = offset+4;
       int textStart = -1;
       String description = null;
       String text = null;
       
       // Find where the description ends
       try {
          for (int i=descStart; i<offset+length; i++) {
             if (encoding.doubleByte && data[i]==0 && data[i+1] == 0) {
                // Handle LE vs BE on low byte text
                if (i+2 < offset+length && data[i+1] == 0 && data[i+2] == 0) {
                   i++;
                }
                textStart = i+2;
                description = new String(data, descStart, i-descStart, encoding.encoding);
                break;
             }
             if (!encoding.doubleByte && data[i]==0) {
                textStart = i+1;
                description = new String(data, descStart, i-descStart, encoding.encoding);
                break;
             }
          }
          
          // Did we find the end?
          if (textStart > -1) {
             text = new String(data, textStart, offset+length-textStart, encoding.encoding);
          } else {
             // Assume everything is the text
             text = new String(data, descStart, offset+length-descStart, encoding.encoding);
          }
          
          // Return
          return new ID3Comment(lang, description, text);
       } catch (UnsupportedEncodingException e) {
          throw new RuntimeException(
                  "Core encoding " + encoding.encoding + " is not available", e);
       }
    }

    /**
     * Returns the String at the given
     *  offset and length. Strings are ISO-8859-1 
     */
    protected static String getString(byte[] data, int offset, int length) {
        try {
            return new String(data, offset, length, "ISO-8859-1");
        } catch (UnsupportedEncodingException e) {
            throw new RuntimeException(
                    "Core encoding ISO-8859-1 encoding is not available", e);
        }
    }


    /**
     * Iterates over id3v2 raw tags.
     * Create an instance of this that configures the
     *  various length and multipliers.
     */
    protected class RawTagIterator implements Iterator<RawTag> {
        private int nameLength;
        private int sizeLength;
        private int sizeMultiplier;
        private int flagLength;

        private int offset = 0;

        protected RawTagIterator(
                int nameLength, int sizeLength, int sizeMultiplier,
                int flagLength) {
            this.nameLength = nameLength;
            this.sizeLength = sizeLength;
            this.sizeMultiplier = sizeMultiplier;
            this.flagLength = flagLength;
        }

        public boolean hasNext() {
            // Check for padding at the end
            return offset < data.length && data[offset] != 0;
        }

        public RawTag next() {
            RawTag tag = new RawTag(nameLength, sizeLength, sizeMultiplier,
                    flagLength, data, offset);
            offset += tag.getSize();
            return tag;
        }

        public void remove() {
        }

    }

    protected static class RawTag {
        private int headerSize;
        protected String name;
        protected int flag;
        protected byte[] data;

        private RawTag(
                int nameLength, int sizeLength, int sizeMultiplier,
                int flagLength, byte[] frameData, int offset) {
            headerSize = nameLength + sizeLength + flagLength;

            // Name, normally 3 or 4 bytes
            name = getString(frameData, offset, nameLength);

            // Size
            int rawSize;
            if (sizeLength == 3) {
                rawSize = getInt3(frameData, offset+nameLength);
            } else {
                rawSize = getInt(frameData, offset+nameLength);
            }
            int size = rawSize * sizeMultiplier;

            // Flag
            if (flagLength > 0) {
                if (flagLength == 1) {
                    flag = (int)frameData[offset+nameLength+sizeLength];
                } else {
                    flag = getInt2(frameData, offset+nameLength+sizeLength);
                }
            }

            // Now data
            int copyFrom = offset+nameLength+sizeLength+flagLength;
            size = Math.max(0, Math.min(size, frameData.length-copyFrom)); // TIKA-1218, prevent negative size for malformed files.
            data = new byte[size];
            System.arraycopy(frameData, copyFrom, data, 0, size);
        }

        protected int getSize() {
            return headerSize + data.length;
        }

    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/mp3/LyricsHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.mp3;

import java.io.IOException;
import java.io.InputStream;

import org.apache.tika.exception.TikaException;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * This is used to parse Lyrics3 tag information
 *  from an MP3 file, if available.
 * Handles lyrics tags of up to 10kb in size.
 * Will process any ID3v1 tag data if present.
 * Ignores extended ID3v1 data in the lyrics block
 *
 * @see <a href="http://www.id3.org/Lyrics3v2">Lyrics3 v2.0 specification</a>
 */
public class LyricsHandler {
    boolean foundLyrics = false;
    String lyricsText = null;
    ID3v1Handler id3v1 = null;

    public LyricsHandler(InputStream stream, ContentHandler handler)
            throws IOException, SAXException, TikaException {
        this(getSuffix(stream, 10240+128));
    }

    /**
     * Looks for the Lyrics data, which will be
     *  just before the ID3v1 data (if present),
     *  and process it.
     * Also sets things up for the ID3v1
     *  processing if required.
     * Creates from the last 128 bytes of a stream.
     */
    protected LyricsHandler(byte[] tagData)
            throws IOException, SAXException, TikaException {
        if(tagData.length < 128) {
            return;
        }

        // Is there ID3v1 data?
        byte[] last128 = new byte[128];
        System.arraycopy(tagData, tagData.length-128, last128, 0, 128);
        id3v1 = new ID3v1Handler(last128);

        if(tagData.length < 137) {
            return;
        }

        // Are there lyrics? Look for the closing Lyrics tag
        //  at the end to decide if there is any
        int lookat = tagData.length - 9;
        if(id3v1.found) {
            lookat -= 128;
        }
        if(tagData[lookat+0] == 'L' && tagData[lookat+1] == 'Y' && 
                tagData[lookat+2] == 'R' && tagData[lookat+3] == 'I' &&
                tagData[lookat+4] == 'C' && tagData[lookat+5] == 'S' &&
                tagData[lookat+6] == '2' && tagData[lookat+7] == '0' &&
                tagData[lookat+8] == '0') {
            foundLyrics = true;

            // The length (6 bytes) comes just before LYRICS200, and is the
            //  size including the LYRICSBEGIN but excluding the 
            //  length+LYRICS200 at the end.
            int length = Integer.parseInt(
                    new String(tagData, lookat-6, 6, "UTF-8")
            );

            String lyrics = new String(
                    tagData, lookat-length+5, length-11,
                    "ASCII"
            );

            // Tags are a 3 letter code, 5 digit length, then data
            int pos = 0;
            while(pos < lyrics.length()-8) {
                String tagName = lyrics.substring(pos, pos+3);
                int tagLen = Integer.parseInt(
                        lyrics.substring(pos+3, pos+8)
                );
                int startPos = pos + 8;
                int endPos = startPos + tagLen;

                if(tagName.equals("LYR")) {
                    lyricsText = lyrics.substring(startPos, endPos);
                }

                pos = endPos;
            }
        }
    }

    public boolean hasID3v1() {
        if(id3v1 == null || id3v1.found == false) {
            return false;
        }
        return true;
    }
    public boolean hasLyrics() {
        return lyricsText != null && lyricsText.length() > 0;
    }

    /**
     * Reads and returns the last <code>length</code> bytes from the
     * given stream.
     * @param stream input stream
     * @param length number of bytes from the end to read and return
     * @return stream the <code>InputStream</code> to read from.
     * @throws IOException if the stream could not be read from.
     */
    protected static byte[] getSuffix(InputStream stream, int length)
            throws IOException {
        byte[] buffer = new byte[2 * length];
        int bytesInBuffer = 0;

        int n = stream.read(buffer);
        while (n != -1) {
            bytesInBuffer += n;
            if (bytesInBuffer == buffer.length) {
                System.arraycopy(buffer, bytesInBuffer - length, buffer, 0, length);
                bytesInBuffer = length;
            }
            n = stream.read(buffer, bytesInBuffer, buffer.length - bytesInBuffer);
        }

        if (bytesInBuffer < length) {
            length = bytesInBuffer;
        }

        byte[] result = new byte[length];
        System.arraycopy(buffer, bytesInBuffer - length, result, 0, length);
        return result;
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/mp3/MP3Frame.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.mp3;


/**
 * A frame in an MP3 file, such as ID3v2 Tags or some
 *  audio.
 */
public interface MP3Frame {
}
"
tika-parsers/src/main/java/org/apache/tika/parser/mp3/Mp3Parser.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.mp3;

import java.io.IOException;
import java.io.InputStream;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import java.util.Set;

import org.apache.tika.exception.TikaException;
import org.apache.tika.io.TailStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.metadata.XMPDM;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.mp3.ID3Tags.ID3Comment;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * The <code>Mp3Parser</code> is used to parse ID3 Version 1 Tag information
 * from an MP3 file, if available.
 *
 * @see <a href="http://www.id3.org/ID3v1">MP3 ID3 Version 1 specification</a>
 * @see <a href="http://www.id3.org/id3v2.4.0-structure">MP3 ID3 Version 2.4 Structure Specification</a>
 * @see <a href="http://www.id3.org/id3v2.4.0-frames">MP3 ID3 Version 2.4 Frames Specification</a>
 */
public class Mp3Parser extends AbstractParser {

    /** Serial version UID */
    private static final long serialVersionUID = 8537074922934844370L;

    private static final Set<MediaType> SUPPORTED_TYPES =
        Collections.singleton(MediaType.audio("mpeg"));

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }


    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        metadata.set(Metadata.CONTENT_TYPE, "audio/mpeg");
        metadata.set(XMPDM.AUDIO_COMPRESSOR, "MP3");

        XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);
        xhtml.startDocument();

        // Create handlers for the various kinds of ID3 tags
        ID3TagsAndAudio audioAndTags = getAllTagHandlers(stream, handler);

        if (audioAndTags.tags.length > 0) {
           CompositeTagHandler tag = new CompositeTagHandler(audioAndTags.tags);

           metadata.set(TikaCoreProperties.TITLE, tag.getTitle());
           metadata.set(TikaCoreProperties.CREATOR, tag.getArtist());
           metadata.set(XMPDM.ARTIST, tag.getArtist());
           metadata.set(XMPDM.ALBUM_ARTIST, tag.getAlbumArtist());
           metadata.set(XMPDM.COMPOSER, tag.getComposer());
           metadata.set(XMPDM.ALBUM, tag.getAlbum());
           metadata.set(XMPDM.COMPILATION, tag.getCompilation());
           metadata.set(XMPDM.RELEASE_DATE, tag.getYear());
           metadata.set(XMPDM.GENRE, tag.getGenre());
           metadata.set(XMPDM.DURATION, audioAndTags.duration);

           List<String> comments = new ArrayList<String>();
           for (ID3Comment comment : tag.getComments()) {
              StringBuffer cmt = new StringBuffer();
              if (comment.getLanguage() != null) {
                 cmt.append(comment.getLanguage());
                 cmt.append(" - ");
              }
              if (comment.getDescription() != null) {
                 cmt.append(comment.getDescription());
                 if (comment.getText() != null) {
                    cmt.append("\n");
                 }
              }
              if (comment.getText() != null) {
                 cmt.append(comment.getText());
              }
              
              comments.add(cmt.toString());
              metadata.add(XMPDM.LOG_COMMENT.getName(), cmt.toString());
           }

           xhtml.element("h1", tag.getTitle());
           xhtml.element("p", tag.getArtist());

            // ID3v1.1 Track addition
            StringBuilder sb = new StringBuilder();
            sb.append(tag.getAlbum());
            if (tag.getTrackNumber() != null) {
                sb.append(", track ").append(tag.getTrackNumber());
                metadata.set(XMPDM.TRACK_NUMBER, tag.getTrackNumber());
            }
            if (tag.getDisc() != null) {
                sb.append(", disc ").append(tag.getDisc());
                metadata.set(XMPDM.DISC_NUMBER, tag.getDisc());
            }
            xhtml.element("p", sb.toString());
            
            xhtml.element("p", tag.getYear());
            xhtml.element("p", tag.getGenre());
            xhtml.element("p", String.valueOf(audioAndTags.duration));
            for (String comment : comments) {
               xhtml.element("p", comment);
            }
        }
        if (audioAndTags.audio != null) {
            metadata.set("samplerate", String.valueOf(audioAndTags.audio.getSampleRate()));
            metadata.set("channels", String.valueOf(audioAndTags.audio.getChannels()));
            metadata.set("version", audioAndTags.audio.getVersion());
            
            metadata.set(
                    XMPDM.AUDIO_SAMPLE_RATE,
                    Integer.toString(audioAndTags.audio.getSampleRate()));
            if(audioAndTags.audio.getChannels() == 1) {
               metadata.set(XMPDM.AUDIO_CHANNEL_TYPE, "Mono");
            } else if(audioAndTags.audio.getChannels() == 2) {
               metadata.set(XMPDM.AUDIO_CHANNEL_TYPE, "Stereo");
            } else if(audioAndTags.audio.getChannels() == 5) {
               metadata.set(XMPDM.AUDIO_CHANNEL_TYPE, "5.1");
            } else if(audioAndTags.audio.getChannels() == 7) {
               metadata.set(XMPDM.AUDIO_CHANNEL_TYPE, "7.1");
            }
        }
        if (audioAndTags.lyrics != null && audioAndTags.lyrics.hasLyrics()) {
           xhtml.startElement("p", "class", "lyrics");
           xhtml.characters(audioAndTags.lyrics.lyricsText);
           xhtml.endElement("p");
        }

        xhtml.endDocument();
    }

    /**
     * Scans the MP3 frames for ID3 tags, and creates ID3Tag Handlers
     *  for each supported set of tags. 
     */
    protected static ID3TagsAndAudio getAllTagHandlers(InputStream stream, ContentHandler handler)
           throws IOException, SAXException, TikaException {
       ID3v24Handler v24 = null;
       ID3v23Handler v23 = null;
       ID3v22Handler v22 = null;
       ID3v1Handler v1 = null;
       LyricsHandler lyrics = null;
       AudioFrame firstAudio = null;

       TailStream tailStream = new TailStream(stream, 10240+128);
       MpegStream mpegStream = new MpegStream(tailStream);

       // ID3v2 tags live at the start of the file
       // You can apparently have several different ID3 tag blocks
       // So, keep going until we don't find any more
       MP3Frame f;
       while ((f = ID3v2Frame.createFrameIfPresent(mpegStream)) != null) {
           if(f instanceof ID3v2Frame) {
               ID3v2Frame id3F = (ID3v2Frame)f;
               if (id3F.getMajorVersion() == 4) {
                   v24 = new ID3v24Handler(id3F);
               } else if(id3F.getMajorVersion() == 3) {
                   v23 = new ID3v23Handler(id3F);
               } else if(id3F.getMajorVersion() == 2) {
                   v22 = new ID3v22Handler(id3F);
               }
           }
       }

        // Now iterate over all audio frames in the file
        AudioFrame frame = mpegStream.nextFrame();
        float duration = 0;
        while (frame != null)
        {
            duration += frame.getDuration();
            if (firstAudio == null)
            {
                firstAudio = frame;
            }
            mpegStream.skipFrame();
            frame = mpegStream.nextFrame();
        }

       // ID3v1 tags live at the end of the file
       // Lyrics live just before ID3v1, at the end of the file
       // Search for both (handlers seek to the end for us)
       lyrics = new LyricsHandler(tailStream.getTail());
       v1 = lyrics.id3v1;

       // Go in order of preference
       // Currently, that's newest to oldest
       List<ID3Tags> tags = new ArrayList<ID3Tags>();

       if(v24 != null && v24.getTagsPresent()) {
          tags.add(v24);
       }
       if(v23 != null && v23.getTagsPresent()) {
          tags.add(v23);
       }
       if(v22 != null && v22.getTagsPresent()) {
          tags.add(v22);
       }
       if(v1 != null && v1.getTagsPresent()) {
          tags.add(v1);
       }
       
       ID3TagsAndAudio ret = new ID3TagsAndAudio();
       ret.audio = firstAudio;
       ret.lyrics = lyrics;
       ret.tags = tags.toArray(new ID3Tags[tags.size()]);
       ret.duration = duration;
       return ret;
    }

    protected static class ID3TagsAndAudio {
        private ID3Tags[] tags;
        private AudioFrame audio;
        private LyricsHandler lyrics;
        private float duration;
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/mp3/MpegStream.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.mp3;

import java.io.IOException;
import java.io.InputStream;
import java.io.PushbackInputStream;

/**
 * <p>
 * A specialized stream class which can be used to extract single frames of MPEG
 * audio files.
 * </p>
 * <p>
 * Instances of this class are constructed with an underlying stream which
 * should point to an audio file. Read operations are possible in the usual way.
 * However, there are special methods for searching and extracting headers of
 * MPEG frames. Some meta information of frames can be queried.
 * </p>
 */
class MpegStream extends PushbackInputStream
{
    /** Bit rate table for MPEG V1, layer 1. */
    private static final int[] BIT_RATE_MPEG1_L1 = {
            0, 32000, 64000, 96000, 128000, 160000, 192000, 224000, 256000,
            288000, 320000, 352000, 384000, 416000, 448000
    };

    /** Bit rate table for MPEG V1, layer 2. */
    private static final int[] BIT_RATE_MPEG1_L2 = {
            0, 32000, 48000, 56000, 64000, 80000, 96000, 112000, 128000,
            160000, 192000, 224000, 256000, 320000, 384000
    };

    /** Bit rate table for MPEG V1, layer 3. */
    private static final int[] BIT_RATE_MPEG1_L3 = {
            0, 32000, 40000, 48000, 56000, 64000, 80000, 96000, 112000, 128000,
            160000, 192000, 224000, 256000, 320000
    };

    /** Bit rate table for MPEG V2/V2.5, layer 1. */
    private static final int[] BIT_RATE_MPEG2_L1 = {
            0, 32000, 48000, 56000, 64000, 80000, 96000, 112000, 128000,
            144000, 160000, 176000, 192000, 224000, 256000
    };

    /** Bit rate table for MPEG V2/V2.5, layer 2 and 3. */
    private static final int[] BIT_RATE_MPEG2_L2 = {
            0, 8000, 16000, 24000, 32000, 40000, 48000, 56000, 64000, 80000,
            96000, 112000, 128000, 144000, 160000
    };

    /** Sample rate table for MPEG V1. */
    private static final int[] SAMPLE_RATE_MPEG1 = {
            44100, 48000, 32000
    };

    /** Sample rate table for MPEG V2. */
    private static final int[] SAMPLE_RATE_MPEG2 = {
            22050, 24000, 16000
    };

    /** Sample rate table for MPEG V2.5. */
    private static final int[] SAMPLE_RATE_MPEG2_5 = {
            11025, 12000, 8000
    };

    /** Sample rate table for all MPEG versions. */
    private static final int[][] SAMPLE_RATE = createSampleRateTable();

    /** Constant for the number of samples for a layer 1 frame. */
    private static final int SAMPLE_COUNT_L1 = 384;

    /** Constant for the number of samples for a layer 2 or 3 frame. */
    private static final int SAMPLE_COUNT_L2 = 1152;

    /** Constant for the size of an MPEG frame header in bytes. */
    private static final int HEADER_SIZE = 4;

    /** The current MPEG header. */
    private AudioFrame currentHeader;

    /** A flag whether the end of the stream is reached. */
    private boolean endOfStream;

    /**
     * Creates a new instance of {@code MpegStream} and initializes it with the
     * underlying stream.
     * 
     * @param in the underlying audio stream
     */
    public MpegStream(InputStream in)
    {
        super(in, 2 * HEADER_SIZE);
    }

    /**
     * Searches for the next MPEG frame header from the current stream position
     * on. This method advances the underlying input stream until it finds a
     * valid frame header or the end of the stream is reached. In the former
     * case a corresponding {@code AudioFrame} object is created. In the latter
     * case there are no more headers, so the end of the stream is probably
     * reached.
     * 
     * @return the next {@code AudioFrame} or <b>null</b>
     * @throws IOException if an IO error occurs
     */
    public AudioFrame nextFrame() throws IOException
    {
        AudioFrame frame = null;
        while (!endOfStream && frame == null)
        {
            findFrameSyncByte();
            if (!endOfStream)
            {
                HeaderBitField headerField = createHeaderField();
                if (!endOfStream)
                {
                    frame = createHeader(headerField);
                    if (frame == null)
                    {
                        pushBack(headerField);
                    }
                }
            }
        }

        currentHeader = frame;
        return frame;
    }

    /**
     * Skips the current MPEG frame. This method can be called after a valid
     * MPEG header has been retrieved using {@code nextFrame()}. In this case
     * the underlying stream is advanced to the end of the associated MPEG
     * frame. Otherwise, this method has no effect. The return value indicates
     * whether a frame could be skipped.
     * 
     * @return <b>true</b> if a frame could be skipped, <b>false</b> otherwise
     * @throws IOException if an IO error occurs
     */
    public boolean skipFrame() throws IOException
    {
        if (currentHeader != null)
        {
            skipStream(in, currentHeader.getLength() - HEADER_SIZE);
            currentHeader = null;
            return true;
        }
        return false;
    }

    /**
     * Advances the underlying stream until the first byte of frame sync is
     * found.
     * 
     * @throws IOException if an error occurs
     */
    private void findFrameSyncByte() throws IOException
    {
        boolean found = false;
        while (!found && !endOfStream)
        {
            if (nextByte() == 0xFF)
            {
                found = true;
            }
        }
    }

    /**
     * Creates a bit field for the MPEG frame header.
     * 
     * @return the bit field
     * @throws IOException if an error occurs
     */
    private HeaderBitField createHeaderField() throws IOException
    {
        HeaderBitField field = new HeaderBitField();
        field.add(nextByte());
        field.add(nextByte());
        field.add(nextByte());
        return field;
    }

    /**
     * Creates an {@code AudioFrame} object based on the given header field. If
     * the header field contains invalid values, result is <b>null</b>.
     * 
     * @param bits the header bit field
     * @return the {@code AudioFrame}
     */
    private AudioFrame createHeader(HeaderBitField bits)
    {
        if (bits.get(21, 23) != 7)
        {
            return null;
        }

        int mpegVer = bits.get(19, 20);
        int layer = bits.get(17, 18);
        int bitRateCode = bits.get(12, 15);
        int sampleRateCode = bits.get(10, 11);
        int padding = bits.get(9);

        if (mpegVer == 1 || layer == 0 || bitRateCode == 0 || bitRateCode == 15
                || sampleRateCode == 3)
        {
            // invalid header values
            return null;
        }

        int bitRate = calculateBitRate(mpegVer, layer, bitRateCode);
        int sampleRate = calculateSampleRate(mpegVer, sampleRateCode);
        int length = calculateFrameLength(layer, bitRate, sampleRate, padding);
        float duration = calculateDuration(layer, sampleRate);
        int channels = calculateChannels(bits.get(6, 7));
        return new AudioFrame(mpegVer, layer, bitRate, sampleRate, channels,
                length, duration);
    }

    /**
     * Reads the next byte.
     * 
     * @return the next byte
     * @throws IOException if an error occurs
     */
    private int nextByte() throws IOException
    {
        int result = 0;
        if (!endOfStream)
        {
            result = read();
            if (result == -1)
            {
                endOfStream = true;
            }
        }
        return endOfStream ? 0 : result;
    }

    /**
     * Pushes the given header field back in the stream so that the bytes are
     * read again. This method is called if an invalid header was detected. Then
     * search has to continue at the next byte after the frame sync byte.
     * 
     * @param field the header bit field with the invalid frame header
     * @throws IOException if an error occurs
     */
    private void pushBack(HeaderBitField field) throws IOException
    {
        unread(field.toArray());
    }

    /**
     * Skips the given number of bytes from the specified input stream.
     * 
     * @param in the input stream
     * @param count the number of bytes to skip
     * @throws IOException if an IO error occurs
     */
    private static void skipStream(InputStream in, long count)
            throws IOException
    {
        long size = count;
        long skipped = 0;
        while (size > 0 && skipped >= 0)
        {
            skipped = in.skip(size);
            if (skipped != -1)
            {
                size -= skipped;
            }
        }
    }
    
    /**
     * Calculates the bit rate based on the given parameters.
     * 
     * @param mpegVer the MPEG version
     * @param layer the layer
     * @param code the code for the bit rate
     * @return the bit rate in bits per second
     */
    private static int calculateBitRate(int mpegVer, int layer, int code)
    {
        int[] arr = null;

        if (mpegVer == AudioFrame.MPEG_V1)
        {
            switch (layer)
            {
            case AudioFrame.LAYER_1:
                arr = BIT_RATE_MPEG1_L1;
                break;
            case AudioFrame.LAYER_2:
                arr = BIT_RATE_MPEG1_L2;
                break;
            case AudioFrame.LAYER_3:
                arr = BIT_RATE_MPEG1_L3;
                break;
            }
        }
        else
        {
            if (layer == AudioFrame.LAYER_1)
            {
                arr = BIT_RATE_MPEG2_L1;
            }
            else
            {
                arr = BIT_RATE_MPEG2_L2;
            }
        }
        return arr[code];
    }

    /**
     * Calculates the sample rate based on the given parameters.
     * 
     * @param mpegVer the MPEG version
     * @param code the code for the sample rate
     * @return the sample rate in samples per second
     */
    private static int calculateSampleRate(int mpegVer, int code)
    {
        return SAMPLE_RATE[mpegVer][code];
    }

    /**
     * Calculates the length of an MPEG frame based on the given parameters.
     * 
     * @param layer the layer
     * @param bitRate the bit rate
     * @param sampleRate the sample rate
     * @param padding the padding flag
     * @return the length of the frame in bytes
     */
    private static int calculateFrameLength(int layer, int bitRate,
            int sampleRate, int padding)
    {
        if (layer == AudioFrame.LAYER_1)
        {
            return (12 * bitRate / sampleRate + padding) * 4;
        }
        else
        {
            return 144 * bitRate / sampleRate + padding;
        }
    }

    /**
     * Calculates the duration of a MPEG frame based on the given parameters.
     * 
     * @param layer the layer
     * @param sampleRate the sample rate
     * @return the duration of this frame in milliseconds
     */
    private static float calculateDuration(int layer, int sampleRate)
    {
        int sampleCount =
                (layer == AudioFrame.LAYER_1) ? SAMPLE_COUNT_L1
                        : SAMPLE_COUNT_L2;
        return (1000.0f / sampleRate) * sampleCount;
    }

    /**
     * Calculates the number of channels based on the given parameters.
     * 
     * @param chan the code for the channels
     * @return the number of channels
     */
    private static int calculateChannels(int chan)
    {
        return chan < 3 ? 2 : 1;
    }

    /**
     * Creates the complete array for the sample rate mapping.
     * 
     * @return the table for the sample rates
     */
    private static int[][] createSampleRateTable()
    {
        int[][] arr = new int[4][];
        arr[AudioFrame.MPEG_V1] = SAMPLE_RATE_MPEG1;
        arr[AudioFrame.MPEG_V2] = SAMPLE_RATE_MPEG2;
        arr[AudioFrame.MPEG_V2_5] = SAMPLE_RATE_MPEG2_5;
        return arr;
    }

    /**
     * A class representing the bit field of an MPEG header. It allows
     * convenient access to specific bit groups.
     */
    private static class HeaderBitField
    {
        /** The internal value. */
        private int value;

        /**
         * Adds a byte to this field.
         * 
         * @param b the byte to be added
         */
        public void add(int b)
        {
            value <<= 8;
            value |= b;
        }

        /**
         * Returns the value of the bit group from the given start and end
         * index. E.g. ''from'' = 0, ''to'' = 3 will return the value of the
         * first 4 bits.
         * 
         * @param the from index
         * @param to the to index
         * @return the value of this group of bits
         */
        public int get(int from, int to)
        {
            int shiftVal = value >> from;
            int mask = (1 << (to - from + 1)) - 1;
            return shiftVal & mask;
        }

        /**
         * Returns the value of the bit with the given index. The bit index is
         * 0-based. Result is either 0 or 1, depending on the value of this bit.
         * 
         * @param bit the bit index
         * @return the value of this bit
         */
        public int get(int bit)
        {
            return get(bit, bit);
        }

        /**
         * Returns the internal value of this field as an array. The array
         * contains 3 bytes.
         * 
         * @return the internal value of this field as int array
         */
        public byte[] toArray()
        {
            byte[] result = new byte[3];
            result[0] = (byte) get(16, 23);
            result[1] = (byte) get(8, 15);
            result[2] = (byte) get(0, 7);
            return result;
        }
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/mp4/DirectFileReadDataSource.java,false,"package org.apache.tika.parser.mp4;

import com.googlecode.mp4parser.DataSource;

import java.io.File;
import java.io.IOException;
import java.io.RandomAccessFile;
import java.nio.ByteBuffer;
import java.nio.channels.WritableByteChannel;

import static com.googlecode.mp4parser.util.CastUtils.l2i;

/**
 * A {@link DataSource} implementation that relies on direct reads from a {@link RandomAccessFile}.
 * It should be slower than {@link com.googlecode.mp4parser.FileDataSourceImpl} but does not incur the implicit file locks of
 * memory mapped I/O on some JVMs. This implementation allows for a more controlled deletion of files
 * and might be preferred when working with temporary files.
 * @see <a href="http://bugs.java.com/view_bug.do?bug_id=4724038">JDK-4724038 : (fs) Add unmap method to MappedByteBuffer</a>
 * @see <a href="http://bugs.java.com/view_bug.do?bug_id=6359560">JDK-6359560 : (fs) File.deleteOnExit() doesn't work when MappedByteBuffer exists (win)</a>
 */
public class DirectFileReadDataSource implements DataSource {

    private static final int TRANSFER_SIZE = 8192;

    private RandomAccessFile raf;

    public DirectFileReadDataSource(File f) throws IOException {
        this.raf = new RandomAccessFile(f, "r");
    }

    public int read(ByteBuffer byteBuffer) throws IOException {
        int len = byteBuffer.remaining();
        int totalRead = 0;
        int bytesRead = 0;
        byte[] buf = new byte[TRANSFER_SIZE];
        while (totalRead < len) {
            int bytesToRead = Math.min((len - totalRead), TRANSFER_SIZE);
            bytesRead = raf.read(buf, 0, bytesToRead);
            if (bytesRead < 0) {
                break;
            } else {
                totalRead += bytesRead;
            }
            byteBuffer.put(buf, 0, bytesRead);
        }
        return ((bytesRead < 0) && (totalRead == 0)) ? -1 : totalRead;
    }

    public int readAllInOnce(ByteBuffer byteBuffer) throws IOException {
        byte[] buf = new byte[byteBuffer.remaining()];
        int read = raf.read(buf);
        byteBuffer.put(buf, 0, read);
        return read;
    }

    public long size() throws IOException {
        return raf.length();
    }

    public long position() throws IOException {
        return raf.getFilePointer();
    }

    public void position(long nuPos) throws IOException {
        raf.seek(nuPos);
    }

    public long transferTo(long position, long count, WritableByteChannel target) throws IOException {
        return target.write(map(position, count));
    }

    public ByteBuffer map(long startPosition, long size) throws IOException {
        raf.seek(startPosition);
        byte[] payload = new byte[l2i(size)];
        raf.readFully(payload);
        return ByteBuffer.wrap(payload);
    }

    public void close() throws IOException {
        raf.close();
    }


}
"
tika-parsers/src/main/java/org/apache/tika/parser/mp4/MP4Parser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.mp4;

import java.io.IOException;
import java.io.InputStream;
import java.util.Arrays;
import java.util.Collections;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Set;

import org.apache.tika.exception.TikaException;
import org.apache.tika.io.TemporaryResources;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.Property;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.metadata.XMP;
import org.apache.tika.metadata.XMPDM;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

import com.coremedia.iso.IsoFile;
import com.coremedia.iso.boxes.Box;
import com.coremedia.iso.boxes.Container;
import com.coremedia.iso.boxes.FileTypeBox;
import com.coremedia.iso.boxes.MetaBox;
import com.coremedia.iso.boxes.MovieBox;
import com.coremedia.iso.boxes.MovieHeaderBox;
import com.coremedia.iso.boxes.SampleDescriptionBox;
import com.coremedia.iso.boxes.SampleTableBox;
import com.coremedia.iso.boxes.TrackBox;
import com.coremedia.iso.boxes.TrackHeaderBox;
import com.coremedia.iso.boxes.UserDataBox;
import com.coremedia.iso.boxes.apple.AppleItemListBox;
import com.coremedia.iso.boxes.sampleentry.AudioSampleEntry;
import com.googlecode.mp4parser.boxes.apple.AppleAlbumBox;
import com.googlecode.mp4parser.boxes.apple.AppleArtistBox;
import com.googlecode.mp4parser.boxes.apple.AppleArtist2Box;
import com.googlecode.mp4parser.boxes.apple.AppleCommentBox;
import com.googlecode.mp4parser.boxes.apple.AppleCompilationBox;
import com.googlecode.mp4parser.boxes.apple.AppleDiskNumberBox;
import com.googlecode.mp4parser.boxes.apple.AppleEncoderBox;
import com.googlecode.mp4parser.boxes.apple.AppleGenreBox;
import com.googlecode.mp4parser.boxes.apple.AppleNameBox;
import com.googlecode.mp4parser.boxes.apple.AppleRecordingYear2Box;
import com.googlecode.mp4parser.boxes.apple.AppleTrackAuthorBox;
import com.googlecode.mp4parser.boxes.apple.AppleTrackNumberBox;
import com.googlecode.mp4parser.boxes.apple.Utf8AppleDataBox;

/**
 * Parser for the MP4 media container format, as well as the older
 *  QuickTime format that MP4 is based on.
 * 
 * This uses the MP4Parser project from http://code.google.com/p/mp4parser/
 *  to do the underlying parsing
 */
public class MP4Parser extends AbstractParser {
    /** Serial version UID */
    private static final long serialVersionUID = 84011216792285L;
    
    // Ensure this stays in Sync with the entries in tika-mimetypes.xml
    private static final Map<MediaType,List<String>> typesMap = new HashMap<MediaType, List<String>>();
    static {
       // All types should be 4 bytes long, space padded as needed
       typesMap.put(MediaType.audio("mp4"), Arrays.asList(
             "M4A ", "M4B ", "F4A ", "F4B "));
       typesMap.put(MediaType.video("3gpp"), Arrays.asList(
             "3ge6", "3ge7", "3gg6", "3gp1", "3gp2", "3gp3", "3gp4", "3gp5", "3gp6", "3gs7"));
       typesMap.put(MediaType.video("3gpp2"), Arrays.asList(
             "3g2a", "3g2b", "3g2c"));
       typesMap.put(MediaType.video("mp4"), Arrays.asList(
             "mp41", "mp42"));
       typesMap.put(MediaType.video("x-m4v"), Arrays.asList(
             "M4V ", "M4VH", "M4VP"));
       
       typesMap.put(MediaType.video("quicktime"), Collections.<String>emptyList());
       typesMap.put(MediaType.application("mp4"), Collections.<String>emptyList());
    }

    private static final Set<MediaType> SUPPORTED_TYPES =
       Collections.unmodifiableSet(typesMap.keySet());

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }


    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        IsoFile isoFile;
        
        // The MP4Parser library accepts either a File, or a byte array
        // As MP4 video files are typically large, always use a file to
        //  avoid OOMs that may occur with in-memory buffering
        TemporaryResources tmp = new TemporaryResources();
        TikaInputStream tstream = TikaInputStream.get(stream, tmp);
        try {
            isoFile = new IsoFile(new DirectFileReadDataSource(tstream.getFile()));
            tmp.addResource(isoFile);

            // Grab the file type box
            FileTypeBox fileType = getOrNull(isoFile, FileTypeBox.class);
            if (fileType != null) {
               // Identify the type
               MediaType type = MediaType.application("mp4");
               for (MediaType t : typesMap.keySet()) {
                  if (typesMap.get(t).contains(fileType.getMajorBrand())) {
                     type = t;
                     break;
                  }
               }
               metadata.set(Metadata.CONTENT_TYPE, type.toString());

               if (type.getType().equals("audio")) {
                  metadata.set(XMPDM.AUDIO_COMPRESSOR, fileType.getMajorBrand().trim());
               }
            } else {
               // Some older QuickTime files lack the FileType
               metadata.set(Metadata.CONTENT_TYPE, "video/quicktime");
            }


            // Get the main MOOV box
            MovieBox moov = getOrNull(isoFile, MovieBox.class);
            if (moov == null) {
               // Bail out
               return;
            }


            XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);
            xhtml.startDocument();


            // Pull out some information from the header box
            MovieHeaderBox mHeader = getOrNull(moov, MovieHeaderBox.class);
            if (mHeader != null) {
               // Get the creation and modification dates
               metadata.set(Metadata.CREATION_DATE, mHeader.getCreationTime());
               metadata.set(TikaCoreProperties.MODIFIED, mHeader.getModificationTime());

               // Get the duration
               double durationSeconds = ((double)mHeader.getDuration()) / mHeader.getTimescale();
               // TODO Use this

               // The timescale is normally the sampling rate
               metadata.set(XMPDM.AUDIO_SAMPLE_RATE, (int)mHeader.getTimescale());
            }


            // Get some more information from the track header
            // TODO Decide how to handle multiple tracks
            List<TrackBox> tb = moov.getBoxes(TrackBox.class);
            if (tb.size() > 0) {
               TrackBox track = tb.get(0);

               TrackHeaderBox header = track.getTrackHeaderBox();
               // Get the creation and modification dates
               metadata.set(TikaCoreProperties.CREATED, header.getCreationTime());
               metadata.set(TikaCoreProperties.MODIFIED, header.getModificationTime());

               // Get the video with and height
               metadata.set(Metadata.IMAGE_WIDTH,  (int)header.getWidth());
               metadata.set(Metadata.IMAGE_LENGTH, (int)header.getHeight());

               // Get the sample information
               SampleTableBox samples = track.getSampleTableBox();
               SampleDescriptionBox sampleDesc = samples.getSampleDescriptionBox();
               if (sampleDesc != null) {
                  // Look for the first Audio Sample, if present
                  AudioSampleEntry sample = getOrNull(sampleDesc, AudioSampleEntry.class);
                  if (sample != null) {
                     XMPDM.ChannelTypePropertyConverter.convertAndSet(metadata, sample.getChannelCount());
                     //metadata.set(XMPDM.AUDIO_SAMPLE_TYPE, sample.getSampleSize());    // TODO Num -> Type mapping
                     metadata.set(XMPDM.AUDIO_SAMPLE_RATE, (int)sample.getSampleRate());
                     //metadata.set(XMPDM.AUDIO_, sample.getSamplesPerPacket());
                     //metadata.set(XMPDM.AUDIO_, sample.getBytesPerSample());
                  }
               }
            }

            // Get metadata from the User Data Box
            UserDataBox userData = getOrNull(moov, UserDataBox.class);
            if (userData != null) {
               MetaBox meta = getOrNull(userData, MetaBox.class);

               // Check for iTunes Metadata
               // See http://atomicparsley.sourceforge.net/mpeg-4files.html and
               //  http://code.google.com/p/mp4v2/wiki/iTunesMetadata for more on these
               AppleItemListBox apple = getOrNull(meta, AppleItemListBox.class);
               if (apple != null) {
                  // Title
                  AppleNameBox title = getOrNull(apple, AppleNameBox.class);
                  addMetadata(TikaCoreProperties.TITLE, metadata, title);

                  // Artist
                  AppleArtistBox artist = getOrNull(apple, AppleArtistBox.class);
                  addMetadata(TikaCoreProperties.CREATOR, metadata, artist);
                  addMetadata(XMPDM.ARTIST, metadata, artist);

                  // Album Artist
                  AppleArtist2Box artist2 = getOrNull(apple, AppleArtist2Box.class);
                  addMetadata(XMPDM.ALBUM_ARTIST, metadata, artist2);

                  // Album
                  AppleAlbumBox album = getOrNull(apple, AppleAlbumBox.class);
                  addMetadata(XMPDM.ALBUM, metadata, album);

                  // Composer
                  AppleTrackAuthorBox composer = getOrNull(apple, AppleTrackAuthorBox.class);
                  addMetadata(XMPDM.COMPOSER, metadata, composer);

                  // Genre
                  AppleGenreBox genre = getOrNull(apple, AppleGenreBox.class);
                  addMetadata(XMPDM.GENRE, metadata, genre);

                  // Year
                  AppleRecordingYear2Box year = getOrNull(apple, AppleRecordingYear2Box.class);
                  if (year != null) {
                      metadata.set(XMPDM.RELEASE_DATE, year.getValue());
                  }

                  // Track number
                  AppleTrackNumberBox trackNum = getOrNull(apple, AppleTrackNumberBox.class);
                  if (trackNum != null) {
                     metadata.set(XMPDM.TRACK_NUMBER, trackNum.getA());
                     //metadata.set(XMPDM.NUMBER_OF_TRACKS, trackNum.getB()); // TODO
                  }

                  // Disc number
                  AppleDiskNumberBox discNum = getOrNull(apple, AppleDiskNumberBox.class);
                  if (discNum != null) {
                     metadata.set(XMPDM.DISC_NUMBER, discNum.getA());
                  }

                  // Compilation
                  AppleCompilationBox compilation = getOrNull(apple, AppleCompilationBox.class);
                  if (compilation != null) {
                      metadata.set(XMPDM.COMPILATION, (int)compilation.getValue());
                  }

                  // Comment
                  AppleCommentBox comment = getOrNull(apple, AppleCommentBox.class);
                  addMetadata(XMPDM.LOG_COMMENT, metadata, comment);

                  // Encoder
                  AppleEncoderBox encoder = getOrNull(apple, AppleEncoderBox.class);
                  if (encoder != null) {
                      metadata.set(XMP.CREATOR_TOOL, encoder.getValue());
                  }


                  // As text
                  for (Box box : apple.getBoxes()) {
                     if (box instanceof Utf8AppleDataBox) {
                        xhtml.element("p", ((Utf8AppleDataBox)box).getValue());
                     }
                  }
               }

               // TODO Check for other kinds too
            }

            // All done
            xhtml.endDocument();

        } finally {
            tmp.dispose();
        }

    }
    
    private static void addMetadata(String key, Metadata m, Utf8AppleDataBox metadata) {
       if (metadata != null) {
          m.add(key, metadata.getValue());
       }
    }
    private static void addMetadata(Property prop, Metadata m, Utf8AppleDataBox metadata) {
       if (metadata != null) {
          m.set(prop, metadata.getValue());
       }
    }
    
    private static <T extends Box> T getOrNull(Container box, Class<T> clazz) {
       if (box == null) return null;

       List<T> boxes = box.getBoxes(clazz);
       if (boxes.size() == 0) {
          return null;
       }
       return boxes.get(0);
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/netcdf/NetCDFParser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.netcdf;

//JDK imports

import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.util.Collections;
import java.util.Set;
import java.util.List;

import org.apache.tika.exception.TikaException;
import org.apache.tika.io.IOUtils;
import org.apache.tika.io.TemporaryResources;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.Property;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.Parser;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

import ucar.nc2.Attribute;
import ucar.nc2.NetcdfFile;
import ucar.nc2.Variable;
import ucar.nc2.Dimension;

/**
 * A {@link Parser} for <a
 * href="http://www.unidata.ucar.edu/software/netcdf/index.html">NetCDF</a>
 * files using the UCAR, MIT-licensed <a
 * href="http://www.unidata.ucar.edu/software/netcdf-java/">NetCDF for Java</a>
 * API.
 */
public class NetCDFParser extends AbstractParser {

    /**
     * Serial version UID
     */
    private static final long serialVersionUID = -5940938274907708665L;

    private final Set<MediaType> SUPPORTED_TYPES =
            Collections.singleton(MediaType.application("x-netcdf"));

    /*
     * (non-Javadoc)
     * 
     * @see
     * org.apache.tika.parser.Parser#getSupportedTypes(org.apache.tika.parser
     * .ParseContext)
     */
    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }

    /*
     * (non-Javadoc)
     * 
     * @see org.apache.tika.parser.Parser#parse(java.io.InputStream,
     * org.xml.sax.ContentHandler, org.apache.tika.metadata.Metadata,
     * org.apache.tika.parser.ParseContext)
     */
    public void parse(InputStream stream, ContentHandler handler,
                      Metadata metadata, ParseContext context) throws IOException,
            SAXException, TikaException {

        TikaInputStream tis = TikaInputStream.get(stream, new TemporaryResources());
        try {
            NetcdfFile ncFile = NetcdfFile.open(tis.getFile().getAbsolutePath());

            // first parse out the set of global attributes
            for (Attribute attr : ncFile.getGlobalAttributes()) {
                Property property = resolveMetadataKey(attr.getName());
                if (attr.getDataType().isString()) {
                    metadata.add(property, attr.getStringValue());
                } else if (attr.getDataType().isNumeric()) {
                    int value = attr.getNumericValue().intValue();
                    metadata.add(property, String.valueOf(value));
                }
            }


            XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);
            xhtml.startDocument();
            xhtml.newline();
            xhtml.element("h1", "dimensions");
            xhtml.startElement("ul");
            xhtml.newline();
            for (Dimension dim : ncFile.getDimensions()) {
                xhtml.element("li", dim.getName() + " = " + dim.getLength());
            }
            xhtml.endElement("ul");

            xhtml.element("h1", "variables");
            xhtml.startElement("ul");
            xhtml.newline();
            for (Variable var : ncFile.getVariables()) {
                xhtml.startElement("li");
                xhtml.characters(var.getDataType() + " " + var.getNameAndDimensions());
                xhtml.newline();
                List<Attribute> attributes = var.getAttributes();
                if (!attributes.isEmpty()) {
                    xhtml.startElement("ul");
                    for (Attribute element : attributes) {
                        xhtml.element("li", element.toString());
                    }
                    xhtml.endElement("ul");
                }
                xhtml.endElement("li");
            }
            xhtml.endElement("ul");

            xhtml.endDocument();

        } catch (IOException e) {
            throw new TikaException("NetCDF parse error", e);
        }
    }

    private Property resolveMetadataKey(String localName) {
        if ("title".equals(localName)) {
            return TikaCoreProperties.TITLE;
        }
        return Property.internalText(localName);
    }
}"
tika-parsers/src/main/java/org/apache/tika/parser/ocr/TesseractOCRConfig.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.ocr;

import java.io.File;
import java.io.IOException;
import java.io.InputStream;
import java.io.Serializable;
import java.util.Properties;

/**
 * Configuration for TesseractOCRParser.
 * 
 * This allows to enable TesseractOCRParser and set its parameters:
 * <p>
 * TesseractOCRConfig config = new TesseractOCRConfig();<br>
 * config.setTesseractPath(tesseractFolder);<br>
 * parseContext.set(TesseractOCRConfig.class, config);<br>
 * </p>
 *
 * Parameters can also be set by either editing the existing TesseractOCRConfig.properties file in,
 * tika-parser/src/main/resources/org/apache/tika/parser/ocr, or overriding it by creating your own
 * and placing it in the package org/apache/tika/parser/ocr on the classpath.
 * 
 */
public class TesseractOCRConfig implements Serializable{

	private static final long serialVersionUID = -4861942486845757891L;
	
	// Path to tesseract installation folder, if not on system path.
	private  String tesseractPath = "";
	
	// Language dictionary to be used.
	private  String language = "eng";
	
	// Tesseract page segmentation mode.
	private  String pageSegMode = "1";
	
	// Minimum file size to submit file to ocr.
	private  int minFileSizeToOcr = 0;
	
	// Maximum file size to submit file to ocr.
	private  int maxFileSizeToOcr = Integer.MAX_VALUE;
	
	// Maximum time (seconds) to wait for the ocring process termination
	private int timeout = 120;

	/**
	 * Default contructor.
	 */
	public TesseractOCRConfig() {
		init(this.getClass().getResourceAsStream("TesseractOCRConfig.properties"));
	}

	/**
	 * Loads properties from InputStream and then tries to close InputStream.
	 * If there is an IOException, this silently swallows the exception
	 * and goes back to the default.
	 *
	 * @param is
	 */
	public TesseractOCRConfig(InputStream is) {
		init(is);
	}

	private void init(InputStream is) {
		if (is == null) {
			return;
		}
		Properties props = new Properties();
		try {
			props.load(is);
		} catch (IOException e) {
		} finally {
			if (is != null) {
				try {
					is.close();
				} catch (IOException e) {
					//swallow
				}
			}
		}

		setTesseractPath(
				getProp(props, "tesseractPath", getTesseractPath()));
		setLanguage(
				getProp(props, "language", getLanguage()));
		setPageSegMode(
				getProp(props, "pageSegMode", getPageSegMode()));
		setMinFileSizeToOcr(
				getProp(props, "minFileSizeToOcr", getMinFileSizeToOcr()));
		setMaxFileSizeToOcr(
				getProp(props, "maxFileSizeToOcr", getMaxFileSizeToOcr()));
		setTimeout(
				getProp(props, "timeout", getTimeout()));

	}

	/** @see #setTesseractPath(String tesseractPath)*/
	public String getTesseractPath() {
		return tesseractPath;
	}
	
	/**
	 * Set tesseract installation folder, needed if it is not on system path.
	 */
	public void setTesseractPath(String tesseractPath) {
		if(!tesseractPath.isEmpty() && !tesseractPath.endsWith(File.separator))
			tesseractPath += File.separator;
		
		this.tesseractPath = tesseractPath;
	}
	
	/** @see #setLanguage(String language)*/
	public String getLanguage() {
		return language;
	}
	
	/**
	 * Set tesseract language dictionary to be used. Default is "eng".
	 * Multiple languages may be specified, separated by plus characters.
	 */
	public void setLanguage(String language) {
		if (!language.matches("([A-Za-z](\\+?))*")) {
			throw new IllegalArgumentException("Invalid language code");
		}
		this.language = language;
	}
	
	/** @see #setPageSegMode(String pageSegMode)*/
	public String getPageSegMode() {
		return pageSegMode;
	}
	
	/**
	 * Set tesseract page segmentation mode.
	 * Default is 1 = Automatic page segmentation with OSD (Orientation and Script Detection)
	 */
	public void setPageSegMode(String pageSegMode) {
		if (!pageSegMode.matches("[1-9]|10")) {
			throw new IllegalArgumentException("Invalid language code");
		}
		this.pageSegMode = pageSegMode;
	}
	
	/** @see #setMinFileSizeToOcr(int minFileSizeToOcr)*/
	public int getMinFileSizeToOcr() {
		return minFileSizeToOcr;
	}
	
	/**
	 * Set minimum file size to submit file to ocr.
	 * Default is 0.
	 */
	public void setMinFileSizeToOcr(int minFileSizeToOcr) {
		this.minFileSizeToOcr = minFileSizeToOcr;
	}
	
	/** @see #setMaxFileSizeToOcr(int maxFileSizeToOcr)*/
	public int getMaxFileSizeToOcr() {
		return maxFileSizeToOcr;
	}
	
	/**
	 * Set maximum file size to submit file to ocr.
	 * Default is Integer.MAX_VALUE.
	 */
	public void setMaxFileSizeToOcr(int maxFileSizeToOcr) {
		this.maxFileSizeToOcr = maxFileSizeToOcr;
	}

	/**
	 * Set maximum time (seconds) to wait for the ocring process to terminate.
	 * Default value is 120s.
	 */
	public void setTimeout(int timeout) {
		this.timeout = timeout;
	}

	/** @see #setTimeout(int timeout)*/
	public int getTimeout() {
		return timeout;
	}

	/**
	 * Get property from the properties file passed in.
	 * @param properties properties file to read from.
	 * @param property the property to fetch.
	 * @param defaultMissing default parameter to use.
	 * @return the value.
	 */
	private int getProp(Properties properties, String property, int defaultMissing) {
		String p = properties.getProperty(property);
		if (p == null || p.isEmpty()){
			return defaultMissing;
		}
		try {
			return Integer.parseInt(p);
		} catch (Throwable ex) {
			throw new RuntimeException(String.format("Cannot parse TesseractOCRConfig variable %s, invalid integer value", property), ex);
		}
	}

	/**
	 * Get property from the properties file passed in.
	 * @param properties properties file to read from.
	 * @param property the property to fetch.
	 * @param defaultMissing default parameter to use.
	 * @return the value.
	 */
	private String getProp(Properties properties, String property, String defaultMissing) {
		return properties.getProperty(property, defaultMissing);
	}
}
"
tika-parsers/src/main/java/org/apache/tika/parser/ocr/TesseractOCRParser.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.ocr;

import java.awt.Image;
import java.awt.image.BufferedImage;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.io.Reader;
import java.util.Arrays;
import java.util.Collections;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.Callable;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.FutureTask;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.TimeoutException;

import javax.imageio.ImageIO;

import org.apache.tika.exception.TikaException;
import org.apache.tika.io.IOUtils;
import org.apache.tika.io.TemporaryResources;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.mime.MediaTypeRegistry;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.CompositeParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.Parser;
import org.apache.tika.parser.external.ExternalParser;
import org.apache.tika.parser.image.ImageParser;
import org.apache.tika.parser.image.TiffParser;
import org.apache.tika.parser.jpeg.JpegParser;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * TesseractOCRParser powered by tesseract-ocr engine. To enable this parser,
 * create a {@link TesseractOCRConfig} object and pass it through a
 * ParseContext. Tesseract-ocr must be installed and on system path or the path
 * to its root folder must be provided:
 * <p>
 * TesseractOCRConfig config = new TesseractOCRConfig();<br>
 * //Needed if tesseract is not on system path<br>
 * config.setTesseractPath(tesseractFolder);<br>
 * parseContext.set(TesseractOCRConfig.class, config);<br>
 * </p>
 *
 *
 */
public class TesseractOCRParser extends AbstractParser {
    private static final long serialVersionUID = -8167538283213097265L;
    private static final TesseractOCRConfig DEFAULT_CONFIG = new TesseractOCRConfig();
    private static final Set<MediaType> SUPPORTED_TYPES = Collections.unmodifiableSet(
            new HashSet<MediaType>(Arrays.asList(new MediaType[] {
                    MediaType.image("png"), MediaType.image("jpeg"), MediaType.image("tiff"),
                    MediaType.image("x-ms-bmp"), MediaType.image("gif")
            })));
    private static Map<String,Boolean> TESSERACT_PRESENT = new HashMap<String, Boolean>();

    @Override
    public Set<MediaType> getSupportedTypes(ParseContext context) {
        // If Tesseract is installed, offer our supported image types
        TesseractOCRConfig config = context.get(TesseractOCRConfig.class, DEFAULT_CONFIG);
        if (hasTesseract(config))
            return SUPPORTED_TYPES;

        // Otherwise don't advertise anything, so the other image parsers
        //  can be selected instead
        return Collections.emptySet();
    }

    private void setEnv(TesseractOCRConfig config, ProcessBuilder pb) {
        if (!config.getTesseractPath().isEmpty()) {
            Map<String, String> env = pb.environment();
            env.put("TESSDATA_PREFIX", config.getTesseractPath());
        }
    }

    private boolean hasTesseract(TesseractOCRConfig config) {
        // Fetch where the config says to find Tesseract
        String tesseract = config.getTesseractPath() + getTesseractProg();

        // Have we already checked for a copy of Tesseract there?
        if (TESSERACT_PRESENT.containsKey(tesseract)) {
            return TESSERACT_PRESENT.get(tesseract);
        }

        // Try running Tesseract from there, and see if it exists + works
        String[] checkCmd = { tesseract };
        try {
            boolean hasTesseract = ExternalParser.check(checkCmd);
            TESSERACT_PRESENT.put(tesseract, hasTesseract);
            return hasTesseract;
        } catch (NoClassDefFoundError e) {
            // This happens under OSGi + Fork Parser - see TIKA-1507
            // As a workaround for now, just say we can't use OCR
            // TODO Resolve it so we don't need this try/catch block
            TESSERACT_PRESENT.put(tesseract, false);
            return false;
        }
    }

    public void parse(Image image, ContentHandler handler, Metadata metadata, ParseContext context) throws IOException,
            SAXException, TikaException {

        TemporaryResources tmp = new TemporaryResources();
        FileOutputStream fos = null;
        TikaInputStream tis = null;
        try {
            int w = image.getWidth(null);
            int h = image.getHeight(null);
            BufferedImage bImage = new BufferedImage(w, h, BufferedImage.TYPE_INT_RGB);
            File file = tmp.createTemporaryFile();
            fos = new FileOutputStream(file);
            ImageIO.write(bImage, "png", fos);
            tis = TikaInputStream.get(file);
            parse(tis, handler, metadata, context);

        } finally {
            tmp.dispose();
            if (tis != null)
                tis.close();
            if (fos != null)
                fos.close();
        }

    }

    @Override
    public void parse(InputStream stream, ContentHandler handler, Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        TesseractOCRConfig config = context.get(TesseractOCRConfig.class, DEFAULT_CONFIG);

        // If Tesseract is not on the path with the current config, do not try to run OCR
        // getSupportedTypes shouldn't have listed us as handling it, so this should only
        //  occur if someone directly calls this parser, not via DefaultParser or similar
        if (! hasTesseract(config))
            return;

        XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);

        TemporaryResources tmp = new TemporaryResources();
        File output = null;
        try {
            TikaInputStream tikaStream = TikaInputStream.get(stream, tmp);
            File input = tikaStream.getFile();
            long size = tikaStream.getLength();

            if (size >= config.getMinFileSizeToOcr() && size <= config.getMaxFileSizeToOcr()) {

                output = tmp.createTemporaryFile();
                doOCR(input, output, config);

                // Tesseract appends .txt to output file name
                output = new File(output.getAbsolutePath() + ".txt");

                if (output.exists())
                    extractOutput(new FileInputStream(output), xhtml);

            }

            // Temporary workaround for TIKA-1445 - until we can specify
            //  composite parsers with strategies (eg Composite, Try In Turn),
            //  always send the image onwards to the regular parser to have
            //  the metadata for them extracted as well
            _TMP_IMAGE_METADATA_PARSER.parse(tikaStream, handler, metadata, context);
        } finally {
            tmp.dispose();
            if (output != null) {
                output.delete();
            }
        }
    }
    // TIKA-1445 workaround parser
    private static Parser _TMP_IMAGE_METADATA_PARSER = new CompositeImageParser();
    private static class CompositeImageParser extends CompositeParser {
        private static final long serialVersionUID = -2398203346206381382L;
        private static List<Parser> imageParsers = Arrays.asList(new Parser[]{
                new ImageParser(), new JpegParser(), new TiffParser()
        });
        CompositeImageParser() {
            super(new MediaTypeRegistry(), imageParsers);
        }
    }

    /**
     * Run external tesseract-ocr process.
     *
     * @param input
     *          File to be ocred
     * @param output
     *          File to collect ocr result
     * @param config
     *          Configuration of tesseract-ocr engine
     * @throws TikaException
     *           if the extraction timed out
     * @throws IOException
     *           if an input error occurred
     */
    private void doOCR(File input, File output, TesseractOCRConfig config) throws IOException, TikaException {
        String[] cmd = { config.getTesseractPath() + getTesseractProg(), input.getPath(), output.getPath(), "-l",
                config.getLanguage(), "-psm", config.getPageSegMode() };

        ProcessBuilder pb = new ProcessBuilder(cmd);
        setEnv(config, pb);
        final Process process = pb.start();

        process.getOutputStream().close();
        InputStream out = process.getInputStream();
        InputStream err = process.getErrorStream();

        logStream("OCR MSG", out, input);
        logStream("OCR ERROR", err, input);

        FutureTask<Integer> waitTask = new FutureTask<Integer>(new Callable<Integer>() {
            public Integer call() throws Exception {
                return process.waitFor();
            }
        });

        Thread waitThread = new Thread(waitTask);
        waitThread.start();

        try {
            waitTask.get(config.getTimeout(), TimeUnit.SECONDS);

        } catch (InterruptedException e) {
            waitThread.interrupt();
            process.destroy();
            Thread.currentThread().interrupt();
            throw new TikaException("TesseractOCRParser interrupted", e);

        } catch (ExecutionException e) {
            // should not be thrown

        } catch (TimeoutException e) {
            waitThread.interrupt();
            process.destroy();
            throw new TikaException("TesseractOCRParser timeout", e);
        }

    }

    /**
     * Reads the contents of the given stream and write it to the given XHTML
     * content handler. The stream is closed once fully processed.
     *
     * @param stream
     *          Stream where is the result of ocr
     * @param xhtml
     *          XHTML content handler
     * @throws SAXException
     *           if the XHTML SAX events could not be handled
     * @throws IOException
     *           if an input error occurred
     */
    private void extractOutput(InputStream stream, XHTMLContentHandler xhtml) throws SAXException, IOException {

        Reader reader = new InputStreamReader(stream, "UTF-8");
        xhtml.startDocument();
        xhtml.startElement("div");
        try {
            char[] buffer = new char[1024];
            for (int n = reader.read(buffer); n != -1; n = reader.read(buffer)) {
                if (n > 0)
                    xhtml.characters(buffer, 0, n);
            }
        } finally {
            reader.close();
        }
        xhtml.endElement("div");
        xhtml.endDocument();
    }

    /**
     * Starts a thread that reads the contents of the standard output or error
     * stream of the given process to not block the process. The stream is closed
     * once fully processed.
     */
    private void logStream(final String logType, final InputStream stream, final File file) {
        new Thread() {
            public void run() {
                Reader reader = new InputStreamReader(stream);
                StringBuilder out = new StringBuilder();
                char[] buffer = new char[1024];
                try {
                    for (int n = reader.read(buffer); n != -1; n = reader.read(buffer))
                        out.append(buffer, 0, n);
                } catch (IOException e) {

                } finally {
                    IOUtils.closeQuietly(stream);
                }

                String msg = out.toString();
                // log or discard message?

            }
        }.start();
    }

    static String getTesseractProg() {
        return System.getProperty("os.name").startsWith("Windows") ? "tesseract.exe" : "tesseract";
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/odf/NSNormalizerContentHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.odf;

import java.io.IOException;
import java.io.StringReader;
import java.util.Locale;

import org.apache.tika.sax.ContentHandlerDecorator;
import org.xml.sax.Attributes;
import org.xml.sax.ContentHandler;
import org.xml.sax.InputSource;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.AttributesImpl;

/**
 * Content handler decorator that:<ul>
 * <li>Maps old OpenOffice 1.0 Namespaces to the OpenDocument ones</li>
 * <li>Returns a fake DTD when parser requests OpenOffice DTD</li>
 * </ul>
 */
public class NSNormalizerContentHandler extends ContentHandlerDecorator {

    private static final String OLD_NS =
        "http://openoffice.org/2000/";

    private static final String NEW_NS =
        "urn:oasis:names:tc:opendocument:xmlns:";

    private static final String DTD_PUBLIC_ID =
        "-//OpenOffice.org//DTD OfficeDocument 1.0//EN";

    public NSNormalizerContentHandler(ContentHandler handler) {
        super(handler);
    }

    private String mapOldNS(String ns) {
        if (ns != null && ns.startsWith(OLD_NS)) {
            return NEW_NS + ns.substring(OLD_NS.length()) + ":1.0";
        } else {
            return ns;
        }
    }

    @Override
    public void startElement(
            String namespaceURI, String localName, String qName,
            Attributes atts) throws SAXException {
        AttributesImpl natts = new AttributesImpl();
        for (int i = 0; i < atts.getLength(); i++) {
            natts.addAttribute(
                    mapOldNS(atts.getURI(i)), atts.getLocalName(i),
                    atts.getQName(i), atts.getType(i), atts.getValue(i));
        }
        super.startElement(mapOldNS(namespaceURI), localName, qName, atts);
    }

    @Override
    public void endElement(String namespaceURI, String localName, String qName)
            throws SAXException {
        super.endElement(mapOldNS(namespaceURI), localName, qName);
    }

    @Override
    public void startPrefixMapping(String prefix, String uri)
            throws SAXException {
        super.startPrefixMapping(prefix, mapOldNS(uri));
    }

    /**
     * do not load any DTDs (may be requested by parser). Fake the DTD by
     * returning a empty string as InputSource
     */
    @Override
    public InputSource resolveEntity(String publicId, String systemId)
            throws IOException, SAXException {
        if ((systemId != null && systemId.toLowerCase(Locale.ROOT).endsWith(".dtd"))
                || DTD_PUBLIC_ID.equals(publicId)) {
            return new InputSource(new StringReader(""));
        } else {
            return super.resolveEntity(publicId, systemId);
        }
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/odf/OpenDocumentContentParser.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.odf;

import static org.apache.tika.sax.XHTMLContentHandler.XHTML;

import java.io.IOException;
import java.io.InputStream;
import java.util.BitSet;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;
import java.util.Stack;

import javax.xml.XMLConstants;
import javax.xml.namespace.QName;
import javax.xml.parsers.ParserConfigurationException;
import javax.xml.parsers.SAXParser;
import javax.xml.parsers.SAXParserFactory;

import org.apache.tika.exception.TikaException;
import org.apache.tika.io.CloseShieldInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.ElementMappingContentHandler;
import org.apache.tika.sax.OfflineContentHandler;
import org.apache.tika.sax.XHTMLContentHandler;
import org.apache.tika.sax.ElementMappingContentHandler.TargetElement;
import org.xml.sax.Attributes;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;
import org.xml.sax.SAXNotRecognizedException;
import org.xml.sax.helpers.AttributesImpl;
import org.xml.sax.helpers.DefaultHandler;

/**
 * Parser for ODF <code>content.xml</code> files.
 */
public class OpenDocumentContentParser extends AbstractParser {

    private static final class OpenDocumentElementMappingContentHandler extends
			ElementMappingContentHandler {
		private final ContentHandler handler;
		private final BitSet textNodeStack = new BitSet();
		private int nodeDepth = 0;
		private int completelyFiltered = 0;
		private Stack<String> headingStack = new Stack<String>();

		private OpenDocumentElementMappingContentHandler(ContentHandler handler,
				Map<QName, TargetElement> mappings) {
			super(handler, mappings);
			this.handler = handler;
		}

		@Override
		public void characters(char[] ch, int start, int length)
		        throws SAXException {
		    // only forward content of tags from text:-namespace
		    if (completelyFiltered == 0 && nodeDepth > 0
		            && textNodeStack.get(nodeDepth - 1)) {
		        super.characters(ch,start,length);
		    }
		}

		// helper for checking tags which need complete filtering
		// (with sub-tags)
		private boolean needsCompleteFiltering(
		        String namespaceURI, String localName) {
		    if (TEXT_NS.equals(namespaceURI)) {
		        return localName.endsWith("-template")
		            || localName.endsWith("-style");
		    } else if (TABLE_NS.equals(namespaceURI)) {
		        return "covered-table-cell".equals(localName);
		    } else {
		        return false;
		    }
		}

		// map the heading level to <hX> HTML tags
		private String getXHTMLHeaderTagName(Attributes atts) {
		    String depthStr = atts.getValue(TEXT_NS, "outline-level");
		    if (depthStr == null) {
		        return "h1";
		    }

		    int depth = Integer.parseInt(depthStr);
		    if (depth >= 6) {
		        return "h6";
		    } else if (depth <= 1) {
		        return "h1";
		    } else {
		        return "h" + depth;
		    }
		}

		/**
		 * Check if a node is a text node
		 */
		private boolean isTextNode(String namespaceURI, String localName) {
		    if (TEXT_NS.equals(namespaceURI) && !localName.equals("page-number") && !localName.equals("page-count")) {
		        return true;
		    }
		    if (SVG_NS.equals(namespaceURI)) {
		        return "title".equals(localName) ||
		                "desc".equals(localName);
		    }
		    return false;
		}

		@Override
		public void startElement(
		        String namespaceURI, String localName, String qName,
		        Attributes atts) throws SAXException {
		    // keep track of current node type. If it is a text node,
		    // a bit at the current depth ist set in textNodeStack.
		    // characters() checks the top bit to determine, if the
		    // actual node is a text node to print out nodeDepth contains
		    // the depth of the current node and also marks top of stack.
		    assert nodeDepth >= 0;

		    textNodeStack.set(nodeDepth++, 
		            isTextNode(namespaceURI, localName));
		    // filter *all* content of some tags
		    assert completelyFiltered >= 0;

		    if (needsCompleteFiltering(namespaceURI, localName)) {
		        completelyFiltered++;
		    }
		    // call next handler if no filtering
		    if (completelyFiltered == 0) {
		        // special handling of text:h, that are directly passed
		        // to incoming handler
		        if (TEXT_NS.equals(namespaceURI) && "h".equals(localName)) {
		            final String el = headingStack.push(getXHTMLHeaderTagName(atts));
		            handler.startElement(XHTMLContentHandler.XHTML, el, el, EMPTY_ATTRIBUTES);
		        } else {
		            super.startElement(
		                    namespaceURI, localName, qName, atts);
		        }
		    }
		}

		@Override
		public void endElement(
		        String namespaceURI, String localName, String qName)
		        throws SAXException {
		    // call next handler if no filtering
		    if (completelyFiltered == 0) {
		        // special handling of text:h, that are directly passed
		        // to incoming handler
		        if (TEXT_NS.equals(namespaceURI) && "h".equals(localName)) {
		            final String el = headingStack.pop();
		            handler.endElement(XHTMLContentHandler.XHTML, el, el);
		        } else {
		            super.endElement(namespaceURI,localName,qName);
		        }

		        // special handling of tabulators
		        if (TEXT_NS.equals(namespaceURI)
		                && ("tab-stop".equals(localName)
		                        || "tab".equals(localName))) {
		            this.characters(TAB, 0, TAB.length);
		        }
		    }

		    // revert filter for *all* content of some tags
		    if (needsCompleteFiltering(namespaceURI,localName)) {
		        completelyFiltered--;
		    }
		    assert completelyFiltered >= 0;

		    // reduce current node depth
		    nodeDepth--;
		    assert nodeDepth >= 0;
		}

		@Override
		public void startPrefixMapping(String prefix, String uri) {
		    // remove prefix mappings as they should not occur in XHTML
		}

		@Override
		public void endPrefixMapping(String prefix) {
		    // remove prefix mappings as they should not occur in XHTML
		}
	}

	public static final String TEXT_NS =
        "urn:oasis:names:tc:opendocument:xmlns:text:1.0";

    public static final String TABLE_NS =
        "urn:oasis:names:tc:opendocument:xmlns:table:1.0";

    public static final String OFFICE_NS =
        "urn:oasis:names:tc:opendocument:xmlns:office:1.0";

    public static final String SVG_NS =
        "urn:oasis:names:tc:opendocument:xmlns:svg-compatible:1.0";

    public static final String PRESENTATION_NS =
        "urn:oasis:names:tc:opendocument:xmlns:presentation:1.0";

    public static final String DRAW_NS =
        "urn:oasis:names:tc:opendocument:xmlns:drawing:1.0";

    public static final String XLINK_NS = "http://www.w3.org/1999/xlink";

    protected static final char[] TAB = new char[] { '\t' };

    private static final Attributes EMPTY_ATTRIBUTES = new AttributesImpl();

    /**
     * Mappings between ODF tag names and XHTML tag names
     * (including attributes). All other tag names/attributes are ignored
     * and left out from event stream. 
     */
    private static final HashMap<QName, TargetElement> MAPPINGS =
        new HashMap<QName, TargetElement>();

    static {
        // general mappings of text:-tags
        MAPPINGS.put(
                new QName(TEXT_NS, "p"),
                new TargetElement(XHTML, "p"));
        // text:h-tags are mapped specifically in startElement/endElement
        MAPPINGS.put(
                new QName(TEXT_NS, "line-break"),
                new TargetElement(XHTML, "br"));
        MAPPINGS.put(
                new QName(TEXT_NS, "list"),
                new TargetElement(XHTML, "ul"));
        MAPPINGS.put(
                new QName(TEXT_NS, "list-item"),
                new TargetElement(XHTML, "li"));
        MAPPINGS.put(
                new QName(TEXT_NS, "note"),
                new TargetElement(XHTML, "div"));
        MAPPINGS.put(
                new QName(OFFICE_NS, "annotation"),
                new TargetElement(XHTML, "div"));
        MAPPINGS.put(
                new QName(PRESENTATION_NS, "notes"),
                new TargetElement(XHTML, "div"));
        MAPPINGS.put(
                new QName(DRAW_NS, "object"),
                new TargetElement(XHTML, "object"));
        MAPPINGS.put(
                new QName(DRAW_NS, "text-box"),
                new TargetElement(XHTML, "div"));
        MAPPINGS.put(
                new QName(SVG_NS, "title"),
                new TargetElement(XHTML, "span"));
        MAPPINGS.put(
                new QName(SVG_NS, "desc"),
                new TargetElement(XHTML, "span"));
        MAPPINGS.put(
                new QName(TEXT_NS, "span"),
                new TargetElement(XHTML, "span"));
        
        final HashMap<QName,QName> aAttsMapping =
            new HashMap<QName,QName>();
        aAttsMapping.put(
                new QName(XLINK_NS, "href"),
                new QName("href"));
        aAttsMapping.put(
                new QName(XLINK_NS, "title"),
                new QName("title"));
        MAPPINGS.put(
                new QName(TEXT_NS, "a"),
                new TargetElement(XHTML, "a", aAttsMapping));

        // create HTML tables from table:-tags
        MAPPINGS.put(
                new QName(TABLE_NS, "table"),
                new TargetElement(XHTML, "table"));
        // repeating of rows is ignored; for columns, see below!
        MAPPINGS.put(
                new QName(TABLE_NS, "table-row"),
                new TargetElement(XHTML, "tr"));
        // special mapping for rowspan/colspan attributes
        final HashMap<QName,QName> tableCellAttsMapping =
            new HashMap<QName,QName>();
        tableCellAttsMapping.put(
                new QName(TABLE_NS, "number-columns-spanned"),
                new QName("colspan"));
        tableCellAttsMapping.put(
                new QName(TABLE_NS, "number-rows-spanned"),
                new QName("rowspan"));
        /* TODO: The following is not correct, the cell should be repeated not spanned!
         * Code generates a HTML cell, spanning all repeated columns, to make the cell look correct.
         * Problems may occur when both spanning and repeating is given, which is not allowed by spec.
         * Cell spanning instead of repeating  is not a problem, because OpenOffice uses it
         * only for empty cells.
         */
        tableCellAttsMapping.put(
                new QName(TABLE_NS, "number-columns-repeated"),
                new QName("colspan"));
        MAPPINGS.put(
                new QName(TABLE_NS, "table-cell"),
                new TargetElement(XHTML, "td", tableCellAttsMapping));
    }

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return Collections.emptySet(); // not a top-level parser
    }

    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        parseInternal(stream,
                      new XHTMLContentHandler(handler,metadata),
                      metadata, context);
    }

    void parseInternal(
            InputStream stream, final ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {

        DefaultHandler dh = new OpenDocumentElementMappingContentHandler(handler, MAPPINGS);

        try {
            SAXParserFactory factory = SAXParserFactory.newInstance();
            factory.setValidating(false);
            factory.setNamespaceAware(true);
            try {
                factory.setFeature(XMLConstants.FEATURE_SECURE_PROCESSING, true);
            } catch (SAXNotRecognizedException e){
                // TIKA-329: Some XML parsers do not support the secure-processing
                // feature, even though it's required by JAXP in Java 5. Ignoring
                // the exception is fine here, deployments without this feature
                // are inherently vulnerable to XML denial-of-service attacks.
            }
            SAXParser parser = factory.newSAXParser();
            parser.parse(
                    new CloseShieldInputStream(stream),
                    new OfflineContentHandler(
                            new NSNormalizerContentHandler(dh)));
        } catch (ParserConfigurationException e) {
            throw new TikaException("XML parser configuration error", e);
        }
    }

}
	"
tika-parsers/src/main/java/org/apache/tika/parser/odf/OpenDocumentMetaParser.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.odf;

import java.io.IOException;
import java.io.InputStream;

import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.DublinCore;
import org.apache.tika.metadata.MSOffice;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.Office;
import org.apache.tika.metadata.OfficeOpenXMLCore;
import org.apache.tika.metadata.PagedText;
import org.apache.tika.metadata.Property;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.xml.AttributeDependantMetadataHandler;
import org.apache.tika.parser.xml.AttributeMetadataHandler;
import org.apache.tika.parser.xml.ElementMetadataHandler;
import org.apache.tika.parser.xml.MetadataHandler;
import org.apache.tika.parser.xml.XMLParser;
import org.apache.tika.sax.TeeContentHandler;
import org.apache.tika.sax.xpath.CompositeMatcher;
import org.apache.tika.sax.xpath.Matcher;
import org.apache.tika.sax.xpath.MatchingContentHandler;
import org.apache.tika.sax.xpath.XPathParser;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * Parser for OpenDocument <code>meta.xml</code> files.
 */
public class OpenDocumentMetaParser extends XMLParser {
    /**
     * Serial version UID
     */
    private static final long serialVersionUID = -8739250869531737584L;
   
    private static final String META_NS = "urn:oasis:names:tc:opendocument:xmlns:meta:1.0"; 
    private static final XPathParser META_XPATH = new XPathParser("meta", META_NS);
    
    /** 
     * @see OfficeOpenXMLCore#SUBJECT 
     * @deprecated use OfficeOpenXMLCore#SUBJECT
     */
    @Deprecated
    private static final Property TRANSITION_INITIAL_CREATOR_TO_INITIAL_AUTHOR = 
        Property.composite(Office.INITIAL_AUTHOR, 
            new Property[] { Property.externalText("initial-creator") });
    
    private static ContentHandler getDublinCoreHandler(
            Metadata metadata, Property property, String element) {
        return new ElementMetadataHandler(
                DublinCore.NAMESPACE_URI_DC, element,
                metadata, property);
    }
    
    private static ContentHandler getMeta(
            ContentHandler ch, Metadata md, Property property, String element) {
        Matcher matcher = new CompositeMatcher(
                META_XPATH.parse("//meta:" + element),
                META_XPATH.parse("//meta:" + element + "//text()"));
        ContentHandler branch =
            new MatchingContentHandler(new MetadataHandler(md, property), matcher);
        return new TeeContentHandler(ch, branch);
    }

    private static ContentHandler getUserDefined(
            ContentHandler ch, Metadata md) {
        Matcher matcher = new CompositeMatcher(
                META_XPATH.parse("//meta:user-defined/@meta:name"),
                META_XPATH.parse("//meta:user-defined//text()"));
        // eg <meta:user-defined meta:name="Info1">Text1</meta:user-defined> becomes custom:Info1=Text1
        ContentHandler branch = new MatchingContentHandler(
              new AttributeDependantMetadataHandler(md, "meta:name", Metadata.USER_DEFINED_METADATA_NAME_PREFIX),
              matcher);
        return new TeeContentHandler(ch, branch);
    }

    @Deprecated private static ContentHandler getStatistic(
            ContentHandler ch, Metadata md, String name, String attribute) {
        Matcher matcher =
            META_XPATH.parse("//meta:document-statistic/@meta:"+attribute);
        ContentHandler branch = new MatchingContentHandler(
              new AttributeMetadataHandler(META_NS, attribute, md, name), matcher);
        return new TeeContentHandler(ch, branch);
    }
    private static ContentHandler getStatistic(
          ContentHandler ch, Metadata md, Property property, String attribute) {
      Matcher matcher =
          META_XPATH.parse("//meta:document-statistic/@meta:"+attribute);
      ContentHandler branch = new MatchingContentHandler(
            new AttributeMetadataHandler(META_NS, attribute, md, property), matcher);
      return new TeeContentHandler(ch, branch);
  }

    protected ContentHandler getContentHandler(ContentHandler ch, Metadata md, ParseContext context) {
        // We can no longer extend DcXMLParser due to the handling of dc:subject and dc:date
        // Process the Dublin Core Attributes 
        ch = new TeeContentHandler(super.getContentHandler(ch, md, context),
                getDublinCoreHandler(md, TikaCoreProperties.TITLE, "title"),
                getDublinCoreHandler(md, TikaCoreProperties.CREATOR, "creator"),
                getDublinCoreHandler(md, TikaCoreProperties.DESCRIPTION, "description"),
                getDublinCoreHandler(md, TikaCoreProperties.PUBLISHER, "publisher"),
                getDublinCoreHandler(md, TikaCoreProperties.CONTRIBUTOR, "contributor"),
                getDublinCoreHandler(md, TikaCoreProperties.TYPE, "type"),
                getDublinCoreHandler(md, TikaCoreProperties.FORMAT, "format"),
                getDublinCoreHandler(md, TikaCoreProperties.IDENTIFIER, "identifier"),
                getDublinCoreHandler(md, TikaCoreProperties.LANGUAGE, "language"),
                getDublinCoreHandler(md, TikaCoreProperties.RIGHTS, "rights"));
        
        // Process the OO Meta Attributes
        ch = getMeta(ch, md, TikaCoreProperties.CREATED, "creation-date");
        // ODF uses dc:date for modified
        ch = new TeeContentHandler(ch, new ElementMetadataHandler(
                DublinCore.NAMESPACE_URI_DC, "date",
                md, TikaCoreProperties.MODIFIED));
        
        // ODF uses dc:subject for description
        ch = new TeeContentHandler(ch, new ElementMetadataHandler(
                DublinCore.NAMESPACE_URI_DC, "subject",
                md, TikaCoreProperties.TRANSITION_SUBJECT_TO_OO_SUBJECT));
        ch = getMeta(ch, md, TikaCoreProperties.TRANSITION_KEYWORDS_TO_DC_SUBJECT, "keyword");
        
        ch = getMeta(ch, md, Property.externalText(MSOffice.EDIT_TIME), "editing-duration");        
        ch = getMeta(ch, md, Property.externalText("editing-cycles"), "editing-cycles");
        ch = getMeta(ch, md, TRANSITION_INITIAL_CREATOR_TO_INITIAL_AUTHOR, "initial-creator");
        ch = getMeta(ch, md, Property.externalText("generator"), "generator");
        
        // Process the user defined Meta Attributes
        ch = getUserDefined(ch, md);
        
        // Process the OO Statistics Attributes
        ch = getStatistic(ch, md, Office.OBJECT_COUNT,  "object-count");
        ch = getStatistic(ch, md, Office.IMAGE_COUNT,   "image-count");
        ch = getStatistic(ch, md, Office.PAGE_COUNT,    "page-count");
        ch = getStatistic(ch, md, PagedText.N_PAGES,    "page-count");
        ch = getStatistic(ch, md, Office.TABLE_COUNT,   "table-count");
        ch = getStatistic(ch, md, Office.PARAGRAPH_COUNT, "paragraph-count");
        ch = getStatistic(ch, md, Office.WORD_COUNT,      "word-count");
        ch = getStatistic(ch, md, Office.CHARACTER_COUNT, "character-count");
        
        // Legacy, Tika-1.0 style attributes
        // TODO Remove these in Tika 2.0
        ch = getStatistic(ch, md, MSOffice.OBJECT_COUNT,  "object-count");
        ch = getStatistic(ch, md, MSOffice.IMAGE_COUNT,   "image-count");
        ch = getStatistic(ch, md, MSOffice.PAGE_COUNT,    "page-count");
        ch = getStatistic(ch, md, MSOffice.TABLE_COUNT,   "table-count");
        ch = getStatistic(ch, md, MSOffice.PARAGRAPH_COUNT, "paragraph-count");
        ch = getStatistic(ch, md, MSOffice.WORD_COUNT,      "word-count");
        ch = getStatistic(ch, md, MSOffice.CHARACTER_COUNT, "character-count");
        
        // Legacy Statistics Attributes, replaced with real keys above
        // TODO Remove these shortly, eg after Tika 1.1 (TIKA-770)
        ch = getStatistic(ch, md, "nbPage", "page-count");
        ch = getStatistic(ch, md, "nbPara", "paragraph-count");
        ch = getStatistic(ch, md, "nbWord", "word-count");
        ch = getStatistic(ch, md, "nbCharacter", "character-count");
        ch = getStatistic(ch, md, "nbTab", "table-count");
        ch = getStatistic(ch, md, "nbObject", "object-count");
        ch = getStatistic(ch, md, "nbImg", "image-count");
        
        // Normalise the rest
        ch = new NSNormalizerContentHandler(ch);
        return ch;
    }
    
    @Override
    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        super.parse(stream, handler, metadata, context);
        // Copy subject to description for OO2
        String odfSubject = metadata.get(OfficeOpenXMLCore.SUBJECT);
        if (odfSubject != null && !odfSubject.equals("") && 
                (metadata.get(TikaCoreProperties.DESCRIPTION) == null || metadata.get(TikaCoreProperties.DESCRIPTION).equals(""))) {
            metadata.set(TikaCoreProperties.DESCRIPTION, odfSubject);
        }
    }
    
}
"
tika-parsers/src/main/java/org/apache/tika/parser/odf/OpenDocumentParser.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.odf;

import java.io.IOException;
import java.io.InputStream;
import java.util.Arrays;
import java.util.Collections;
import java.util.Enumeration;
import java.util.HashSet;
import java.util.Set;
import java.util.zip.ZipEntry;
import java.util.zip.ZipFile;
import java.util.zip.ZipInputStream;

import org.apache.tika.exception.TikaException;
import org.apache.tika.io.IOUtils;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.Parser;
import org.apache.tika.sax.EndDocumentShieldingContentHandler;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.DefaultHandler;

/**
 * OpenOffice parser
 */
public class OpenDocumentParser extends AbstractParser {

    /** Serial version UID */
    private static final long serialVersionUID = -6410276875438618287L;

    private static final Set<MediaType> SUPPORTED_TYPES =
        Collections.unmodifiableSet(new HashSet<MediaType>(Arrays.asList(
                MediaType.application("vnd.sun.xml.writer"),
                MediaType.application("vnd.oasis.opendocument.text"),
                MediaType.application("vnd.oasis.opendocument.graphics"),
                MediaType.application("vnd.oasis.opendocument.presentation"),
                MediaType.application("vnd.oasis.opendocument.spreadsheet"),
                MediaType.application("vnd.oasis.opendocument.chart"),
                MediaType.application("vnd.oasis.opendocument.image"),
                MediaType.application("vnd.oasis.opendocument.formula"),
                MediaType.application("vnd.oasis.opendocument.text-master"),
                MediaType.application("vnd.oasis.opendocument.text-web"),
                MediaType.application("vnd.oasis.opendocument.text-template"),
                MediaType.application("vnd.oasis.opendocument.graphics-template"),
                MediaType.application("vnd.oasis.opendocument.presentation-template"),
                MediaType.application("vnd.oasis.opendocument.spreadsheet-template"),
                MediaType.application("vnd.oasis.opendocument.chart-template"),
                MediaType.application("vnd.oasis.opendocument.image-template"),
                MediaType.application("vnd.oasis.opendocument.formula-template"),
                MediaType.application("x-vnd.oasis.opendocument.text"),
                MediaType.application("x-vnd.oasis.opendocument.graphics"),
                MediaType.application("x-vnd.oasis.opendocument.presentation"),
                MediaType.application("x-vnd.oasis.opendocument.spreadsheet"),
                MediaType.application("x-vnd.oasis.opendocument.chart"),
                MediaType.application("x-vnd.oasis.opendocument.image"),
                MediaType.application("x-vnd.oasis.opendocument.formula"),
                MediaType.application("x-vnd.oasis.opendocument.text-master"),
                MediaType.application("x-vnd.oasis.opendocument.text-web"),
                MediaType.application("x-vnd.oasis.opendocument.text-template"),
                MediaType.application("x-vnd.oasis.opendocument.graphics-template"),
                MediaType.application("x-vnd.oasis.opendocument.presentation-template"),
                MediaType.application("x-vnd.oasis.opendocument.spreadsheet-template"),
                MediaType.application("x-vnd.oasis.opendocument.chart-template"),
                MediaType.application("x-vnd.oasis.opendocument.image-template"),
                MediaType.application("x-vnd.oasis.opendocument.formula-template"))));

    private static final String META_NAME = "meta.xml";
    
    private Parser meta = new OpenDocumentMetaParser();

    private Parser content = new OpenDocumentContentParser();

    public Parser getMetaParser() {
        return meta;
    }

    public void setMetaParser(Parser meta) {
        this.meta = meta;
    }

    public Parser getContentParser() {
        return content;
    }

    public void setContentParser(Parser content) {
        this.content = content;
    }

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }

    public void parse(
            InputStream stream, ContentHandler baseHandler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {

        // Open the Zip stream
        // Use a File if we can, and an already open zip is even better
        ZipFile zipFile = null;
        ZipInputStream zipStream = null;
        if (stream instanceof TikaInputStream) {
            TikaInputStream tis = (TikaInputStream) stream;
            Object container = ((TikaInputStream) stream).getOpenContainer();
            if (container instanceof ZipFile) {
                zipFile = (ZipFile) container;
            } else if (tis.hasFile()) {
                zipFile = new ZipFile(tis.getFile());                
            } else {
                zipStream = new ZipInputStream(stream);
            }
        } else {
            zipStream = new ZipInputStream(stream);
        }

        // Prepare to handle the content
        XHTMLContentHandler xhtml = new XHTMLContentHandler(baseHandler, metadata);

        // As we don't know which of the metadata or the content
        //  we'll hit first, catch the endDocument call initially
        EndDocumentShieldingContentHandler handler = 
          new EndDocumentShieldingContentHandler(xhtml);
        
        // If we can, process the metadata first, then the
        //  rest of the file afterwards
        // Only possible to guarantee that when opened from a file not a stream
        ZipEntry entry = null;
        if (zipFile != null) {
            entry = zipFile.getEntry(META_NAME);
            handleZipEntry(entry, zipFile.getInputStream(entry), metadata, context, handler);

            Enumeration<? extends ZipEntry> entries = zipFile.entries();
            while (entries.hasMoreElements()) {
                entry = entries.nextElement();
                if (! META_NAME.equals(entry.getName())) {
                    handleZipEntry(entry, zipFile.getInputStream(entry), metadata, context, handler);
                }
            }
            zipFile.close();
        } else {
            do {
                entry = zipStream.getNextEntry();
                handleZipEntry(entry, zipStream, metadata, context, handler);
            } while (entry != null);
            zipStream.close();
        }
        
        // Only now call the end document
        if(handler.getEndDocumentWasCalled()) {
           handler.reallyEndDocument();
        }
    }
    
    private void handleZipEntry(ZipEntry entry, InputStream zip, Metadata metadata, 
            ParseContext context, EndDocumentShieldingContentHandler handler)
            throws IOException, SAXException, TikaException {
        if (entry == null) return;
        
        if (entry.getName().equals("mimetype")) {
            String type = IOUtils.toString(zip, "UTF-8");
            metadata.set(Metadata.CONTENT_TYPE, type);
        } else if (entry.getName().equals(META_NAME)) {
            meta.parse(zip, new DefaultHandler(), metadata, context);
        } else if (entry.getName().endsWith("content.xml")) {
            if (content instanceof OpenDocumentContentParser) {
                ((OpenDocumentContentParser) content).parseInternal(zip, handler, metadata, context);
            } else {
                // Foreign content parser was set:
                content.parse(zip, handler, metadata, context);
            }
        } else if (entry.getName().endsWith("styles.xml")) {
            if (content instanceof OpenDocumentContentParser) {
                ((OpenDocumentContentParser) content).parseInternal(zip, handler, metadata, context);
            } else {
                // Foreign content parser was set:
                content.parse(zip, handler, metadata, context);
            }
        }
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/opendocument/OpenOfficeParser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.opendocument;

import org.apache.tika.parser.odf.OpenDocumentParser;

/**
 * OpenOffice parser
 *
 * @deprecated Use the {@link OpenDocumentParser} class instead.
 *             This class will be removed in Apache Tika 1.0.
 */
public class OpenOfficeParser extends OpenDocumentParser {
}
"
tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDF2XHTML.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.pdf;

import java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.io.Writer;
import java.text.SimpleDateFormat;
import java.util.Calendar;
import java.util.HashMap;
import java.util.List;
import java.util.ListIterator;
import java.util.Locale;
import java.util.Map;
import java.util.TreeMap;

import org.apache.pdfbox.pdmodel.PDDocument;
import org.apache.pdfbox.pdmodel.PDDocumentCatalog;
import org.apache.pdfbox.pdmodel.PDDocumentNameDictionary;
import org.apache.pdfbox.pdmodel.PDEmbeddedFilesNameTreeNode;
import org.apache.pdfbox.pdmodel.PDPage;
import org.apache.pdfbox.pdmodel.PDResources;
import org.apache.pdfbox.pdmodel.common.COSObjectable;
import org.apache.pdfbox.pdmodel.common.PDNameTreeNode;
import org.apache.pdfbox.pdmodel.common.filespecification.PDComplexFileSpecification;
import org.apache.pdfbox.pdmodel.common.filespecification.PDEmbeddedFile;
import org.apache.pdfbox.pdmodel.graphics.xobject.PDCcitt;
import org.apache.pdfbox.pdmodel.graphics.xobject.PDJpeg;
import org.apache.pdfbox.pdmodel.graphics.xobject.PDPixelMap;
import org.apache.pdfbox.pdmodel.graphics.xobject.PDXObject;
import org.apache.pdfbox.pdmodel.graphics.xobject.PDXObjectForm;
import org.apache.pdfbox.pdmodel.graphics.xobject.PDXObjectImage;
import org.apache.pdfbox.pdmodel.interactive.action.type.PDAction;
import org.apache.pdfbox.pdmodel.interactive.action.type.PDActionURI;
import org.apache.pdfbox.pdmodel.interactive.annotation.PDAnnotation;
import org.apache.pdfbox.pdmodel.interactive.annotation.PDAnnotationFileAttachment;
import org.apache.pdfbox.pdmodel.interactive.annotation.PDAnnotationLink;
import org.apache.pdfbox.pdmodel.interactive.annotation.PDAnnotationMarkup;
import org.apache.pdfbox.pdmodel.interactive.digitalsignature.PDSignature;
import org.apache.pdfbox.pdmodel.interactive.documentnavigation.outline.PDDocumentOutline;
import org.apache.pdfbox.pdmodel.interactive.documentnavigation.outline.PDOutlineItem;
import org.apache.pdfbox.pdmodel.interactive.documentnavigation.outline.PDOutlineNode;
import org.apache.pdfbox.pdmodel.interactive.form.PDAcroForm;
import org.apache.pdfbox.pdmodel.interactive.form.PDField;
import org.apache.pdfbox.pdmodel.interactive.form.PDSignatureField;
import org.apache.pdfbox.util.PDFTextStripper;
import org.apache.pdfbox.util.TextPosition;
import org.apache.tika.exception.TikaException;
import org.apache.tika.extractor.EmbeddedDocumentExtractor;
import org.apache.tika.extractor.ParsingEmbeddedDocumentExtractor;
import org.apache.tika.io.IOExceptionWithCause;
import org.apache.tika.io.IOUtils;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.EmbeddedContentHandler;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.AttributesImpl;

/**
 * Utility class that overrides the {@link PDFTextStripper} functionality
 * to produce a semi-structured XHTML SAX events instead of a plain text
 * stream.
 */
class PDF2XHTML extends PDFTextStripper {
    
    /**
     * Format used for signature dates
     * TODO Make this thread-safe
     */
    private final SimpleDateFormat dateFormat = new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ssZ", Locale.ROOT);
 
    /**
     * Maximum recursive depth during AcroForm processing.
     * Prevents theoretical AcroForm recursion bomb. 
     */
    private final static int MAX_ACROFORM_RECURSIONS = 10;


    // TODO: remove once PDFBOX-2160 is fixed:
    private boolean inParagraph = false;

    /**
     * This keeps track of the pdf object ids for inline
     * images that have been processed.  If {@link PDFParserConfig#getExtractUniqueInlineImagesOnly()
     * is true, this will be checked before extracting an embedded image.
     * The integer keeps track of the inlineImageCounter for that image.
     * This integer is used to identify images in the markup.
     */
    private Map<String, Integer> processedInlineImages = new HashMap<String, Integer>();

    private int inlineImageCounter = 0;

    /**
     * Converts the given PDF document (and related metadata) to a stream
     * of XHTML SAX events sent to the given content handler.
     *
     * @param document PDF document
     * @param handler SAX content handler
     * @param metadata PDF metadata
     * @throws SAXException if the content handler fails to process SAX events
     * @throws TikaException if the PDF document can not be processed
     */
    public static void process(
            PDDocument document, ContentHandler handler, ParseContext context, Metadata metadata,
            PDFParserConfig config)
            throws SAXException, TikaException {
        try {
            // Extract text using a dummy Writer as we override the
            // key methods to output to the given content
            // handler.
            PDF2XHTML pdf2XHTML = new PDF2XHTML(handler, context, metadata, config);
            
            config.configure(pdf2XHTML);

            pdf2XHTML.writeText(document, new Writer() {
                @Override
                public void write(char[] cbuf, int off, int len) {
                }
                @Override
                public void flush() {
                }
                @Override
                public void close() {
                }
            });

        } catch (IOException e) {
            if (e.getCause() instanceof SAXException) {
                throw (SAXException) e.getCause();
            } else {
                throw new TikaException("Unable to extract PDF content", e);
            }
        }
    }
    
    private final ContentHandler originalHandler;
    private final ParseContext context;
    private final XHTMLContentHandler handler;
    private final PDFParserConfig config;
    
    private PDF2XHTML(ContentHandler handler, ParseContext context, Metadata metadata, 
            PDFParserConfig config)
            throws IOException {
        //source of config (derives from context or PDFParser?) is
        //already determined in PDFParser.  No need to check context here.
        this.config = config;
        this.originalHandler = handler;
        this.context = context;
        this.handler = new XHTMLContentHandler(handler, metadata);
    }

    void extractBookmarkText() throws SAXException {
        PDDocumentOutline outline = document.getDocumentCatalog().getDocumentOutline();
        if (outline != null) {
            extractBookmarkText(outline);
        }
    }

    void extractBookmarkText(PDOutlineNode bookmark) throws SAXException {
        PDOutlineItem current = bookmark.getFirstChild();
        if (current != null) {
            handler.startElement("ul");
            while (current != null) {
                handler.startElement("li");
                handler.characters(current.getTitle());
                handler.endElement("li");
                // Recurse:
                extractBookmarkText(current);
                current = current.getNextSibling();
            }
            handler.endElement("ul");
        }
    }

    @Override
    protected void startDocument(PDDocument pdf) throws IOException {
        try {
            handler.startDocument();
        } catch (SAXException e) {
            throw new IOExceptionWithCause("Unable to start a document", e);
        }
    }

    @Override
    protected void endDocument(PDDocument pdf) throws IOException {
        try {
            // Extract text for any bookmarks:
            extractBookmarkText();
            extractEmbeddedDocuments(pdf, originalHandler);
            
            //extract acroform data at end of doc
            if (config.getExtractAcroFormContent() == true) {
                extractAcroForm(pdf, handler);
             }
            handler.endDocument();
        } catch (TikaException e) {
           throw new IOExceptionWithCause("Unable to end a document", e);
        } catch (SAXException e) {
            throw new IOExceptionWithCause("Unable to end a document", e);
        }
    }

    @Override
    protected void startPage(PDPage page) throws IOException {
        try {
            handler.startElement("div", "class", "page");
        } catch (SAXException e) {
            throw new IOExceptionWithCause("Unable to start a page", e);
        }
        writeParagraphStart();
    }

    @Override
    protected void endPage(PDPage page) throws IOException {
        try {
            writeParagraphEnd();

            extractImages(page.getResources());

            EmbeddedDocumentExtractor extractor = getEmbeddedDocumentExtractor();
            for (PDAnnotation annotation : page.getAnnotations()) {

                if (annotation instanceof PDAnnotationFileAttachment){
                    PDAnnotationFileAttachment fann = (PDAnnotationFileAttachment) annotation;
                    PDComplexFileSpecification fileSpec = (PDComplexFileSpecification) fann.getFile();
                    try {
                        extractMultiOSPDEmbeddedFiles("", fileSpec, extractor);
                    } catch (SAXException e) {
                        throw new IOExceptionWithCause("file embedded in annotation sax exception", e);
                    } catch (TikaException e) {
                        throw new IOExceptionWithCause("file embedded in annotation tika exception", e);
                    }
                }
                // TODO: remove once PDFBOX-1143 is fixed:
                if (config.getExtractAnnotationText()) {
                    if (annotation instanceof PDAnnotationLink) {
                        PDAnnotationLink annotationlink = (PDAnnotationLink) annotation;
                        if (annotationlink.getAction() != null) {
                            PDAction action = annotationlink.getAction();
                            if (action instanceof PDActionURI) {
                                PDActionURI uri = (PDActionURI) action;
                                String link = uri.getURI();
                                if (link != null) {
                                    handler.startElement("div", "class", "annotation");
                                    handler.startElement("a", "href", link);
                                    handler.endElement("a");
                                    handler.endElement("div");
                                }
                            }
                        }
                    }

                    if (annotation instanceof PDAnnotationMarkup) {
                        PDAnnotationMarkup annotationMarkup = (PDAnnotationMarkup) annotation;
                        String title = annotationMarkup.getTitlePopup();
                        String subject = annotationMarkup.getSubject();
                        String contents = annotationMarkup.getContents();
                        // TODO: maybe also annotationMarkup.getRichContents()?
                        if (title != null || subject != null || contents != null) {
                            handler.startElement("div", "class", "annotation");

                            if (title != null) {
                                handler.startElement("div", "class", "annotationTitle");
                                handler.characters(title);
                                handler.endElement("div");
                            }

                            if (subject != null) {
                                handler.startElement("div", "class", "annotationSubject");
                                handler.characters(subject);
                                handler.endElement("div");
                            }

                            if (contents != null) {
                                handler.startElement("div", "class", "annotationContents");
                                handler.characters(contents);
                                handler.endElement("div");
                            }

                            handler.endElement("div");
                        }
                    }
                }
            }

            handler.endElement("div");
        } catch (SAXException e) {
            throw new IOExceptionWithCause("Unable to end a page", e);
        }
        page.clear();
    }

    private void extractImages(PDResources resources) throws SAXException {
        if (resources == null || config.getExtractInlineImages() == false) {
            return;
        }

        Map<String, PDXObject> xObjects = resources.getXObjects();
        if (xObjects == null) {
            return;
        }

        for (Map.Entry<String, PDXObject> entry : xObjects.entrySet()) {
                        
            PDXObject object = entry.getValue();
            if (object instanceof PDXObjectForm) {
                extractImages(((PDXObjectForm) object).getResources());
            } else if (object instanceof PDXObjectImage) {

                PDXObjectImage image = (PDXObjectImage) object;

                Metadata metadata = new Metadata();
                String extension = "";
                if (image instanceof PDJpeg) {
                    metadata.set(Metadata.CONTENT_TYPE, "image/jpeg");
                    extension = ".jpg";
                } else if (image instanceof PDCcitt) {
                    metadata.set(Metadata.CONTENT_TYPE, "image/tiff");
                    extension = ".tif";
                } else if (image instanceof PDPixelMap) {
                    metadata.set(Metadata.CONTENT_TYPE, "image/png");
                    extension = ".png";
                }

                Integer imageNumber = processedInlineImages.get(entry.getKey());
                if (imageNumber == null) {
                    imageNumber = inlineImageCounter++;
                }
                String fileName = "image"+imageNumber+extension;
                metadata.set(Metadata.RESOURCE_NAME_KEY, fileName);

                // Output the img tag
                AttributesImpl attr = new AttributesImpl();
                attr.addAttribute("", "src", "src", "CDATA", "embedded:" + fileName);
                attr.addAttribute("", "alt", "alt", "CDATA", fileName);
                handler.startElement("img", attr);
                handler.endElement("img");

                //Do we only want to process unique COSObject ids?
                //If so, have we already processed this one?
                if (config.getExtractUniqueInlineImagesOnly() == true) {
                    String cosObjectId = entry.getKey();
                    if (processedInlineImages.containsKey(cosObjectId)){
                        continue;
                    }
                    processedInlineImages.put(cosObjectId, imageNumber);
                }

                metadata.set(TikaCoreProperties.EMBEDDED_RESOURCE_TYPE,
                        TikaCoreProperties.EmbeddedResourceType.INLINE.toString());

                EmbeddedDocumentExtractor extractor =
                        getEmbeddedDocumentExtractor();
                if (extractor.shouldParseEmbedded(metadata)) {
                    ByteArrayOutputStream buffer = new ByteArrayOutputStream();
                    try {
                        image.write2OutputStream(buffer);
                        image.clear();
                        extractor.parseEmbedded(
                                new ByteArrayInputStream(buffer.toByteArray()),
                                new EmbeddedContentHandler(handler),
                                metadata, false);
                    } catch (IOException e) {
                        // could not extract this image, so just skip it...
                    }
                }
            }
        }
        resources.clear();
    }

    protected EmbeddedDocumentExtractor getEmbeddedDocumentExtractor() {
        EmbeddedDocumentExtractor extractor =
                context.get(EmbeddedDocumentExtractor.class);
        if (extractor == null) {
            extractor = new ParsingEmbeddedDocumentExtractor(context);
        }
        return extractor;
    }

    @Override
    protected void writeParagraphStart() throws IOException {
        // TODO: remove once PDFBOX-2160 is fixed
        if (inParagraph) {
            // Close last paragraph
            writeParagraphEnd();
        }
        assert !inParagraph;
        inParagraph = true;
        try {
            handler.startElement("p");
        } catch (SAXException e) {
            throw new IOExceptionWithCause("Unable to start a paragraph", e);
        }
    }

    @Override
    protected void writeParagraphEnd() throws IOException {
        // TODO: remove once PDFBOX-2160 is fixed
        if (!inParagraph) {
            writeParagraphStart();
        }
        assert inParagraph;
        inParagraph = false;
        try {
            handler.endElement("p");
        } catch (SAXException e) {
            throw new IOExceptionWithCause("Unable to end a paragraph", e);
        }
    }

    @Override
    protected void writeString(String text) throws IOException {
        try {
            handler.characters(text);
        } catch (SAXException e) {
            throw new IOExceptionWithCause(
                    "Unable to write a string: " + text, e);
        }
    }

    @Override
    protected void writeCharacters(TextPosition text) throws IOException {
        try {
            handler.characters(text.getCharacter());
        } catch (SAXException e) {
            throw new IOExceptionWithCause(
                    "Unable to write a character: " + text.getCharacter(), e);
        }
    }

    @Override
    protected void writeWordSeparator() throws IOException {
        try {
            handler.characters(getWordSeparator());
        } catch (SAXException e) {
            throw new IOExceptionWithCause(
                    "Unable to write a space character", e);
        }
    }

    @Override
    protected void writeLineSeparator() throws IOException {
        try {
            handler.newline();
        } catch (SAXException e) {
            throw new IOExceptionWithCause(
                    "Unable to write a newline character", e);
        }
    }
    
    private void extractEmbeddedDocuments(PDDocument document, ContentHandler handler)
            throws IOException, SAXException, TikaException {
        PDDocumentCatalog catalog = document.getDocumentCatalog();
        PDDocumentNameDictionary names = catalog.getNames();
        if (names == null) {
            return;
        }
        PDEmbeddedFilesNameTreeNode embeddedFiles = names.getEmbeddedFiles();

        if (embeddedFiles == null) {
            return;
        }

        Map<String, COSObjectable> embeddedFileNames = embeddedFiles.getNames();
        //For now, try to get the embeddedFileNames out of embeddedFiles or its kids.
        //This code follows: pdfbox/examples/pdmodel/ExtractEmbeddedFiles.java
        //If there is a need we could add a fully recursive search to find a non-null
        //Map<String, COSObjectable> that contains the doc info.
        if (embeddedFileNames != null) {
            processEmbeddedDocNames(embeddedFileNames);
        } else {
            List<PDNameTreeNode> kids = embeddedFiles.getKids();
            if (kids == null) {
                return;
            }
            for (PDNameTreeNode n : kids) {
                Map<String, COSObjectable> childNames = n.getNames();
                if (childNames != null) {
                    processEmbeddedDocNames(childNames);
                }
            }
        }
    }


    private void processEmbeddedDocNames(Map<String, COSObjectable> embeddedFileNames)
            throws IOException, SAXException, TikaException {
        if (embeddedFileNames == null || embeddedFileNames.isEmpty()) {
            return;
        }

        EmbeddedDocumentExtractor extractor = getEmbeddedDocumentExtractor();
        for (Map.Entry<String,COSObjectable> ent : embeddedFileNames.entrySet()) {
            PDComplexFileSpecification spec = (PDComplexFileSpecification) ent.getValue();
            extractMultiOSPDEmbeddedFiles(ent.getKey(), spec, extractor);
        }
    }

    private void extractMultiOSPDEmbeddedFiles(String defaultName,
        PDComplexFileSpecification spec, EmbeddedDocumentExtractor extractor) throws IOException,
            SAXException, TikaException {

        if (spec == null) {
            return;
        }
        //current strategy is to pull all, not just first non-null
        extractPDEmbeddedFile(defaultName, spec.getFile(), spec.getEmbeddedFile(), extractor);
        extractPDEmbeddedFile(defaultName, spec.getFileMac(), spec.getEmbeddedFileMac(), extractor);
        extractPDEmbeddedFile(defaultName, spec.getFileDos(), spec.getEmbeddedFileDos(), extractor);
        extractPDEmbeddedFile(defaultName, spec.getFileUnix(), spec.getEmbeddedFileUnix(), extractor);
    }

    private void extractPDEmbeddedFile(String defaultName, String fileName, PDEmbeddedFile file,
                              EmbeddedDocumentExtractor extractor)
            throws SAXException, IOException, TikaException{

        if (file == null) {
            //skip silently
            return;
        }

        fileName = (fileName == null) ? defaultName : fileName;

        // TODO: other metadata?
        Metadata metadata = new Metadata();
        metadata.set(Metadata.RESOURCE_NAME_KEY, fileName);
        metadata.set(Metadata.CONTENT_TYPE, file.getSubtype());
        metadata.set(Metadata.CONTENT_LENGTH, Long.toString(file.getSize()));
        metadata.set(TikaCoreProperties.EMBEDDED_RESOURCE_TYPE,
                TikaCoreProperties.EmbeddedResourceType.ATTACHMENT.toString());

        if (extractor.shouldParseEmbedded(metadata)) {
            TikaInputStream stream = null;
            try{
                stream = TikaInputStream.get(file.createInputStream());
                extractor.parseEmbedded(
                        stream,
                        new EmbeddedContentHandler(handler),
                        metadata, false);

                AttributesImpl attributes = new AttributesImpl();
                attributes.addAttribute("", "class", "class", "CDATA", "embedded");
                attributes.addAttribute("", "id", "id", "CDATA", fileName);
                handler.startElement("div", attributes);
                handler.endElement("div");
            } finally {
                IOUtils.closeQuietly(stream);
            }
        }
    }

    private void extractAcroForm(PDDocument pdf, XHTMLContentHandler handler) throws IOException, 
    SAXException {
        //Thank you, Ben Litchfield, for org.apache.pdfbox.examples.fdf.PrintFields
        //this code derives from Ben's code
        PDDocumentCatalog catalog = pdf.getDocumentCatalog();

        if (catalog == null)
            return;

        PDAcroForm form = catalog.getAcroForm();
        if (form == null)
            return;

        @SuppressWarnings("rawtypes")
        List fields = form.getFields();

        if (fields == null)
            return;

        @SuppressWarnings("rawtypes")
        ListIterator itr  = fields.listIterator();

        if (itr == null)
            return;

        handler.startElement("div", "class", "acroform");
        handler.startElement("ol");

        while (itr.hasNext()) {
            Object obj = itr.next();
            if (obj != null && obj instanceof PDField) {
                processAcroField((PDField)obj, handler, 0);
            }
        }
        handler.endElement("ol");
        handler.endElement("div");
    }

    private void processAcroField(PDField field, XHTMLContentHandler handler, final int currentRecursiveDepth)
            throws SAXException, IOException { 

        if (currentRecursiveDepth >= MAX_ACROFORM_RECURSIONS) {
            return;
        }

        addFieldString(field, handler);

        List<COSObjectable> kids = field.getKids();
        if(kids != null) {

            int r = currentRecursiveDepth+1;
            handler.startElement("ol");
            //TODO: can generate <ol/>. Rework to avoid that.
            for(COSObjectable pdfObj : kids) {
                if(pdfObj != null && pdfObj instanceof PDField) {
                    PDField kid = (PDField)pdfObj;
                    //recurse
                    processAcroField(kid, handler, r);
                }
            }
            handler.endElement("ol");
        }
    }

    private void addFieldString(PDField field, XHTMLContentHandler handler) throws SAXException {
        //Pick partial name to present in content and altName for attribute
        //Ignoring FullyQualifiedName for now
        String partName = field.getPartialName();
        String altName = field.getAlternateFieldName();

        StringBuilder sb = new StringBuilder();
        AttributesImpl attrs = new AttributesImpl();

        if (partName != null) {
            sb.append(partName).append(": ");
        }
        if (altName != null) {
            attrs.addAttribute("", "altName", "altName", "CDATA", altName);
        }
        //return early if PDSignature field
        if (field instanceof PDSignatureField) {
            handleSignature(attrs, (PDSignatureField)field, handler);
            return;
        }
        try {
            //getValue can throw an IOException if there is no value
            String value = field.getValue();
            if (value != null && ! value.equals("null")) {
                sb.append(value);
            }
        } catch (IOException e) {
            //swallow
        } catch (NullPointerException e) {
            //TODO: remove once PDFBOX-2161 is fixed
        }

        if (attrs.getLength() > 0 || sb.length() > 0) {
            handler.startElement("li", attrs);
            handler.characters(sb.toString());
            handler.endElement("li");
        }
    }

    private void handleSignature(AttributesImpl parentAttributes, PDSignatureField sigField,
            XHTMLContentHandler handler) throws SAXException {


        PDSignature sig = sigField.getSignature();
        if (sig == null) {
            return;
        }
        Map<String, String> vals= new TreeMap<String, String>();
        vals.put("name", sig.getName());
        vals.put("contactInfo", sig.getContactInfo());
        vals.put("location", sig.getLocation());
        vals.put("reason", sig.getReason());

        Calendar cal = sig.getSignDate();
        if (cal != null) {
            dateFormat.setTimeZone(cal.getTimeZone());
            vals.put("date", dateFormat.format(cal.getTime()));
        }
        //see if there is any data
        int nonNull = 0;
        for (String val : vals.keySet()) {
            if (val != null && ! val.equals("")) {
                nonNull++;
            }
        }
        //if there is, process it
        if (nonNull > 0) {
            handler.startElement("li", parentAttributes);

            AttributesImpl attrs = new AttributesImpl();
            attrs.addAttribute("", "type", "type", "CDATA", "signaturedata");

            handler.startElement("ol", attrs);
            for (Map.Entry<String, String> e : vals.entrySet()) {
                if (e.getValue() == null || e.getValue().equals("")) {
                    continue;
                }
                attrs = new AttributesImpl();
                attrs.addAttribute("", "signdata", "signdata", "CDATA", e.getKey());
                handler.startElement("li", attrs);
                handler.characters(e.getValue());
                handler.endElement("li");
            }
            handler.endElement("ol");
            handler.endElement("li");
        }
    }
}

"
tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDFParser.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.pdf;

import java.io.IOException;
import java.io.InputStream;
import java.util.Arrays;
import java.util.Calendar;
import java.util.Collections;
import java.util.List;
import java.util.Locale;
import java.util.Set;

import org.apache.jempbox.xmp.XMPSchema;
import org.apache.jempbox.xmp.XMPSchemaDublinCore;
import org.apache.jempbox.xmp.pdfa.XMPSchemaPDFAId;
import org.apache.pdfbox.cos.COSArray;
import org.apache.pdfbox.cos.COSBase;
import org.apache.pdfbox.cos.COSDictionary;
import org.apache.pdfbox.cos.COSName;
import org.apache.pdfbox.cos.COSString;
import org.apache.pdfbox.io.RandomAccess;
import org.apache.pdfbox.io.RandomAccessBuffer;
import org.apache.pdfbox.io.RandomAccessFile;
import org.apache.pdfbox.pdmodel.font.PDFont;
import org.apache.pdfbox.pdmodel.PDDocument;
import org.apache.pdfbox.pdmodel.PDDocumentInformation;
import org.apache.tika.exception.TikaException;
import org.apache.tika.extractor.EmbeddedDocumentExtractor;
import org.apache.tika.io.CloseShieldInputStream;
import org.apache.tika.io.TemporaryResources;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.PagedText;
import org.apache.tika.metadata.Property;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.PasswordProvider;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * PDF parser.
 * <p>
 * This parser can process also encrypted PDF documents if the required
 * password is given as a part of the input metadata associated with a
 * document. If no password is given, then this parser will try decrypting
 * the document using the empty password that's often used with PDFs. If
 * the PDF contains any embedded documents (for example as part of a PDF
 * package) then this parser will use the {@link EmbeddedDocumentExtractor}
 * to handle them.
 * <p>
 * As of Tika 1.6, it is possible to extract inline images with
 * the {@link EmbeddedDocumentExtractor} as if they were regular
 * attachments.  By default, this feature is turned off because of
 * the potentially enormous number and size of inline images.  To
 * turn this feature on, see
 * {@link PDFParserConfig#setExtractInlineImages(boolean)}.
 */
public class PDFParser extends AbstractParser {


    private static final MediaType MEDIA_TYPE = MediaType.application("pdf");

    /** Serial version UID */
    private static final long serialVersionUID = -752276948656079347L;

    private PDFParserConfig defaultConfig = new PDFParserConfig();
    /**
     * Metadata key for giving the document password to the parser.
     *
     * @since Apache Tika 0.5
     * @deprecated Supply a {@link PasswordProvider} on the {@link ParseContext} instead
     */
    public static final String PASSWORD = "org.apache.tika.parser.pdf.password";

    private static final Set<MediaType> SUPPORTED_TYPES =
        Collections.singleton(MEDIA_TYPE);

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }

    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
       
        PDDocument pdfDocument = null;
        TemporaryResources tmp = new TemporaryResources();
        //config from context, or default if not set via context
        PDFParserConfig localConfig = context.get(PDFParserConfig.class, defaultConfig);
        String password = "";
        try {
            // PDFBox can process entirely in memory, or can use a temp file
            //  for unpacked / processed resources
            // Decide which to do based on if we're reading from a file or not already
            TikaInputStream tstream = TikaInputStream.cast(stream);
            password = getPassword(metadata, context);
            if (tstream != null && tstream.hasFile()) {
                // File based, take that as a cue to use a temporary file
                RandomAccess scratchFile = new RandomAccessFile(tmp.createTemporaryFile(), "rw");
                if (localConfig.getUseNonSequentialParser() == true) {
                    pdfDocument = PDDocument.loadNonSeq(new CloseShieldInputStream(stream), scratchFile, password);
                } else {
                    pdfDocument = PDDocument.load(new CloseShieldInputStream(stream), scratchFile, true);
                }
            } else {
                // Go for the normal, stream based in-memory parsing
                if (localConfig.getUseNonSequentialParser() == true) {
                    pdfDocument = PDDocument.loadNonSeq(new CloseShieldInputStream(stream), new RandomAccessBuffer(), password);
                } else {
                    pdfDocument = PDDocument.load(new CloseShieldInputStream(stream), true);
                }
            }
            metadata.set("pdf:encrypted", Boolean.toString(pdfDocument.isEncrypted()));

            //if using the classic parser and the doc is encrypted, we must manually decrypt
            if (! localConfig.getUseNonSequentialParser() && pdfDocument.isEncrypted()) {
                try {
                    pdfDocument.decrypt(password);
                } catch (Exception e) {
                    // Ignore
                }
            }

            metadata.set(Metadata.CONTENT_TYPE, "application/pdf");
            extractMetadata(pdfDocument, metadata);
            if (handler != null) {
                PDF2XHTML.process(pdfDocument, handler, context, metadata, localConfig);
            }
            
        } finally {
            if (pdfDocument != null) {
               pdfDocument.close();
            }
            tmp.dispose();
            //TODO: once we migrate to PDFBox 2.0, remove this (PDFBOX-2200)
            PDFont.clearResources();
        }
    }

    private String getPassword(Metadata metadata, ParseContext context) {
        String password = null;

        // Did they supply a new style Password Provider?
        PasswordProvider passwordProvider = context.get(PasswordProvider.class);
        if (passwordProvider != null) {
            password = passwordProvider.getPassword(metadata);
        }

        // Fall back on the old style metadata if set
        if (password == null && metadata.get(PASSWORD) != null) {
            password = metadata.get(PASSWORD);
        }

        // If no password is given, use an empty string as the default
        if (password == null) {
            password = "";
        }
        return password;
    }


    private void extractMetadata(PDDocument document, Metadata metadata)
            throws TikaException {

        org.apache.jempbox.xmp.XMPMetadata xmp = null;
        XMPSchemaDublinCore dcSchema = null;
        try{
            if (document.getDocumentCatalog().getMetadata() != null) {
                xmp = document.getDocumentCatalog().getMetadata().exportXMPMetadata();
            }
            if (xmp != null) {
                dcSchema = xmp.getDublinCoreSchema();
            }
        } catch (IOException e) {
            //swallow
        }
        PDDocumentInformation info = document.getDocumentInformation();
        metadata.set(PagedText.N_PAGES, document.getNumberOfPages());
        extractMultilingualItems(metadata, TikaCoreProperties.TITLE, info.getTitle(), dcSchema);
        extractDublinCoreListItems(metadata, TikaCoreProperties.CREATOR, info.getAuthor(), dcSchema);
        extractDublinCoreListItems(metadata, TikaCoreProperties.CONTRIBUTOR, null, dcSchema);
        addMetadata(metadata, TikaCoreProperties.CREATOR_TOOL, info.getCreator());
        addMetadata(metadata, TikaCoreProperties.KEYWORDS, info.getKeywords());
        addMetadata(metadata, "producer", info.getProducer());
        extractMultilingualItems(metadata, TikaCoreProperties.DESCRIPTION, null, dcSchema);

        // TODO: Move to description in Tika 2.0
        addMetadata(metadata, TikaCoreProperties.TRANSITION_SUBJECT_TO_OO_SUBJECT, info.getSubject());
        addMetadata(metadata, "trapped", info.getTrapped());
        try {
            // TODO Remove these in Tika 2.0
            addMetadata(metadata, "created", info.getCreationDate());
            addMetadata(metadata, TikaCoreProperties.CREATED, info.getCreationDate());
        } catch (IOException e) {
            // Invalid date format, just ignore
        }
        try {
            Calendar modified = info.getModificationDate();
            addMetadata(metadata, Metadata.LAST_MODIFIED, modified);
            addMetadata(metadata, TikaCoreProperties.MODIFIED, modified);
        } catch (IOException e) {
            // Invalid date format, just ignore
        }
        
        // All remaining metadata is custom
        // Copy this over as-is
        List<String> handledMetadata = Arrays.asList("Author", "Creator", "CreationDate", "ModDate",
                "Keywords", "Producer", "Subject", "Title", "Trapped");
        for(COSName key : info.getDictionary().keySet()) {
            String name = key.getName();
            if(! handledMetadata.contains(name)) {
        	addMetadata(metadata, name, info.getDictionary().getDictionaryObject(key));
            }
        }

        //try to get the various versions
        //Caveats:
        //    there is currently a fair amount of redundancy
        //    TikaCoreProperties.FORMAT can be multivalued
        //    There are also three potential pdf specific version keys: pdf:PDFVersion, pdfa:PDFVersion, pdf:PDFExtensionVersion        
        metadata.set("pdf:PDFVersion", Float.toString(document.getDocument().getVersion()));
        metadata.add(TikaCoreProperties.FORMAT.getName(), 
            MEDIA_TYPE.toString()+"; version="+
            Float.toString(document.getDocument().getVersion()));

        try {           
            if( xmp != null ) {
                xmp.addXMLNSMapping(XMPSchemaPDFAId.NAMESPACE, XMPSchemaPDFAId.class);
                XMPSchemaPDFAId pdfaxmp = (XMPSchemaPDFAId) xmp.getSchemaByClass(XMPSchemaPDFAId.class);
                if( pdfaxmp != null ) {
                    metadata.set("pdfaid:part", Integer.toString(pdfaxmp.getPart()));
                    if (pdfaxmp.getConformance() != null) {
                        metadata.set("pdfaid:conformance", pdfaxmp.getConformance());
                        String version = "A-"+pdfaxmp.getPart()+pdfaxmp.getConformance().toLowerCase(Locale.ROOT);
                        metadata.set("pdfa:PDFVersion", version );
                        metadata.add(TikaCoreProperties.FORMAT.getName(), 
                            MEDIA_TYPE.toString()+"; version=\""+version+"\"" );
                    }
                } 
                // TODO WARN if this XMP version is inconsistent with document header version?          
            }
        } catch (IOException e) {
            metadata.set(TikaCoreProperties.TIKA_META_PREFIX+"pdf:metadata-xmp-parse-failed", ""+e);
        }
        //TODO: Let's try to move this into PDFBox.
        //Attempt to determine Adobe extension level, if present:
        COSDictionary root = document.getDocumentCatalog().getCOSDictionary();
        COSDictionary extensions = (COSDictionary) root.getDictionaryObject(COSName.getPDFName("Extensions") );
        if( extensions != null ) {
            for( COSName extName : extensions.keySet() ) {
                // If it's an Adobe one, interpret it to determine the extension level:
                if( extName.equals( COSName.getPDFName("ADBE") )) {
                    COSDictionary adobeExt = (COSDictionary) extensions.getDictionaryObject(extName);
                    if( adobeExt != null ) {
                        String baseVersion = adobeExt.getNameAsString(COSName.getPDFName("BaseVersion"));
                        int el = adobeExt.getInt(COSName.getPDFName("ExtensionLevel"));
                        //-1 is sentinel value that something went wrong in getInt
                        if (el != -1) {
                            metadata.set("pdf:PDFExtensionVersion", baseVersion+" Adobe Extension Level "+el );
                            metadata.add(TikaCoreProperties.FORMAT.getName(), 
                                MEDIA_TYPE.toString()+"; version=\""+baseVersion+" Adobe Extension Level "+el+"\"");
                        }
                    }                   
                } else {
                    // WARN that there is an Extension, but it's not Adobe's, and so is a 'new' format'.
                    metadata.set("pdf:foundNonAdobeExtensionName", extName.getName());
                }
            }
        }
    }

   /**
     * Try to extract all multilingual items from the XMPSchema
     * <p>
     * This relies on the property having a valid xmp getName()
     * <p>
     * For now, this only extracts the first language if the property does not allow multiple values (see TIKA-1295)
     * @param metadata
     * @param property
     * @param pdfBoxBaseline
     * @param schema
     */
    private void extractMultilingualItems(Metadata metadata, Property property,
            String pdfBoxBaseline, XMPSchema schema) {
        //if schema is null, just go with pdfBoxBaseline
        if (schema == null) {
            if (pdfBoxBaseline != null && pdfBoxBaseline.length() > 0) {
                metadata.set(property, pdfBoxBaseline);
            }
            return;
        }

        for (String lang : schema.getLanguagePropertyLanguages(property.getName())) {
            String value = schema.getLanguageProperty(property.getName(), lang);

            if (value != null && value.length() > 0) {
                //if you're going to add it below in the baseline addition, don't add it now
                if (pdfBoxBaseline != null && value.equals(pdfBoxBaseline)){
                    continue;
                }
                metadata.add(property, value); 
                if (! property.isMultiValuePermitted()){
                    return;
                }
            }
        }

        if (pdfBoxBaseline != null && pdfBoxBaseline.length() > 0) {
            //if we've already added something above and multivalue is not permitted
            //return.
            if (! property.isMultiValuePermitted()){
                if (metadata.get(property) != null){
                    return;
                }
            }
            metadata.add(property,  pdfBoxBaseline);
        }
    }


    /**
     * This tries to read a list from a particular property in
     * XMPSchemaDublinCore.
     * If it can't find the information, it falls back to the 
     * pdfboxBaseline.  The pdfboxBaseline should be the value
     * that pdfbox returns from its PDDocumentInformation object
     * (e.g. getAuthor()) This method is designed include the pdfboxBaseline,
     * and it should not duplicate the pdfboxBaseline.
     * <p>
     * Until PDFBOX-1803/TIKA-1233 are fixed, do not call this
     * on dates!
     * <p>
     * This relies on the property having a DublinCore compliant getName()
     * 
     * @param property
     * @param pdfBoxBaseline
     * @param dc
     * @param metadata
     */
    private void extractDublinCoreListItems(Metadata metadata, Property property, 
            String pdfBoxBaseline, XMPSchemaDublinCore dc) {
        //if no dc, add baseline and return
        if (dc == null) {
            if (pdfBoxBaseline != null && pdfBoxBaseline.length() > 0) {
                addMetadata(metadata, property, pdfBoxBaseline);
            }
            return;
        }
        List<String> items = getXMPBagOrSeqList(dc, property.getName());
        if (items == null) {
            if (pdfBoxBaseline != null && pdfBoxBaseline.length() > 0) {
                addMetadata(metadata, property, pdfBoxBaseline);
            }
            return;
        }
        for (String item : items) {
            if (pdfBoxBaseline != null && ! item.equals(pdfBoxBaseline)) {
                addMetadata(metadata, property, item);
            }
        }
        //finally, add the baseline
        if (pdfBoxBaseline != null && pdfBoxBaseline.length() > 0) {
            addMetadata(metadata, property, pdfBoxBaseline);
        }    
    }

    /**
     * As of this writing, XMPSchema can contain bags or sequence lists
     * for some attributes...despite standards documentation.  
     * JempBox expects one or the other for specific attributes.
     * Until more flexibility is added to JempBox, Tika will have to handle both.
     * 
     * @param schema
     * @param name
     * @return list of values or null
     */
    private List<String> getXMPBagOrSeqList(XMPSchema schema, String name) {
        List<String> ret = schema.getBagList(name);
        if (ret == null) {
            ret = schema.getSequenceList(name);
        }
        return ret;
    }

    private void addMetadata(Metadata metadata, Property property, String value) {
        if (value != null) {
            metadata.add(property, value);
        }
    }
    
    private void addMetadata(Metadata metadata, String name, String value) {
        if (value != null) {
            metadata.add(name, value);
        }
    }

    private void addMetadata(Metadata metadata, String name, Calendar value) {
        if (value != null) {
            metadata.set(name, value.getTime().toString());
        }
    }

    private void addMetadata(Metadata metadata, Property property, Calendar value) {
        if (value != null) {
            metadata.set(property, value.getTime());
        }
    }

    /**
     * Used when processing custom metadata entries, as PDFBox won't do
     *  the conversion for us in the way it does for the standard ones
     */
    private void addMetadata(Metadata metadata, String name, COSBase value) {
        if(value instanceof COSArray) {
            for(Object v : ((COSArray)value).toList()) {
                addMetadata(metadata, name, ((COSBase) v));
            }
        } else if(value instanceof COSString) {
            addMetadata(metadata, name, ((COSString)value).getString());
        } else if (value != null) {
            addMetadata(metadata, name, value.toString());
        }
    }

    public void setPDFParserConfig(PDFParserConfig config) {
        this.defaultConfig = config;
    }
    
    public PDFParserConfig getPDFParserConfig() {
        return defaultConfig;
    }
    
    /**
     * If true, the parser will use the NonSequentialParser.  This may
     * be faster than the full doc parser.
     * If false (default), this will use the full doc parser.
     * 
     * @deprecated use {@link #setPDFParserConfig(PDFParserConfig)}
     */
    public void setUseNonSequentialParser(boolean v) {
        defaultConfig.setUseNonSequentialParser(v);
    }
    
    /** 
     * @see #setUseNonSequentialParser(boolean) 
     * @deprecated use {@link #getPDFParserConfig()}
     */
    public boolean getUseNonSequentialParser() {
        return defaultConfig.getUseNonSequentialParser();
    }
    
    /**
     *  If true (the default), the parser should estimate
     *  where spaces should be inserted between words.  For
     *  many PDFs this is necessary as they do not include
     *  explicit whitespace characters.
     *
     *  @deprecated use {@link #setPDFParserConfig(PDFParserConfig)}
     */
    public void setEnableAutoSpace(boolean v) {
        defaultConfig.setEnableAutoSpace(v);
    }

    /** 
     * @see #setEnableAutoSpace(boolean) 
     * @deprecated use {@link #getPDFParserConfig()}
     */
    public boolean getEnableAutoSpace() {
        return defaultConfig.getEnableAutoSpace();
    }

    /**
     * If true (the default), text in annotations will be
     * extracted.
     * @deprecated use {@link #setPDFParserConfig(PDFParserConfig)}
     */
    public void setExtractAnnotationText(boolean v) {
        defaultConfig.setExtractAnnotationText(v);
    }

    /**
     * If true, text in annotations will be extracted.
     * 
     * @deprecated use {@link #getPDFParserConfig()}
     */
    public boolean getExtractAnnotationText() {
        return defaultConfig.getExtractAnnotationText();
    }

    /**
     *  If true, the parser should try to remove duplicated
     *  text over the same region.  This is needed for some
     *  PDFs that achieve bolding by re-writing the same
     *  text in the same area.  Note that this can
     *  slow down extraction substantially (PDFBOX-956) and
     *  sometimes remove characters that were not in fact
     *  duplicated (PDFBOX-1155).  By default this is disabled.
     *  
     *  @deprecated use {@link #setPDFParserConfig(PDFParserConfig)}
     */
    public void setSuppressDuplicateOverlappingText(boolean v) {
        defaultConfig.setSuppressDuplicateOverlappingText(v);
    }

    /** 
     * @see #setSuppressDuplicateOverlappingText(boolean) 
     * 
     * @deprecated use {@link #getPDFParserConfig()}
     */
    public boolean getSuppressDuplicateOverlappingText() {
        return defaultConfig.getSuppressDuplicateOverlappingText();
    }

    /**
     *  If true, sort text tokens by their x/y position
     *  before extracting text.  This may be necessary for
     *  some PDFs (if the text tokens are not rendered "in
     *  order"), while for other PDFs it can produce the
     *  wrong result (for example if there are 2 columns,
     *  the text will be interleaved).  Default is false.
     *  
     *  @deprecated use {@link #setPDFParserConfig(PDFParserConfig)}
     */
    public void setSortByPosition(boolean v) {
        defaultConfig.setSortByPosition(v);
    }

    /** 
     * @see #setSortByPosition(boolean) 
     * 
     * @deprecated use {@link #getPDFParserConfig()}
     */
    public boolean getSortByPosition() {
        return defaultConfig.getSortByPosition();
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDFParserConfig.java,true,"package org.apache.tika.parser.pdf;

/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.IOException;
import java.io.InputStream;
import java.io.Serializable;
import java.util.Locale;
import java.util.Properties;

import org.apache.pdfbox.util.PDFTextStripper;

/**
 * Config for PDFParser.
 * 
 * This allows parameters to be set programmatically:
 * <ol>
 * <li>Calls to PDFParser, i.e. parser.getPDFParserConfig().setEnableAutoSpace() (as before)</li>
 * <li>Constructor of PDFParser</li>
 * <li>Passing to PDFParser through a ParseContext: context.set(PDFParserConfig.class, config);</li>
 * </ol>
 * 
 * Parameters can also be set by modifying the PDFParserConfig.properties file,
 * which lives in the expected places, in trunk:
 * tika-parsers/src/main/resources/org/apache/tika/parser/pdf
 * 
 * Or, in tika-app-x.x.jar or tika-parsers-x.x.jar:
 * org/apache/tika/parser/pdf
 *
 */
public class PDFParserConfig implements Serializable{

    private static final long serialVersionUID = 6492570218190936986L;

    // True if we let PDFBox "guess" where spaces should go:
    private boolean enableAutoSpace = true;

    // True if we let PDFBox remove duplicate overlapping text:
    private boolean suppressDuplicateOverlappingText;

    // True if we extract annotation text ourselves
    // (workaround for PDFBOX-1143):
    private boolean extractAnnotationText = true;

    // True if we should sort text tokens by position
    // (necessary for some PDFs, but messes up other PDFs):
    private boolean sortByPosition = false;

    //True if we should use PDFBox's NonSequentialParser
    private boolean useNonSequentialParser = false;
    
    //True if acroform content should be extracted
    private boolean extractAcroFormContent = true;

    //True if inline PDXImage objects should be extracted
    private boolean extractInlineImages = false;

    //True if inline images (as identified by their object id within
    //a pdf file) should only be extracted once.
    private boolean extractUniqueInlineImagesOnly = true;
    
    //The character width-based tolerance value used to estimate where spaces in text should be added
    private Float averageCharTolerance;
    
    //The space width-based tolerance value used to estimate where spaces in text should be added
    private Float spacingTolerance;

    public PDFParserConfig() {
        init(this.getClass().getResourceAsStream("PDFParser.properties"));
    }

    /**
     * Loads properties from InputStream and then tries to close InputStream.
     * If there is an IOException, this silently swallows the exception
     * and goes back to the default.
     * 
     * @param is
     */
    public PDFParserConfig(InputStream is) {
        init(is);
    }

    //initializes object and then tries to close inputstream
    private void init(InputStream is) {

        if (is == null) {
            return;
        }
        Properties props = new Properties();
        try {
            props.load(is);
        } catch (IOException e) {
        } finally {
            if (is != null) {
                try{
                    is.close();
                } catch (IOException e) {
                    //swallow
                }
            }
        }
        setEnableAutoSpace(
                getProp(props.getProperty("enableAutoSpace"), getEnableAutoSpace()));
        setSuppressDuplicateOverlappingText(
                getProp(props.getProperty("suppressDuplicateOverlappingText"), 
                        getSuppressDuplicateOverlappingText()));
        setExtractAnnotationText(
                getProp(props.getProperty("extractAnnotationText"), 
                        getExtractAnnotationText()));
        setSortByPosition(
                getProp(props.getProperty("sortByPosition"), 
                        getSortByPosition()));
        setUseNonSequentialParser(
                getProp(props.getProperty("useNonSequentialParser"), 
                        getUseNonSequentialParser()));
        setExtractAcroFormContent(
                getProp(props.getProperty("extractAcroFormContent"),
                getExtractAcroFormContent()));
        setExtractInlineImages(
                getProp(props.getProperty("extractInlineImages"),
                getExtractInlineImages()));
        setExtractUniqueInlineImagesOnly(
                getProp(props.getProperty("extractUniqueInlineImagesOnly"),
                getExtractUniqueInlineImagesOnly()));
    }
    
    /**
     * Configures the given pdf2XHTML.
     * 
     * @param pdf2XHTML
     */
    public void configure(PDF2XHTML pdf2XHTML) {
        pdf2XHTML.setForceParsing(true);
        pdf2XHTML.setSortByPosition(getSortByPosition());
        if (getEnableAutoSpace()) {
            pdf2XHTML.setWordSeparator(" ");
        } else {
            pdf2XHTML.setWordSeparator("");
        }
        if (getAverageCharTolerance() != null) {
            pdf2XHTML.setAverageCharTolerance(getAverageCharTolerance());
        }
        if (getSpacingTolerance() != null) {
            pdf2XHTML.setSpacingTolerance(getSpacingTolerance());
        }
        pdf2XHTML.setSuppressDuplicateOverlappingText(getSuppressDuplicateOverlappingText());
    }

    
    /**
     * If true (the default), extract content from AcroForms
     * at the end of the document.
     * 
     * @param extractAcroFormContent
     */
    public void setExtractAcroFormContent(boolean extractAcroFormContent) {
        this.extractAcroFormContent = extractAcroFormContent;
        
    }

    /** @see #setExtractAcroFormContent(boolean) */
    public boolean getExtractAcroFormContent() {
        return extractAcroFormContent;
    }

    /**
     * If true, extract inline embedded OBXImages.
     * <b>Beware:</b> some PDF documents of modest size (~4MB) can contain
     * thousands of embedded images totaling > 2.5 GB.  Also, at least as of PDFBox 1.8.5, 
     * there can be surprisingly large memory consumption and/or out of memory errors.
     * Set to <code>true</code> with caution.
     * <p>
     * The default is <code>false</code>.
     * <p>
     * See also: {@see #setExtractUniqueInlineImagesOnly(boolean)};
     * 
     * @param extractInlineImages
     */
    public void setExtractInlineImages(boolean extractInlineImages) {
        this.extractInlineImages = extractInlineImages;        
    }

    /** @see #setExtractInlineImages(boolean) */
    public boolean getExtractInlineImages() {
        return extractInlineImages;
    }

    /**
     * Multiple pages within a PDF file might refer to the same underlying image.
     * If {@link #extractUniqueInlineImagesOnly} is set to <code>false</code>, the
     * parser will call the EmbeddedExtractor each time the image appears on a page.
     * This might be desired for some use cases.  However, to avoid duplication of 
     * extracted images, set this to <code>true</code>.  The default is <code>true</code>.
     * <p>
     * Note that uniqueness is determined only by the underlying PDF COSObject id, not by 
     * file hash or similar equality metric.
     * If the PDF actually contains multiple copies of the same image 
     * -- all with different object ids -- then all images will be extracted.
     * <p>
     * For this parameter to have any effect, {@link #extractInlineImages} must be 
     * set to <code>true</code>.
     * 
     * @param extractUniqueInlineImagesOnly
     */
    public void setExtractUniqueInlineImagesOnly(boolean extractUniqueInlineImagesOnly) {
        this.extractUniqueInlineImagesOnly = extractUniqueInlineImagesOnly;
        
    }

    /** @see #setExtractUniqueInlineImagesOnly(boolean) */
    public boolean getExtractUniqueInlineImagesOnly() {
        return extractUniqueInlineImagesOnly;
    }


    /** @see #setEnableAutoSpace(boolean) */
    public boolean getEnableAutoSpace() {
        return enableAutoSpace;
    }

    /**
     *  If true (the default), the parser should estimate
     *  where spaces should be inserted between words.  For
     *  many PDFs this is necessary as they do not include
     *  explicit whitespace characters.
     */
    public void setEnableAutoSpace(boolean enableAutoSpace) {
        this.enableAutoSpace = enableAutoSpace;
    }

    /** @see #setSuppressDuplicateOverlappingText(boolean)*/
    public boolean getSuppressDuplicateOverlappingText() {
        return suppressDuplicateOverlappingText;
    }

    /**
     *  If true, the parser should try to remove duplicated
     *  text over the same region.  This is needed for some
     *  PDFs that achieve bolding by re-writing the same
     *  text in the same area.  Note that this can
     *  slow down extraction substantially (PDFBOX-956) and
     *  sometimes remove characters that were not in fact
     *  duplicated (PDFBOX-1155).  By default this is disabled.
     */
    public void setSuppressDuplicateOverlappingText(
            boolean suppressDuplicateOverlappingText) {
        this.suppressDuplicateOverlappingText = suppressDuplicateOverlappingText;
    }

    /** @see #setExtractAnnotationText(boolean)*/
    public boolean getExtractAnnotationText() {
        return extractAnnotationText;
    }

    /**
     * If true (the default), text in annotations will be
     * extracted.
     */
    public void setExtractAnnotationText(boolean extractAnnotationText) {
        this.extractAnnotationText = extractAnnotationText;
    }
    /** @see #setSortByPosition(boolean)*/
    public boolean getSortByPosition() {
        return sortByPosition;
    }

    /**
     *  If true, sort text tokens by their x/y position
     *  before extracting text.  This may be necessary for
     *  some PDFs (if the text tokens are not rendered "in
     *  order"), while for other PDFs it can produce the
     *  wrong result (for example if there are 2 columns,
     *  the text will be interleaved).  Default is false.
     */
    public void setSortByPosition(boolean sortByPosition) {
        this.sortByPosition = sortByPosition;
    }

    /** @see #setUseNonSequentialParser(boolean)*/
    public boolean getUseNonSequentialParser() {
        return useNonSequentialParser;
    }

    /**
     * If true, uses PDFBox's non-sequential parser.
     * The non-sequential parser should be much faster than the traditional
     * full doc parser.  However, until PDFBOX-XXX is fixed, 
     * the non-sequential parser fails
     * to extract some document metadata.
     * <p>
     * Default is false (use the traditional parser)
     * @param useNonSequentialParser
     */
    public void setUseNonSequentialParser(boolean useNonSequentialParser) {
        this.useNonSequentialParser = useNonSequentialParser;
    }

    /** @see #setAverageCharTolerance(Float)*/
    public Float getAverageCharTolerance() {
        return averageCharTolerance;
    }

    /**
     * See {@link PDFTextStripper#setAverageCharTolerance(float)}
     */
    public void setAverageCharTolerance(Float averageCharTolerance) {
        this.averageCharTolerance = averageCharTolerance;
    }

    /** @see #setSpacingTolerance(Float)*/
    public Float getSpacingTolerance() {
        return spacingTolerance;
    }

    /**
     * See {@link PDFTextStripper#setSpacingTolerance(float)}
     */
    public void setSpacingTolerance(Float spacingTolerance) {
        this.spacingTolerance = spacingTolerance;
    }

    private boolean getProp(String p, boolean defaultMissing){
        if (p == null){
            return defaultMissing;
        }
        if (p.toLowerCase(Locale.ROOT).equals("true")) {
            return true;
        } else if (p.toLowerCase(Locale.ROOT).equals("false")) {
            return false;
        } else {
            return defaultMissing;
        }
    }

    @Override
    public int hashCode() {
        final int prime = 31;
        int result = 1;
        result = prime
                * result
                + ((averageCharTolerance == null) ? 0 : averageCharTolerance
                        .hashCode());
        result = prime * result + (enableAutoSpace ? 1231 : 1237);
        result = prime * result + (extractAcroFormContent ? 1231 : 1237);
        result = prime * result + (extractAnnotationText ? 1231 : 1237);
        result = prime * result + (extractInlineImages ? 1231 : 1237);
        result = prime * result + (extractUniqueInlineImagesOnly ? 1231 : 1237);
        result = prime * result + (sortByPosition ? 1231 : 1237);
        result = prime
                * result
                + ((spacingTolerance == null) ? 0 : spacingTolerance.hashCode());
        result = prime * result
                + (suppressDuplicateOverlappingText ? 1231 : 1237);
        result = prime * result + (useNonSequentialParser ? 1231 : 1237);
        return result;
    }

    @Override
    public boolean equals(Object obj) {
        if (this == obj)
            return true;
        if (obj == null)
            return false;
        if (getClass() != obj.getClass())
            return false;
        PDFParserConfig other = (PDFParserConfig) obj;
        if (averageCharTolerance == null) {
            if (other.averageCharTolerance != null)
                return false;
        } else if (!averageCharTolerance.equals(other.averageCharTolerance))
            return false;
        if (enableAutoSpace != other.enableAutoSpace)
            return false;
        if (extractAcroFormContent != other.extractAcroFormContent)
            return false;
        if (extractAnnotationText != other.extractAnnotationText)
            return false;
        if (extractInlineImages != other.extractInlineImages)
            return false;
        if (extractUniqueInlineImagesOnly != other.extractUniqueInlineImagesOnly)
            return false;
        if (sortByPosition != other.sortByPosition)
            return false;
        if (spacingTolerance == null) {
            if (other.spacingTolerance != null)
                return false;
        } else if (!spacingTolerance.equals(other.spacingTolerance))
            return false;
        if (suppressDuplicateOverlappingText != other.suppressDuplicateOverlappingText)
            return false;
        if (useNonSequentialParser != other.useNonSequentialParser)
            return false;
        return true;
    }

    @Override
    public String toString() {
        return "PDFParserConfig [enableAutoSpace=" + enableAutoSpace
                + ", suppressDuplicateOverlappingText="
                + suppressDuplicateOverlappingText + ", extractAnnotationText="
                + extractAnnotationText + ", sortByPosition=" + sortByPosition
                + ", useNonSequentialParser=" + useNonSequentialParser
                + ", extractAcroFormContent=" + extractAcroFormContent
                + ", extractInlineImages=" + extractInlineImages
                + ", extractUniqueInlineImagesOnly="
                + extractUniqueInlineImagesOnly + ", averageCharTolerance="
                + averageCharTolerance + ", spacingTolerance="
                + spacingTolerance + "]";
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/pkg/CompressorParser.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.pkg;

import static org.apache.tika.metadata.HttpHeaders.CONTENT_TYPE;

import java.io.BufferedInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.util.Set;

import org.apache.commons.compress.compressors.CompressorException;
import org.apache.commons.compress.compressors.CompressorInputStream;
import org.apache.commons.compress.compressors.CompressorStreamFactory;
import org.apache.commons.compress.compressors.bzip2.BZip2CompressorInputStream;
import org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream;
import org.apache.commons.compress.compressors.gzip.GzipUtils;
import org.apache.commons.compress.compressors.pack200.Pack200CompressorInputStream;
import org.apache.commons.compress.compressors.xz.XZCompressorInputStream;
import org.apache.tika.exception.TikaException;
import org.apache.tika.extractor.EmbeddedDocumentExtractor;
import org.apache.tika.extractor.ParsingEmbeddedDocumentExtractor;
import org.apache.tika.io.CloseShieldInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * Parser for various compression formats.
 */
public class CompressorParser extends AbstractParser {

    /** Serial version UID */
    private static final long serialVersionUID = 2793565792967222459L;

    private static final MediaType BZIP = MediaType.application("x-bzip");
    private static final MediaType BZIP2 = MediaType.application("x-bzip2");
    private static final MediaType GZIP = MediaType.application("gzip");
    private static final MediaType GZIP_ALT = MediaType.application("x-gzip");
    private static final MediaType XZ = MediaType.application("x-xz");
    private static final MediaType PACK = MediaType.application("application/x-java-pack200");

    private static final Set<MediaType> SUPPORTED_TYPES =
            MediaType.set(BZIP, BZIP2, GZIP, GZIP_ALT, XZ, PACK);

    static MediaType getMediaType(CompressorInputStream stream) {
        // TODO Add support for the remaining CompressorInputStream formats:
        //   FramedSnappyCompressorInputStream
        //   LZMACompressorInputStream
        //   SnappyCompressorInputStream
        //   ZCompressorInputStream
        if (stream instanceof BZip2CompressorInputStream) {
            return BZIP2;
        } else if (stream instanceof GzipCompressorInputStream) {
            return GZIP;
        } else if (stream instanceof XZCompressorInputStream) {
            return XZ;
        } else if (stream instanceof Pack200CompressorInputStream) {
            return PACK;
        } else {
            return MediaType.OCTET_STREAM;
        }
    }

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }

    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        // At the end we want to close the compression stream to release
        // any associated resources, but the underlying document stream
        // should not be closed
        stream = new CloseShieldInputStream(stream);

        // Ensure that the stream supports the mark feature
        stream = new BufferedInputStream(stream);

        CompressorInputStream cis;
        try {
            CompressorStreamFactory factory = new CompressorStreamFactory();
            CompressorParserOptions options =
                 context.get(CompressorParserOptions.class, new CompressorParserOptions() {
                     public boolean decompressConcatenated(Metadata metadata) {
                         return false;
                     }
                 });
            factory.setDecompressConcatenated(options.decompressConcatenated(metadata));
            cis = factory.createCompressorInputStream(stream);
        } catch (CompressorException e) {
            throw new TikaException("Unable to uncompress document stream", e);
        }

        MediaType type = getMediaType(cis);
        if (!type.equals(MediaType.OCTET_STREAM)) {
            metadata.set(CONTENT_TYPE, type.toString());
        }

        XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);
        xhtml.startDocument();

        try {
            Metadata entrydata = new Metadata();
            String name = metadata.get(Metadata.RESOURCE_NAME_KEY);
            if (name != null) {
                if (name.endsWith(".tbz")) {
                    name = name.substring(0, name.length() - 4) + ".tar";
                } else if (name.endsWith(".tbz2")) {
                    name = name.substring(0, name.length() - 5) + ".tar";
                } else if (name.endsWith(".bz")) {
                    name = name.substring(0, name.length() - 3);
                } else if (name.endsWith(".bz2")) {
                    name = name.substring(0, name.length() - 4);
                } else if (name.endsWith(".xz")) {
                    name = name.substring(0, name.length() - 3);
                } else if (name.endsWith(".pack")) {
                    name = name.substring(0, name.length() - 5);
                } else if (name.length() > 0) {
                    name = GzipUtils.getUncompressedFilename(name);
                }
                entrydata.set(Metadata.RESOURCE_NAME_KEY, name);
            }

            // Use the delegate parser to parse the compressed document
            EmbeddedDocumentExtractor extractor = context.get(
                    EmbeddedDocumentExtractor.class,
                    new ParsingEmbeddedDocumentExtractor(context));
            if (extractor.shouldParseEmbedded(entrydata)) {
                extractor.parseEmbedded(cis, xhtml, entrydata, true);
            }
        } finally {
            cis.close();
        }

        xhtml.endDocument();
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/pkg/CompressorParserOptions.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.pkg;

import org.apache.tika.metadata.Metadata;

/**
 * Interface for setting options for the {@link CompressorParser} by passing
 * via the {@link ParseContext}.
 */
public interface CompressorParserOptions {

    /**
     * @param metadata document metadata
     * @return whether to decompress concatenated streams or not
     */
    boolean decompressConcatenated(Metadata metadata);
}
"
tika-parsers/src/main/java/org/apache/tika/parser/pkg/PackageParser.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.pkg;

import java.io.BufferedInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.util.Set;

import org.apache.commons.compress.archivers.ArchiveEntry;
import org.apache.commons.compress.archivers.ArchiveException;
import org.apache.commons.compress.archivers.ArchiveInputStream;
import org.apache.commons.compress.archivers.ArchiveStreamFactory;
import org.apache.commons.compress.archivers.StreamingNotSupportedException;
import org.apache.commons.compress.archivers.ar.ArArchiveInputStream;
import org.apache.commons.compress.archivers.cpio.CpioArchiveInputStream;
import org.apache.commons.compress.archivers.dump.DumpArchiveInputStream;
import org.apache.commons.compress.archivers.jar.JarArchiveInputStream;
import org.apache.commons.compress.archivers.sevenz.SevenZFile;
import org.apache.commons.compress.archivers.tar.TarArchiveInputStream;
import org.apache.commons.compress.archivers.zip.ZipArchiveInputStream;
import org.apache.tika.exception.TikaException;
import org.apache.tika.extractor.EmbeddedDocumentExtractor;
import org.apache.tika.extractor.ParsingEmbeddedDocumentExtractor;
import org.apache.tika.io.CloseShieldInputStream;
import org.apache.tika.io.TemporaryResources;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.AttributesImpl;

import static org.apache.tika.metadata.HttpHeaders.CONTENT_TYPE;

/**
 * Parser for various packaging formats. Package entries will be written to
 * the XHTML event stream as &lt;div class="package-entry"&gt; elements that
 * contain the (optional) entry name as a &lt;h1&gt; element and the full
 * structured body content of the parsed entry.
 */
public class PackageParser extends AbstractParser {

    /** Serial version UID */
    private static final long serialVersionUID = -5331043266963888708L;

    private static final MediaType ZIP = MediaType.APPLICATION_ZIP;
    private static final MediaType JAR = MediaType.application("java-archive");
    private static final MediaType AR = MediaType.application("x-archive");
    private static final MediaType CPIO = MediaType.application("x-cpio");
    private static final MediaType DUMP = MediaType.application("x-tika-unix-dump");
    private static final MediaType TAR = MediaType.application("x-tar");
    private static final MediaType SEVENZ = MediaType.application("x-7z-compressed");

    private static final Set<MediaType> SUPPORTED_TYPES =
            MediaType.set(ZIP, JAR, AR, CPIO, DUMP, TAR, SEVENZ);

    static MediaType getMediaType(ArchiveInputStream stream) {
        if (stream instanceof JarArchiveInputStream) {
            return JAR;
        } else if (stream instanceof ZipArchiveInputStream) {
            return ZIP;
        } else if (stream instanceof ArArchiveInputStream) {
            return AR;
        } else if (stream instanceof CpioArchiveInputStream) {
            return CPIO;
        } else if (stream instanceof DumpArchiveInputStream) {
            return DUMP;
        } else if (stream instanceof TarArchiveInputStream) {
            return TAR;
        } else if (stream instanceof SevenZWrapper) {
            return SEVENZ;
        } else {
            return MediaType.OCTET_STREAM;
        }
    }

    static boolean isZipArchive(MediaType type) {
        return type.equals(ZIP) || type.equals(JAR);
    }

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }

    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
       
        // Ensure that the stream supports the mark feature
        if (! TikaInputStream.isTikaInputStream(stream))
            stream = new BufferedInputStream(stream);
        
        
        TemporaryResources tmp = new TemporaryResources();
        ArchiveInputStream ais = null;
        try {
            ArchiveStreamFactory factory = context.get(ArchiveStreamFactory.class, new ArchiveStreamFactory());
            // At the end we want to close the archive stream to release
            // any associated resources, but the underlying document stream
            // should not be closed
            ais = factory.createArchiveInputStream(new CloseShieldInputStream(stream));
            
        } catch (StreamingNotSupportedException sne) {
            // Most archive formats work on streams, but a few need files
            if (sne.getFormat().equals(ArchiveStreamFactory.SEVEN_Z)) {
                // Rework as a file, and wrap
                stream.reset();
                TikaInputStream tstream = TikaInputStream.get(stream, tmp);
                
                // Pending a fix for COMPRESS-269, this bit is a little nasty
                ais = new SevenZWrapper(new SevenZFile(tstream.getFile()));
                
            } else {
                tmp.close();
                throw new TikaException("Unknown non-streaming format " + sne.getFormat(), sne);
            }
        } catch (ArchiveException e) {
            tmp.close();
            throw new TikaException("Unable to unpack document stream", e);
        }

        MediaType type = getMediaType(ais);
        if (!type.equals(MediaType.OCTET_STREAM)) {
            metadata.set(CONTENT_TYPE, type.toString());
        }

        // Use the delegate parser to parse the contained document
        EmbeddedDocumentExtractor extractor = context.get(
                EmbeddedDocumentExtractor.class,
                new ParsingEmbeddedDocumentExtractor(context));

        XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);
        xhtml.startDocument();

        try {
            ArchiveEntry entry = ais.getNextEntry();
            while (entry != null) {
                if (!entry.isDirectory()) {
                    parseEntry(ais, entry, extractor, xhtml);
                }
                entry = ais.getNextEntry();
            }
        } finally {
            ais.close();
            tmp.close();
        }

        xhtml.endDocument();
    }

    private void parseEntry(
            ArchiveInputStream archive, ArchiveEntry entry,
            EmbeddedDocumentExtractor extractor, XHTMLContentHandler xhtml)
            throws SAXException, IOException, TikaException {
        String name = entry.getName();
        if (archive.canReadEntryData(entry)) {
            // Fetch the metadata on the entry contained in the archive
            Metadata entrydata = new Metadata();
            entrydata.set(TikaCoreProperties.MODIFIED, entry.getLastModifiedDate());
            entrydata.set(Metadata.CONTENT_LENGTH, Long.toString(entry.getSize()));
            if (name != null && name.length() > 0) {
                entrydata.set(Metadata.RESOURCE_NAME_KEY, name);
                AttributesImpl attributes = new AttributesImpl();
                attributes.addAttribute("", "class", "class", "CDATA", "embedded");
                attributes.addAttribute("", "id", "id", "CDATA", name);
                xhtml.startElement("div", attributes);
                xhtml.endElement("div");

                entrydata.set(Metadata.EMBEDDED_RELATIONSHIP_ID, name);
            }
            
            if (extractor.shouldParseEmbedded(entrydata)) {
                // For detectors to work, we need a mark/reset supporting
                // InputStream, which ArchiveInputStream isn't, so wrap
                TemporaryResources tmp = new TemporaryResources();
                try {
                    TikaInputStream tis = TikaInputStream.get(archive, tmp);
                    extractor.parseEmbedded(tis, xhtml, entrydata, true);
                } finally {
                    tmp.dispose();
                }
            }
        } else if (name != null && name.length() > 0) {
            xhtml.element("p", name);
        }
    }

    // Pending a fix for COMPRESS-269, we have to wrap ourselves
    private static class SevenZWrapper extends ArchiveInputStream {
        private SevenZFile file;
        private SevenZWrapper(SevenZFile file) {
            this.file = file;
        }
        
        @Override
        public int read() throws IOException {
            return file.read();
        }
        @Override
        public int read(byte[] b) throws IOException {
            return file.read(b);
        }
        @Override
        public int read(byte[] b, int off, int len) throws IOException {
            return file.read(b, off, len);
        }

        @Override
        public ArchiveEntry getNextEntry() throws IOException {
            return file.getNextEntry();
        }
        
        @Override
        public void close() throws IOException {
            file.close();
        }
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/pkg/ZipContainerDetector.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.pkg;

import java.io.ByteArrayInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.util.Enumeration;
import java.util.HashSet;
import java.util.Iterator;
import java.util.Locale;
import java.util.Set;
import java.util.regex.Pattern;

import org.apache.commons.compress.archivers.ArchiveException;
import org.apache.commons.compress.archivers.ArchiveInputStream;
import org.apache.commons.compress.archivers.ArchiveStreamFactory;
import org.apache.commons.compress.archivers.tar.TarArchiveInputStream;
import org.apache.commons.compress.archivers.zip.ZipArchiveEntry;
import org.apache.commons.compress.archivers.zip.ZipFile;
import org.apache.commons.compress.compressors.CompressorException;
import org.apache.commons.compress.compressors.CompressorInputStream;
import org.apache.commons.compress.compressors.CompressorStreamFactory;
import org.apache.poi.extractor.ExtractorFactory;
import org.apache.poi.openxml4j.exceptions.InvalidFormatException;
import org.apache.poi.openxml4j.opc.OPCPackage;
import org.apache.poi.openxml4j.opc.PackageAccess;
import org.apache.poi.openxml4j.opc.PackagePart;
import org.apache.poi.openxml4j.opc.PackageRelationshipCollection;
import org.apache.tika.detect.Detector;
import org.apache.tika.exception.TikaException;
import org.apache.tika.io.IOUtils;
import org.apache.tika.io.TemporaryResources;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.iwork.IWorkPackageParser;
import org.apache.tika.parser.iwork.IWorkPackageParser.IWORKDocumentType;

/**
 * A detector that works on Zip documents and other archive and compression
 * formats to figure out exactly what the file is.
 */
public class ZipContainerDetector implements Detector {
    private static final Pattern MACRO_TEMPLATE_PATTERN = Pattern.compile("macroenabledtemplate$", Pattern.CASE_INSENSITIVE);

    /** Serial version UID */
    private static final long serialVersionUID = 2891763938430295453L;

    public MediaType detect(InputStream input, Metadata metadata)
            throws IOException {
        // Check if we have access to the document
        if (input == null) {
            return MediaType.OCTET_STREAM;
        }

        TemporaryResources tmp = new TemporaryResources();
        try {
            TikaInputStream tis = TikaInputStream.get(input, tmp);

            byte[] prefix = new byte[1024]; // enough for all known formats
            int length = tis.peek(prefix);

            MediaType type = detectArchiveFormat(prefix, length);
            if (PackageParser.isZipArchive(type)
                    && TikaInputStream.isTikaInputStream(input)) {
                return detectZipFormat(tis);
            } else if (!type.equals(MediaType.OCTET_STREAM)) {
                return type;
            } else {
                return detectCompressorFormat(prefix, length);
            }
        } finally {
            try {
                tmp.dispose();
            } catch (TikaException e) {
                // ignore
            }
        }
    }

    private static MediaType detectCompressorFormat(byte[] prefix, int length) {
        try {
            CompressorStreamFactory factory = new CompressorStreamFactory();
            CompressorInputStream cis = factory.createCompressorInputStream(
                    new ByteArrayInputStream(prefix, 0, length));
            try {
                return CompressorParser.getMediaType(cis);
            } finally {
                IOUtils.closeQuietly(cis);
            }
        } catch (CompressorException e) {
            return MediaType.OCTET_STREAM;
        }
    }

    private static MediaType detectArchiveFormat(byte[] prefix, int length) {
        try {
            ArchiveStreamFactory factory = new ArchiveStreamFactory();
            ArchiveInputStream ais = factory.createArchiveInputStream(
                    new ByteArrayInputStream(prefix, 0, length));
            try {
                if ((ais instanceof TarArchiveInputStream)
                        && !TarArchiveInputStream.matches(prefix, length)) {
                    // ArchiveStreamFactory is too relaxed, see COMPRESS-117
                    return MediaType.OCTET_STREAM;
                } else {
                    return PackageParser.getMediaType(ais);
                }
            } finally {
                IOUtils.closeQuietly(ais);
            }
        } catch (ArchiveException e) {
            return MediaType.OCTET_STREAM;
        }
    }

    private static MediaType detectZipFormat(TikaInputStream tis) {
        try {
            ZipFile zip = new ZipFile(tis.getFile()); // TODO: hasFile()?
            try {
                MediaType type = detectOpenDocument(zip);
                if (type == null) {
                    type = detectOPCBased(zip, tis);
                }
                if (type == null) {
                    type = detectIWork(zip);
                }
                if (type == null) {
                    type = detectJar(zip);
                }
                if (type == null) {
                    type = detectKmz(zip);
                }
                if (type == null) {
                    type = detectIpa(zip);
                }
                if (type != null) {
                    return type;
                }
            } finally {
                // TODO: shouldn't we record the open
                // container so it can be later
                // reused...?
                // tis.setOpenContainer(zip);
                try {
                    zip.close();
                } catch (IOException e) {
                    // ignore
                }
            }
        } catch (IOException e) {
            // ignore
        }
        // Fallback: it's still a zip file, we just don't know what kind of one
        return MediaType.APPLICATION_ZIP;
    }

    /**
     * OpenDocument files, along with EPub files, have a mimetype
     *  entry in the root of their Zip file. This entry contains the
     *  mimetype of the overall file, stored as a single string.  
     */
    private static MediaType detectOpenDocument(ZipFile zip) {
        try {
            ZipArchiveEntry mimetype = zip.getEntry("mimetype");
            if (mimetype != null) {
                InputStream stream = zip.getInputStream(mimetype);
                try {
                    return MediaType.parse(IOUtils.toString(stream, "UTF-8"));
                } finally {
                    stream.close();
                }
            } else {
                return null;
            }
        } catch (IOException e) {
            return null;
        }
    }

    private static MediaType detectOPCBased(ZipFile zip, TikaInputStream stream) {
        try {
            if (zip.getEntry("_rels/.rels") != null
                    || zip.getEntry("[Content_Types].xml") != null) {
                // Use POI to open and investigate it for us
                OPCPackage pkg = OPCPackage.open(stream.getFile().getPath(), PackageAccess.READ);
                stream.setOpenContainer(pkg);

                // Is at an OOXML format?
                MediaType type = detectOfficeOpenXML(pkg);
                if (type != null) return type;
                
                // Is it XPS format?
                type = detectXPSOPC(pkg);
                if (type != null) return type;
                
                // Is it an AutoCAD format?
                type = detectAutoCADOPC(pkg);
                if (type != null) return type;
                
                // We don't know what it is, sorry
                return null;
            } else {
                return null;
            }
        } catch (IOException e) {
            return null;
        } catch (RuntimeException e) {
            return null;
        } catch (InvalidFormatException e) {
            return null;
        }
    }
    /**
     * Detects the type of an OfficeOpenXML (OOXML) file from
     *  opened Package 
     */
    public static MediaType detectOfficeOpenXML(OPCPackage pkg) {
        PackageRelationshipCollection core = 
           pkg.getRelationshipsByType(ExtractorFactory.CORE_DOCUMENT_REL);
        if (core.size() != 1) {
            // Invalid OOXML Package received
            return null;
        }

        // Get the type of the core document part
        PackagePart corePart = pkg.getPart(core.getRelationship(0));
        String coreType = corePart.getContentType();

        // Turn that into the type of the overall document
        String docType = coreType.substring(0, coreType.lastIndexOf('.'));

        // The Macro Enabled formats are a little special
        if(docType.toLowerCase(Locale.ROOT).endsWith("macroenabled")) {
            docType = docType.toLowerCase(Locale.ROOT) + ".12";
        }

        if(docType.toLowerCase(Locale.ROOT).endsWith("macroenabledtemplate")) {
            docType = MACRO_TEMPLATE_PATTERN.matcher(docType).replaceAll("macroenabled.12");
        }

        // Build the MediaType object and return
        return MediaType.parse(docType);
    }
    /**
     * Detects Open XML Paper Specification (XPS)
     */
    private static MediaType detectXPSOPC(OPCPackage pkg) {
        PackageRelationshipCollection xps = 
                pkg.getRelationshipsByType("http://schemas.microsoft.com/xps/2005/06/fixedrepresentation");
        if (xps.size() == 1) {
            return MediaType.application("vnd.ms-xpsdocument");
        } else {
            // Non-XPS Package received
            return null;
        }
    }
    /**
     * Detects AutoCAD formats that live in OPC packaging
     */
    private static MediaType detectAutoCADOPC(OPCPackage pkg) {
        PackageRelationshipCollection dwfxSeq = 
                pkg.getRelationshipsByType("http://schemas.autodesk.com/dwfx/2007/relationships/documentsequence");
        if (dwfxSeq.size() == 1) {
            return MediaType.parse("model/vnd.dwfx+xps");
        } else {
            // Non-AutoCAD Package received
            return null;
        }
    }

    private static MediaType detectIWork(ZipFile zip) {
        if (zip.getEntry(IWorkPackageParser.IWORK_COMMON_ENTRY) != null) {
            // Locate the appropriate index file entry, and reads from that
            // the root element of the document. That is used to the identify
            // the correct type of the keynote container.
            for (String entryName : IWorkPackageParser.IWORK_CONTENT_ENTRIES) {
               IWORKDocumentType type = IWORKDocumentType.detectType(zip.getEntry(entryName), zip); 
               if (type != null) {
                  return type.getType();
               }
            }
            
            // Not sure, fallback to the container type
            return MediaType.application("vnd.apple.iwork");
        } else {
            return null;
        }
    }
    
    private static MediaType detectJar(ZipFile zip) {
       if (zip.getEntry("META-INF/MANIFEST.MF") != null) {
          // It's a Jar file, or something based on Jar
          
          // Is it an Android APK?
          if (zip.getEntry("AndroidManifest.xml") != null) {
             return MediaType.application("vnd.android.package-archive");
          }
          
          // Check for WAR and EAR
          if (zip.getEntry("WEB-INF/") != null) {
             return MediaType.application("x-tika-java-web-archive");
          }
          if (zip.getEntry("META-INF/application.xml") != null) {
             return MediaType.application("x-tika-java-enterprise-archive");
          }
          
          // Looks like a regular Jar Archive
          return MediaType.application("java-archive");
       } else {
          // Some Android APKs miss the default Manifest
          if (zip.getEntry("AndroidManifest.xml") != null) {
             return MediaType.application("vnd.android.package-archive");
          }
          
          return null;
       }
    }

    private static MediaType detectKmz(ZipFile zip) {
        boolean kmlFound = false;

        Enumeration<ZipArchiveEntry> entries = zip.getEntries();
        while (entries.hasMoreElements()) {
            ZipArchiveEntry entry = entries.nextElement();
            String name = entry.getName();
            if (!entry.isDirectory()
                    && name.indexOf('/') == -1 && name.indexOf('\\') == -1) {
                if (name.endsWith(".kml") && !kmlFound) {
                    kmlFound = true;
                } else {
                    return null;
                }
            }
        }

        if (kmlFound) {
            return MediaType.application("vnd.google-earth.kmz");
        } else {
            return null;
        }
    }

    /**
     * To be considered as an IPA file, it needs to match all of these
     */
    private static HashSet<Pattern> ipaEntryPatterns = new HashSet<Pattern>() {
        private static final long serialVersionUID = 6545295886322115362L;
        {
           add(Pattern.compile("^Payload/$"));
           add(Pattern.compile("^Payload/.*\\.app/$"));
           add(Pattern.compile("^Payload/.*\\.app/_CodeSignature/$"));
           add(Pattern.compile("^Payload/.*\\.app/_CodeSignature/CodeResources$"));
           add(Pattern.compile("^Payload/.*\\.app/CodeResources$"));
           add(Pattern.compile("^Payload/.*\\.app/Info\\.plist$"));
           add(Pattern.compile("^Payload/.*\\.app/PkgInfo$"));
           add(Pattern.compile("^Payload/.*\\.app/ResourceRules\\.plist$"));
    }};
    @SuppressWarnings("unchecked")
    private static MediaType detectIpa(ZipFile zip) {
        // Note - consider generalising this logic, if another format needs many regexp matching
        Set<Pattern> tmpPatterns = (Set<Pattern>)ipaEntryPatterns.clone();
        
        Enumeration<ZipArchiveEntry> entries = zip.getEntries();
        while (entries.hasMoreElements()) {
            ZipArchiveEntry entry = entries.nextElement();
            String name = entry.getName();
            
            Iterator<Pattern> ip = tmpPatterns.iterator();
            while (ip.hasNext()) {
                if (ip.next().matcher(name).matches()) {
                    ip.remove();
                }
            }
            if (tmpPatterns.isEmpty()) {
                // We've found everything we need to find
                return MediaType.application("x-itunes-ipa");
            }
        }
        
        // If we get here, not all required entries were found
        return null;
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/prt/PRTParser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.prt;

import java.io.IOException;
import java.io.InputStream;
import java.io.UnsupportedEncodingException;
import java.util.Collections;
import java.util.Set;

import org.apache.poi.util.IOUtils;
import org.apache.tika.exception.TikaException;
import org.apache.tika.io.EndianUtils;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * A basic text extracting parser for the CADKey PRT (CAD Drawing)
 *  format. It outputs text from note entries.
 */

public class PRTParser extends AbstractParser {

    /** Serial version UID */
    private static final long serialVersionUID = 4659638314375035178L;

    private static final Set<MediaType> SUPPORTED_TYPES = Collections.singleton(MediaType.application("x-prt"));
    public static final String PRT_MIME_TYPE = "application/x-prt";

    public Set<MediaType> getSupportedTypes(ParseContext context) {
       return SUPPORTED_TYPES;
    }

    /**
     * How long do we allow a text run to claim to be, before we
     * decide we're confused and it's not really text after all?
     */
    private static final int MAX_SANE_TEXT_LENGTH = 0x0800;
    
    /*
     * Text types:
     *   00 00 00 00 f0 [3b]f sz sz TEXT     *view name*
     *   00 00 00 00 f0 3f 00 00 00 00 00 00 00 00 sz sz TEXT  *view name*
     *   (anything)  e0 3f sz sz TEXT    *view name*
     *   3x 33 33 33 33 33 e3 3f 0x 00 00 0x 00 00 0x 0x 1f sz sz TEXT    *note entries* 
     *   
     *  Note - all text is null terminated
     */
      
    public void parse(InputStream stream, ContentHandler handler, Metadata metadata, 
          ParseContext context) throws IOException, SAXException, TikaException {
       
       XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);
       Last5 l5 = new Last5();
       int read;
       
       // Try to get the creation date, which is YYYYMMDDhhmm
       byte[] header = new byte[30];
       IOUtils.readFully(stream, header);
       byte[] date = new byte[12];
       IOUtils.readFully(stream, date);
       
       String dateStr = new String(date, "ASCII");
       if(dateStr.startsWith("19") || dateStr.startsWith("20")) {
          String formattedDate = dateStr.substring(0, 4) + "-" + dateStr.substring(4,6) +
             "-" + dateStr.substring(6,8) + "T" + dateStr.substring(8,10) + ":" +
             dateStr.substring(10, 12) + ":00";
          metadata.set(TikaCoreProperties.CREATED, formattedDate);
          // TODO Metadata.DATE is used as modified, should it be here?
          metadata.set(Metadata.DATE, formattedDate);
       }
       metadata.set(Metadata.CONTENT_TYPE, PRT_MIME_TYPE);
       
       // The description, if set, is the next up-to-500 bytes
       byte[] desc = new byte[500];
       IOUtils.readFully(stream, desc);
       String description = extractText(desc, true);
       if(description.length() > 0) {
          metadata.set(TikaCoreProperties.DESCRIPTION, description);
       }
       
       // Now look for text
       while( (read = stream.read()) > -1) {
          if(read == 0xe0 || read == 0xe3 || read == 0xf0) {
             int nread = stream.read();
             if(nread == 0x3f || nread == 0xbf) {
                // Looks promising, check back for a suitable value
                if(read == 0xe3 && nread == 0x3f) {
                   if(l5.is33()) {
                      // Bingo, note text
                      handleNoteText(stream, xhtml);
                   }
                } else if(l5.is00()) {
                   // Likely view name
                   handleViewName(read, nread, stream, xhtml, l5);
                }
             }
          } else {
             l5.record(read);
          }
       }
    }
    
    private void handleNoteText(InputStream stream, XHTMLContentHandler xhtml) 
    throws IOException, SAXException, TikaException {
       // Ensure we have the right padding text
       int read;
       for(int i=0; i<10; i++) {
          read = stream.read();
          if(read >= 0 && read <= 0x0f) {
             // Promising
          } else {
             // Wrong, false detection
             return;
          }
       }
       read = stream.read();
       if(read != 0x1f) {
          // Wrong, false detection
          return;
       }
       
       int length = EndianUtils.readUShortLE(stream);
       if(length <= MAX_SANE_TEXT_LENGTH) {
          // Length sanity check passed
          handleText(length, stream, xhtml);
       }
    }
    
    private void handleViewName(int typeA, int typeB, InputStream stream, 
          XHTMLContentHandler xhtml, Last5 l5) 
    throws IOException, SAXException, TikaException {
       // Is it 8 byte zero padded?
       int maybeLength = EndianUtils.readUShortLE(stream);
       if(maybeLength == 0) {
          // Check the next 6 bytes too
          for(int i=0; i<6; i++) {
             int read = stream.read();
             if(read >= 0 && read <= 0x0f) {
                // Promising
             } else {
                // Wrong, false detection
                return;
             }
          }
          
          byte[] b2 = new byte[2];
          IOUtils.readFully(stream, b2);
          int length = EndianUtils.getUShortLE(b2);
          if(length > 1 && length <= MAX_SANE_TEXT_LENGTH) {
             // Length sanity check passed
             handleText(length, stream, xhtml);
          } else {
             // Was probably something else
             l5.record(b2[0]);
             l5.record(b2[1]);
          }
       } else if(maybeLength > 0 && maybeLength < MAX_SANE_TEXT_LENGTH) {
          // Looks like it's straight into the text
          handleText(maybeLength, stream, xhtml);
       }
    }
    
    private void handleText(int length, InputStream stream, XHTMLContentHandler xhtml) 
    throws IOException, SAXException, TikaException {
       byte[] str = new byte[length];
       IOUtils.readFully(stream, str);
       if(str[length-1] != 0) {
          // Not properly null terminated, must be wrong
          return;
       }
       
       String text = extractText(str, false);
       
       xhtml.startElement("p");
       xhtml.characters(text);
       xhtml.endElement("p");
    }
    
    /**
     * Does our best to turn the bytes into text
     */
    private String extractText(byte[] data, boolean trim) throws TikaException {
       // The text is always stored null terminated, but sometimes
       //  may have extra null padding too
       int length = data.length - 1;
       if(trim) {
          for(int i=0; i<data.length; i++) {
             if(data[i] == 0) {
                length = i;
                break;
             }
          }
       }
       
       // We believe that the text is basically stored as CP437
       // That said, there are a few characters slightly wrong for that...
       String text;
       try {
          text = new String(data, 0, length, "cp437");
       } catch(UnsupportedEncodingException e) {
          throw new TikaException("JVM Broken, core codepage CP437 missing!");
       }
       
       // Fix up the known character issues
       text = text.replace("\u03C6","\u00D8");

       // All done, as best as we can!
       return text;
    }
    
    /**
     * Provides a view on the previous 5 bytes
     */
    private static class Last5 {
       byte[] data = new byte[5];
       int pos = 0;
       
       private void record(int b) {
          data[pos] = (byte)b;
          pos++;
          if(pos >= data.length) {
             pos = 0;
          }
       }
       
       private byte[] get() {
          byte[] ret = new byte[5];
          for(int i=0; i<ret.length; i++) {
             int p = pos - i;
             if(p < 0) { p += ret.length; }
             ret[i] = data[p];
          }
          return ret;
       }
       
       private boolean is33() {
          byte[] last5 = get();
          for(byte b : last5) {
             if(b != 0x33) return false;
          }
          return true;
       }
       
       private boolean is00() {
          byte[] last5 = get();
          for(byte b : last5) {
             if(b != 0x00) return false;
          }
          return true;
       }
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/rtf/GroupState.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.rtf;

import java.nio.charset.Charset;

/* Holds all state associated with current RTF group, ie {
 * ... }. */

class GroupState {
    public int depth;
    public boolean bold;
    public boolean italic;
    // True if we are skipping all text in current group,
    // eg if group leads with a \*:
    public boolean ignore;
    // Default is 1 if no uc control has been seen yet:
    public int ucSkip = 1;
    public int list;
    public int listLevel;
    public Charset fontCharset;
    //in objdata
    public boolean objdata;
    //depth in pict, 1 = at pict level
    public int pictDepth;
    //in picprop key/value pair
    public boolean sp;
    //in picprop's name 
    public boolean sn;
    //in picprop's value
    public boolean sv;
    //in embedded object or not
    public boolean object;

    // Create default (root) GroupState
    public GroupState() {
    }

    // Create new GroupState, inheriting all properties from current one, adding 1 to the depth
    public GroupState(GroupState other) {
        bold = other.bold;
        italic = other.italic;
        ignore = other.ignore;
        ucSkip = other.ucSkip;
        list = other.list;
        listLevel = other.listLevel;
        fontCharset = other.fontCharset;
        depth = 1+other.depth;
        pictDepth = other.pictDepth > 0 ? other.pictDepth + 1 : 0;
        //do not inherit object, sn, sv or sp

    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/rtf/ListDescriptor.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.rtf;

/**
 * Contains the information for a single list in the list or list override tables.
 */
public class ListDescriptor {
    public final static int NUMBER_TYPE_BULLET = 23;

    public int id;
    // We record this but don't make use if it today:
    public int templateID;
    // We record this but don't make use if it today:
    public boolean isStyle;
    public int[] numberType = new int[9];

    public boolean isUnordered(int level)
    {
        return numberType[level] == NUMBER_TYPE_BULLET;
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/rtf/RTFEmbObjHandler.java,true,"package org.apache.tika.parser.rtf; 
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */ 
import java.io.ByteArrayOutputStream; 
import java.io.IOException; 
import java.io.InputStream; 
import java.util.concurrent.atomic.AtomicInteger; 
import org.apache.tika.config.TikaConfig; 
import org.apache.tika.detect.Detector; 
import org.apache.tika.exception.TikaException; 
import org.apache.tika.extractor.EmbeddedDocumentExtractor; 
import org.apache.tika.extractor.ParsingEmbeddedDocumentExtractor; 
import org.apache.tika.io.FilenameUtils; 
import org.apache.tika.io.TikaInputStream; 
import org.apache.tika.metadata.Metadata; 
import org.apache.tika.metadata.RTFMetadata; 
import org.apache.tika.mime.MediaType; 
import org.apache.tika.mime.MimeType; 
import org.apache.tika.mime.MimeTypeException; 
import org.apache.tika.mime.MimeTypes; 
import org.apache.tika.parser.ParseContext; 
import org.apache.tika.sax.EmbeddedContentHandler; 
import org.xml.sax.ContentHandler; 
import org.xml.sax.SAXException; 

/**
 * This class buffers data from embedded objects and pictures.
 *
 * <p/>
 *
 * When the parser has finished an object or picture and called
 * {@link #handleCompletedObject()}, this will write the object
 * to the {@link #handler}.
 *
 * <p/>
 *
 * This (in combination with TextExtractor) expects basically a flat parse.  It will pull out
 * all pict whether they are tied to objdata or are intended
 * to be standalone.
 *
 * <p/>
 * This tries to pull metadata around a pict that is encoded
 * with {sp {sn} {sv}} types of data.  This information
 * sometimes contains the name and even full file path of the original file.
 *
 */ class RTFEmbObjHandler {
    
    private static final String EMPTY_STRING = "";
    
    private enum EMB_STATE {
      PICT, //recording pict data
      OBJDATA, //recording objdata
      NADA
    };
    
    //high hex cached for writing hexpair chars (data)
    private int hi = -1;
    
    private int thumbCount = 0;
    //don't need atomic, do need mutable
    private AtomicInteger unknownFilenameCount = new AtomicInteger();
    
    private boolean inObject = false;
    
    private String sv = EMPTY_STRING;
    private String sn = EMPTY_STRING;
    
    private StringBuilder sb = new StringBuilder();
    
    private final ContentHandler handler;
    private Metadata metadata;
    private final ParseContext context;
    
    private final ByteArrayOutputStream os;
    private EMB_STATE state = EMB_STATE.NADA;
    
    protected RTFEmbObjHandler(ContentHandler handler, Metadata metadata, ParseContext context) {
        this.handler = handler;
        this.context = context;
        os = new ByteArrayOutputStream();
    }
    protected void startPict() {
        state = EMB_STATE.PICT;
        metadata = new Metadata();
    }
    
    protected void startObjData() {
        state = EMB_STATE.OBJDATA;
        metadata = new Metadata();
    }
    
    protected void startSN() {
        sb.setLength(0);
        sb.append(RTFMetadata.RTF_PICT_META_PREFIX);
    }
    
    protected void endSN() {
        sn = sb.toString();
    }
    
    protected void startSV() {
        sb.setLength(0);
    }
    
    protected void endSV() {
        sv = sb.toString();
    }
    
    //end metadata pair
    protected void endSP() {
        metadata.add(sn, sv);
    }
    
    protected void setInObject(boolean v) {
        inObject = v;
    }
    
    protected boolean getInObject() {
        return inObject;
    }
    
    protected void writeMetadataChar(char c) {
        sb.append(c);
    }
    
    protected void writeHexChar(int b) throws IOException, TikaException {
        //if not hexchar, ignore
        //white space is common
        if (TextExtractor.isHexChar(b)) {
            if (hi == -1) {
                hi = 16*TextExtractor.hexValue(b);
            } else {
                long sum = hi+TextExtractor.hexValue(b);
                if (sum > Integer.MAX_VALUE || sum < 0) {
                    throw new IOException("hex char to byte overflow");
                }
                
                os.write((int)sum);
                
                hi = -1;
            }
            return;
        }
        if (b == -1) {
            throw new TikaException("hit end of stream before finishing byte pair");
        }
    }
    
    
    protected void writeBytes(InputStream is, int len) throws IOException, TikaException {
        if (len < 0 || len > RTFParser.getMaxBytesForEmbeddedObject()) {
            throw new IOException("length of bytes to read out of bounds: " + len);
        }
        
        byte[] bytes = new byte[len];
        int bytesRead = is.read(bytes);
        if (bytesRead < len) {
            throw new TikaException("unexpected end of file: need " + len +
                   " bytes of binary data, found " + (len-bytesRead));
        }
        os.write(bytes);
    }
    
    /**
     * Call this when the objdata/pict has completed
     * @throws IOException
     * @throws SAXException
     * @throws TikaException
     */
    protected void handleCompletedObject() throws IOException, SAXException, TikaException {
       EmbeddedDocumentExtractor embeddedExtractor = context.get(EmbeddedDocumentExtractor.class);
       
       if (embeddedExtractor == null) {
           embeddedExtractor = new ParsingEmbeddedDocumentExtractor(context);
       }
       
       byte[] bytes = os.toByteArray();
       if (state == EMB_STATE.OBJDATA) {
           RTFObjDataParser objParser = new RTFObjDataParser();
           try{
               byte[] objBytes = objParser.parse(bytes, metadata, unknownFilenameCount);
               extractObj(objBytes, handler, embeddedExtractor, metadata);
           } catch (IOException e) {
              //swallow.  If anything goes wrong, ignore.
           }
       } else if (state == EMB_STATE.PICT) {
           String filePath = metadata.get(RTFMetadata.RTF_PICT_META_PREFIX+"wzDescription");
           if (filePath != null && filePath.length() > 0){
               metadata.set(Metadata.EMBEDDED_RELATIONSHIP_ID, filePath);
               metadata.set(Metadata.RESOURCE_NAME_KEY, FilenameUtils.getName(filePath));
           }
           metadata.set(RTFMetadata.THUMBNAIL, Boolean.toString(inObject));
           extractObj(bytes, handler, embeddedExtractor, metadata);
           
       } else if (state == EMB_STATE.NADA) {
           //swallow...no start for pict or embed?!
       }
       reset();
    }
    
    private void extractObj(byte[] bytes, ContentHandler handler,
            EmbeddedDocumentExtractor embeddedExtractor, Metadata metadata)
                    throws SAXException, IOException, TikaException {
        
        if (bytes == null) {
            return;
        }
        
        metadata.set(Metadata.CONTENT_LENGTH, Integer.toString(bytes.length));
        
        if (embeddedExtractor.shouldParseEmbedded(metadata)) {
            TikaInputStream stream = TikaInputStream.get(bytes);
            if (metadata.get(Metadata.RESOURCE_NAME_KEY) == null) {
                String extension = getExtension(stream, metadata);
                stream.reset();
                if (inObject && state == EMB_STATE.PICT) {
                    metadata.set(Metadata.RESOURCE_NAME_KEY, "thumbnail_"+thumbCount++ + extension);
                    metadata.set(RTFMetadata.THUMBNAIL, "true");
                } else {
                    metadata.set(Metadata.RESOURCE_NAME_KEY, "file_"+unknownFilenameCount.getAndIncrement() + 
extension);
                }
            }
            try {
                embeddedExtractor.parseEmbedded(
                        stream,
                        new EmbeddedContentHandler(handler),
                        metadata, false);
            } finally {
                stream.close();
            }
        }
    }
    
    private String getExtension(TikaInputStream is, Metadata metadata) {
        String cType = metadata.get(Metadata.CONTENT_TYPE);
        TikaConfig config = getConfig();
        if (cType == null) {
            Detector detector = config.getDetector();
            try {
                MediaType mediaType = detector.detect(is, metadata);
                MimeTypes types = config.getMimeRepository();
                MimeType mime = types.forName(mediaType.toString());
                metadata.set(Metadata.CONTENT_TYPE, mediaType.getSubtype());
                return mime.getExtension();
            } catch (IOException e) {
                //swallow
            } catch (MimeTypeException e) {
                
            }
        }
        return ".bin";
    }
    
    private TikaConfig getConfig() {
        TikaConfig config = context.get(TikaConfig.class);
        if (config == null) {
            config = TikaConfig.getDefaultConfig();
        }
        return config;
    }
    
    /**
     * reset state after each object.
     * Do not reset unknown file number.
     */
    protected void reset() {
        state = EMB_STATE.NADA;
        os.reset();
        metadata = new Metadata();
        hi = -1;
        sv = EMPTY_STRING;
        sn = EMPTY_STRING;
        sb.setLength(0);
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/rtf/RTFObjDataParser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */


package org.apache.tika.parser.rtf;

import java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.io.InputStream;
import java.io.UnsupportedEncodingException;
import java.util.Locale;
import java.util.concurrent.atomic.AtomicInteger;

import org.apache.poi.poifs.filesystem.DirectoryNode;
import org.apache.poi.poifs.filesystem.DocumentEntry;
import org.apache.poi.poifs.filesystem.DocumentInputStream;
import org.apache.poi.poifs.filesystem.Entry;
import org.apache.poi.poifs.filesystem.NPOIFSFileSystem;
import org.apache.poi.poifs.filesystem.Ole10Native;
import org.apache.poi.poifs.filesystem.Ole10NativeException;
import org.apache.poi.util.IOUtils;
import org.apache.tika.io.FilenameUtils;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.RTFMetadata;
import org.apache.tika.parser.microsoft.OfficeParser.POIFSDocumentType;

/**
 * Many thanks to Simon Mourier for:
 * http://stackoverflow.com/questions/14779647/extract-embedded-image-object-in-rtf 
 * and for granting permission to use his code in Tika.
 * 
 */
class RTFObjDataParser {

    private final static int[] INT_LE_POWS = new int[]{
        1, 256, 65536, 16777216
    };

    private final static String WIN_ASCII = "WINDOWS-1252";

    /**
     * Parses the embedded object/pict string
     * 
     * @param bytes actual bytes (already converted from the 
     *  hex pair string stored in the embedded object data into actual bytes or read
     *  as raw binary bytes)
     * @return a SimpleRTFEmbObj or null
     * @throws IOException if there are any surprise surprises during parsing
     */
    
    /**
     * 
     * @param bytes
     * @param metadata incoming metadata
     * @param unknownFilenameCount 
     * @return byte[] for contents of obj data
     * @throws IOException
     */
    protected byte[] parse(byte[] bytes, Metadata metadata, AtomicInteger unknownFilenameCount) 
            throws IOException {
        ByteArrayInputStream is = new ByteArrayInputStream(bytes);
        long version = readUInt(is);
        metadata.add(RTFMetadata.EMB_APP_VERSION, Long.toString(version));

        long formatId = readUInt(is);
        //2 is an embedded object. 1 is a link.
        if (formatId != 2L) {
            return null;
        }
        String className = readLengthPrefixedAnsiString(is).trim(); 
        String topicName = readLengthPrefixedAnsiString(is).trim();
        String itemName = readLengthPrefixedAnsiString(is).trim(); 

        if (className != null && className.length() > 0) {
            metadata.add(RTFMetadata.EMB_CLASS, className);
        }
        if (topicName != null && topicName.length() > 0) {
            metadata.add(RTFMetadata.EMB_TOPIC, topicName);
        }
        if (itemName != null && itemName.length() > 0) {
            metadata.add(RTFMetadata.EMB_ITEM, itemName);
        }

        long dataSz = readUInt(is);

        //readBytes tests for reading too many bytes
        byte[] embObjBytes = readBytes(is, dataSz);

        if (className.toLowerCase(Locale.ROOT).equals("package")){
            return handlePackage(embObjBytes, metadata);
        } else if (className.toLowerCase(Locale.ROOT).equals("pbrush")) {
            //simple bitmap bytes
            return embObjBytes;
        } else {
            ByteArrayInputStream embIs = new ByteArrayInputStream(embObjBytes);
            if (NPOIFSFileSystem.hasPOIFSHeader(embIs)){
                try{
                    return handleEmbeddedPOIFS(embIs, metadata, unknownFilenameCount);
                } catch (IOException e) {
                    //swallow
                }   
            }
        }
        return embObjBytes;
    }


    //will throw IOException if not actually POIFS
    //can return null byte[]
    private byte[] handleEmbeddedPOIFS(InputStream is, Metadata metadata, 
            AtomicInteger unknownFilenameCount) 
            throws IOException {

        NPOIFSFileSystem fs = null;
        byte[] ret = null;
        try {

            fs = new NPOIFSFileSystem(is);

            DirectoryNode root = fs.getRoot();

            if (root == null) {
                return ret;
            }

            if (root.hasEntry("Package")){
                Entry ooxml = root.getEntry("Package");
                TikaInputStream stream = TikaInputStream.get(new DocumentInputStream((DocumentEntry) ooxml));

                ByteArrayOutputStream out = new ByteArrayOutputStream();

                IOUtils.copy(stream, out);
                ret = out.toByteArray();
            } else {
                //try poifs
                POIFSDocumentType type = POIFSDocumentType.detectType(root);
                if (type == POIFSDocumentType.OLE10_NATIVE) {
                    try {
                        // Try to un-wrap the OLE10Native record:
                        Ole10Native ole = Ole10Native.createFromEmbeddedOleObject(root);
                        ret = ole.getDataBuffer();
                    } catch (Ole10NativeException ex) {
                        // Not a valid OLE10Native record, skip it
                    }
                } else if (type == POIFSDocumentType.COMP_OBJ) {

                    DocumentEntry contentsEntry;
                    try {
                        contentsEntry = (DocumentEntry)root.getEntry("CONTENTS");
                    } catch (FileNotFoundException ioe) {
                        contentsEntry = (DocumentEntry)root.getEntry("Contents");
                    }

                    DocumentInputStream inp = null;
                    try {
                        inp = new DocumentInputStream(contentsEntry);
                        ret = new byte[contentsEntry.getSize()];
                        inp.readFully(ret);
                    } finally {
                        if (inp != null) {
                            inp.close();
                        }
                    }
                } else {

                    ByteArrayOutputStream out = new ByteArrayOutputStream();
                    is.reset();
                    IOUtils.copy(is, out);
                    ret = out.toByteArray();
                    metadata.set(Metadata.RESOURCE_NAME_KEY, "file_"+unknownFilenameCount.getAndIncrement() + "."+type.getExtension());
                    metadata.set(Metadata.CONTENT_TYPE, type.getType().toString());
                }
            }
        } finally {
            if (fs != null) {
                fs.close();
            }
        }
        return ret;
    }



    /**
     * can return null if there is a linked object 
     * instead of an embedded file
     */
    private byte[] handlePackage(byte[] pkgBytes, Metadata metadata) throws IOException { 
        //now parse the package header
        ByteArrayInputStream is = new ByteArrayInputStream(pkgBytes);
        readUShort(is);

        String displayName = readAnsiString(is);

        //should we add this to the metadata?
        readAnsiString(is); //iconFilePath
        readUShort(is); //iconIndex
        int type = readUShort(is); //type

        //1 is link, 3 is embedded object
        //this only handles embedded objects
        if (type != 3) {
            return null;
        }
        //should we really be ignoring this filePathLen?
        readUInt(is); //filePathLen

        String ansiFilePath = readAnsiString(is); //filePath
        long bytesLen = readUInt(is);
        byte[] objBytes = initByteArray(bytesLen);
        is.read(objBytes);
        StringBuilder unicodeFilePath = new StringBuilder();

        try {
            long unicodeLen = readUInt(is);

            for (int i = 0; i < unicodeLen; i++){
                int lo = is.read();
                int hi = is.read();
                int sum = lo+256*hi;
                if (hi == -1 || lo == -1){
                    //stream ran out; empty SB and stop
                    unicodeFilePath.setLength(0);
                    break;
                }
                unicodeFilePath.append((char)sum);
            }
        } catch (IOException e) {
            //swallow; the unicode file path is optional and might not happen
            unicodeFilePath.setLength(0);
        }
        String fileNameToUse = "";
        String pathToUse = "";
        if (unicodeFilePath.length() > 0){
            String p = unicodeFilePath.toString();
            fileNameToUse = p;
            pathToUse = p;
        } else {
            fileNameToUse = displayName == null ? "" : displayName;
            pathToUse = ansiFilePath == null ? "" : ansiFilePath;
        }
        metadata.set(Metadata.RESOURCE_NAME_KEY, FilenameUtils.getName(fileNameToUse));
        metadata.set(Metadata.EMBEDDED_RELATIONSHIP_ID, pathToUse);

        return objBytes;
    }


    private int readUShort(InputStream is) throws IOException {
        int lo = is.read();
        int hi = is.read()*256;
        if (lo == -1 || hi == -1) {
            throw new IOException("Hit end of stream before reading little endian unsigned short.");
        }
        return hi+lo;
    }

    private long readUInt(InputStream is) throws IOException {
        long sum = 0;
        for (int i = 0; i < 4; i++){
            int v = is.read();
            if (v == -1) {
                throw new IOException("Hit end of stream before finishing little endian unsigned int.");
            }
            sum += v*(long)INT_LE_POWS[i];
        }
        return sum;
    }

    private String readAnsiString(InputStream is) throws IOException {
        StringBuilder sb = new StringBuilder();
        int c = is.read();
        while (c > 0) {
            sb.append((char)c);
            c = is.read();
        }
        if (c == -1) {
            throw new IOException("Hit end of stream before end of AnsiString");
        }
        return sb.toString();
    }

    private String readLengthPrefixedAnsiString(InputStream is) throws IOException {
        long len = readUInt(is);
        byte[] bytes = readBytes(is, len);
        try {
            return new String(bytes, WIN_ASCII);
        } catch (UnsupportedEncodingException e) {
            //shouldn't ever happen
            throw new IOException("Unsupported encoding");
        }
    }


    private byte[] readBytes(InputStream is, long len) throws IOException {
        //initByteArray tests for "reading of too many bytes"
        byte[] bytes = initByteArray(len);
        int read = is.read(bytes);
        if (read != len) {
            throw new IOException("Hit end of stream before reading all bytes");
        }

        return bytes;
    }
    
    private byte[] initByteArray(long len) throws IOException {
        if (len < 0 || len > RTFParser.getMaxBytesForEmbeddedObject()) {
            throw new IOException("Requested length for reading bytes is out of bounds: " + len);
        }
        return new byte[(int)len];
        
    }
}

"
tika-parsers/src/main/java/org/apache/tika/parser/rtf/RTFParser.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.rtf;

import java.io.IOException;
import java.io.InputStream;
import java.util.Collections;
import java.util.Set;

import org.apache.tika.exception.TikaException;
import org.apache.tika.io.TaggedInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * RTF parser
 */
public class RTFParser extends AbstractParser {

    /** Serial version UID */
    private static final long serialVersionUID = -4165069489372320313L;

    private static final Set<MediaType> SUPPORTED_TYPES =
            Collections.singleton(MediaType.application("rtf"));

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }

    /** maximum number of bytes per embedded object/pict (default: 20MB)*/
    private static int EMB_OBJ_MAX_BYTES = 20*1024*1024; //20MB

    /**
     * Bytes for embedded objects are currently cached in memory.  
     * If something goes wrong during the parsing of an embedded object, 
     * it is possible that a read length may be crazily too long 
     * and cause a heap crash.
     *  
     * @param max maximum number of bytes to allow for embedded objects.  If 
     * the embedded object has more than this number of bytes, skip it.
     */
    public static void setMaxBytesForEmbeddedObject(int max) {
        EMB_OBJ_MAX_BYTES = max;
    }
    
    /**
     * See {@link #setMaxBytesForEmbeddedObject(int)}.
     * 
     * @return maximum number of bytes allowed for an embedded object.
     * 
     */
    public static int getMaxBytesForEmbeddedObject() {
        return EMB_OBJ_MAX_BYTES;
    }

    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
        throws IOException, SAXException, TikaException {
        TaggedInputStream tagged = new TaggedInputStream(stream);
        try {
            RTFEmbObjHandler embObjHandler = new RTFEmbObjHandler(handler,
                    metadata, context);
            final TextExtractor ert = 
                    new TextExtractor(new XHTMLContentHandler(handler, 
                    metadata), metadata, embObjHandler);
            ert.extract(stream);
            metadata.add(Metadata.CONTENT_TYPE, "application/rtf");
        } catch (IOException e) {
            tagged.throwIfCauseOf(e);
            throw new TikaException("Error parsing an RTF document", e);
        }
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/rtf/TextExtractor.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.parser.rtf;

import java.io.IOException;
import java.io.InputStream;
import java.io.PushbackInputStream;
import java.nio.ByteBuffer;
import java.nio.CharBuffer;
import java.nio.charset.Charset;
import java.nio.charset.CharsetDecoder;
import java.nio.charset.CoderResult;
import java.nio.charset.CodingErrorAction;
import java.util.Calendar;
import java.util.HashMap;
import java.util.LinkedList;
import java.util.Locale;
import java.util.Map;
import java.util.TimeZone;

import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.Office;
import org.apache.tika.metadata.OfficeOpenXMLCore;
import org.apache.tika.metadata.OfficeOpenXMLExtended;
import org.apache.tika.metadata.Property;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.sax.XHTMLContentHandler;
import org.apache.tika.utils.CharsetUtils;
import org.xml.sax.SAXException;

/* Tokenizes and performs a "shallow" parse of the RTF
 * document, just enough to properly decode the text.
 *
 * TODO: we should cutover to a "real" tokenizer (eg JFlex);
 * it should give better perf, by replacing the excessive
 * "else if" string compares with FSA traversal. */

final class TextExtractor {

    private static final Charset ASCII = Charset.forName("US-ASCII");

    private static Charset getCharset(String name) {
        try {
            return CharsetUtils.forName(name);
        } catch (Exception e) {
            return ASCII;
        }
    }

    private static final Charset WINDOWS_1252 = getCharset("WINDOWS-1252");
    private static final Charset MAC_ROMAN = getCharset("MacRoman");
    private static final Charset SHIFT_JIS = getCharset("Shift_JIS");
    private static final Charset WINDOWS_57011 = getCharset("windows-57011");
    private static final Charset WINDOWS_57010 = getCharset("windows-57010");
    private static final Charset WINDOWS_57009 = getCharset("windows-57009");
    private static final Charset WINDOWS_57008 = getCharset("windows-57008");
    private static final Charset WINDOWS_57007 = getCharset("windows-57007");
    private static final Charset WINDOWS_57006 = getCharset("windows-57006");
    private static final Charset WINDOWS_57005 = getCharset("windows-57005");
    private static final Charset WINDOWS_57004 = getCharset("windows-57004");
    private static final Charset WINDOWS_57003 = getCharset("windows-57003");
    private static final Charset X_ISCII91 = getCharset("x-ISCII91");
    private static final Charset X_MAC_CENTRAL_EUROPE = getCharset("x-MacCentralEurope");
    private static final Charset MAC_CYRILLIC = getCharset("MacCyrillic");
    private static final Charset X_JOHAB = getCharset("x-Johab");
    private static final Charset CP12582 = getCharset("CP1258");
    private static final Charset CP12572 = getCharset("CP1257");
    private static final Charset CP12562 = getCharset("CP1256");
    private static final Charset CP12552 = getCharset("CP1255");
    private static final Charset CP12542 = getCharset("CP1254");
    private static final Charset CP12532 = getCharset("CP1253");
    private static final Charset CP1252 = getCharset("CP1252");
    private static final Charset CP12512 = getCharset("CP1251");
    private static final Charset CP12502 = getCharset("CP1250");
    private static final Charset CP950 = getCharset("CP950");
    private static final Charset CP949 = getCharset("CP949");
    private static final Charset MS9362 = getCharset("MS936");
    private static final Charset MS8742 = getCharset("MS874");
    private static final Charset CP866 = getCharset("CP866");
    private static final Charset CP865 = getCharset("CP865");
    private static final Charset CP864 = getCharset("CP864");
    private static final Charset CP863 = getCharset("CP863");
    private static final Charset CP862 = getCharset("CP862");
    private static final Charset CP860 = getCharset("CP860");
    private static final Charset CP852 = getCharset("CP852");
    private static final Charset CP8502 = getCharset("CP850");
    private static final Charset CP819 = getCharset("CP819");
    private static final Charset WINDOWS_720 = getCharset("windows-720");
    private static final Charset WINDOWS_711 = getCharset("windows-711");
    private static final Charset WINDOWS_710 = getCharset("windows-710");
    private static final Charset WINDOWS_709 = getCharset("windows-709");
    private static final Charset ISO_8859_6 = getCharset("ISO-8859-6");
    private static final Charset CP4372 = getCharset("CP437");
    private static final Charset CP850 = getCharset("cp850");
    private static final Charset CP437 = getCharset("cp437");
    private static final Charset MS874 = getCharset("ms874");
    private static final Charset CP1257 = getCharset("cp1257");
    private static final Charset CP1256 = getCharset("cp1256");
    private static final Charset CP1255 = getCharset("cp1255");
    private static final Charset CP1258 = getCharset("cp1258");
    private static final Charset CP1254 = getCharset("cp1254");
    private static final Charset CP1253 = getCharset("cp1253");
    private static final Charset MS950 = getCharset("ms950");
    private static final Charset MS936 = getCharset("ms936");
    private static final Charset MS1361 = getCharset("ms1361");
    private static final Charset MS932 = getCharset("MS932");
    private static final Charset CP1251 = getCharset("cp1251");
    private static final Charset CP1250 = getCharset("cp1250");
    private static final Charset MAC_THAI = getCharset("MacThai");
    private static final Charset MAC_TURKISH = getCharset("MacTurkish");
    private static final Charset MAC_GREEK = getCharset("MacGreek");
    private static final Charset MAC_ARABIC = getCharset("MacArabic");
    private static final Charset MAC_HEBREW = getCharset("MacHebrew");
    private static final Charset JOHAB = getCharset("johab");
    private static final Charset BIG5 = getCharset("Big5");
    private static final Charset GB2312 = getCharset("GB2312");
    private static final Charset MS949 = getCharset("ms949");

    private int written = 0;
    // Hold pending bytes (encoded in the current charset)
    // for text output:
    private byte[] pendingBytes = new byte[16];
    private int pendingByteCount;
    private ByteBuffer pendingByteBuffer = ByteBuffer.wrap(pendingBytes);
      
    // Holds pending chars for text output
    private char[] pendingChars = new char[10];
    private int pendingCharCount;

    // Holds chars for a still-being-tokenized control word
    private byte[] pendingControl = new byte[10];
    private int pendingControlCount;

    // Used when we decode bytes -> chars using CharsetDecoder:
    private final char[] outputArray = new char[128];
    private final CharBuffer outputBuffer = CharBuffer.wrap(outputArray);

    // Reused when possible:
    private CharsetDecoder decoder;
    private Charset lastCharset;

    private Charset globalCharset = WINDOWS_1252;
    private int globalDefaultFont = -1;
    private int curFontID = -1;

    // Holds the font table from this RTF doc, mapping
    // the font number (from \fN control word) to the
    // corresponding charset:
    private final Map<Integer, Charset> fontToCharset =
            new HashMap<Integer, Charset>();

    // Group stack: when we open a new group, we push
    // the previous group state onto the stack; when we
    // close the group, we restore it
    private final LinkedList<GroupState> groupStates = new LinkedList<GroupState>();

    // Current group state; in theory this initial
    // GroupState is unused because the RTF doc should
    // immediately open the top group (start with {):
    private GroupState groupState = new GroupState();

    private boolean inHeader = true;
    private int fontTableState;
    private int fontTableDepth;

    // Non null if we are processing metadata (title,
    // keywords, etc.) inside the info group:
    private Property nextMetaData;
    private boolean inParagraph;

    // Non-zero if we are processing inside a field destination:
    private int fieldState;

    // Non-zero list index
    private int pendingListEnd;
    private Map<Integer, ListDescriptor> listTable = new HashMap<Integer, ListDescriptor>();
    private Map<Integer, ListDescriptor> listOverrideTable = new HashMap<Integer, ListDescriptor>();
    private Map<Integer, ListDescriptor> currentListTable;
    private ListDescriptor currentList;
    private int listTableLevel = -1;
    private boolean ignoreLists;

    // Non-null if we've seen the url for a HYPERLINK but not yet
    // its text:
    private String pendingURL;

    private final StringBuilder pendingBuffer = new StringBuilder();

    // Used to process the sub-groups inside the upr
    // group:
    private int uprState = -1;

    private final XHTMLContentHandler out;
    private final Metadata metadata;
    private final RTFEmbObjHandler embObjHandler;

    // Used when extracting CREATION date:
    private int year, month, day, hour, minute;

    // How many next ansi chars we should skip; this
    // is 0 except when we are still in the "ansi
    // shadow" after seeing a unicode escape, at which
    // point it's set to the last ucN skip we had seen:
    int ansiSkip = 0;

    // The RTF doc has a "font table" that assigns ords
    // (f0, f1, f2, etc.) to fonts and charsets, using the
    // \fcharsetN control word.  This mapping maps from the
    // N to corresponding Java charset:
    private static final Map<Integer, Charset> FCHARSET_MAP =
            new HashMap<Integer, Charset>();

    static {
        FCHARSET_MAP.put(0, WINDOWS_1252); // ANSI
        // charset 1 is Default
        // charset 2 is Symbol

        FCHARSET_MAP.put(77, MAC_ROMAN); // Mac Roman
        FCHARSET_MAP.put(78, SHIFT_JIS); // Mac Shift Jis
        FCHARSET_MAP.put(79, MS949); // Mac Hangul
        FCHARSET_MAP.put(80, GB2312); // Mac GB2312
        FCHARSET_MAP.put(81, BIG5); // Mac Big5
        FCHARSET_MAP.put(82, JOHAB); // Mac Johab (old)
        FCHARSET_MAP.put(83, MAC_HEBREW); // Mac Hebrew
        FCHARSET_MAP.put(84, MAC_ARABIC); // Mac Arabic
        FCHARSET_MAP.put(85, MAC_GREEK); // Mac Greek
        FCHARSET_MAP.put(86, MAC_TURKISH); // Mac Turkish
        FCHARSET_MAP.put(87, MAC_THAI); // Mac Thai
        FCHARSET_MAP.put(88, CP1250); // Mac East Europe
        FCHARSET_MAP.put(89, CP1251); // Mac Russian

        FCHARSET_MAP.put(128, MS932); // Shift JIS
        FCHARSET_MAP.put(129, MS949); // Hangul
        FCHARSET_MAP.put(130, MS1361); // Johab
        FCHARSET_MAP.put(134, MS936); // GB2312
        FCHARSET_MAP.put(136, MS950); // Big5
        FCHARSET_MAP.put(161, CP1253); // Greek
        FCHARSET_MAP.put(162, CP1254); // Turkish
        FCHARSET_MAP.put(163, CP1258); // Vietnamese
        FCHARSET_MAP.put(177, CP1255); // Hebrew
        FCHARSET_MAP.put(178, CP1256); // Arabic
        // FCHARSET_MAP.put( 179, "" ); // Arabic Traditional
        // FCHARSET_MAP.put( 180, "" ); // Arabic user
        // FCHARSET_MAP.put( 181, "" ); // Hebrew user
        FCHARSET_MAP.put(186, CP1257); // Baltic

        FCHARSET_MAP.put(204, CP1251); // Russian
        FCHARSET_MAP.put(222, MS874); // Thai
        FCHARSET_MAP.put(238, CP1250); // Eastern European
        FCHARSET_MAP.put(254, CP437); // PC 437
        FCHARSET_MAP.put(255, CP850); // OEM
    }

    // The RTF may specify the \ansicpgN charset in the
    // header; this maps the N to the corresponding Java
    // character set:
    private static final Map<Integer, Charset> ANSICPG_MAP =
            new HashMap<Integer, Charset>();
    static {
        ANSICPG_MAP.put(437, CP4372);   // US IBM
        ANSICPG_MAP.put(708, ISO_8859_6);   // Arabic (ASMO 708)
      
        ANSICPG_MAP.put(709, WINDOWS_709);  // Arabic (ASMO 449+, BCON V4)
        ANSICPG_MAP.put(710, WINDOWS_710);  // Arabic (transparent Arabic)
        ANSICPG_MAP.put(710, WINDOWS_711);  // Arabic (Nafitha Enhanced)
        ANSICPG_MAP.put(710, WINDOWS_720);  // Arabic (transparent ASMO)
        ANSICPG_MAP.put(819, CP819);  // Windows 3.1 (US & Western Europe)
        ANSICPG_MAP.put(819, CP819);  // Windows 3.1 (US & Western Europe)

        ANSICPG_MAP.put(819, CP819);  // Windows 3.1 (US & Western Europe)
        ANSICPG_MAP.put(850, CP8502);  // IBM Multilingual
        ANSICPG_MAP.put(852, CP852);  // Eastern European
        ANSICPG_MAP.put(860, CP860);  // Portuguese
        ANSICPG_MAP.put(862, CP862);  // Hebrew
        ANSICPG_MAP.put(863, CP863);  // French Canadian
        ANSICPG_MAP.put(864, CP864);  // Arabic
        ANSICPG_MAP.put(865, CP865);  // Norwegian
        ANSICPG_MAP.put(866, CP866);  // Soviet Union
        ANSICPG_MAP.put(874, MS8742);  // Thai
        ANSICPG_MAP.put(932, MS932);  // Japanese
        ANSICPG_MAP.put(936, MS9362);  // Simplified Chinese
        ANSICPG_MAP.put(949, CP949);  // Korean
        ANSICPG_MAP.put(950, CP950);  // Traditional Chinese
        ANSICPG_MAP.put(1250, CP12502);  // Eastern European
        ANSICPG_MAP.put(1251, CP12512);  // Cyrillic
        ANSICPG_MAP.put(1252, CP1252);  // Western European
        ANSICPG_MAP.put(1253, CP12532);  // Greek
        ANSICPG_MAP.put(1254, CP12542);  // Turkish
        ANSICPG_MAP.put(1255, CP12552);  // Hebrew
        ANSICPG_MAP.put(1256, CP12562);  // Arabic
        ANSICPG_MAP.put(1257, CP12572);  // Baltic
        ANSICPG_MAP.put(1258, CP12582);  // Vietnamese
        ANSICPG_MAP.put(1361, X_JOHAB);  // Johab
        ANSICPG_MAP.put(10000, MAC_ROMAN);  // Mac Roman
        ANSICPG_MAP.put(10001, SHIFT_JIS);  // Mac Japan
        ANSICPG_MAP.put(10004, MAC_ARABIC);  // Mac Arabic
        ANSICPG_MAP.put(10005, MAC_HEBREW);  // Mac Hebrew
        ANSICPG_MAP.put(10006, MAC_GREEK);  // Mac Hebrew
        ANSICPG_MAP.put(10007, MAC_CYRILLIC);  // Mac Cyrillic
        ANSICPG_MAP.put(10029, X_MAC_CENTRAL_EUROPE);  // MAC Latin2
        ANSICPG_MAP.put(10081, MAC_TURKISH);  // Mac Turkish
        ANSICPG_MAP.put(57002, X_ISCII91);   // Devanagari

        // TODO: in theory these other charsets are simple
        // shifts off of Devanagari, so we could impl that
        // here:
        ANSICPG_MAP.put(57003, WINDOWS_57003);   // Bengali
        ANSICPG_MAP.put(57004, WINDOWS_57004);   // Tamil
        ANSICPG_MAP.put(57005, WINDOWS_57005);   // Telugu
        ANSICPG_MAP.put(57006, WINDOWS_57006);   // Assamese
        ANSICPG_MAP.put(57007, WINDOWS_57007);   // Oriya
        ANSICPG_MAP.put(57008, WINDOWS_57008);   // Kannada
        ANSICPG_MAP.put(57009, WINDOWS_57009);   // Malayalam
        ANSICPG_MAP.put(57010, WINDOWS_57010);   // Gujariti
        ANSICPG_MAP.put(57011, WINDOWS_57011);   // Punjabi
    }

    public TextExtractor(XHTMLContentHandler out, Metadata metadata,
            RTFEmbObjHandler embObjHandler) {
        this.metadata = metadata;
        this.out = out;
        this.embObjHandler = embObjHandler;
    }

    public boolean isIgnoringLists() {
        return ignoreLists;
    }

    public void setIgnoreLists(boolean ignore) {
        this.ignoreLists = ignore;
    }

    protected static boolean isHexChar(int ch) {
        return (ch >= '0' && ch <= '9') ||
            (ch >= 'a' && ch <= 'f') ||
            (ch >= 'A' && ch <= 'F');
    }

    private static boolean isAlpha(int ch) {
        return (ch >= 'a' && ch <= 'z') ||
            (ch >= 'A' && ch <= 'Z');
    }

    private static boolean isDigit(int ch) {
        return ch >= '0' && ch <= '9';
    }

    protected static int hexValue(int ch) {
        if (ch >= '0' && ch <= '9') {
            return ch - '0';
        } else if (ch >= 'a' && ch <= 'z') {
            return 10 + (ch - 'a');
        } else {
            assert ch >= 'A' && ch <= 'Z';
            return 10 + (ch - 'A');
        }
    }

    // Push pending bytes or pending chars:
    private void pushText() throws IOException, SAXException, TikaException {
        if (pendingByteCount != 0) {
            assert pendingCharCount == 0;
            pushBytes();
        } else {
            pushChars();
        }
    }

    // Buffers the byte (unit in the current charset) for
    // output:
    private void addOutputByte(int b) throws IOException, SAXException, TikaException {
        assert b >= 0 && b < 256 : "byte value out of range: " + b;

        if (pendingCharCount != 0) {
            pushChars();
        }
        if (groupState.pictDepth > 0) {
            embObjHandler.writeMetadataChar((char)b);
        } else {
            // Save the byte in pending buffer:
            if (pendingByteCount == pendingBytes.length) {
                // Gradual but exponential growth:
                final byte[] newArray = new byte[(int) (pendingBytes.length*1.25)];
                System.arraycopy(pendingBytes, 0, newArray, 0, pendingBytes.length);
                pendingBytes = newArray;
                pendingByteBuffer = ByteBuffer.wrap(pendingBytes);
            }
            pendingBytes[pendingByteCount++] = (byte) b;
       }
    }

   // Buffers a byte as part of a control word:
    private void addControl(int b) {
        assert isAlpha(b);
        // Save the byte in pending buffer:
        if (pendingControlCount == pendingControl.length) {
            // Gradual but exponential growth:
            final byte[] newArray = new byte[(int) (pendingControl.length*1.25)];
            System.arraycopy(pendingControl, 0, newArray, 0, pendingControl.length);
            pendingControl = newArray;
        }
        pendingControl[pendingControlCount++] = (byte) b;
    }

    // Buffers a UTF16 code unit for output
    private void addOutputChar(char ch) throws IOException, SAXException, TikaException {
        if (pendingByteCount != 0) {
            pushBytes();
        }

        if (inHeader || fieldState == 1) {
            pendingBuffer.append(ch);
        } else if (groupState.sn == true || groupState.sv == true) {
            embObjHandler.writeMetadataChar(ch);
        } else {
            if (pendingCharCount == pendingChars.length) {
                // Gradual but exponential growth:
                final char[] newArray = new char[(int) (pendingChars.length*1.25)];
                System.arraycopy(pendingChars, 0, newArray, 0, pendingChars.length);
                pendingChars = newArray;
            }
            pendingChars[pendingCharCount++] = ch;
        }
    }

    // Shallow parses the entire doc, writing output to
    // this.out and this.metadata
    public void extract(InputStream in) throws IOException, SAXException, TikaException {
//        in = new FilterInputStream(in) {
//            public int read() throws IOException {
//                int r = super.read();
//                System.out.write(r);
//                System.out.flush();
//                return r;
//            }
//            public int read(byte b[], int off, int len) throws IOException {
//                int r = super.read(b, off, len);
//                System.out.write(b, off, r);
//                System.out.flush();
//                return r;
//            }
//        };
        extract(new PushbackInputStream(in, 2));
    }
    
    private void extract(PushbackInputStream in) throws IOException, SAXException, TikaException {
        out.startDocument();

        while (true) {
            final int b = in.read();
            if (b == -1) {
                break;
            } else if (b == '\\') {
                parseControlToken(in);
            } else if (b == '{') {
                pushText();
                processGroupStart(in);
             } else if (b == '}') {
                pushText();
                processGroupEnd();
                if (groupStates.isEmpty()) {
                    // parsed document closing brace
                    break;
                }
            } else if (groupState.objdata == true ||
                groupState.pictDepth == 1) {
                embObjHandler.writeHexChar(b);
            } else if (b != '\r' && b != '\n' 
                    && (!groupState.ignore || nextMetaData != null ||
                    groupState.sn == true || groupState.sv == true)) {
                // Linefeed and carriage return are not
                // significant
                if (ansiSkip != 0) {
                    ansiSkip--;
                } else {
                    addOutputByte(b);
                }
            }
        }

        endParagraph(false);
        out.endDocument();
    }
    
    private void parseControlToken(PushbackInputStream in) throws IOException, SAXException, TikaException {
        int b = in.read();
        if (b == '\'') {
            // escaped hex char
            parseHexChar(in);
        } else if (isAlpha(b)) {
            // control word
            parseControlWord((char)b, in);
        } else if (b == '{' || b == '}' || b == '\\' || b == '\r' || b == '\n') {
            // escaped char
            addOutputByte(b);
        } else if (b != -1) {
            // control symbol, eg \* or \~
            processControlSymbol((char)b);
        }
    }
    
    private void parseHexChar(PushbackInputStream in) throws IOException, SAXException, TikaException {
        int hex1 = in.read();
        if (!isHexChar(hex1)) {
            // DOC ERROR (malformed hex escape): ignore 
            in.unread(hex1);
            return;
        }
        
        int hex2 = in.read();
        if (!isHexChar(hex2)) {
            // TODO: log a warning here, somehow?
            // DOC ERROR (malformed hex escape):
            // ignore
            in.unread(hex2);
            return;
        }
        
        if (ansiSkip != 0) {
            // Skip this ansi char since we are
            // still in the shadow of a unicode
            // escape:
            ansiSkip--;
        } else {
            // Unescape:
            addOutputByte(16*hexValue(hex1) + hexValue(hex2));
        }
    }

    private void parseControlWord(int firstChar, PushbackInputStream in) throws IOException, SAXException, TikaException {
        addControl(firstChar);
        
        int b = in.read();
        while (isAlpha(b)) {
            addControl(b);
            b = in.read();
        }
        
        boolean hasParam = false;
        boolean negParam = false;
        if (b == '-') {
            negParam = true;
            hasParam = true;
            b = in.read();
        }

        int param = 0;
        while (isDigit(b)) {
            param *= 10;
            param += (b - '0');
            hasParam = true;
            b = in.read();
        }
        
        // space is consumed as part of the
        // control word, but is not added to the
        // control word
        if (b != ' ') {
            in.unread(b);
        }
        
        if (hasParam) {
            if (negParam) {
                param = -param;
            }
            processControlWord(param, in);
        } else {
            processControlWord();
        }
        
        pendingControlCount = 0;
    }

    private void lazyStartParagraph() throws IOException, SAXException, TikaException {
        if (!inParagraph) {
            // Ensure </i></b> order
            if (groupState.italic) {
                end("i");
            }
            if (groupState.bold) {
                end("b");
            }
            if (pendingListEnd != 0 && groupState.list != pendingListEnd) {
                endList(pendingListEnd);
                pendingListEnd = 0;
            }
            if (inList() && pendingListEnd != groupState.list) {
                startList(groupState.list);
            }            
            if (inList()) {
                out.startElement("li");
            } else {
                out.startElement("p");
            }

            // Ensure <b><i> order
            if (groupState.bold) {
                start("b");
            }
            if (groupState.italic) {
                start("i");
            }
            inParagraph = true;
        }
    }

    private void endParagraph(boolean preserveStyles) throws IOException, SAXException, TikaException {
        pushText();
        if (inParagraph) {
            if (groupState.italic) {
                end("i");
                groupState.italic = preserveStyles;
            }
            if (groupState.bold) {
                end("b");
                groupState.bold = preserveStyles;
            }
            if (inList()) {
                out.endElement("li");
            } else {
                out.endElement("p");
            }

            if (preserveStyles && (groupState.bold || groupState.italic)) {
                start("p");
                if (groupState.bold) {
                    start("b");
                }
                if (groupState.italic) {
                    start("i");
                }
                inParagraph = true;
            } else {
                inParagraph = false;
            }
        }

        // Ensure closing the list at document end
        if (!preserveStyles && pendingListEnd != 0) {
            endList(pendingListEnd);
            pendingListEnd = 0;
        }
    }

    // Push pending UTF16 units to out ContentHandler
    private void pushChars() throws IOException, SAXException, TikaException {
        if (pendingCharCount != 0) {
            lazyStartParagraph();
            out.characters(pendingChars, 0, pendingCharCount);
            pendingCharCount = 0;
        }
    }

    // Decodes the buffered bytes in pendingBytes
    // into UTF16 code units, and sends the characters
    // to the out ContentHandler, if we are in the body,
    // else appends the characters to the pendingBuffer
    private void pushBytes() throws IOException, SAXException, TikaException {
        if (pendingByteCount > 0 && (!groupState.ignore || nextMetaData != null)) {

            final CharsetDecoder decoder = getDecoder();
            pendingByteBuffer.limit(pendingByteCount);
            assert pendingByteBuffer.position() == 0;
            assert outputBuffer.position() == 0;

            while (true) {
                // We pass true for endOfInput because, when
                // we are called, we should have seen a
                // complete sequence of characters for this
                // charset:
                final CoderResult result = decoder.decode(pendingByteBuffer, outputBuffer, true);

                final int pos = outputBuffer.position();
                if (pos > 0) {
                    if (inHeader || fieldState == 1) {
                        pendingBuffer.append(outputArray, 0, pos);
                    } else {
                        lazyStartParagraph();
                        out.characters(outputArray, 0, pos);
                    }
                    outputBuffer.position(0);
                }

                if (result == CoderResult.UNDERFLOW) {
                    break;
                }
            }

            while (true) {
                final CoderResult result = decoder.flush(outputBuffer);

                final int pos = outputBuffer.position();
                if (pos > 0) {
                    if (inHeader || fieldState == 1) {
                        pendingBuffer.append(outputArray, 0, pos);
                    } else {
                        lazyStartParagraph();
                        out.characters(outputArray, 0, pos);
                    }
                    outputBuffer.position(0);
                }

                if (result == CoderResult.UNDERFLOW) {
                    break;
                }
            }

            // Reset for next decode
            decoder.reset();
            pendingByteBuffer.position(0);
        }

        pendingByteCount = 0;
    }

    // NOTE: s must be ascii alpha only
    private boolean equals(String s) {
        if (pendingControlCount != s.length()) {
            return false;
        }
        for(int idx=0;idx<pendingControlCount;idx++) {
            assert isAlpha(s.charAt(idx));
            if (((byte) s.charAt(idx)) != pendingControl[idx]) {
                return false;
            }
        }
        return true;
    }

    private void processControlSymbol(char ch) throws IOException, SAXException, TikaException {
        switch(ch) {
        case '~':
            // Non-breaking space -> unicode NON-BREAKING SPACE
            addOutputChar('\u00a0');
            break;
        case '*':
            // Ignorable destination (control words defined after
            // the 1987 RTF spec). These are already handled by
            // processGroupStart()
            break;
        case '-':
            // Optional hyphen -> unicode SOFT HYPHEN
            addOutputChar('\u00ad');
            break;
        case '_':
            // Non-breaking hyphen -> unicode NON-BREAKING HYPHEN
            addOutputChar('\u2011');
            break;
        default:
            break;
        }
    }

    private CharsetDecoder getDecoder() throws TikaException {
        Charset charset = getCharset();

        // Common case: charset is same as last time, so
        // just reuse it:
        if (lastCharset == null || !charset.equals(lastCharset)) {
            decoder = charset.newDecoder();
            decoder.onMalformedInput(CodingErrorAction.REPLACE);
            decoder.onUnmappableCharacter(CodingErrorAction.REPLACE);
            lastCharset = charset;
        }

        return decoder;
    }

    // Return current charset in-use
    private Charset getCharset() throws TikaException {
        // If a specific font (fN) was set, use its charset
        if (groupState.fontCharset != null) {
            return groupState.fontCharset;
        }

        // Else, if global default font (defN) was set, use that one
        if (globalDefaultFont != -1 && !inHeader) {
            Charset cs = fontToCharset.get(globalDefaultFont);
            if (cs != null) {
                return cs;
            }
        }

        // Else, use the global charset
        if (globalCharset == null) {
            throw new TikaException("unable to determine charset");
        }

        return globalCharset;
    }

    // Handle control word that takes a parameter:
    private void processControlWord(int param, PushbackInputStream in) throws IOException, SAXException, TikaException {

        // TODO: afN?  (associated font number)

        // TODO: do these alter text output...?
        /*
            } else if (equals("stshfdbch")) {
                // font to be used by default in
                // style sheet for East Asian chars
                // arg N is font table entry
            } else if (equals("stshfloch")) {
                // font to be used by default in
                // style sheet for ASCII chars
                // arg N is font table entry
            } else if (equals("stshfhich")) {
                // font to be used by default in
                // style sheet for High Ansi chars
                // arg N is font table entry
            } else if (equals("stshfbi")) {
                // style sheet for Complex Scripts (BIDI) chars
                // arg N is font table entry
                */

        // TODO: inefficient that we check equals N times;
        // we'd get better perf w/ real lexer (eg
        // JFlex), which uses single-pass FSM to do cmp:
        if (inHeader) {
            if (equals("ansicpg")) {
                // ANSI codepage
                Charset cs = ANSICPG_MAP.get(param);
                if (cs != null) {
                    globalCharset = cs;
                }
            } else if (equals("deff")) {
                // Default font
                globalDefaultFont = param;
            } else if (equals("nofpages")) {
                metadata.add(Office.PAGE_COUNT, Integer.toString(param));
            } else if (equals("nofwords")) {
                metadata.add(Office.WORD_COUNT, Integer.toString(param));
            } else if (equals("nofchars")) {
                metadata.add(Office.CHARACTER_COUNT, Integer.toString(param));
            } else if (equals("yr")) {
                year = param;
            } else if (equals("mo")) {
                month = param;
            } else if (equals("dy")) {
                day = param;
            } else if (equals("hr")) {
                hour = param;
            } else if (equals("min")) {
                minute = param;
            }

            if (fontTableState == 1) {
                // Still inside font table -- record the
                // mappings of fN to the fcharset:
                if (groupState.depth < fontTableDepth) {
                    fontTableState = 2;
                } else {
                    if (equals("f")) {
                        // Start new font definition
                        curFontID = param;
                    } else if (equals("fcharset")) {
                        Charset cs = FCHARSET_MAP.get(param);
                        if (cs != null) {
                            fontToCharset.put(curFontID, cs);
                        }
                    }
                }
            }

            if (currentList != null) {
                if (equals("listid")) {
                    currentList.id = param;
                    currentListTable.put(currentList.id, currentList);
                } else if (equals("listtemplateid")) {
                    currentList.templateID = param;
                } else if (equals("levelnfc") || equals("levelnfcn")) {
                    //sanity check to make sure list information isn't corrupt
                    if (listTableLevel > -1 && 
                        listTableLevel < currentList.numberType.length ) {
                        currentList.numberType[listTableLevel] = param;
                    }
                }
            }
        } else {
            // In document
            if (equals("b")) {
                // b0
                assert param == 0;
                if (groupState.bold) {
                    pushText();
                    if (groupState.italic) {
                        end("i");
                    }
                    end("b");
                    if (groupState.italic) {
                        start("i");
                    }
                    groupState.bold = false;
                }
            } else if (equals("i")) {
                // i0
                assert param == 0;
                if (groupState.italic) {
                    pushText();
                    end("i");
                    groupState.italic = false;
                }
            } else if (equals("f")) {
                // Change current font
                Charset fontCharset = fontToCharset.get(param);

                // Push any buffered text before changing
                // font:
                pushText();

                if (fontCharset != null) {
                    groupState.fontCharset = fontCharset;
                } else {
                    // DOC ERROR: font change referenced a
                    // non-table'd font number
                    // TODO: log a warning?  Throw an exc?
                    groupState.fontCharset = null;
                }
            } else if (equals("ls")) {
                groupState.list = param;
            } else if (equals("lslvl")) {
                groupState.listLevel = param;
            }
        }

        // Process unicode escape. This can appear in doc
        // or in header, since the metadata (info) fields
        // in the header can be unicode escaped as well:
        if (equals("u")) {
            // Unicode escape
            if (!groupState.ignore || groupState.sv || groupState.sn) {
                final char utf16CodeUnit = (char) (param & 0xffff);
                addOutputChar(utf16CodeUnit);
            }

            // After seeing a unicode escape we must
            // skip the next ucSkip ansi chars (the
            // "unicode shadow")
            ansiSkip = groupState.ucSkip;
        } else if (equals("uc")) {
            // Change unicode shadow length
            groupState.ucSkip = param;
        } else if (equals("bin")) {
            if (param >= 0) {
                if (groupState.pictDepth == 1) {
                    try{
                        embObjHandler.writeBytes(in, param);
                    } catch (IOException e) {
                        //param was out of bounds or something went wrong during writing.
                        //skip this obj and move on
                        //TODO: log.warn
                        embObjHandler.reset();
                    }
                } else {
                    int bytesToRead = param;
                    byte[] tmpArray = new byte[Math.min(1024, bytesToRead)];
                    while (bytesToRead > 0) {
                        int r = in.read(tmpArray, 0, Math.min(bytesToRead, tmpArray.length));
                        if (r < 0) {
                            throw new TikaException("unexpected end of file: need " + param + " bytes of binary data, found " + (param-bytesToRead));
                        }
                        bytesToRead -= r;
                    }
                }
            } else {
                // log some warning?
            }
        }
    }

    private boolean inList() {
        return !ignoreLists && groupState.list != 0;
    }

    /**
     * Marks the current list as pending to end. This is done to be able to merge list items of
     * the same list within the same enclosing list tag (ie. either <code>"ul"</code>, or
     * <code>"ol"</code>).
     */
    private void pendingListEnd() {
        pendingListEnd = groupState.list;
        groupState.list = 0;
    }

    /**
     * Emits the end tag of a list. Uses {@link #isUnorderedList(int)} to determine the list
     * type for the given <code>listID</code>.
     * @param listID The ID of the list.
     * @throws IOException
     * @throws SAXException
     * @throws TikaException
     */
    private void endList(int listID) throws IOException, SAXException, TikaException {
        if (!ignoreLists) {
            out.endElement(isUnorderedList(listID) ? "ul" : "ol");
        }
    }

    /**
     * Emits the start tag of a list. Uses {@link #isUnorderedList(int)} to determine the list
     * type for the given <code>listID</code>.
     * @param listID The ID of the list.
     * @throws IOException
     * @throws SAXException
     * @throws TikaException
     */
    private void startList(int listID) throws IOException, SAXException, TikaException {
        if (!ignoreLists) {
            out.startElement(isUnorderedList(listID) ? "ul" : "ol");
        }
    }

    private boolean isUnorderedList(int listID) {
        ListDescriptor list = listTable.get(listID);
        if (list != null) {
            return list.isUnordered(groupState.listLevel);
        }
        return true;
    }

    private void end(String tag) throws IOException, SAXException, TikaException {
        out.endElement(tag);
    }

    private void start(String tag) throws IOException, SAXException, TikaException {
        out.startElement(tag);
    }

    // Handle non-parameter control word:
    private void processControlWord() throws IOException, SAXException, TikaException {
        if (inHeader) {
            if (equals("ansi")) {
                globalCharset = WINDOWS_1252;
            } else if (equals("pca")) { 
                globalCharset = CP850;
            } else if (equals("pc")) { 
                globalCharset = CP437;
            } else if (equals("mac")) { 
                globalCharset = MAC_ROMAN;
            }

            if (equals("colortbl") || equals("stylesheet") || equals("fonttbl")) {
                groupState.ignore = true;
            } else if (equals("listtable")) {
                currentListTable = listTable;
            } else if (equals("listoverridetable")) {
                currentListTable = listOverrideTable;
            }

            if (uprState == -1) {
                // TODO: we can also parse \creatim, \revtim,
                // \printim, \version, etc.
                if (equals("author")) {
                    nextMetaData = TikaCoreProperties.CREATOR;
                } else if (equals("title")) {
                    nextMetaData = TikaCoreProperties.TITLE;
                } else if (equals("subject")) {
                    // TODO: Move to OO subject in Tika 2.0
                    nextMetaData = TikaCoreProperties.TRANSITION_SUBJECT_TO_OO_SUBJECT;
                } else if (equals("keywords")) {
                    nextMetaData = TikaCoreProperties.TRANSITION_KEYWORDS_TO_DC_SUBJECT;
                } else if (equals("category")) {
                    nextMetaData = OfficeOpenXMLCore.CATEGORY;
                } else if (equals("comment")) {
                    nextMetaData = TikaCoreProperties.COMMENTS;
                } else if (equals("company")) {
                    nextMetaData = OfficeOpenXMLExtended.COMPANY;
                } else if (equals("manager")) {
                    nextMetaData = OfficeOpenXMLExtended.MANAGER;
                } else if (equals("template")) {
                    nextMetaData = OfficeOpenXMLExtended.TEMPLATE;
                } else if (equals("creatim")) {
                    nextMetaData = TikaCoreProperties.CREATED;
                }
            }

            if (fontTableState == 0) {
                // Didn't see font table yet
                if (equals("fonttbl")) {
                    fontTableState = 1;
                    fontTableDepth = groupState.depth;
                }
            } else if (fontTableState == 1) {
                // Inside font table
                if (groupState.depth < fontTableDepth) {
                    fontTableState = 2;
                }
            }

            // List table handling
            if (currentListTable != null) {
                if (equals("list") || equals("listoverride")) {
                    currentList = new ListDescriptor();
                    listTableLevel = -1;
                } else if (currentList != null) {
                    if (equals("liststylename")) {
                        currentList.isStyle = true;
                    } else if (equals("listlevel")) {
                        listTableLevel++;
                    }
                }
            }

            if (!groupState.ignore && (equals("par") || equals("pard") || equals("sect") || equals("sectd") || equals("plain") || equals("ltrch") || equals("rtlch"))) {
                inHeader = false;
            }
        } else {
            if (equals("b")) {
                if (!groupState.bold) {
                    pushText();
                    lazyStartParagraph();
                    if (groupState.italic) {
                        // Make sure nesting is always <b><i>
                        end("i");
                    }
                    groupState.bold = true;
                    start("b");
                    if (groupState.italic) {
                        start("i");
                    }
                }
            } else if (equals("i")) {
                if (!groupState.italic) {
                    pushText();
                    lazyStartParagraph();
                    groupState.italic = true;
                    start("i");
                }
            }
        }

        final boolean ignored = groupState.ignore;

        if (equals("pard")) {
            // Reset styles
            pushText();
            if (groupState.italic) {
                end("i");
                groupState.italic = false;
            }
            if (groupState.bold) {
                end("b");
                groupState.bold = false;
            }
            if (inList()) { // && (groupStates.size() == 1 || groupStates.peekLast().list < 0))
                pendingListEnd();
            }
        } else if (equals("par")) {
            if (!ignored) {
                endParagraph(true);
            }
        } else if (equals("shptxt")) {
            pushText();
            // Text inside a shape
            groupState.ignore = false;
        } else if (equals("atnid")) {
            pushText();
            // Annotation ID
            groupState.ignore = false;
        } else if (equals("atnauthor")) {
            pushText();
            // Annotation author
            groupState.ignore = false;
        } else if (equals("annotation")) {
            pushText();
            // Annotation
            groupState.ignore = false;
        } else if (equals("listtext")) {
            groupState.ignore = true;
        } else if (equals("cell")) {
            // TODO: we should produce a table output here?
            //addOutputChar(' ');
            endParagraph(true);
        } else if (equals("sp")) {
            groupState.sp = true;
        } else if (equals("sn")) {
            embObjHandler.startSN();
            groupState.sn = true;
        } else if (equals("sv")) {
            embObjHandler.startSV();
            groupState.sv = true;
        } else if (equals("object")) {
            pushText();
            embObjHandler.setInObject(true);
            groupState.object = true;
        } else if (equals("objdata")) {
            groupState.objdata = true;
            embObjHandler.startObjData();
        } else if (equals("pict")) {
            pushText();
            // TODO: create img tag?  but can that support
            // embedded image data?
            groupState.pictDepth = 1;
            embObjHandler.startPict();
        } else if (equals("line")) {
            if (!ignored) {
                addOutputChar('\n');
            }
        } else if (equals("column")) {
            if (!ignored) {
                addOutputChar(' ');
            }
        } else if (equals("page")) {
            if (!ignored) {
                addOutputChar('\n');
            }
        } else if (equals("softline")) {
            if (!ignored) {
                addOutputChar('\n');
            }
        } else if (equals("softcolumn")) {
            if (!ignored) {
                addOutputChar(' ');
            }
        } else if (equals("softpage")) {
            if (!ignored) {
                addOutputChar('\n');
            }
        } else if (equals("tab")) {
            if (!ignored) {
                addOutputChar('\t');
            }
        } else if (equals("upr")) {
            uprState = 0;
        } else if (equals("ud") && uprState == 1) {
            uprState = -1;
            // 2nd group inside the upr destination, which
            // contains the unicode encoding of the text, so
            // we want to keep that:
            groupState.ignore = false;
        } else if (equals("bullet")) {
            if (!ignored) {
                // unicode BULLET
                addOutputChar('\u2022');
            }
        } else if (equals("endash")) {
            if (!ignored) {
                // unicode EN DASH
                addOutputChar('\u2013');
            }
        } else if (equals("emdash")) {
            if (!ignored) {
                // unicode EM DASH
                addOutputChar('\u2014');
            }
        } else if (equals("enspace")) {
            if (!ignored) {
                // unicode EN SPACE
                addOutputChar('\u2002');
            }
        } else if (equals("qmspace")) {
            if (!ignored) {
                // quarter em space -> unicode FOUR-PER-EM SPACE
                addOutputChar('\u2005');
            }
        } else if (equals("emspace")) {
            if (!ignored) {
                // unicode EM SPACE
                addOutputChar('\u2003');
            }
        } else if (equals("lquote")) {
            if (!ignored) {
                // unicode LEFT SINGLE QUOTATION MARK
                addOutputChar('\u2018');
            }
        } else if (equals("rquote")) {
            if (!ignored) {
                // unicode RIGHT SINGLE QUOTATION MARK
                addOutputChar('\u2019');
            }
        } else if (equals("ldblquote")) {
            if (!ignored) {
                // unicode LEFT DOUBLE QUOTATION MARK
                addOutputChar('\u201C');
            }
        } else if (equals("rdblquote")) {
            if (!ignored) {
                // unicode RIGHT DOUBLE QUOTATION MARK
                addOutputChar('\u201D');
            }
        } else if (equals("fldinst")) {
            fieldState = 1;
            groupState.ignore = false;
        } else if (equals("fldrslt") && fieldState == 2) {
            assert pendingURL != null;
            lazyStartParagraph();
            out.startElement("a", "href", pendingURL);
            pendingURL = null;
            fieldState = 3;
            groupState.ignore = false;
        }
    }

    // Push new GroupState
    private void processGroupStart(PushbackInputStream in) throws IOException {
        ansiSkip = 0;
        // Push current groupState onto the stack
        groupStates.add(groupState);

        // Make new GroupState
        groupState = new GroupState(groupState);
        assert groupStates.size() == groupState.depth: "size=" + groupStates.size() + " depth=" + groupState.depth;

        if (uprState == 0) {
            uprState = 1;
            groupState.ignore = true;
        }
        
        // Check for ignorable groups. Note that
        // sometimes we un-ignore within this group, eg
        // when handling upr escape.
        int b2 = in.read();
        if (b2 == '\\') {
            int b3 = in.read();
            if (b3 == '*') {
                groupState.ignore = true;
            }
               in.unread(b3);
        }
        in.unread(b2);
    }

    // Pop current GroupState
    private void processGroupEnd() throws IOException, SAXException, TikaException {
        if (inHeader) {
            if (nextMetaData != null) {
                if (nextMetaData == TikaCoreProperties.CREATED) {
                    Calendar cal = Calendar.getInstance(TimeZone.getDefault(), Locale.ROOT);
                    cal.set(year, month-1, day, hour, minute, 0);
                    metadata.set(nextMetaData, cal.getTime());
                } else if (nextMetaData.isMultiValuePermitted()) {
                    metadata.add(nextMetaData, pendingBuffer.toString());
                } else {
                    metadata.set(nextMetaData, pendingBuffer.toString());
                }
                nextMetaData = null;
            }
            pendingBuffer.setLength(0);
        }

        assert groupState.depth > 0;
        ansiSkip = 0;
        
        if (groupState.objdata == true) {
            embObjHandler.handleCompletedObject();
            groupState.objdata = false;
        } else if (groupState.pictDepth > 0) {
            if (groupState.sn == true) {
                embObjHandler.endSN();
            } else if (groupState.sv == true) {
                embObjHandler.endSV();
            } else if (groupState.sp == true) {
                embObjHandler.endSP();
            } else if (groupState.pictDepth == 1) {
                embObjHandler.handleCompletedObject();
            }
        }

        if (groupState.object == true) {
            embObjHandler.setInObject(false);
        }

        // Be robust if RTF doc is corrupt (has too many
        // closing }s):
        // TODO: log a warning?
        if (groupStates.size() > 0) {
            // Restore group state:
            final GroupState outerGroupState = groupStates.removeLast();

            // Close italic, if outer does not have italic or
            // bold changed:
            if (groupState.italic) {
                if (!outerGroupState.italic ||
                    groupState.bold != outerGroupState.bold) {
                    end("i");
                    groupState.italic = false;
                }
            }

            // Close bold
            if (groupState.bold && !outerGroupState.bold) {
                end("b");
            }

            // Open bold
            if (!groupState.bold && outerGroupState.bold) {
                start("b");
            }

            // Open italic
            if (!groupState.italic && outerGroupState.italic) {
                start("i");
            }
            groupState = outerGroupState;
        }
        assert groupStates.size() == groupState.depth;

        if (fieldState == 1) {
            String s = pendingBuffer.toString().trim();
            pendingBuffer.setLength(0);
            if (s.startsWith("HYPERLINK")) {
                s = s.substring(9).trim();
                // TODO: what other instructions can be in a
                // HYPERLINK destination?
                final boolean isLocalLink = s.contains("\\l ");
                int idx = s.indexOf('"');
                if (idx != -1) {
                    int idx2 = s.indexOf('"', 1+idx);
                    if (idx2 != -1) {
                        s = s.substring(1+idx, idx2);
                    }
                }
                pendingURL = (isLocalLink ? "#" : "") + s;
                fieldState = 2;
            } else {
                fieldState = 0;
            }

            // TODO: we could process the other known field
            // types.  Right now, we will extract their text
            // inlined, but fail to record them in metadata
            // as a field value.
        } else if (fieldState == 3) {
            out.endElement("a");
            fieldState = 0;
        }
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/txt/CharsetDetector.java,true,"/**
*******************************************************************************
* Copyright (C) 2005-2009, International Business Machines Corporation and    *
* others. All Rights Reserved.                                                *
*******************************************************************************
*/
package org.apache.tika.parser.txt;

import java.io.InputStream;
import java.io.Reader;
import java.io.IOException;
import java.nio.charset.Charset;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Arrays;


/**
 * <code>CharsetDetector</code> provides a facility for detecting the
 * charset or encoding of character data in an unknown format.
 * The input data can either be from an input stream or an array of bytes.
 * The result of the detection operation is a list of possibly matching
 * charsets, or, for simple use, you can just ask for a Java Reader that
 * will will work over the input data.
 * <p/>
 * Character set detection is at best an imprecise operation.  The detection
 * process will attempt to identify the charset that best matches the characteristics
 * of the byte data, but the process is partly statistical in nature, and
 * the results can not be guaranteed to always be correct.
 * <p/>
 * For best accuracy in charset detection, the input data should be primarily
 * in a single language, and a minimum of a few hundred bytes worth of plain text
 * in the language are needed.  The detection process will attempt to
 * ignore html or xml style markup that could otherwise obscure the content.
 * <p/>
 * @stable ICU 3.4
 */
public class CharsetDetector {

//   Question: Should we have getters corresponding to the setters for input text
//   and declared encoding?

//   A thought: If we were to create our own type of Java Reader, we could defer
//   figuring out an actual charset for data that starts out with too much English
//   only ASCII until the user actually read through to something that didn't look
//   like 7 bit English.  If  nothing else ever appeared, we would never need to
//   actually choose the "real" charset.  All assuming that the application just
//   wants the data, and doesn't care about a char set name.

    /**
     *   Constructor
     * 
     * @stable ICU 3.4
     */
    public CharsetDetector() {
    }

    /**
     * Set the declared encoding for charset detection.
     *  The declared encoding of an input text is an encoding obtained
     *  from an http header or xml declaration or similar source that
     *  can be provided as additional information to the charset detector.  
     *  A match between a declared encoding and a possible detected encoding
     *  will raise the quality of that detected encoding by a small delta,
     *  and will also appear as a "reason" for the match.
     * <p/>
     * A declared encoding that is incompatible with the input data being
     * analyzed will not be added to the list of possible encodings.
     * 
     *  @param encoding The declared encoding 
     *
     * @stable ICU 3.4
     */
    public CharsetDetector setDeclaredEncoding(String encoding) {
        setCanonicalDeclaredEncoding(encoding);
        return this;
    }
    
    /**
     * Set the input text (byte) data whose charset is to be detected.
     * 
     * @param in the input text of unknown encoding
     * 
     * @return This CharsetDetector
     *
     * @stable ICU 3.4
     */
    public CharsetDetector setText(byte [] in) {
        fRawInput  = in;
        fRawLength = in.length;
        
        MungeInput();
        
        return this;
    }
    
    private static final int kBufSize = 12000;

    private static final int MAX_CONFIDENCE = 100;

    /**
     * Set the input text (byte) data whose charset is to be detected.
     *  <p/>
     *   The input stream that supplies the character data must have markSupported()
     *   == true; the charset detection process will read a small amount of data,
     *   then return the stream to its original position via
     *   the InputStream.reset() operation.  The exact amount that will
     *   be read depends on the characteristics of the data itself.
     *
     * @param in the input text of unknown encoding
     * 
     * @return This CharsetDetector
     *
     * @stable ICU 3.4
     */
    
    public CharsetDetector setText(InputStream in) throws IOException {
        fInputStream = in;
        fInputStream.mark(kBufSize);
        fRawInput = new byte[kBufSize];   // Always make a new buffer because the
                                          //   previous one may have come from the caller,
                                          //   in which case we can't touch it.
        fRawLength = 0;
        int remainingLength = kBufSize;
        while (remainingLength > 0 ) {
            // read() may give data in smallish chunks, esp. for remote sources.  Hence, this loop.
            int  bytesRead = fInputStream.read(fRawInput, fRawLength, remainingLength);
            if (bytesRead <= 0) {
                 break;
            }
            fRawLength += bytesRead;
            remainingLength -= bytesRead;
        }
        fInputStream.reset();
        
        MungeInput();                     // Strip html markup, collect byte stats.
        return this;
    }

  
    /**
     * Return the charset that best matches the supplied input data.
     * 
     * Note though, that because the detection 
     * only looks at the start of the input data,
     * there is a possibility that the returned charset will fail to handle
     * the full set of input data.
     * <p/>
     * Raise an exception if 
     *  <ul>
     *    <li>no charset appears to match the data.</li>
     *    <li>no input text has been provided</li>
     *  </ul>
     *
     * @return a CharsetMatch object representing the best matching charset, or
     *         <code>null</code> if there are no matches.
     *
     * @stable ICU 3.4
     */
    public CharsetMatch detect() {
//   TODO:  A better implementation would be to copy the detect loop from
//          detectAll(), and cut it short as soon as a match with a high confidence
//          is found.  This is something to be done later, after things are otherwise
//          working.
        CharsetMatch matches[] = detectAll();
        
        if (matches == null || matches.length == 0) {
            return null;
        }
        
        return matches[0];
     }
    
    /**
     *  Return an array of all charsets that appear to be plausible
     *  matches with the input data.  The array is ordered with the
     *  best quality match first.
     * <p/>
     * Raise an exception if 
     *  <ul>
     *    <li>no charsets appear to match the input data.</li>
     *    <li>no input text has been provided</li>
     *  </ul>
     * 
     * @return An array of CharsetMatch objects representing possibly matching charsets.
     *
     * @stable ICU 3.4
     */
    public CharsetMatch[] detectAll() {
        CharsetRecognizer csr;
        int               i;
        int               detectResults;
        int               confidence;
        ArrayList<CharsetMatch> matches = new ArrayList<CharsetMatch>();
        
        //  Iterate over all possible charsets, remember all that
        //    give a match quality > 0.
        for (i=0; i<fCSRecognizers.size(); i++) {
            csr = fCSRecognizers.get(i);
            detectResults = csr.match(this);
            confidence = detectResults & 0x000000ff;
            if (confidence > 0) {
                // Just to be safe, constrain
                confidence = Math.min(confidence, MAX_CONFIDENCE);
                
                // Apply charset hint.
                if ((fDeclaredEncoding != null) && (fDeclaredEncoding.equalsIgnoreCase(csr.getName()))) {
                    // Reduce lack of confidence (delta between "sure" and current) by 50%.
                    confidence += (MAX_CONFIDENCE - confidence)/2;
                }
                
                CharsetMatch  m = new CharsetMatch(this, csr, confidence);
                matches.add(m);
            }
        }
        
        Collections.sort(matches);      // CharsetMatch compares on confidence
        Collections.reverse(matches);   //  Put best match first.
        CharsetMatch [] resultArray = new CharsetMatch[matches.size()];
        resultArray = matches.toArray(resultArray);
        return resultArray;
    }

    
    /**
     * Autodetect the charset of an inputStream, and return a Java Reader
     * to access the converted input data.
     * <p/>
     * This is a convenience method that is equivalent to
     *   <code>this.setDeclaredEncoding(declaredEncoding).setText(in).detect().getReader();</code>
     * <p/>
     *   For the input stream that supplies the character data, markSupported()
     *   must be true; the  charset detection will read a small amount of data,
     *   then return the stream to its original position via
     *   the InputStream.reset() operation.  The exact amount that will
     *    be read depends on the characteristics of the data itself.
     *<p/>
     * Raise an exception if no charsets appear to match the input data.
     * 
     * @param in The source of the byte data in the unknown charset.
     *
     * @param declaredEncoding  A declared encoding for the data, if available,
     *           or null or an empty string if none is available.
     *
     * @stable ICU 3.4
     */
    public Reader getReader(InputStream in, String declaredEncoding) {
        setCanonicalDeclaredEncoding(declaredEncoding);
        
        try {
            setText(in);
            
            CharsetMatch match = detect();
            
            if (match == null) {
                return null;
            }
            
            return match.getReader();
        } catch (IOException e) {
            return null;
        }
    }

    /**
     * Autodetect the charset of an inputStream, and return a String
     * containing the converted input data.
     * <p/>
     * This is a convenience method that is equivalent to
     *   <code>this.setDeclaredEncoding(declaredEncoding).setText(in).detect().getString();</code>
     *<p/>
     * Raise an exception if no charsets appear to match the input data.
     * 
     * @param in The source of the byte data in the unknown charset.
     *
     * @param declaredEncoding  A declared encoding for the data, if available,
     *           or null or an empty string if none is available.
     *
     * @stable ICU 3.4
     */
    public String getString(byte[] in, String declaredEncoding) {
        setCanonicalDeclaredEncoding(declaredEncoding);
       
        try {
            setText(in);
            
            CharsetMatch match = detect();
            
            if (match == null) {
                return null;
            }
            
            return match.getString(-1);
        } catch (IOException e) {
            return null;
        }
    }

 
    /**
     * Get the names of all char sets that can be recognized by the char set detector.
     *
     * @return an array of the names of all charsets that can be recognized
     * by the charset detector.
     *
     * @stable ICU 3.4
     */
    public static String[] getAllDetectableCharsets() {
        return fCharsetNames;
    }
    
    /**
     * Test whether or not input filtering is enabled.
     * 
     * @return <code>true</code> if input text will be filtered.
     * 
     * @see #enableInputFilter
     *
     * @stable ICU 3.4
     */
    public boolean inputFilterEnabled()
    {
        return fStripTags;
    }
    
    /**
     * Enable filtering of input text. If filtering is enabled,
     * text within angle brackets ("<" and ">") will be removed
     * before detection.
     * 
     * @param filter <code>true</code> to enable input text filtering.
     * 
     * @return The previous setting.
     *
     * @stable ICU 3.4
     */
    public boolean enableInputFilter(boolean filter)
    {
        boolean previous = fStripTags;
        
        fStripTags = filter;
        
        return previous;
    }
    
    /**
     * Try to set fDeclaredEncoding to the canonical name for <encoding>, if it exists.
     * 
     * @param encoding - name of character encoding
     */
    private void setCanonicalDeclaredEncoding(String encoding) {
        if ((encoding == null) || encoding.isEmpty()) {
            return;
        }
        
        Charset cs = Charset.forName(encoding);
        if (cs != null) {
            fDeclaredEncoding = cs.name();
        }
    }
    
    /*
     *  MungeInput - after getting a set of raw input data to be analyzed, preprocess
     *               it by removing what appears to be html markup.
     */
    private void MungeInput() {
        int srci = 0;
        int dsti = 0;
        byte b;
        boolean  inMarkup = false;
        int      openTags = 0;
        int      badTags  = 0;
        
        //
        //  html / xml markup stripping.
        //     quick and dirty, not 100% accurate, but hopefully good enough, statistically.
        //     discard everything within < brackets >
        //     Count how many total '<' and illegal (nested) '<' occur, so we can make some
        //     guess as to whether the input was actually marked up at all.
        if (fStripTags) {
            for (srci = 0; srci < fRawLength && dsti < fInputBytes.length; srci++) {
                b = fRawInput[srci];
                if (b == (byte)'<') {
                    if (inMarkup) {
                        badTags++;
                    }
                    inMarkup = true;
                    openTags++;
                }
                
                if (! inMarkup) {
                    fInputBytes[dsti++] = b;
                }
                
                if (b == (byte)'>') {
                    inMarkup = false;
                }        
            }
            
            fInputLen = dsti;
        }
        
        //
        //  If it looks like this input wasn't marked up, or if it looks like it's
        //    essentially nothing but markup abandon the markup stripping.
        //    Detection will have to work on the unstripped input.
        //
        if (openTags<5 || openTags/5 < badTags || 
                (fInputLen < 100 && fRawLength>600)) {
            int limit = fRawLength;
            
            if (limit > kBufSize) {
                limit = kBufSize;
            }
            
            for (srci=0; srci<limit; srci++) {
                fInputBytes[srci] = fRawInput[srci];
            }
            fInputLen = srci;
        }
        
        //
        // Tally up the byte occurence statistics.
        //   These are available for use by the various detectors.
        //
        Arrays.fill(fByteStats, (short)0);
        for (srci=0; srci<fInputLen; srci++) {
            int val = fInputBytes[srci] & 0x00ff;
            fByteStats[val]++;
        }
        
        fC1Bytes = false;
        for (int i = 0x80; i <= 0x9F; i += 1) {
            if (fByteStats[i] != 0) {
                fC1Bytes = true;
                break;
            }
        }
     }

    /*
     *  The following items are accessed by individual CharsetRecongizers during
     *     the recognition process
     * 
     */
    byte[]      fInputBytes =       // The text to be checked.  Markup will have been
                   new byte[kBufSize];  //   removed if appropriate.
    
    int         fInputLen;          // Length of the byte data in fInputText.
    
    short       fByteStats[] =      // byte frequency statistics for the input text.
                   new short[256];  //   Value is percent, not absolute.
                                    //   Value is rounded up, so zero really means zero occurences.
    
    boolean     fC1Bytes =          // True if any bytes in the range 0x80 - 0x9F are in the input;
                   false;
    
    String      fDeclaredEncoding;
    
    

    //
    //  Stuff private to CharsetDetector
    //
    byte[]               fRawInput;     // Original, untouched input bytes.
                                        //  If user gave us a byte array, this is it.
                                        //  If user gave us a stream, it's read to a 
                                        //  buffer here.
    int                  fRawLength;    // Length of data in fRawInput array.
    
    InputStream          fInputStream;  // User's input stream, or null if the user
                                        //   gave us a byte array.
     
    boolean              fStripTags =   // If true, setText() will strip tags from input text.
                           false;
    
    
    /*
     * List of recognizers for all charsets known to the implementation.
     */
    private static ArrayList<CharsetRecognizer> fCSRecognizers = createRecognizers();
    private static String [] fCharsetNames;
    
    /*
     * Create the singleton instances of the CharsetRecognizer classes
     */
    private static ArrayList<CharsetRecognizer> createRecognizers() {
        ArrayList<CharsetRecognizer> recognizers = new ArrayList<CharsetRecognizer>();
        
        recognizers.add(new CharsetRecog_UTF8());
        
        recognizers.add(new CharsetRecog_Unicode.CharsetRecog_UTF_16_BE());
        recognizers.add(new CharsetRecog_Unicode.CharsetRecog_UTF_16_LE());
        recognizers.add(new CharsetRecog_Unicode.CharsetRecog_UTF_32_BE());
        recognizers.add(new CharsetRecog_Unicode.CharsetRecog_UTF_32_LE());
        
        recognizers.add(new CharsetRecog_mbcs.CharsetRecog_sjis());
        recognizers.add(new CharsetRecog_2022.CharsetRecog_2022JP());
        recognizers.add(new CharsetRecog_2022.CharsetRecog_2022CN());
        recognizers.add(new CharsetRecog_2022.CharsetRecog_2022KR());
        recognizers.add(new CharsetRecog_mbcs.CharsetRecog_euc.CharsetRecog_gb_18030());
        recognizers.add(new CharsetRecog_mbcs.CharsetRecog_euc.CharsetRecog_euc_jp());
        recognizers.add(new CharsetRecog_mbcs.CharsetRecog_euc.CharsetRecog_euc_kr());
        recognizers.add(new CharsetRecog_mbcs.CharsetRecog_big5());
        
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_8859_1_da());
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_8859_1_de());
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_8859_1_en());
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_8859_1_es());
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_8859_1_fr());
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_8859_1_it());
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_8859_1_nl());
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_8859_1_no());
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_8859_1_pt());
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_8859_1_sv());
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_8859_2_cs());
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_8859_2_hu());
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_8859_2_pl());
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_8859_2_ro());
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_8859_5_ru());
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_8859_6_ar());
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_8859_7_el());
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_8859_8_I_he());
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_8859_8_he());
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_windows_1251());
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_windows_1256());
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_KOI8_R());
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_8859_9_tr());
        
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_IBM424_he_rtl());
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_IBM424_he_ltr());
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_IBM420_ar_rtl());
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_IBM420_ar_ltr());

        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_EBCDIC_500_en());
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_EBCDIC_500_de());
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_EBCDIC_500_es());
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_EBCDIC_500_fr());
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_EBCDIC_500_it());
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_EBCDIC_500_nl());
        
        recognizers.add(new CharsetRecog_sbcs.CharsetRecog_IBM866_ru());

        // Create an array of all charset names, as a side effect.
        // Needed for the getAllDetectableCharsets() API.
        String[] charsetNames = new String [recognizers.size()];
        int out = 0;
        
        for (CharsetRecognizer recognizer : recognizers) {
            String name = recognizer.getName();
            
            if (out == 0 || ! name.equals(charsetNames[out - 1])) {
                charsetNames[out++] = name;
            }
        }
        
        fCharsetNames = new String[out];
        System.arraycopy(charsetNames, 0, fCharsetNames, 0, out);
        
        return recognizers;
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/txt/CharsetMatch.java,true,"/**
*******************************************************************************
* Copyright (C) 2005-2007, International Business Machines Corporation and    *
* others. All Rights Reserved.                                                *
*******************************************************************************
*/
package org.apache.tika.parser.txt;

import java.io.ByteArrayInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.io.Reader;


/**
 * This class represents a charset that has been identified by a CharsetDetector
 * as a possible encoding for a set of input data.  From an instance of this
 * class, you can ask for a confidence level in the charset identification,
 * or for Java Reader or String to access the original byte data in Unicode form.
 * <p/>
 * Instances of this class are created only by CharsetDetectors.
 * <p/>
 * Note:  this class has a natural ordering that is inconsistent with equals.
 *        The natural ordering is based on the match confidence value.
 *
 * @stable ICU 3.4
 */
public class CharsetMatch implements Comparable<CharsetMatch> {

    
    /**
     * Create a java.io.Reader for reading the Unicode character data corresponding
     * to the original byte data supplied to the Charset detect operation.
     * <p/>
     * CAUTION:  if the source of the byte data was an InputStream, a Reader
     * can be created for only one matching char set using this method.  If more 
     * than one charset needs to be tried, the caller will need to reset
     * the InputStream and create InputStreamReaders itself, based on the charset name.
     *
     * @return the Reader for the Unicode character data.
     *
     * @stable ICU 3.4
     */
    public Reader getReader() {
        InputStream inputStream = fInputStream;
        
        if (inputStream == null) {
            inputStream = new ByteArrayInputStream(fRawInput, 0, fRawLength);
        }
        
        try {
            inputStream.reset();
            return new InputStreamReader(inputStream, getName());
        } catch (IOException e) {
            return null;
        }
    }

    /**
     * Create a Java String from Unicode character data corresponding
     * to the original byte data supplied to the Charset detect operation.
     *
     * @return a String created from the converted input data.
     *
     * @stable ICU 3.4
     */
    public String getString()  throws java.io.IOException {
        return getString(-1);

    }

    /**
     * Create a Java String from Unicode character data corresponding
     * to the original byte data supplied to the Charset detect operation.
     * The length of the returned string is limited to the specified size;
     * the string will be trunctated to this length if necessary.  A limit value of
     * zero or less is ignored, and treated as no limit.
     *
     * @param maxLength The maximium length of the String to be created when the
     *                  source of the data is an input stream, or -1 for
     *                  unlimited length.
     * @return a String created from the converted input data.
     *
     * @stable ICU 3.4
     */
    public String getString(int maxLength) throws java.io.IOException {
        String result = null;
        if (fInputStream != null) {
            StringBuffer sb = new StringBuffer();
            char[] buffer = new char[1024];
            Reader reader = getReader();
            int max = maxLength < 0? Integer.MAX_VALUE : maxLength;
            int bytesRead = 0;
            
            while ((bytesRead = reader.read(buffer, 0, Math.min(max, 1024))) >= 0) {
                sb.append(buffer, 0, bytesRead);
                max -= bytesRead;
            }
            
            reader.close();
            
            return sb.toString();
        } else {
            result = new String(fRawInput, getName());            
        }
        return result;

    }
    
    /**
     * Get an indication of the confidence in the charset detected.
     * Confidence values range from 0-100, with larger numbers indicating
     * a better match of the input data to the characteristics of the
     * charset.
     *
     * @return the confidence in the charset match
     *
     * @stable ICU 3.4
     */
    public int getConfidence() {
        return fConfidence;
    }
    

    /**
     * Bit flag indicating the match is based on the the encoding scheme.
     *
     * @see #getMatchType
     * @stable ICU 3.4
     */
    static public final int ENCODING_SCHEME    = 1;
    
    /**
     * Bit flag indicating the match is based on the presence of a BOM.
     * 
     * @see #getMatchType
     * @stable ICU 3.4
     */
    static public final int BOM                = 2;
    
    /**
     * Bit flag indicating he match is based on the declared encoding.
     * 
     * @see #getMatchType
     * @stable ICU 3.4
     */
    static public final int DECLARED_ENCODING  = 4;
    
    /**
     * Bit flag indicating the match is based on language statistics.
     *
     * @see #getMatchType
     * @stable ICU 3.4
     */
    static public final int LANG_STATISTICS    = 8;
    
    /**
     * Return flags indicating what it was about the input data 
     * that caused this charset to be considered as a possible match.
     * The result is a bitfield containing zero or more of the flags
     * ENCODING_SCHEME, BOM, DECLARED_ENCODING, and LANG_STATISTICS.
     * A result of zero means no information is available.
     * <p>
     * Note: currently, this method always returns zero.
     * <p>
     *
     * @return the type of match found for this charset.
     *
     * @draft ICU 3.4
     * @provisional This API might change or be removed in a future release.
     */
    public int getMatchType() {
//      TODO: create a list of enum-like constants for common combinations of types of matches.
        return 0;
    }

    /**
     * Get the name of the detected charset.  
     * The name will be one that can be used with other APIs on the
     * platform that accept charset names.  It is the "Canonical name"
     * as defined by the class java.nio.charset.Charset; for
     * charsets that are registered with the IANA charset registry,
     * this is the MIME-preferred registerd name.
     *
     * @see java.nio.charset.Charset
     * @see java.io.InputStreamReader
     *
     * @return The name of the charset.
     *
     * @stable ICU 3.4
     */
    public String getName() {
        return fRecognizer.getName();
    }
    
    /**
     * Get the ISO code for the language of the detected charset.  
     *
     * @return The ISO code for the language or <code>null</code> if the language cannot be determined.
     *
     * @stable ICU 3.4
     */
    public String getLanguage() {
        return fRecognizer.getLanguage();
    }

    /**
     * Compare to other CharsetMatch objects.
     * Comparison is based on the match confidence value, which 
     *   allows CharsetDetector.detectAll() to order its results. 
     *
     * @param o the CharsetMatch object to compare against.
     * @return  a negative integer, zero, or a positive integer as the 
     *          confidence level of this CharsetMatch
     *          is less than, equal to, or greater than that of
     *          the argument.
     * @throws ClassCastException if the argument is not a CharsetMatch.
     * @stable ICU 3.4
     */
    public int compareTo(CharsetMatch other) {
        int compareResult = 0;
        if (this.fConfidence > other.fConfidence) {
            compareResult = 1;
        } else if (this.fConfidence < other.fConfidence) {
            compareResult = -1;
        }
        return compareResult;
    }

    /**
     * compare this CharsetMatch to another based on confidence value
     * @param o the CharsetMatch object to compare against
     * @return true if equal
     */
    public boolean equals(Object o) {
        if (o instanceof CharsetMatch) {
            CharsetMatch that = (CharsetMatch) o;
            return (this.fConfidence == that.fConfidence);
        }

        return false;
    }

    /**
     * generates a hashCode based on the confidence value
     * @return the hashCode
     */
    public int hashCode() {
        return fConfidence;
    }
    
    /*
     *  Constructor.  Implementation internal
     */
    CharsetMatch(CharsetDetector det, CharsetRecognizer rec, int conf) {
        fRecognizer = rec;
        fConfidence = conf;
        
        // The references to the original aplication input data must be copied out
        //   of the charset recognizer to here, in case the application resets the
        //   recognizer before using this CharsetMatch.
        if (det.fInputStream == null) {
            // We only want the existing input byte data if it came straight from the user,
            //   not if is just the head of a stream.
            fRawInput    = det.fRawInput;
            fRawLength   = det.fRawLength;
        }
        fInputStream = det.fInputStream;
    }

    
    //
    //   Private Data
    //
    private int                 fConfidence;
    private CharsetRecognizer   fRecognizer;
    private byte[]              fRawInput = null;     // Original, untouched input bytes.
                                                      //  If user gave us a byte array, this is it.
    private int                 fRawLength;           // Length of data in fRawInput array.

    private InputStream         fInputStream = null;  // User's input stream, or null if the user
                                                      //   gave us a byte array.

    public String toString() {
       String s = "Match of " + fRecognizer.getName();
       if(fRecognizer.getLanguage() != null) {
          s += " in " + fRecognizer.getLanguage();
       }
       s += " with confidence " + fConfidence;
       return s;
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/txt/CharsetRecognizer.java,false,"/**
*******************************************************************************
* Copyright (C) 2005, International Business Machines Corporation and         *
* others. All Rights Reserved.                                                *
*******************************************************************************
*/
package org.apache.tika.parser.txt;

/**
 * Abstract class for recognizing a single charset.
 * Part of the implementation of ICU's CharsetDetector.
 * 
 * Each specific charset that can be recognized will have an instance
 * of some subclass of this class.  All interaction between the overall
 * CharsetDetector and the stuff specific to an individual charset happens
 * via the interface provided here.
 * 
 * Instances of CharsetDetector DO NOT have or maintain 
 * state pertaining to a specific match or detect operation.
 * The WILL be shared by multiple instances of CharsetDetector.
 * They encapsulate const charset-specific information.
 * 
 * @internal
 */
abstract class CharsetRecognizer {
    /**
     * Get the IANA name of this charset.
     * @return the charset name.
     */
    abstract String      getName();
    
    /**
     * Get the ISO language code for this charset.
     * @return the language code, or <code>null</code> if the language cannot be determined.
     */
    public   String      getLanguage()
    {
        return null;
    }
    
    /**
     * Test the match of this charset with the input text data
     *      which is obtained via the CharsetDetector object.
     * 
     * @param det  The CharsetDetector, which contains the input text
     *             to be checked for being in this charset.
     * @return     Two values packed into one int  (Damn java, anyhow)
     *             <br/>
     *             bits 0-7:  the match confidence, ranging from 0-100
     *             <br/>
     *             bits 8-15: The match reason, an enum-like value.
     */
    abstract int         match(CharsetDetector det);

}
"
tika-parsers/src/main/java/org/apache/tika/parser/txt/CharsetRecog_2022.java,true,"/*
*******************************************************************************
* Copyright (C) 2005 - 2008, International Business Machines Corporation and  *
* others. All Rights Reserved.                                                *
*******************************************************************************
*/
package org.apache.tika.parser.txt;

/**
 *  class CharsetRecog_2022  part of the ICU charset detection imlementation.
 *                           This is a superclass for the individual detectors for
 *                           each of the detectable members of the ISO 2022 family
 *                           of encodings.
 * 
 *                           The separate classes are nested within this class.
 * 
 * @internal
 */
abstract class CharsetRecog_2022 extends CharsetRecognizer {

    
    /**
     * Matching function shared among the 2022 detectors JP, CN and KR
     * Counts up the number of legal an unrecognized escape sequences in
     * the sample of text, and computes a score based on the total number &
     * the proportion that fit the encoding.
     * 
     * 
     * @param text the byte buffer containing text to analyse
     * @param textLen  the size of the text in the byte.
     * @param escapeSequences the byte escape sequences to test for.
     * @return match quality, in the range of 0-100.
     */
    int   match(byte [] text, int textLen, byte [][] escapeSequences) {
        int     i, j;
        int     escN;
        int     hits   = 0;
        int     misses = 0;
        int     shifts = 0;
        int     quality;
        scanInput:
            for (i=0; i<textLen; i++) {
                if (text[i] == 0x1b) {
                    checkEscapes:
                        for (escN=0; escN<escapeSequences.length; escN++) {
                            byte [] seq = escapeSequences[escN];
                            
                            if ((textLen - i) < seq.length) {
                                continue checkEscapes;
                            }
                            
                            for (j=1; j<seq.length; j++) {
                                if (seq[j] != text[i+j])  {
                                    continue checkEscapes;
                                }                                   
                            }
                            
                            hits++; 
                            i += seq.length-1;
                            continue scanInput;
                        }
                
                        misses++;                  
                }
                
                if (text[i] == 0x0e || text[i] == 0x0f) {
                    // Shift in/out
                    shifts++;
                }
            }
        
        if (hits == 0) {
            return 0;
        }
        
        //
        // Initial quality is based on relative proportion of recongized vs.
        //   unrecognized escape sequences. 
        //   All good:  quality = 100;
        //   half or less good: quality = 0;
        //   linear inbetween.
        quality = (100*hits - 100*misses) / (hits + misses);
        
        // Back off quality if there were too few escape sequences seen.
        //   Include shifts in this computation, so that KR does not get penalized
        //   for having only a single Escape sequence, but many shifts.
        if (hits+shifts < 5) {
            quality -= (5-(hits+shifts))*10;
        }
        
        if (quality < 0) {
            quality = 0;
        }        
        return quality;
    }

    
 
    
    static class CharsetRecog_2022JP extends CharsetRecog_2022 {
        private byte [] [] escapeSequences = {
                {0x1b, 0x24, 0x28, 0x43},   // KS X 1001:1992
                {0x1b, 0x24, 0x28, 0x44},   // JIS X 212-1990
                {0x1b, 0x24, 0x40},         // JIS C 6226-1978
                {0x1b, 0x24, 0x41},         // GB 2312-80
                {0x1b, 0x24, 0x42},         // JIS X 208-1983
                {0x1b, 0x26, 0x40},         // JIS X 208 1990, 1997
                {0x1b, 0x28, 0x42},         // ASCII
                {0x1b, 0x28, 0x48},         // JIS-Roman
                {0x1b, 0x28, 0x49},         // Half-width katakana
                {0x1b, 0x28, 0x4a},         // JIS-Roman
                {0x1b, 0x2e, 0x41},         // ISO 8859-1
                {0x1b, 0x2e, 0x46}          // ISO 8859-7
                };
        
        String getName() {
            return "ISO-2022-JP";
        }
        
        int   match(CharsetDetector det) {
            return match(det.fInputBytes, det.fInputLen, escapeSequences);
        }
    }

    static class CharsetRecog_2022KR extends CharsetRecog_2022 {
        private byte [] [] escapeSequences = {
                {0x1b, 0x24, 0x29, 0x43}   
                 };
        
        String getName() {
            return "ISO-2022-KR";
        }
        
        int   match(CharsetDetector det) {
            return match(det.fInputBytes, det.fInputLen, escapeSequences);
        }
        
    }

    static class CharsetRecog_2022CN extends CharsetRecog_2022 {
        private byte [] [] escapeSequences = {
                {0x1b, 0x24, 0x29, 0x41},   // GB 2312-80
                {0x1b, 0x24, 0x29, 0x47},   // CNS 11643-1992 Plane 1
                {0x1b, 0x24, 0x2A, 0x48},   // CNS 11643-1992 Plane 2
                {0x1b, 0x24, 0x29, 0x45},   // ISO-IR-165
                {0x1b, 0x24, 0x2B, 0x49},   // CNS 11643-1992 Plane 3
                {0x1b, 0x24, 0x2B, 0x4A},   // CNS 11643-1992 Plane 4
                {0x1b, 0x24, 0x2B, 0x4B},   // CNS 11643-1992 Plane 5
                {0x1b, 0x24, 0x2B, 0x4C},   // CNS 11643-1992 Plane 6
                {0x1b, 0x24, 0x2B, 0x4D},   // CNS 11643-1992 Plane 7
                {0x1b, 0x4e},               // SS2
                {0x1b, 0x4f},               // SS3
        };
        
        String getName() {
            return "ISO-2022-CN";
        }
        
        
        int   match(CharsetDetector det) {
            return match(det.fInputBytes, det.fInputLen, escapeSequences);
        }
    }
    
    }

"
tika-parsers/src/main/java/org/apache/tika/parser/txt/CharsetRecog_mbcs.java,false,"/*
 ****************************************************************************
 * Copyright (C) 2005-2008, International Business Machines Corporation and *
 * others. All Rights Reserved.                                             *
 ****************************************************************************
 *
 */
package org.apache.tika.parser.txt;

import java.util.Arrays;

/**
 * CharsetRecognizer implemenation for Asian  - double or multi-byte - charsets.
 *                   Match is determined mostly by the input data adhering to the
 *                   encoding scheme for the charset, and, optionally,
 *                   frequency-of-occurence of characters.
 * <p/>
 *                   Instances of this class are singletons, one per encoding
 *                   being recognized.  They are created in the main
 *                   CharsetDetector class and kept in the global list of available
 *                   encodings to be checked.  The specific encoding being recognized
 *                   is determined by subclass.
 * 
 * @internal                  
 */
abstract class CharsetRecog_mbcs extends CharsetRecognizer {

   /**
     * Get the IANA name of this charset.
     * @return the charset name.
     */
    abstract String      getName() ;
    
    
    /**
     * Test the match of this charset with the input text data
     *      which is obtained via the CharsetDetector object.
     * 
     * @param det  The CharsetDetector, which contains the input text
     *             to be checked for being in this charset.
     * @return     Two values packed into one int  (Damn java, anyhow)
     *             <br/>
     *             bits 0-7:  the match confidence, ranging from 0-100
     *             <br/>
     *             bits 8-15: The match reason, an enum-like value.
     */
    int match(CharsetDetector det, int [] commonChars) {
        int   singleByteCharCount = 0;
        int   doubleByteCharCount = 0;
        int   commonCharCount     = 0;
        int   badCharCount        = 0;
        int   totalCharCount      = 0;
        int   confidence          = 0;
        iteratedChar   iter       = new iteratedChar();
        
        detectBlock: {
            for (iter.reset(); nextChar(iter, det);) {
                totalCharCount++;
                if (iter.error) {
                    badCharCount++; 
                } else {
                    long cv = iter.charValue & 0xFFFFFFFFL;
                                        
                    if (cv <= 0xff) {
                        singleByteCharCount++;
                    } else {
                        doubleByteCharCount++;
                        if (commonChars != null) {
                            // NOTE: This assumes that there are no 4-byte common chars.
                            if (Arrays.binarySearch(commonChars, (int) cv) >= 0) {
                                commonCharCount++;
                            }
                        }
                    }
                }
                if (badCharCount >= 2 && badCharCount*5 >= doubleByteCharCount) {
                    // Bail out early if the byte data is not matching the encoding scheme.
                    break detectBlock;
                }
            }
            
            if (doubleByteCharCount <= 10 && badCharCount== 0) {
                // Not many multi-byte chars.
                if (doubleByteCharCount == 0 && totalCharCount < 10) {
                    // There weren't any multibyte sequences, and there was a low density of non-ASCII single bytes.
                    // We don't have enough data to have any confidence.
                    // Statistical analysis of single byte non-ASCII charcters would probably help here.
                    confidence = 0;
                }
                else {
                    //   ASCII or ISO file?  It's probably not our encoding,
                    //   but is not incompatible with our encoding, so don't give it a zero.
                    confidence = 10;
                }
                
                break detectBlock;
            }
            
            //
            //  No match if there are too many characters that don't fit the encoding scheme.
            //    (should we have zero tolerance for these?)
            //
            if (doubleByteCharCount < 20*badCharCount) {
                confidence = 0;
                break detectBlock;
            }
            
            if (commonChars == null) {
                // We have no statistics on frequently occuring characters.
                //  Assess confidence purely on having a reasonable number of
                //  multi-byte characters (the more the better
                confidence = 30 + doubleByteCharCount - 20*badCharCount;
                if (confidence > 100) {
                    confidence = 100;
                }
            }else {
                //
                // Frequency of occurence statistics exist.
                //
                double maxVal = Math.log((float)doubleByteCharCount / 4);
                double scaleFactor = 90.0 / maxVal;
                confidence = (int)(Math.log(commonCharCount+1) * scaleFactor + 10);
                confidence = Math.min(confidence, 100);
            }
        }   // end of detectBlock:
        
        return confidence;
    }
    
     // "Character"  iterated character class.
     //    Recognizers for specific mbcs encodings make their "characters" available
     //    by providing a nextChar() function that fills in an instance of iteratedChar
     //    with the next char from the input.
     //    The returned characters are not converted to Unicode, but remain as the raw
     //    bytes (concatenated into an int) from the codepage data.
     //
     //  For Asian charsets, use the raw input rather than the input that has been
     //   stripped of markup.  Detection only considers multi-byte chars, effectively
     //   stripping markup anyway, and double byte chars do occur in markup too.
     //
     static class iteratedChar {
         int             charValue = 0;             // 1-4 bytes from the raw input data
         int             index     = 0;
         int             nextIndex = 0;
         boolean         error     = false;
         boolean         done      = false;
         
         void reset() {
             charValue = 0;
             index     = -1;
             nextIndex = 0;
             error     = false;
             done      = false;
         }
         
         int nextByte(CharsetDetector det) {
             if (nextIndex >= det.fRawLength) {
                 done = true;
                 return -1;
             }
             int byteValue = (int)det.fRawInput[nextIndex++] & 0x00ff;
             return byteValue;
         }       
     }
     
     /**
      * Get the next character (however many bytes it is) from the input data
      *    Subclasses for specific charset encodings must implement this function
      *    to get characters according to the rules of their encoding scheme.
      * 
      *  This function is not a method of class iteratedChar only because
      *   that would require a lot of extra derived classes, which is awkward.
      * @param it  The iteratedChar "struct" into which the returned char is placed.
      * @param det The charset detector, which is needed to get at the input byte data
      *            being iterated over.
      * @return    True if a character was returned, false at end of input.
      */
     abstract boolean nextChar(iteratedChar it, CharsetDetector det);
     


     
     
     /**
      *   Shift-JIS charset recognizer.   
      *
      */
     static class CharsetRecog_sjis extends CharsetRecog_mbcs {
         static int [] commonChars = 
             // TODO:  This set of data comes from the character frequency-
             //        of-occurence analysis tool.  The data needs to be moved
             //        into a resource and loaded from there.
            {0x8140, 0x8141, 0x8142, 0x8145, 0x815b, 0x8169, 0x816a, 0x8175, 0x8176, 0x82a0, 
             0x82a2, 0x82a4, 0x82a9, 0x82aa, 0x82ab, 0x82ad, 0x82af, 0x82b1, 0x82b3, 0x82b5, 
             0x82b7, 0x82bd, 0x82be, 0x82c1, 0x82c4, 0x82c5, 0x82c6, 0x82c8, 0x82c9, 0x82cc, 
             0x82cd, 0x82dc, 0x82e0, 0x82e7, 0x82e8, 0x82e9, 0x82ea, 0x82f0, 0x82f1, 0x8341, 
             0x8343, 0x834e, 0x834f, 0x8358, 0x835e, 0x8362, 0x8367, 0x8375, 0x8376, 0x8389, 
             0x838a, 0x838b, 0x838d, 0x8393, 0x8e96, 0x93fa, 0x95aa};
         
         boolean nextChar(iteratedChar it, CharsetDetector det) {
             it.index = it.nextIndex;
             it.error = false;
             int firstByte;
             firstByte = it.charValue = it.nextByte(det);
             if (firstByte < 0) {
                 return false;
             }
             
             if (firstByte <= 0x7f || (firstByte>0xa0 && firstByte<=0xdf)) {
                 return true;
             }
             
             int secondByte = it.nextByte(det);
             if (secondByte < 0)  {
                 return false;          
             }
             it.charValue = (firstByte << 8) | secondByte;
             if (! ((secondByte>=0x40 && secondByte<=0x7f) || (secondByte>=0x80 && secondByte<=0xff))) {
                 // Illegal second byte value.
                 it.error = true;
             }
             return true;
         }
         
         int match(CharsetDetector det) {
             return match(det, commonChars);
         }
         
         String getName() {
             return "Shift_JIS";
         }
         
         public String getLanguage()
         {
             return "ja";
         }

         
     }
     
     
     /**
      *   Big5 charset recognizer.   
      *
      */
     static class CharsetRecog_big5 extends CharsetRecog_mbcs {
         static int [] commonChars = 
             // TODO:  This set of data comes from the character frequency-
             //        of-occurence analysis tool.  The data needs to be moved
             //        into a resource and loaded from there.
            {0xa140, 0xa141, 0xa142, 0xa143, 0xa147, 0xa149, 0xa175, 0xa176, 0xa440, 0xa446, 
             0xa447, 0xa448, 0xa451, 0xa454, 0xa457, 0xa464, 0xa46a, 0xa46c, 0xa477, 0xa4a3, 
             0xa4a4, 0xa4a7, 0xa4c1, 0xa4ce, 0xa4d1, 0xa4df, 0xa4e8, 0xa4fd, 0xa540, 0xa548, 
             0xa558, 0xa569, 0xa5cd, 0xa5e7, 0xa657, 0xa661, 0xa662, 0xa668, 0xa670, 0xa6a8, 
             0xa6b3, 0xa6b9, 0xa6d3, 0xa6db, 0xa6e6, 0xa6f2, 0xa740, 0xa751, 0xa759, 0xa7da, 
             0xa8a3, 0xa8a5, 0xa8ad, 0xa8d1, 0xa8d3, 0xa8e4, 0xa8fc, 0xa9c0, 0xa9d2, 0xa9f3, 
             0xaa6b, 0xaaba, 0xaabe, 0xaacc, 0xaafc, 0xac47, 0xac4f, 0xacb0, 0xacd2, 0xad59, 
             0xaec9, 0xafe0, 0xb0ea, 0xb16f, 0xb2b3, 0xb2c4, 0xb36f, 0xb44c, 0xb44e, 0xb54c, 
             0xb5a5, 0xb5bd, 0xb5d0, 0xb5d8, 0xb671, 0xb7ed, 0xb867, 0xb944, 0xbad8, 0xbb44, 
             0xbba1, 0xbdd1, 0xc2c4, 0xc3b9, 0xc440, 0xc45f};
          
         boolean nextChar(iteratedChar it, CharsetDetector det) {
             it.index = it.nextIndex;
             it.error = false;
             int firstByte;
             firstByte = it.charValue = it.nextByte(det);
             if (firstByte < 0) {
                 return false;
             }
             
             if (firstByte <= 0x7f || firstByte==0xff) {
                 // single byte character.
                 return true;
             }
             
             int secondByte = it.nextByte(det);
             if (secondByte < 0)  {
                 return false;          
             }
             it.charValue = (it.charValue << 8) | secondByte;

             if (secondByte < 0x40 ||
                 secondByte ==0x7f ||
                 secondByte == 0xff) {
                     it.error = true;
             }
             return true;
         }
         
         int match(CharsetDetector det) {
             return match(det, commonChars);
         }
         
         String getName() {
             return "Big5";
         }
         
         
         public String getLanguage()
         {
             return "zh";
         }
     }
     
     
     /**
      *   EUC charset recognizers.  One abstract class that provides the common function
      *             for getting the next character according to the EUC encoding scheme,
      *             and nested derived classes for EUC_KR, EUC_JP, EUC_CN.   
      *
      */
     abstract static class CharsetRecog_euc extends CharsetRecog_mbcs {
         
         /*
          *  (non-Javadoc)
          *  Get the next character value for EUC based encodings.
          *  Character "value" is simply the raw bytes that make up the character
          *     packed into an int.
          */
         boolean nextChar(iteratedChar it, CharsetDetector det) {
             it.index = it.nextIndex;
             it.error = false;
             int firstByte  = 0;
             int secondByte = 0;
             int thirdByte  = 0;
             //int fourthByte = 0;
             
             buildChar: {
                 firstByte = it.charValue = it.nextByte(det);                 
                 if (firstByte < 0) {
                     // Ran off the end of the input data
                     it.done = true;
                     break buildChar;
                 }
                 if (firstByte <= 0x8d) {
                     // single byte char
                     break buildChar;
                 }
                 
                 secondByte = it.nextByte(det);
                 it.charValue = (it.charValue << 8) | secondByte;
                 
                 if (firstByte >= 0xA1 && firstByte <= 0xfe) {
                     // Two byte Char
                     if (secondByte < 0xa1) {
                         it.error = true;
                     }
                     break buildChar;
                 }
                 if (firstByte == 0x8e) {
                     // Code Set 2.
                     //   In EUC-JP, total char size is 2 bytes, only one byte of actual char value.
                     //   In EUC-TW, total char size is 4 bytes, three bytes contribute to char value.
                     // We don't know which we've got.
                     // Treat it like EUC-JP.  If the data really was EUC-TW, the following two
                     //   bytes will look like a well formed 2 byte char.  
                     if (secondByte < 0xa1) {
                         it.error = true;
                     }
                     break buildChar;                     
                 }
                 
                 if (firstByte == 0x8f) {
                     // Code set 3.
                     // Three byte total char size, two bytes of actual char value.
                     thirdByte    = it.nextByte(det);
                     it.charValue = (it.charValue << 8) | thirdByte;
                     if (thirdByte < 0xa1) {
                         it.error = true;
                     }
                 }
              }
             
             return (it.done == false);
         }
         
         /**
          * The charset recognize for EUC-JP.  A singleton instance of this class
          *    is created and kept by the public CharsetDetector class
          */
         static class CharsetRecog_euc_jp extends CharsetRecog_euc {
             static int [] commonChars = 
                 // TODO:  This set of data comes from the character frequency-
                 //        of-occurence analysis tool.  The data needs to be moved
                 //        into a resource and loaded from there.
                {0xa1a1, 0xa1a2, 0xa1a3, 0xa1a6, 0xa1bc, 0xa1ca, 0xa1cb, 0xa1d6, 0xa1d7, 0xa4a2, 
                 0xa4a4, 0xa4a6, 0xa4a8, 0xa4aa, 0xa4ab, 0xa4ac, 0xa4ad, 0xa4af, 0xa4b1, 0xa4b3, 
                 0xa4b5, 0xa4b7, 0xa4b9, 0xa4bb, 0xa4bd, 0xa4bf, 0xa4c0, 0xa4c1, 0xa4c3, 0xa4c4, 
                 0xa4c6, 0xa4c7, 0xa4c8, 0xa4c9, 0xa4ca, 0xa4cb, 0xa4ce, 0xa4cf, 0xa4d0, 0xa4de, 
                 0xa4df, 0xa4e1, 0xa4e2, 0xa4e4, 0xa4e8, 0xa4e9, 0xa4ea, 0xa4eb, 0xa4ec, 0xa4ef, 
                 0xa4f2, 0xa4f3, 0xa5a2, 0xa5a3, 0xa5a4, 0xa5a6, 0xa5a7, 0xa5aa, 0xa5ad, 0xa5af, 
                 0xa5b0, 0xa5b3, 0xa5b5, 0xa5b7, 0xa5b8, 0xa5b9, 0xa5bf, 0xa5c3, 0xa5c6, 0xa5c7, 
                 0xa5c8, 0xa5c9, 0xa5cb, 0xa5d0, 0xa5d5, 0xa5d6, 0xa5d7, 0xa5de, 0xa5e0, 0xa5e1, 
                 0xa5e5, 0xa5e9, 0xa5ea, 0xa5eb, 0xa5ec, 0xa5ed, 0xa5f3, 0xb8a9, 0xb9d4, 0xbaee, 
                 0xbbc8, 0xbef0, 0xbfb7, 0xc4ea, 0xc6fc, 0xc7bd, 0xcab8, 0xcaf3, 0xcbdc, 0xcdd1};             
             String getName() {
                 return "EUC-JP";
             }
             
             int match(CharsetDetector det) {
                 return match(det, commonChars);
             }
             
             public String getLanguage()
             {
                 return "ja";
             }
         }
         
         /**
          * The charset recognize for EUC-KR.  A singleton instance of this class
          *    is created and kept by the public CharsetDetector class
          */
         static class CharsetRecog_euc_kr extends CharsetRecog_euc {
             static int [] commonChars = 
                 // TODO:  This set of data comes from the character frequency-
                 //        of-occurence analysis tool.  The data needs to be moved
                 //        into a resource and loaded from there.
                {0xb0a1, 0xb0b3, 0xb0c5, 0xb0cd, 0xb0d4, 0xb0e6, 0xb0ed, 0xb0f8, 0xb0fa, 0xb0fc, 
                 0xb1b8, 0xb1b9, 0xb1c7, 0xb1d7, 0xb1e2, 0xb3aa, 0xb3bb, 0xb4c2, 0xb4cf, 0xb4d9, 
                 0xb4eb, 0xb5a5, 0xb5b5, 0xb5bf, 0xb5c7, 0xb5e9, 0xb6f3, 0xb7af, 0xb7c2, 0xb7ce, 
                 0xb8a6, 0xb8ae, 0xb8b6, 0xb8b8, 0xb8bb, 0xb8e9, 0xb9ab, 0xb9ae, 0xb9cc, 0xb9ce, 
                 0xb9fd, 0xbab8, 0xbace, 0xbad0, 0xbaf1, 0xbbe7, 0xbbf3, 0xbbfd, 0xbcad, 0xbcba, 
                 0xbcd2, 0xbcf6, 0xbdba, 0xbdc0, 0xbdc3, 0xbdc5, 0xbec6, 0xbec8, 0xbedf, 0xbeee, 
                 0xbef8, 0xbefa, 0xbfa1, 0xbfa9, 0xbfc0, 0xbfe4, 0xbfeb, 0xbfec, 0xbff8, 0xc0a7, 
                 0xc0af, 0xc0b8, 0xc0ba, 0xc0bb, 0xc0bd, 0xc0c7, 0xc0cc, 0xc0ce, 0xc0cf, 0xc0d6, 
                 0xc0da, 0xc0e5, 0xc0fb, 0xc0fc, 0xc1a4, 0xc1a6, 0xc1b6, 0xc1d6, 0xc1df, 0xc1f6, 
                 0xc1f8, 0xc4a1, 0xc5cd, 0xc6ae, 0xc7cf, 0xc7d1, 0xc7d2, 0xc7d8, 0xc7e5, 0xc8ad};
             
             String getName() {
                 return "EUC-KR";
             }
             
             int match(CharsetDetector det) {
                 return match(det, commonChars);
             }
             
             public String getLanguage()
             {
                 return "ko";
             }
         }
     }
     
     /**
      * 
      *   GB-18030 recognizer. Uses simplified Chinese statistics.   
      *
      */
     static class CharsetRecog_gb_18030 extends CharsetRecog_mbcs {
         
         /*
          *  (non-Javadoc)
          *  Get the next character value for EUC based encodings.
          *  Character "value" is simply the raw bytes that make up the character
          *     packed into an int.
          */
         boolean nextChar(iteratedChar it, CharsetDetector det) {
             it.index = it.nextIndex;
             it.error = false;
             int firstByte  = 0;
             int secondByte = 0;
             int thirdByte  = 0;
             int fourthByte = 0;
             
             buildChar: {
                 firstByte = it.charValue = it.nextByte(det); 
                 
                 if (firstByte < 0) {
                     // Ran off the end of the input data
                     it.done = true;
                     break buildChar;
                 }
                 
                 if (firstByte <= 0x80) {
                     // single byte char
                     break buildChar;
                 }
                 
                 secondByte = it.nextByte(det);
                 it.charValue = (it.charValue << 8) | secondByte;
                 
                 if (firstByte >= 0x81 && firstByte <= 0xFE) {
                     // Two byte Char
                     if ((secondByte >= 0x40 && secondByte <= 0x7E) || (secondByte >=80 && secondByte <=0xFE)) {
                         break buildChar;
                     }
                     
                     // Four byte char
                     if (secondByte >= 0x30 && secondByte <= 0x39) {
                         thirdByte = it.nextByte(det);
                         
                         if (thirdByte >= 0x81 && thirdByte <= 0xFE) {
                             fourthByte = it.nextByte(det);
                             
                             if (fourthByte >= 0x30 && fourthByte <= 0x39) {
                                 it.charValue = (it.charValue << 16) | (thirdByte << 8) | fourthByte;
                                 break buildChar;
                             }
                         }
                     }
                     
                     it.error = true;
                     break buildChar;
                 }
             }
                 
             return (it.done == false);
         }
         
         static int [] commonChars = 
             // TODO:  This set of data comes from the character frequency-
             //        of-occurence analysis tool.  The data needs to be moved
             //        into a resource and loaded from there.
            {0xa1a1, 0xa1a2, 0xa1a3, 0xa1a4, 0xa1b0, 0xa1b1, 0xa1f1, 0xa1f3, 0xa3a1, 0xa3ac, 
             0xa3ba, 0xb1a8, 0xb1b8, 0xb1be, 0xb2bb, 0xb3c9, 0xb3f6, 0xb4f3, 0xb5bd, 0xb5c4, 
             0xb5e3, 0xb6af, 0xb6d4, 0xb6e0, 0xb7a2, 0xb7a8, 0xb7bd, 0xb7d6, 0xb7dd, 0xb8b4, 
             0xb8df, 0xb8f6, 0xb9ab, 0xb9c9, 0xb9d8, 0xb9fa, 0xb9fd, 0xbacd, 0xbba7, 0xbbd6, 
             0xbbe1, 0xbbfa, 0xbcbc, 0xbcdb, 0xbcfe, 0xbdcc, 0xbecd, 0xbedd, 0xbfb4, 0xbfc6, 
             0xbfc9, 0xc0b4, 0xc0ed, 0xc1cb, 0xc2db, 0xc3c7, 0xc4dc, 0xc4ea, 0xc5cc, 0xc6f7, 
             0xc7f8, 0xc8ab, 0xc8cb, 0xc8d5, 0xc8e7, 0xc9cf, 0xc9fa, 0xcab1, 0xcab5, 0xcac7, 
             0xcad0, 0xcad6, 0xcaf5, 0xcafd, 0xccec, 0xcdf8, 0xceaa, 0xcec4, 0xced2, 0xcee5, 
             0xcfb5, 0xcfc2, 0xcfd6, 0xd0c2, 0xd0c5, 0xd0d0, 0xd0d4, 0xd1a7, 0xd2aa, 0xd2b2, 
             0xd2b5, 0xd2bb, 0xd2d4, 0xd3c3, 0xd3d0, 0xd3fd, 0xd4c2, 0xd4da, 0xd5e2, 0xd6d0};

         
         String getName() {
             return "GB18030";
         }
         
         int match(CharsetDetector det) {
             return match(det, commonChars);
         }
         
         public String getLanguage()
         {
             return "zh";
         }
     }
     
     
}
"
tika-parsers/src/main/java/org/apache/tika/parser/txt/CharsetRecog_sbcs.java,true,"/*
 ****************************************************************************
 * Copyright (C) 2005-2009, International Business Machines Corporation and *
 * others. All Rights Reserved.                                             *
 ************************************************************************** *
 *
 */
package org.apache.tika.parser.txt;

import java.nio.ByteBuffer;

/**
 * This class recognizes single-byte encodings. Because the encoding scheme is so
 * simple, language statistics are used to do the matching.
 * 
 * The Recognizer works by first mapping from bytes in the encoding under test
 *  into that Recognizer's ngram space. Normally this means performing a
 *  lowercase, and excluding codepoints that don't correspond to numbers of
 *  letters. (Accented letters may or may not be ignored or normalised, depending 
 *  on the needs of the ngrams)
 * Then, ngram analysis is run against the transformed text, and a confidence
 *  is calculated.
 * 
 * For many of our Recognizers, we have one ngram set per language in each
 *  encoding, and do a simultanious language+charset detection.
 *  
 * When adding new Recognizers, the easiest way is to byte map to an existing
 *  encoding for which we have ngrams, excluding non text, and re-use the ngrams.
 * 
 * @internal
 */
abstract class CharsetRecog_sbcs extends CharsetRecognizer {

    /* (non-Javadoc)
     * @see com.ibm.icu.text.CharsetRecognizer#getName()
     */
    abstract String getName();

    /* (non-Javadoc)
     * @see com.ibm.icu.text.CharsetRecognizer#match(com.ibm.icu.text.CharsetDetector)
     */
    abstract int match(CharsetDetector det);
    
    static class NGramParser
    {
//        private static final int N_GRAM_SIZE = 3;
        private static final int N_GRAM_MASK = 0xFFFFFF;

        private int byteIndex = 0;
        private int ngram = 0;
        
        private int[] ngramList;
        private byte[] byteMap;
        
        private int ngramCount;
        private int hitCount;
        
        private byte spaceChar;
        
        public NGramParser(int[] theNgramList, byte[] theByteMap)
        {
            ngramList = theNgramList;
            byteMap   = theByteMap;
            
            ngram = 0;
            
            ngramCount = hitCount = 0;
        }
        
        /*
         * Binary search for value in table, which must have exactly 64 entries.
         */
        private static int search(int[] table, int value)
        {
            int index = 0;
            
            if (table[index + 32] <= value) {
                index += 32;
            }
            
            if (table[index + 16] <= value) {
                index += 16;
            }

            if (table[index + 8] <= value) {
                index += 8;
            }

            if (table[index + 4] <= value) {
                index += 4;
            }

            if (table[index + 2] <= value) {
                index += 2;
            }

            if (table[index + 1] <= value) {
                index += 1;
            }

            if (table[index] > value) {
                index -= 1;
            }
            
            if (index < 0 || table[index] != value) {
                return -1;
            }
            
            return index;
        }

        private void lookup(int thisNgram)
        {
            ngramCount += 1;
            
            if (search(ngramList, thisNgram) >= 0) {
                hitCount += 1;
            }
            
        }
        
        private void addByte(int b)
        {
            ngram = ((ngram << 8) + (b & 0xFF)) & N_GRAM_MASK;
            lookup(ngram);
        }
        
        private int nextByte(CharsetDetector det)
        {
            if (byteIndex >= det.fInputLen) {
                return -1;
            }
            
            return det.fInputBytes[byteIndex++] & 0xFF;
        }
        
        public int parse(CharsetDetector det)
        {
            return parse (det, (byte)0x20);
        }
        public int parse(CharsetDetector det, byte spaceCh)
        {
            int b;
            boolean ignoreSpace = false;
            this.spaceChar = spaceCh;
            
            while ((b = nextByte(det)) >= 0) {
                byte mb = byteMap[b];
                
                // TODO: 0x20 might not be a space in all character sets...
                if (mb != 0) {
                    if (!(mb == spaceChar && ignoreSpace)) {
                        addByte(mb);                    
                    }
                    
                    ignoreSpace = (mb == spaceChar);
                } else if(mb == 0 && b != 0) {
                   // Indicates an invalid character in the charset
                   // Bump the ngram count up a bit to indicate uncertainty
                   ngramCount += 4;
                }
            }
            
            // TODO: Is this OK? The buffer could have ended in the middle of a word...
            addByte(spaceChar);

            double rawPercent = (double) hitCount / (double) ngramCount;
            
//                if (rawPercent <= 2.0) {
//                    return 0;
//                }
            
            // TODO - This is a bit of a hack to take care of a case
            // were we were getting a confidence of 135...
            if (rawPercent > 0.33) {
                return 98;
            }
            
            return (int) (rawPercent * 300.0);
        }
    }
    
    protected boolean haveC1Bytes = false;
    
    int match(CharsetDetector det, int[] ngrams,  byte[] byteMap)
    {
        return match (det, ngrams, byteMap, (byte)0x20);
    }
    
    int match(CharsetDetector det, int[] ngrams,  byte[] byteMap, byte spaceChar)
    {
        NGramParser parser = new NGramParser(ngrams, byteMap);
        
        haveC1Bytes = det.fC1Bytes;
        
        return parser.parse(det, spaceChar);
    }
    
    abstract static class CharsetRecog_8859_1 extends CharsetRecog_sbcs
    {
        protected static byte[] byteMap = {
/* 0x00-0x07 */ (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
/* 0x08-0x0f */ (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
/* 0x10-0x17 */ (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
/* 0x18-0x1f */ (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
/* 0x20-0x27 */ (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x00, 
/* 0x28-0x2f */ (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
/* 0x30-0x37 */ (byte) 0x30, (byte) 0x31, (byte) 0x32, (byte) 0x33, (byte) 0x34, (byte) 0x35, (byte) 0x36, (byte) 0x37, 
/* 0x38-0x3f */ (byte) 0x38, (byte) 0x39, (byte) 0x40, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
/* 0x40-0x47 */ (byte) 0x20, (byte) 0x61, (byte) 0x62, (byte) 0x63, (byte) 0x64, (byte) 0x65, (byte) 0x66, (byte) 0x67, 
/* 0x48-0x4f */ (byte) 0x68, (byte) 0x69, (byte) 0x6A, (byte) 0x6B, (byte) 0x6C, (byte) 0x6D, (byte) 0x6E, (byte) 0x6F, 
/* 0x50-0x57 */ (byte) 0x70, (byte) 0x71, (byte) 0x72, (byte) 0x73, (byte) 0x74, (byte) 0x75, (byte) 0x76, (byte) 0x77, 
/* 0x58-0x0f */ (byte) 0x78, (byte) 0x79, (byte) 0x7A, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
/* 0x60-0x67 */ (byte) 0x20, (byte) 0x61, (byte) 0x62, (byte) 0x63, (byte) 0x64, (byte) 0x65, (byte) 0x66, (byte) 0x67, 
/* 0x68-0x6f */ (byte) 0x68, (byte) 0x69, (byte) 0x6A, (byte) 0x6B, (byte) 0x6C, (byte) 0x6D, (byte) 0x6E, (byte) 0x6F, 
/* 0x70-0x77 */ (byte) 0x70, (byte) 0x71, (byte) 0x72, (byte) 0x73, (byte) 0x74, (byte) 0x75, (byte) 0x76, (byte) 0x77, 
/* 0x78-0x7f */ (byte) 0x78, (byte) 0x79, (byte) 0x7A, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
/* 0x80-0x87 */ (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
/* 0x88-0x8f */ (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
/* 0x90-0x97 */ (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
/* 0x98-0x9f */ (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
/* 0xa0-0xa7 */ (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
/* 0xa8-0xaf */ (byte) 0x20, (byte) 0x20, (byte) 0xAA, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
/* 0xb0-0xb7 */ (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0xB5, (byte) 0x20, (byte) 0x20, 
/* 0xb8-0xbf */ (byte) 0x20, (byte) 0x20, (byte) 0xBA, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
/* 0xc0-0xc7 */ (byte) 0xE0, (byte) 0xE1, (byte) 0xE2, (byte) 0xE3, (byte) 0xE4, (byte) 0xE5, (byte) 0xE6, (byte) 0xE7, 
/* 0xc8-0xcf */ (byte) 0xE8, (byte) 0xE9, (byte) 0xEA, (byte) 0xEB, (byte) 0xEC, (byte) 0xED, (byte) 0xEE, (byte) 0xEF, 
/* 0xd0-0xd7 */ (byte) 0xF0, (byte) 0xF1, (byte) 0xF2, (byte) 0xF3, (byte) 0xF4, (byte) 0xF5, (byte) 0xF6, (byte) 0x20, 
/* 0xd8-0xdf */ (byte) 0xF8, (byte) 0xF9, (byte) 0xFA, (byte) 0xFB, (byte) 0xFC, (byte) 0xFD, (byte) 0xFE, (byte) 0xDF, 
/* 0xe0-0xe7 */ (byte) 0xE0, (byte) 0xE1, (byte) 0xE2, (byte) 0xE3, (byte) 0xE4, (byte) 0xE5, (byte) 0xE6, (byte) 0xE7, 
/* 0xe8-0xef */ (byte) 0xE8, (byte) 0xE9, (byte) 0xEA, (byte) 0xEB, (byte) 0xEC, (byte) 0xED, (byte) 0xEE, (byte) 0xEF, 
/* 0xf0-0xf7 */ (byte) 0xF0, (byte) 0xF1, (byte) 0xF2, (byte) 0xF3, (byte) 0xF4, (byte) 0xF5, (byte) 0xF6, (byte) 0x20, 
/* 0xf8-0xff */ (byte) 0xF8, (byte) 0xF9, (byte) 0xFA, (byte) 0xFB, (byte) 0xFC, (byte) 0xFD, (byte) 0xFE, (byte) 0xFF, 
        };

        public String getName()
        {
            return haveC1Bytes? "windows-1252" : "ISO-8859-1";
        }
    }

    static class CharsetRecog_8859_1_da extends CharsetRecog_8859_1
    {
        private static int[] ngrams = {
            0x206166, 0x206174, 0x206465, 0x20656E, 0x206572, 0x20666F, 0x206861, 0x206920, 0x206D65, 0x206F67, 0x2070E5, 0x207369, 0x207374, 0x207469, 0x207669, 0x616620, 
            0x616E20, 0x616E64, 0x617220, 0x617420, 0x646520, 0x64656E, 0x646572, 0x646574, 0x652073, 0x656420, 0x656465, 0x656E20, 0x656E64, 0x657220, 0x657265, 0x657320, 
            0x657420, 0x666F72, 0x676520, 0x67656E, 0x676572, 0x696765, 0x696C20, 0x696E67, 0x6B6520, 0x6B6B65, 0x6C6572, 0x6C6967, 0x6C6C65, 0x6D6564, 0x6E6465, 0x6E6520, 
            0x6E6720, 0x6E6765, 0x6F6720, 0x6F6D20, 0x6F7220, 0x70E520, 0x722064, 0x722065, 0x722073, 0x726520, 0x737465, 0x742073, 0x746520, 0x746572, 0x74696C, 0x766572, 
        };

        public String getLanguage()
        {
            return "da";
        }
        
        public int match(CharsetDetector det)
        {
            return match(det, ngrams, byteMap);
        }
    }

    static class CharsetRecog_8859_1_de extends CharsetRecog_8859_1
    {
        private static int[] ngrams = {
            0x20616E, 0x206175, 0x206265, 0x206461, 0x206465, 0x206469, 0x206569, 0x206765, 0x206861, 0x20696E, 0x206D69, 0x207363, 0x207365, 0x20756E, 0x207665, 0x20766F, 
            0x207765, 0x207A75, 0x626572, 0x636820, 0x636865, 0x636874, 0x646173, 0x64656E, 0x646572, 0x646965, 0x652064, 0x652073, 0x65696E, 0x656974, 0x656E20, 0x657220, 
            0x657320, 0x67656E, 0x68656E, 0x687420, 0x696368, 0x696520, 0x696E20, 0x696E65, 0x697420, 0x6C6963, 0x6C6C65, 0x6E2061, 0x6E2064, 0x6E2073, 0x6E6420, 0x6E6465, 
            0x6E6520, 0x6E6720, 0x6E6765, 0x6E7465, 0x722064, 0x726465, 0x726569, 0x736368, 0x737465, 0x742064, 0x746520, 0x74656E, 0x746572, 0x756E64, 0x756E67, 0x766572, 
        };

        public String getLanguage()
        {
            return "de";
        }
        
        public int match(CharsetDetector det)
        {
            return match(det, ngrams, byteMap);
        }
    }
    
    static class CharsetRecog_8859_1_en extends CharsetRecog_8859_1
    {
        private static int[] ngrams = {
            0x206120, 0x20616E, 0x206265, 0x20636F, 0x20666F, 0x206861, 0x206865, 0x20696E, 0x206D61, 0x206F66, 0x207072, 0x207265, 0x207361, 0x207374, 0x207468, 0x20746F, 
            0x207768, 0x616964, 0x616C20, 0x616E20, 0x616E64, 0x617320, 0x617420, 0x617465, 0x617469, 0x642061, 0x642074, 0x652061, 0x652073, 0x652074, 0x656420, 0x656E74, 
            0x657220, 0x657320, 0x666F72, 0x686174, 0x686520, 0x686572, 0x696420, 0x696E20, 0x696E67, 0x696F6E, 0x697320, 0x6E2061, 0x6E2074, 0x6E6420, 0x6E6720, 0x6E7420, 
            0x6F6620, 0x6F6E20, 0x6F7220, 0x726520, 0x727320, 0x732061, 0x732074, 0x736169, 0x737420, 0x742074, 0x746572, 0x746861, 0x746865, 0x74696F, 0x746F20, 0x747320, 
        };
            
        public String getLanguage()
        {
            return "en";
        }
        
        public int match(CharsetDetector det)
        {
            return match(det, ngrams, byteMap);
        }
    }
    
    static class CharsetRecog_8859_1_es extends CharsetRecog_8859_1
    {
        private static int[] ngrams = {
            0x206120, 0x206361, 0x20636F, 0x206465, 0x20656C, 0x20656E, 0x206573, 0x20696E, 0x206C61, 0x206C6F, 0x207061, 0x20706F, 0x207072, 0x207175, 0x207265, 0x207365, 
            0x20756E, 0x207920, 0x612063, 0x612064, 0x612065, 0x61206C, 0x612070, 0x616369, 0x61646F, 0x616C20, 0x617220, 0x617320, 0x6369F3, 0x636F6E, 0x646520, 0x64656C, 
            0x646F20, 0x652064, 0x652065, 0x65206C, 0x656C20, 0x656E20, 0x656E74, 0x657320, 0x657374, 0x69656E, 0x69F36E, 0x6C6120, 0x6C6F73, 0x6E2065, 0x6E7465, 0x6F2064, 
            0x6F2065, 0x6F6E20, 0x6F7220, 0x6F7320, 0x706172, 0x717565, 0x726120, 0x726573, 0x732064, 0x732065, 0x732070, 0x736520, 0x746520, 0x746F20, 0x756520, 0xF36E20, 
        };
            
        public String getLanguage()
        {
            return "es";
        }
        
        public int match(CharsetDetector det)
        {
            return match(det, ngrams, byteMap);
        }
    }
    
    static class CharsetRecog_8859_1_fr extends CharsetRecog_8859_1
    {
        private static int[] ngrams = {
            0x206175, 0x20636F, 0x206461, 0x206465, 0x206475, 0x20656E, 0x206574, 0x206C61, 0x206C65, 0x207061, 0x20706F, 0x207072, 0x207175, 0x207365, 0x20736F, 0x20756E, 
            0x20E020, 0x616E74, 0x617469, 0x636520, 0x636F6E, 0x646520, 0x646573, 0x647520, 0x652061, 0x652063, 0x652064, 0x652065, 0x65206C, 0x652070, 0x652073, 0x656E20, 
            0x656E74, 0x657220, 0x657320, 0x657420, 0x657572, 0x696F6E, 0x697320, 0x697420, 0x6C6120, 0x6C6520, 0x6C6573, 0x6D656E, 0x6E2064, 0x6E6520, 0x6E7320, 0x6E7420, 
            0x6F6E20, 0x6F6E74, 0x6F7572, 0x717565, 0x72206C, 0x726520, 0x732061, 0x732064, 0x732065, 0x73206C, 0x732070, 0x742064, 0x746520, 0x74696F, 0x756520, 0x757220, 
        };

        public String getLanguage()
        {
            return "fr";
        }
        
        public int match(CharsetDetector det)
        {
            return match(det, ngrams, byteMap);
        }
    }
    
    static class CharsetRecog_8859_1_it extends CharsetRecog_8859_1
    {
        private static int[] ngrams = {
            0x20616C, 0x206368, 0x20636F, 0x206465, 0x206469, 0x206520, 0x20696C, 0x20696E, 0x206C61, 0x207065, 0x207072, 0x20756E, 0x612063, 0x612064, 0x612070, 0x612073, 
            0x61746F, 0x636865, 0x636F6E, 0x64656C, 0x646920, 0x652061, 0x652063, 0x652064, 0x652069, 0x65206C, 0x652070, 0x652073, 0x656C20, 0x656C6C, 0x656E74, 0x657220, 
            0x686520, 0x692061, 0x692063, 0x692064, 0x692073, 0x696120, 0x696C20, 0x696E20, 0x696F6E, 0x6C6120, 0x6C6520, 0x6C6920, 0x6C6C61, 0x6E6520, 0x6E6920, 0x6E6F20, 
            0x6E7465, 0x6F2061, 0x6F2064, 0x6F2069, 0x6F2073, 0x6F6E20, 0x6F6E65, 0x706572, 0x726120, 0x726520, 0x736920, 0x746120, 0x746520, 0x746920, 0x746F20, 0x7A696F, 
        };

        public String getLanguage()
        {
            return "it";
        }
        
        public int match(CharsetDetector det)
        {
            return match(det, ngrams, byteMap);
        }
    }
    
    static class CharsetRecog_8859_1_nl extends CharsetRecog_8859_1
    {
        private static int[] ngrams = {
            0x20616C, 0x206265, 0x206461, 0x206465, 0x206469, 0x206565, 0x20656E, 0x206765, 0x206865, 0x20696E, 0x206D61, 0x206D65, 0x206F70, 0x207465, 0x207661, 0x207665, 
            0x20766F, 0x207765, 0x207A69, 0x61616E, 0x616172, 0x616E20, 0x616E64, 0x617220, 0x617420, 0x636874, 0x646520, 0x64656E, 0x646572, 0x652062, 0x652076, 0x65656E, 
            0x656572, 0x656E20, 0x657220, 0x657273, 0x657420, 0x67656E, 0x686574, 0x696520, 0x696E20, 0x696E67, 0x697320, 0x6E2062, 0x6E2064, 0x6E2065, 0x6E2068, 0x6E206F, 
            0x6E2076, 0x6E6465, 0x6E6720, 0x6F6E64, 0x6F6F72, 0x6F7020, 0x6F7220, 0x736368, 0x737465, 0x742064, 0x746520, 0x74656E, 0x746572, 0x76616E, 0x766572, 0x766F6F, 
        };

        public String getLanguage()
        {
            return "nl";
        }
        
        public int match(CharsetDetector det)
        {
            return match(det, ngrams, byteMap);
        }
    }
    
    static class CharsetRecog_8859_1_no extends CharsetRecog_8859_1
    {
        private static int[] ngrams = {
            0x206174, 0x206176, 0x206465, 0x20656E, 0x206572, 0x20666F, 0x206861, 0x206920, 0x206D65, 0x206F67, 0x2070E5, 0x207365, 0x20736B, 0x20736F, 0x207374, 0x207469, 
            0x207669, 0x20E520, 0x616E64, 0x617220, 0x617420, 0x646520, 0x64656E, 0x646574, 0x652073, 0x656420, 0x656E20, 0x656E65, 0x657220, 0x657265, 0x657420, 0x657474, 
            0x666F72, 0x67656E, 0x696B6B, 0x696C20, 0x696E67, 0x6B6520, 0x6B6B65, 0x6C6520, 0x6C6C65, 0x6D6564, 0x6D656E, 0x6E2073, 0x6E6520, 0x6E6720, 0x6E6765, 0x6E6E65, 
            0x6F6720, 0x6F6D20, 0x6F7220, 0x70E520, 0x722073, 0x726520, 0x736F6D, 0x737465, 0x742073, 0x746520, 0x74656E, 0x746572, 0x74696C, 0x747420, 0x747465, 0x766572, 
        };

        public String getLanguage()
        {
            return "no";
        }
        
        public int match(CharsetDetector det)
        {
            return match(det, ngrams, byteMap);
        }
    }
    
    static class CharsetRecog_8859_1_pt extends CharsetRecog_8859_1
    {
        private static int[] ngrams = {
            0x206120, 0x20636F, 0x206461, 0x206465, 0x20646F, 0x206520, 0x206573, 0x206D61, 0x206E6F, 0x206F20, 0x207061, 0x20706F, 0x207072, 0x207175, 0x207265, 0x207365, 
            0x20756D, 0x612061, 0x612063, 0x612064, 0x612070, 0x616465, 0x61646F, 0x616C20, 0x617220, 0x617261, 0x617320, 0x636F6D, 0x636F6E, 0x646120, 0x646520, 0x646F20, 
            0x646F73, 0x652061, 0x652064, 0x656D20, 0x656E74, 0x657320, 0x657374, 0x696120, 0x696361, 0x6D656E, 0x6E7465, 0x6E746F, 0x6F2061, 0x6F2063, 0x6F2064, 0x6F2065, 
            0x6F2070, 0x6F7320, 0x706172, 0x717565, 0x726120, 0x726573, 0x732061, 0x732064, 0x732065, 0x732070, 0x737461, 0x746520, 0x746F20, 0x756520, 0xE36F20, 0xE7E36F, 
        };

        public String getLanguage()
        {
            return "pt";
        }
        
        public int match(CharsetDetector det)
        {
            return match(det, ngrams, byteMap);
        }
    }
    
    static class CharsetRecog_8859_1_sv extends CharsetRecog_8859_1
    {
        private static int[] ngrams = {
            0x206174, 0x206176, 0x206465, 0x20656E, 0x2066F6, 0x206861, 0x206920, 0x20696E, 0x206B6F, 0x206D65, 0x206F63, 0x2070E5, 0x20736B, 0x20736F, 0x207374, 0x207469, 
            0x207661, 0x207669, 0x20E472, 0x616465, 0x616E20, 0x616E64, 0x617220, 0x617474, 0x636820, 0x646520, 0x64656E, 0x646572, 0x646574, 0x656420, 0x656E20, 0x657220, 
            0x657420, 0x66F672, 0x67656E, 0x696C6C, 0x696E67, 0x6B6120, 0x6C6C20, 0x6D6564, 0x6E2073, 0x6E6120, 0x6E6465, 0x6E6720, 0x6E6765, 0x6E696E, 0x6F6368, 0x6F6D20, 
            0x6F6E20, 0x70E520, 0x722061, 0x722073, 0x726120, 0x736B61, 0x736F6D, 0x742073, 0x746120, 0x746520, 0x746572, 0x74696C, 0x747420, 0x766172, 0xE47220, 0xF67220, 
        };

        public String getLanguage()
        {
            return "sv";
        }
        
        public int match(CharsetDetector det)
        {
            return match(det, ngrams, byteMap);
        }
    }
    
    abstract static class CharsetRecog_8859_2 extends CharsetRecog_sbcs
    {
        protected static byte[] byteMap = {
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x00, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x61, (byte) 0x62, (byte) 0x63, (byte) 0x64, (byte) 0x65, (byte) 0x66, (byte) 0x67, 
            (byte) 0x68, (byte) 0x69, (byte) 0x6A, (byte) 0x6B, (byte) 0x6C, (byte) 0x6D, (byte) 0x6E, (byte) 0x6F, 
            (byte) 0x70, (byte) 0x71, (byte) 0x72, (byte) 0x73, (byte) 0x74, (byte) 0x75, (byte) 0x76, (byte) 0x77, 
            (byte) 0x78, (byte) 0x79, (byte) 0x7A, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x61, (byte) 0x62, (byte) 0x63, (byte) 0x64, (byte) 0x65, (byte) 0x66, (byte) 0x67, 
            (byte) 0x68, (byte) 0x69, (byte) 0x6A, (byte) 0x6B, (byte) 0x6C, (byte) 0x6D, (byte) 0x6E, (byte) 0x6F, 
            (byte) 0x70, (byte) 0x71, (byte) 0x72, (byte) 0x73, (byte) 0x74, (byte) 0x75, (byte) 0x76, (byte) 0x77, 
            (byte) 0x78, (byte) 0x79, (byte) 0x7A, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0xB1, (byte) 0x20, (byte) 0xB3, (byte) 0x20, (byte) 0xB5, (byte) 0xB6, (byte) 0x20, 
            (byte) 0x20, (byte) 0xB9, (byte) 0xBA, (byte) 0xBB, (byte) 0xBC, (byte) 0x20, (byte) 0xBE, (byte) 0xBF, 
            (byte) 0x20, (byte) 0xB1, (byte) 0x20, (byte) 0xB3, (byte) 0x20, (byte) 0xB5, (byte) 0xB6, (byte) 0xB7, 
            (byte) 0x20, (byte) 0xB9, (byte) 0xBA, (byte) 0xBB, (byte) 0xBC, (byte) 0x20, (byte) 0xBE, (byte) 0xBF, 
            (byte) 0xE0, (byte) 0xE1, (byte) 0xE2, (byte) 0xE3, (byte) 0xE4, (byte) 0xE5, (byte) 0xE6, (byte) 0xE7, 
            (byte) 0xE8, (byte) 0xE9, (byte) 0xEA, (byte) 0xEB, (byte) 0xEC, (byte) 0xED, (byte) 0xEE, (byte) 0xEF, 
            (byte) 0xF0, (byte) 0xF1, (byte) 0xF2, (byte) 0xF3, (byte) 0xF4, (byte) 0xF5, (byte) 0xF6, (byte) 0x20, 
            (byte) 0xF8, (byte) 0xF9, (byte) 0xFA, (byte) 0xFB, (byte) 0xFC, (byte) 0xFD, (byte) 0xFE, (byte) 0xDF, 
            (byte) 0xE0, (byte) 0xE1, (byte) 0xE2, (byte) 0xE3, (byte) 0xE4, (byte) 0xE5, (byte) 0xE6, (byte) 0xE7, 
            (byte) 0xE8, (byte) 0xE9, (byte) 0xEA, (byte) 0xEB, (byte) 0xEC, (byte) 0xED, (byte) 0xEE, (byte) 0xEF, 
            (byte) 0xF0, (byte) 0xF1, (byte) 0xF2, (byte) 0xF3, (byte) 0xF4, (byte) 0xF5, (byte) 0xF6, (byte) 0x20, 
            (byte) 0xF8, (byte) 0xF9, (byte) 0xFA, (byte) 0xFB, (byte) 0xFC, (byte) 0xFD, (byte) 0xFE, (byte) 0x20, 
        };

        public String getName()
        {
            return haveC1Bytes? "windows-1250" : "ISO-8859-2";
        }
    }
    
    static class CharsetRecog_8859_2_cs extends CharsetRecog_8859_2
    {
        private static int[] ngrams = {
            0x206120, 0x206279, 0x20646F, 0x206A65, 0x206E61, 0x206E65, 0x206F20, 0x206F64, 0x20706F, 0x207072, 0x2070F8, 0x20726F, 0x207365, 0x20736F, 0x207374, 0x20746F, 
            0x207620, 0x207679, 0x207A61, 0x612070, 0x636520, 0x636820, 0x652070, 0x652073, 0x652076, 0x656D20, 0x656EED, 0x686F20, 0x686F64, 0x697374, 0x6A6520, 0x6B7465, 
            0x6C6520, 0x6C6920, 0x6E6120, 0x6EE920, 0x6EEC20, 0x6EED20, 0x6F2070, 0x6F646E, 0x6F6A69, 0x6F7374, 0x6F7520, 0x6F7661, 0x706F64, 0x706F6A, 0x70726F, 0x70F865, 
            0x736520, 0x736F75, 0x737461, 0x737469, 0x73746E, 0x746572, 0x746EED, 0x746F20, 0x752070, 0xBE6520, 0xE16EED, 0xE9686F, 0xED2070, 0xED2073, 0xED6D20, 0xF86564, 
        };

        public String getLanguage()
        {
            return "cs";
        }
        
        public int match(CharsetDetector det)
        {
            return match(det, ngrams, byteMap);
        }
    }
    
    static class CharsetRecog_8859_2_hu extends CharsetRecog_8859_2
    {
        private static int[] ngrams = {
            0x206120, 0x20617A, 0x206265, 0x206567, 0x20656C, 0x206665, 0x206861, 0x20686F, 0x206973, 0x206B65, 0x206B69, 0x206BF6, 0x206C65, 0x206D61, 0x206D65, 0x206D69, 
            0x206E65, 0x20737A, 0x207465, 0x20E973, 0x612061, 0x61206B, 0x61206D, 0x612073, 0x616B20, 0x616E20, 0x617A20, 0x62616E, 0x62656E, 0x656779, 0x656B20, 0x656C20, 
            0x656C65, 0x656D20, 0x656E20, 0x657265, 0x657420, 0x657465, 0x657474, 0x677920, 0x686F67, 0x696E74, 0x697320, 0x6B2061, 0x6BF67A, 0x6D6567, 0x6D696E, 0x6E2061, 
            0x6E616B, 0x6E656B, 0x6E656D, 0x6E7420, 0x6F6779, 0x732061, 0x737A65, 0x737A74, 0x737AE1, 0x73E967, 0x742061, 0x747420, 0x74E173, 0x7A6572, 0xE16E20, 0xE97320, 
        };

        public String getLanguage()
        {
            return "hu";
        }
        
        public int match(CharsetDetector det)
        {
            return match(det, ngrams, byteMap);
        }
    }
    
    static class CharsetRecog_8859_2_pl extends CharsetRecog_8859_2
    {
        private static int[] ngrams = {
            0x20637A, 0x20646F, 0x206920, 0x206A65, 0x206B6F, 0x206D61, 0x206D69, 0x206E61, 0x206E69, 0x206F64, 0x20706F, 0x207072, 0x207369, 0x207720, 0x207769, 0x207779, 
            0x207A20, 0x207A61, 0x612070, 0x612077, 0x616E69, 0x636820, 0x637A65, 0x637A79, 0x646F20, 0x647A69, 0x652070, 0x652073, 0x652077, 0x65207A, 0x65676F, 0x656A20, 
            0x656D20, 0x656E69, 0x676F20, 0x696120, 0x696520, 0x69656A, 0x6B6120, 0x6B6920, 0x6B6965, 0x6D6965, 0x6E6120, 0x6E6961, 0x6E6965, 0x6F2070, 0x6F7761, 0x6F7769, 
            0x706F6C, 0x707261, 0x70726F, 0x70727A, 0x727A65, 0x727A79, 0x7369EA, 0x736B69, 0x737461, 0x776965, 0x796368, 0x796D20, 0x7A6520, 0x7A6965, 0x7A7920, 0xF37720, 
        };

        public String getLanguage()
        {
            return "pl";
        }
        
        public int match(CharsetDetector det)
        {
            return match(det, ngrams, byteMap);
        }
    }
    
    static class CharsetRecog_8859_2_ro extends CharsetRecog_8859_2
    {
        private static int[] ngrams = {
            0x206120, 0x206163, 0x206361, 0x206365, 0x20636F, 0x206375, 0x206465, 0x206469, 0x206C61, 0x206D61, 0x207065, 0x207072, 0x207365, 0x2073E3, 0x20756E, 0x20BA69, 
            0x20EE6E, 0x612063, 0x612064, 0x617265, 0x617420, 0x617465, 0x617520, 0x636172, 0x636F6E, 0x637520, 0x63E320, 0x646520, 0x652061, 0x652063, 0x652064, 0x652070, 
            0x652073, 0x656120, 0x656920, 0x656C65, 0x656E74, 0x657374, 0x692061, 0x692063, 0x692064, 0x692070, 0x696520, 0x696920, 0x696E20, 0x6C6120, 0x6C6520, 0x6C6F72, 
            0x6C7569, 0x6E6520, 0x6E7472, 0x6F7220, 0x70656E, 0x726520, 0x726561, 0x727520, 0x73E320, 0x746520, 0x747275, 0x74E320, 0x756920, 0x756C20, 0xBA6920, 0xEE6E20, 
        };

        public String getLanguage()
        {
            return "ro";
        }
        
        public int match(CharsetDetector det)
        {
            return match(det, ngrams, byteMap);
        }
    }
    
    abstract static class CharsetRecog_8859_5 extends CharsetRecog_sbcs
    {
        protected static byte[] byteMap = {
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x00, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x61, (byte) 0x62, (byte) 0x63, (byte) 0x64, (byte) 0x65, (byte) 0x66, (byte) 0x67, 
            (byte) 0x68, (byte) 0x69, (byte) 0x6A, (byte) 0x6B, (byte) 0x6C, (byte) 0x6D, (byte) 0x6E, (byte) 0x6F, 
            (byte) 0x70, (byte) 0x71, (byte) 0x72, (byte) 0x73, (byte) 0x74, (byte) 0x75, (byte) 0x76, (byte) 0x77, 
            (byte) 0x78, (byte) 0x79, (byte) 0x7A, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x61, (byte) 0x62, (byte) 0x63, (byte) 0x64, (byte) 0x65, (byte) 0x66, (byte) 0x67, 
            (byte) 0x68, (byte) 0x69, (byte) 0x6A, (byte) 0x6B, (byte) 0x6C, (byte) 0x6D, (byte) 0x6E, (byte) 0x6F, 
            (byte) 0x70, (byte) 0x71, (byte) 0x72, (byte) 0x73, (byte) 0x74, (byte) 0x75, (byte) 0x76, (byte) 0x77, 
            (byte) 0x78, (byte) 0x79, (byte) 0x7A, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0xF1, (byte) 0xF2, (byte) 0xF3, (byte) 0xF4, (byte) 0xF5, (byte) 0xF6, (byte) 0xF7, 
            (byte) 0xF8, (byte) 0xF9, (byte) 0xFA, (byte) 0xFB, (byte) 0xFC, (byte) 0x20, (byte) 0xFE, (byte) 0xFF, 
            (byte) 0xD0, (byte) 0xD1, (byte) 0xD2, (byte) 0xD3, (byte) 0xD4, (byte) 0xD5, (byte) 0xD6, (byte) 0xD7, 
            (byte) 0xD8, (byte) 0xD9, (byte) 0xDA, (byte) 0xDB, (byte) 0xDC, (byte) 0xDD, (byte) 0xDE, (byte) 0xDF, 
            (byte) 0xE0, (byte) 0xE1, (byte) 0xE2, (byte) 0xE3, (byte) 0xE4, (byte) 0xE5, (byte) 0xE6, (byte) 0xE7, 
            (byte) 0xE8, (byte) 0xE9, (byte) 0xEA, (byte) 0xEB, (byte) 0xEC, (byte) 0xED, (byte) 0xEE, (byte) 0xEF, 
            (byte) 0xD0, (byte) 0xD1, (byte) 0xD2, (byte) 0xD3, (byte) 0xD4, (byte) 0xD5, (byte) 0xD6, (byte) 0xD7, 
            (byte) 0xD8, (byte) 0xD9, (byte) 0xDA, (byte) 0xDB, (byte) 0xDC, (byte) 0xDD, (byte) 0xDE, (byte) 0xDF, 
            (byte) 0xE0, (byte) 0xE1, (byte) 0xE2, (byte) 0xE3, (byte) 0xE4, (byte) 0xE5, (byte) 0xE6, (byte) 0xE7, 
            (byte) 0xE8, (byte) 0xE9, (byte) 0xEA, (byte) 0xEB, (byte) 0xEC, (byte) 0xED, (byte) 0xEE, (byte) 0xEF, 
            (byte) 0x20, (byte) 0xF1, (byte) 0xF2, (byte) 0xF3, (byte) 0xF4, (byte) 0xF5, (byte) 0xF6, (byte) 0xF7, 
            (byte) 0xF8, (byte) 0xF9, (byte) 0xFA, (byte) 0xFB, (byte) 0xFC, (byte) 0x20, (byte) 0xFE, (byte) 0xFF, 
        };

        public String getName()
        {
            return "ISO-8859-5";
        }
    }
    
    static class CharsetRecog_8859_5_ru extends CharsetRecog_8859_5
    {
        private static int[] ngrams = {
            0x20D220, 0x20D2DE, 0x20D4DE, 0x20D7D0, 0x20D820, 0x20DAD0, 0x20DADE, 0x20DDD0, 0x20DDD5, 0x20DED1, 0x20DFDE, 0x20DFE0, 0x20E0D0, 0x20E1DE, 0x20E1E2, 0x20E2DE, 
            0x20E7E2, 0x20EDE2, 0xD0DDD8, 0xD0E2EC, 0xD3DE20, 0xD5DBEC, 0xD5DDD8, 0xD5E1E2, 0xD5E220, 0xD820DF, 0xD8D520, 0xD8D820, 0xD8EF20, 0xDBD5DD, 0xDBD820, 0xDBECDD, 
            0xDDD020, 0xDDD520, 0xDDD8D5, 0xDDD8EF, 0xDDDE20, 0xDDDED2, 0xDE20D2, 0xDE20DF, 0xDE20E1, 0xDED220, 0xDED2D0, 0xDED3DE, 0xDED920, 0xDEDBEC, 0xDEDC20, 0xDEE1E2, 
            0xDFDEDB, 0xDFE0D5, 0xDFE0D8, 0xDFE0DE, 0xE0D0D2, 0xE0D5D4, 0xE1E2D0, 0xE1E2D2, 0xE1E2D8, 0xE1EF20, 0xE2D5DB, 0xE2DE20, 0xE2DEE0, 0xE2EC20, 0xE7E2DE, 0xEBE520, 
        };

        public String getLanguage()
        {
            return "ru";
        }
        
        public int match(CharsetDetector det)
        {
            return match(det, ngrams, byteMap);
        }
    }
    
    abstract static class CharsetRecog_8859_6 extends CharsetRecog_sbcs
    {
        protected static byte[] byteMap = {
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x00, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x61, (byte) 0x62, (byte) 0x63, (byte) 0x64, (byte) 0x65, (byte) 0x66, (byte) 0x67, 
            (byte) 0x68, (byte) 0x69, (byte) 0x6A, (byte) 0x6B, (byte) 0x6C, (byte) 0x6D, (byte) 0x6E, (byte) 0x6F, 
            (byte) 0x70, (byte) 0x71, (byte) 0x72, (byte) 0x73, (byte) 0x74, (byte) 0x75, (byte) 0x76, (byte) 0x77, 
            (byte) 0x78, (byte) 0x79, (byte) 0x7A, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x61, (byte) 0x62, (byte) 0x63, (byte) 0x64, (byte) 0x65, (byte) 0x66, (byte) 0x67, 
            (byte) 0x68, (byte) 0x69, (byte) 0x6A, (byte) 0x6B, (byte) 0x6C, (byte) 0x6D, (byte) 0x6E, (byte) 0x6F, 
            (byte) 0x70, (byte) 0x71, (byte) 0x72, (byte) 0x73, (byte) 0x74, (byte) 0x75, (byte) 0x76, (byte) 0x77, 
            (byte) 0x78, (byte) 0x79, (byte) 0x7A, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0xC1, (byte) 0xC2, (byte) 0xC3, (byte) 0xC4, (byte) 0xC5, (byte) 0xC6, (byte) 0xC7, 
            (byte) 0xC8, (byte) 0xC9, (byte) 0xCA, (byte) 0xCB, (byte) 0xCC, (byte) 0xCD, (byte) 0xCE, (byte) 0xCF, 
            (byte) 0xD0, (byte) 0xD1, (byte) 0xD2, (byte) 0xD3, (byte) 0xD4, (byte) 0xD5, (byte) 0xD6, (byte) 0xD7, 
            (byte) 0xD8, (byte) 0xD9, (byte) 0xDA, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0xE0, (byte) 0xE1, (byte) 0xE2, (byte) 0xE3, (byte) 0xE4, (byte) 0xE5, (byte) 0xE6, (byte) 0xE7, 
            (byte) 0xE8, (byte) 0xE9, (byte) 0xEA, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
        };

        public String getName()
        {
            return "ISO-8859-6";
        }
    }
    
    static class CharsetRecog_8859_6_ar extends CharsetRecog_8859_6
    {
        private static int[] ngrams = {
            0x20C7E4, 0x20C7E6, 0x20C8C7, 0x20D9E4, 0x20E1EA, 0x20E4E4, 0x20E5E6, 0x20E8C7, 0xC720C7, 0xC7C120, 0xC7CA20, 0xC7D120, 0xC7E420, 0xC7E4C3, 0xC7E4C7, 0xC7E4C8, 
            0xC7E4CA, 0xC7E4CC, 0xC7E4CD, 0xC7E4CF, 0xC7E4D3, 0xC7E4D9, 0xC7E4E2, 0xC7E4E5, 0xC7E4E8, 0xC7E4EA, 0xC7E520, 0xC7E620, 0xC7E6CA, 0xC820C7, 0xC920C7, 0xC920E1, 
            0xC920E4, 0xC920E5, 0xC920E8, 0xCA20C7, 0xCF20C7, 0xCFC920, 0xD120C7, 0xD1C920, 0xD320C7, 0xD920C7, 0xD9E4E9, 0xE1EA20, 0xE420C7, 0xE4C920, 0xE4E920, 0xE4EA20, 
            0xE520C7, 0xE5C720, 0xE5C920, 0xE5E620, 0xE620C7, 0xE720C7, 0xE7C720, 0xE8C7E4, 0xE8E620, 0xE920C7, 0xEA20C7, 0xEA20E5, 0xEA20E8, 0xEAC920, 0xEAD120, 0xEAE620, 
        };

        public String getLanguage()
        {
            return "ar";
        }
        
        public int match(CharsetDetector det)
        {
            return match(det, ngrams, byteMap);
        }
    }
    
    abstract static class CharsetRecog_8859_7 extends CharsetRecog_sbcs
    {
        protected static byte[] byteMap = {
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x00, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x61, (byte) 0x62, (byte) 0x63, (byte) 0x64, (byte) 0x65, (byte) 0x66, (byte) 0x67, 
            (byte) 0x68, (byte) 0x69, (byte) 0x6A, (byte) 0x6B, (byte) 0x6C, (byte) 0x6D, (byte) 0x6E, (byte) 0x6F, 
            (byte) 0x70, (byte) 0x71, (byte) 0x72, (byte) 0x73, (byte) 0x74, (byte) 0x75, (byte) 0x76, (byte) 0x77, 
            (byte) 0x78, (byte) 0x79, (byte) 0x7A, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x61, (byte) 0x62, (byte) 0x63, (byte) 0x64, (byte) 0x65, (byte) 0x66, (byte) 0x67, 
            (byte) 0x68, (byte) 0x69, (byte) 0x6A, (byte) 0x6B, (byte) 0x6C, (byte) 0x6D, (byte) 0x6E, (byte) 0x6F, 
            (byte) 0x70, (byte) 0x71, (byte) 0x72, (byte) 0x73, (byte) 0x74, (byte) 0x75, (byte) 0x76, (byte) 0x77, 
            (byte) 0x78, (byte) 0x79, (byte) 0x7A, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0xA1, (byte) 0xA2, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0xDC, (byte) 0x20, 
            (byte) 0xDD, (byte) 0xDE, (byte) 0xDF, (byte) 0x20, (byte) 0xFC, (byte) 0x20, (byte) 0xFD, (byte) 0xFE, 
            (byte) 0xC0, (byte) 0xE1, (byte) 0xE2, (byte) 0xE3, (byte) 0xE4, (byte) 0xE5, (byte) 0xE6, (byte) 0xE7, 
            (byte) 0xE8, (byte) 0xE9, (byte) 0xEA, (byte) 0xEB, (byte) 0xEC, (byte) 0xED, (byte) 0xEE, (byte) 0xEF, 
            (byte) 0xF0, (byte) 0xF1, (byte) 0x20, (byte) 0xF3, (byte) 0xF4, (byte) 0xF5, (byte) 0xF6, (byte) 0xF7, 
            (byte) 0xF8, (byte) 0xF9, (byte) 0xFA, (byte) 0xFB, (byte) 0xDC, (byte) 0xDD, (byte) 0xDE, (byte) 0xDF, 
            (byte) 0xE0, (byte) 0xE1, (byte) 0xE2, (byte) 0xE3, (byte) 0xE4, (byte) 0xE5, (byte) 0xE6, (byte) 0xE7, 
            (byte) 0xE8, (byte) 0xE9, (byte) 0xEA, (byte) 0xEB, (byte) 0xEC, (byte) 0xED, (byte) 0xEE, (byte) 0xEF, 
            (byte) 0xF0, (byte) 0xF1, (byte) 0xF2, (byte) 0xF3, (byte) 0xF4, (byte) 0xF5, (byte) 0xF6, (byte) 0xF7, 
            (byte) 0xF8, (byte) 0xF9, (byte) 0xFA, (byte) 0xFB, (byte) 0xFC, (byte) 0xFD, (byte) 0xFE, (byte) 0x20, 
        };

        public String getName()
        {
            return haveC1Bytes? "windows-1253" : "ISO-8859-7";
        }
    }
    
    static class CharsetRecog_8859_7_el extends CharsetRecog_8859_7
    {
        private static int[] ngrams = {
            0x20E1ED, 0x20E1F0, 0x20E3E9, 0x20E4E9, 0x20E5F0, 0x20E720, 0x20EAE1, 0x20ECE5, 0x20EDE1, 0x20EF20, 0x20F0E1, 0x20F0EF, 0x20F0F1, 0x20F3F4, 0x20F3F5, 0x20F4E7, 
            0x20F4EF, 0xDFE120, 0xE120E1, 0xE120F4, 0xE1E920, 0xE1ED20, 0xE1F0FC, 0xE1F220, 0xE3E9E1, 0xE5E920, 0xE5F220, 0xE720F4, 0xE7ED20, 0xE7F220, 0xE920F4, 0xE9E120, 
            0xE9EADE, 0xE9F220, 0xEAE1E9, 0xEAE1F4, 0xECE520, 0xED20E1, 0xED20E5, 0xED20F0, 0xEDE120, 0xEFF220, 0xEFF520, 0xF0EFF5, 0xF0F1EF, 0xF0FC20, 0xF220E1, 0xF220E5, 
            0xF220EA, 0xF220F0, 0xF220F4, 0xF3E520, 0xF3E720, 0xF3F4EF, 0xF4E120, 0xF4E1E9, 0xF4E7ED, 0xF4E7F2, 0xF4E9EA, 0xF4EF20, 0xF4EFF5, 0xF4F9ED, 0xF9ED20, 0xFEED20, 
        };

        public String getLanguage()
        {
            return "el";
        }
        
        public int match(CharsetDetector det)
        {
            return match(det, ngrams, byteMap);
        }
    }
    
    abstract static class CharsetRecog_8859_8 extends CharsetRecog_sbcs
    {
        protected static byte[] byteMap = {
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x00, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x61, (byte) 0x62, (byte) 0x63, (byte) 0x64, (byte) 0x65, (byte) 0x66, (byte) 0x67, 
            (byte) 0x68, (byte) 0x69, (byte) 0x6A, (byte) 0x6B, (byte) 0x6C, (byte) 0x6D, (byte) 0x6E, (byte) 0x6F, 
            (byte) 0x70, (byte) 0x71, (byte) 0x72, (byte) 0x73, (byte) 0x74, (byte) 0x75, (byte) 0x76, (byte) 0x77, 
            (byte) 0x78, (byte) 0x79, (byte) 0x7A, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x61, (byte) 0x62, (byte) 0x63, (byte) 0x64, (byte) 0x65, (byte) 0x66, (byte) 0x67, 
            (byte) 0x68, (byte) 0x69, (byte) 0x6A, (byte) 0x6B, (byte) 0x6C, (byte) 0x6D, (byte) 0x6E, (byte) 0x6F, 
            (byte) 0x70, (byte) 0x71, (byte) 0x72, (byte) 0x73, (byte) 0x74, (byte) 0x75, (byte) 0x76, (byte) 0x77, 
            (byte) 0x78, (byte) 0x79, (byte) 0x7A, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0xB5, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0xE0, (byte) 0xE1, (byte) 0xE2, (byte) 0xE3, (byte) 0xE4, (byte) 0xE5, (byte) 0xE6, (byte) 0xE7, 
            (byte) 0xE8, (byte) 0xE9, (byte) 0xEA, (byte) 0xEB, (byte) 0xEC, (byte) 0xED, (byte) 0xEE, (byte) 0xEF, 
            (byte) 0xF0, (byte) 0xF1, (byte) 0xF2, (byte) 0xF3, (byte) 0xF4, (byte) 0xF5, (byte) 0xF6, (byte) 0xF7, 
            (byte) 0xF8, (byte) 0xF9, (byte) 0xFA, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
        };

        public String getName()
        {
            return haveC1Bytes? "windows-1255" : "ISO-8859-8";
        }
    }
    
    static class CharsetRecog_8859_8_I_he extends CharsetRecog_8859_8
    {
        private static int[] ngrams = {
            0x20E0E5, 0x20E0E7, 0x20E0E9, 0x20E0FA, 0x20E1E9, 0x20E1EE, 0x20E4E0, 0x20E4E5, 0x20E4E9, 0x20E4EE, 0x20E4F2, 0x20E4F9, 0x20E4FA, 0x20ECE0, 0x20ECE4, 0x20EEE0, 
            0x20F2EC, 0x20F9EC, 0xE0FA20, 0xE420E0, 0xE420E1, 0xE420E4, 0xE420EC, 0xE420EE, 0xE420F9, 0xE4E5E0, 0xE5E020, 0xE5ED20, 0xE5EF20, 0xE5F820, 0xE5FA20, 0xE920E4, 
            0xE9E420, 0xE9E5FA, 0xE9E9ED, 0xE9ED20, 0xE9EF20, 0xE9F820, 0xE9FA20, 0xEC20E0, 0xEC20E4, 0xECE020, 0xECE420, 0xED20E0, 0xED20E1, 0xED20E4, 0xED20EC, 0xED20EE, 
            0xED20F9, 0xEEE420, 0xEF20E4, 0xF0E420, 0xF0E920, 0xF0E9ED, 0xF2EC20, 0xF820E4, 0xF8E9ED, 0xF9EC20, 0xFA20E0, 0xFA20E1, 0xFA20E4, 0xFA20EC, 0xFA20EE, 0xFA20F9, 
        };

        public String getName()
        {
            return haveC1Bytes? "windows-1255" : /*"ISO-8859-8-I"*/ "ISO-8859-8";
        }

        public String getLanguage()
        {
            return "he";
        }
        
        public int match(CharsetDetector det)
        {
            return match(det, ngrams, byteMap);
        }
    }
    
    static class CharsetRecog_8859_8_he extends CharsetRecog_8859_8
    {
        private static int[] ngrams = {
            0x20E0E5, 0x20E0EC, 0x20E4E9, 0x20E4EC, 0x20E4EE, 0x20E4F0, 0x20E9F0, 0x20ECF2, 0x20ECF9, 0x20EDE5, 0x20EDE9, 0x20EFE5, 0x20EFE9, 0x20F8E5, 0x20F8E9, 0x20FAE0, 
            0x20FAE5, 0x20FAE9, 0xE020E4, 0xE020EC, 0xE020ED, 0xE020FA, 0xE0E420, 0xE0E5E4, 0xE0EC20, 0xE0EE20, 0xE120E4, 0xE120ED, 0xE120FA, 0xE420E4, 0xE420E9, 0xE420EC, 
            0xE420ED, 0xE420EF, 0xE420F8, 0xE420FA, 0xE4EC20, 0xE5E020, 0xE5E420, 0xE7E020, 0xE9E020, 0xE9E120, 0xE9E420, 0xEC20E4, 0xEC20ED, 0xEC20FA, 0xECF220, 0xECF920, 
            0xEDE9E9, 0xEDE9F0, 0xEDE9F8, 0xEE20E4, 0xEE20ED, 0xEE20FA, 0xEEE120, 0xEEE420, 0xF2E420, 0xF920E4, 0xF920ED, 0xF920FA, 0xF9E420, 0xFAE020, 0xFAE420, 0xFAE5E9, 
        };

        public String getLanguage()
        {
            return "he";
        }
        
        public int match(CharsetDetector det)
        {
            return match(det, ngrams, byteMap);
        }
    }
    
    abstract static class CharsetRecog_8859_9 extends CharsetRecog_sbcs
    {
        protected static byte[] byteMap = {
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x00, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x61, (byte) 0x62, (byte) 0x63, (byte) 0x64, (byte) 0x65, (byte) 0x66, (byte) 0x67, 
            (byte) 0x68, (byte) 0x69, (byte) 0x6A, (byte) 0x6B, (byte) 0x6C, (byte) 0x6D, (byte) 0x6E, (byte) 0x6F, 
            (byte) 0x70, (byte) 0x71, (byte) 0x72, (byte) 0x73, (byte) 0x74, (byte) 0x75, (byte) 0x76, (byte) 0x77, 
            (byte) 0x78, (byte) 0x79, (byte) 0x7A, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x61, (byte) 0x62, (byte) 0x63, (byte) 0x64, (byte) 0x65, (byte) 0x66, (byte) 0x67, 
            (byte) 0x68, (byte) 0x69, (byte) 0x6A, (byte) 0x6B, (byte) 0x6C, (byte) 0x6D, (byte) 0x6E, (byte) 0x6F, 
            (byte) 0x70, (byte) 0x71, (byte) 0x72, (byte) 0x73, (byte) 0x74, (byte) 0x75, (byte) 0x76, (byte) 0x77, 
            (byte) 0x78, (byte) 0x79, (byte) 0x7A, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0xAA, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0xB5, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0xBA, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0xE0, (byte) 0xE1, (byte) 0xE2, (byte) 0xE3, (byte) 0xE4, (byte) 0xE5, (byte) 0xE6, (byte) 0xE7, 
            (byte) 0xE8, (byte) 0xE9, (byte) 0xEA, (byte) 0xEB, (byte) 0xEC, (byte) 0xED, (byte) 0xEE, (byte) 0xEF, 
            (byte) 0xF0, (byte) 0xF1, (byte) 0xF2, (byte) 0xF3, (byte) 0xF4, (byte) 0xF5, (byte) 0xF6, (byte) 0x20, 
            (byte) 0xF8, (byte) 0xF9, (byte) 0xFA, (byte) 0xFB, (byte) 0xFC, (byte) 0x69, (byte) 0xFE, (byte) 0xDF, 
            (byte) 0xE0, (byte) 0xE1, (byte) 0xE2, (byte) 0xE3, (byte) 0xE4, (byte) 0xE5, (byte) 0xE6, (byte) 0xE7, 
            (byte) 0xE8, (byte) 0xE9, (byte) 0xEA, (byte) 0xEB, (byte) 0xEC, (byte) 0xED, (byte) 0xEE, (byte) 0xEF, 
            (byte) 0xF0, (byte) 0xF1, (byte) 0xF2, (byte) 0xF3, (byte) 0xF4, (byte) 0xF5, (byte) 0xF6, (byte) 0x20, 
            (byte) 0xF8, (byte) 0xF9, (byte) 0xFA, (byte) 0xFB, (byte) 0xFC, (byte) 0xFD, (byte) 0xFE, (byte) 0xFF, 
        };

        public String getName()
        {
            return haveC1Bytes? "windows-1254" : "ISO-8859-9";
        }
    }
    
    static class CharsetRecog_8859_9_tr extends CharsetRecog_8859_9
    {
        private static int[] ngrams = {
            0x206261, 0x206269, 0x206275, 0x206461, 0x206465, 0x206765, 0x206861, 0x20696C, 0x206B61, 0x206B6F, 0x206D61, 0x206F6C, 0x207361, 0x207461, 0x207665, 0x207961, 
            0x612062, 0x616B20, 0x616C61, 0x616D61, 0x616E20, 0x616EFD, 0x617220, 0x617261, 0x6172FD, 0x6173FD, 0x617961, 0x626972, 0x646120, 0x646520, 0x646920, 0x652062, 
            0x65206B, 0x656469, 0x656E20, 0x657220, 0x657269, 0x657369, 0x696C65, 0x696E20, 0x696E69, 0x697220, 0x6C616E, 0x6C6172, 0x6C6520, 0x6C6572, 0x6E2061, 0x6E2062, 
            0x6E206B, 0x6E6461, 0x6E6465, 0x6E6520, 0x6E6920, 0x6E696E, 0x6EFD20, 0x72696E, 0x72FD6E, 0x766520, 0x796120, 0x796F72, 0xFD6E20, 0xFD6E64, 0xFD6EFD, 0xFDF0FD, 
        };

        public String getLanguage()
        {
            return "tr";
        }
        
        public int match(CharsetDetector det)
        {
            return match(det, ngrams, byteMap);
        }
    }
    
    static class CharsetRecog_windows_1251 extends CharsetRecog_sbcs
    {
        private static int[] ngrams = {
            0x20E220, 0x20E2EE, 0x20E4EE, 0x20E7E0, 0x20E820, 0x20EAE0, 0x20EAEE, 0x20EDE0, 0x20EDE5, 0x20EEE1, 0x20EFEE, 0x20EFF0, 0x20F0E0, 0x20F1EE, 0x20F1F2, 0x20F2EE, 
            0x20F7F2, 0x20FDF2, 0xE0EDE8, 0xE0F2FC, 0xE3EE20, 0xE5EBFC, 0xE5EDE8, 0xE5F1F2, 0xE5F220, 0xE820EF, 0xE8E520, 0xE8E820, 0xE8FF20, 0xEBE5ED, 0xEBE820, 0xEBFCED, 
            0xEDE020, 0xEDE520, 0xEDE8E5, 0xEDE8FF, 0xEDEE20, 0xEDEEE2, 0xEE20E2, 0xEE20EF, 0xEE20F1, 0xEEE220, 0xEEE2E0, 0xEEE3EE, 0xEEE920, 0xEEEBFC, 0xEEEC20, 0xEEF1F2, 
            0xEFEEEB, 0xEFF0E5, 0xEFF0E8, 0xEFF0EE, 0xF0E0E2, 0xF0E5E4, 0xF1F2E0, 0xF1F2E2, 0xF1F2E8, 0xF1FF20, 0xF2E5EB, 0xF2EE20, 0xF2EEF0, 0xF2FC20, 0xF7F2EE, 0xFBF520, 
        };

        private static byte[] byteMap = {
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x00, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x61, (byte) 0x62, (byte) 0x63, (byte) 0x64, (byte) 0x65, (byte) 0x66, (byte) 0x67, 
            (byte) 0x68, (byte) 0x69, (byte) 0x6A, (byte) 0x6B, (byte) 0x6C, (byte) 0x6D, (byte) 0x6E, (byte) 0x6F, 
            (byte) 0x70, (byte) 0x71, (byte) 0x72, (byte) 0x73, (byte) 0x74, (byte) 0x75, (byte) 0x76, (byte) 0x77, 
            (byte) 0x78, (byte) 0x79, (byte) 0x7A, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x61, (byte) 0x62, (byte) 0x63, (byte) 0x64, (byte) 0x65, (byte) 0x66, (byte) 0x67, 
            (byte) 0x68, (byte) 0x69, (byte) 0x6A, (byte) 0x6B, (byte) 0x6C, (byte) 0x6D, (byte) 0x6E, (byte) 0x6F, 
            (byte) 0x70, (byte) 0x71, (byte) 0x72, (byte) 0x73, (byte) 0x74, (byte) 0x75, (byte) 0x76, (byte) 0x77, 
            (byte) 0x78, (byte) 0x79, (byte) 0x7A, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x90, (byte) 0x83, (byte) 0x20, (byte) 0x83, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x9A, (byte) 0x20, (byte) 0x9C, (byte) 0x9D, (byte) 0x9E, (byte) 0x9F, 
            (byte) 0x90, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x9A, (byte) 0x20, (byte) 0x9C, (byte) 0x9D, (byte) 0x9E, (byte) 0x9F, 
            (byte) 0x20, (byte) 0xA2, (byte) 0xA2, (byte) 0xBC, (byte) 0x20, (byte) 0xB4, (byte) 0x20, (byte) 0x20, 
            (byte) 0xB8, (byte) 0x20, (byte) 0xBA, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0xBF, 
            (byte) 0x20, (byte) 0x20, (byte) 0xB3, (byte) 0xB3, (byte) 0xB4, (byte) 0xB5, (byte) 0x20, (byte) 0x20, 
            (byte) 0xB8, (byte) 0x20, (byte) 0xBA, (byte) 0x20, (byte) 0xBC, (byte) 0xBE, (byte) 0xBE, (byte) 0xBF, 
            (byte) 0xE0, (byte) 0xE1, (byte) 0xE2, (byte) 0xE3, (byte) 0xE4, (byte) 0xE5, (byte) 0xE6, (byte) 0xE7, 
            (byte) 0xE8, (byte) 0xE9, (byte) 0xEA, (byte) 0xEB, (byte) 0xEC, (byte) 0xED, (byte) 0xEE, (byte) 0xEF, 
            (byte) 0xF0, (byte) 0xF1, (byte) 0xF2, (byte) 0xF3, (byte) 0xF4, (byte) 0xF5, (byte) 0xF6, (byte) 0xF7, 
            (byte) 0xF8, (byte) 0xF9, (byte) 0xFA, (byte) 0xFB, (byte) 0xFC, (byte) 0xFD, (byte) 0xFE, (byte) 0xFF, 
            (byte) 0xE0, (byte) 0xE1, (byte) 0xE2, (byte) 0xE3, (byte) 0xE4, (byte) 0xE5, (byte) 0xE6, (byte) 0xE7, 
            (byte) 0xE8, (byte) 0xE9, (byte) 0xEA, (byte) 0xEB, (byte) 0xEC, (byte) 0xED, (byte) 0xEE, (byte) 0xEF, 
            (byte) 0xF0, (byte) 0xF1, (byte) 0xF2, (byte) 0xF3, (byte) 0xF4, (byte) 0xF5, (byte) 0xF6, (byte) 0xF7, 
            (byte) 0xF8, (byte) 0xF9, (byte) 0xFA, (byte) 0xFB, (byte) 0xFC, (byte) 0xFD, (byte) 0xFE, (byte) 0xFF, 
        };

        public String getName()
        {
            return  "windows-1251";
        }
        
        public String getLanguage()
        {
            return "ru";
        }
        
        public int match(CharsetDetector det)
        {
            return match(det, ngrams, byteMap);
        }
    }

    static class CharsetRecog_IBM866_ru extends CharsetRecog_sbcs
    {
        private static int[] ngrams = {
            0x20E220, 0x20E2EE, 0x20E4EE, 0x20E7E0, 0x20E820, 0x20EAE0, 0x20EAEE, 0x20EDE0, 0x20EDE5, 0x20EEE1, 0x20EFEE, 0x20EFF0, 0x20F0E0, 0x20F1EE, 0x20F1F2, 0x20F2EE,
            0x20F7F2, 0x20FDF2, 0xE0EDE8, 0xE0F2FC, 0xE3EE20, 0xE5EBFC, 0xE5EDE8, 0xE5F1F2, 0xE5F220, 0xE820EF, 0xE8E520, 0xE8E820, 0xE8FF20, 0xEBE5ED, 0xEBE820, 0xEBFCED,
            0xEDE020, 0xEDE520, 0xEDE8E5, 0xEDE8FF, 0xEDEE20, 0xEDEEE2, 0xEE20E2, 0xEE20EF, 0xEE20F1, 0xEEE220, 0xEEE2E0, 0xEEE3EE, 0xEEE920, 0xEEEBFC, 0xEEEC20, 0xEEF1F2,
            0xEFEEEB, 0xEFF0E5, 0xEFF0E8, 0xEFF0EE, 0xF0E0E2, 0xF0E5E4, 0xF1F2E0, 0xF1F2E2, 0xF1F2E8, 0xF1FF20, 0xF2E5EB, 0xF2EE20, 0xF2EEF0, 0xF2FC20, 0xF7F2EE, 0xFBF520,
        };

        // bytemap converts cp866 chars to cp1251 chars, so ngrams are still unchanged
        private static byte[] byteMap = {
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20,
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20,
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20,
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20,
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x00,
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20,
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20,
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20,
            (byte) 0x20, (byte) 0x61, (byte) 0x62, (byte) 0x63, (byte) 0x64, (byte) 0x65, (byte) 0x66, (byte) 0x67,
            (byte) 0x68, (byte) 0x69, (byte) 0x6A, (byte) 0x6B, (byte) 0x6C, (byte) 0x6D, (byte) 0x6E, (byte) 0x6F,
            (byte) 0x70, (byte) 0x71, (byte) 0x72, (byte) 0x73, (byte) 0x74, (byte) 0x75, (byte) 0x76, (byte) 0x77,
            (byte) 0x78, (byte) 0x79, (byte) 0x7A, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20,
            (byte) 0x20, (byte) 0x61, (byte) 0x62, (byte) 0x63, (byte) 0x64, (byte) 0x65, (byte) 0x66, (byte) 0x67,
            (byte) 0x68, (byte) 0x69, (byte) 0x6A, (byte) 0x6B, (byte) 0x6C, (byte) 0x6D, (byte) 0x6E, (byte) 0x6F,
            (byte) 0x70, (byte) 0x71, (byte) 0x72, (byte) 0x73, (byte) 0x74, (byte) 0x75, (byte) 0x76, (byte) 0x77,
            (byte) 0x78, (byte) 0x79, (byte) 0x7A, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20,
            (byte) 0xE0, (byte) 0xE1, (byte) 0xE2, (byte) 0xE3, (byte) 0xE4, (byte) 0xE5, (byte) 0xE6, (byte) 0xE7,
            (byte) 0xE8, (byte) 0xE9, (byte) 0xEA, (byte) 0xEB, (byte) 0xEC, (byte) 0xED, (byte) 0xEE, (byte) 0xEF,
            (byte) 0xF0, (byte) 0xF1, (byte) 0xF2, (byte) 0xF3, (byte) 0xF4, (byte) 0xF5, (byte) 0xF6, (byte) 0xF7,
            (byte) 0xF8, (byte) 0xF9, (byte) 0xFA, (byte) 0xFB, (byte) 0xFC, (byte) 0xFD, (byte) 0xFE, (byte) 0xFF,
            (byte) 0xE0, (byte) 0xE1, (byte) 0xE2, (byte) 0xE3, (byte) 0xE4, (byte) 0xE5, (byte) 0xE6, (byte) 0xE7,
            (byte) 0xE8, (byte) 0xE9, (byte) 0xEA, (byte) 0xEB, (byte) 0xEC, (byte) 0xED, (byte) 0xEE, (byte) 0xEF,
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20,
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20,
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20,
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20,
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20,
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20,
            (byte) 0xF0, (byte) 0xF1, (byte) 0xF2, (byte) 0xF3, (byte) 0xF4, (byte) 0xF5, (byte) 0xF6, (byte) 0xF7,
            (byte) 0xF8, (byte) 0xF9, (byte) 0xFA, (byte) 0xFB, (byte) 0xFC, (byte) 0xFD, (byte) 0xFE, (byte) 0xFF,
            (byte) 0xB8, (byte) 0xB8, (byte) 0xBA, (byte) 0xBA, (byte) 0xBF, (byte) 0xBF, (byte) 0xA2, (byte) 0xA2,
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20,
        };

        public String getName()
        {
            return  "IBM866";
        }

        public String getLanguage()
        {
            return "ru";
        }

        public int match(CharsetDetector det)
        {
            return match(det, ngrams, byteMap);
        }
    }

    static class CharsetRecog_windows_1256 extends CharsetRecog_sbcs
    {
        private static int[] ngrams = {
            0x20C7E1, 0x20C7E4, 0x20C8C7, 0x20DAE1, 0x20DDED, 0x20E1E1, 0x20E3E4, 0x20E6C7, 0xC720C7, 0xC7C120, 0xC7CA20, 0xC7D120, 0xC7E120, 0xC7E1C3, 0xC7E1C7, 0xC7E1C8, 
            0xC7E1CA, 0xC7E1CC, 0xC7E1CD, 0xC7E1CF, 0xC7E1D3, 0xC7E1DA, 0xC7E1DE, 0xC7E1E3, 0xC7E1E6, 0xC7E1ED, 0xC7E320, 0xC7E420, 0xC7E4CA, 0xC820C7, 0xC920C7, 0xC920DD, 
            0xC920E1, 0xC920E3, 0xC920E6, 0xCA20C7, 0xCF20C7, 0xCFC920, 0xD120C7, 0xD1C920, 0xD320C7, 0xDA20C7, 0xDAE1EC, 0xDDED20, 0xE120C7, 0xE1C920, 0xE1EC20, 0xE1ED20, 
            0xE320C7, 0xE3C720, 0xE3C920, 0xE3E420, 0xE420C7, 0xE520C7, 0xE5C720, 0xE6C7E1, 0xE6E420, 0xEC20C7, 0xED20C7, 0xED20E3, 0xED20E6, 0xEDC920, 0xEDD120, 0xEDE420, 
        };

        private static byte[] byteMap = {
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x00, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x61, (byte) 0x62, (byte) 0x63, (byte) 0x64, (byte) 0x65, (byte) 0x66, (byte) 0x67, 
            (byte) 0x68, (byte) 0x69, (byte) 0x6A, (byte) 0x6B, (byte) 0x6C, (byte) 0x6D, (byte) 0x6E, (byte) 0x6F, 
            (byte) 0x70, (byte) 0x71, (byte) 0x72, (byte) 0x73, (byte) 0x74, (byte) 0x75, (byte) 0x76, (byte) 0x77, 
            (byte) 0x78, (byte) 0x79, (byte) 0x7A, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x61, (byte) 0x62, (byte) 0x63, (byte) 0x64, (byte) 0x65, (byte) 0x66, (byte) 0x67, 
            (byte) 0x68, (byte) 0x69, (byte) 0x6A, (byte) 0x6B, (byte) 0x6C, (byte) 0x6D, (byte) 0x6E, (byte) 0x6F, 
            (byte) 0x70, (byte) 0x71, (byte) 0x72, (byte) 0x73, (byte) 0x74, (byte) 0x75, (byte) 0x76, (byte) 0x77, 
            (byte) 0x78, (byte) 0x79, (byte) 0x7A, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x81, (byte) 0x20, (byte) 0x83, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x88, (byte) 0x20, (byte) 0x8A, (byte) 0x20, (byte) 0x9C, (byte) 0x8D, (byte) 0x8E, (byte) 0x8F, 
            (byte) 0x90, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x98, (byte) 0x20, (byte) 0x9A, (byte) 0x20, (byte) 0x9C, (byte) 0x20, (byte) 0x20, (byte) 0x9F, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0xAA, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0xB5, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0xC0, (byte) 0xC1, (byte) 0xC2, (byte) 0xC3, (byte) 0xC4, (byte) 0xC5, (byte) 0xC6, (byte) 0xC7, 
            (byte) 0xC8, (byte) 0xC9, (byte) 0xCA, (byte) 0xCB, (byte) 0xCC, (byte) 0xCD, (byte) 0xCE, (byte) 0xCF, 
            (byte) 0xD0, (byte) 0xD1, (byte) 0xD2, (byte) 0xD3, (byte) 0xD4, (byte) 0xD5, (byte) 0xD6, (byte) 0x20, 
            (byte) 0xD8, (byte) 0xD9, (byte) 0xDA, (byte) 0xDB, (byte) 0xDC, (byte) 0xDD, (byte) 0xDE, (byte) 0xDF, 
            (byte) 0xE0, (byte) 0xE1, (byte) 0xE2, (byte) 0xE3, (byte) 0xE4, (byte) 0xE5, (byte) 0xE6, (byte) 0xE7, 
            (byte) 0xE8, (byte) 0xE9, (byte) 0xEA, (byte) 0xEB, (byte) 0xEC, (byte) 0xED, (byte) 0xEE, (byte) 0xEF, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0xF4, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0xF9, (byte) 0x20, (byte) 0xFB, (byte) 0xFC, (byte) 0x20, (byte) 0x20, (byte) 0xFF, 
        };

        public String getName()
        {
            return  "windows-1256";
        }
        
        public String getLanguage()
        {
            return "ar";
        }
        
        public int match(CharsetDetector det)
        {
            return match(det, ngrams, byteMap);
        }
    }
        
    static class CharsetRecog_KOI8_R extends CharsetRecog_sbcs
    {
        private static int[] ngrams = {
            0x20C4CF, 0x20C920, 0x20CBC1, 0x20CBCF, 0x20CEC1, 0x20CEC5, 0x20CFC2, 0x20D0CF, 0x20D0D2, 0x20D2C1, 0x20D3CF, 0x20D3D4, 0x20D4CF, 0x20D720, 0x20D7CF, 0x20DAC1, 
            0x20DCD4, 0x20DED4, 0xC1CEC9, 0xC1D4D8, 0xC5CCD8, 0xC5CEC9, 0xC5D3D4, 0xC5D420, 0xC7CF20, 0xC920D0, 0xC9C520, 0xC9C920, 0xC9D120, 0xCCC5CE, 0xCCC920, 0xCCD8CE, 
            0xCEC120, 0xCEC520, 0xCEC9C5, 0xCEC9D1, 0xCECF20, 0xCECFD7, 0xCF20D0, 0xCF20D3, 0xCF20D7, 0xCFC7CF, 0xCFCA20, 0xCFCCD8, 0xCFCD20, 0xCFD3D4, 0xCFD720, 0xCFD7C1, 
            0xD0CFCC, 0xD0D2C5, 0xD0D2C9, 0xD0D2CF, 0xD2C1D7, 0xD2C5C4, 0xD3D120, 0xD3D4C1, 0xD3D4C9, 0xD3D4D7, 0xD4C5CC, 0xD4CF20, 0xD4CFD2, 0xD4D820, 0xD9C820, 0xDED4CF, 
        };

        private static byte[] byteMap = {
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x00, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x61, (byte) 0x62, (byte) 0x63, (byte) 0x64, (byte) 0x65, (byte) 0x66, (byte) 0x67, 
            (byte) 0x68, (byte) 0x69, (byte) 0x6A, (byte) 0x6B, (byte) 0x6C, (byte) 0x6D, (byte) 0x6E, (byte) 0x6F, 
            (byte) 0x70, (byte) 0x71, (byte) 0x72, (byte) 0x73, (byte) 0x74, (byte) 0x75, (byte) 0x76, (byte) 0x77, 
            (byte) 0x78, (byte) 0x79, (byte) 0x7A, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x61, (byte) 0x62, (byte) 0x63, (byte) 0x64, (byte) 0x65, (byte) 0x66, (byte) 0x67, 
            (byte) 0x68, (byte) 0x69, (byte) 0x6A, (byte) 0x6B, (byte) 0x6C, (byte) 0x6D, (byte) 0x6E, (byte) 0x6F, 
            (byte) 0x70, (byte) 0x71, (byte) 0x72, (byte) 0x73, (byte) 0x74, (byte) 0x75, (byte) 0x76, (byte) 0x77, 
            (byte) 0x78, (byte) 0x79, (byte) 0x7A, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0xA3, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0xA3, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, (byte) 0x20, 
            (byte) 0xC0, (byte) 0xC1, (byte) 0xC2, (byte) 0xC3, (byte) 0xC4, (byte) 0xC5, (byte) 0xC6, (byte) 0xC7, 
            (byte) 0xC8, (byte) 0xC9, (byte) 0xCA, (byte) 0xCB, (byte) 0xCC, (byte) 0xCD, (byte) 0xCE, (byte) 0xCF, 
            (byte) 0xD0, (byte) 0xD1, (byte) 0xD2, (byte) 0xD3, (byte) 0xD4, (byte) 0xD5, (byte) 0xD6, (byte) 0xD7, 
            (byte) 0xD8, (byte) 0xD9, (byte) 0xDA, (byte) 0xDB, (byte) 0xDC, (byte) 0xDD, (byte) 0xDE, (byte) 0xDF, 
            (byte) 0xC0, (byte) 0xC1, (byte) 0xC2, (byte) 0xC3, (byte) 0xC4, (byte) 0xC5, (byte) 0xC6, (byte) 0xC7, 
            (byte) 0xC8, (byte) 0xC9, (byte) 0xCA, (byte) 0xCB, (byte) 0xCC, (byte) 0xCD, (byte) 0xCE, (byte) 0xCF, 
            (byte) 0xD0, (byte) 0xD1, (byte) 0xD2, (byte) 0xD3, (byte) 0xD4, (byte) 0xD5, (byte) 0xD6, (byte) 0xD7, 
            (byte) 0xD8, (byte) 0xD9, (byte) 0xDA, (byte) 0xDB, (byte) 0xDC, (byte) 0xDD, (byte) 0xDE, (byte) 0xDF, 
        };
        
        public String getName()
        {
            return  "KOI8-R";
        }
        
        public String getLanguage()
        {
            return "ru";
        }
        
        public int match(CharsetDetector det)
        {
            return match(det, ngrams, byteMap);
        }
    }
    
    abstract static class CharsetRecog_IBM424_he extends CharsetRecog_sbcs
    {
        protected static byte[] byteMap = {
/*                 -0           -1           -2           -3           -4           -5           -6           -7           -8           -9           -A           -B           -C           -D           -E           -F   */
/* 0- */    (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, 
/* 1- */    (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, 
/* 2- */    (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, 
/* 3- */    (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, 
/* 4- */    (byte) 0x40, (byte) 0x41, (byte) 0x42, (byte) 0x43, (byte) 0x44, (byte) 0x45, (byte) 0x46, (byte) 0x47, (byte) 0x48, (byte) 0x49, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, 
/* 5- */    (byte) 0x40, (byte) 0x51, (byte) 0x52, (byte) 0x53, (byte) 0x54, (byte) 0x55, (byte) 0x56, (byte) 0x57, (byte) 0x58, (byte) 0x59, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, 
/* 6- */    (byte) 0x40, (byte) 0x40, (byte) 0x62, (byte) 0x63, (byte) 0x64, (byte) 0x65, (byte) 0x66, (byte) 0x67, (byte) 0x68, (byte) 0x69, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, 
/* 7- */    (byte) 0x40, (byte) 0x71, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x00, (byte) 0x40, (byte) 0x40, 
/* 8- */    (byte) 0x40, (byte) 0x81, (byte) 0x82, (byte) 0x83, (byte) 0x84, (byte) 0x85, (byte) 0x86, (byte) 0x87, (byte) 0x88, (byte) 0x89, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, 
/* 9- */    (byte) 0x40, (byte) 0x91, (byte) 0x92, (byte) 0x93, (byte) 0x94, (byte) 0x95, (byte) 0x96, (byte) 0x97, (byte) 0x98, (byte) 0x99, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, 
/* A- */    (byte) 0xA0, (byte) 0x40, (byte) 0xA2, (byte) 0xA3, (byte) 0xA4, (byte) 0xA5, (byte) 0xA6, (byte) 0xA7, (byte) 0xA8, (byte) 0xA9, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, 
/* B- */    (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, 
/* C- */    (byte) 0x40, (byte) 0x81, (byte) 0x82, (byte) 0x83, (byte) 0x84, (byte) 0x85, (byte) 0x86, (byte) 0x87, (byte) 0x88, (byte) 0x89, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, 
/* D- */    (byte) 0x40, (byte) 0x91, (byte) 0x92, (byte) 0x93, (byte) 0x94, (byte) 0x95, (byte) 0x96, (byte) 0x97, (byte) 0x98, (byte) 0x99, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, 
/* E- */    (byte) 0x40, (byte) 0x40, (byte) 0xA2, (byte) 0xA3, (byte) 0xA4, (byte) 0xA5, (byte) 0xA6, (byte) 0xA7, (byte) 0xA8, (byte) 0xA9, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, 
/* F- */    (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, 
        };

        public String getLanguage()
        {
            return "he";
        }
    }
    static class CharsetRecog_IBM424_he_rtl extends CharsetRecog_IBM424_he 
    {
        public String getName()
        {
            return "IBM424_rtl";
        }
        private static int[] ngrams = {
            0x404146, 0x404148, 0x404151, 0x404171, 0x404251, 0x404256, 0x404541, 0x404546, 0x404551, 0x404556, 0x404562, 0x404569, 0x404571, 0x405441, 0x405445, 0x405641, 
            0x406254, 0x406954, 0x417140, 0x454041, 0x454042, 0x454045, 0x454054, 0x454056, 0x454069, 0x454641, 0x464140, 0x465540, 0x465740, 0x466840, 0x467140, 0x514045, 
            0x514540, 0x514671, 0x515155, 0x515540, 0x515740, 0x516840, 0x517140, 0x544041, 0x544045, 0x544140, 0x544540, 0x554041, 0x554042, 0x554045, 0x554054, 0x554056, 
            0x554069, 0x564540, 0x574045, 0x584540, 0x585140, 0x585155, 0x625440, 0x684045, 0x685155, 0x695440, 0x714041, 0x714042, 0x714045, 0x714054, 0x714056, 0x714069, 
        };
        public int match(CharsetDetector det)
        {
            return match(det, ngrams, byteMap, (byte)0x40);
        }
    }
    static class CharsetRecog_IBM424_he_ltr extends CharsetRecog_IBM424_he 
    {
        public String getName()
        {
            return "IBM424_ltr";
        }
        private static int[] ngrams = {
            0x404146, 0x404154, 0x404551, 0x404554, 0x404556, 0x404558, 0x405158, 0x405462, 0x405469, 0x405546, 0x405551, 0x405746, 0x405751, 0x406846, 0x406851, 0x407141,
            0x407146, 0x407151, 0x414045, 0x414054, 0x414055, 0x414071, 0x414540, 0x414645, 0x415440, 0x415640, 0x424045, 0x424055, 0x424071, 0x454045, 0x454051, 0x454054,
            0x454055, 0x454057, 0x454068, 0x454071, 0x455440, 0x464140, 0x464540, 0x484140, 0x514140, 0x514240, 0x514540, 0x544045, 0x544055, 0x544071, 0x546240, 0x546940,
            0x555151, 0x555158, 0x555168, 0x564045, 0x564055, 0x564071, 0x564240, 0x564540, 0x624540, 0x694045, 0x694055, 0x694071, 0x694540, 0x714140, 0x714540, 0x714651

        };
        public int match(CharsetDetector det)
        {
            return match(det, ngrams, byteMap, (byte)0x40);
        }
    }
    
    abstract static class CharsetRecog_IBM420_ar extends CharsetRecog_sbcs
    {
        //arabic shaping class, method shape/unshape
        //protected static ArabicShaping as = new ArabicShaping(ArabicShaping.LETTERS_UNSHAPE);
        protected byte[] prev_fInputBytes = null;

        protected static byte[] byteMap = {
/*                 -0           -1           -2           -3           -4           -5           -6           -7           -8           -9           -A           -B           -C           -D           -E           -F   */
/* 0- */    (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, 
/* 1- */    (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, 
/* 2- */    (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, 
/* 3- */    (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, 
/* 4- */    (byte) 0x40, (byte) 0x40, (byte) 0x42, (byte) 0x43, (byte) 0x44, (byte) 0x45, (byte) 0x46, (byte) 0x47, (byte) 0x48, (byte) 0x49, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, 
/* 5- */    (byte) 0x40, (byte) 0x51, (byte) 0x52, (byte) 0x40, (byte) 0x40, (byte) 0x55, (byte) 0x56, (byte) 0x57, (byte) 0x58, (byte) 0x59, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, 
/* 6- */    (byte) 0x40, (byte) 0x40, (byte) 0x62, (byte) 0x63, (byte) 0x64, (byte) 0x65, (byte) 0x66, (byte) 0x67, (byte) 0x68, (byte) 0x69, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, 
/* 7- */    (byte) 0x70, (byte) 0x71, (byte) 0x72, (byte) 0x73, (byte) 0x74, (byte) 0x75, (byte) 0x76, (byte) 0x77, (byte) 0x78, (byte) 0x79, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, 
/* 8- */    (byte) 0x80, (byte) 0x81, (byte) 0x82, (byte) 0x83, (byte) 0x84, (byte) 0x85, (byte) 0x86, (byte) 0x87, (byte) 0x88, (byte) 0x89, (byte) 0x8A, (byte) 0x8B, (byte) 0x8C, (byte) 0x8D, (byte) 0x8E, (byte) 0x8F, 
/* 9- */    (byte) 0x90, (byte) 0x91, (byte) 0x92, (byte) 0x93, (byte) 0x94, (byte) 0x95, (byte) 0x96, (byte) 0x97, (byte) 0x98, (byte) 0x99, (byte) 0x9A, (byte) 0x9B, (byte) 0x9C, (byte) 0x9D, (byte) 0x9E, (byte) 0x9F, 
/* A- */    (byte) 0xA0, (byte) 0x40, (byte) 0xA2, (byte) 0xA3, (byte) 0xA4, (byte) 0xA5, (byte) 0xA6, (byte) 0xA7, (byte) 0xA8, (byte) 0xA9, (byte) 0xAA, (byte) 0xAB, (byte) 0xAC, (byte) 0xAD, (byte) 0xAE, (byte) 0xAF, 
/* B- */    (byte) 0xB0, (byte) 0xB1, (byte) 0xB2, (byte) 0xB3, (byte) 0xB4, (byte) 0xB5, (byte) 0x40, (byte) 0x40, (byte) 0xB8, (byte) 0xB9, (byte) 0xBA, (byte) 0xBB, (byte) 0xBC, (byte) 0xBD, (byte) 0xBE, (byte) 0xBF, 
/* C- */    (byte) 0x40, (byte) 0x81, (byte) 0x82, (byte) 0x83, (byte) 0x84, (byte) 0x85, (byte) 0x86, (byte) 0x87, (byte) 0x88, (byte) 0x89, (byte) 0x40, (byte) 0xCB, (byte) 0x40, (byte) 0xCD, (byte) 0x40, (byte) 0xCF, 
/* D- */    (byte) 0x40, (byte) 0x91, (byte) 0x92, (byte) 0x93, (byte) 0x94, (byte) 0x95, (byte) 0x96, (byte) 0x97, (byte) 0x98, (byte) 0x99, (byte) 0xDA, (byte) 0xDB, (byte) 0xDC, (byte) 0xDD, (byte) 0xDE, (byte) 0xDF, 
/* E- */    (byte) 0x40, (byte) 0x40, (byte) 0xA2, (byte) 0xA3, (byte) 0xA4, (byte) 0xA5, (byte) 0xA6, (byte) 0xA7, (byte) 0xA8, (byte) 0xA9, (byte) 0xEA, (byte) 0xEB, (byte) 0x40, (byte) 0xED, (byte) 0xEE, (byte) 0xEF, 
/* F- */    (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0xFB, (byte) 0xFC, (byte) 0xFD, (byte) 0xFE, (byte) 0x40, 
        };
        
        protected static byte[] unshapeMap = {
/*                 -0           -1           -2           -3           -4           -5           -6           -7           -8           -9           -A           -B           -C           -D           -E           -F   */
/* 0- */    (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, 
/* 1- */    (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, 
/* 2- */    (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, 
/* 3- */    (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, (byte) 0x40, 
/* 4- */    (byte) 0x40, (byte) 0x40, (byte) 0x42, (byte) 0x42, (byte) 0x44, (byte) 0x45, (byte) 0x46, (byte) 0x47, (byte) 0x47, (byte) 0x49, (byte) 0x4A, (byte) 0x4B, (byte) 0x4C, (byte) 0x4D, (byte) 0x4E, (byte) 0x4F, 
/* 5- */    (byte) 0x50, (byte) 0x49, (byte) 0x52, (byte) 0x53, (byte) 0x54, (byte) 0x55, (byte) 0x56, (byte) 0x56, (byte) 0x58, (byte) 0x58, (byte) 0x5A, (byte) 0x5B, (byte) 0x5C, (byte) 0x5D, (byte) 0x5E, (byte) 0x5F, 
/* 6- */    (byte) 0x60, (byte) 0x61, (byte) 0x62, (byte) 0x63, (byte) 0x63, (byte) 0x65, (byte) 0x65, (byte) 0x67, (byte) 0x67, (byte) 0x69, (byte) 0x6A, (byte) 0x6B, (byte) 0x6C, (byte) 0x6D, (byte) 0x6E, (byte) 0x6F, 
/* 7- */    (byte) 0x69, (byte) 0x71, (byte) 0x71, (byte) 0x73, (byte) 0x74, (byte) 0x75, (byte) 0x76, (byte) 0x77, (byte) 0x77, (byte) 0x79, (byte) 0x7A, (byte) 0x7B, (byte) 0x7C, (byte) 0x7D, (byte) 0x7E, (byte) 0x7F, 
/* 8- */    (byte) 0x80, (byte) 0x81, (byte) 0x82, (byte) 0x83, (byte) 0x84, (byte) 0x85, (byte) 0x86, (byte) 0x87, (byte) 0x88, (byte) 0x89, (byte) 0x80, (byte) 0x8B, (byte) 0x8B, (byte) 0x8D, (byte) 0x8D, (byte) 0x8F, 
/* 9- */    (byte) 0x90, (byte) 0x91, (byte) 0x92, (byte) 0x93, (byte) 0x94, (byte) 0x95, (byte) 0x96, (byte) 0x97, (byte) 0x98, (byte) 0x99, (byte) 0x9A, (byte) 0x9A, (byte) 0x9A, (byte) 0x9A, (byte) 0x9E, (byte) 0x9E, 
/* A- */    (byte) 0x9E, (byte) 0xA1, (byte) 0xA2, (byte) 0xA3, (byte) 0xA4, (byte) 0xA5, (byte) 0xA6, (byte) 0xA7, (byte) 0xA8, (byte) 0xA9, (byte) 0x9E, (byte) 0xAB, (byte) 0xAB, (byte) 0xAD, (byte) 0xAD, (byte) 0xAF, 
/* B- */    (byte) 0xAF, (byte) 0xB1, (byte) 0xB2, (byte) 0xB3, (byte) 0xB4, (byte) 0xB5, (byte) 0xB6, (byte) 0xB7, (byte) 0xB8, (byte) 0xB9, (byte) 0xB1, (byte) 0xBB, (byte) 0xBB, (byte) 0xBD, (byte) 0xBD, (byte) 0xBF, 
/* C- */    (byte) 0xC0, (byte) 0xC1, (byte) 0xC2, (byte) 0xC3, (byte) 0xC4, (byte) 0xC5, (byte) 0xC6, (byte) 0xC7, (byte) 0xC8, (byte) 0xC9, (byte) 0xCA, (byte) 0xBF, (byte) 0xCC, (byte) 0xBF, (byte) 0xCE, (byte) 0xCF, 
/* D- */    (byte) 0xD0, (byte) 0xD1, (byte) 0xD2, (byte) 0xD3, (byte) 0xD4, (byte) 0xD5, (byte) 0xD6, (byte) 0xD7, (byte) 0xD8, (byte) 0xD9, (byte) 0xDA, (byte) 0xDA, (byte) 0xDC, (byte) 0xDC, (byte) 0xDC, (byte) 0xDF, 
/* E- */    (byte) 0xE0, (byte) 0xE1, (byte) 0xE2, (byte) 0xE3, (byte) 0xE4, (byte) 0xE5, (byte) 0xE6, (byte) 0xE7, (byte) 0xE8, (byte) 0xE9, (byte) 0xEA, (byte) 0xEB, (byte) 0xEC, (byte) 0xED, (byte) 0xEE, (byte) 0xEF, 
/* F- */    (byte) 0xF0, (byte) 0xF1, (byte) 0xF2, (byte) 0xF3, (byte) 0xF4, (byte) 0xF5, (byte) 0xF6, (byte) 0xF7, (byte) 0xF8, (byte) 0xF9, (byte) 0xFA, (byte) 0xFB, (byte) 0xFC, (byte) 0xFD, (byte) 0xFE, (byte) 0xFF, 
        };

        public String getLanguage()
        {
            return "ar";
        }
        protected void matchInit(CharsetDetector det) 
        {
            prev_fInputBytes = det.fInputBytes.clone();
            byte bb[] = unshape(det.fInputBytes);
            det.setText(bb);
        }
        
        /*
         * Arabic shaping needs to be done manually. Cannot call ArabicShaping class
         * because CharsetDetector is dealing with bytes not Unicode code points. We could
         * convert the bytes to Unicode code points but that would leave us dependent
         * on CharsetICU which we try to avoid. IBM420 converter amongst different versions
         * of JDK can produce different results and therefore is also avoided.
         */
        private byte[] unshape(byte[] inputBytes) {
            byte resultByteArr[] = unshapeLamAlef(inputBytes);
            
            for (int i=0; i<inputBytes.length; i++){
                resultByteArr[i] = unshapeMap[resultByteArr[i]& 0xFF];
            }
            return resultByteArr;
        }

        private byte[] unshapeLamAlef(byte[] inputBytes) {
            ByteBuffer resultBigBuffer =  ByteBuffer.allocate(inputBytes.length*2);
            ByteBuffer resultBuffer;
            byte unshapedLamAlef[] = {(byte)0xb1, (byte)0x56};

            for (byte inputByte : inputBytes){
                if (isLamAlef(inputByte))
                    resultBigBuffer.put(unshapedLamAlef);
                else
                    resultBigBuffer.put(inputByte);
            }
            resultBuffer = ByteBuffer.allocate(resultBigBuffer.position());
            resultBuffer.put(resultBigBuffer.array(),0, resultBigBuffer.position());
            return resultBuffer.array();
        }
        
        private boolean isLamAlef(byte b) {
            // Return true if byte is any of these:
            //
            //   {(byte)0xb2,(byte)0xb3,(byte)0xb4,(byte)0xb5,(byte)0xb7,(byte)0xb8}
            // 
            // NOTE: 0xb2 is -78; 0xb8 is -72:
            return (b <= (byte)0xb8) && (b >= (byte)0xb2) && (b != (byte)0xb6);
        }
        
        protected void matchFinish(CharsetDetector det) {
            if (prev_fInputBytes != null)
                det.setText(prev_fInputBytes);
        }
        
    }
    static class CharsetRecog_IBM420_ar_rtl extends CharsetRecog_IBM420_ar 
    {
        private static int[] ngrams = {
            0x4056B1, 0x4056BD, 0x405856, 0x409AB1, 0x40ABDC, 0x40B1B1, 0x40BBBD, 0x40CF56, 0x564056, 0x564640, 0x566340, 0x567540, 0x56B140, 0x56B149, 0x56B156, 0x56B158,
            0x56B163, 0x56B167, 0x56B169, 0x56B173, 0x56B178, 0x56B19A, 0x56B1AD, 0x56B1BB, 0x56B1CF, 0x56B1DC, 0x56BB40, 0x56BD40, 0x56BD63, 0x584056, 0x624056, 0x6240AB,
            0x6240B1, 0x6240BB, 0x6240CF, 0x634056, 0x734056, 0x736240, 0x754056, 0x756240, 0x784056, 0x9A4056, 0x9AB1DA, 0xABDC40, 0xB14056, 0xB16240, 0xB1DA40, 0xB1DC40,
            0xBB4056, 0xBB5640, 0xBB6240, 0xBBBD40, 0xBD4056, 0xBF4056, 0xBF5640, 0xCF56B1, 0xCFBD40, 0xDA4056, 0xDC4056, 0xDC40BB, 0xDC40CF, 0xDC6240, 0xDC7540, 0xDCBD40,
        };

        public String getName()
        {
            return "IBM420_rtl";
        }
        public int match(CharsetDetector det)
        {
            matchInit(det);
            int result =  match(det, ngrams, byteMap, (byte)0x40);
            matchFinish(det);
            return result;
        }
        
    }
    static class CharsetRecog_IBM420_ar_ltr extends CharsetRecog_IBM420_ar 
    {
        private static int[] ngrams = {
            0x404656, 0x4056BB, 0x4056BF, 0x406273, 0x406275, 0x4062B1, 0x4062BB, 0x4062DC, 0x406356, 0x407556, 0x4075DC, 0x40B156, 0x40BB56, 0x40BD56, 0x40BDBB, 0x40BDCF, 
            0x40BDDC, 0x40DAB1, 0x40DCAB, 0x40DCB1, 0x49B156, 0x564056, 0x564058, 0x564062, 0x564063, 0x564073, 0x564075, 0x564078, 0x56409A, 0x5640B1, 0x5640BB, 0x5640BD,
            0x5640BF, 0x5640DA, 0x5640DC, 0x565840, 0x56B156, 0x56CF40, 0x58B156, 0x63B156, 0x63BD56, 0x67B156, 0x69B156, 0x73B156, 0x78B156, 0x9AB156, 0xAB4062, 0xADB156,
            0xB14062, 0xB15640, 0xB156CF, 0xB19A40, 0xB1B140, 0xBB4062, 0xBB40DC, 0xBBB156, 0xBD5640, 0xBDBB40, 0xCF4062, 0xCF40DC, 0xCFB156, 0xDAB19A, 0xDCAB40, 0xDCB156
        };

        public String getName()
        {
            return "IBM420_ltr";
        }
        public int match(CharsetDetector det)
        {
            matchInit(det);
            int result = match(det, ngrams, byteMap, (byte)0x40);
            matchFinish(det);
            return result;
        }
    }
    
    static abstract class CharsetRecog_EBCDIC_500 extends CharsetRecog_sbcs
    {
        // This maps EBCDIC 500 codepoints onto either space (not of interest), or a lower
        //  case ISO_8859_1 number/letter/accented-letter codepoint for ngram matching
        // Because we map to ISO_8859_1, we can re-use the ngrams from those detectors
        // To avoid mis-detection, we skip many of the control characters in the 0x00-0x3f range 
        protected static byte[] byteMap = {
/* 0x00-0x07 */ (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x00,
/* 0x08-0x0f */ (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20,
/* 0x10-0x17 */ (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20,
/* 0x18-0x1f */ (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20,
/* 0x20-0x27 */ (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20,
/* 0x28-0x2f */ (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x00, (byte)0x00,
/* 0x30-0x37 */ (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x00,
/* 0x38-0x3f */ (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x00, (byte)0x00, (byte)0x00, (byte)0x00,
/* 0x40-0x47 */ (byte)0x20, (byte)0x20, (byte)0xe2, (byte)0xe4, (byte)0xe0, (byte)0xe1, (byte)0xe3, (byte)0xe5,
/* 0x48-0x4f */ (byte)0xe7, (byte)0xf1, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20,
/* 0x50-0x57 */ (byte)0x20, (byte)0xe9, (byte)0xea, (byte)0xeb, (byte)0xe8, (byte)0xed, (byte)0xee, (byte)0xef,
/* 0x58-0x5f */ (byte)0xec, (byte)0xdf, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20,
/* 0x60-0x67 */ (byte)0x20, (byte)0x20, (byte)0xe2, (byte)0xe4, (byte)0xe0, (byte)0xe1, (byte)0xe3, (byte)0xe5,
/* 0x68-0x6f */ (byte)0xe7, (byte)0xf1, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20,
/* 0x70-0x77 */ (byte)0xf8, (byte)0xe9, (byte)0xea, (byte)0xeb, (byte)0xe8, (byte)0xed, (byte)0xee, (byte)0xef,
/* 0x78-0x7f */ (byte)0xec, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20,
/* 0x80-0x87 */ (byte)0xd8, (byte)'a',  (byte)'b',  (byte)'c',  (byte)'d',  (byte)'e',  (byte)'f',  (byte)'g',
/* 0x88-0x8f */ (byte)'h',  (byte)'i',  (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20,
/* 0x90-0x97 */ (byte)0x20, (byte)'j',  (byte)'k',  (byte)'l',  (byte)'m',  (byte)'n',  (byte)'o',  (byte)'p', 
/* 0x98-0x9f */ (byte)'q',  (byte)'r',  (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20,
/* 0xa0-0xa7 */ (byte)0x20, (byte)0x20, (byte)'s',  (byte)'t',  (byte)'u',  (byte)'v',  (byte)'w',  (byte)'x',
/* 0xa8-0xaf */ (byte)'y',  (byte)'z',  (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20,
/* 0xb0-0xb7 */ (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20,
/* 0xb8-0xbf */ (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20, (byte)0x20,
/* 0xc0-0xc7 */ (byte)0x20, (byte)'a',  (byte)'b',  (byte)'c',  (byte)'d',  (byte)'e',  (byte)'f',  (byte)'g',
/* 0xc8-0xcf */ (byte)'h',  (byte)'i',  (byte)0x20, (byte)0xf4, (byte)0xf6, (byte)0xf2, (byte)0xf3, (byte)0xf5,
/* 0xd0-0xd7 */ (byte)0x20, (byte)'j',  (byte)'k',  (byte)'l',  (byte)'m',  (byte)'n',  (byte)'o',  (byte)'p', 
/* 0xd8-0xdf */ (byte)'q',  (byte)'r',  (byte)0x20, (byte)0xfb, (byte)0xfc, (byte)0xf9, (byte)0xfa, (byte)0xff,
/* 0xe0-0xe7 */ (byte)0x20, (byte)0x20, (byte)'s',  (byte)'t',  (byte)'u',  (byte)'v',  (byte)'w',  (byte)'x',
/* 0xe8-0xef */ (byte)'y',  (byte)'z',  (byte)0x20, (byte)0xf4, (byte)0xf6, (byte)0xf2, (byte)0xf3, (byte)0xf5,
/* 0xf0-0xf7 */ (byte)'0',  (byte)'1',  (byte)'2',  (byte)'3',  (byte)'4',  (byte)'5',  (byte)'6',  (byte)'7', 
/* 0xf8-0xff */ (byte)'8',  (byte)'9',  (byte)0x20, (byte)0xfb, (byte)0xfc, (byte)0xf9, (byte)0xfa, (byte)0x20,
        };
        
        public String getName()
        {
            return "IBM500";
        }
    }
    
    static class CharsetRecog_EBCDIC_500_en extends CharsetRecog_EBCDIC_500
    {
        public String getLanguage()
        {
            return "en";
        }
        public int match(CharsetDetector det)
        {
            return match(det, CharsetRecog_8859_1_en.ngrams, byteMap);
        }
    }
    
    static class CharsetRecog_EBCDIC_500_de extends CharsetRecog_EBCDIC_500
    {
        public String getLanguage()
        {
            return "de";
        }
        public int match(CharsetDetector det)
        {
            return match(det, CharsetRecog_8859_1_de.ngrams, byteMap);
        }
    }
    
    static class CharsetRecog_EBCDIC_500_fr extends CharsetRecog_EBCDIC_500
    {
        public String getLanguage()
        {
            return "fr";
        }
        public int match(CharsetDetector det)
        {
            return match(det, CharsetRecog_8859_1_fr.ngrams, byteMap);
        }
    }
    
    static class CharsetRecog_EBCDIC_500_es extends CharsetRecog_EBCDIC_500
    {
        public String getLanguage()
        {
            return "es";
        }
        public int match(CharsetDetector det)
        {
            return match(det, CharsetRecog_8859_1_es.ngrams, byteMap);
        }
    }
    
    static class CharsetRecog_EBCDIC_500_it extends CharsetRecog_EBCDIC_500
    {
        public String getLanguage()
        {
            return "it";
        }
        public int match(CharsetDetector det)
        {
            return match(det, CharsetRecog_8859_1_it.ngrams, byteMap);
        }
    }
    
    static class CharsetRecog_EBCDIC_500_nl extends CharsetRecog_EBCDIC_500
    {
        public String getLanguage()
        {
            return "nl";
        }
        public int match(CharsetDetector det)
        {
            return match(det, CharsetRecog_8859_1_nl.ngrams, byteMap);
        }
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/txt/CharsetRecog_Unicode.java,true,"/*
 *******************************************************************************
 * Copyright (C) 1996-2007, International Business Machines Corporation and    *
 * others. All Rights Reserved.                                                *
 *******************************************************************************
 *
 */
package org.apache.tika.parser.txt;

/**
 * This class matches UTF-16 and UTF-32, both big- and little-endian. The
 * BOM will be used if it is present.
 * 
 * @internal
 */
abstract class CharsetRecog_Unicode extends CharsetRecognizer {

    /* (non-Javadoc)
     * @see com.ibm.icu.text.CharsetRecognizer#getName()
     */
    abstract String getName();

    /* (non-Javadoc)
     * @see com.ibm.icu.text.CharsetRecognizer#match(com.ibm.icu.text.CharsetDetector)
     */
    abstract int match(CharsetDetector det);
    
    static class CharsetRecog_UTF_16_BE extends CharsetRecog_Unicode
    {
        String getName()
        {
            return "UTF-16BE";
        }
        
        int match(CharsetDetector det)
        {
            byte[] input = det.fRawInput;
            
            if (input.length>=2 && ((input[0] & 0xFF) == 0xFE && (input[1] & 0xFF) == 0xFF)) {
                return 100;
            }
            
            // TODO: Do some statistics to check for unsigned UTF-16BE
            return 0;
        }
    }
    
    static class CharsetRecog_UTF_16_LE extends CharsetRecog_Unicode
    {
        String getName()
        {
            return "UTF-16LE";
        }
        
        int match(CharsetDetector det)
        {
            byte[] input = det.fRawInput;
            
            if (input.length >= 2 && ((input[0] & 0xFF) == 0xFF && (input[1] & 0xFF) == 0xFE))
            {
               // An LE BOM is present.
               if (input.length>=4 && input[2] == 0x00 && input[3] == 0x00) {
                   // It is probably UTF-32 LE, not UTF-16
                   return 0;
               }
               return 100;
            }        
            
            // TODO: Do some statistics to check for unsigned UTF-16LE
            return 0;
        }
    }
    
    static abstract class CharsetRecog_UTF_32 extends CharsetRecog_Unicode
    {
        abstract int getChar(byte[] input, int index);
        
        abstract String getName();
        
        int match(CharsetDetector det)
        {
            byte[] input   = det.fRawInput;
            int limit      = (det.fRawLength / 4) * 4;
            int numValid   = 0;
            int numInvalid = 0;
            boolean hasBOM = false;
            int confidence = 0;
            
            if (limit==0) {
                return 0;
            }
            if (getChar(input, 0) == 0x0000FEFF) {
                hasBOM = true;
            }
            
            for(int i = 0; i < limit; i += 4) {
                int ch = getChar(input, i);
                
                if (ch < 0 || ch >= 0x10FFFF || (ch >= 0xD800 && ch <= 0xDFFF)) {
                    numInvalid += 1;
                } else {
                    numValid += 1;
                }
            }
            
            
            // Cook up some sort of confidence score, based on presence of a BOM
            //    and the existence of valid and/or invalid multi-byte sequences.
            if (hasBOM && numInvalid==0) {
                confidence = 100;
            } else if (hasBOM && numValid > numInvalid*10) {
                confidence = 80;
            } else if (numValid > 3 && numInvalid == 0) {
                confidence = 100;            
            } else if (numValid > 0 && numInvalid == 0) {
                confidence = 80;
            } else if (numValid > numInvalid*10) {
                // Probably corrupt UTF-32BE data.  Valid sequences aren't likely by chance.
                confidence = 25;
            }
            
            return confidence;
        }
    }
    
    static class CharsetRecog_UTF_32_BE extends CharsetRecog_UTF_32
    {
        int getChar(byte[] input, int index)
        {
            return (input[index + 0] & 0xFF) << 24 | (input[index + 1] & 0xFF) << 16 |
                   (input[index + 2] & 0xFF) <<  8 | (input[index + 3] & 0xFF);
        }
        
        String getName()
        {
            return "UTF-32BE";
        }
    }

    
    static class CharsetRecog_UTF_32_LE extends CharsetRecog_UTF_32
    {
        int getChar(byte[] input, int index)
        {
            return (input[index + 3] & 0xFF) << 24 | (input[index + 2] & 0xFF) << 16 |
                   (input[index + 1] & 0xFF) <<  8 | (input[index + 0] & 0xFF);
        }
        
        String getName()
        {
            return "UTF-32LE";
        }
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/txt/CharsetRecog_UTF8.java,true,"/**
*******************************************************************************
* Copyright (C) 2005 - 2007, International Business Machines Corporation and  *
* others. All Rights Reserved.                                                *
*******************************************************************************
*/
package org.apache.tika.parser.txt;

/**
 * Charset recognizer for UTF-8
 *
 * @internal
 */
class CharsetRecog_UTF8 extends CharsetRecognizer {

    String getName() {
        return "UTF-8";
    }

    /* (non-Javadoc)
     * @see com.ibm.icu.text.CharsetRecognizer#match(com.ibm.icu.text.CharsetDetector)
     */
    int match(CharsetDetector det) {
        boolean     hasBOM = false;
        int         numValid = 0;
        int         numInvalid = 0;
        byte        input[] = det.fRawInput;
        int         i;
        int         trailBytes = 0;
        int         confidence;
        
        if (det.fRawLength >= 3 && 
                (input[0] & 0xFF) == 0xef && (input[1] & 0xFF) == 0xbb && (input[2] & 0xFF) == 0xbf) {
            hasBOM = true;
        }
        
        // Scan for multi-byte sequences
        for (i=0; i<det.fRawLength; i++) {
            int b = input[i];
            if ((b & 0x80) == 0) {
                continue;   // ASCII
            }
            
            // Hi bit on char found.  Figure out how long the sequence should be
            if ((b & 0x0e0) == 0x0c0) {
                trailBytes = 1;                
            } else if ((b & 0x0f0) == 0x0e0) {
                trailBytes = 2;
            } else if ((b & 0x0f8) == 0xf0) {
                trailBytes = 3;
            } else {
                numInvalid++;
                if (numInvalid > 5) {
                    break;
                }
                trailBytes = 0;
            }
                
            // Verify that we've got the right number of trail bytes in the sequence
            for (;;) {
                i++;
                if (i>=det.fRawLength) {
                    break;
                }
                b = input[i];
                if ((b & 0xc0) != 0x080) {
                    numInvalid++;
                    break;
                }
                if (--trailBytes == 0) {
                    numValid++;
                    break;
                }
            }
                        
        }
        
        // Cook up some sort of confidence score, based on presense of a BOM
        //    and the existence of valid and/or invalid multi-byte sequences.
        confidence = 0;
        if (hasBOM && numInvalid==0) {
            confidence = 100;
        } else if (hasBOM && numValid > numInvalid*10) {
            confidence = 80;
        } else if (numValid > 3 && numInvalid == 0) {
            confidence = 100;            
        } else if (numValid > 0 && numInvalid == 0) {
            confidence = 80;
        } else if (numValid == 0 && numInvalid == 0) {
            // Plain ASCII.  
            confidence = 10;            
        } else if (numValid > numInvalid*10) {
            // Probably corruput utf-8 data.  Valid sequences aren't likely by chance.
            confidence = 25;
        }
        return confidence;
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/txt/Icu4jEncodingDetector.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.txt;

import java.io.IOException;
import java.io.InputStream;
import java.nio.charset.Charset;

import org.apache.tika.detect.EncodingDetector;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.utils.CharsetUtils;

public class Icu4jEncodingDetector implements EncodingDetector {

    public Charset detect(InputStream input, Metadata metadata)
            throws IOException {
        if (input == null) {
            return null;
        }

        CharsetDetector detector = new CharsetDetector();

        String incomingCharset = metadata.get(Metadata.CONTENT_ENCODING);
        String incomingType = metadata.get(Metadata.CONTENT_TYPE);
        if (incomingCharset == null && incomingType != null) {
            // TIKA-341: Use charset in content-type
            MediaType mt = MediaType.parse(incomingType);
            if (mt != null) {
                incomingCharset = mt.getParameters().get("charset");
            }
        }

        if (incomingCharset != null) {
            String cleaned = CharsetUtils.clean(incomingCharset);
            if (cleaned != null) {
                detector.setDeclaredEncoding(cleaned);
            } else {
                // TODO: log a warning?
            }
        }

        // TIKA-341 without enabling input filtering (stripping of tags)
        // short HTML tests don't work well
        detector.enableInputFilter(true);

        detector.setText(input);

        for (CharsetMatch match : detector.detectAll()) {
            try {
                return CharsetUtils.forName(match.getName());
            } catch (Exception e) {
                // ignore
            }
        }

        return null;
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/txt/TXTParser.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.txt;

import java.io.IOException;
import java.io.InputStream;
import java.nio.charset.Charset;
import java.util.Collections;
import java.util.Set;

import org.apache.tika.config.ServiceLoader;
import org.apache.tika.detect.AutoDetectReader;
import org.apache.tika.exception.TikaException;
import org.apache.tika.io.CloseShieldInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * Plain text parser. The text encoding of the document stream is
 * automatically detected based on the byte patterns found at the
 * beginning of the stream and the given document metadata, most
 * notably the <code>charset</code> parameter of a
 * {@link org.apache.tika.metadata.HttpHeaders#CONTENT_TYPE} value.
 * <p>
 * This parser sets the following output metadata entries:
 * <dl>
 *   <dt>{@link org.apache.tika.metadata.HttpHeaders#CONTENT_TYPE}</dt>
 *   <dd><code>text/plain; charset=...</code></dd>
 * </dl>
 */
public class TXTParser extends AbstractParser {

    /** Serial version UID */
    private static final long serialVersionUID = -6656102320836888910L;

    private static final Set<MediaType> SUPPORTED_TYPES =
        Collections.singleton(MediaType.TEXT_PLAIN);

    private static final ServiceLoader LOADER =
            new ServiceLoader(TXTParser.class.getClassLoader());

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }

    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        // Automatically detect the character encoding
        AutoDetectReader reader = new AutoDetectReader(
                new CloseShieldInputStream(stream), metadata,
                context.get(ServiceLoader.class, LOADER));
        try {
            Charset charset = reader.getCharset();
            MediaType type = new MediaType(MediaType.TEXT_PLAIN, charset);
            metadata.set(Metadata.CONTENT_TYPE, type.toString());
            // deprecated, see TIKA-431
            metadata.set(Metadata.CONTENT_ENCODING, charset.name());

            XHTMLContentHandler xhtml =
                    new XHTMLContentHandler(handler, metadata);
            xhtml.startDocument();

            xhtml.startElement("p");
            char[] buffer = new char[4096];
            int n = reader.read(buffer);
            while (n != -1) {
                xhtml.characters(buffer, 0, n);
                n = reader.read(buffer);
            }
            xhtml.endElement("p");

            xhtml.endDocument();
        } finally {
            reader.close();
        }
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/txt/UniversalEncodingDetector.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.txt;

import java.io.IOException;
import java.io.InputStream;
import java.nio.charset.Charset;

import org.apache.tika.detect.EncodingDetector;
import org.apache.tika.metadata.Metadata;

public class UniversalEncodingDetector implements EncodingDetector {

    private static final int BUFSIZE = 1024;

    private static final int LOOKAHEAD = 16 * BUFSIZE;

    public Charset detect(InputStream input, Metadata metadata)
            throws IOException {
        if (input == null) {
            return null;
        }

        input.mark(LOOKAHEAD);
        try {
            UniversalEncodingListener listener =
                    new UniversalEncodingListener(metadata);

            byte[] b = new byte[BUFSIZE];
            int n = 0;
            int m = input.read(b);
            while (m != -1 && n < LOOKAHEAD && !listener.isDone()) {
                n += m;
                listener.handleData(b, 0, m);
                m = input.read(b, 0, Math.min(b.length, LOOKAHEAD - n));
            }

            return listener.dataEnd();
        } catch (LinkageError e) {
            return null; // juniversalchardet is not available
        } finally {
            input.reset();
        }
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/txt/UniversalEncodingListener.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.txt;

import java.nio.charset.Charset;

import org.apache.tika.detect.TextStatistics;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.utils.CharsetUtils;
import org.mozilla.universalchardet.CharsetListener;
import org.mozilla.universalchardet.Constants;
import org.mozilla.universalchardet.UniversalDetector;

/**
 * Helper class used by {@link UniversalEncodingDetector} to access the
 * <code>juniversalchardet</code> detection logic.
 */
class UniversalEncodingListener implements CharsetListener {

    private static final String CHARSET_ISO_8859_1 = "ISO-8859-1";

    private static final String CHARSET_ISO_8859_15 = "ISO-8859-15";

    private final TextStatistics statistics = new TextStatistics();

    private final UniversalDetector detector = new UniversalDetector(this);

    private String hint = null;

    private Charset charset = null;

    public UniversalEncodingListener(Metadata metadata) {
        MediaType type = MediaType.parse(metadata.get(Metadata.CONTENT_TYPE));
        if (type != null) {
            hint = type.getParameters().get("charset");
        }
        if (hint == null) {
            hint = metadata.get(Metadata.CONTENT_ENCODING);
        }
    }

    public void report(String name) {
        if (Constants.CHARSET_WINDOWS_1252.equals(name)) {
            if (hint != null) {
                // Use the encoding hint when available
                name = hint;
            } else if (statistics.count('\r') == 0) {
                // If there are no CR(LF)s, then the encoding is more
                // likely to be ISO-8859-1(5) than windows-1252
                if (statistics.count(0xa4) > 0) { // currency/euro sign
                    // The general currency sign is hardly ever used in
                    // ISO-8859-1, so it's more likely that we're dealing
                    // with ISO-8859-15, where the character is used for
                    // the euro symbol, which is more commonly used.
                    name = CHARSET_ISO_8859_15;
                } else {
                    name = CHARSET_ISO_8859_1;
                }
            }
        }
        try {
            this.charset = CharsetUtils.forName(name);
        } catch (Exception e) {
            // ignore
        }
    }

    public boolean isDone() {
        return detector.isDone();
    }

    public void handleData(byte[] buf, int offset, int length) {
        statistics.addData(buf, offset, length);
        detector.handleData(buf, offset, length);
    }

    public Charset dataEnd() {
        detector.dataEnd();
        if (charset == null && statistics.isMostlyAscii()) {
            report(Constants.CHARSET_WINDOWS_1252);
        }
        return charset;
    }

}"
tika-parsers/src/main/java/org/apache/tika/parser/video/FLVParser.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.video;

import java.io.ByteArrayInputStream;
import java.io.DataInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Date;
import java.util.HashMap;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Set;

import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * <p>
 * Parser for metadata contained in Flash Videos (.flv). Resources:
 * http://osflash.org/flv and for AMF:
 * http://download.macromedia.com/pub/labs/amf/amf0_spec_121207.pdf
 * <p>
 * This parser is capable of extracting the general metadata from header as well
 * as embedded metadata.
 * <p>
 * Known keys for metadata (from file header):
 * <ol>
 * <li>hasVideo: true|false
 * <li>hasSound: true|false
 * </ol>
 * <p>
 * In addition to the above values also metadata that is inserted in to the
 * actual stream will be picked. Usually there are keys like:
 * hasKeyframes, lastkeyframetimestamp, audiocodecid, keyframes, filepositions,
 * hasMetadata, audiosamplerate, videodatarate metadatadate, videocodecid,
 * metadatacreator, audiosize, hasVideo, height, audiosamplesize, framerate,
 * hasCuePoints width, cuePoints, lasttimestamp, canSeekToEnd, datasize,
 * duration, videosize, filesize, audiodatarate, hasAudio, stereo audiodelay
 */
public class FLVParser extends AbstractParser {

    /** Serial version UID */
    private static final long serialVersionUID = -8718013155719197679L;

    private static int TYPE_METADATA = 0x12;
    private static byte MASK_AUDIO = 1;
    private static byte MASK_VIDEO = 4;

    private static final Set<MediaType> SUPPORTED_TYPES =
        Collections.singleton(MediaType.video("x-flv"));

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }

    private long readUInt32(DataInputStream input) throws IOException {
        return input.readInt() & 0xFFFFFFFFL;
    }

    private int readUInt24(DataInputStream input) throws IOException {
        int uint = input.read()<<16;
        uint += input.read()<<8;
        uint += input.read(); 
        return uint;
    }

    private Object readAMFData(DataInputStream input, int type)
            throws IOException {
        if (type == -1) {
            type = input.readUnsignedByte();
        }
        switch (type) {
        case 0:
            return input.readDouble();
        case 1:
            return input.readUnsignedByte() == 1;
        case 2:
            return readAMFString(input);
        case 3:
            return readAMFObject(input);
        case 8:
            return readAMFEcmaArray(input);
        case 10:
            return readAMFStrictArray(input);
        case 11:
            final Date date = new Date((long) input.readDouble());
            input.readShort(); // time zone
            return date;
        case 13:
            return "UNDEFINED";
        default:
            return null;
        }
    }

    private Object readAMFStrictArray(DataInputStream input) throws IOException {
        long count = readUInt32(input);
        ArrayList<Object> list = new ArrayList<Object>();
        for (int i = 0; i < count; i++) {
            list.add(readAMFData(input, -1));
        }
        return list;
    }


    private String readAMFString(DataInputStream input) throws IOException {
        int size = input.readUnsignedShort();
        byte[] chars = new byte[size];
        input.readFully(chars);
        return new String(chars, "UTF-8");
    }

    private Object readAMFObject(DataInputStream input) throws IOException {
        HashMap<String, Object> array = new HashMap<String, Object>();
        while (true) {
            String key = readAMFString(input);
            int dataType = input.read();
            if (dataType == 9) { // object end marker
                break;
            }
            array.put(key, readAMFData(input, dataType));
        }
        return array;
    }

    private Object readAMFEcmaArray(DataInputStream input) throws IOException {
        long size = readUInt32(input);
        HashMap<String, Object> array = new HashMap<String, Object>();
        for (int i = 0; i < size; i++) {
            String key = readAMFString(input);
            int dataType = input.read();
            array.put(key, readAMFData(input, dataType));
        }
        return array;
    }

    private boolean checkSignature(DataInputStream fis) throws IOException {
        return fis.read() == 'F' && fis.read() == 'L' && fis.read() == 'V';
    }

    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        DataInputStream datainput = new DataInputStream(stream);
        if (!checkSignature(datainput)) {
            throw new TikaException("FLV signature not detected");
        }

        // header
        int version = datainput.readUnsignedByte();
        if (version != 1) {
            // should be 1, perhaps this is not flv?
            throw new TikaException("Unpexpected FLV version: " + version);
        }

        int typeFlags = datainput.readUnsignedByte();

        long len = readUInt32(datainput);
        if (len != 9) {
            // we only know about format with header of 9 bytes
            throw new TikaException("Unpexpected FLV header length: " + len);
        }

        long sizePrev = readUInt32(datainput);
        if (sizePrev != 0) {
            // should be 0, perhaps this is not flv?
            throw new TikaException(
                    "Unpexpected FLV first previous block size: " + sizePrev);
        }

        metadata.set(Metadata.CONTENT_TYPE, "video/x-flv");
        metadata.set("hasVideo", Boolean.toString((typeFlags & MASK_VIDEO) != 0));
        metadata.set("hasAudio", Boolean.toString((typeFlags & MASK_AUDIO) != 0));

        XHTMLContentHandler xhtml = new XHTMLContentHandler(handler, metadata);
        xhtml.startDocument();

        // flv tag stream follows...
        while (true) {
            int type = datainput.read();
            if (type == -1) {
                // EOF
                break;
            }

            int datalen = readUInt24(datainput); //body length
            readUInt32(datainput); // timestamp
            readUInt24(datainput); // streamid

            if (type == TYPE_METADATA) {
                // found metadata Tag, read content to buffer
                byte[] metaBytes = new byte[datalen];
                for (int readCount = 0; readCount < datalen;) {
                    int r = stream.read(metaBytes, readCount, datalen - readCount);
                    if(r!=-1) {
                        readCount += r;

                    } else {
                        break;
                    }
                }

                ByteArrayInputStream is = new ByteArrayInputStream(metaBytes);

                DataInputStream dis = new DataInputStream(is);

                Object data = null;

                for (int i = 0; i < 2; i++) {
                    data = readAMFData(dis, -1);
                }

                if (data instanceof Map) {
                    // TODO if there are multiple metadata values with same key (in
                    // separate AMF blocks, we currently loose previous values)
                    Map<String, Object> extractedMetadata = (Map<String, Object>) data;
                    for (Entry<String, Object> entry : extractedMetadata.entrySet()) {
                        if (entry.getValue() == null) {
                            continue;
                        }
                        metadata.set(entry.getKey(), entry.getValue().toString());
                    }
                }

            } else {
                // Tag was not metadata, skip over data we cannot handle
                for (int i = 0; i < datalen; i++) {
                    datainput.readByte();
                }
            }

            sizePrev = readUInt32(datainput); // previous block size
            if (sizePrev != datalen + 11) {
                // file was corrupt or we could not parse it...
                break;
            }
        }

        xhtml.endDocument();
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/xml/AbstractMetadataHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.xml;

import java.util.Arrays;
import java.util.List;

import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.Property;
import org.xml.sax.helpers.DefaultHandler;

/**
 * Base class for SAX handlers that map SAX events into document metadata.
 *
 * @since Apache Tika 0.10
 */
class AbstractMetadataHandler extends DefaultHandler {

    private final Metadata metadata;
    private final Property property;
    private final String name;

    protected AbstractMetadataHandler(Metadata metadata, String name) {
        this.metadata = metadata;
        this.property = null;
        this.name = name;
    }
    protected AbstractMetadataHandler(Metadata metadata, Property property) {
       this.metadata = metadata;
       this.property = property;
       this.name = property.getName();
   }

    /**
     * Adds the given metadata value. The value is ignored if it is
     * <code>null</code> or empty. If the metadata entry already exists,
     * then the given value is appended to it with a comma as the separator.
     *
     * @param value metadata value
     */
    protected void addMetadata(String value) {
        if (value != null && value.length() > 0) {
            if (metadata.isMultiValued(name)) {
                // Add the value, assuming it's not already there
                List<String> previous = Arrays.asList(metadata.getValues(name));
                if (!previous.contains(value)) {
                    if (property != null) {
                       metadata.add(property, value);
                    } else {
                       metadata.add(name, value);
                    }
                }
            } else {
                // Set the value, assuming it's not already there
                String previous = metadata.get(name);
                if (previous != null && previous.length() > 0) {
                    if (!previous.equals(value)) {
                       if (property != null) {
                          if (property.isMultiValuePermitted()) {
                              metadata.add(property, value);
                          } else {
                              // Replace the existing value if isMultiValuePermitted is false
                              metadata.set(property, value);
                          }
                       } else {
                          metadata.add(name, value);
                       }
                    }
                } else {
                   if (property != null) {
                      metadata.set(property, value);
                   } else {
                      metadata.set(name, value);
                   }
                }
            }
        }
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/xml/AttributeDependantMetadataHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.xml;

import org.apache.tika.metadata.Metadata;
import org.xml.sax.Attributes;
import org.xml.sax.helpers.DefaultHandler;

/**
 * This adds a Metadata entry for a given node.
 * The textual content of the node is used as the
 *  value, and the Metadata name is taken from
 *  an attribute, with a prefix if required. 
 */
public class AttributeDependantMetadataHandler extends DefaultHandler {

    private final Metadata metadata;

    private final String nameHoldingAttribute;
    private final String namePrefix;
    private String name;

    private final StringBuilder buffer = new StringBuilder();

    public AttributeDependantMetadataHandler(Metadata metadata, String nameHoldingAttribute, String namePrefix) {
        this.metadata = metadata;
        this.nameHoldingAttribute = nameHoldingAttribute;
        this.namePrefix = namePrefix;
    }

    public void addMetadata(String value) {
        if(name == null || name.length() == 0) {
           // We didn't find the attribute which holds the name
           return;
        }
        if (value.length() > 0) {
            String previous = metadata.get(name);
            if (previous != null && previous.length() > 0) {
                value = previous + ", " + value;
            }
            metadata.set(name, value);
        }
    }

    public void endElement(String uri, String localName, String name) {
        addMetadata(buffer.toString());
        buffer.setLength(0);
    }

    public void startElement(
            String uri, String localName, String name, Attributes attributes) {
        String rawName = attributes.getValue(nameHoldingAttribute);
        if (rawName != null) {
           if (namePrefix == null) {
              this.name = rawName;
           } else {
              this.name = namePrefix + rawName;
           }
        }
        // All other attributes are ignored
    }

    
    public void characters(char[] ch, int start, int length) {
        buffer.append(ch, start, length);
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/xml/AttributeMetadataHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.xml;

import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.Property;
import org.xml.sax.Attributes;
import org.xml.sax.SAXException;

/**
 * SAX event handler that maps the contents of an XML attribute into
 * a metadata field.
 *
 * @since Apache Tika 0.10
 */
public class AttributeMetadataHandler extends AbstractMetadataHandler {

    private final String uri;

    private final String localName;

    public AttributeMetadataHandler(
            String uri, String localName, Metadata metadata, String name) {
        super(metadata, name);
        this.uri = uri;
        this.localName = localName;
    }
    public AttributeMetadataHandler(
          String uri, String localName, Metadata metadata, Property property) {
      super(metadata, property);
      this.uri = uri;
      this.localName = localName;
  }

    @Override
    public void startElement(
            String uri, String localName, String qName, Attributes attributes)
            throws SAXException {
        for (int i = 0; i < attributes.getLength(); i++) {
            if (attributes.getURI(i).equals(this.uri)
                    && attributes.getLocalName(i).equals(this.localName)) {
                addMetadata(attributes.getValue(i).trim());
            }
        }
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/xml/DcXMLParser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.xml;

import org.apache.tika.metadata.DublinCore;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.Property;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.TeeContentHandler;
import org.xml.sax.ContentHandler;

/**
 * Dublin Core metadata parser
 */
public class DcXMLParser extends XMLParser {

    /** Serial version UID */
    private static final long serialVersionUID = 4905318835463880819L;

    private static ContentHandler getDublinCoreHandler(
            Metadata metadata, Property property, String element) {
        return new ElementMetadataHandler(
                DublinCore.NAMESPACE_URI_DC, element,
                metadata, property);
    }

    protected ContentHandler getContentHandler(
            ContentHandler handler, Metadata metadata, ParseContext context) {
        return new TeeContentHandler(
                super.getContentHandler(handler, metadata, context),
                getDublinCoreHandler(metadata, TikaCoreProperties.TITLE, "title"),
                getDublinCoreHandler(metadata, TikaCoreProperties.KEYWORDS, "subject"),
                getDublinCoreHandler(metadata, TikaCoreProperties.CREATOR, "creator"),
                getDublinCoreHandler(metadata, TikaCoreProperties.DESCRIPTION, "description"),
                getDublinCoreHandler(metadata, TikaCoreProperties.PUBLISHER, "publisher"),
                getDublinCoreHandler(metadata, TikaCoreProperties.CONTRIBUTOR, "contributor"),
                getDublinCoreHandler(metadata, TikaCoreProperties.CREATED, "date"),
                getDublinCoreHandler(metadata, TikaCoreProperties.TYPE, "type"),
                getDublinCoreHandler(metadata, TikaCoreProperties.FORMAT, "format"),
                getDublinCoreHandler(metadata, TikaCoreProperties.IDENTIFIER, "identifier"),
                getDublinCoreHandler(metadata, TikaCoreProperties.LANGUAGE, "language"),
                getDublinCoreHandler(metadata, TikaCoreProperties.RIGHTS, "rights"));
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/xml/ElementMetadataHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.xml;

import java.util.Arrays;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.Property;
import org.xml.sax.Attributes;

/**
 * SAX event handler that maps the contents of an XML element into
 * a metadata field.
 *
 * @since Apache Tika 0.10
 */
public class ElementMetadataHandler extends AbstractMetadataHandler {
    /**
     * Logger for this class
     */
    private static final Log logger = LogFactory
            .getLog(ElementMetadataHandler.class);

    private static final String LOCAL_NAME_RDF_BAG = "Bag";
    private static final String LOCAL_NAME_RDF_LI = "li";
    private static final String URI_RDF = "http://www.w3.org/1999/02/22-rdf-syntax-ns#";

    private final String uri;

    private final String localName;

    private final Metadata metadata;

    private final String name;
    private Property targetProperty;

    private final boolean allowDuplicateValues;
    private final boolean allowEmptyValues;

    /**
     * The buffer used to capture characters when inside a bag li element.
     */
    private final StringBuilder bufferBagged = new StringBuilder();

    /**
     * The buffer used to capture characters inside standard elements.
     */
    private final StringBuilder bufferBagless = new StringBuilder();

    /**
     * Whether or not the value was found in a standard element structure or inside a bag.
     */
    private boolean isBagless = true;

    private int matchLevel = 0;
    private int parentMatchLevel = 0;

    /**
     * Constructor for string metadata keys.
     *
     * @param uri the uri of the namespace of the element
     * @param localName the local name of the element
     * @param metadata the Tika metadata object to populate
     * @param name the Tika metadata field key
     */
    public ElementMetadataHandler(
            String uri, String localName, Metadata metadata, String name) {
        super(metadata, name);
        this.uri = uri;
        this.localName = localName;
        this.metadata = metadata;
        this.name = name;
        this.allowDuplicateValues = false;
        this.allowEmptyValues = false;
        if (logger.isTraceEnabled()) {
            logger.trace("created simple handler for " + this.name);
        }
    }

    /**
     * Constructor for string metadata keys which allows change of behavior
     * for duplicate and empty entry values.
     *
     * @param uri the uri of the namespace of the element
     * @param localName the local name of the element
     * @param metadata the Tika metadata object to populate
     * @param name the Tika metadata field key
     * @param allowDuplicateValues add duplicate values to the Tika metadata
     * @param allowEmptyValues add empty values to the Tika metadata
     */
    public ElementMetadataHandler(
            String uri, String localName, Metadata metadata, String name, boolean allowDuplicateValues, boolean allowEmptyValues) {
        super(metadata, name);
        this.uri = uri;
        this.localName = localName;
        this.metadata = metadata;
        this.name = name;
        this.allowDuplicateValues = allowDuplicateValues;
        this.allowEmptyValues = allowEmptyValues;
        if (logger.isTraceEnabled()) {
                logger.trace("created simple handler for " + this.name);
        }
    }

    /**
     * Constructor for Property metadata keys.
     *
     * @param uri the uri of the namespace of the element
     * @param localName the local name of the element
     * @param metadata the Tika metadata object to populate
     * @param targetProperty the Tika metadata Property key
     */
    public ElementMetadataHandler(
            String uri, String localName, Metadata metadata, Property targetProperty) {
        super(metadata, targetProperty);
        this.uri = uri;
        this.localName = localName;
        this.metadata = metadata;
        this.targetProperty = targetProperty;
        this.name = targetProperty.getName();
        this.allowDuplicateValues = false;
        this.allowEmptyValues = false;
        if (logger.isTraceEnabled()) {
            logger.trace("created property handler for " + this.name);
        }
    }

    /**
     * Constructor for Property metadata keys which allows change of behavior
     * for duplicate and empty entry values.
     *
     * @param uri the uri of the namespace of the element
     * @param localName the local name of the element
     * @param metadata the Tika metadata object to populate
     * @param targetProperty the Tika metadata Property key
     * @param allowDuplicateValues add duplicate values to the Tika metadata
     * @param allowEmptyValues add empty values to the Tika metadata
     */
    public ElementMetadataHandler(
            String uri, String localName, Metadata metadata, Property targetProperty, boolean allowDuplicateValues, boolean allowEmptyValues) {
        super(metadata, targetProperty);
        this.uri = uri;
        this.localName = localName;
        this.metadata = metadata;
        this.targetProperty = targetProperty;
        this.name = targetProperty.getName();
        this.allowDuplicateValues = allowDuplicateValues;
        this.allowEmptyValues = allowEmptyValues;
        if (logger.isTraceEnabled()) {
                logger.trace("created property handler for " + this.name);
        }
    }

    protected boolean isMatchingParentElement(String uri, String localName) {
        return (uri.equals(this.uri) && localName.equals(this.localName));
    }

    protected boolean isMatchingElement(String uri, String localName) {
        // match if we're inside the parent element or within some bag element
        return (uri.equals(this.uri) && localName.equals(this.localName)) ||
                (parentMatchLevel > 0 &&
                        ((uri.equals(URI_RDF) && localName.equals(LOCAL_NAME_RDF_BAG)) ||
                        (uri.equals(URI_RDF) && localName.equals(LOCAL_NAME_RDF_LI))
                )
        );
    }

    @Override
    public void startElement(
            String uri, String localName, String name, Attributes attributes) {
        if (isMatchingElement(uri, localName)) {
            matchLevel++;
        }
        if (isMatchingParentElement(uri, localName)) {
            parentMatchLevel++;
        }
    }

    @Override
    public void endElement(String uri, String localName, String name) {
        if (isMatchingParentElement(uri, localName)) {
            parentMatchLevel--;
        }
        if (isMatchingElement(uri, localName)) {
            matchLevel--;
            if (matchLevel == 2) {
                // we're inside a bag li element, add the bagged buffer
                addMetadata(bufferBagged.toString().trim());
                bufferBagged.setLength(0);
                isBagless = false;
            }
            if (matchLevel == 0 && isBagless) {
                String valueBagless = bufferBagless.toString();
                if (valueBagless.length() > 0 && !valueBagless.contains(LOCAL_NAME_RDF_BAG)) {
                    // we're in a standard element, add the bagless buffer
                    addMetadata(valueBagless.trim());
                    bufferBagless.setLength(0);
                }
                isBagless = true;
            }
        }
    }

    @Override
    public void characters(char[] ch, int start, int length) {
        // We need to append to both buffers since we don't if we're inside a bag until we're done
        if (parentMatchLevel > 0 && matchLevel > 2) {
            bufferBagged.append(ch, start, length);
        }
        if (parentMatchLevel > 0 && matchLevel > 0) {
            bufferBagless.append(ch, start, length);
        }
    }

    @Override
    public void ignorableWhitespace(char[] ch, int start, int length) {
        characters(ch, start, length);
    }

    @Override
    protected void addMetadata(String value) {
        if (logger.isTraceEnabled()) {
            logger.trace("adding " + name + "=" + value);
        }
        if (targetProperty != null && targetProperty.isMultiValuePermitted()) {
            if ((value != null && value.length() > 0) || allowEmptyValues) {
                if (value == null || value.length() == 0 && allowEmptyValues) {
                    value = "";
                }
                String[] previous = metadata.getValues(name);
                if (previous == null || !Arrays.asList(previous).contains(value) || allowDuplicateValues) {
                    metadata.add(targetProperty, value);
                }
            }
        } else {
            super.addMetadata(value);
        }
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/xml/FictionBookParser.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.xml;

import org.apache.commons.codec.binary.Base64;
import org.apache.tika.extractor.EmbeddedDocumentExtractor;
import org.apache.tika.extractor.ParsingEmbeddedDocumentExtractor;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.TikaMetadataKeys;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.ParseContext;
import org.xml.sax.Attributes;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.DefaultHandler;

import java.io.ByteArrayInputStream;
import java.io.IOException;
import java.util.Collections;
import java.util.Set;

public class FictionBookParser extends XMLParser {
    private static final long serialVersionUID = 4195954546491524374L;
    
    @Override
    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return Collections.singleton(MediaType.application("x-fictionbook+xml"));
    }

    @Override
    protected ContentHandler getContentHandler(ContentHandler handler, Metadata metadata, ParseContext context) {
        EmbeddedDocumentExtractor ex = context.get(EmbeddedDocumentExtractor.class);

        if (ex == null) {
            ex = new ParsingEmbeddedDocumentExtractor(context);
        }

        return new BinaryElementsDataHandler(ex, handler);
    }

    private static class BinaryElementsDataHandler extends DefaultHandler {
        private static final String ELEMENT_BINARY = "binary";

        private boolean binaryMode = false;
        private static final String ATTRIBUTE_ID = "id";

        private final EmbeddedDocumentExtractor partExtractor;
        private final ContentHandler handler;
        private final StringBuilder binaryData = new StringBuilder();
        private Metadata metadata;
        private static final String ATTRIBUTE_CONTENT_TYPE = "content-type";

        private BinaryElementsDataHandler(EmbeddedDocumentExtractor partExtractor, ContentHandler handler) {
            this.partExtractor = partExtractor;
            this.handler = handler;
        }

        @Override
        public void startElement(String uri, String localName, String qName, Attributes attributes) throws SAXException {
            binaryMode = ELEMENT_BINARY.equals(localName);
            if (binaryMode) {
                binaryData.setLength(0);
                metadata = new Metadata();

                metadata.set(TikaMetadataKeys.RESOURCE_NAME_KEY, attributes.getValue(ATTRIBUTE_ID));
                metadata.set(Metadata.CONTENT_TYPE, attributes.getValue(ATTRIBUTE_CONTENT_TYPE));
            }
        }

        @Override
        public void endElement(String uri, String localName, String qName) throws SAXException {
            if (binaryMode) {
                try {
                    partExtractor.parseEmbedded(
                            new ByteArrayInputStream(Base64.decodeBase64(binaryData.toString())),
                            handler,
                            metadata,
                            true
                    );
                } catch (IOException e) {
                    throw new SAXException("IOException in parseEmbedded", e);
                }

                binaryMode = false;
                binaryData.setLength(0);
            }
        }

        @Override
        public void characters(char[] ch, int start, int length) throws SAXException {
            if (!binaryMode) {
                handler.characters(ch, start, length);
            } else {
                binaryData.append(ch, start, length);
            }
        }

        @Override
        public void ignorableWhitespace(char[] ch, int start, int length) throws SAXException {
            handler.ignorableWhitespace(ch, start, length);
        }
    }
}
"
tika-parsers/src/main/java/org/apache/tika/parser/xml/MetadataHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.xml;

import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.Property;
import org.xml.sax.Attributes;
import org.xml.sax.helpers.DefaultHandler;

/**
 * This adds Metadata entries with a specified name for
 *  the textual content of a node (if present), and 
 *  all attribute values passed through the matcher
 *  (but not their names). 
 *
 * @deprecated Use the {@link AttributeMetadataHandler} and
 *             {@link ElementMetadataHandler} classes instead
 */
public class MetadataHandler extends DefaultHandler {

    private final Metadata metadata;

    private final Property property;
    private final String name;

    private final StringBuilder buffer = new StringBuilder();

    public MetadataHandler(Metadata metadata, String name) {
        this.metadata = metadata;
        this.property = null;
        this.name = name;
    }
    public MetadataHandler(Metadata metadata, Property property) {
       this.metadata = metadata;
       this.property = property;
       this.name = property.getName();
   }

    public void addMetadata(String value) {
        if (value.length() > 0) {
            String previous = metadata.get(name);
            if (previous != null && previous.length() > 0) {
                value = previous + ", " + value;
            }
            
            if (this.property != null) {
               metadata.set(property, value);
            } else {
               metadata.set(name, value);
            }
        }
    }

    public void endElement(String uri, String localName, String name) {
        addMetadata(buffer.toString());
        buffer.setLength(0);
    }

    public void startElement(
            String uri, String localName, String name, Attributes attributes) {
        for (int i = 0; i < attributes.getLength(); i++) {
            addMetadata(attributes.getValue(i));
        }
    }

    
    public void characters(char[] ch, int start, int length) {
        buffer.append(ch, start, length);
    }

}
"
tika-parsers/src/main/java/org/apache/tika/parser/xml/XMLParser.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.parser.xml;

import java.io.IOException;
import java.io.InputStream;
import java.util.Arrays;
import java.util.Collections;
import java.util.HashSet;
import java.util.Set;

import org.apache.tika.exception.TikaException;
import org.apache.tika.io.CloseShieldInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AbstractParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.EmbeddedContentHandler;
import org.apache.tika.sax.OfflineContentHandler;
import org.apache.tika.sax.TaggedContentHandler;
import org.apache.tika.sax.TextContentHandler;
import org.apache.tika.sax.XHTMLContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

/**
 * XML parser.
 */
public class XMLParser extends AbstractParser {

    /** Serial version UID */
    private static final long serialVersionUID = -6028836725280212837L;

    private static final Set<MediaType> SUPPORTED_TYPES =
        Collections.unmodifiableSet(new HashSet<MediaType>(Arrays.asList(
                MediaType.application("xml"),
                MediaType.image("svg+xml"))));

    public Set<MediaType> getSupportedTypes(ParseContext context) {
        return SUPPORTED_TYPES;
    }

    public void parse(
            InputStream stream, ContentHandler handler,
            Metadata metadata, ParseContext context)
            throws IOException, SAXException, TikaException {
        if (metadata.get(Metadata.CONTENT_TYPE) == null) {
            metadata.set(Metadata.CONTENT_TYPE, "application/xml");
        }

        final XHTMLContentHandler xhtml =
            new XHTMLContentHandler(handler, metadata);
        xhtml.startDocument();
        xhtml.startElement("p");

        TaggedContentHandler tagged = new TaggedContentHandler(handler);
        try {
            context.getSAXParser().parse(
                    new CloseShieldInputStream(stream),
                    new OfflineContentHandler(new EmbeddedContentHandler(
                            getContentHandler(tagged, metadata, context))));
        } catch (SAXException e) {
            tagged.throwIfCauseOf(e);
            throw new TikaException("XML parse error", e);
        } finally {
            xhtml.endElement("p");
            xhtml.endDocument();
        }
    }

    protected ContentHandler getContentHandler(
            ContentHandler handler, Metadata metadata, ParseContext context) {
        return new TextContentHandler(handler, true);
    }
}
"
tika-serialization/src/main/java/org/apache/tika/metadata/serialization/JsonMetadata.java,false,"package org.apache.tika.metadata.serialization;

/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */


import java.io.Reader;
import java.io.Writer;

import com.google.gson.Gson;
import com.google.gson.JsonIOException;
import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;

public class JsonMetadata extends JsonMetadataBase{
    private static Gson GSON;

    static {
        GSON = defaultInit();
    }
    /**
     * Serializes a Metadata object to Json.  This does not flush or close the writer.
     * 
     * @param metadata metadata to write
     * @param writer writer
     * @throws TikaException if there is an IOException during writing
     */
    public static void toJson(Metadata metadata, Writer writer) throws TikaException {
        try {
            GSON.toJson(metadata, writer);
        } catch (JsonIOException e) {
            throw new TikaException(e.getMessage());
        }
    }
        
    /**
     * Read metadata from reader.
     *
     * @param reader reader to read from
     * @return Metadata or null if nothing could be read from the reader
     * @throws TikaException in case of parse failure by Gson or IO failure with Reader
     */
    public static Metadata fromJson(Reader reader) throws TikaException {
        Metadata m = null;
        try {
            m = GSON.fromJson(reader, Metadata.class);
        } catch (com.google.gson.JsonParseException e){
            //covers both io and parse exceptions
            throw new TikaException(e.getMessage());
        }
        return m;
    }

    /**
     * Enables setting custom configurations on Gson.  Remember to register
     * a serializer and a deserializer for Metadata.  This does a literal set
     * and does not add the default serializer and deserializers.
     *
     * @param gson
     */
    public static void setGson(Gson gson) {
        GSON = gson;
    }

    public static void setPrettyPrinting(boolean prettyPrint) {
        if (prettyPrint) {
            GSON = prettyInit();
        } else {
            GSON = defaultInit();
        }
    }

}
"
tika-serialization/src/main/java/org/apache/tika/metadata/serialization/JsonMetadataBase.java,true,"package org.apache.tika.metadata.serialization;

/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.util.Arrays;

import com.google.gson.Gson;
import com.google.gson.GsonBuilder;
import org.apache.tika.metadata.Metadata;

public class JsonMetadataBase {


    static Gson defaultInit() {
        GsonBuilder builder = new GsonBuilder();
        builder.registerTypeHierarchyAdapter(Metadata.class, new JsonMetadataSerializer());
        builder.registerTypeHierarchyAdapter(Metadata.class, new JsonMetadataDeserializer());
        return builder.create();
    }

    static Gson prettyInit() {
        GsonBuilder builder = new GsonBuilder();
        builder.registerTypeHierarchyAdapter(Metadata.class, new SortedJsonMetadataSerializer());
        builder.registerTypeHierarchyAdapter(Metadata.class, new JsonMetadataDeserializer());
        builder.setPrettyPrinting();
        return builder.create();
    }

    private static class SortedJsonMetadataSerializer extends JsonMetadataSerializer {
        @Override
        public String[] getNames(Metadata m) {
            String[] names = m.names();
            Arrays.sort(names, new PrettyMetadataKeyComparator());
            return names;
        }
    }
}
"
tika-serialization/src/main/java/org/apache/tika/metadata/serialization/JsonMetadataDeserializer.java,false,"package org.apache.tika.metadata.serialization;

/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.lang.reflect.Type;
import java.util.Iterator;
import java.util.Map;

import org.apache.tika.metadata.Metadata;

import com.google.gson.JsonArray;
import com.google.gson.JsonDeserializationContext;
import com.google.gson.JsonDeserializer;
import com.google.gson.JsonElement;
import com.google.gson.JsonObject;
import com.google.gson.JsonParseException;


/**
 * Deserializer for Metadata
 *
 * If overriding this, remember that this is called from a static context.
 * Share state only with great caution.
 */
public class JsonMetadataDeserializer implements JsonDeserializer<Metadata> {

    /**
     * Deserializes a json object (equivalent to: Map<String, String[]>) 
     * into a Metadata object.
     * 
     * @param element to serialize
     * @param type (ignored)
     * @param context (ignored)
     * @return Metadata 
     * @throws JsonParseException if element is not able to be parsed
     */
    @Override
    public Metadata deserialize(JsonElement element, Type type,
            JsonDeserializationContext context) throws JsonParseException {

        final JsonObject obj = element.getAsJsonObject();
        Metadata m = new Metadata();
        for (Map.Entry<String, JsonElement> entry : obj.entrySet()){
            String key = entry.getKey();
            JsonElement v = entry.getValue();
            if (v.isJsonPrimitive()){
                m.set(key, v.getAsString());
            } else if (v.isJsonArray()){
                JsonArray vArr = v.getAsJsonArray();
                Iterator<JsonElement> itr = vArr.iterator();
                while (itr.hasNext()){
                    JsonElement valueItem = itr.next();
                    m.add(key, valueItem.getAsString());
                }

            }
        }
        return m;
    }
}
"
tika-serialization/src/main/java/org/apache/tika/metadata/serialization/JsonMetadataList.java,false,"package org.apache.tika.metadata.serialization;

/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */


import java.io.Reader;
import java.io.Writer;
import java.lang.reflect.Type;
import java.util.List;

import com.google.gson.Gson;
import com.google.gson.JsonIOException;
import com.google.gson.reflect.TypeToken;
import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;

public class JsonMetadataList extends JsonMetadataBase {
    
    private final static Type listType = new TypeToken<List<Metadata>>(){}.getType();
    private static Gson GSON;
    static {
        GSON = defaultInit();
    }

    /**
     * Serializes a Metadata object to Json.  This does not flush or close the writer.
     * 
     * @param metadataList list of metadata to write
     * @param writer writer
     * @throws org.apache.tika.exception.TikaException if there is an IOException during writing
     */
    public static void toJson(List<Metadata> metadataList, Writer writer) throws TikaException {
        try {
            GSON.toJson(metadataList, writer);
        } catch (JsonIOException e) {
            throw new TikaException(e.getMessage());
        }
    }
        
    /**
     * Read metadata from reader.
     *
     * @param reader
     * @return Metadata or null if nothing could be read from the reader
     * @throws org.apache.tika.exception.TikaException in case of parse failure by Gson or IO failure with Reader
     */
    public static List<Metadata> fromJson(Reader reader) throws TikaException {
        List<Metadata> ms = null;
        if (reader == null) {
            return ms;
        }
        try {
            ms = GSON.fromJson(reader, listType);
        } catch (com.google.gson.JsonParseException e){
            //covers both io and parse exceptions
            throw new TikaException(e.getMessage());
        }
        return ms;
    }

    /**
     * Enables setting custom configurations on Gson.  Remember to register
     * a serializer and a deserializer for Metadata.  This does a literal set
     * and does not add the default serializer and deserializers.
     *
     * @param gson
     */
    public static void setGson(Gson gson) {
        GSON = gson;
    }

    public static void setPrettyPrinting(boolean prettyPrint) {
        if (prettyPrint) {
            GSON = prettyInit();
        } else {
            GSON = defaultInit();
        }
    }


}
"
tika-serialization/src/main/java/org/apache/tika/metadata/serialization/JsonMetadataSerializer.java,false,"package org.apache.tika.metadata.serialization;

/*
* Licensed to the Apache Software Foundation (ASF) under one or more
* contributor license agreements.  See the NOTICE file distributed with
* this work for additional information regarding copyright ownership.
* The ASF licenses this file to You under the Apache License, Version 2.0
* (the "License"); you may not use this file except in compliance with
* the License.  You may obtain a copy of the License at
*
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*/

import java.lang.reflect.Type;
import java.util.Arrays;

import org.apache.tika.metadata.Metadata;

import com.google.gson.JsonArray;
import com.google.gson.JsonElement;
import com.google.gson.JsonNull;
import com.google.gson.JsonObject;
import com.google.gson.JsonPrimitive;
import com.google.gson.JsonSerializationContext;
import com.google.gson.JsonSerializer;


/**
 * Serializer for Metadata
 * 
 * If overriding this, remember that this is called from a static context.
 * Share state only with great caution.
 *
 */
public class JsonMetadataSerializer implements JsonSerializer<Metadata> {


    /**
     * Serializes a Metadata object into effectively Map<String, String[]>.
     * 
     * @param metadata object to serialize
     * @param type (ignored)
     * @param context (ignored)
     * @return JsonElement with key/value(s) pairs or JsonNull if metadata is null.
     */
    @Override
    public JsonElement serialize(Metadata metadata, Type type, JsonSerializationContext context) {
        if (metadata == null){
            return new JsonNull();
        }
        String[] names = getNames(metadata);
        if (names == null) {
            return new JsonNull();
        }

        JsonObject root = new JsonObject();

        for (String n : names) {
            
            String[] vals = metadata.getValues(n);
            if (vals == null) {
                //silently skip
                continue;
            }
            
            if (vals.length == 1){
                root.addProperty(n, vals[0]);
            } else {
                JsonArray jArr = new JsonArray();
                for (int i = 0; i < vals.length; i++) {
                    jArr.add(new JsonPrimitive(vals[i]));
                }
                root.add(n, jArr);
            }
        }
        return root;
    }
    
    /**
     * Override to get a custom sort order
     * or to filter names.
     * 
     * @param metadata metadata from which to grab names
     * @return list of names in the order in which they should be serialized
     */
    protected String[] getNames(Metadata metadata) {
        String[] names = metadata.names();
        Arrays.sort(names);
        return names;
    }
}
"
tika-serialization/src/main/java/org/apache/tika/metadata/serialization/PrettyMetadataKeyComparator.java,false,"package org.apache.tika.metadata.serialization;

/*
* Licensed to the Apache Software Foundation (ASF) under one or more
* contributor license agreements.  See the NOTICE file distributed with
* this work for additional information regarding copyright ownership.
* The ASF licenses this file to You under the Apache License, Version 2.0
* (the "License"); you may not use this file except in compliance with
* the License.  You may obtain a copy of the License at
*
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*/

public class PrettyMetadataKeyComparator implements java.util.Comparator<String> {
    @Override
    public int compare(String s1, String s2) {
        if (s1 == null) {
            return 1;
        } else if (s2 == null) {
            return -1;
        }

        //this is stinky.  This should reference RecursiveParserWrapper.TIKA_CONTENT
        //but that would require making core a dependency of serialization...
        //do we want to do that?
        if (s1.equals("tika:content")) {
            if (s2.equals("tika:content")) {
                return 0;
            }
            return 2;
        } else if (s2.equals("tika:content")) {
            return -2;
        }
        //do we want to lowercase?
        return s1.compareTo(s2);
    }
}

"
tika-server/src/main/java/org/apache/tika/server/CSVMessageBodyWriter.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.server;

import org.apache.tika.metadata.Metadata;

import javax.ws.rs.Produces;
import javax.ws.rs.WebApplicationException;
import javax.ws.rs.core.MediaType;
import javax.ws.rs.core.MultivaluedMap;
import javax.ws.rs.ext.MessageBodyWriter;
import javax.ws.rs.ext.Provider;

import java.io.IOException;
import java.io.OutputStream;
import java.io.OutputStreamWriter;
import java.lang.annotation.Annotation;
import java.lang.reflect.Type;
import java.util.ArrayList;
import java.util.Arrays;

import au.com.bytecode.opencsv.CSVWriter;

@Provider
@Produces("text/csv")
public class CSVMessageBodyWriter implements MessageBodyWriter<Metadata> {

  public boolean isWriteable(Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType) {
    return Metadata.class.isAssignableFrom(type);
  }

  public long getSize(Metadata data, Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType) {
    return -1;
  }

  @Override
  @SuppressWarnings("resource")
  public void writeTo(Metadata metadata, Class<?> type, Type genericType, Annotation[] annotations,
      MediaType mediaType, MultivaluedMap<String, Object> httpHeaders, OutputStream entityStream) throws IOException,
      WebApplicationException {

    CSVWriter writer = new CSVWriter(new OutputStreamWriter(entityStream, "UTF-8"));

    for (String name : metadata.names()) {
      String[] values = metadata.getValues(name);
      ArrayList<String> list = new ArrayList<String>(values.length + 1);
      list.add(name);
      list.addAll(Arrays.asList(values));
      writer.writeNext(list.toArray(values));
    }
    
    // Don't close, just flush the stream
    writer.flush();
  }
}
"
tika-server/src/main/java/org/apache/tika/server/DetectorResource.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.server;

import java.io.IOException;
import java.io.InputStream;

import javax.ws.rs.Consumes;
import javax.ws.rs.PUT;
import javax.ws.rs.Path;
import javax.ws.rs.Produces;
import javax.ws.rs.core.Context;
import javax.ws.rs.core.HttpHeaders;
import javax.ws.rs.core.UriInfo;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.tika.config.TikaConfig;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.mime.MediaType;

@Path("/detect")
public class DetectorResource {

	private static final Log logger = LogFactory.getLog(DetectorResource.class
			.getName());

	private TikaConfig config = null;

	public DetectorResource(TikaConfig config) {
		this.config = config;
	}

	@PUT
	@Path("stream")
	@Consumes("*/*")
	@Produces("text/plain")
	public String detect(final InputStream is,
			@Context HttpHeaders httpHeaders, @Context final UriInfo info) {
		Metadata met = new Metadata();
		TikaInputStream tis = TikaInputStream.get(is);
		String filename = TikaResource.detectFilename(httpHeaders
				.getRequestHeaders());
		logger.info("Detecting media type for Filename: " + filename);
		met.add(Metadata.RESOURCE_NAME_KEY, filename);
		try {
			return this.config.getDetector().detect(tis, met).toString();
		} catch (IOException e) {
			logger.warn("Unable to detect MIME type for file. Reason: "
					+ e.getMessage());
			e.printStackTrace();
			return MediaType.OCTET_STREAM.toString();
		}
	}

}
"
tika-server/src/main/java/org/apache/tika/server/HTMLHelper.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.server;

import java.io.IOException;
import java.io.InputStream;

import org.apache.tika.io.IOUtils;

/**
 * Helps produce user facing HTML output.
 * 
 * TODO Decide if this would be better done as a MessageBodyWriter
 */
public class HTMLHelper {
    private static final String PATH = "/tikaserver-template.html";
    private static final String TITLE_VAR = "[[TITLE]]";
    private static final String BODY_VAR = "[[BODY]]";
    private String PRE_BODY;
    private String POST_BODY;
    
    public HTMLHelper() {
        InputStream htmlStr = getClass().getResourceAsStream(PATH);
        if (htmlStr == null) {
            throw new IllegalArgumentException("Template Not Found - " + PATH);
        }
        try {
            String html = IOUtils.toString(htmlStr, "UTF-8");
            int bodyAt = html.indexOf(BODY_VAR);
            PRE_BODY = html.substring(0, bodyAt);
            POST_BODY = html.substring(bodyAt + BODY_VAR.length());
        } catch (IOException e) {
            throw new IllegalStateException("Unable to read template");
        }
    }
    
    /**
     * Generates the HTML Header for the user facing page, adding
     *  in the given title as required
     */
    public void generateHeader(StringBuffer html, String title) {
        html.append(PRE_BODY.replace(TITLE_VAR, title));
    }
    public void generateFooter(StringBuffer html) {
        html.append(POST_BODY);
    }
}"
tika-server/src/main/java/org/apache/tika/server/JSONMessageBodyWriter.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.server;

import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.serialization.JsonMetadata;

import javax.ws.rs.Produces;
import javax.ws.rs.WebApplicationException;
import javax.ws.rs.core.MediaType;
import javax.ws.rs.core.MultivaluedMap;
import javax.ws.rs.ext.MessageBodyWriter;
import javax.ws.rs.ext.Provider;

import java.io.IOException;
import java.io.OutputStream;
import java.io.OutputStreamWriter;
import java.io.Writer;
import java.lang.annotation.Annotation;
import java.lang.reflect.Type;

@Provider
@Produces(MediaType.APPLICATION_JSON)
public class JSONMessageBodyWriter implements MessageBodyWriter<Metadata> {

  public boolean isWriteable(Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType) {
    return Metadata.class.isAssignableFrom(type);
  }

  public long getSize(Metadata data, Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType) {
    return -1;
  }

  @Override
  public void writeTo(Metadata metadata, Class<?> type, Type genericType, Annotation[] annotations,
      MediaType mediaType, MultivaluedMap<String, Object> httpHeaders, OutputStream entityStream) throws IOException,
      WebApplicationException {
        try {
            Writer writer = new OutputStreamWriter(entityStream, "UTF-8");
            JsonMetadata.toJson(metadata, writer);
            writer.flush();
        } catch (TikaException e) {
            throw new IOException(e);
        }
        entityStream.flush();
  }
}
"
tika-server/src/main/java/org/apache/tika/server/MetadataList.java,false,"package org.apache.tika.server;

/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import org.apache.tika.metadata.Metadata;

import java.util.List;

/**
 * wrapper class to make isWriteable in MetadataListMBW simpler
 */
class MetadataList {
    private final List<Metadata> metadata;

    MetadataList(List<Metadata> metadata) {
        this.metadata = metadata;
    }

    List<Metadata> getMetadata() {
        return metadata;
    }
}
"
tika-server/src/main/java/org/apache/tika/server/MetadataListMessageBodyWriter.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.server;

import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.serialization.JsonMetadataList;

import javax.ws.rs.Produces;
import javax.ws.rs.WebApplicationException;
import javax.ws.rs.core.MediaType;
import javax.ws.rs.core.MultivaluedMap;
import javax.ws.rs.ext.MessageBodyWriter;
import javax.ws.rs.ext.Provider;
import java.io.IOException;
import java.io.OutputStream;
import java.io.OutputStreamWriter;
import java.io.Writer;
import java.lang.annotation.Annotation;
import java.lang.reflect.Type;

@Provider
@Produces(MediaType.APPLICATION_JSON)
public class MetadataListMessageBodyWriter implements MessageBodyWriter<MetadataList> {

  public boolean isWriteable(Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType) {
      if (! MediaType.APPLICATION_JSON_TYPE.equals(mediaType) ){
          return false;
      }
      return type.isAssignableFrom(MetadataList.class);
  }

  public long getSize(MetadataList data, Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType) {
    return -1;
  }

  @Override
  public void writeTo(MetadataList list, Class<?> type, Type genericType, Annotation[] annotations,
      MediaType mediaType, MultivaluedMap<String, Object> httpHeaders, OutputStream entityStream) throws IOException,
      WebApplicationException {
        try {
            Writer writer = new OutputStreamWriter(entityStream, "UTF-8");
            JsonMetadataList.toJson(list.getMetadata(), writer);
            writer.flush();
        } catch (TikaException e) {
            throw new IOException(e);
        }
        entityStream.flush();
  }
}
"
tika-server/src/main/java/org/apache/tika/server/MetadataResource.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.server;

import java.io.InputStream;

import javax.ws.rs.Consumes;
import javax.ws.rs.PUT;
import javax.ws.rs.Path;
import javax.ws.rs.PathParam;
import javax.ws.rs.Produces;
import javax.ws.rs.core.Context;
import javax.ws.rs.core.HttpHeaders;
import javax.ws.rs.core.MultivaluedMap;
import javax.ws.rs.core.Response;
import javax.ws.rs.core.UriInfo;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.cxf.jaxrs.ext.multipart.Attachment;
import org.apache.tika.config.TikaConfig;

import org.apache.tika.metadata.Metadata;
import org.apache.tika.parser.AutoDetectParser;
import org.apache.tika.parser.ParseContext;
import org.xml.sax.helpers.DefaultHandler;


@Path("/meta")
public class MetadataResource {
  private static final Log logger = LogFactory.getLog(MetadataResource.class);

  private TikaConfig tikaConfig;

  public MetadataResource(TikaConfig tikaConfig) {
    this.tikaConfig = tikaConfig;
  }

  @PUT
  @Consumes("multipart/form-data")
  @Produces({"text/csv", "application/json", "application/rdf+xml"})
  @Path("form")
  public Response getMetadataFromMultipart(Attachment att, @Context UriInfo info) throws Exception {
    return Response.ok(
            parseMetadata(att.getObject(InputStream.class), att.getHeaders(), info)).build();
  }

  @PUT
  @Produces({"text/csv", "application/json", "application/rdf+xml"})
  public Response getMetadata(InputStream is, @Context HttpHeaders httpHeaders, @Context UriInfo info) throws Exception {
    return Response.ok(
            parseMetadata(is, httpHeaders.getRequestHeaders(), info)).build();
  }

  /**
   * Get a specific metadata field. If the input stream cannot be parsed, but a
   * value was found for the given metadata field, then the value of the field
   * is returned as part of a 200 OK response; otherwise a
   * {@link javax.ws.rs.core.Response.Status#BAD_REQUEST} is generated. If the stream was successfully
   * parsed but the specific metadata field was not found, then a
   * {@link javax.ws.rs.core.Response.Status#NOT_FOUND} is returned.
   * <p>
   * Note that this method handles multivalue fields and returns possibly more
   * metadata value than requested.
   * <p>
   * If you want XMP, you must be careful to specify the exact XMP key.
   * For example, "Author" will return nothing, but "dc:creator" will return the correct value.
   *
   * @param is inputstream
   * @param httpHeaders httpheaders
   * @param info info
   * @param field the tika metadata field name
   * @return one of {@link javax.ws.rs.core.Response.Status#OK}, {@link javax.ws.rs.core.Response.Status#NOT_FOUND}, or
   *         {@link javax.ws.rs.core.Response.Status#BAD_REQUEST}
   * @throws Exception
   */
  @PUT
  @Path("{field}")
  @Produces({"text/csv", "application/json", "application/rdf+xml", "text/plain"})
  public Response getMetadataField(InputStream is, @Context HttpHeaders httpHeaders,
                                   @Context UriInfo info, @PathParam("field") String field) throws Exception {

    // use BAD request to indicate that we may not have had enough data to
    // process the request
    Response.Status defaultErrorResponse = Response.Status.BAD_REQUEST;
    Metadata metadata = null;
    try {
      metadata = parseMetadata(is, httpHeaders.getRequestHeaders(), info);
      // once we've parsed the document successfully, we should use NOT_FOUND
      // if we did not see the field
      defaultErrorResponse = Response.Status.NOT_FOUND;
    } catch (Exception e) {
      logger.info("Failed to process field " + field, e);
    }

    if (metadata == null || metadata.get(field) == null) {
      return Response.status(defaultErrorResponse).entity("Failed to get metadata field " + field).build();
    }

    // remove fields we don't care about for the response
    for (String name : metadata.names()) {
      if (!field.equals(name)) {
        metadata.remove(name);
      }
    }
    return Response.ok(metadata).build();
  }

  private Metadata parseMetadata(InputStream is,
                                 MultivaluedMap<String, String> httpHeaders, UriInfo info) throws Exception {
    final Metadata metadata = new Metadata();
    final ParseContext context = new ParseContext();
    AutoDetectParser parser = TikaResource.createParser(tikaConfig);
    TikaResource.fillMetadata(parser, metadata, context, httpHeaders);
    TikaResource.fillParseContext(context, httpHeaders);
    TikaResource.logRequest(logger, info, metadata);

    parser.parse(is, new DefaultHandler(), metadata, context);
    return metadata;
  }
}
"
tika-server/src/main/java/org/apache/tika/server/RecursiveMetadataResource.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.server;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.cxf.jaxrs.ext.multipart.Attachment;
import org.apache.tika.config.TikaConfig;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.parser.AutoDetectParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.RecursiveParserWrapper;
import org.apache.tika.sax.BasicContentHandlerFactory;
import org.xml.sax.helpers.DefaultHandler;

import javax.ws.rs.Consumes;
import javax.ws.rs.PUT;
import javax.ws.rs.Path;
import javax.ws.rs.Produces;
import javax.ws.rs.core.Context;
import javax.ws.rs.core.HttpHeaders;
import javax.ws.rs.core.MultivaluedMap;
import javax.ws.rs.core.Response;
import javax.ws.rs.core.UriInfo;
import java.io.InputStream;

@Path("/rmeta")
public class RecursiveMetadataResource {
  private static final Log logger = LogFactory.getLog(RecursiveMetadataResource.class);

  private TikaConfig tikaConfig;

  public RecursiveMetadataResource(TikaConfig tikaConfig) {
    this.tikaConfig = tikaConfig;
  }

  @PUT
  @Consumes("multipart/form-data")
  @Produces({"text/csv", "application/json"})
  @Path("form")
  public Response getMetadataFromMultipart(Attachment att, @Context UriInfo info) throws Exception {
    return Response.ok(
            parseMetadata(att.getObject(InputStream.class), att.getHeaders(), info)).build();
  }

  @PUT
  @Produces("application/json")
  public Response getMetadata(InputStream is, @Context HttpHeaders httpHeaders, @Context UriInfo info) throws Exception {
    return Response.ok(
            parseMetadata(is, httpHeaders.getRequestHeaders(), info)).build();
  }

  private MetadataList parseMetadata(InputStream is,
                                 MultivaluedMap<String, String> httpHeaders, UriInfo info) throws Exception {
    final Metadata metadata = new Metadata();
    final ParseContext context = new ParseContext();
    AutoDetectParser parser = TikaResource.createParser(tikaConfig);
    //TODO: parameterize choice of handler and max chars?
    BasicContentHandlerFactory.HANDLER_TYPE type = BasicContentHandlerFactory.HANDLER_TYPE.TEXT;
    RecursiveParserWrapper wrapper = new RecursiveParserWrapper(parser,
            new BasicContentHandlerFactory(type, -1));
    TikaResource.fillMetadata(parser, metadata, context, httpHeaders);
    TikaResource.fillParseContext(context, httpHeaders);
    TikaResource.logRequest(logger, info, metadata);

    wrapper.parse(is, new DefaultHandler(), metadata, context);
    return new MetadataList(wrapper.getMetadata());
  }
}
"
tika-server/src/main/java/org/apache/tika/server/RichTextContentHandler.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.server;

import org.apache.tika.sax.WriteOutContentHandler;
import org.xml.sax.Attributes;
import org.xml.sax.SAXException;

import java.io.Writer;

class RichTextContentHandler extends WriteOutContentHandler {
  public RichTextContentHandler(Writer writer) {
    super(writer);
  }

  @Override
  public void startElement(String uri, String localName, String qName, Attributes attributes) throws SAXException {
    super.startElement(uri, localName, qName, attributes);

    if ("img".equals(localName) && attributes.getValue("alt")!=null) {
      String nfo = "[image: "+attributes.getValue("alt")+ ']';

      characters(nfo.toCharArray(), 0, nfo.length());
    }

    if ("a".equals(localName) && attributes.getValue("name")!=null) {
      String nfo = "[bookmark: "+attributes.getValue("name")+ ']';

      characters(nfo.toCharArray(), 0, nfo.length());
    }
  }
}
"
tika-server/src/main/java/org/apache/tika/server/TarWriter.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.server;

import org.apache.commons.compress.archivers.tar.TarArchiveEntry;
import org.apache.commons.compress.archivers.tar.TarArchiveOutputStream;

import javax.ws.rs.Produces;
import javax.ws.rs.WebApplicationException;
import javax.ws.rs.core.MediaType;
import javax.ws.rs.core.MultivaluedMap;
import javax.ws.rs.ext.MessageBodyWriter;
import javax.ws.rs.ext.Provider;
import java.io.IOException;
import java.io.OutputStream;
import java.lang.annotation.Annotation;
import java.lang.reflect.Type;
import java.util.Map;

@Provider
@Produces("application/x-tar")
public class TarWriter implements MessageBodyWriter<Map<String, byte[]>> {
  private static void tarStoreBuffer(TarArchiveOutputStream zip, String name, byte[] dataBuffer) throws IOException {
    TarArchiveEntry entry = new TarArchiveEntry(name);

    entry.setSize(dataBuffer.length);

    zip.putArchiveEntry(entry);

    zip.write(dataBuffer);

    zip.closeArchiveEntry();
  }

  public boolean isWriteable(Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType) {
    return Map.class.isAssignableFrom(type);
  }

  public long getSize(Map<String, byte[]> stringMap, Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType) {
    return -1;
  }

  public void writeTo(Map<String, byte[]> parts, Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType, MultivaluedMap<String, Object> httpHeaders, OutputStream entityStream) throws IOException, WebApplicationException {
    TarArchiveOutputStream zip = new TarArchiveOutputStream(entityStream);

    for (Map.Entry<String, byte[]> entry : parts.entrySet()) {
      tarStoreBuffer(zip, entry.getKey(), entry.getValue());
    }

    zip.close();
  }
}
"
tika-server/src/main/java/org/apache/tika/server/TextMessageBodyWriter.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.server;

import au.com.bytecode.opencsv.CSVWriter;
import org.apache.tika.metadata.Metadata;

import javax.ws.rs.Produces;
import javax.ws.rs.WebApplicationException;
import javax.ws.rs.core.MediaType;
import javax.ws.rs.core.MultivaluedMap;
import javax.ws.rs.ext.MessageBodyWriter;
import javax.ws.rs.ext.Provider;
import java.io.IOException;
import java.io.OutputStream;
import java.io.OutputStreamWriter;
import java.io.Writer;
import java.lang.annotation.Annotation;
import java.lang.reflect.Type;
import java.util.ArrayList;
import java.util.Arrays;

/**
 * Returns simple text string for a particular metadata value.
 * This assumes that the metadata object only has one key;
 * if there is more than one key or no keys, this will throw a webapp exception.
 * <p>
 * This will choose the first value returned for the one key.
 */
@Provider
@Produces(MediaType.TEXT_PLAIN)
public class TextMessageBodyWriter implements MessageBodyWriter<Metadata> {

  public boolean isWriteable(Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType) {
    return mediaType.equals(MediaType.TEXT_PLAIN_TYPE) && Metadata.class.isAssignableFrom(type);
  }

  public long getSize(Metadata data, Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType) {
    return -1;
  }

  @Override
  @SuppressWarnings("resource")
  public void writeTo(Metadata metadata, Class<?> type, Type genericType, Annotation[] annotations,
      MediaType mediaType, MultivaluedMap<String, Object> httpHeaders, OutputStream entityStream) throws IOException,
      WebApplicationException {

    if (metadata.names().length != 1) {
      throw new WebApplicationException("Metadata object must only have one entry!");
    }
    Writer writer = new OutputStreamWriter(entityStream, "UTF-8");

    for (String name : metadata.names()) {
      writer.write(metadata.get(name));
    }
    
    // Don't close, just flush the stream
    writer.flush();
  }
}

"
tika-server/src/main/java/org/apache/tika/server/TikaDetectors.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.server;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import javax.ws.rs.GET;
import javax.ws.rs.Path;
import javax.ws.rs.Produces;

import org.apache.tika.config.TikaConfig;
import org.apache.tika.detect.CompositeDetector;
import org.apache.tika.detect.Detector;
import org.eclipse.jetty.util.ajax.JSON;

/**
 * <p>Provides details of all the {@link Detector}s registered with
 *  Apache Tika, similar to <em>--list-detectors</em> with the Tika CLI.
 */
@Path("/detectors")
public class TikaDetectors {
    private TikaConfig tika;
    private HTMLHelper html;
    
    public TikaDetectors(TikaConfig tika) {
        this.tika = tika;
        this.html = new HTMLHelper();
    }
    
    @GET
    @Produces("text/html")
    public String getDectorsHTML() {
        StringBuffer h = new StringBuffer();
        html.generateHeader(h, "Detectors available to Apache Tika");
        detectorAsHTML(tika.getDetector(), h, 2);
        html.generateFooter(h);
        return h.toString();
    }
    private void detectorAsHTML(Detector d, StringBuffer html, int level) {
        html.append("<h");
        html.append(level);
        html.append(">");
        String name = d.getClass().getName();
        html.append(name.substring(name.lastIndexOf('.')+1));
        html.append("</h");
        html.append(level);
        html.append(">");
        html.append("<p>Class: ");
        html.append(name);
        html.append("</p>");
        if (d instanceof CompositeDetector) {
            html.append("<p>Composite Detector</p>");
            for (Detector cd : ((CompositeDetector)d).getDetectors()) {
                detectorAsHTML(cd, html, level+1);
            }            
        }
    }
    
    @GET
    @Produces(javax.ws.rs.core.MediaType.APPLICATION_JSON)
    public String getDetectorsJSON() {
        Map<String,Object> details = new HashMap<String, Object>();
        detectorAsMap(tika.getDetector(), details);
        return JSON.toString(details);
    }
    private void detectorAsMap(Detector d, Map<String, Object> details) {
        details.put("name", d.getClass().getName());
        
        boolean isComposite = (d instanceof CompositeDetector);
        details.put("composite", isComposite);
        if (isComposite) {
            List<Map<String, Object>> c = new ArrayList<Map<String,Object>>();
            for (Detector cd : ((CompositeDetector)d).getDetectors()) {
                Map<String,Object> cdet = new HashMap<String, Object>();
                detectorAsMap(cd, cdet);
                c.add(cdet);
            }
            details.put("children", c);
        }
    }
    
    @GET
    @Produces("text/plain")
    public String getDetectorsPlain() {
        StringBuffer text = new StringBuffer();
        renderDetector(tika.getDetector(), text, 0);
        return text.toString();
    }
    private void renderDetector(Detector d, StringBuffer text, int indent) {
        boolean isComposite = (d instanceof CompositeDetector);
        String name = d.getClass().getName();
        
        for (int i=0; i<indent; i++) {
            text.append("  ");
        }
        text.append(name);
        if (isComposite) {
            text.append(" (Composite Detector):\n");

            List<Detector> subDetectors = ((CompositeDetector)d).getDetectors();
            for(Detector sd : subDetectors) {
                renderDetector(sd, text, indent+1);
            }
        } else {
            text.append("\n");
        }
    }
}"
tika-server/src/main/java/org/apache/tika/server/TikaExceptionMapper.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.server;

import org.apache.tika.exception.TikaException;

import javax.ws.rs.WebApplicationException;
import javax.ws.rs.core.Response;
import javax.ws.rs.ext.ExceptionMapper;
import javax.ws.rs.ext.Provider;

@Provider
public class TikaExceptionMapper implements ExceptionMapper<TikaException> {
  public Response toResponse(TikaException e) {
    if (e.getCause() !=null && e.getCause() instanceof WebApplicationException) {
      return ((WebApplicationException) e.getCause()).getResponse();
    } else {
      return Response.serverError().build();
    }
  }
}
"
tika-server/src/main/java/org/apache/tika/server/TikaLoggingFilter.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.server;

import java.io.IOException;

import javax.ws.rs.container.ContainerRequestContext;
import javax.ws.rs.container.ContainerRequestFilter;
import javax.ws.rs.container.PreMatching;
import javax.ws.rs.ext.Provider;

import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;

@Provider
@PreMatching
public class TikaLoggingFilter implements ContainerRequestFilter {
	private static final Log logger = LogFactory.getLog(TikaLoggingFilter.class);
	private boolean infoLevel;
	public TikaLoggingFilter(boolean infoLevel) {
		this.infoLevel = infoLevel;
	}
	@Override
	public void filter(ContainerRequestContext requestContext) throws IOException {
		String requestUri = requestContext.getUriInfo().getRequestUri().toString();
		String logMessage = "Request URI: " + requestUri;
		if (infoLevel) {
			logger.info(logMessage);
		} else {
			logger.debug(logMessage);
		}
	}
  
}
"
tika-server/src/main/java/org/apache/tika/server/TikaMimeTypes.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.server;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.SortedMap;
import java.util.TreeMap;

import javax.ws.rs.GET;
import javax.ws.rs.Path;
import javax.ws.rs.Produces;

import org.apache.tika.config.TikaConfig;
import org.apache.tika.mime.MediaType;
import org.apache.tika.mime.MediaTypeRegistry;
import org.apache.tika.parser.CompositeParser;
import org.apache.tika.parser.Parser;
import org.eclipse.jetty.util.ajax.JSON;

/**
 * <p>Provides details of all the mimetypes known to Apache Tika,
 *  similar to <em>--list-supported-types</em> with the Tika CLI.
 */
@Path("/mime-types")
public class TikaMimeTypes {
    private TikaConfig tika;
    private HTMLHelper html;
    
    public TikaMimeTypes(TikaConfig tika) {
        this.tika = tika;
        this.html = new HTMLHelper();
    }
    
    @GET
    @Produces("text/html")
    public String getMimeTypesHTML() {
        StringBuffer h = new StringBuffer();
        html.generateHeader(h, "Apache Tika Supported Mime Types");
        
        // Get our types
        List<MediaTypeDetails> types = getMediaTypes();
        
        // Get the first type in each section
        SortedMap<String,String> firstType = new TreeMap<String, String>();
        for (MediaTypeDetails type : types) {
            if (! firstType.containsKey(type.type.getType())) {
                firstType.put(type.type.getType(), type.type.toString());
            }
        }
        h.append("<ul>");
        for (String section : firstType.keySet()) {
            h.append("<li><a href=\"#").append(firstType.get(section)).append("\">").append(section).append("</a></li>\n");
        }
        h.append("</ul>");
        
        // Output all of them
        for (MediaTypeDetails type : types) {
            h.append("<a name=\"").append(type.type).append("\"></a>\n");
            h.append("<h2>").append(type.type).append("</h2>\n");
            
            for (MediaType alias : type.aliases) {
                h.append("<div>Alias: ").append(alias).append("</div>\n");
            }
            if (type.supertype != null) {
                h.append("<div>Super Type: <a href=\"#").append(type.supertype).append("\">").append(type.supertype).append("</a></div>\n");
            }
            
            if (type.parser != null) {
                h.append("<div>Parser: ").append(type.parser).append("</div>\n");
            }
        }

        html.generateFooter(h);
        return h.toString();
    }
    
    @GET
    @Produces(javax.ws.rs.core.MediaType.APPLICATION_JSON)
    public String getMimeTypesJSON() {
        Map<String,Object> details = new HashMap<String, Object>();
        
        for (MediaTypeDetails type : getMediaTypes()) {
            Map<String,Object> typeDets = new HashMap<String, Object>();

            typeDets.put("alias", type.aliases);
            if (type.supertype != null) {
                typeDets.put("supertype", type.supertype);
            }
            if (type.parser != null) {
                typeDets.put("parser", type.parser);
            }

            details.put(type.type.toString(), typeDets);
        }
        
        return JSON.toString(details);
    }
    
    @GET
    @Produces("text/plain")
    public String getMimeTypesPlain() {
        StringBuffer text = new StringBuffer();
        
        for (MediaTypeDetails type : getMediaTypes()) {
            text.append(type.type.toString());
            text.append("\n");
            
            for (MediaType alias : type.aliases) {
                text.append("  alias:     ").append(alias).append("\n");
            }
            if (type.supertype != null) {
                text.append("  supertype: ").append(type.supertype.toString()).append("\n");
            }
            
            if (type.parser != null) {
                text.append("  parser:    ").append(type.parser).append("\n");
            }
        }

        return text.toString();
    }
    
    protected List<MediaTypeDetails> getMediaTypes() {
        MediaTypeRegistry registry = tika.getMediaTypeRegistry();
        Map<MediaType, Parser> parsers = ((CompositeParser)tika.getParser()).getParsers();
        List<MediaTypeDetails> types = 
                new ArrayList<TikaMimeTypes.MediaTypeDetails>(registry.getTypes().size());

        for (MediaType type : registry.getTypes()) {
            MediaTypeDetails details = new MediaTypeDetails();
            details.type = type;
            details.aliases = registry.getAliases(type).toArray(new MediaType[0]);
            
            MediaType supertype = registry.getSupertype(type);
            if (supertype != null && !MediaType.OCTET_STREAM.equals(supertype)) {
                details.supertype = supertype;
            }
            
            Parser p = parsers.get(type);
            if (p != null) {
                if (p instanceof CompositeParser) {
                    p = ((CompositeParser)p).getParsers().get(type);
                }
                details.parser = p.getClass().getName();
            }
            
            types.add(details);
        }
        
        return types;
    }
    
    private static class MediaTypeDetails {
        private MediaType type;
        private MediaType[] aliases;
        private MediaType supertype;
        private String parser;
    }
}
"
tika-server/src/main/java/org/apache/tika/server/TikaParsers.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.server;

import java.util.ArrayList;
import java.util.Collections;
import java.util.Comparator;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;

import javax.ws.rs.GET;
import javax.ws.rs.Path;
import javax.ws.rs.Produces;

import org.apache.tika.config.TikaConfig;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.CompositeParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.Parser;
import org.apache.tika.parser.ParserDecorator;
import org.eclipse.jetty.util.ajax.JSON;

/**
 * <p>Provides details of all the {@link Parser}s registered with
 *  Apache Tika, similar to <em>--list-parsers</em> and
 *  <em>--list-parser-details</em> within the Tika CLI.
 */
@Path("/parsers")
public class TikaParsers {
    private static final ParseContext EMPTY_PC = new ParseContext();
    private TikaConfig tika;
    private HTMLHelper html;
    
    public TikaParsers(TikaConfig tika) {
        this.tika = tika;
        this.html = new HTMLHelper();
    }
    
    @GET
    @Path("/details")
    @Produces("text/html")
    public String getParserDetailsHTML() {
        return getParsersHTML(true);
    }
    @GET
    @Produces("text/html")
    public String getParsersHTML() {
        return getParsersHTML(false);
    }
    protected String getParsersHTML(boolean withMimeTypes) {
        ParserDetails p = new ParserDetails(tika.getParser());
        
        StringBuffer h = new StringBuffer();
        html.generateHeader(h, "Parsers available to Apache Tika");
        parserAsHTML(p, withMimeTypes, h, 2);
        html.generateFooter(h);
        return h.toString();
    }
    private void parserAsHTML(ParserDetails p, boolean withMimeTypes, StringBuffer html, int level) {
        html.append("<h");
        html.append(level);
        html.append(">");
        html.append(p.shortName);
        html.append("</h");
        html.append(level);
        html.append(">");
        html.append("<p>Class: ");
        html.append(p.className);
        html.append("</p>");
        if (p.isDecorated) {
            html.append("<p>Decorated Parser</p>");
        }
        if (p.isComposite) {
            html.append("<p>Composite Parser</p>");
            for (Parser cp : p.childParsers) {
                parserAsHTML(new ParserDetails(cp), withMimeTypes, html, level+1);
            }
        } else if (withMimeTypes) {
            html.append("<p>Mime Types:");
            html.append("<ul>");
            for (MediaType mt : p.supportedTypes) {
                html.append("<li>");
                html.append(mt.toString());
                html.append("</li>");
            }
            html.append("</ul>");
            html.append("</p>");
        }
    }
    
    @GET
    @Path("/details")
    @Produces(javax.ws.rs.core.MediaType.APPLICATION_JSON)
    public String getParserDetailsJSON() {
        return getParsersJSON(true);
    }
    @GET
    @Produces(javax.ws.rs.core.MediaType.APPLICATION_JSON)
    public String getParsersJSON() {
        return getParsersJSON(false);
    }
    protected String getParsersJSON(boolean withMimeTypes) {
        Map<String,Object> details = new HashMap<String, Object>();
        parserAsMap(new ParserDetails(tika.getParser()), withMimeTypes, details);
        return JSON.toString(details);
    }
    private void parserAsMap(ParserDetails p, boolean withMimeTypes, Map<String, Object> details) {
        details.put("name", p.className);
        details.put("composite", p.isComposite);
        details.put("decorated", p.isDecorated);
        
        if (p.isComposite) {
            List<Map<String, Object>> c = new ArrayList<Map<String,Object>>();
            for (Parser cp : p.childParsers) {
                Map<String,Object> cdet = new HashMap<String, Object>();
                parserAsMap(new ParserDetails(cp), withMimeTypes, cdet);
                c.add(cdet);
            }
            details.put("children", c);
        } else if (withMimeTypes) {
            List<String> mts = new ArrayList<String>(p.supportedTypes.size());
            for (MediaType mt : p.supportedTypes) {
                mts.add(mt.toString());
            }
            details.put("supportedTypes", mts);
        }
    }
    
    @GET
    @Path("/details")
    @Produces("text/plain")
    public String getParserDetailssPlain() {
        return getParsersPlain(true);
    }
    @GET
    @Produces("text/plain")
    public String getParsersPlain() {
        return getParsersPlain(false);
    }
    protected String getParsersPlain(boolean withMimeTypes) {
        StringBuffer text = new StringBuffer();
        renderParser(new ParserDetails(tika.getParser()), withMimeTypes, text, "");
        return text.toString();
    }
    private void renderParser(ParserDetails p, boolean withMimeTypes, StringBuffer text, String indent) {
        String nextIndent = indent + "  ";
        
        text.append(indent);
        text.append(p.className);
        if (p.isDecorated) {
            text.append(" (Decorated Parser)");
        }
        if (p.isComposite) {
            text.append(" (Composite Parser):\n");

            for (Parser cp : p.childParsers) {
                renderParser(new ParserDetails(cp), withMimeTypes, text, nextIndent);
            }
        } else {
            text.append("\n");
            if (withMimeTypes) {
                for (MediaType mt : p.supportedTypes) {
                    text.append(nextIndent);
                    text.append("Supports: ");
                    text.append(mt.toString());
                    text.append("\n");
                }
            }
        }
    }
    
    private static class ParserDetails {
        private String className;
        private String shortName;
        private boolean isComposite;
        private boolean isDecorated;
        private Set<MediaType> supportedTypes;
        private List<Parser> childParsers;
        
        private ParserDetails(Parser p) {
            if (p instanceof ParserDecorator) {
                isDecorated = true;
                p = ((ParserDecorator)p).getWrappedParser();
            }
            
            className = p.getClass().getName();
            shortName = className.substring(className.lastIndexOf('.')+1);
            
            if (p instanceof CompositeParser) {
                isComposite = true;
                supportedTypes = Collections.emptySet();
                
                // Get the unique set of child parsers
                Set<Parser> children = new HashSet<Parser>(
                        ((CompositeParser)p).getParsers(EMPTY_PC).values());
                // Sort it by class name
                childParsers = new ArrayList<Parser>(children);
                Collections.sort(childParsers, new Comparator<Parser>() {
                    @Override
                    public int compare(Parser p1, Parser p2) {
                        return p1.getClass().getName().compareTo(p2.getClass().getName());
                    }
                });
            } else {
                supportedTypes = p.getSupportedTypes(EMPTY_PC);
            }
        }
    }
}"
tika-server/src/main/java/org/apache/tika/server/TikaResource.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.server;

import javax.mail.internet.ContentDisposition;
import javax.mail.internet.ParseException;
import javax.ws.rs.Consumes;
import javax.ws.rs.GET;
import javax.ws.rs.PUT;
import javax.ws.rs.Path;
import javax.ws.rs.Produces;
import javax.ws.rs.WebApplicationException;
import javax.ws.rs.core.Context;
import javax.ws.rs.core.HttpHeaders;
import javax.ws.rs.core.MultivaluedMap;
import javax.ws.rs.core.Response;
import javax.ws.rs.core.StreamingOutput;
import javax.ws.rs.core.UriInfo;
import javax.xml.transform.OutputKeys;
import javax.xml.transform.TransformerConfigurationException;
import javax.xml.transform.sax.SAXTransformerFactory;
import javax.xml.transform.sax.TransformerHandler;
import javax.xml.transform.stream.StreamResult;

import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.io.OutputStreamWriter;
import java.io.Writer;
import java.lang.reflect.Field;
import java.util.Locale;
import java.util.Map;
import java.util.Set;

import org.apache.commons.lang.StringUtils;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.cxf.jaxrs.ext.multipart.Attachment;
import org.apache.poi.extractor.ExtractorFactory;
import org.apache.poi.hwpf.OldWordFileFormatException;
import org.apache.tika.config.TikaConfig;
import org.apache.tika.detect.Detector;
import org.apache.tika.exception.EncryptedDocumentException;
import org.apache.tika.exception.TikaException;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.TikaMetadataKeys;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.AutoDetectParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.Parser;
import org.apache.tika.parser.PasswordProvider;
import org.apache.tika.parser.html.HtmlParser;
import org.apache.tika.parser.ocr.TesseractOCRConfig;
import org.apache.tika.parser.pdf.PDFParserConfig;
import org.apache.tika.sax.BodyContentHandler;
import org.apache.tika.sax.ExpandedTitleContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;

@Path("/tika")
public class TikaResource {
  public static final String GREETING = "This is Tika Server. Please PUT\n";
  public static final String X_TIKA_OCR_HEADER_PREFIX = "X-Tika-OCR";
  public static final String X_TIKA_PDF_HEADER_PREFIX = "X-Tika-PDF";


  private final Log logger = LogFactory.getLog(TikaResource.class);
  
  private TikaConfig tikaConfig;
  public TikaResource(TikaConfig tikaConfig) {
      this.tikaConfig = tikaConfig;
  }

  static {
    ExtractorFactory.setAllThreadsPreferEventExtractors(true);
  }
  
  @GET
  @Produces("text/plain")
  public String getMessage() {
    return GREETING;
  }

  @SuppressWarnings("serial")
  public static AutoDetectParser createParser(TikaConfig tikaConfig) {
    final AutoDetectParser parser = new AutoDetectParser(tikaConfig);

    Map<MediaType,Parser> parsers = parser.getParsers();
    parsers.put(MediaType.APPLICATION_XML, new HtmlParser());
    parser.setParsers(parsers);

    parser.setFallback(new Parser() {
      public Set<MediaType> getSupportedTypes(ParseContext parseContext) {
        return parser.getSupportedTypes(parseContext);
      }

      public void parse(InputStream inputStream, ContentHandler contentHandler, Metadata metadata, ParseContext parseContext) {
        throw new WebApplicationException(Response.Status.UNSUPPORTED_MEDIA_TYPE);
      }
    });

    return parser;
  }

  public static String detectFilename(MultivaluedMap<String, String> httpHeaders) {

    String disposition = httpHeaders.getFirst("Content-Disposition");
    if (disposition != null) {
      try {
        ContentDisposition c = new ContentDisposition(disposition);

        // only support "attachment" dispositions
        if ("attachment".equals(c.getDisposition())) {
          String fn = c.getParameter("filename");
          if (fn != null) {
            return fn;
          }
        }
      } catch (ParseException e) {
        // not a valid content-disposition field
      }
    }

    // this really should not be used, since it's not an official field
    return httpHeaders.getFirst("File-Name");
  }

  public static void fillParseContext(ParseContext parseContext, MultivaluedMap<String, String> httpHeaders) {
    TesseractOCRConfig ocrConfig = new TesseractOCRConfig();
    PDFParserConfig pdfParserConfig = new PDFParserConfig();
    for (String key : httpHeaders.keySet()) {
      if (StringUtils.startsWith(key, X_TIKA_OCR_HEADER_PREFIX)) {
          processHeaderConfig(httpHeaders, ocrConfig, key, X_TIKA_OCR_HEADER_PREFIX);
      } else if (StringUtils.startsWith(key, X_TIKA_PDF_HEADER_PREFIX)) {
        processHeaderConfig(httpHeaders, pdfParserConfig, key, X_TIKA_PDF_HEADER_PREFIX);
      }
    }
    parseContext.set(TesseractOCRConfig.class, ocrConfig);
    parseContext.set(PDFParserConfig.class, pdfParserConfig);
  }

  /**
   * Utility method to set a property on a class via reflection.
   *
   * @param httpHeaders the HTTP headers set.
   * @param object the <code>Object</code> to set the property on.
   * @param key the key of the HTTP Header.
   * @param prefix the name of the HTTP Header prefix used to find property.
   * @throws WebApplicationException thrown when field cannot be found.
   */
  private static void processHeaderConfig(MultivaluedMap<String, String> httpHeaders, Object object, String key, String prefix) {
    try {
      String property = StringUtils.removeStart(key, prefix);
      Field field = object.getClass().getDeclaredField(StringUtils.uncapitalize(property));
      field.setAccessible(true);
      if (field.getType() == String.class) {
        field.set(object, httpHeaders.getFirst(key));
      } else if (field.getType() == int.class) {
        field.setInt(object, Integer.parseInt(httpHeaders.getFirst(key)));
      } else if (field.getType() == double.class) {
        field.setDouble(object, Double.parseDouble(httpHeaders.getFirst(key)));
      } else if (field.getType() == boolean.class) {
        field.setBoolean(object, Boolean.parseBoolean(httpHeaders.getFirst(key)));
      }
    } catch (Throwable ex) {
      throw new WebApplicationException(String.format("%s is an invalid %s header", key, X_TIKA_OCR_HEADER_PREFIX));
    }
  }

  @SuppressWarnings("serial")
public static void fillMetadata(AutoDetectParser parser, Metadata metadata, ParseContext context, MultivaluedMap<String, String> httpHeaders) {
    String fileName = detectFilename(httpHeaders);
    if (fileName != null) {
      metadata.set(TikaMetadataKeys.RESOURCE_NAME_KEY, fileName);
    }

    String contentTypeHeader = httpHeaders.getFirst(HttpHeaders.CONTENT_TYPE);
    javax.ws.rs.core.MediaType mediaType = contentTypeHeader == null ? null 
        : javax.ws.rs.core.MediaType.valueOf(contentTypeHeader);
    if (mediaType!=null && "xml".equals(mediaType.getSubtype()) ) {
      mediaType = null;
    }

    if (mediaType !=null && mediaType.equals(javax.ws.rs.core.MediaType.APPLICATION_OCTET_STREAM_TYPE)) {
      mediaType = null;
    }

    if (mediaType !=null) {
      metadata.add(org.apache.tika.metadata.HttpHeaders.CONTENT_TYPE, mediaType.toString());

      final Detector detector = parser.getDetector();

      parser.setDetector(new Detector() {
        public MediaType detect(InputStream inputStream, Metadata metadata) throws IOException {
          String ct = metadata.get(org.apache.tika.metadata.HttpHeaders.CONTENT_TYPE);

          if (ct!=null) {
            return MediaType.parse(ct);
          } else {
            return detector.detect(inputStream, metadata);
          }
        }
      });
    }
    
    final String password = httpHeaders.getFirst("Password");
    if (password != null) {
        context.set(PasswordProvider.class, new PasswordProvider() {
            @Override
            public String getPassword(Metadata metadata) {
                return password;
            }
        });
    }
  }

  @PUT
  @Consumes("multipart/form-data")
  @Produces("text/plain")
  @Path("form")
  public StreamingOutput getTextFromMultipart(Attachment att, @Context final UriInfo info) {
	  return produceText(att.getObject(InputStream.class), att.getHeaders(), info);
  }
  
  @PUT
  @Consumes("*/*")
  @Produces("text/plain")
  public StreamingOutput getText(final InputStream is, @Context HttpHeaders httpHeaders, @Context final UriInfo info) {
	  return produceText(is, httpHeaders.getRequestHeaders(), info);
  }
  public StreamingOutput produceText(final InputStream is, MultivaluedMap<String, String> httpHeaders, final UriInfo info) {	  
    final AutoDetectParser parser = createParser(tikaConfig);
    final Metadata metadata = new Metadata();
    final ParseContext context = new ParseContext();

    fillMetadata(parser, metadata, context, httpHeaders);
    fillParseContext(context, httpHeaders);

    logRequest(logger, info, metadata);

    return new StreamingOutput() {
      public void write(OutputStream outputStream) throws IOException, WebApplicationException {
        Writer writer = new OutputStreamWriter(outputStream, "UTF-8");

        BodyContentHandler body = new BodyContentHandler(new RichTextContentHandler(writer));

        TikaInputStream tis = TikaInputStream.get(is);

        try {
            parser.parse(tis, body, metadata, context);
        } catch (SAXException e) {
          throw new WebApplicationException(e);
        } catch (EncryptedDocumentException e) {
          logger.warn(String.format(
                  Locale.ROOT,
                  "%s: Encrypted document",
                  info.getPath()
          ), e);

          throw new WebApplicationException(e, Response.status(422).build());
        } catch (TikaException e) {
          logger.warn(String.format(
            Locale.ROOT,
            "%s: Text extraction failed",
            info.getPath()
          ), e);

          if (e.getCause()!=null && e.getCause() instanceof WebApplicationException) {
            throw (WebApplicationException) e.getCause();
          }

          if (e.getCause()!=null && e.getCause() instanceof IllegalStateException) {
            throw new WebApplicationException(Response.status(422).build());
          }

          if (e.getCause()!=null && e.getCause() instanceof OldWordFileFormatException) {
            throw new WebApplicationException(Response.status(422).build());
          }

          throw new WebApplicationException(Response.Status.INTERNAL_SERVER_ERROR);
        } finally {
          tis.close();
        }
      }
    };
  }

  @PUT
  @Consumes("multipart/form-data")
  @Produces("text/html")
  @Path("form")
  public StreamingOutput getHTMLFromMultipart(Attachment att, @Context final UriInfo info) {
	  return produceOutput(att.getObject(InputStream.class), att.getHeaders(), info, "html");
  }

  @PUT
  @Consumes("*/*")
  @Produces("text/html")
  public StreamingOutput getHTML(final InputStream is, @Context HttpHeaders httpHeaders, @Context final UriInfo info) {
	  return produceOutput(is, httpHeaders.getRequestHeaders(), info, "html");
  }

  @PUT
  @Consumes("multipart/form-data")
  @Produces("text/xml")
  @Path("form")
  public StreamingOutput getXMLFromMultipart(Attachment att, @Context final UriInfo info) {
	  return produceOutput(att.getObject(InputStream.class), att.getHeaders(), info, "xml");
  }
  
  @PUT
  @Consumes("*/*")
  @Produces("text/xml")
  public StreamingOutput getXML(final InputStream is, @Context HttpHeaders httpHeaders, @Context final UriInfo info) {
    return produceOutput(is, httpHeaders.getRequestHeaders(), info, "xml");
  }
  
  private StreamingOutput produceOutput(final InputStream is, final MultivaluedMap<String, String> httpHeaders, 
        final UriInfo info, final String format) {
    final AutoDetectParser parser = createParser(tikaConfig);
    final Metadata metadata = new Metadata();
    final ParseContext context = new ParseContext();

    fillMetadata(parser, metadata, context, httpHeaders);
    fillParseContext(context, httpHeaders);


    logRequest(logger, info, metadata);

    return new StreamingOutput() {
      public void write(OutputStream outputStream)
        throws IOException, WebApplicationException {
        Writer writer = new OutputStreamWriter(outputStream, "UTF-8");
        ContentHandler content;

        try {
          SAXTransformerFactory factory = (SAXTransformerFactory)SAXTransformerFactory.newInstance( );
          TransformerHandler handler = factory.newTransformerHandler( );
          handler.getTransformer().setOutputProperty(OutputKeys.METHOD, format);
          handler.getTransformer().setOutputProperty(OutputKeys.INDENT, "yes");
          handler.getTransformer().setOutputProperty(OutputKeys.ENCODING, "UTF-8");
          handler.setResult(new StreamResult(writer));
          content = new ExpandedTitleContentHandler( handler );
        }
        catch ( TransformerConfigurationException e ) {
          throw new WebApplicationException( e );
        }

        TikaInputStream tis = TikaInputStream.get(is);

        try {
          parser.parse(tis, content, metadata, context);
        }
        catch (SAXException e) {
          throw new WebApplicationException(e);
        }
        catch (EncryptedDocumentException e) {
          logger.warn(String.format(
            Locale.ROOT,
            "%s: Encrypted document",
            info.getPath()
          ), e);
          throw new WebApplicationException(e, Response.status(422).build());
        }
        catch (TikaException e) {
          logger.warn(String.format(
            Locale.ROOT,
            "%s: Text extraction failed",
            info.getPath()
          ), e);

          if (e.getCause()!=null && e.getCause() instanceof WebApplicationException)
            throw (WebApplicationException) e.getCause();

          if (e.getCause()!=null && e.getCause() instanceof IllegalStateException)
            throw new WebApplicationException(Response.status(422).build());

          if (e.getCause()!=null && e.getCause() instanceof OldWordFileFormatException)
            throw new WebApplicationException(Response.status(422).build());

          throw new WebApplicationException(Response.Status.INTERNAL_SERVER_ERROR);
        }
        finally {
          tis.close();
        }
      }
    };
  }

  public static void logRequest(Log logger, UriInfo info, Metadata metadata) {
    if (metadata.get(org.apache.tika.metadata.HttpHeaders.CONTENT_TYPE)==null) {
      logger.info(String.format(
              Locale.ROOT,
              "%s (autodetecting type)",
              info.getPath()
      ));
    } else {
      logger.info(String.format(
              Locale.ROOT,
              "%s (%s)",
              info.getPath(),
              metadata.get(org.apache.tika.metadata.HttpHeaders.CONTENT_TYPE)
      ));
    }
  }
}
"
tika-server/src/main/java/org/apache/tika/server/TikaServerCli.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.server;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.CommandLineParser;
import org.apache.commons.cli.GnuParser;
import org.apache.commons.cli.HelpFormatter;
import org.apache.commons.cli.Options;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.cxf.binding.BindingFactoryManager;
import org.apache.cxf.jaxrs.JAXRSBindingFactory;
import org.apache.cxf.jaxrs.JAXRSServerFactoryBean;
import org.apache.cxf.jaxrs.lifecycle.ResourceProvider;
import org.apache.cxf.jaxrs.lifecycle.SingletonResourceProvider;
import org.apache.tika.Tika;
import org.apache.tika.config.TikaConfig;

public class TikaServerCli {
  private static final Log logger = LogFactory.getLog(TikaServerCli.class);
  public static final int DEFAULT_PORT = 9998;
  public static final String DEFAULT_HOST = "localhost";
  public static final Set<String> LOG_LEVELS = 
		  new HashSet<String>(Arrays.asList("debug", "info"));
  
  private static Options getOptions() {
    Options options = new Options();
    options.addOption("h", "host", true, "host name (default = " + DEFAULT_HOST + ')');
    options.addOption("p", "port", true, "listen port (default = " + DEFAULT_PORT + ')');
    options.addOption("l", "log", true, "request URI log level ('debug' or 'info')");
    options.addOption("?", "help", false, "this help message");

    return options;
  }

  public static void main(String[] args) {
    
    logger.info("Starting " + new Tika().toString() + " server");

    try {
      Options options = getOptions();

      CommandLineParser cliParser = new GnuParser();
      CommandLine line = cliParser.parse(options, args);

      if (line.hasOption("help")) {
          HelpFormatter helpFormatter = new HelpFormatter();
          helpFormatter.printHelp("tikaserver", options);
          System.exit(-1);
      }
      
      String host = DEFAULT_HOST;

      if (line.hasOption("host")) {
        host = line.getOptionValue("host");
      }
      
      int port = DEFAULT_PORT;

      if (line.hasOption("port")) {
        port = Integer.valueOf(line.getOptionValue("port"));
      }
      
      TikaLoggingFilter logFilter = null;
      if (line.hasOption("log")) {
        String logLevel = line.getOptionValue("log");
        if (LOG_LEVELS.contains(logLevel)) {
            boolean isInfoLevel = "info".equals(logLevel);
            logFilter = new TikaLoggingFilter(isInfoLevel);
        } else {
        	logger.info("Unsupported request URI log level: " + logLevel);
        }
      }
      // The Tika Configuration to use throughout
      TikaConfig tika = TikaConfig.getDefaultConfig();

      JAXRSServerFactoryBean sf = new JAXRSServerFactoryBean();

      List<ResourceProvider> rCoreProviders = new ArrayList<ResourceProvider>();
      rCoreProviders.add(new SingletonResourceProvider(new MetadataResource(tika)));
      rCoreProviders.add(new SingletonResourceProvider(new RecursiveMetadataResource(tika)));
      rCoreProviders.add(new SingletonResourceProvider(new DetectorResource(tika)));
      rCoreProviders.add(new SingletonResourceProvider(new TikaResource(tika)));
      rCoreProviders.add(new SingletonResourceProvider(new UnpackerResource(tika)));
      rCoreProviders.add(new SingletonResourceProvider(new TikaMimeTypes(tika)));
      rCoreProviders.add(new SingletonResourceProvider(new TikaDetectors(tika)));
      rCoreProviders.add(new SingletonResourceProvider(new TikaParsers(tika)));
      rCoreProviders.add(new SingletonResourceProvider(new TikaVersion(tika)));
      List<ResourceProvider> rAllProviders = new ArrayList<ResourceProvider>(rCoreProviders);
      rAllProviders.add(new SingletonResourceProvider(new TikaWelcome(tika, rCoreProviders)));
      sf.setResourceProviders(rAllProviders);
      
      List<Object> providers = new ArrayList<Object>();
      providers.add(new TarWriter());
      providers.add(new ZipWriter());
      providers.add(new CSVMessageBodyWriter());
      providers.add(new MetadataListMessageBodyWriter());
      providers.add(new JSONMessageBodyWriter());
      providers.add(new XMPMessageBodyWriter());
      providers.add(new TextMessageBodyWriter());
      providers.add(new TikaExceptionMapper());
      if (logFilter != null) {
    	  providers.add(logFilter);
      }
      sf.setProviders(providers);
      
      sf.setAddress("http://" + host + ":" + port + "/");
      BindingFactoryManager manager = sf.getBus().getExtension(
				BindingFactoryManager.class);
      JAXRSBindingFactory factory = new JAXRSBindingFactory();
      factory.setBus(sf.getBus());
      manager.registerBindingFactory(JAXRSBindingFactory.JAXRS_BINDING_ID,
				factory);
      sf.create();
      logger.info("Started");
    } catch (Exception ex) {
      logger.fatal("Can't start", ex);
      System.exit(-1);
    }
  }
}
"
tika-server/src/main/java/org/apache/tika/server/TikaVersion.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.server;

import javax.ws.rs.GET;
import javax.ws.rs.Path;
import javax.ws.rs.Produces;

import org.apache.tika.Tika;
import org.apache.tika.config.TikaConfig;

@Path("/version")
public class TikaVersion {
    private Tika tika;
    public TikaVersion(TikaConfig tika) {
        this.tika = new Tika(tika);
    }
    
    @GET
    @Produces("text/plain")
    public String getVersion() {
      return tika.toString();
    }
}
"
tika-server/src/main/java/org/apache/tika/server/TikaWelcome.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.server;

import java.lang.annotation.Annotation;
import java.lang.reflect.Method;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collections;
import java.util.Comparator;
import java.util.HashMap;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;

import javax.ws.rs.DELETE;
import javax.ws.rs.GET;
import javax.ws.rs.HEAD;
import javax.ws.rs.OPTIONS;
import javax.ws.rs.POST;
import javax.ws.rs.PUT;
import javax.ws.rs.Path;
import javax.ws.rs.Produces;

import org.apache.cxf.jaxrs.lifecycle.ResourceProvider;
import org.apache.tika.Tika;
import org.apache.tika.config.TikaConfig;

/**
 * <p>Provides a basic welcome to the Apache Tika Server.</p>
 */
@Path("/")
public class TikaWelcome {
    private static final String DOCS_URL = "https://wiki.apache.org/tika/TikaJAXRS";
    
    private static final Map<Class<? extends Annotation>, String> HTTP_METHODS =
            new HashMap<Class<? extends Annotation>, String>();
    static {
        HTTP_METHODS.put(DELETE.class , "DELETE");
        HTTP_METHODS.put(GET.class,     "GET");
        HTTP_METHODS.put(HEAD.class,    "HEAD");
        HTTP_METHODS.put(OPTIONS.class, "OPTIONS");
        HTTP_METHODS.put(POST.class,    "POST");
        HTTP_METHODS.put(PUT.class,     "PUT");
    }
    
    private Tika tika;
    private HTMLHelper html;
    private List<Class<?>> endpoints = new LinkedList<Class<?>>();
    
    public TikaWelcome(TikaConfig tika, List<ResourceProvider> rCoreProviders) {
        this.tika = new Tika(tika);
        this.html = new HTMLHelper();
        for (ResourceProvider rp : rCoreProviders) {
            this.endpoints.add(rp.getResourceClass());
        }
    }
    
    protected List<Endpoint> identifyEndpoints() {
        List<Endpoint> found = new ArrayList<Endpoint>();
        for (Class<?> endpoint : endpoints) {
            Path p = endpoint.getAnnotation(Path.class);
            String basePath = null;
            if (p != null)
                basePath = p.value();

            for (Method m : endpoint.getMethods()) {
                String httpMethod = null;
                String methodPath = null;
                String[] produces = null;
                
                for (Annotation a : m.getAnnotations()) {
                    for (Class<? extends Annotation> httpMethAnn : HTTP_METHODS.keySet()) {
                        if (httpMethAnn.isInstance(a)) {
                            httpMethod = HTTP_METHODS.get(httpMethAnn);
                        }
                    }
                    if (a instanceof Path) {
                        methodPath = ((Path)a).value();
                    }
                    if (a instanceof Produces) {
                        produces = ((Produces)a).value();
                    }
                }
                
                if (httpMethod != null) {
                    String mPath = basePath;
                    if (mPath == null) {
                        mPath = "";
                    }
                    if (methodPath != null) {
                        mPath += methodPath;
                    }
                    if (produces == null) {
                        produces = new String[0];
                    }
                    found.add(new Endpoint(endpoint, m, mPath, httpMethod, produces));
                }
            }
        }
        Collections.sort(found, new Comparator<Endpoint>() {
            @Override
            public int compare(Endpoint e1, Endpoint e2) {
                int res = e1.path.compareTo(e2.path);
                if (res == 0) {
                    res = e1.methodName.compareTo(e2.methodName);
                }
                return res;
            }
        });
        return found;
    }
    
    @GET
    @Produces("text/html")
    public String getWelcomeHTML() {
        StringBuffer h = new StringBuffer();
        html.generateHeader(h, "Welcome to the " + tika.toString() + " Server");
        
        h.append("<p>For endpoints, please see <a href=\"");
        h.append(DOCS_URL);
        h.append("\">");
        h.append(DOCS_URL);
        h.append("</a></p>\n");

        h.append("<ul>\n");
        for (Endpoint e : identifyEndpoints()) {
            h.append("<li><b>");
            h.append(e.httpMethod);
            h.append("</b> <i><a href=\"");
            h.append(e.path);
            h.append("\">");
            h.append(e.path);
            h.append("</a></i><br />");
            h.append("Class: ");
            h.append(e.className);
            h.append("<br />Method: ");
            h.append(e.methodName);
            for (String produces : e.produces) {
                h.append("<br />Produces: ");
                h.append(produces);
            }
            h.append("</li>\n");
        }
        h.append("</ul>\n");

        html.generateFooter(h);
        return h.toString();
    }

    @GET
    @Produces("text/plain")
    public String getWelcomePlain() {
        StringBuffer text = new StringBuffer();
        
        text.append(tika.toString());
        text.append("\n");
        text.append("For endpoints, please see ");
        text.append(DOCS_URL);
        text.append("\n\n");
        
        for (Endpoint e : identifyEndpoints()) {
            text.append(e.httpMethod);
            text.append(" ");
            text.append(e.path);
            text.append("\n");
            for (String produces : e.produces) {
                text.append(" => ");
                text.append(produces);
                text.append("\n");
            }
        }

        return text.toString();
    }
    
    protected class Endpoint {
        public final String className;
        public final String methodName;
        public final String path;
        public final String httpMethod;
        public final List<String> produces;
        protected Endpoint(Class<?> endpoint, Method method, String path,
                           String httpMethod, String[] produces) {
            this.className = endpoint.getCanonicalName();
            this.methodName = method.getName();
            this.path = path;
            this.httpMethod = httpMethod;
            this.produces = Collections.unmodifiableList(Arrays.asList(produces));
        }
    }
}
"
tika-server/src/main/java/org/apache/tika/server/UnpackerResource.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.server;

import java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;
import java.io.OutputStreamWriter;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.Locale;
import java.util.Map;

import javax.ws.rs.PUT;
import javax.ws.rs.Path;
import javax.ws.rs.Produces;
import javax.ws.rs.WebApplicationException;
import javax.ws.rs.core.Context;
import javax.ws.rs.core.HttpHeaders;
import javax.ws.rs.core.Response;
import javax.ws.rs.core.UriInfo;

import au.com.bytecode.opencsv.CSVWriter;
import org.apache.commons.lang.mutable.MutableInt;
import org.apache.commons.logging.Log;
import org.apache.commons.logging.LogFactory;
import org.apache.poi.poifs.filesystem.DirectoryEntry;
import org.apache.poi.poifs.filesystem.DocumentEntry;
import org.apache.poi.poifs.filesystem.DocumentInputStream;
import org.apache.poi.poifs.filesystem.Entry;
import org.apache.poi.poifs.filesystem.Ole10Native;
import org.apache.poi.poifs.filesystem.Ole10NativeException;
import org.apache.poi.poifs.filesystem.POIFSFileSystem;
import org.apache.poi.util.IOUtils;
import org.apache.tika.config.TikaConfig;
import org.apache.tika.exception.TikaException;
import org.apache.tika.extractor.EmbeddedDocumentExtractor;
import org.apache.tika.io.TikaInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.TikaMetadataKeys;
import org.apache.tika.mime.MimeTypeException;
import org.apache.tika.parser.AutoDetectParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.microsoft.OfficeParser;
import org.apache.tika.sax.BodyContentHandler;
import org.xml.sax.ContentHandler;
import org.xml.sax.SAXException;
import org.xml.sax.helpers.DefaultHandler;

@Path("/unpack")
public class UnpackerResource {
  private static final Log logger = LogFactory.getLog(UnpackerResource.class);
  public static final String TEXT_FILENAME = "__TEXT__";
  private static final String META_FILENAME = "__METADATA__";

  private TikaConfig tikaConfig;
  public UnpackerResource(TikaConfig tikaConfig) {
      this.tikaConfig = tikaConfig;
  }

  @Path("/{id:(/.*)?}")
  @PUT
  @Produces({"application/zip", "application/x-tar"})
  public Map<String, byte[]> unpack(
          InputStream is,
          @Context HttpHeaders httpHeaders,
          @Context UriInfo info
  ) throws Exception {
    return process(is, httpHeaders, info, false);
  }

  @Path("/all{id:(/.*)?}")
  @PUT
  @Produces({"application/zip", "application/x-tar"})
  public Map<String, byte[]> unpackAll(
          InputStream is,
          @Context HttpHeaders httpHeaders,
          @Context UriInfo info
  ) throws Exception {
    return process(is, httpHeaders, info, true);
  }

  private Map<String, byte[]> process(
          InputStream is,
          @Context HttpHeaders httpHeaders,
          @Context UriInfo info,
          boolean saveAll
  ) throws Exception {
    Metadata metadata = new Metadata();
    ParseContext pc = new ParseContext();

    AutoDetectParser parser = TikaResource.createParser(tikaConfig);

    TikaResource.fillMetadata(parser, metadata, pc, httpHeaders.getRequestHeaders());
    TikaResource.logRequest(logger, info, metadata);

    ContentHandler ch;
    ByteArrayOutputStream text = new ByteArrayOutputStream();

    if (saveAll) {
      ch = new BodyContentHandler(new RichTextContentHandler(new OutputStreamWriter(text, "UTF-8")));
    } else {
      ch = new DefaultHandler();
    }

    Map<String, byte[]> files = new HashMap<String, byte[]>();
    MutableInt count = new MutableInt();

    pc.set(EmbeddedDocumentExtractor.class, new MyEmbeddedDocumentExtractor(count, files));

    try {
      parser.parse(is, ch, metadata, pc);
    } catch (TikaException ex) {
      logger.warn(String.format(
              Locale.ROOT,
              "%s: Unpacker failed",
              info.getPath()
      ), ex);

      throw ex;
    }

    if (count.intValue() == 0 && !saveAll) {
      throw new WebApplicationException(Response.Status.NO_CONTENT);
    }

    if (saveAll) {
      files.put(TEXT_FILENAME, text.toByteArray());

      ByteArrayOutputStream metaStream = new ByteArrayOutputStream();
      metadataToCsv(metadata, metaStream);

      files.put(META_FILENAME, metaStream.toByteArray());
    }

    return files;
  }

  public static void metadataToCsv(Metadata metadata, OutputStream outputStream) throws IOException {
    CSVWriter writer = new CSVWriter(new OutputStreamWriter(outputStream, "UTF-8"));

    for (String name : metadata.names()) {
      String[] values = metadata.getValues(name);
      ArrayList<String> list = new ArrayList<String>(values.length+1);
      list.add(name);
      list.addAll(Arrays.asList(values));
      writer.writeNext(list.toArray(values));
    }

    writer.close();
  }

  private class MyEmbeddedDocumentExtractor implements EmbeddedDocumentExtractor {
    private final MutableInt count;
    private final Map<String, byte[]> zout;

    MyEmbeddedDocumentExtractor(MutableInt count, Map<String, byte[]> zout) {
      this.count = count;
      this.zout = zout;
    }

    public boolean shouldParseEmbedded(Metadata metadata) {
      return true;
    }

    public void parseEmbedded(InputStream inputStream, ContentHandler contentHandler, Metadata metadata, boolean b) throws SAXException, IOException {
      ByteArrayOutputStream bos = new ByteArrayOutputStream();
      IOUtils.copy(inputStream, bos);
      byte[] data = bos.toByteArray();

      String name = metadata.get(TikaMetadataKeys.RESOURCE_NAME_KEY);
      String contentType = metadata.get(org.apache.tika.metadata.HttpHeaders.CONTENT_TYPE);

      if (name == null) {
        name = Integer.toString(count.intValue());
      }

      if (!name.contains(".") && contentType!=null) {
        try {
          String ext = tikaConfig.getMimeRepository().forName(contentType).getExtension();

          if (ext!=null) {
            name += ext;
          }
        } catch (MimeTypeException e) {
          logger.warn("Unexpected MimeTypeException", e);
        }
      }

      if ("application/vnd.openxmlformats-officedocument.oleObject".equals(contentType)) {
        POIFSFileSystem poifs = new POIFSFileSystem(new ByteArrayInputStream(data));
        OfficeParser.POIFSDocumentType type = OfficeParser.POIFSDocumentType.detectType(poifs);

        if (type == OfficeParser.POIFSDocumentType.OLE10_NATIVE) {
          try {
            Ole10Native ole = Ole10Native.createFromEmbeddedOleObject(poifs);
            if (ole.getDataSize()>0) {
              String label = ole.getLabel();

              if (label.startsWith("ole-")) {
                label = Integer.toString(count.intValue()) + '-' + label;
              }

              name = label;

              data = ole.getDataBuffer();
            }
          } catch (Ole10NativeException ex) {
            logger.warn("Skipping invalid part", ex);
          }
        } else {
          name += '.' + type.getExtension();
        }
      }

      final String finalName = name;

      if (data.length > 0) {
        zout.put(finalName, data);

        count.increment();
      } else {
        if (inputStream instanceof TikaInputStream) {
          TikaInputStream tin = (TikaInputStream)  inputStream;

          if (tin.getOpenContainer()!=null && tin.getOpenContainer() instanceof DirectoryEntry) {
            POIFSFileSystem fs = new POIFSFileSystem();
            copy((DirectoryEntry) tin.getOpenContainer(), fs.getRoot());
            ByteArrayOutputStream bos2 = new ByteArrayOutputStream();
            fs.writeFilesystem(bos2);
            bos2.close();

            zout.put(finalName, bos2.toByteArray());
          }
        }
      }
    }

    protected void copy(DirectoryEntry sourceDir, DirectoryEntry destDir)
            throws IOException {
      for (Entry entry : sourceDir) {
        if (entry instanceof DirectoryEntry) {
          // Need to recurse
          DirectoryEntry newDir = destDir.createDirectory(entry.getName());
          copy((DirectoryEntry) entry, newDir);
        } else {
          // Copy entry
          InputStream contents = new DocumentInputStream((DocumentEntry) entry);
          try {
            destDir.createDocument(entry.getName(), contents);
          } finally {
            contents.close();
          }
        }
      }
    }
  }
}
"
tika-server/src/main/java/org/apache/tika/server/XMPMessageBodyWriter.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.server;

import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.xmp.XMPMetadata;

import javax.ws.rs.Produces;
import javax.ws.rs.WebApplicationException;
import javax.ws.rs.core.MediaType;
import javax.ws.rs.core.MultivaluedMap;
import javax.ws.rs.ext.MessageBodyWriter;
import javax.ws.rs.ext.Provider;
import java.io.IOException;
import java.io.OutputStream;
import java.io.OutputStreamWriter;
import java.io.Writer;
import java.lang.annotation.Annotation;
import java.lang.reflect.Type;

@Provider
@Produces("application/rdf+xml")
public class XMPMessageBodyWriter implements MessageBodyWriter<Metadata> {

    private static MediaType RDF_XML = MediaType.valueOf("application/rdf+xml");

    public boolean isWriteable(Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType) {
        return mediaType.equals(RDF_XML) && Metadata.class.isAssignableFrom(type);
    }

    public long getSize(Metadata data, Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType) {
        return -1;
    }

    @Override
    public void writeTo(Metadata metadata, Class<?> type, Type genericType, Annotation[] annotations,
          MediaType mediaType, MultivaluedMap<String, Object> httpHeaders, OutputStream entityStream) throws IOException,
        WebApplicationException {
            try {
                Writer writer = new OutputStreamWriter(entityStream, "UTF-8");
                XMPMetadata xmp = new XMPMetadata(metadata);
                writer.write(xmp.toString());
                writer.flush();
            } catch (TikaException e) {
                throw new IOException(e);
            }
            entityStream.flush();
    }
}
"
tika-server/src/main/java/org/apache/tika/server/ZipWriter.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.server;

import org.apache.commons.compress.archivers.zip.ZipArchiveEntry;
import org.apache.commons.compress.archivers.zip.ZipArchiveOutputStream;

import javax.ws.rs.Produces;
import javax.ws.rs.WebApplicationException;
import javax.ws.rs.core.MediaType;
import javax.ws.rs.core.MultivaluedMap;
import javax.ws.rs.ext.MessageBodyWriter;
import javax.ws.rs.ext.Provider;
import java.io.IOException;
import java.io.OutputStream;
import java.lang.annotation.Annotation;
import java.lang.reflect.Type;
import java.util.Map;
import java.util.UUID;
import java.util.zip.CRC32;
import java.util.zip.ZipEntry;
import java.util.zip.ZipException;
import java.util.zip.ZipOutputStream;

@Provider
@Produces("application/zip")
public class ZipWriter implements MessageBodyWriter<Map<String, byte[]>> {
  private static void zipStoreBuffer(ZipArchiveOutputStream zip, String name, byte[] dataBuffer) throws IOException {
    ZipEntry zipEntry = new ZipEntry(name!=null?name: UUID.randomUUID().toString());
    zipEntry.setMethod(ZipOutputStream.STORED);

    zipEntry.setSize(dataBuffer.length);
    CRC32 crc32 = new CRC32();
    crc32.update(dataBuffer);
    zipEntry.setCrc(crc32.getValue());

    try {
      zip.putArchiveEntry(new ZipArchiveEntry(zipEntry));
    } catch (ZipException ex) {
      if (name!=null) {
        zipStoreBuffer(zip, "x-"+name, dataBuffer);
        return;
      }
    }

    zip.write(dataBuffer);

    zip.closeArchiveEntry();
  }

  public boolean isWriteable(Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType) {
    return Map.class.isAssignableFrom(type);
  }

  public long getSize(Map<String, byte[]> stringMap, Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType) {
    return -1;
  }

  public void writeTo(Map<String, byte[]> parts, Class<?> type, Type genericType, Annotation[] annotations, MediaType mediaType, MultivaluedMap<String, Object> httpHeaders, OutputStream entityStream) throws IOException, WebApplicationException {
    ZipArchiveOutputStream zip = new ZipArchiveOutputStream(entityStream);

    zip.setMethod(ZipArchiveOutputStream.STORED);

    for (Map.Entry<String, byte[]> entry : parts.entrySet()) {
      zipStoreBuffer(zip, entry.getKey(), entry.getValue());
    }

    zip.close();
  }
}
"
tika-translate/src/main/java/org/apache/tika/language/translate/CachedTranslator.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.language.translate;

import com.fasterxml.jackson.databind.util.LRUMap;
import org.apache.tika.language.LanguageIdentifier;
import org.apache.tika.language.LanguageProfile;

import java.util.HashMap;

/**
 * CachedTranslator. Saves a map of previous translations in order to prevent repetitive translation requests.
 */
public class CachedTranslator implements Translator {
    private static final int INITIAL_ENTRIES = 100;
    private static final int MAX_ENTRIES = 1000;
    private Translator translator;
    // The cache is a map from sourceLang:targetLang to an LRUMap of previously translated pairs.
    // Old entries are removed from the cache when it reaches its limit.
    // For example, {en:fr -> {hello -> salut}}.
    private HashMap<String, LRUMap<String, String>> cache;

    /**
     * Create a new CachedTranslator.
     *
     * @param translator The translator that should be used for the underlying translation service. The properties
     *                   for that service must be set properly!
     */
    public CachedTranslator(Translator translator) {
        this.translator = translator;
        cache = new HashMap<String, LRUMap<String, String>>();
    }

    @Override
    public String translate(String text, String sourceLanguage, String targetLanguage) throws Exception {
        HashMap<String, String> translationCache = getTranslationCache(sourceLanguage, targetLanguage);
        String translatedText = translationCache.get(text);
        if (translatedText == null) {
            translatedText = translator.translate(text, sourceLanguage, targetLanguage);
            translationCache.put(text, translatedText);
        }
        return translatedText;
    }

    @Override
    public String translate(String text, String targetLanguage) throws Exception {
        LanguageIdentifier language = new LanguageIdentifier(
                new LanguageProfile(text));
        String sourceLanguage = language.getLanguage();
        return translate(text, sourceLanguage, targetLanguage);
    }

    @Override
    public boolean isAvailable() {
        return translator.isAvailable();
    }

    /**
     * Get the number of different source/target translation pairs this CachedTranslator
     * currently has in its cache.
     *
     * @return Number of translation source/target pairs in this CachedTranslator's cache.
     * @since Tika 1.6
     */
    public int getNumTranslationPairs() {
        return cache.size();
    }

    /**
     * Get the number of different translations from the source language to the target language
     * this CachedTranslator has in its cache.
     *
     * @param sourceLanguage The source language of translation.
     * @param targetLanguage The target language of translation.
     * @return The number of translations between source and target.
     * @since Tika 1.6
     */
    public int getNumTranslationsFor(String sourceLanguage, String targetLanguage) {
        HashMap<String, String> translationCache = cache.get(buildCacheKeyString(sourceLanguage, targetLanguage));
        if (translationCache == null) return 0;
        else return translationCache.size();
    }

    /**
     * Check whether this CachedTranslator's cache contains a translation of the text from the
     * source language to the target language.
     *
     * @param text What string to check for.
     * @param sourceLanguage The source language of translation.
     * @param targetLanguage The target language of translation.
     * @return true if the cache contains a translation of the text, false otherwise.
     */
    public boolean contains(String text, String sourceLanguage, String targetLanguage) {
        HashMap<String, String> translationCache = getTranslationCache(sourceLanguage, targetLanguage);
        return translationCache.containsKey(text);
    }

    /**
     * Check whether this CachedTranslator's cache contains a translation of the text to the target language,
     * attempting to auto-detect the source language.
     *
     * @param text What string to check for.
     * @param targetLanguage The target language of translation.
     * @return true if the cache contains a translation of the text, false otherwise.
     */
    public boolean contains(String text, String targetLanguage) {
        LanguageIdentifier language = new LanguageIdentifier(
                new LanguageProfile(text));
        String sourceLanguage = language.getLanguage();
        return contains(text, sourceLanguage, targetLanguage);
    }

    /**
     * Build the String to be used as the key into this CachedTranslator's cache.
     *
     * @param sourceLanguage The source language of translation.
     * @param targetLanguage The target language of translation.
     * @return The string to be used as the key into this CachedTranslator's cache.
     */
    private String buildCacheKeyString(String sourceLanguage, String targetLanguage) {
        return sourceLanguage + ":" + targetLanguage;
    }

    /**
     * Get the cache of translations from the given source language to target language.
     *
     * @param sourceLanguage The source language of translation.
     * @param targetLanguage The target language of translation.
     * @return The LRUMap representing the translation cache.
     */
    private LRUMap<String, String> getTranslationCache(String sourceLanguage, String targetLanguage) {
        LRUMap<String, String> translationCache = cache.get(buildCacheKeyString(sourceLanguage, targetLanguage));
        if (translationCache == null) {
            translationCache = new LRUMap<String, String>(INITIAL_ENTRIES, MAX_ENTRIES);
            cache.put(buildCacheKeyString(sourceLanguage, targetLanguage), translationCache);
        }
        return translationCache;
    }
}
"
tika-translate/src/main/java/org/apache/tika/language/translate/ExternalTranslator.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.language.translate;

import org.apache.tika.language.LanguageIdentifier;
import org.apache.tika.language.LanguageProfile;

import java.io.BufferedReader;
import java.io.File;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.Reader;
import java.nio.charset.Charset;
import java.util.Locale;

/**
 * Abstract class used to interact with command line/external Translators.
 *
 * @see org.apache.tika.language.translate.MosesTranslator for an example of extending this class.
 *
 * @since Tika 1.7
 */
public abstract class ExternalTranslator implements Translator {

    /**
     * Run the given command and return the output written to standard out.
     *
     * @param command The complete command to run.
     * @param env The environment to pass along to the Runtime.
     * @param workingDirectory The directory from which to run the command.
     * @return The output of the command written to standard out.
     * @throws IOException
     * @throws InterruptedException
     */
    public Reader runAndGetOutput(String command, String[] env, File workingDirectory) throws IOException, InterruptedException {
        Process process = Runtime.getRuntime().exec(command, env, workingDirectory);
        InputStreamReader reader = new InputStreamReader(process.getInputStream(), Charset.defaultCharset());
        BufferedReader bufferedReader = new BufferedReader(reader);
        process.waitFor();
        return bufferedReader;
    }

    /**
     * Checks to see if the command can be run. Typically used with
     *  something like "myapp --version" to check to see if "myapp"
     *  is installed and on the path.
     *
     * @param checkCommandString The command to run and check the return code of.
     * @param successCodes Return codes that signify success.
     */
    public boolean checkCommand(String checkCommandString, int... successCodes) {
        try {
            Process process = Runtime.getRuntime().exec(checkCommandString);
            process.waitFor();
            int result = process.waitFor();
            for (int code : successCodes) {
                if (code == result) return true;
            }
            return false;
        } catch(IOException e) {
            // Some problem, command is there or is broken
            System.err.println("Broken pipe");
            return false;
        } catch (InterruptedException ie) {
            // Some problem, command is there or is broken
            System.err.println("Interrupted");
            return false;
        }
    }

    /**
     * Default translate method which uses built Tika language identification.
     * @param text The text to translate.
     * @param targetLanguage The desired language to translate to (for example, "hi").
     * @return The translated text.
     * @throws Exception
     */
    @Override
    public String translate(String text, String targetLanguage) throws Exception {
        LanguageIdentifier language = new LanguageIdentifier(
                new LanguageProfile(text));
        String sourceLanguage = language.getLanguage();
        return translate(text, sourceLanguage, targetLanguage);
    }
}
"
tika-translate/src/main/java/org/apache/tika/language/translate/GoogleTranslator.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.language.translate;

import java.io.BufferedReader;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.util.Properties;
import java.util.logging.Logger;

import javax.ws.rs.core.MediaType;
import javax.ws.rs.core.Response;

import org.apache.cxf.jaxrs.client.WebClient;
import org.apache.tika.language.LanguageIdentifier;
import org.apache.tika.language.LanguageProfile;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;

/**
 * An implementation of a REST client to the <a
 * href="https://www.googleapis.com/language/translate/v2">Google Translate v2
 * API</a>. Based on the <a
 * href="http://hayageek.com/google-translate-api-tutorial/">great tutorial</a>
 * from <a href="http://hayageek.com">hayageek.com</a>. Set your API key in
 * translator.google.properties.
 * 
 * 
 */
public class GoogleTranslator implements Translator {

	private static final String GOOGLE_TRANSLATE_URL_BASE = "https://www.googleapis.com/language/translate/v2";

	private static final String DEFAULT_KEY = "dummy-secret";

	private static final Logger LOG = Logger.getLogger(GoogleTranslator.class
			.getName());

	private WebClient client;

	private String apiKey;

	private boolean isAvailable;

	public GoogleTranslator() {
		this.client = WebClient.create(GOOGLE_TRANSLATE_URL_BASE);
		this.isAvailable = true;
		Properties config = new Properties();
		try {
			config.load(GoogleTranslator.class
					.getClassLoader()
					.getResourceAsStream(
							"org/apache/tika/language/translate/translator.google.properties"));
			this.apiKey = config.getProperty("translator.client-secret");
			if (this.apiKey.equals(DEFAULT_KEY))
				this.isAvailable = false;
		} catch (Exception e) {
			e.printStackTrace();
			isAvailable = false;
		}
	}

	@Override
	public String translate(String text, String sourceLanguage,
			String targetLanguage) throws Exception {
		if (!this.isAvailable)
			return text;
		Response response = client.accept(MediaType.APPLICATION_JSON)
				.query("key", apiKey).query("source", sourceLanguage)
				.query("target", targetLanguage).query("q", text).get();
		BufferedReader reader = new BufferedReader(new InputStreamReader(
				(InputStream) response.getEntity(), "UTF-8"));
		String line = null;
		StringBuffer responseText = new StringBuffer();
		while ((line = reader.readLine()) != null) {
			responseText.append(line);
		}

		ObjectMapper mapper = new ObjectMapper();
		JsonNode jsonResp = mapper.readTree(responseText.toString());
		return jsonResp.findValuesAsText("translatedText").get(0);
	}

	@Override
	public String translate(String text, String targetLanguage)
			throws Exception {
		if (!this.isAvailable)
			return text;
		LanguageIdentifier language = new LanguageIdentifier(
				new LanguageProfile(text));
		String sourceLanguage = language.getLanguage();
		return translate(text, sourceLanguage, targetLanguage);
	}

	@Override
	public boolean isAvailable() {
		return this.isAvailable;
	}

}
"
tika-translate/src/main/java/org/apache/tika/language/translate/Lingo24Translator.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.language.translate;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.apache.cxf.jaxrs.client.WebClient;
import org.apache.tika.exception.TikaException;
import org.apache.tika.language.LanguageIdentifier;
import org.apache.tika.language.LanguageProfile;

import javax.ws.rs.core.MediaType;
import javax.ws.rs.core.Response;
import java.io.BufferedReader;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.util.Properties;

/**
 * An implementation of a REST client for the
 * <a href="https://developer.lingo24.com/premium-machine-translation-api">Premium MT API v1</a>.
 * You can sign up for an access plan online on the <a href="https://developer.lingo24.com/plans">Lingo24 Developer Portal</a>
 * and set your Application's User Key in the <code>translator.lingo24.properties</code> file.
 */
public class Lingo24Translator implements Translator {

    private static final String LINGO24_TRANSLATE_URL_BASE = "https://api.lingo24.com/mt/v1/translate";

    private static final String DEFAULT_KEY = "dummy-key";

    private WebClient client;

    private String userKey;

    private boolean isAvailable;

    public Lingo24Translator() {
        this.client = WebClient.create(LINGO24_TRANSLATE_URL_BASE);
        this.isAvailable = true;
        Properties config = new Properties();
        try {
            config.load(Lingo24Translator.class
                    .getClassLoader()
                    .getResourceAsStream(
                            "org/apache/tika/language/translate/translator.lingo24.properties"));
            this.userKey = config.getProperty("translator.user-key");
            if (this.userKey.equals(DEFAULT_KEY))
                this.isAvailable = false;
        } catch (Exception e) {
            e.printStackTrace();
            isAvailable = false;
        }
    }

    @Override
    public String translate(String text, String sourceLanguage,
                            String targetLanguage) throws Exception {
        if (!this.isAvailable)
            return text;
        Response response = client.accept(MediaType.APPLICATION_JSON)
                .query("user_key", userKey).query("source", sourceLanguage)
                .query("target", targetLanguage).query("q", text).get();
        BufferedReader reader = new BufferedReader(new InputStreamReader(
                (InputStream) response.getEntity(), "UTF-8"));
        String line = null;
        StringBuffer responseText = new StringBuffer();
        while ((line = reader.readLine()) != null) {
            responseText.append(line);
        }

        ObjectMapper mapper = new ObjectMapper();
        JsonNode jsonResp = mapper.readTree(responseText.toString());
        if (jsonResp.findValuesAsText("errors").isEmpty()) {
            return jsonResp.findValuesAsText("translation").get(0);
        } else {
            throw new TikaException(jsonResp.findValue("errors").get(0).asText());
        }
    }

    @Override
    public String translate(String text, String targetLanguage)
            throws Exception {
        if (!this.isAvailable)
            return text;
        LanguageIdentifier language = new LanguageIdentifier(
                new LanguageProfile(text));
        String sourceLanguage = language.getLanguage();
        return translate(text, sourceLanguage, targetLanguage);
    }

    @Override
    public boolean isAvailable() {
        return this.isAvailable;
    }

}
"
tika-translate/src/main/java/org/apache/tika/language/translate/MicrosoftTranslator.java,true,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.language.translate;

import com.memetix.mst.language.Language;
import com.memetix.mst.translate.Translate;

import java.io.IOException;
import java.io.InputStream;
import java.util.Properties;

/**
 * Wrapper class to access the Windows translation service. This class uses the com.memetix.mst
 * package as a wrapper for the API calls.
 * @since Tika 1.6
 */
public class MicrosoftTranslator implements Translator {

    boolean available;              // Flag for whether or not translation is available.
    String clientId, clientSecret;  // Keys used for the API calls.

    public static final String PROPERTIES_FILE = "org/apache/tika/language/translate/translator.microsoft.properties";
    public static final String ID_PROPERTY = "translator.client-id";
    public static final String SECRET_PROPERTY = "translator.client-secret";
    public static final String DEFAULT_ID = "dummy-id";
    public static final String DEFAULT_SECRET = "dummy-secret";

    /**
     * Create a new MicrosoftTranslator with the client keys specified in
     * resources/org/apache/tika/language/translate/translator.microsoft.properties. Silently becomes unavailable
     * when client keys are unavailable. translator.microsoft.client-id and translator.client-secret must be set
     * in translator.microsoft.properties for translation to work.
     * @since Tika 1.6
     */
    public MicrosoftTranslator() {
        Properties props = new Properties();
        InputStream stream;
        stream = MicrosoftTranslator.class.getResourceAsStream(PROPERTIES_FILE);
        try {
            if(stream != null) {
                props.load(stream);
                clientId = props.getProperty(ID_PROPERTY);
                clientSecret = props.getProperty(SECRET_PROPERTY);
                this.available = checkAvailable();   
            }
        } catch (IOException e) {
        	e.printStackTrace();
            // Error with properties file. Translation will not work.
            available = false;
        }
    }

    /**
     * Use the Microsoft service to translate the given text from the given source language to the given target.
     * You must set the client keys in translator.microsoft.properties.
     *
     * @param text The text to translate.
     * @param sourceLanguage The input text language (for example, "en").
     * @param targetLanguage The desired language to translate to (for example, "fr").
     * @return The translated text. If translation is unavailable, returns the unchanged text.
     * @throws Exception
     * @see org.apache.tika.language.translate.Translator
     * @since Tika 1.6
     */
    public String translate(String text, String sourceLanguage, String targetLanguage) throws Exception {
        if (!available) return text;
        Language source = Language.fromString(sourceLanguage);
        Language target = Language.fromString(targetLanguage);
        Translate.setClientId(clientId);
        Translate.setClientSecret(clientSecret);
        return Translate.execute(text, source, target);
    }

    /**
     * Use the Microsoft service to translate the given text to the given target language. The source language
     * is automatically detected by Microsoft. You must set the client keys in translator.microsoft.properties.
     * @param text The text to translate.
     * @param targetLanguage The desired language to translate to (for example, "hi").
     * @return The translated text. If translation is unavailable, returns the unchanged text.
     * @throws Exception
     * @see org.apache.tika.language.translate.Translator
     * @since Tika 1.6
     */
    public String translate(String text, String targetLanguage) throws Exception {
        if (!available) return text;
        Language target = Language.fromString(targetLanguage);
        Translate.setClientId(clientId);
        Translate.setClientSecret(clientSecret);
        return Translate.execute(text, target);
    }

    /**
     * Check whether this instance has a working property file and its keys are not the defaults.
     * This is not guaranteed to work, since keys may be incorrect or the webservice may be down.
     * @return whether translation will probably work.
     */
    public boolean isAvailable(){
        return available;
    }
    
    /**
     * Sets the client Id for the translator API.
     * @param id The ID to set.
     */
    public void setId(String id){
    	this.clientId = id;
        this.available = checkAvailable();   
    }
    
    /**
     * Sets the client secret for the translator API.
     * @param secret The secret to set.
     */
    public void setSecret(String secret){
    	this.clientSecret = secret;
        this.available = checkAvailable();   	
    }
    
    private boolean checkAvailable(){
       return clientId != null && 
    		   !clientId.equals(DEFAULT_ID) && 
    		   clientSecret != null && 
    		   !clientSecret.equals(DEFAULT_SECRET);
    }
}
"
tika-translate/src/main/java/org/apache/tika/language/translate/MosesTranslator.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.tika.language.translate;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.OutputStreamWriter;
import java.nio.charset.Charset;
import java.util.Properties;

/**
 * Translator that uses the Moses decoder for translation.
 * Users must install the Moses system before using this Translator. @link http://www.statmt.org/moses/.
 */
public class MosesTranslator extends ExternalTranslator {

    private static final String DEFAULT_PATH = "dummy-path";
    private static final String TMP_FILE_NAME = "tika.moses.translation.tmp";

    private String smtPath = DEFAULT_PATH;
    private String scriptPath = DEFAULT_PATH;

    /**
     * Default constructor that attempts to read the smt jar and script paths from the
     * translator.moses.properties file.
     *
     * @throws java.lang.AssertionError When the properties file is unreadable.
     */
    public MosesTranslator() {
        Properties config = new Properties();
        try {
            config.load(MosesTranslator.class
                    .getClassLoader()
                    .getResourceAsStream("org/apache/tika/language/translate/translator.moses.properties"));
            new MosesTranslator(
                    config.getProperty("translator.smt_path"),
                    config.getProperty("translator.script_path"));
        } catch (IOException e) {
            throw new AssertionError("Failed to read translator.moses.properties.");
        }
    }

    /**
     * Create a Moses Translator with the specified smt jar and script paths.
     *
     * @param smtPath Full path to the jar to run.
     * @param scriptPath Full path to the script to pass to the smt jar.
     */
    public MosesTranslator(String smtPath, String scriptPath) {
        this.smtPath = smtPath;
        this.scriptPath = scriptPath;
        System.out.println(buildCommand(smtPath, scriptPath));
    }

    @Override
    public String translate(String text, String sourceLanguage, String targetLanguage) throws Exception {
        if (!isAvailable() || !checkCommand(buildCheckCommand(smtPath), 1)) return text;
        File tmpFile = new File(TMP_FILE_NAME);
        OutputStreamWriter out = new OutputStreamWriter(new FileOutputStream(tmpFile), Charset.defaultCharset());
        out.append(text).append('\n').close();

        Runtime.getRuntime().exec(buildCommand(smtPath, scriptPath), new String[]{}, buildWorkingDirectory(scriptPath));

        File tmpTranslatedFile = new File(TMP_FILE_NAME + ".translated");

        StringBuilder stringBuilder = new StringBuilder();
        BufferedReader reader = new BufferedReader(new InputStreamReader(
                new FileInputStream(tmpTranslatedFile),
                Charset.defaultCharset()
        ));
        String line;
        while ((line = reader.readLine()) != null) stringBuilder.append(line);

        if (!tmpFile.delete() || !tmpTranslatedFile.delete()){
            throw new IOException("Failed to delete temporary files.");
        }
        return stringBuilder.toString();
    }

    @Override
    public boolean isAvailable() {
        return !smtPath.equals(DEFAULT_PATH) && !scriptPath.equals(DEFAULT_PATH);
    }

    /**
     * Build the command String to be executed.
     * @param smtPath Full path to the jar to run.
     * @param scriptPath Full path to the script to pass to the smt jar.
     * @return String to run on the command line.
     */
    private String buildCommand(String smtPath, String scriptPath) {
        return "java -jar " + smtPath +
                " -c NONE " +
                scriptPath + " " +
                System.getProperty("user.dir") + "/" + TMP_FILE_NAME;
    }

    /**
     * Build the command String to check if we can execute the smt jar.
     * @param smtPath Full path to the jar to run.
     * @return String to run on the command line.
     */
    private String buildCheckCommand(String smtPath) {
        return "java -jar " + smtPath;
    }

    /**
     * Build the File that represents the desired working directory. In this case,
     * the directory the script is in.
     * @param scriptPath Full path to the script passed to the smt jar.
     * @return File of the directory with the script in it.
     */
    private File buildWorkingDirectory(String scriptPath) {
        return new File(scriptPath.substring(0, scriptPath.lastIndexOf("/") + 1));
    }

}
"
tika-xmp/src/main/java/org/apache/tika/xmp/XMPMetadata.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.xmp;

import java.io.IOException;
import java.io.NotSerializableException;
import java.io.ObjectInputStream;
import java.io.ObjectOutputStream;
import java.util.Calendar;
import java.util.Date;
import java.util.Enumeration;
import java.util.Map;
import java.util.Properties;

import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.Property;
import org.apache.tika.metadata.Property.PropertyType;
import org.apache.tika.metadata.PropertyTypeException;
import org.apache.tika.xmp.convert.TikaToXMP;

import com.adobe.xmp.XMPDateTime;
import com.adobe.xmp.XMPException;
import com.adobe.xmp.XMPIterator;
import com.adobe.xmp.XMPMeta;
import com.adobe.xmp.XMPMetaFactory;
import com.adobe.xmp.XMPSchemaRegistry;
import com.adobe.xmp.XMPUtils;
import com.adobe.xmp.options.IteratorOptions;
import com.adobe.xmp.options.PropertyOptions;
import com.adobe.xmp.options.SerializeOptions;
import com.adobe.xmp.properties.XMPProperty;

/**
 * Provides a conversion of the Metadata map from Tika to the XMP data model by also providing the
 * Metadata API for clients to ease transition. But clients can also work directly on the XMP data
 * model, by getting the XMPMeta reference from this class. Usually the instance would be
 * initialized by providing the Metadata object that had been returned from Tika-core which
 * populates the XMP data model with all properties that can be converted.
 *
 * This class is not serializable!
 */
@SuppressWarnings("serial")
public class XMPMetadata extends Metadata {
    /** The XMP data */
    private XMPMeta xmpData;
    /** Use the XMP namespace registry implementation */
    private static final XMPSchemaRegistry registry = XMPMetaFactory.getSchemaRegistry();

    /**
     * Initializes with an empty XMP packet
     */
    public XMPMetadata() {
        xmpData = XMPMetaFactory.create();
    }

    /**
     * @see #XMPMetadata(Metadata, String)
     * But the mimetype is retrieved from the metadata map.
     */
    public XMPMetadata(Metadata meta) throws TikaException {
        this.xmpData = TikaToXMP.convert( meta );
    }

    /**
     * Initializes the data by converting the Metadata information to XMP. If a mimetype is
     * provided, a specific converter can be used, that converts all available metadata. If there is
     * no mimetype provided or no specific converter available a generic conversion is done which
     * will convert only those properties that are in known namespaces and are using the correct
     * prefixes
     *
     * @param meta
     *            the Metadata information from Tika-core
     * @param mimetype
     *            mimetype information
     * @throws In
     *             case an error occured during conversion
     */
    public XMPMetadata(Metadata meta, String mimetype) throws TikaException {
        this.xmpData = TikaToXMP.convert( meta, mimetype );
    }

    /**
     * @see org.apache.tika.xmp.XMPMetadata#process(org.apache.tika.metadata.Metadata,
     *      java.lang.String)
     *  But the mimetype is retrieved from the metadata map.
     */
    public void process(Metadata meta) throws TikaException {
        this.xmpData = TikaToXMP.convert( meta );
    }

    /**
     * Converts the Metadata information to XMP. If a mimetype is provided, a specific converter can
     * be used, that converts all available metadata. If there is no mimetype provided or no
     * specific converter available a generic conversion is done which will convert only those
     * properties that are in known namespaces and are using the correct prefixes
     *
     * @param meta
     *            the Metadata information from Tika-core
     * @param mimetype
     *            mimetype information
     * @throws In
     *             case an error occured during conversion
     */
    public void process(Metadata meta, String mimetype) throws TikaException {
        this.xmpData = TikaToXMP.convert( meta, mimetype );
    }

    /**
     * Provides direct access to the XMP data model, in case a client prefers to work directly on it
     * instead of using the Metadata API
     *
     * @return the "internal" XMP data object
     */
    public XMPMeta getXMPData() {
        return xmpData;
    }

    // === Namespace Registry API === //
    /**
     * Register a namespace URI with a suggested prefix. It is not an error if the URI is already
     * registered, no matter what the prefix is. If the URI is not registered but the suggested
     * prefix is in use, a unique prefix is created from the suggested one. The actual registeed
     * prefix is always returned. The function result tells if the registered prefix is the
     * suggested one.
     * Note: No checking is presently done on either the URI or the prefix.
     *
     * @param namespaceURI
     *            The URI for the namespace. Must be a valid XML URI.
     * @param suggestedPrefix
     *            The suggested prefix to be used if the URI is not yet registered. Must be a valid
     *            XML name.
     * @return Returns the registered prefix for this URI, is equal to the suggestedPrefix if the
     *         namespace hasn't been registered before, otherwise the existing prefix.
     * @throws XMPException
     *             If the parameters are not accordingly set
     */
    public static String registerNamespace(String namespaceURI, String suggestedPrefix)
            throws XMPException {
        return registry.registerNamespace( namespaceURI, suggestedPrefix );
    }

    /**
     * Obtain the prefix for a registered namespace URI.
     * It is not an error if the namespace URI is not registered.
     *
     * @param namespaceURI
     *            The URI for the namespace. Must not be null or the empty string.
     * @return Returns the prefix registered for this namespace URI or null.
     */
    public static String getNamespacePrefix(String namespaceURI) {
        return registry.getNamespacePrefix( namespaceURI );
    }

    /**
     * Obtain the URI for a registered namespace prefix.
     * It is not an error if the namespace prefix is not registered.
     *
     * @param namespacePrefix
     *            The prefix for the namespace. Must not be null or the empty string.
     * @return Returns the URI registered for this prefix or null.
     */
    public static String getNamespaceURI(String namespacePrefix) {
        return registry.getNamespaceURI( namespacePrefix );
    }

    /**
     * @return Returns the registered prefix/namespace-pairs as map, where the keys are the
     *         namespaces and the values are the prefixes.
     */
    @SuppressWarnings("unchecked")
    public static Map<String, String> getNamespaces() {
        return registry.getNamespaces();
    }

    /**
     * @return Returns the registered namespace/prefix-pairs as map, where the keys are the prefixes
     *         and the values are the namespaces.
     */
    @SuppressWarnings("unchecked")
    public static Map<String, String> getPrefixes() {
        return registry.getPrefixes();
    }

    /**
     * Deletes a namespace from the registry.
     * <p>
     * Does nothing if the URI is not registered, or if the namespaceURI parameter is null or the
     * empty string.
     * <p>
     * Note: Not yet implemented.
     *
     * @param namespaceURI
     *            The URI for the namespace.
     */
    public static void deleteNamespace(String namespaceURI) {
        registry.deleteNamespace( namespaceURI );
    }

    // === Metadata API === //
    /**
     * @see org.apache.tika.xmp.XMPMetadata#isMultiValued(java.lang.String)
     */
    @Override
    public boolean isMultiValued(Property property) {
        return this.isMultiValued( property.getName() );
    }

    /**
     * Checks if the named property is an array.
     *
     * @see org.apache.tika.metadata.Metadata#isMultiValued(java.lang.String)
     */
    @Override
    public boolean isMultiValued(String name) {
        checkKey( name );

        String[] keyParts = splitKey( name );

        String ns = registry.getNamespaceURI( keyParts[0] );
        if (ns != null) {
            try {
                XMPProperty prop = xmpData.getProperty( ns, keyParts[1] );

                return prop.getOptions().isArray();
            }
            catch (XMPException e) {
                // Ignore
            }
        }

        return false;
    }

    /**
     * For XMP it is not clear what that API should return, therefor not implemented
     */
    @Override
    public String[] names() {
        throw new UnsupportedOperationException( "Not implemented" );
    }

    /**
     * Returns the value of a simple property or the first one of an array. The given name must
     * contain a namespace prefix of a registered namespace.
     *
     * @see org.apache.tika.metadata.Metadata#get(java.lang.String)
     */
    @Override
    public String get(String name) {
        checkKey( name );

        String value = null;
        String[] keyParts = splitKey( name );

        String ns = registry.getNamespaceURI( keyParts[0] );
        if (ns != null) {
            try {
                XMPProperty prop = xmpData.getProperty( ns, keyParts[1] );

                if (prop != null && prop.getOptions().isSimple()) {
                    value = prop.getValue();
                }
                else if (prop != null && prop.getOptions().isArray()) {
                    prop = xmpData.getArrayItem( ns, keyParts[1], 1 );
                    value = prop.getValue();
                }
                // in all other cases, null is returned
            }
            catch (XMPException e) {
                // Ignore
            }
        }

        return value;
    }

    /**
     * @see org.apache.tika.xmp.XMPMetadata#get(java.lang.String)
     */
    @Override
    public String get(Property property) {
        return this.get( property.getName() );
    }

    /**
     * @see org.apache.tika.xmp.XMPMetadata#get(java.lang.String)
     */
    @Override
    public Integer getInt(Property property) {
        Integer result = null;

        try {
            result = new Integer( XMPUtils.convertToInteger( this.get( property.getName() ) ) );
        }
        catch (XMPException e) {
            // Ignore
        }

        return result;
    }

    /**
     * @see org.apache.tika.xmp.XMPMetadata#get(java.lang.String)
     */
    @Override
    public Date getDate(Property property) {
        Date result = null;

        try {
            XMPDateTime xmpDate = XMPUtils.convertToDate( this.get( property.getName() ) );
            if (xmpDate != null) {
                Calendar cal = xmpDate.getCalendar();
                // TODO Timezone is currently lost
                // need another solution that preserves the timezone
                result = cal.getTime();
            }
        }
        catch (XMPException e) {
            // Ignore
        }

        return result;
    }

    /**
     * @see org.apache.tika.xmp.XMPMetadata#getValues(java.lang.String)
     */
    @Override
    public String[] getValues(Property property) {
        return this.getValues( property.getName() );
    }

    /**
     * Returns the value of a simple property or all if the property is an array and the elements
     * are of simple type. The given name must contain a namespace prefix of a registered namespace.
     *
     * @see org.apache.tika.metadata.Metadata#getValues(java.lang.String)
     */
    @Override
    public String[] getValues(String name) {
        checkKey( name );

        String[] value = null;
        String[] keyParts = splitKey( name );

        String ns = registry.getNamespaceURI( keyParts[0] );
        if (ns != null) {
            try {
                XMPProperty prop = xmpData.getProperty( ns, keyParts[1] );

                if (prop != null && prop.getOptions().isSimple()) {
                    value = new String[1];
                    value[0] = prop.getValue();
                }
                else if (prop != null && prop.getOptions().isArray()) {
                    int size = xmpData.countArrayItems( ns, keyParts[1] );
                    value = new String[size];
                    boolean onlySimpleChildren = true;

                    for (int i = 0; i < size && onlySimpleChildren; i++) {
                        prop = xmpData.getArrayItem( ns, keyParts[1], i + 1 );
                        if (prop.getOptions().isSimple()) {
                            value[i] = prop.getValue();
                        }
                        else {
                            onlySimpleChildren = false;
                        }
                    }

                    if (!onlySimpleChildren) {
                        value = null;
                    }
                }
                // in all other cases, null is returned
            }
            catch (XMPException e) {
                // Ignore
            }
        }

        return value;
    }

    /**
     * As this API could only possibly work for simple properties in XMP, it just calls the set
     * method, which replaces any existing value
     *
     * @see org.apache.tika.metadata.Metadata#add(java.lang.String, java.lang.String)
     */
    @Override
    public void add(String name, String value) {
        set( name, value );
    }

    /**
     * Sets the given property. If the property already exists, it is overwritten. Only simple
     * properties that use a registered prefix are stored in the XMP.
     *
     * @see org.apache.tika.metadata.Metadata#set(java.lang.String, java.lang.String)
     */
    @Override
    public void set(String name, String value) {
        checkKey( name );

        String[] keyParts = splitKey( name );

        String ns = registry.getNamespaceURI( keyParts[0] );
        if (ns != null) {
            try {
                xmpData.setProperty( ns, keyParts[1], value );
            }
            catch (XMPException e) {
                // Ignore
            }
        }
    }

    /**
     * @see org.apache.tika.xmp.XMPMetadata#set(java.lang.String, java.lang.String)
     */
    @Override
    public void set(Property property, String value) {
        this.set( property.getName(), value );
    }

    /**
     * @see org.apache.tika.xmp.XMPMetadata#set(java.lang.String, java.lang.String)
     */
    @Override
    public void set(Property property, int value) {
        // Can reuse the checks from the base class implementation which will call
        // the set(String, String) method in the end
        super.set( property, value );
    }

    /**
     * @see org.apache.tika.xmp.XMPMetadata#set(java.lang.String, java.lang.String)
     */
    @Override
    public void set(Property property, double value) {
        super.set( property, value );
    }

    /**
     * @see org.apache.tika.xmp.XMPMetadata#set(java.lang.String, java.lang.String)
     */
    @Override
    public void set(Property property, Date date) {
        super.set( property, date );
    }

    /**
     * Sets array properties. If the property already exists, it is overwritten. Only array
     * properties that use a registered prefix are stored in the XMP.
     *
     * @see org.apache.tika.metadata.Metadata#set(org.apache.tika.metadata.Property,
     *      java.lang.String[])
     */
    @Override
    public void set(Property property, String[] values) {
        checkKey( property.getName() );

        if (!property.isMultiValuePermitted()) {
            throw new PropertyTypeException( "Property is not of an array type" );
        }

        String[] keyParts = splitKey( property.getName() );

        String ns = registry.getNamespaceURI( keyParts[0] );
        if (ns != null) {
            try {
                int arrayType = tikaToXMPArrayType( property.getPrimaryProperty().getPropertyType() );
                xmpData.setProperty( ns, keyParts[1], null, new PropertyOptions( arrayType ) );

                for (String value : values) {
                    xmpData.appendArrayItem( ns, keyParts[1], value );
                }
            }
            catch (XMPException e) {
                // Ignore
            }
        }
    }

    /**
     * It will set all simple and array properties that have QName keys in registered namespaces.
     *
     * @see org.apache.tika.metadata.Metadata#setAll(java.util.Properties)
     */
    @Override
    public void setAll(Properties properties) {
        @SuppressWarnings("unchecked")
        Enumeration<String> names = (Enumeration<String>) properties.propertyNames();

        while (names.hasMoreElements()) {
            String name = names.nextElement();
            Property property = Property.get( name );
            if (property == null) {
                throw new PropertyTypeException( "Unknown property: " + name );
            }

            String value = properties.getProperty( name );

            if (property.isMultiValuePermitted()) {
                this.set( property, new String[] { value } );
            }
            else {
                this.set( property, value );
            }
        }
    }

    /**
     * @see org.apache.tika.xmp.XMPMetadata#remove(java.lang.String)
     */
    public void remove(Property property) {
        this.remove( property.getName() );
    }

    /**
     * Removes the given property from the XMP data. If it is a complex property the whole subtree
     * is removed
     *
     * @see org.apache.tika.metadata.Metadata#remove(java.lang.String)
     */
    @Override
    public void remove(String name) {
        checkKey( name );

        String[] keyParts = splitKey( name );

        String ns = registry.getNamespaceURI( keyParts[0] );
        if (ns != null) {
            xmpData.deleteProperty( ns, keyParts[1] );
        }
    }

    /**
     * Returns the number of top-level namespaces
     */
    @Override
    public int size() {
        int size = 0;

        try {
            // Get an iterator for the XMP packet, starting at the top level schema nodes
            XMPIterator nsIter = xmpData.iterator( new IteratorOptions().setJustChildren( true )
                    .setOmitQualifiers( true ) );
            // iterate all top level namespaces
            while (nsIter.hasNext()) {
                nsIter.next();
                size++;
            }
        }
        catch (XMPException e) {
            // ignore
        }

        return size;
    }

    /**
     * This method is not implemented, yet. It is very tedious to check for semantic equality of XMP
     * packets
     */
    @Override
    public boolean equals(Object o) {
        throw new UnsupportedOperationException( "Not implemented" );
    }

    /**
     * Serializes the XMP data in compact form without packet wrapper
     *
     * @see org.apache.tika.metadata.Metadata#toString()
     */
    @Override
    public String toString() {
        String result = null;
        try {
            result = XMPMetaFactory.serializeToString( xmpData, new SerializeOptions()
                    .setOmitPacketWrapper( true ).setUseCompactFormat( true ) );
        }
        catch (XMPException e) {
            // ignore
        }
        return result;
    }

    // The XMP object is not serializable!
    private void readObject(ObjectInputStream ois) throws ClassNotFoundException, IOException {
        throw new NotSerializableException();
    }

    // The XMP object is not serializable!
    private void writeObject(ObjectOutputStream ois) throws IOException {
        throw new NotSerializableException();
    }

    /**
     * Checks if the given key is a valid QName with a known standard namespace prefix
     *
     * @param key
     *            the key to check
     * @return true if the key is valid otherwise false
     */
    private void checkKey(String key) throws PropertyTypeException {
        if (key == null || key.length() == 0) {
            throw new PropertyTypeException( "Key must not be null" );
        }

        String[] keyParts = splitKey( key );
        if (keyParts == null) {
            throw new PropertyTypeException( "Key must be a QName in the form prefix:localName" );
        }

        if (registry.getNamespaceURI( keyParts[0] ) == null) {
            throw new PropertyTypeException( "Key does not use a registered Namespace prefix" );
        }
    }

    /**
     * Split the given key at the namespace prefix delimiter
     *
     * @param key
     *            the key to split
     * @return prefix and local name of the property or null if the key did not contain a delimiter
     *         or too much of them
     */
    private String[] splitKey(String key) {
        String[] keyParts = key.split( Metadata.NAMESPACE_PREFIX_DELIMITER );
        if (keyParts.length > 0 && keyParts.length <= 2) {
            return keyParts;
        }

        return null;
    }// checkKeyPrefix

    /**
     * Convert Tika array types to XMP array types
     *
     * @param type
     * @return
     */
    private int tikaToXMPArrayType(PropertyType type) {
        int result = 0;
        switch (type) {
            case BAG:
                result = PropertyOptions.ARRAY;
                break;
            case SEQ:
                result = PropertyOptions.ARRAY_ORDERED;
                break;
            case ALT:
                result = PropertyOptions.ARRAY_ALTERNATE;
                break;
        }
        return result;
    }
}
"
tika-xmp/src/main/java/org/apache/tika/xmp/convert/AbstractConverter.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.xmp.convert;

import java.util.Set;

import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.Property;

import com.adobe.xmp.XMPConst;
import com.adobe.xmp.XMPException;
import com.adobe.xmp.XMPMeta;
import com.adobe.xmp.XMPMetaFactory;
import com.adobe.xmp.XMPSchemaRegistry;
import com.adobe.xmp.XMPUtils;
import com.adobe.xmp.options.PropertyOptions;

/**
 * Base class for Tika Metadata to XMP converter which provides some needed common functionality.
 */
public abstract class AbstractConverter implements ITikaToXMPConverter {
    private Metadata metadata;
    protected XMPMeta meta;

    abstract public XMPMeta process(Metadata metadata) throws XMPException;

    /**
     * Every Converter has to provide information about namespaces that are used additionally to the
     * core set of XMP namespaces.
     *
     * @return the additional namespace information
     */
    abstract protected Set<Namespace> getAdditionalNamespaces();

    public AbstractConverter() throws TikaException {
        meta = XMPMetaFactory.create();
        metadata = new Metadata();
        registerNamespaces( getAdditionalNamespaces() );
    }

    public void setMetadata(Metadata metadata) {
        this.metadata = metadata;
    }

    public XMPMeta getXMPMeta() {
        return meta;
    }

    // --- utility methods used by sub-classes ---

    /**
     * Registers a number <code>Namespace</code> information with XMPCore. Any already registered
     * namespace is not registered again.
     *
     * @param namespaces
     *            the list of namespaces to be registered
     * @throws TikaException
     *             in case a namespace oculd not be registered
     */
    protected void registerNamespaces(Set<Namespace> namespaces) throws TikaException {
        XMPSchemaRegistry registry = XMPMetaFactory.getSchemaRegistry();

        for (Namespace namespace : namespaces) {
            // Any already registered namespace is not registered again
            try {
                registry.registerNamespace( namespace.uri, namespace.prefix );
            }
            catch (XMPException e) {
                throw new TikaException(
                        "Namespace needed by converter could not be registiered with XMPCore", e );
            }
        }
    }

    /**
     * @see AbstractConverter#createProperty(String, String, String)
     */
    protected void createProperty(Property metadataProperty, String ns, String propertyName)
            throws XMPException {
        createProperty( metadataProperty.getName(), ns, propertyName );
    }

    /**
     * Creates a simple property.
     *
     * @param tikaKey
     *            Key in the Tika metadata map
     * @param ns
     *            namespace the property should be created in
     * @param propertyName
     *            name of the property
     * @throws XMPException
     *             if the property could not be created
     */
    protected void createProperty(String tikaKey, String ns, String propertyName)
            throws XMPException {
        String value = metadata.get( tikaKey );
        if (value != null && value.length() > 0) {
            meta.setProperty( ns, propertyName, value );
        }
    }

    /**
     * @see AbstractConverter#createLangAltProperty(String, String, String)
     */
    protected void createLangAltProperty(Property metadataProperty, String ns, String propertyName)
            throws XMPException {
        createLangAltProperty( metadataProperty.getName(), ns, propertyName );
    }

    /**
     * Creates a language alternative property in the x-default language
     *
     * @param tikaKey
     *            Key in the Tika metadata map
     * @param ns
     *            namespace the property should be created in
     * @param propertyName
     *            name of the property
     * @throws XMPException
     *             if the property could not be created
     */
    protected void createLangAltProperty(String tikaKey, String ns, String propertyName)
            throws XMPException {
        String value = metadata.get( tikaKey );
        if (value != null && value.length() > 0) {
            meta.setLocalizedText( ns, propertyName, null, XMPConst.X_DEFAULT, value );
        }
    }

    protected void createArrayProperty(Property metadataProperty, String nsDc,
            String arrayProperty, int arrayType) throws XMPException {
        createArrayProperty( metadataProperty.getName(), nsDc, arrayProperty, arrayType );
    }

    /**
     * Creates an array property from a list of values.
     *
     * @param tikaKey
     *            Key in the Tika metadata map
     * @param ns
     *            namespace the property should be created in
     * @param propertyName
     *            name of the property
     * @param arrayType
     *            depicts which kind of array shall be created
     * @throws XMPException
     *             if the property could not be created
     */
    protected void createArrayProperty(String tikaKey, String ns, String propertyName, int arrayType)
            throws XMPException {
        String[] values = metadata.getValues( tikaKey );
        if (values != null) {
            meta.setProperty( ns, propertyName, null, new PropertyOptions( arrayType ) );
            for (String value : values) {
                meta.appendArrayItem( ns, propertyName, value );
            }
        }
    }

    protected void createCommaSeparatedArray(Property metadataProperty, String nsDc,
            String arrayProperty, int arrayType) throws XMPException {
        createCommaSeparatedArray( metadataProperty.getName(), nsDc, arrayProperty, arrayType );
    }

    /**
     * Creates an array property from a comma separated list.
     *
     * @param tikaKey
     *            Key in the Tika metadata map
     * @param ns
     *            namespace the property should be created in
     * @param propertyName
     *            name of the property
     * @param arrayType
     *            depicts which kind of array shall be created
     * @throws XMPException
     *             if the property could not be created
     */
    protected void createCommaSeparatedArray(String tikaKey, String ns, String propertyName,
            int arrayType) throws XMPException {
        String value = metadata.get( tikaKey );
        if (value != null && value.length() > 0) {
            XMPUtils.separateArrayItems( meta, ns, propertyName, value, new PropertyOptions(
                    arrayType ), false );
        }
    }

}
"
tika-xmp/src/main/java/org/apache/tika/xmp/convert/GenericConverter.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.xmp.convert;

import java.util.Collections;
import java.util.HashSet;
import java.util.Set;

import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.DublinCore;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.Property;
import org.apache.tika.metadata.XMPRights;
import org.apache.tika.metadata.Property.PropertyType;

import com.adobe.xmp.XMPException;
import com.adobe.xmp.XMPMeta;
import com.adobe.xmp.XMPMetaFactory;
import com.adobe.xmp.XMPSchemaRegistry;
import com.adobe.xmp.options.PropertyOptions;

/**
 * Trys to convert as much of the properties in the <code>Metadata</code> map to XMP namespaces.
 * only those properties will be cnverted where the name contains a prefix and this prefix
 * correlates with a "known" prefix for a standard namespace. For example "dc:title" would be mapped
 * to the "title" property in the DublinCore namespace.
 */
public class GenericConverter extends AbstractConverter {
    public GenericConverter() throws TikaException {
        super();
    }

    @Override
    public XMPMeta process(Metadata metadata) throws XMPException {
        setMetadata( metadata );
        XMPSchemaRegistry registry = XMPMetaFactory.getSchemaRegistry();

        String[] keys = metadata.names();
        for (String key : keys) {
            String[] keyParts = key.split( Metadata.NAMESPACE_PREFIX_DELIMITER );
            if (keyParts.length > 0 && keyParts.length <= 2) {
                String uri = registry.getNamespaceURI( keyParts[0] );

                if (uri != null) {
                    // Tika properties where the type differs from the XMP specification
                    if (key.equals( DublinCore.TITLE.getName() )
                            || key.equals( DublinCore.DESCRIPTION.getName() )
                            || key.equals( XMPRights.USAGE_TERMS.getName() )) {
                        createLangAltProperty( key, uri, keyParts[1] );
                    }
                    else if (key.equals( DublinCore.CREATOR.getName() )) {
                        createArrayProperty( key, uri, keyParts[1], PropertyOptions.ARRAY_ORDERED );
                    }
                    else {
                        PropertyType type = Property.getPropertyType( key );
                        if (type != null) {
                            switch (type) {
                                case SIMPLE:
                                    createProperty( key, uri, keyParts[1] );
                                    break;
                                case BAG:
                                    createArrayProperty( key, uri, keyParts[1],
                                            PropertyOptions.ARRAY );
                                    break;
                                case SEQ:
                                    createArrayProperty( key, uri, keyParts[1],
                                            PropertyOptions.ARRAY_ORDERED );
                                    break;
                                case ALT:
                                    createArrayProperty( key, uri, keyParts[1],
                                            PropertyOptions.ARRAY_ALTERNATE );
                                    break;
                            // TODO Add support for structs and lang-alts, but those types are
                            // currently not used in Tika
                            }
                        }
                    }
                }
            } // ignore keys that are not qualified
        }

        return getXMPMeta();
    }

    @Override
    public Set<Namespace> getAdditionalNamespaces() {
        // no additional namespaces needed
        return Collections.unmodifiableSet( new HashSet<Namespace>() );
    }
}
"
tika-xmp/src/main/java/org/apache/tika/xmp/convert/ITikaToXMPConverter.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.xmp.convert;

import org.apache.tika.metadata.Metadata;

import com.adobe.xmp.XMPException;
import com.adobe.xmp.XMPMeta;

/**
 * Interface for the specific <code>Metadata</code> to XMP converters
 */
public interface ITikaToXMPConverter {
    /**
     * Converts a Tika {@link Metadata}-object into an {@link XMPMeta} containing the useful
     * properties.
     *
     * @param metadata
     *            a Tika Metadata object
     * @return Returns an XMPMeta object.
     * @throws XMPException
     *             If an error occurs during the creation of the XMP object.
     */
    XMPMeta process(Metadata metadata) throws XMPException;
}
"
tika-xmp/src/main/java/org/apache/tika/xmp/convert/MSOfficeBinaryConverter.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.xmp.convert;

import java.util.Arrays;
import java.util.Collections;
import java.util.HashSet;
import java.util.Set;

import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.HttpHeaders;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.Office;
import org.apache.tika.metadata.OfficeOpenXMLCore;
import org.apache.tika.metadata.OfficeOpenXMLExtended;
import org.apache.tika.metadata.TikaCoreProperties;

import com.adobe.xmp.XMPConst;
import com.adobe.xmp.XMPException;
import com.adobe.xmp.XMPMeta;
import com.adobe.xmp.options.PropertyOptions;

/**
 * Tika to XMP mapping for the binary MS formats Word (.doc), Excel (.xls) and PowerPoint (.ppt).
 */
public class MSOfficeBinaryConverter extends AbstractConverter {
    public MSOfficeBinaryConverter() throws TikaException {
        super();
    }

    protected static final Set<Namespace> ADDITIONAL_NAMESPACES = Collections
            .unmodifiableSet( new HashSet<Namespace>( Arrays.asList( new Namespace(
                    OfficeOpenXMLCore.NAMESPACE_URI, OfficeOpenXMLCore.PREFIX ), new Namespace(
                    OfficeOpenXMLExtended.NAMESPACE_URI, OfficeOpenXMLExtended.PREFIX ) ) ) );

    /**
     * @throws XMPException
     *             Forwards XMP errors
     * @see ITikaToXMPConverter#process(Metadata)
     */
    public XMPMeta process(Metadata metadata) throws XMPException {
        super.setMetadata( metadata );

        // For all formats, Tika uses the same keys
        createProperty( HttpHeaders.CONTENT_TYPE, XMPConst.NS_DC, "format" );
        createProperty( OfficeOpenXMLExtended.APPLICATION, XMPConst.NS_XMP, "CreatorTool" );
        createCommaSeparatedArray( TikaCoreProperties.CREATOR, XMPConst.NS_DC, "creator",
                PropertyOptions.ARRAY_ORDERED );
        createProperty( OfficeOpenXMLCore.CATEGORY, XMPConst.NS_IPTCCORE, "intellectualGenre" );
        createProperty( TikaCoreProperties.CREATED, XMPConst.NS_XMP, "CreateDate" );
        createProperty( Office.CHARACTER_COUNT, OfficeOpenXMLExtended.NAMESPACE_URI, "Characters" );
        createProperty( TikaCoreProperties.COMMENTS, XMPConst.NS_PDFX, "Comments" );
        createProperty( OfficeOpenXMLExtended.COMPANY, OfficeOpenXMLExtended.NAMESPACE_URI,
                "Company" );
        createCommaSeparatedArray( TikaCoreProperties.KEYWORDS, XMPConst.NS_DC, "subject",
                PropertyOptions.ARRAY );
        createLangAltProperty( TikaCoreProperties.DESCRIPTION, XMPConst.NS_DC, "description" );
        createProperty( TikaCoreProperties.LANGUAGE, OfficeOpenXMLCore.NAMESPACE_URI, "language" );
        createProperty( TikaCoreProperties.PRINT_DATE, OfficeOpenXMLCore.NAMESPACE_URI,
                "lastPrinted" );
        createProperty( TikaCoreProperties.MODIFIED, XMPConst.NS_XMP, "ModifyDate" );
        createProperty( Office.PAGE_COUNT, XMPConst.TYPE_PAGEDFILE, "NPages" );
        createProperty( OfficeOpenXMLCore.REVISION, OfficeOpenXMLCore.NAMESPACE_URI, "revision" );
        createProperty( Office.SLIDE_COUNT, OfficeOpenXMLExtended.NAMESPACE_URI, "Pages" );
        createProperty( OfficeOpenXMLExtended.TEMPLATE, OfficeOpenXMLExtended.NAMESPACE_URI,
                "Template" );
        createLangAltProperty( TikaCoreProperties.TITLE, XMPConst.NS_DC, "title" );
        createProperty( Office.WORD_COUNT, OfficeOpenXMLExtended.NAMESPACE_URI, "Words" );
        // Not mapped: (MSOffice) Edit-Time ???
        // Not mapped: (MSOffice) Last-Author ???
        // not mapped: (MSOffice) Security ???

        return super.getXMPMeta();
    }

    protected Set<Namespace> getAdditionalNamespaces() {
        return ADDITIONAL_NAMESPACES;
    }
}
"
tika-xmp/src/main/java/org/apache/tika/xmp/convert/MSOfficeXMLConverter.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.xmp.convert;

import java.util.Arrays;
import java.util.Collections;
import java.util.HashSet;
import java.util.Set;

import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.HttpHeaders;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.Office;
import org.apache.tika.metadata.OfficeOpenXMLCore;
import org.apache.tika.metadata.OfficeOpenXMLExtended;
import org.apache.tika.metadata.TikaCoreProperties;

import com.adobe.xmp.XMPConst;
import com.adobe.xmp.XMPException;
import com.adobe.xmp.XMPMeta;
import com.adobe.xmp.options.PropertyOptions;

/**
 * Tika to XMP mapping for the Office Open XML formats Word (.docx), Excel (.xlsx) and PowerPoint
 * (.pptx).
 */
public class MSOfficeXMLConverter extends AbstractConverter {
    protected static final Set<Namespace> ADDITIONAL_NAMESPACES = Collections
            .unmodifiableSet( new HashSet<Namespace>( Arrays.asList( new Namespace(
                    OfficeOpenXMLCore.NAMESPACE_URI, OfficeOpenXMLCore.PREFIX ), new Namespace(
                    OfficeOpenXMLExtended.NAMESPACE_URI, OfficeOpenXMLExtended.PREFIX ) ) ) );

    public MSOfficeXMLConverter() throws TikaException {
        super();
    }

    @Override
    public XMPMeta process(Metadata metadata) throws XMPException {
        super.setMetadata( metadata );

        createProperty( HttpHeaders.CONTENT_TYPE, XMPConst.NS_DC, "format" );

        // Core Properties
        createProperty( OfficeOpenXMLCore.CATEGORY, XMPConst.NS_IPTCCORE, "intellectualGenre" );
        createProperty( OfficeOpenXMLCore.CONTENT_STATUS, OfficeOpenXMLCore.NAMESPACE_URI,
                "contentStatus" );
        createProperty( TikaCoreProperties.CREATED, XMPConst.NS_XMP, "CreateDate" );
        createCommaSeparatedArray( TikaCoreProperties.CREATOR, XMPConst.NS_DC, "creator",
                PropertyOptions.ARRAY_ORDERED );
        createProperty( TikaCoreProperties.COMMENTS, XMPConst.NS_PDFX, "Comments" );
        createProperty( TikaCoreProperties.IDENTIFIER, XMPConst.NS_DC, "identifier" );
        createCommaSeparatedArray( TikaCoreProperties.KEYWORDS, XMPConst.NS_DC, "subject",
                PropertyOptions.ARRAY );
        createLangAltProperty( TikaCoreProperties.DESCRIPTION, XMPConst.NS_DC, "description" );
        createProperty( TikaCoreProperties.LANGUAGE, XMPConst.NS_DC, "language" );
        createProperty( TikaCoreProperties.MODIFIER, OfficeOpenXMLCore.NAMESPACE_URI,
                "lastModifiedBy" );
        createProperty( TikaCoreProperties.PRINT_DATE, OfficeOpenXMLCore.NAMESPACE_URI,
                "lastPrinted" );
        createProperty( TikaCoreProperties.MODIFIED, XMPConst.NS_XMP, "ModifyDate" );
        createProperty( OfficeOpenXMLCore.REVISION, OfficeOpenXMLCore.NAMESPACE_URI, "revision" );
        createLangAltProperty( TikaCoreProperties.TITLE, XMPConst.NS_DC, "title" );
        createProperty( OfficeOpenXMLCore.VERSION, OfficeOpenXMLCore.NAMESPACE_URI, "version" );

        // Extended Properties

        // Put both App name and version in xmp:CreatorTool
        String creatorTool = "";
        String value = metadata.get( OfficeOpenXMLExtended.APPLICATION );
        if (value != null && value.length() > 0) {
            creatorTool = value;

            value = metadata.get( OfficeOpenXMLExtended.APP_VERSION );
            if (value != null && value.length() > 0) {
                creatorTool += " " + value;
            }
        }

        if (creatorTool.length() > 0) {
            meta.setProperty( XMPConst.NS_XMP, "CreatorTool", creatorTool );
        }

        createProperty( Office.CHARACTER_COUNT, OfficeOpenXMLExtended.NAMESPACE_URI, "Characters" );
        createProperty( Office.CHARACTER_COUNT_WITH_SPACES, OfficeOpenXMLExtended.NAMESPACE_URI,
                "CharactersWithSpaces" );
        createProperty( TikaCoreProperties.PUBLISHER, OfficeOpenXMLExtended.NAMESPACE_URI,
                "Company" );
        createProperty( Office.LINE_COUNT, OfficeOpenXMLExtended.NAMESPACE_URI, "Lines" );
        createProperty( OfficeOpenXMLExtended.MANAGER, OfficeOpenXMLExtended.NAMESPACE_URI,
                "Manager" );
        createProperty( OfficeOpenXMLExtended.NOTES, OfficeOpenXMLExtended.NAMESPACE_URI, "Notes" );
        createProperty( Office.PAGE_COUNT, XMPConst.TYPE_PAGEDFILE, "NPages" );
        createProperty( Office.PARAGRAPH_COUNT, OfficeOpenXMLExtended.NAMESPACE_URI, "Paragraphs" );
        createProperty( OfficeOpenXMLExtended.PRESENTATION_FORMAT,
                OfficeOpenXMLExtended.NAMESPACE_URI, "PresentationFormat" );
        createProperty( Office.SLIDE_COUNT, OfficeOpenXMLExtended.NAMESPACE_URI, "Slides" );
        createProperty( OfficeOpenXMLExtended.TEMPLATE, OfficeOpenXMLExtended.NAMESPACE_URI,
                "Template" );
        createProperty( OfficeOpenXMLExtended.TOTAL_TIME, OfficeOpenXMLExtended.NAMESPACE_URI,
                "TotalTime" );
        createProperty( Office.WORD_COUNT, OfficeOpenXMLExtended.NAMESPACE_URI, "Words" );

        return super.getXMPMeta();
    }

    @Override
    protected Set<Namespace> getAdditionalNamespaces() {
        return ADDITIONAL_NAMESPACES;
    }

}
"
tika-xmp/src/main/java/org/apache/tika/xmp/convert/Namespace.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.xmp.convert;

/**
 * Utility class to hold namespace information.
 */
public class Namespace {
    public String uri;
    public String prefix;

    public Namespace(String uri, String prefix) {
        this.uri = uri;
        this.prefix = prefix;
    }
}
"
tika-xmp/src/main/java/org/apache/tika/xmp/convert/OpenDocumentConverter.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.xmp.convert;

import java.util.Arrays;
import java.util.Collections;
import java.util.HashSet;
import java.util.Set;

import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.HttpHeaders;
import org.apache.tika.metadata.MSOffice;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.Office;
import org.apache.tika.metadata.PagedText;
import org.apache.tika.metadata.TikaCoreProperties;

import com.adobe.xmp.XMPConst;
import com.adobe.xmp.XMPException;
import com.adobe.xmp.XMPMeta;
import com.adobe.xmp.options.PropertyOptions;

/**
 * Tika to XMP mapping for the Open Document formats: Text (.odt), Spreatsheet (.ods), Graphics
 * (.odg) and Presentation (.odp).
 */
public class OpenDocumentConverter extends AbstractConverter {
    protected static final Set<Namespace> ADDITIONAL_NAMESPACES = Collections
            .unmodifiableSet( new HashSet<Namespace>( Arrays.asList( new Namespace(
                    Office.NAMESPACE_URI_DOC_META, Office.PREFIX_DOC_META ) ) ) );

    public OpenDocumentConverter() throws TikaException {
        super();
    }

    /**
     * @throws XMPException
     *             Forwards XMP errors
     * @see ITikaToXMPConverter#process(Metadata)
     */
    @Override
    public XMPMeta process(Metadata metadata) throws XMPException {
        super.setMetadata( metadata );

        createProperty( HttpHeaders.CONTENT_TYPE, XMPConst.NS_DC, "format" );

        createProperty( Office.CHARACTER_COUNT, Office.NAMESPACE_URI_DOC_META, "character-count" );
        createProperty( TikaCoreProperties.CREATED, XMPConst.NS_XMP, "CreateDate" );
        createCommaSeparatedArray( TikaCoreProperties.CREATOR, XMPConst.NS_DC, "creator",
                PropertyOptions.ARRAY_ORDERED );
        createProperty( TikaCoreProperties.MODIFIED, XMPConst.NS_XMP, "ModifyDate" );
        createProperty( TikaCoreProperties.COMMENTS, XMPConst.NS_PDFX, "Comments" );
        createCommaSeparatedArray( TikaCoreProperties.KEYWORDS, XMPConst.NS_DC, "subject",
                PropertyOptions.ARRAY );
        createLangAltProperty( TikaCoreProperties.DESCRIPTION, XMPConst.NS_DC, "description" );
        createProperty( MSOffice.EDIT_TIME, Office.NAMESPACE_URI_DOC_META, "editing-duration" );
        createProperty( "editing-cycles", Office.NAMESPACE_URI_DOC_META, "editing-cycles" );
        createProperty( "generator", XMPConst.NS_XMP, "CreatorTool" );
        createProperty( Office.IMAGE_COUNT, Office.NAMESPACE_URI_DOC_META, "image-count" );
        createProperty( "initial-creator", Office.NAMESPACE_URI_DOC_META, "initial-creator" );
        createProperty( Office.OBJECT_COUNT, Office.NAMESPACE_URI_DOC_META, "object-count" );
        createProperty( PagedText.N_PAGES, XMPConst.TYPE_PAGEDFILE, "NPages" );
        createProperty( Office.PARAGRAPH_COUNT, Office.NAMESPACE_URI_DOC_META, "paragraph-count" );
        createProperty( Office.TABLE_COUNT, Office.NAMESPACE_URI_DOC_META, "table-count" );
        createLangAltProperty( TikaCoreProperties.TITLE, XMPConst.NS_DC, "title" );
        createProperty( Office.WORD_COUNT, Office.NAMESPACE_URI_DOC_META, "word-count" );

        // duplicate properties not mapped:
        // nbImg | 0
        // nbObject | 0
        // nbPage | 1
        // nbPara | 3
        // nbTab | 0
        // nbWord | 5

        return super.getXMPMeta();
    }

    @Override
    protected Set<Namespace> getAdditionalNamespaces() {
        return ADDITIONAL_NAMESPACES;
    }
}
"
tika-xmp/src/main/java/org/apache/tika/xmp/convert/RTFConverter.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.xmp.convert;

import java.util.Arrays;
import java.util.Collections;
import java.util.HashSet;
import java.util.Set;

import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.HttpHeaders;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.OfficeOpenXMLCore;
import org.apache.tika.metadata.OfficeOpenXMLExtended;
import org.apache.tika.metadata.TikaCoreProperties;

import com.adobe.xmp.XMPConst;
import com.adobe.xmp.XMPException;
import com.adobe.xmp.XMPMeta;
import com.adobe.xmp.options.PropertyOptions;

/**
 * Tika to XMP mapping for the RTF format.
 */
public class RTFConverter extends AbstractConverter {
    protected static final Set<Namespace> ADDITIONAL_NAMESPACES = Collections
            .unmodifiableSet( new HashSet<Namespace>( Arrays.asList( new Namespace(
                    OfficeOpenXMLExtended.NAMESPACE_URI, OfficeOpenXMLExtended.PREFIX ) ) ) );

    public RTFConverter() throws TikaException {
        super();
    }

    @Override
    public XMPMeta process(Metadata metadata) throws XMPException {
        setMetadata( metadata );

        createProperty( HttpHeaders.CONTENT_TYPE, XMPConst.NS_DC, "format" );

        createCommaSeparatedArray( TikaCoreProperties.CREATOR, XMPConst.NS_DC, "creator",
                PropertyOptions.ARRAY_ORDERED );
        createLangAltProperty( TikaCoreProperties.TITLE, XMPConst.NS_DC, "title" );
        createLangAltProperty( TikaCoreProperties.DESCRIPTION, XMPConst.NS_DC, "description" );
        createCommaSeparatedArray( TikaCoreProperties.KEYWORDS, XMPConst.NS_DC, "subject",
                PropertyOptions.ARRAY );
        createProperty( OfficeOpenXMLCore.CATEGORY, XMPConst.NS_IPTCCORE, "intellectualGenre" );
        createProperty( OfficeOpenXMLExtended.TEMPLATE, OfficeOpenXMLExtended.NAMESPACE_URI,
                "Template" );
        createProperty( TikaCoreProperties.COMMENTS, XMPConst.NS_PDFX, "Comments" );
        createProperty( OfficeOpenXMLExtended.COMPANY, OfficeOpenXMLExtended.NAMESPACE_URI,
                "Company" );
        createProperty( OfficeOpenXMLExtended.MANAGER, OfficeOpenXMLExtended.NAMESPACE_URI,
                "Manager" );

        return getXMPMeta();
    }

    @Override
    protected Set<Namespace> getAdditionalNamespaces() {
        return ADDITIONAL_NAMESPACES;
    }
}
"
tika-xmp/src/main/java/org/apache/tika/xmp/convert/TikaToXMP.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.tika.xmp.convert;

import java.util.HashMap;
import java.util.Map;
import java.util.Set;

import org.apache.tika.exception.TikaException;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.metadata.TikaCoreProperties;
import org.apache.tika.mime.MediaType;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.parser.microsoft.OfficeParser;
import org.apache.tika.parser.microsoft.ooxml.OOXMLParser;
import org.apache.tika.parser.odf.OpenDocumentParser;
import org.apache.tika.parser.rtf.RTFParser;

import com.adobe.xmp.XMPException;
import com.adobe.xmp.XMPMeta;
import com.adobe.xmp.XMPMetaFactory;

public class TikaToXMP {
    /**
     * Map from mimetype to converter class Must only be accessed through
     * <code>getConverterMap</code>
     */
    private static Map<MediaType, Class<? extends ITikaToXMPConverter>> converterMap;

    // --- public API implementation---

    public TikaToXMP() {
        // Nothing to do
    }

    /**
     * @see TikaToXMP#convert(Metadata, String) But the mimetype is retrieved from the metadata
     *      map.
     */
    public static XMPMeta convert(Metadata tikaMetadata) throws TikaException {
        if (tikaMetadata == null) {
            throw new IllegalArgumentException( "Metadata parameter must not be null" );
        }

        String mimetype = tikaMetadata.get( Metadata.CONTENT_TYPE );
        if (mimetype == null) {
            mimetype = tikaMetadata.get( TikaCoreProperties.FORMAT );
        }

        return convert( tikaMetadata, mimetype );
    }

    /**
     * Convert the given Tika metadata map to XMP object. If a mimetype is provided in the Metadata
     * map, a specific converter can be used, that converts all available metadata. If there is no
     * mimetype provided or no specific converter available a generic conversion is done which will
     * convert only those properties that are in known namespaces and are using the correct
     * prefixes.
     *
     * @param tikaMetadata
     *            the Metadata map from Tika
     * @param mimetype
     *            depicts the format's converter to use
     * @return XMP object
     * @throws TikaException
     */
    public static XMPMeta convert(Metadata tikaMetadata, String mimetype) throws TikaException {
        if (tikaMetadata == null) {
            throw new IllegalArgumentException( "Metadata parameter must not be null" );
        }

        ITikaToXMPConverter converter = null;

        if (isConverterAvailable( mimetype )) {
            converter = getConverter( mimetype );
        }
        else {
            converter = new GenericConverter();
        }

        XMPMeta xmp = null;

        if (converter != null) {
            try {
                xmp = converter.process( tikaMetadata );
            }
            catch (XMPException e) {
                throw new TikaException( "Tika metadata could not be converted to XMP", e );
            }
        }
        else {
            xmp = XMPMetaFactory.create(); // empty packet
        }

        return xmp;
    }

    /**
     * Check if there is a converter available which allows to convert the Tika metadata to XMP
     *
     * @param mimetype
     *            the Mimetype
     * @return true if the Metadata object can be converted or false if not
     */
    public static boolean isConverterAvailable(String mimetype) {
        MediaType type = MediaType.parse( mimetype );

        if (type != null) {
            return (getConverterMap().get( type ) != null);
        }

        return false;
    }

    /**
     * Retrieve a specific converter according to the mimetype
     *
     * @param mimetype
     *            the Mimetype
     * @return the converter or null, if none exists
     * @throws TikaException
     */
    public static ITikaToXMPConverter getConverter(String mimetype) throws TikaException {
        if (mimetype == null) {
            throw new IllegalArgumentException( "mimetype must not be null" );
        }

        ITikaToXMPConverter converter = null;

        MediaType type = MediaType.parse( mimetype );

        if (type != null) {
            Class<? extends ITikaToXMPConverter> clazz = getConverterMap().get( type );
            if (clazz != null) {
                try {
                    converter = clazz.newInstance();
                }
                catch (Exception e) {
                    throw new TikaException(
                            "TikaToXMP converter class cannot be instantiated for mimetype: "
                                    + type.toString(), e );
                }
            }
        }

        return converter;
    }

    // --- Private methods ---

    private static Map<MediaType, Class<? extends ITikaToXMPConverter>> getConverterMap() {
        if (converterMap == null) {
            converterMap = new HashMap<MediaType, Class<? extends ITikaToXMPConverter>>();
            initialize();
        }
        return converterMap;
    }

    /**
     * Initializes the map with supported converters.
     */
    private static void initialize() {
        // No particular parsing context is needed
        ParseContext parseContext = new ParseContext();

        // MS Office Binary File Format
        addConverter( new OfficeParser().getSupportedTypes( parseContext ),
                MSOfficeBinaryConverter.class );

        // Rich Text Format
        addConverter( new RTFParser().getSupportedTypes( parseContext ), RTFConverter.class );

        // MS Open XML Format
        addConverter( new OOXMLParser().getSupportedTypes( parseContext ),
                MSOfficeXMLConverter.class );

        // Open document format
        addConverter( new OpenDocumentParser().getSupportedTypes( parseContext ),
                OpenDocumentConverter.class );
    }

    private static void addConverter(Set<MediaType> supportedTypes,
            Class<? extends ITikaToXMPConverter> converter) {
        for (MediaType type : supportedTypes) {
            getConverterMap().put( type, converter );
        }
    }
}
"
