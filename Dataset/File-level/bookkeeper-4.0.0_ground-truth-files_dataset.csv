File,Bug,SRC
bookkeeper-benchmark/src/main/java/org/apache/bookkeeper/benchmark/MySqlClient.java,true,"package org.apache.bookkeeper.benchmark;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.FileOutputStream;
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.SQLException;
import java.sql.Statement;
import java.util.HashMap;

import org.apache.bookkeeper.client.BookKeeper;
import org.apache.bookkeeper.client.LedgerHandle;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


import org.apache.zookeeper.KeeperException;

public class MySqlClient {
    static Logger LOG = LoggerFactory.getLogger(MySqlClient.class);

    BookKeeper x;
    LedgerHandle lh;
    Integer entryId;
    HashMap<Integer, Integer> map;

    FileOutputStream fStream;
    FileOutputStream fStreamLocal;
    long start, lastId;
    Connection con;
    Statement stmt;


    public MySqlClient(String hostport, String user, String pass)
            throws ClassNotFoundException {
        entryId = 0;
        map = new HashMap<Integer, Integer>();
        Class.forName("com.mysql.jdbc.Driver");
        // database is named "bookkeeper"
        String url = "jdbc:mysql://" + hostport + "/bookkeeper";
        try {
            con = DriverManager.getConnection(url, user, pass);
            stmt = con.createStatement();
            // drop table and recreate it
            stmt.execute("DROP TABLE IF EXISTS data;");
            stmt.execute("create table data(transaction_id bigint PRIMARY KEY AUTO_INCREMENT, content TEXT);");
            LOG.info("Database initialization terminated");
        } catch (SQLException e) {

            // TODO Auto-generated catch block
            e.printStackTrace();
        }
    }

    public void closeHandle() throws KeeperException, InterruptedException, SQLException {
        con.close();
    }
    /**
     * First parameter is an integer defining the length of the message
     * Second parameter is the number of writes
     * Third parameter is host:port
     * Fourth parameter is username
     * Fifth parameter is password
     * @param args
     * @throws ClassNotFoundException
     * @throws SQLException
     */
    public static void main(String[] args) throws ClassNotFoundException, SQLException {
        int lenght = Integer.parseInt(args[1]);
        StringBuilder sb = new StringBuilder();
        while(lenght-- > 0) {
            sb.append('a');
        }
        try {
            MySqlClient c = new MySqlClient(args[2], args[3], args[4]);
            c.writeSameEntryBatch(sb.toString().getBytes(), Integer.parseInt(args[0]));
            c.writeSameEntry(sb.toString().getBytes(), Integer.parseInt(args[0]));
            c.closeHandle();
        } catch (NumberFormatException e) {
            e.printStackTrace();
        } catch (InterruptedException e) {
            e.printStackTrace();
        } catch (KeeperException e) {
            e.printStackTrace();
        }

    }

    /**
     * 	Adds  data entry to the DB
     * 	@param data 	the entry to be written, given as a byte array
     * 	@param times	the number of times the entry should be written on the DB	*/
    void writeSameEntryBatch(byte[] data, int times) throws InterruptedException, SQLException {
        start = System.currentTimeMillis();
        int count = times;
        String content = new String(data);
        System.out.println("Data: " + content + ", " + data.length);
        while(count-- > 0) {
            stmt.addBatch("insert into data(content) values(\"" + content + "\");");
        }
        LOG.info("Finished writing batch SQL command in ms: " + (System.currentTimeMillis() - start));
        start = System.currentTimeMillis();
        stmt.executeBatch();
        System.out.println("Finished " + times + " writes in ms: " + (System.currentTimeMillis() - start));
        LOG.info("Ended computation");
    }

    void writeSameEntry(byte[] data, int times) throws InterruptedException, SQLException {
        start = System.currentTimeMillis();
        int count = times;
        String content = new String(data);
        System.out.println("Data: " + content + ", " + data.length);
        while(count-- > 0) {
            stmt.executeUpdate("insert into data(content) values(\"" + content + "\");");
        }
        System.out.println("Finished " + times + " writes in ms: " + (System.currentTimeMillis() - start));
        LOG.info("Ended computation");
    }

}
"
bookkeeper-benchmark/src/main/java/org/apache/bookkeeper/benchmark/TestClient.java,true,"package org.apache.bookkeeper.benchmark;
/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */


import java.io.FileNotFoundException;
import java.io.FileOutputStream;
import java.io.IOException;
import java.util.Enumeration;
import java.util.HashMap;

import org.apache.bookkeeper.client.AsyncCallback.AddCallback;
import org.apache.bookkeeper.client.AsyncCallback.ReadCallback;
import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.client.BookKeeper;
import org.apache.bookkeeper.client.LedgerEntry;
import org.apache.bookkeeper.client.LedgerHandle;
import org.apache.bookkeeper.client.BookKeeper.DigestType;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.zookeeper.KeeperException;

/**
 * This is a simple test program to compare the performance of writing to
 * BookKeeper and to the local file system.
 *
 */

public class TestClient
    implements AddCallback, ReadCallback {
    private static final Logger LOG = LoggerFactory.getLogger(TestClient.class);

    BookKeeper x;
    LedgerHandle lh;
    Integer entryId;
    HashMap<Integer, Integer> map;

    FileOutputStream fStream;
    FileOutputStream fStreamLocal;
    long start, lastId;

    public TestClient() {
        entryId = 0;
        map = new HashMap<Integer, Integer>();
    }

    public TestClient(String servers) throws KeeperException, IOException, InterruptedException {
        this();
        x = new BookKeeper(servers);
        try {
            lh = x.createLedger(DigestType.MAC, new byte[] {'a', 'b'});
        } catch (BKException e) {
            LOG.error(e.toString());
        }
    }

    public TestClient(String servers, int ensSize, int qSize)
            throws KeeperException, IOException, InterruptedException {
        this();
        x = new BookKeeper(servers);
        try {
            lh = x.createLedger(ensSize, qSize, DigestType.MAC, new byte[] {'a', 'b'});
        } catch (BKException e) {
            LOG.error(e.toString());
        }
    }

    public TestClient(FileOutputStream fStream)
            throws FileNotFoundException {
        this.fStream = fStream;
        this.fStreamLocal = new FileOutputStream("./local.log");
    }


    public Integer getFreshEntryId(int val) {
        ++this.entryId;
        synchronized (map) {
            map.put(this.entryId, val);
        }
        return this.entryId;
    }

    public boolean removeEntryId(Integer id) {
        boolean retVal = false;
        synchronized (map) {
            map.remove(id);
            retVal = true;

            if(map.size() == 0) map.notifyAll();
            else {
                if(map.size() < 4)
                    LOG.error(map.toString());
            }
        }
        return retVal;
    }

    public void closeHandle() throws KeeperException, InterruptedException, BKException {
        lh.close();
    }
    /**
     * First says if entries should be written to BookKeeper (0) or to the local
     * disk (1). Second parameter is an integer defining the length of a ledger entry.
     * Third parameter is the number of writes.
     *
     * @param args
     */
    public static void main(String[] args) {

        int lenght = Integer.parseInt(args[1]);
        StringBuilder sb = new StringBuilder();
        while(lenght-- > 0) {
            sb.append('a');
        }

        Integer selection = Integer.parseInt(args[0]);
        switch(selection) {
        case 0:
            StringBuilder servers_sb = new StringBuilder();
            for (int i = 4; i < args.length; i++) {
                servers_sb.append(args[i] + " ");
            }

            String servers = servers_sb.toString().trim().replace(' ', ',');
            try {
                TestClient c = new TestClient(servers, Integer.parseInt(args[3]), Integer.parseInt(args[4]));
                c.writeSameEntryBatch(sb.toString().getBytes(), Integer.parseInt(args[2]));
                //c.writeConsecutiveEntriesBatch(Integer.parseInt(args[0]));
                c.closeHandle();
            } catch (Exception e) {
                LOG.error("Exception occurred", e);
            } 
            break;
        case 1:

            try {
                TestClient c = new TestClient(new FileOutputStream(args[2]));
                c.writeSameEntryBatchFS(sb.toString().getBytes(), Integer.parseInt(args[3]));
            } catch(FileNotFoundException e) {
                LOG.error("File not found", e);
            }
            break;
        case 2:
            break;
        }
    }

    void writeSameEntryBatch(byte[] data, int times) throws InterruptedException {
        start = System.currentTimeMillis();
        int count = times;
        LOG.debug("Data: " + new String(data) + ", " + data.length);
        while(count-- > 0) {
            lh.asyncAddEntry(data, this, this.getFreshEntryId(2));
        }
        LOG.debug("Finished " + times + " async writes in ms: " + (System.currentTimeMillis() - start));
        synchronized (map) {
            if(map.size() != 0)
                map.wait();
        }
        LOG.debug("Finished processing in ms: " + (System.currentTimeMillis() - start));

        LOG.debug("Ended computation");
    }

    void writeConsecutiveEntriesBatch(int times) throws InterruptedException {
        start = System.currentTimeMillis();
        int count = times;
        while(count-- > 0) {
            byte[] write = new byte[2];
            int j = count%100;
            int k = (count+1)%100;
            write[0] = (byte) j;
            write[1] = (byte) k;
            lh.asyncAddEntry(write, this, this.getFreshEntryId(2));
        }
        LOG.debug("Finished " + times + " async writes in ms: " + (System.currentTimeMillis() - start));
        synchronized (map) {
            if(map.size() != 0)
                map.wait();
        }
        LOG.debug("Finished processing writes (ms): " + (System.currentTimeMillis() - start));

        Integer mon = Integer.valueOf(0);
        synchronized(mon) {
            lh.asyncReadEntries(1, times - 1, this, mon);
            mon.wait();
        }
        LOG.error("Ended computation");
    }

    void writeSameEntryBatchFS(byte[] data, int times) {
        int count = times;
        LOG.debug("Data: " + data.length + ", " + times);
        try {
            start = System.currentTimeMillis();
            while(count-- > 0) {
                fStream.write(data);
                fStreamLocal.write(data);
                fStream.flush();
            }
            fStream.close();
            System.out.println("Finished processing writes (ms): " + (System.currentTimeMillis() - start));
        } catch(IOException e) {
            LOG.error("IOException occurred", e);
        }
    }


    @Override
    public void addComplete(int rc, LedgerHandle lh, long entryId, Object ctx) {
        this.removeEntryId((Integer) ctx);
    }

    @Override
    public void readComplete(int rc, LedgerHandle lh, Enumeration<LedgerEntry> seq, Object ctx) {
        System.out.println("Read callback: " + rc);
        while(seq.hasMoreElements()) {
            LedgerEntry le = seq.nextElement();
            LOG.debug(new String(le.getEntry()));
        }
        synchronized(ctx) {
            ctx.notify();
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/Bookie.java,true,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.InputStreamReader;
import java.io.OutputStreamWriter;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.RandomAccessFile;
import java.net.InetAddress;
import java.net.InetSocketAddress;
import java.nio.ByteBuffer;
import java.nio.channels.FileChannel;
import java.util.ArrayList;
import java.util.Collections;
import java.util.HashMap;
import java.util.LinkedList;
import java.util.List;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.atomic.AtomicBoolean;

import org.apache.bookkeeper.meta.LedgerManager;
import org.apache.bookkeeper.meta.LedgerManagerFactory;
import org.apache.bookkeeper.bookie.BookieException;
import org.apache.bookkeeper.conf.ServerConfiguration;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.WriteCallback;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.WatchedEvent;
import org.apache.zookeeper.Watcher;
import org.apache.zookeeper.ZooKeeper;
import org.apache.zookeeper.ZooDefs.Ids;

/**
 * Implements a bookie.
 *
 */

public class Bookie extends Thread {
    HashMap<Long, LedgerDescriptor> ledgers = new HashMap<Long, LedgerDescriptor>();
    static Logger LOG = LoggerFactory.getLogger(Bookie.class);
    final static long MB = 1024 * 1024L;
    // max journal file size
    final long maxJournalSize;
    // number journal files kept before marked journal
    final int maxBackupJournals;

    final File journalDirectory;

    final File ledgerDirectories[];

    final ServerConfiguration conf;

    final SyncThread syncThread;
    final LedgerManager ledgerManager;

    /**
     * Current directory layout version. Increment this 
     * when you make a change to the format of any of the files in 
     * this directory or to the general layout of the directory.
     */
    static final int CURRENT_DIRECTORY_LAYOUT_VERSION = 1;
    static final String VERSION_FILENAME = "VERSION";
    
    // ZK registration path for this bookie
    static final String BOOKIE_REGISTRATION_PATH = "/ledgers/available/";

    // ZooKeeper client instance for the Bookie
    ZooKeeper zk;
    private volatile boolean isZkExpired = true;

    // Running flag
    private volatile boolean running = false;

    public static class NoLedgerException extends IOException {
        private static final long serialVersionUID = 1L;
        private long ledgerId;
        public NoLedgerException(long ledgerId) {
            this.ledgerId = ledgerId;
        }
        public long getLedgerId() {
            return ledgerId;
        }
    }
    public static class NoEntryException extends IOException {
        private static final long serialVersionUID = 1L;
        private long ledgerId;
        private long entryId;
        public NoEntryException(long ledgerId, long entryId) {
            super("Entry " + entryId + " not found in " + ledgerId);
            this.ledgerId = ledgerId;
            this.entryId = entryId;
        }
        public long getLedger() {
            return ledgerId;
        }
        public long getEntry() {
            return entryId;
        }
    }

    EntryLogger entryLogger;
    LedgerCache ledgerCache;
    /**
     * SyncThread is a background thread which flushes ledger index pages periodically.
     * Also it takes responsibility of garbage collecting journal files.
     *
     * <p>
     * Before flushing, SyncThread first records a log marker {journalId, journalPos} in memory,
     * which indicates entries before this log marker would be persisted to ledger files.
     * Then sync thread begins flushing ledger index pages to ledger index files, flush entry
     * logger to ensure all entries persisted to entry loggers for future reads.
     * </p>
     * <p>
     * After all data has been persisted to ledger index files and entry loggers, it is safe
     * to persist the log marker to disk. If bookie failed after persist log mark,
     * bookie is able to relay journal entries started from last log mark without losing
     * any entries.
     * </p>
     * <p>
     * Those journal files whose id are less than the log id in last log mark, could be
     * removed safely after persisting last log mark. We provide a setting to let user keeping
     * number of old journal files which may be used for manual recovery in critical disaster.
     * </p>
     */
    class SyncThread extends Thread {
        volatile boolean running = true;
        // flag to ensure sync thread will not be interrupted during flush
        final AtomicBoolean flushing = new AtomicBoolean(false);
        // make flush interval as a parameter
        final int flushInterval;
        public SyncThread(ServerConfiguration conf) {
            super("SyncThread");
            flushInterval = conf.getFlushInterval();
            if (LOG.isDebugEnabled()) {
                LOG.debug("Flush Interval : " + flushInterval);
            }
        }
        @Override
        public void run() {
            while(running) {
                synchronized(this) {
                    try {
                        wait(flushInterval);
                        if (!entryLogger.testAndClearSomethingWritten()) {
                            continue;
                        }
                    } catch (InterruptedException e) {
                        Thread.currentThread().interrupt();
                        continue;
                    }
                }

                // try to mark flushing flag to make sure it would not be interrupted
                // by shutdown during flushing. otherwise it will receive
                // ClosedByInterruptException which may cause index file & entry logger
                // closed and corrupted.
                if (!flushing.compareAndSet(false, true)) {
                    // set flushing flag failed, means flushing is true now
                    // indicates another thread wants to interrupt sync thread to exit
                    break;
                }

                lastLogMark.markLog();
                try {
                    ledgerCache.flushLedger(true);
                } catch (IOException e) {
                    LOG.error("Exception flushing Ledger", e);
                }
                try {
                    entryLogger.flush();
                } catch (IOException e) {
                    LOG.error("Exception flushing entry logger", e);
                }
                lastLogMark.rollLog();

                // list the journals that have been marked
                List<Long> logs = listJournalIds(journalDirectory, new JournalIdFilter() {
                    @Override
                    public boolean accept(long journalId) {
                        if (journalId < lastLogMark.lastMark.txnLogId) {
                            return true;
                        } else {
                            return false;
                        }
                    }
                });

                // keep MAX_BACKUP_JOURNALS journal files before marked journal
                if (logs.size() >= maxBackupJournals) {
                    int maxIdx = logs.size() - maxBackupJournals;
                    for (int i=0; i<maxIdx; i++) {
                        long id = logs.get(i);
                        // make sure the journal id is smaller than marked journal id
                        if (id < lastLogMark.lastMark.txnLogId) {
                            File journalFile = new File(journalDirectory, Long.toHexString(id) + ".txn");
                            journalFile.delete();
                            LOG.info("garbage collected journal " + journalFile.getName());
                        }
                    }
                }

                // clear flushing flag
                flushing.set(false);
            }
        }

        // shutdown sync thread
        void shutdown() throws InterruptedException {
            running = false;
            if (flushing.compareAndSet(false, true)) {
                // if setting flushing flag succeed, means syncThread is not flushing now
                // it is safe to interrupt itself now 
                this.interrupt();
            }
            this.join();
        }
    }

    public Bookie(ServerConfiguration conf) 
            throws IOException, KeeperException, InterruptedException {
        this.conf = conf;
        this.journalDirectory = conf.getJournalDir();
        this.ledgerDirectories = conf.getLedgerDirs();
        this.maxJournalSize = conf.getMaxJournalSize() * MB;
        this.maxBackupJournals = conf.getMaxBackupJournals();

        // check directory layouts
        checkDirectoryLayoutVersion(journalDirectory);
        for (File dir : ledgerDirectories) {
            checkDirectoryLayoutVersion(dir);
        }

        // instantiate zookeeper client to initialize ledger manager
        ZooKeeper newZk = instantiateZookeeperClient(conf.getZkServers());
        ledgerManager = LedgerManagerFactory.newLedgerManager(conf, newZk);

        syncThread = new SyncThread(conf);
        entryLogger = new EntryLogger(conf, this);
        ledgerCache = new LedgerCache(conf, ledgerManager);

        lastLogMark.readLog();
        if (LOG.isDebugEnabled()) {
            LOG.debug("Last Log Mark : " + lastLogMark);
        }
        final long markedLogId = lastLogMark.txnLogId;
        List<Long> logs = listJournalIds(journalDirectory, new JournalIdFilter() {
            @Override
            public boolean accept(long journalId) {
                if (journalId < markedLogId) {
                    return false;
                }
                return true;
            }
        });
        // last log mark may be missed due to no sync up before
        // validate filtered log ids only when we have markedLogId
        if (markedLogId > 0) {
            if (logs.size() == 0 || logs.get(0) != markedLogId) {
                throw new IOException("Recovery log " + markedLogId + " is missing");
            }
        }
        if (LOG.isDebugEnabled()) {
            LOG.debug("Try to relay journal logs : " + logs);
        }
        // TODO: When reading in the journal logs that need to be synced, we
        // should use BufferedChannels instead to minimize the amount of
        // system calls done.
        ByteBuffer lenBuff = ByteBuffer.allocate(4);
        ByteBuffer recBuff = ByteBuffer.allocate(64*1024);
        for(Long id: logs) {
            FileChannel recLog ;
            if(id == markedLogId) {
              long markedLogPosition = lastLogMark.txnLogPosition;
              recLog = openChannel(id, markedLogPosition);
            } else {
              recLog = openChannel(id);
            }

            while(true) {
                lenBuff.clear();
                fullRead(recLog, lenBuff);
                if (lenBuff.remaining() != 0) {
                    break;
                }
                lenBuff.flip();
                int len = lenBuff.getInt();
                if (len == 0) {
                    break;
                }
                recBuff.clear();
                if (recBuff.remaining() < len) {
                    recBuff = ByteBuffer.allocate(len);
                }
                recBuff.limit(len);
                if (fullRead(recLog, recBuff) != len) {
                    // This seems scary, but it just means that this is where we
                    // left off writing
                    break;
                }
                recBuff.flip();
                long ledgerId = recBuff.getLong();
                if (LOG.isDebugEnabled()) {
                    LOG.debug("Relay journal - ledger id : " + ledgerId);
                }
                LedgerDescriptor handle = getHandle(ledgerId, false);
                try {
                    recBuff.rewind();
                    handle.addEntry(recBuff);
                } finally {
                    putHandle(handle);
                }
            }
        }
        // pass zookeeper instance here
        // since GarbageCollector thread should only start after journal
        // finished replay
        this.zk = newZk;
        // make the bookie available
        registerBookie(conf.getBookiePort());
        setDaemon(true);
        LOG.debug("I'm starting a bookie with journal directory " + journalDirectory.getName());
        start();
        syncThread.start();
        // set running here.
        // since bookie server use running as a flag to tell bookie server whether it is alive
        // if setting it in bookie thread, the watcher might run before bookie thread.
        running = true;
    }

    public static interface JournalIdFilter {
        public boolean accept(long journalId);
    }

    /**
     * List all journal ids by a specified journal id filer
     *
     * @param journalDir journal dir
     * @param filter journal id filter
     * @return list of filtered ids
     */
    public static List<Long> listJournalIds(File journalDir, JournalIdFilter filter) {
        File logFiles[] = journalDir.listFiles();
        List<Long> logs = new ArrayList<Long>();
        for(File f: logFiles) {
            String name = f.getName();
            if (!name.endsWith(".txn")) {
                continue;
            }
            String idString = name.split("\\.")[0];
            long id = Long.parseLong(idString, 16);
            if (filter != null) {
                if (filter.accept(id)) {
                    logs.add(id);
                }
            } else {
                logs.add(id);
            }
        }
        Collections.sort(logs);
        return logs;
    }

    /**
     * Instantiate the ZooKeeper client for the Bookie.
     */
    private ZooKeeper instantiateZookeeperClient(String zkServers) throws IOException {
        if (zkServers == null) {
            LOG.warn("No ZK servers passed to Bookie constructor so BookKeeper clients won't know about this server!");
            isZkExpired = false;
            return null;
        }
        int zkTimeout = conf.getZkTimeout();
        // Create the ZooKeeper client instance
        return newZookeeper(zkServers, zkTimeout);
    }

    /**
     * Register as an available bookie
     */
    private void registerBookie(int port) throws IOException {
        if (null == zk) {
            // zookeeper instance is null, means not register itself to zk
            return;
        }
        // Create the ZK ephemeral node for this Bookie.
        try {
            zk.create(BOOKIE_REGISTRATION_PATH + InetAddress.getLocalHost().getHostAddress() + ":" + port, new byte[0],
                      Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL);
        } catch (Exception e) {
            LOG.error("ZK exception registering ephemeral Znode for Bookie!", e);
            // Throw an IOException back up. This will cause the Bookie
            // constructor to error out. Alternatively, we could do a System
            // exit here as this is a fatal error.
            throw new IOException(e);
        }
    }

    /**
     * Create a new zookeeper client to zk cluster.
     *
     * <p>
     * Bookie Server just used zk client when syncing ledgers for garbage collection.
     * So when zk client is expired, it means this bookie server is not available in
     * bookie server list. The bookie client will be notified for its expiration. No
     * more bookie request will be sent to this server. So it's better to exit when zk
     * expired.
     * </p>
     * <p>
     * Since there are lots of bk operations cached in queue, so we wait for all the operations
     * are processed and quit. It is done by calling <b>shutdown</b>.
     * </p>
     *
     * @param zkServers the quorum list of zk servers
     * @param sessionTimeout session timeout of zk connection
     *
     * @return zk client instance
     */
    private ZooKeeper newZookeeper(final String zkServers,
                                   final int sessionTimeout) throws IOException {
        ZooKeeper newZk = new ZooKeeper(zkServers, sessionTimeout,
        new Watcher() {
            @Override
            public void process(WatchedEvent event) {
                // handle session disconnects and expires
                if (event.getType()
                .equals(Watcher.Event.EventType.None)) {
                    if (event.getState().equals(
                    Watcher.Event.KeeperState.Disconnected)) {
                        LOG.warn("ZK client has been disconnected to the ZK server!");
                    } else if (event.getState().equals(
                    Watcher.Event.KeeperState.SyncConnected)) {
                        LOG.info("ZK client has been reconnected to the ZK server!");
                    }
                }
                // Check for expired connection.
                if (event.getState().equals(
                Watcher.Event.KeeperState.Expired)) {
                    LOG.error("ZK client connection to the ZK server has expired!");
                    isZkExpired = true;
                    try {
                        shutdown();
                    } catch (InterruptedException ie) {
                        System.exit(-1);
                    }
                }
            }
        });
        isZkExpired = false;
        return newZk;
    }

    /**
     * Check the layout version of a directory. If it is outside of the 
     * range which this version of the software can handle, throw an
     * exception.
     *
     * @param dir Directory to check
     * @throws IOException if layout version if is outside usable range
     *               or if there is a problem reading the version file
     */
    private void checkDirectoryLayoutVersion(File dir) 
            throws IOException {
        if (!dir.isDirectory()) {
            throw new IOException("Directory("+dir+") isn't a directory");
        }
        File versionFile = new File(dir, VERSION_FILENAME);
        
        FileInputStream fis;
        try {
            fis = new FileInputStream(versionFile);
        } catch (FileNotFoundException e) {
            /* 
             * If the version file is not found, this must
             * either be the first time we've used this directory,
             * or it must date from before layout versions were introduced.
             * In both cases, we just create the version file
             */
            LOG.info("No version file found, creating");
            createDirectoryLayoutVersionFile(dir);
            return;
        }
        
        BufferedReader br = new BufferedReader(new InputStreamReader(fis));
        try {
            String layoutVersionStr = br.readLine();
            int layoutVersion = Integer.parseInt(layoutVersionStr);
            if (layoutVersion != CURRENT_DIRECTORY_LAYOUT_VERSION) {
                String errmsg = "Directory has an invalid version, expected " 
                    + CURRENT_DIRECTORY_LAYOUT_VERSION + ", found " + layoutVersion;
                LOG.error(errmsg);
                throw new IOException(errmsg);
            }
        } catch(NumberFormatException e) {
            throw new IOException("Version file has invalid content", e);
        } finally {
            try {
                fis.close();
            } catch (IOException e) {
                LOG.warn("Error closing version file", e);
            }
        }
    }
    
    /**
     * Create the directory layout version file with the current
     * directory layout version
     */
    private void createDirectoryLayoutVersionFile(File dir) throws IOException {
        File versionFile = new File(dir, VERSION_FILENAME);

        FileOutputStream fos = new FileOutputStream(versionFile);
        BufferedWriter bw = null;
        try {
            bw = new BufferedWriter(new OutputStreamWriter(fos));
            bw.write(String.valueOf(CURRENT_DIRECTORY_LAYOUT_VERSION));
        } finally {
            if (bw != null) {
                bw.close();
            }
            fos.close();
        }
    }

    private static int fullRead(FileChannel fc, ByteBuffer bb) throws IOException {
        int total = 0;
        while(bb.remaining() > 0) {
            int rc = fc.read(bb);
            if (rc <= 0) {
                return total;
            }
            total += rc;
        }
        return total;
    }
    private void putHandle(LedgerDescriptor handle) {
        synchronized (ledgers) {
            handle.decRef();
        }
    }

    private LedgerDescriptor getHandle(long ledgerId, boolean readonly, byte[] masterKey) throws IOException {
        LedgerDescriptor handle = null;
        synchronized (ledgers) {
            handle = ledgers.get(ledgerId);
            if (handle == null) {
                FileInfo fi = null;
                try {
                    // get file info will throw NoLedgerException
                    fi = ledgerCache.getFileInfo(ledgerId, !readonly);

                    // if an existed ledger index file, we can get its master key
                    // if an new created ledger index file, we will get a null master key
                    byte[] existingMasterKey = fi.readMasterKey();
                    ByteBuffer masterKeyToSet = ByteBuffer.wrap(masterKey);
                    if (existingMasterKey == null) {
                        // no master key set before
                        fi.writeMasterKey(masterKey);
                    } else if (!masterKeyToSet.equals(ByteBuffer.wrap(existingMasterKey))) {
                        throw new IOException("Wrong master key for ledger " + ledgerId);
                    }
                    handle = createHandle(ledgerId, readonly);
                    ledgers.put(ledgerId, handle);
                    handle.setMasterKey(masterKeyToSet);
                } finally {
                    if (fi != null) {
                        fi.release();
                    }
                }
            }
            handle.incRef();
        }
        return handle;
    }

    private LedgerDescriptor getHandle(long ledgerId, boolean readonly) throws IOException {
        LedgerDescriptor handle = null;
        synchronized (ledgers) {
            handle = ledgers.get(ledgerId);
            if (handle == null) {
                FileInfo fi = null;
                try {
                    // get file info will throw NoLedgerException
                    fi = ledgerCache.getFileInfo(ledgerId, !readonly);

                    // if an existed ledger index file, we can get its master key
                    // if an new created ledger index file, we will get a null master key
                    byte[] existingMasterKey = fi.readMasterKey();
                    if (existingMasterKey == null) {
                        throw new IOException("Weird! No master key found in ledger " + ledgerId);
                    }

                    handle = createHandle(ledgerId, readonly);
                    ledgers.put(ledgerId, handle);
                    handle.setMasterKey(ByteBuffer.wrap(existingMasterKey));
                } finally {
                    if (fi != null) {
                        fi.release();
                    }
                }
            }
            handle.incRef();
        }
        return handle;
    }


    private LedgerDescriptor createHandle(long ledgerId, boolean readOnly) throws IOException {
        return new LedgerDescriptor(ledgerId, entryLogger, ledgerCache);
    }

    static class QueueEntry {
        QueueEntry(ByteBuffer entry, long ledgerId, long entryId,
                   WriteCallback cb, Object ctx) {
            this.entry = entry.duplicate();
            this.cb = cb;
            this.ctx = ctx;
            this.ledgerId = ledgerId;
            this.entryId = entryId;
        }

        ByteBuffer entry;

        long ledgerId;

        long entryId;

        WriteCallback cb;

        Object ctx;
    }

    LinkedBlockingQueue<QueueEntry> queue = new LinkedBlockingQueue<QueueEntry>();

    public final static long preAllocSize = 4*1024*1024;

    public final static ByteBuffer zeros = ByteBuffer.allocate(512);

    class LastLogMark {
        long txnLogId;
        long txnLogPosition;
        LastLogMark lastMark;
        LastLogMark(long logId, long logPosition) {
            this.txnLogId = logId;
            this.txnLogPosition = logPosition;
        }
        synchronized void setLastLogMark(long logId, long logPosition) {
            txnLogId = logId;
            txnLogPosition = logPosition;
        }
        synchronized void markLog() {
            lastMark = new LastLogMark(txnLogId, txnLogPosition);
        }
        synchronized void rollLog() {
            byte buff[] = new byte[16];
            ByteBuffer bb = ByteBuffer.wrap(buff);
            // we should record <logId, logPosition> marked in markLog
            // which is safe since records before lastMark have been
            // persisted to disk (both index & entry logger)
            bb.putLong(lastMark.txnLogId);
            bb.putLong(lastMark.txnLogPosition);
            if (LOG.isDebugEnabled()) {
                LOG.debug("RollLog to persist last marked log : " + lastMark);
            }
            for(File dir: ledgerDirectories) {
                File file = new File(dir, "lastMark");
                try {
                    FileOutputStream fos = new FileOutputStream(file);
                    fos.write(buff);
                    fos.getChannel().force(true);
                    fos.close();
                } catch (IOException e) {
                    LOG.error("Problems writing to " + file, e);
                }
            }
        }

        /**
         * Read last mark from lastMark file.
         * The last mark should first be max journal log id,
         * and then max log position in max journal log.
         */
        synchronized void readLog() {
            byte buff[] = new byte[16];
            ByteBuffer bb = ByteBuffer.wrap(buff);
            for(File dir: ledgerDirectories) {
                File file = new File(dir, "lastMark");
                try {
                    FileInputStream fis = new FileInputStream(file);
                    fis.read(buff);
                    fis.close();
                    bb.clear();
                    long i = bb.getLong();
                    long p = bb.getLong();
                    if (i > txnLogId) {
                        txnLogId = i;
                        if(p > txnLogPosition) {
                          txnLogPosition = p;
                        }
                    }
                } catch (IOException e) {
                    LOG.error("Problems reading from " + file + " (this is okay if it is the first time starting this bookie");
                }
            }
        }

        @Override
        public String toString() {
            StringBuilder sb = new StringBuilder();
            
            sb.append("LastMark: logId - ").append(txnLogId)
              .append(" , position - ").append(txnLogPosition);
            
            return sb.toString();
        }
    }

    private LastLogMark lastLogMark = new LastLogMark(0, 0);

    LastLogMark getLastLogMark() {
        return lastLogMark;
    }

    public boolean isRunning() {
        return running;
    }

    /**
     * A thread used for persisting journal entries to journal files.
     * 
     * <p>
     * Besides persisting journal entries, it also takes responsibility of
     * rolling journal files when a journal file reaches journal file size
     * limitation.
     * </p>
     * <p>
     * During journal rolling, it first closes the writing journal, generates
     * new journal file using current timestamp, and continue persistence logic.
     * Those journals will be garbage collected in SyncThread.
     * </p>
     */
    @Override
    public void run() {
        LinkedList<QueueEntry> toFlush = new LinkedList<QueueEntry>();
        ByteBuffer lenBuff = ByteBuffer.allocate(4);
        try {
            long logId = 0;
            FileChannel logFile = null;
            BufferedChannel bc = null;
            long nextPrealloc = 0;
            long lastFlushPosition = 0;

            QueueEntry qe = null;
            while (true) {
                // new journal file to write
                if (null == logFile) {
                    logId = System.currentTimeMillis();
                    logFile = openChannel(logId);
                    bc = new BufferedChannel(logFile, 65536);
                    zeros.clear();
                    nextPrealloc = preAllocSize;
                    lastFlushPosition = 0;
                    logFile.write(zeros, nextPrealloc);
                }

                if (qe == null) {
                    if (toFlush.isEmpty()) {
                        qe = queue.take();
                    } else {
                        qe = queue.poll();
                        if (qe == null || bc.position() > lastFlushPosition + 512*1024) {
                            //logFile.force(false);
                            bc.flush(true);
                            lastFlushPosition = bc.position();
                            lastLogMark.setLastLogMark(logId, lastFlushPosition);
                            for (QueueEntry e : toFlush) {
                                e.cb.writeComplete(0, e.ledgerId, e.entryId, null, e.ctx);
                            }
                            toFlush.clear();

                            // check whether journal file is over file limit
                            if (bc.position() > maxJournalSize) {
                                logFile.close();
                                logFile = null;
                                continue;
                            }
                        }
                    }
                }

                if (isZkExpired) {
                    LOG.warn("Exiting... zk client has expired.");
                    break;
                }
                if (qe == null) { // no more queue entry
                    continue;
                }
                lenBuff.clear();
                lenBuff.putInt(qe.entry.remaining());
                lenBuff.flip();
                //
                // we should be doing the following, but then we run out of
                // direct byte buffers
                // logFile.write(new ByteBuffer[] { lenBuff, qe.entry });
                bc.write(lenBuff);
                bc.write(qe.entry);
                if (bc.position() > nextPrealloc) {
                    nextPrealloc = (logFile.size() / preAllocSize + 1) * preAllocSize;
                    zeros.clear();
                    logFile.write(zeros, nextPrealloc);
                }
                toFlush.add(qe);
                qe = null;
            }
        } catch (Exception e) {
            LOG.error("Bookie thread exiting", e);
        }
    }

    private FileChannel openChannel(long logId) throws FileNotFoundException {
        return openChannel(logId, 0);
    }

    private FileChannel openChannel(long logId, long position) throws FileNotFoundException {
        FileChannel logFile = new RandomAccessFile(new File(journalDirectory,
                Long.toHexString(logId) + ".txn"),
                "rw").getChannel();
        try {
            logFile.position(position);
        } catch (IOException e) {
            LOG.error("Bookie journal file can seek to position :", e);
        }
        return logFile;
    }

    public synchronized void shutdown() throws InterruptedException {
        if (!running) { // avoid shutdown twice
            return;
        }
        // Shutdown the ZK client
        if(zk != null) zk.close();
        this.interrupt();
        this.join();
        syncThread.shutdown(); 
        for(LedgerDescriptor d: ledgers.values()) {
            d.close();
        }
        // Shutdown the EntryLogger which has the GarbageCollector Thread running
        entryLogger.shutdown();
        // close Ledger Manager
        ledgerManager.close();
        // setting running to false here, so watch thread in bookie server know it only after bookie shut down
        running = false;
    }

    /** 
     * Retrieve the ledger descriptor for the ledger which entry should be added to.
     * The LedgerDescriptor returned from this method should be eventually freed with 
     * #putHandle().
     *
     * @throws BookieException if masterKey does not match the master key of the ledger
     */
    private LedgerDescriptor getLedgerForEntry(ByteBuffer entry, byte[] masterKey) 
            throws IOException, BookieException {
        long ledgerId = entry.getLong();
        LedgerDescriptor handle = getHandle(ledgerId, false, masterKey);

        if(!handle.cmpMasterKey(ByteBuffer.wrap(masterKey))) {
            putHandle(handle);
            throw BookieException.create(BookieException.Code.UnauthorizedAccessException);
        }
        return handle;
    }

    /**
     * Add an entry to a ledger as specified by handle. 
     */
    private void addEntryInternal(LedgerDescriptor handle, ByteBuffer entry, WriteCallback cb, Object ctx)
            throws IOException, BookieException {
        long ledgerId = handle.getLedgerId();
        entry.rewind();
        long entryId = handle.addEntry(entry);

        entry.rewind();
        if (LOG.isTraceEnabled()) {
            LOG.trace("Adding " + entryId + "@" + ledgerId);
        }
        queue.add(new QueueEntry(entry, ledgerId, entryId, cb, ctx));
    }

    /**
     * Add entry to a ledger, even if the ledger has previous been fenced. This should only
     * happen in bookie recovery or ledger recovery cases, where entries are being replicates 
     * so that they exist on a quorum of bookies. The corresponding client side call for this
     * is not exposed to users.
     */
    public void recoveryAddEntry(ByteBuffer entry, WriteCallback cb, Object ctx, byte[] masterKey) 
            throws IOException, BookieException {
        LedgerDescriptor handle = getLedgerForEntry(entry, masterKey);
        synchronized (handle) {
            try {
                addEntryInternal(handle, entry, cb, ctx);
            } finally {
                putHandle(handle);
            }
        }
    }
    
    /** 
     * Add entry to a ledger.
     * @throws BookieException.LedgerFencedException if the ledger is fenced
     */
    public void addEntry(ByteBuffer entry, WriteCallback cb, Object ctx, byte[] masterKey)
            throws IOException, BookieException {
        LedgerDescriptor handle = getLedgerForEntry(entry, masterKey);
        synchronized (handle) {
            try {
                if (handle.isFenced()) {
                    throw BookieException.create(BookieException.Code.LedgerFencedException);
                }
                
                addEntryInternal(handle, entry, cb, ctx);
            } finally {
                putHandle(handle);
            }
        }
    }

    /**
     * Fences a ledger. From this point on, clients will be unable to 
     * write to this ledger. Only recoveryAddEntry will be
     * able to add entries to the ledger.
     * This method is idempotent. Once a ledger is fenced, it can
     * never be unfenced. Fencing a fenced ledger has no effect.
     */
    public void fenceLedger(long ledgerId) throws IOException {
        LedgerDescriptor handle = getHandle(ledgerId, true);
        synchronized (handle) {
            handle.setFenced();
        }
    }

    public ByteBuffer readEntry(long ledgerId, long entryId) throws IOException {
        LedgerDescriptor handle = getHandle(ledgerId, true);
        try {
            if (LOG.isTraceEnabled()) {
                LOG.trace("Reading " + entryId + "@" + ledgerId);
            }
            return handle.readEntry(entryId);
        } finally {
            putHandle(handle);
        }
    }

    // The rest of the code is test stuff
    static class CounterCallback implements WriteCallback {
        int count;

        synchronized public void writeComplete(int rc, long l, long e, InetSocketAddress addr, Object ctx) {
            count--;
            if (count == 0) {
                notifyAll();
            }
        }

        synchronized public void incCount() {
            count++;
        }

        synchronized public void waitZero() throws InterruptedException {
            while (count > 0) {
                wait();
            }
        }
    }

    /**
     * @param args
     * @throws IOException
     * @throws InterruptedException
     */
    public static void main(String[] args) 
            throws IOException, InterruptedException, BookieException, KeeperException {
        Bookie b = new Bookie(new ServerConfiguration());
        CounterCallback cb = new CounterCallback();
        long start = System.currentTimeMillis();
        for (int i = 0; i < 100000; i++) {
            ByteBuffer buff = ByteBuffer.allocate(1024);
            buff.putLong(1);
            buff.putLong(i);
            buff.limit(1024);
            buff.position(0);
            cb.incCount();
            b.addEntry(buff, cb, null, new byte[0]);
        }
        cb.waitZero();
        long end = System.currentTimeMillis();
        System.out.println("Took " + (end-start) + "ms");
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/BookieException.java,true,"package org.apache.bookkeeper.bookie;

/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */


import java.lang.Exception;

@SuppressWarnings("serial")
public abstract class BookieException extends Exception {

    private int code;
    public BookieException(int code) {
        this.code = code;
    }

    public static BookieException create(int code) {
        switch(code) {
        case Code.UnauthorizedAccessException:
            return new BookieUnauthorizedAccessException();
        case Code.LedgerFencedException:
            return new LedgerFencedException();
        default:
            return new BookieIllegalOpException();
        }
    }

    public interface Code {
        int OK = 0;
        int UnauthorizedAccessException = -1;

        int IllegalOpException = -100;
        int LedgerFencedException = -101;
    }

    public void setCode(int code) {
        this.code = code;
    }

    public int getCode() {
        return this.code;
    }

    public String getMessage(int code) {
        switch(code) {
        case Code.OK:
            return "No problem";
        case Code.UnauthorizedAccessException:
            return "Error while reading ledger";
        case Code.LedgerFencedException:
            return "Ledger has been fenced; No more entries can be added";
        default:
            return "Invalid operation";
        }
    }

    public static class BookieUnauthorizedAccessException extends BookieException {
        public BookieUnauthorizedAccessException() {
            super(Code.UnauthorizedAccessException);
        }
    }

    public static class BookieIllegalOpException extends BookieException {
        public BookieIllegalOpException() {
            super(Code.UnauthorizedAccessException);
        }
    }

    public static class LedgerFencedException extends BookieException {
        public LedgerFencedException() {
            super(Code.LedgerFencedException);
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/BufferedChannel.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

import java.io.IOException;
import java.nio.ByteBuffer;
import java.nio.channels.FileChannel;

/**
 * Provides a buffering layer in front of a FileChannel.
 */
public class BufferedChannel
{
    ByteBuffer writeBuffer;
    ByteBuffer readBuffer;
    private FileChannel bc;
    long position;
    int capacity;
    long readBufferStartPosition;
    long writeBufferStartPosition;
    // make constructor to be public for unit test
    public BufferedChannel(FileChannel bc, int capacity) throws IOException {
        this.bc = bc;
        this.capacity = capacity;
        position = bc.position();
        writeBufferStartPosition = position;
    }

    /**
     * @return file channel
     */
    FileChannel getFileChannel() {
        return this.bc;
    }

    /*    public void close() throws IOException {
            bc.close();
        }
    */
//    public boolean isOpen() {
//        return bc.isOpen();
//    }

    synchronized public int write(ByteBuffer src) throws IOException {
        int copied = 0;
        if (writeBuffer == null) {
            writeBuffer = ByteBuffer.allocateDirect(capacity);
        }
        while(src.remaining() > 0) {
            int truncated = 0;
            if (writeBuffer.remaining() < src.remaining()) {
                truncated = src.remaining() - writeBuffer.remaining();
                src.limit(src.limit()-truncated);
            }
            copied += src.remaining();
            writeBuffer.put(src);
            src.limit(src.limit()+truncated);
            if (writeBuffer.remaining() == 0) {
                writeBuffer.flip();
                bc.write(writeBuffer);
                writeBuffer.clear();
                writeBufferStartPosition = bc.position();
            }
        }
        position += copied;
        return copied;
    }

    public long position() {
        return position;
    }

    /**
     * Retrieve the current size of the underlying FileChannel
     *
     * @return FileChannel size measured in bytes
     *
     * @throws IOException if some I/O error occurs reading the FileChannel
     */
    public long size() throws IOException {
        return bc.size();
    }

    public void flush(boolean sync) throws IOException {
        synchronized(this) {
            if (writeBuffer == null) {
                return;
            }
            writeBuffer.flip();
            bc.write(writeBuffer);
            writeBuffer.clear();
            writeBufferStartPosition = bc.position();
        }
        if (sync) {
            bc.force(false);
        }
    }

    /*public Channel getInternalChannel() {
        return bc;
    }*/
    synchronized public int read(ByteBuffer buff, long pos) throws IOException {
        if (readBuffer == null) {
            readBuffer = ByteBuffer.allocateDirect(capacity);
            readBufferStartPosition = Long.MIN_VALUE;
        }
        long prevPos = pos;
        while(buff.remaining() > 0) {
            // check if it is in the write buffer
            if (writeBuffer != null && writeBufferStartPosition <= pos) {
                long positionInBuffer = pos - writeBufferStartPosition;
                long bytesToCopy = writeBuffer.position()-positionInBuffer;
                if (bytesToCopy > buff.remaining()) {
                    bytesToCopy = buff.remaining();
                }
                if (bytesToCopy == 0) {
                    throw new IOException("Read past EOF");
                }
                ByteBuffer src = writeBuffer.duplicate();
                src.position((int) positionInBuffer);
                src.limit((int) (positionInBuffer+bytesToCopy));
                buff.put(src);
                pos+= bytesToCopy;
            } else if (writeBuffer == null && writeBufferStartPosition <= pos) {
                // here we reach the end
                break;
                // first check if there is anything we can grab from the readBuffer
            } else if (readBufferStartPosition <= pos && pos < readBufferStartPosition+readBuffer.capacity()) {
                long positionInBuffer = pos - readBufferStartPosition;
                long bytesToCopy = readBuffer.capacity()-positionInBuffer;
                if (bytesToCopy > buff.remaining()) {
                    bytesToCopy = buff.remaining();
                }
                ByteBuffer src = readBuffer.duplicate();
                src.position((int) positionInBuffer);
                src.limit((int) (positionInBuffer+bytesToCopy));
                buff.put(src);
                pos += bytesToCopy;
                // let's read it
            } else {
                readBufferStartPosition = pos;
                readBuffer.clear();
                // make sure that we don't overlap with the write buffer
                if (readBufferStartPosition + readBuffer.capacity() >= writeBufferStartPosition) {
                    readBufferStartPosition = writeBufferStartPosition - readBuffer.capacity();
                    if (readBufferStartPosition < 0) {
                        readBuffer.put(LedgerEntryPage.zeroPage, 0, (int)-readBufferStartPosition);
                    }
                }
                while(readBuffer.remaining() > 0) {
                    if (bc.read(readBuffer, readBufferStartPosition+readBuffer.position()) <= 0) {
                        throw new IOException("Short read");
                    }
                }
                readBuffer.put(LedgerEntryPage.zeroPage, 0, readBuffer.remaining());
                readBuffer.clear();
            }
        }
        return (int)(pos - prevPos);
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/EntryLogger.java,true,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.OutputStreamWriter;
import java.io.RandomAccessFile;
import java.nio.ByteBuffer;
import java.nio.channels.FileChannel;
import java.util.Arrays;
import java.util.Collections;
import java.util.List;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ConcurrentMap;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.apache.bookkeeper.conf.ServerConfiguration;
import org.apache.bookkeeper.meta.LedgerManager;

/**
 * This class manages the writing of the bookkeeper entries. All the new
 * entries are written to a common log. The LedgerCache will have pointers
 * into files created by this class with offsets into the files to find
 * the actual ledger entry. The entry log files created by this class are
 * identified by a long.
 */
public class EntryLogger {
    private static final Logger LOG = LoggerFactory.getLogger(EntryLogger.class);
    private File dirs[];
    // This is a handle to the Bookie parent instance. We need this to get
    // access to the LedgerCache as well as the ZooKeeper client handle.
    private final Bookie bookie;

    private long logId;
    /**
     * The maximum size of a entry logger file.
     */
    final long logSizeLimit;
    private volatile BufferedChannel logChannel;
    /**
     * The 1K block at the head of the entry logger file
     * that contains the fingerprint and (future) meta-data
     */
    final static int LOGFILE_HEADER_SIZE = 1024;
    final ByteBuffer LOGFILE_HEADER = ByteBuffer.allocate(LOGFILE_HEADER_SIZE);

    // this indicates that a write has happened since the last flush
    private volatile boolean somethingWritten = false;

    // Maps entry log files to the set of ledgers that comprise the file.
    private ConcurrentMap<Long, ConcurrentHashMap<Long, Boolean>> entryLogs2LedgersMap = new ConcurrentHashMap<Long, ConcurrentHashMap<Long, Boolean>>();
    // This is the thread that garbage collects the entry logs that do not
    // contain any active ledgers in them.
    GarbageCollectorThread gcThread = new GarbageCollectorThread();
    // This is how often we want to run the Garbage Collector Thread (in milliseconds).
    final long gcWaitTime;

    /**
     * Create an EntryLogger that stores it's log files in the given
     * directories
     */
    public EntryLogger(ServerConfiguration conf, Bookie bookie) throws IOException {
        this.dirs = conf.getLedgerDirs();
        this.bookie = bookie;
        // log size limit
        this.logSizeLimit = conf.getEntryLogSizeLimit();
        this.gcWaitTime = conf.getGcWaitTime();
        // Initialize the entry log header buffer. This cannot be a static object
        // since in our unit tests, we run multiple Bookies and thus EntryLoggers
        // within the same JVM. All of these Bookie instances access this header
        // so there can be race conditions when entry logs are rolled over and
        // this header buffer is cleared before writing it into the new logChannel.
        LOGFILE_HEADER.put("BKLO".getBytes());
        // Find the largest logId
        for(File f: dirs) {
            long lastLogId = getLastLogId(f);
            if (lastLogId >= logId) {
                logId = lastLogId+1;
            }
        }
        createLogId(logId);
        // Start the Garbage Collector thread to prune unneeded entry logs.
        gcThread.start();
    }

    /**
     * Maps entry log files to open channels.
     */
    private ConcurrentHashMap<Long, BufferedChannel> channels = new ConcurrentHashMap<Long, BufferedChannel>();

    /**
     * This is the garbage collector thread that runs in the background to
     * remove any entry log files that no longer contains any active ledger.
     */
    class GarbageCollectorThread extends Thread {
        volatile boolean running = true;

        public GarbageCollectorThread() {
            super("GarbageCollectorThread");
        }

        @Override
        public void run() {
            while (running) {
                synchronized (this) {
                    try {
                        wait(gcWaitTime);
                    } catch (InterruptedException e) {
                        Thread.currentThread().interrupt();
                        continue;
                    }
                }
                // Initialization check. No need to run any logic if we are still starting up.
                if (bookie.zk == null || entryLogs2LedgersMap.isEmpty() ||
                    bookie.ledgerCache == null) {
                    continue;
                }

                // gc inactive/deleted ledgers
                doGcLedgers();

                // gc entry logs
                doGcEntryLogs();
            }
        }

        /**
         * Do garbage collection ledger index files
         */
        private void doGcLedgers() {
            bookie.ledgerCache.activeLedgerManager.garbageCollectLedgers(
            new LedgerManager.GarbageCollector() {
                @Override
                public void gc(long ledgerId) {
                    try {
                        bookie.ledgerCache.deleteLedger(ledgerId);
                    } catch (IOException e) {
                        LOG.error("Exception when deleting the ledger index file on the Bookie: ", e);
                    }
                }
            });
        }

        /**
         * Garbage collect those entry loggers which are not associated with any active ledgers
         */
        private void doGcEntryLogs() {
            // Loop through all of the entry logs and remove the non-active ledgers.
            for (Long entryLogId : entryLogs2LedgersMap.keySet()) {
                ConcurrentHashMap<Long, Boolean> entryLogLedgers = entryLogs2LedgersMap.get(entryLogId);
                for (Long entryLogLedger : entryLogLedgers.keySet()) {
                    // Remove the entry log ledger from the set if it isn't active.
                    if (!bookie.ledgerCache.activeLedgerManager.containsActiveLedger(entryLogLedger)) {
                        entryLogLedgers.remove(entryLogLedger);
                    }
                }
                if (entryLogLedgers.isEmpty()) {
                    // This means the entry log is not associated with any active ledgers anymore.
                    // We can remove this entry log file now.
                    LOG.info("Deleting entryLogId " + entryLogId + " as it has no active ledgers!");
                    BufferedChannel bc = channels.remove(entryLogId);
                    if (null != bc) {
                        // close its underlying file channel, so it could be deleted really
                        try {
                            bc.getFileChannel().close();
                        } catch (IOException ie) {
                            LOG.warn("Exception while closing garbage collected entryLog file : ", ie);
                        }
                    }
                    File entryLogFile;
                    try {
                        entryLogFile = findFile(entryLogId);
                    } catch (FileNotFoundException e) {
                        LOG.error("Trying to delete an entryLog file that could not be found: "
                                + entryLogId + ".log");
                        continue;
                    }
                    entryLogFile.delete();
                    entryLogs2LedgersMap.remove(entryLogId);
                }
            }
        }
    }

    /**
     * Creates a new log file with the given id.
     */
    private void createLogId(long logId) throws IOException {
        List<File> list = Arrays.asList(dirs);
        Collections.shuffle(list);
        File firstDir = list.get(0);
        if (logChannel != null) {
            logChannel.flush(true);
        }
        logChannel = new BufferedChannel(new RandomAccessFile(new File(firstDir, Long.toHexString(logId)+".log"), "rw").getChannel(), 64*1024);
        logChannel.write((ByteBuffer) LOGFILE_HEADER.clear());
        channels.put(logId, logChannel);
        for(File f: dirs) {
            setLastLogId(f, logId);
        }
        // Extract all of the ledger ID's that comprise all of the entry logs
        // (except for the current new one which is still being written to).
        extractLedgersFromEntryLogs();
    }

    /**
     * writes the given id to the "lastId" file in the given directory.
     */
    private void setLastLogId(File dir, long logId) throws IOException {
        FileOutputStream fos;
        fos = new FileOutputStream(new File(dir, "lastId"));
        BufferedWriter bw = new BufferedWriter(new OutputStreamWriter(fos));
        try {
            bw.write(Long.toHexString(logId) + "\n");
            bw.flush();
        } finally {
            try {
                fos.close();
            } catch (IOException e) {
            }
        }
    }

    /**
     * reads id from the "lastId" file in the given directory.
     */
    private long getLastLogId(File f) {
        FileInputStream fis;
        try {
            fis = new FileInputStream(new File(f, "lastId"));
        } catch (FileNotFoundException e) {
            return -1;
        }
        BufferedReader br = new BufferedReader(new InputStreamReader(fis));
        try {
            String lastIdString = br.readLine();
            return Long.parseLong(lastIdString);
        } catch (IOException e) {
            return -1;
        } catch(NumberFormatException e) {
            return -1;
        } finally {
            try {
                fis.close();
            } catch (IOException e) {
            }
        }
    }

    private void openNewChannel() throws IOException {
        createLogId(++logId);
    }

    synchronized void flush() throws IOException {
        if (logChannel != null) {
            logChannel.flush(true);
        }
    }
    synchronized long addEntry(long ledger, ByteBuffer entry) throws IOException {
        if (logChannel.position() + entry.remaining() + 4 > logSizeLimit) {
            openNewChannel();
        }
        ByteBuffer buff = ByteBuffer.allocate(4);
        buff.putInt(entry.remaining());
        buff.flip();
        logChannel.write(buff);
        long pos = logChannel.position();
        logChannel.write(entry);
        //logChannel.flush(false);
        somethingWritten = true;
        return (logId << 32L) | pos;
    }

    byte[] readEntry(long ledgerId, long entryId, long location) throws IOException {
        long entryLogId = location >> 32L;
        long pos = location & 0xffffffffL;
        ByteBuffer sizeBuff = ByteBuffer.allocate(4);
        pos -= 4; // we want to get the ledgerId and length to check
        BufferedChannel fc;
        try {
            fc = getChannelForLogId(entryLogId);
        } catch (FileNotFoundException e) {
            FileNotFoundException newe = new FileNotFoundException(e.getMessage() + " for " + ledgerId + " with location " + location);
            newe.setStackTrace(e.getStackTrace());
            throw newe;
        }
        if (fc.read(sizeBuff, pos) != sizeBuff.capacity()) {
            throw new IOException("Short read from entrylog " + entryLogId);
        }
        pos += 4;
        sizeBuff.flip();
        int entrySize = sizeBuff.getInt();
        // entrySize does not include the ledgerId
        if (entrySize > 1024*1024) {
            LOG.error("Sanity check failed for entry size of " + entrySize + " at location " + pos + " in " + entryLogId);

        }
        byte data[] = new byte[entrySize];
        ByteBuffer buff = ByteBuffer.wrap(data);
        int rc = fc.read(buff, pos);
        if ( rc != data.length) {
            throw new IOException("Short read for " + ledgerId + "@" + entryId + " in " + entryLogId + "@" + pos + "("+rc+"!="+data.length+")");
        }
        buff.flip();
        long thisLedgerId = buff.getLong();
        if (thisLedgerId != ledgerId) {
            throw new IOException("problem found in " + entryLogId + "@" + entryId + " at position + " + pos + " entry belongs to " + thisLedgerId + " not " + ledgerId);
        }
        long thisEntryId = buff.getLong();
        if (thisEntryId != entryId) {
            throw new IOException("problem found in " + entryLogId + "@" + entryId + " at position + " + pos + " entry is " + thisEntryId + " not " + entryId);
        }

        return data;
    }

    private BufferedChannel getChannelForLogId(long entryLogId) throws IOException {
        BufferedChannel fc = channels.get(entryLogId);
        if (fc != null) {
            return fc;
        }
        File file = findFile(entryLogId);
        FileChannel newFc = new RandomAccessFile(file, "rw").getChannel();
        // If the file already exists before creating a BufferedChannel layer above it,
        // set the FileChannel's position to the end so the write buffer knows where to start.
        newFc.position(newFc.size());
        synchronized (channels) {
            fc = channels.get(entryLogId);
            if (fc != null) {
                newFc.close();
                return fc;
            }
            fc = new BufferedChannel(newFc, 8192);
            channels.put(entryLogId, fc);
            return fc;
        }
    }

    private File findFile(long logId) throws FileNotFoundException {
        for(File d: dirs) {
            File f = new File(d, Long.toHexString(logId)+".log");
            if (f.exists()) {
                return f;
            }
        }
        throw new FileNotFoundException("No file for log " + Long.toHexString(logId));
    }

    synchronized public boolean testAndClearSomethingWritten() {
        try {
            return somethingWritten;
        } finally {
            somethingWritten = false;
        }
    }

    /**
     * Method to read in all of the entry logs (those that we haven't done so yet),
     * and find the set of ledger ID's that make up each entry log file.
     */
    private void extractLedgersFromEntryLogs() throws IOException {
        // Extract it for every entry log except for the current one.
        // Entry Log ID's are just a long value that starts at 0 and increments
        // by 1 when the log fills up and we roll to a new one.
        ByteBuffer sizeBuff = ByteBuffer.allocate(4);
        BufferedChannel bc;
        for (long entryLogId = 0; entryLogId < logId; entryLogId++) {
            // Comb the current entry log file if it has not already been extracted.
            if (entryLogs2LedgersMap.containsKey(entryLogId)) {
                continue;
            }
            LOG.info("Extracting the ledgers from entryLogId: " + entryLogId);
            // Get the BufferedChannel for the current entry log file
            try {
                bc = getChannelForLogId(entryLogId);
            } catch (FileNotFoundException e) {
                // If we can't find the entry log file, just log a warning message and continue.
                // This could be a deleted/garbage collected entry log.
                LOG.warn("Entry Log file not found in log directories: " + entryLogId + ".log");
                continue;
            }
            // Start the read position in the current entry log file to be after
            // the header where all of the ledger entries are.
            long pos = LOGFILE_HEADER_SIZE;
            ConcurrentHashMap<Long, Boolean> entryLogLedgers = new ConcurrentHashMap<Long, Boolean>();
            // Read through the entry log file and extract the ledger ID's.
            try {
                while (true) {
                    // Check if we've finished reading the entry log file.
                    if (pos >= bc.size()) {
                        break;
                    }
                    if (bc.read(sizeBuff, pos) != sizeBuff.capacity()) {
                        throw new IOException("Short read from entrylog " + entryLogId);
                    }
                    pos += 4;
                    sizeBuff.flip();
                    int entrySize = sizeBuff.getInt();
                    if (entrySize > 1024 * 1024) {
                        LOG.error("Sanity check failed for entry size of " + entrySize + " at location " + pos + " in "
                                + entryLogId);
                    }
                    byte data[] = new byte[entrySize];
                    ByteBuffer buff = ByteBuffer.wrap(data);
                    int rc = bc.read(buff, pos);
                    if (rc != data.length) {
                        throw new IOException("Short read for entryLog " + entryLogId + "@" + pos + "(" + rc + "!="
                                + data.length + ")");
                    }
                    buff.flip();
                    long ledgerId = buff.getLong();
                    entryLogLedgers.put(ledgerId, true);
                    // Advance position to the next entry and clear sizeBuff.
                    pos += entrySize;
                    sizeBuff.clear();
                }
            } catch(IOException e) {
              LOG.info("Premature exception when processing " + entryLogId + 
                       "recovery will take care of the problem", e);
            }
            LOG.info("Retrieved all ledgers that comprise entryLogId: " + entryLogId + ", values: " + entryLogLedgers);
            entryLogs2LedgersMap.put(entryLogId, entryLogLedgers);
        }
    }

    /**
     * Shutdown method to gracefully stop all threads spawned in this class and exit.
     *
     * @throws InterruptedException if there is an exception stopping threads.
     */
    public void shutdown() throws InterruptedException {
        gcThread.running = false;
        gcThread.interrupt();
        gcThread.join();
        // since logChannel is buffered channel, do flush when shutting down
        try {
            flush();
        } catch (IOException ie) {
            // we have no idea how to avoid io exception during shutting down, so just ignore it
            LOG.error("Error flush entry log during shutting down, which may cause entry log corrupted.", ie);
        }
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/FileInfo.java,true,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

import java.io.File;
import java.io.IOException;
import java.io.RandomAccessFile;
import java.nio.ByteBuffer;
import java.nio.channels.FileChannel;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * This is the file handle for a ledger's index file that maps entry ids to location.
 * It is used by LedgerCache.
 *
 * <p>
 * Ledger index file is made of a header and several fixed-length index pages, which records the offsets of data stored in entry loggers
 * <pre>&lt;header&gt;&lt;index pages&gt;</pre>
 * <b>Header</b> is formated as below:
 * <pre>&lt;magic bytes&gt;&lt;len of master key&gt;&lt;master key&gt;</pre>
 * <ul>
 * <li>magic bytes: 8 bytes, 'BKLE\0\0\0\0'
 * <li>len of master key: indicates length of master key. -1 means no master key stored in header.
 * <li>master key: master key
 * </ul>
 * <b>Index page</b> is a fixed-length page, which contains serveral entries which point to the offsets of data stored in entry loggers.
 * </p>
 */
class FileInfo {
    static Logger LOG = LoggerFactory.getLogger(FileInfo.class);

    static final int NO_MASTER_KEY = -1;

    private FileChannel fc;
    private final File lf;
    /**
     * The fingerprint of a ledger index file
     */
    private byte header[] = "BKLE\0\0\0\0".getBytes();
    static final long START_OF_DATA = 1024;
    private long size;
    private int useCount;
    private boolean isClosed;
    public FileInfo(File lf) throws IOException {
        this.lf = lf;
        fc = new RandomAccessFile(lf, "rws").getChannel();
        size = fc.size();
        if (size == 0) {
            fc.write(ByteBuffer.wrap(header));
            // write NO_MASTER_KEY, which means there is no master key
            ByteBuffer buf = ByteBuffer.allocate(4);
            buf.putInt(NO_MASTER_KEY);
            buf.flip();
            fc.write(buf);
        }
    }

    /**
     * Write master key to index file header
     *
     * @param masterKey master key to store
     * @return void
     * @throws IOException
     */
    synchronized public void writeMasterKey(byte[] masterKey) throws IOException {
        // write master key
        if (masterKey == null ||
            masterKey.length + 4 + header.length > START_OF_DATA) {
            throw new IOException("master key is more than " + (START_OF_DATA - 4 - header.length));
        }

        int len = masterKey.length;
        ByteBuffer lenBuf = ByteBuffer.allocate(4);
        lenBuf.putInt(len);
        lenBuf.flip();
        fc.position(header.length);
        fc.write(lenBuf);
        fc.write(ByteBuffer.wrap(masterKey));
    }

    /**
     * Read master key
     *
     * @return master key. null means no master key stored in index header
     * @throws IOException
     */
    synchronized public byte[] readMasterKey() throws IOException {
        ByteBuffer lenBuf = ByteBuffer.allocate(4);
        int total = readAbsolute(lenBuf, header.length);
        if (total != 4) {
            throw new IOException("Short read during reading master key length");
        }
        lenBuf.rewind();
        int len = lenBuf.getInt();
        if (len == NO_MASTER_KEY) {
            return null;
        }

        byte[] masterKey = new byte[len];
        total = readAbsolute(ByteBuffer.wrap(masterKey), header.length + 4);
        if (total != len) {
            throw new IOException("Short read during reading master key");
        }
        return masterKey;
    }

    synchronized public long size() {
        long rc = size-START_OF_DATA;
        if (rc < 0) {
            rc = 0;
        }
        return rc;
    }

    synchronized public int read(ByteBuffer bb, long position) throws IOException {
        return readAbsolute(bb, position + START_OF_DATA);
    }

    private int readAbsolute(ByteBuffer bb, long start) throws IOException {
        int total = 0;
        while(bb.remaining() > 0) {
            int rc = fc.read(bb, start);
            if (rc <= 0) {
                throw new IOException("Short read");
            }
            total += rc;
            // should move read position
            start += rc;
        }
        return total;
    }

    synchronized public void close() throws IOException {
        isClosed = true;
        if (useCount == 0) {
            fc.close();
        }
    }

    synchronized public long write(ByteBuffer[] buffs, long position) throws IOException {
        long total = 0;
        try {
            fc.position(position+START_OF_DATA);
            while(buffs[buffs.length-1].remaining() > 0) {
                long rc = fc.write(buffs);
                if (rc <= 0) {
                    throw new IOException("Short write");
                }
                total += rc;
            }
        } finally {
            long newsize = position+START_OF_DATA+total;
            if (newsize > size) {
                size = newsize;
            }
        }
        return total;
    }

    synchronized public void use() {
        useCount++;
    }

    synchronized public void release() {
        useCount--;
        if (isClosed && useCount == 0) {
            try {
                fc.close();
            } catch (IOException e) {
                LOG.error("Error closing file channel", e);
            }
        }
    }

    /**
     * Getter to a handle on the actual ledger index file.
     * This is used when we are deleting a ledger and want to physically remove the index file.
     */
    File getFile() {
        return lf;
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/LedgerCache.java,true,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

import java.io.File;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Comparator;
import java.util.HashMap;
import java.util.Iterator;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.Random;

import org.apache.bookkeeper.meta.LedgerManager;
import org.apache.bookkeeper.conf.ServerConfiguration;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * This class maps a ledger entry number into a location (entrylogid, offset) in
 * an entry log file. It does user level caching to more efficiently manage disk
 * head scheduling.
 */
public class LedgerCache {
    private final static Logger LOG = LoggerFactory.getLogger(LedgerDescriptor.class);

    final File ledgerDirectories[];

    public LedgerCache(ServerConfiguration conf, LedgerManager alm) {
        this.ledgerDirectories = conf.getLedgerDirs();
        this.openFileLimit = conf.getOpenFileLimit();
        this.pageSize = conf.getPageSize();
        this.entriesPerPage = pageSize / 8;
        
        if (conf.getPageLimit() <= 0) {
            // allocate half of the memory to the page cache
            this.pageLimit = (int)((Runtime.getRuntime().maxMemory() / 3) / this.pageSize);
        } else {
            this.pageLimit = conf.getPageLimit();
        }
        LOG.info("maxMemory = " + Runtime.getRuntime().maxMemory());
        LOG.info("openFileLimit is " + openFileLimit + ", pageSize is " + pageSize + ", pageLimit is " + pageLimit);
        activeLedgerManager = alm;
        // Retrieve all of the active ledgers.
        getActiveLedgers();
    }
    /**
     * the list of potentially clean ledgers
     */
    LinkedList<Long> cleanLedgers = new LinkedList<Long>();

    /**
     * the list of potentially dirty ledgers
     */
    LinkedList<Long> dirtyLedgers = new LinkedList<Long>();

    HashMap<Long, FileInfo> fileInfoCache = new HashMap<Long, FileInfo>();

    LinkedList<Long> openLedgers = new LinkedList<Long>();

    // Manage all active ledgers in LedgerManager
    // so LedgerManager has knowledge to garbage collect inactive/deleted ledgers
    final LedgerManager activeLedgerManager;

    final int openFileLimit;
    final int pageSize;
    final int pageLimit;
    final int entriesPerPage;

    /**
     * @return page size used in ledger cache
     */
    public int getPageSize() {
        return pageSize;
    }

    /**
     * @return entries per page used in ledger cache
     */
    public int getEntriesPerPage() {
        return entriesPerPage;
    }

    // The number of pages that have actually been used
    private int pageCount = 0;
    HashMap<Long, HashMap<Long,LedgerEntryPage>> pages = new HashMap<Long, HashMap<Long,LedgerEntryPage>>();

    private void putIntoTable(HashMap<Long, HashMap<Long,LedgerEntryPage>> table, LedgerEntryPage lep) {
        HashMap<Long, LedgerEntryPage> map = table.get(lep.getLedger());
        if (map == null) {
            map = new HashMap<Long, LedgerEntryPage>();
            table.put(lep.getLedger(), map);
        }
        map.put(lep.getFirstEntry(), lep);
    }

    private static LedgerEntryPage getFromTable(HashMap<Long, HashMap<Long,LedgerEntryPage>> table, Long ledger, Long firstEntry) {
        HashMap<Long, LedgerEntryPage> map = table.get(ledger);
        if (map != null) {
            return map.get(firstEntry);
        }
        return null;
    }

    synchronized private LedgerEntryPage getLedgerEntryPage(Long ledger, Long firstEntry, boolean onlyDirty) {
        LedgerEntryPage lep = getFromTable(pages, ledger, firstEntry);
        try {
            if (onlyDirty && lep.isClean()) {
                return null;
            }
            return lep;
        } finally {
            if (lep != null) {
                lep.usePage();
            }
        }
    }

    public void putEntryOffset(long ledger, long entry, long offset) throws IOException {
        int offsetInPage = (int) (entry % entriesPerPage);
        // find the id of the first entry of the page that has the entry
        // we are looking for
        long pageEntry = entry-offsetInPage;
        LedgerEntryPage lep = getLedgerEntryPage(ledger, pageEntry, false);
        if (lep == null) {
            // find a free page
            lep = grabCleanPage(ledger, pageEntry);
            updatePage(lep);
            synchronized(this) {
                putIntoTable(pages, lep);
            }
        }
        if (lep != null) {
            lep.setOffset(offset, offsetInPage*8);
            lep.releasePage();
            return;
        }
    }

    public long getEntryOffset(long ledger, long entry) throws IOException {
        int offsetInPage = (int) (entry%entriesPerPage);
        // find the id of the first entry of the page that has the entry
        // we are looking for
        long pageEntry = entry-offsetInPage;
        LedgerEntryPage lep = getLedgerEntryPage(ledger, pageEntry, false);
        try {
            if (lep == null) {
                lep = grabCleanPage(ledger, pageEntry);
                synchronized(this) {
                    putIntoTable(pages, lep);
                }
                updatePage(lep);

            }
            return lep.getOffset(offsetInPage*8);
        } finally {
            if (lep != null) {
                lep.releasePage();
            }
        }
    }

    static final private String getLedgerName(long ledgerId) {
        int parent = (int) (ledgerId & 0xff);
        int grandParent = (int) ((ledgerId & 0xff00) >> 8);
        StringBuilder sb = new StringBuilder();
        sb.append(Integer.toHexString(grandParent));
        sb.append('/');
        sb.append(Integer.toHexString(parent));
        sb.append('/');
        sb.append(Long.toHexString(ledgerId));
        sb.append(".idx");
        return sb.toString();
    }

    static final private void checkParents(File f) throws IOException {
        File parent = f.getParentFile();
        if (parent.exists()) {
            return;
        }
        if (parent.mkdirs() == false) {
            throw new IOException("Counldn't mkdirs for " + parent);
        }
    }

    static final private Random rand = new Random();

    static final private File pickDirs(File dirs[]) {
        return dirs[rand.nextInt(dirs.length)];
    }

    FileInfo getFileInfo(Long ledger, boolean create) throws IOException {
        synchronized(fileInfoCache) {
            FileInfo fi = fileInfoCache.get(ledger);
            if (fi == null) {
                String ledgerName = getLedgerName(ledger);
                File lf = null;
                for(File d: ledgerDirectories) {
                    lf = new File(d, ledgerName);
                    if (lf.exists()) {
                        break;
                    }
                    lf = null;
                }
                if (lf == null) {
                    if (!create) {
                        throw new Bookie.NoLedgerException(ledger);
                    }
                    File dir = pickDirs(ledgerDirectories);
                    lf = new File(dir, ledgerName);
                    checkParents(lf);
                    // A new ledger index file has been created for this Bookie.
                    // Add this new ledger to the set of active ledgers.
                    if (LOG.isDebugEnabled()) {
                        LOG.debug("New ledger index file created for ledgerId: " + ledger);
                    }
                    activeLedgerManager.addActiveLedger(ledger, true);
                }
                if (openLedgers.size() > openFileLimit) {
                    fileInfoCache.remove(openLedgers.removeFirst()).close();
                }
                fi = new FileInfo(lf);
                fileInfoCache.put(ledger, fi);
                openLedgers.add(ledger);
            }
            if (fi != null) {
                fi.use();
            }
            return fi;
        }
    }
    private void updatePage(LedgerEntryPage lep) throws IOException {
        if (!lep.isClean()) {
            throw new IOException("Trying to update a dirty page");
        }
        FileInfo fi = null;
        try {
            fi = getFileInfo(lep.getLedger(), true);
            long pos = lep.getFirstEntry()*8;
            if (pos >= fi.size()) {
                lep.zeroPage();
            } else {
                lep.readPage(fi);
            }
        } finally {
            if (fi != null) {
                fi.release();
            }
        }
    }

    void flushLedger(boolean doAll) throws IOException {
        synchronized(dirtyLedgers) {
            if (dirtyLedgers.isEmpty()) {
                synchronized(this) {
                    for(Long l: pages.keySet()) {
                        if (LOG.isTraceEnabled()) {
                            LOG.trace("Adding " + Long.toHexString(l) + " to dirty pages");
                        }
                        dirtyLedgers.add(l);
                    }
                }
            }
            if (dirtyLedgers.isEmpty()) {
                return;
            }
            while(!dirtyLedgers.isEmpty()) {
                Long l = dirtyLedgers.removeFirst();
                LinkedList<Long> firstEntryList;
                synchronized(this) {
                    HashMap<Long, LedgerEntryPage> pageMap = pages.get(l);
                    if (pageMap == null || pageMap.isEmpty()) {
                        continue;
                    }
                    firstEntryList = new LinkedList<Long>();
                    for(Map.Entry<Long, LedgerEntryPage> entry: pageMap.entrySet()) {
                        LedgerEntryPage lep = entry.getValue();
                        if (lep.isClean()) {
                            if (LOG.isTraceEnabled()) {
                                LOG.trace("Page is clean " + lep);
                            }
                            continue;
                        }
                        firstEntryList.add(lep.getFirstEntry());
                    }
                }
                // Now flush all the pages of a ledger
                List<LedgerEntryPage> entries = new ArrayList<LedgerEntryPage>(firstEntryList.size());
                FileInfo fi = null;
                try {
                    for(Long firstEntry: firstEntryList) {
                        LedgerEntryPage lep = getLedgerEntryPage(l, firstEntry, true);
                        if (lep != null) {
                            entries.add(lep);
                        }
                    }
                    Collections.sort(entries, new Comparator<LedgerEntryPage>() {
                        @Override
                        public int compare(LedgerEntryPage o1, LedgerEntryPage o2) {
                            return (int)(o1.getFirstEntry()-o2.getFirstEntry());
                        }
                    });
                    ArrayList<Integer> versions = new ArrayList<Integer>(entries.size());
                    fi = getFileInfo(l, true);
                    int start = 0;
                    long lastOffset = -1;
                    for(int i = 0; i < entries.size(); i++) {
                        versions.add(i, entries.get(i).getVersion());
                        if (lastOffset != -1 && (entries.get(i).getFirstEntry() - lastOffset) != entriesPerPage) {
                            // send up a sequential list
                            int count = i - start;
                            if (count == 0) {
                                System.out.println("Count cannot possibly be zero!");
                            }
                            writeBuffers(l, entries, fi, start, count);
                            start = i;
                        }
                        lastOffset = entries.get(i).getFirstEntry();
                    }
                    if (entries.size()-start == 0 && entries.size() != 0) {
                        System.out.println("Nothing to write, but there were entries!");
                    }
                    writeBuffers(l, entries, fi, start, entries.size()-start);
                    synchronized(this) {
                        for(int i = 0; i < entries.size(); i++) {
                            LedgerEntryPage lep = entries.get(i);
                            lep.setClean(versions.get(i));
                        }
                    }
                } finally {
                    for(LedgerEntryPage lep: entries) {
                        lep.releasePage();
                    }
                    if (fi != null) {
                        fi.release();
                    }
                }
                if (!doAll) {
                    break;
                }
                // Yeild. if we are doing all the ledgers we don't want to block other flushes that
                // need to happen
                try {
                    dirtyLedgers.wait(1);
                } catch (InterruptedException e) {
                    // just pass it on
                    Thread.currentThread().interrupt();
                }
            }
        }
    }

    private void writeBuffers(Long ledger,
                              List<LedgerEntryPage> entries, FileInfo fi,
                              int start, int count) throws IOException {
        if (LOG.isTraceEnabled()) {
            LOG.trace("Writing " + count + " buffers of " + Long.toHexString(ledger));
        }
        if (count == 0) {
            //System.out.println("Count is zero!");
            return;
        }
        ByteBuffer buffs[] = new ByteBuffer[count];
        for(int j = 0; j < count; j++) {
            buffs[j] = entries.get(start+j).getPageToWrite();
            if (entries.get(start+j).getLedger() != ledger) {
                throw new IOException("Writing to " + ledger + " but page belongs to " + entries.get(start+j).getLedger());
            }
        }
        long totalWritten = 0;
        while(buffs[buffs.length-1].remaining() > 0) {
            long rc = fi.write(buffs, entries.get(start+0).getFirstEntry()*8);
            if (rc <= 0) {
                throw new IOException("Short write to ledger " + ledger + " rc = " + rc);
            }
            //System.out.println("Wrote " + rc + " to " + ledger);
            totalWritten += rc;
        }
        if (totalWritten != count * pageSize) {
            throw new IOException("Short write to ledger " + ledger + " wrote " + totalWritten + " expected " + count * pageSize);
        }
    }
    private LedgerEntryPage grabCleanPage(long ledger, long entry) throws IOException {
        if (entry % entriesPerPage != 0) {
            throw new IllegalArgumentException(entry + " is not a multiple of " + entriesPerPage);
        }
        synchronized(this) {
            if (pageCount  < pageLimit) {
                // let's see if we can allocate something
                LedgerEntryPage lep = new LedgerEntryPage(pageSize, entriesPerPage);
                lep.setLedger(ledger);
                lep.setFirstEntry(entry);
                // note, this will not block since it is a new page
                lep.usePage();
                pageCount++;
                return lep;
            }
        }

        outerLoop:
        while(true) {
            synchronized(cleanLedgers) {
                if (cleanLedgers.isEmpty()) {
                    flushLedger(false);
                    synchronized(this) {
                        for(Long l: pages.keySet()) {
                            cleanLedgers.add(l);
                        }
                    }
                }
                synchronized(this) {
                    Long cleanLedger = cleanLedgers.getFirst();
                    Map<Long, LedgerEntryPage> map = pages.get(cleanLedger);
                    if (map == null || map.isEmpty()) {
                        cleanLedgers.removeFirst();
                        continue;
                    }
                    Iterator<Map.Entry<Long, LedgerEntryPage>> it = map.entrySet().iterator();
                    LedgerEntryPage lep = it.next().getValue();
                    while((lep.inUse() || !lep.isClean())) {
                        if (!it.hasNext()) {
                            continue outerLoop;
                        }
                        lep = it.next().getValue();
                    }
                    it.remove();
                    if (map.isEmpty()) {
                        pages.remove(lep.getLedger());
                    }
                    lep.usePage();
                    lep.zeroPage();
                    lep.setLedger(ledger);
                    lep.setFirstEntry(entry);
                    return lep;
                }
            }
        }
    }

    public long getLastEntry(long ledgerId) {
        long lastEntry = 0;
        // Find the last entry in the cache
        synchronized(this) {
            Map<Long, LedgerEntryPage> map = pages.get(ledgerId);
            if (map != null) {
                for(LedgerEntryPage lep: map.values()) {
                    if (lep.getFirstEntry() + entriesPerPage < lastEntry) {
                        continue;
                    }
                    lep.usePage();
                    long highest = lep.getLastEntry();
                    if (highest > lastEntry) {
                        lastEntry = highest;
                    }
                    lep.releasePage();
                }
            }
        }

        return lastEntry;
    }

    /**
     * This method will look within the ledger directories for the ledger index
     * files. That will comprise the set of active ledgers this particular
     * BookieServer knows about that have not yet been deleted by the BookKeeper
     * Client. This is called only once during initialization.
     */
    private void getActiveLedgers() {
        // Ledger index files are stored in a file hierarchy with a parent and
        // grandParent directory. We'll have to go two levels deep into these
        // directories to find the index files.
        for (File ledgerDirectory : ledgerDirectories) {
            for (File grandParent : ledgerDirectory.listFiles()) {
                if (grandParent.isDirectory()) {
                    for (File parent : grandParent.listFiles()) {
                        if (parent.isDirectory()) {
                            for (File index : parent.listFiles()) {
                                if (!index.isFile() || !index.getName().endsWith(".idx")) {
                                    continue;
                                }
                                // We've found a ledger index file. The file name is the
                                // HexString representation of the ledgerId.
                                String ledgerIdInHex = index.getName().substring(0, index.getName().length() - 4);
                                activeLedgerManager.addActiveLedger(Long.parseLong(ledgerIdInHex, 16), true);
                            }
                        }
                    }
                }
            }
        }
    }

    /**
     * This method is called whenever a ledger is deleted by the BookKeeper Client
     * and we want to remove all relevant data for it stored in the LedgerCache.
     */
    void deleteLedger(long ledgerId) throws IOException {
        if (LOG.isDebugEnabled())
            LOG.debug("Deleting ledgerId: " + ledgerId);
        // Delete the ledger's index file and close the FileInfo
        FileInfo fi = getFileInfo(ledgerId, false);
        fi.getFile().delete();
        fi.close();

        // Remove it from the active ledger manager
        activeLedgerManager.removeActiveLedger(ledgerId);

        // Now remove it from all the other lists and maps.
        // These data structures need to be synchronized first before removing entries.
        synchronized(this) {
            pages.remove(ledgerId);
        }
        synchronized(fileInfoCache) {
            fileInfoCache.remove(ledgerId);
        }
        synchronized(cleanLedgers) {
            cleanLedgers.remove(ledgerId);
        }
        synchronized(dirtyLedgers) {
            dirtyLedgers.remove(ledgerId);
        }
        synchronized(openLedgers) {
            openLedgers.remove(ledgerId);
        }
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/LedgerDescriptor.java,true,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

import java.io.IOException;
import java.nio.ByteBuffer;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;



/**
 * Implements a ledger inside a bookie. In particular, it implements operations
 * to write entries to a ledger and read entries from a ledger.
 *
 */
public class LedgerDescriptor {
    final static Logger LOG = LoggerFactory.getLogger(LedgerDescriptor.class);
    LedgerCache ledgerCache;
    LedgerDescriptor(long ledgerId, EntryLogger entryLogger, LedgerCache ledgerCache) {
        this.ledgerId = ledgerId;
        this.entryLogger = entryLogger;
        this.ledgerCache = ledgerCache;
    }

    private ByteBuffer masterKey = null;
    volatile private boolean fenced = false;

    void setMasterKey(ByteBuffer masterKey) {
        this.masterKey = masterKey;
    }

    boolean cmpMasterKey(ByteBuffer masterKey) {
        return this.masterKey.equals(masterKey);
    }

    private long ledgerId;
    public long getLedgerId() {
        return ledgerId;
    }

    EntryLogger entryLogger;
    private int refCnt;
    synchronized public void incRef() {
        refCnt++;
    }
    synchronized public void decRef() {
        refCnt--;
    }
    synchronized public int getRefCnt() {
        return refCnt;
    }
    
    void setFenced() {
        fenced = true;
    }
    
    boolean isFenced() {
        return fenced;
    }

    long addEntry(ByteBuffer entry) throws IOException {
        long ledgerId = entry.getLong();

        if (ledgerId != this.ledgerId) {
            throw new IOException("Entry for ledger " + ledgerId + " was sent to " + this.ledgerId);
        }
        long entryId = entry.getLong();
        entry.rewind();

        /*
         * Log the entry
         */
        long pos = entryLogger.addEntry(ledgerId, entry);


        /*
         * Set offset of entry id to be the current ledger position
         */
        ledgerCache.putEntryOffset(ledgerId, entryId, pos);
        return entryId;
    }
    ByteBuffer readEntry(long entryId) throws IOException {
        long offset;
        /*
         * If entryId is -1, then return the last written.
         */
        if (entryId == -1) {
            long lastEntry = ledgerCache.getLastEntry(ledgerId);
            FileInfo fi = null;
            try {
                fi = ledgerCache.getFileInfo(ledgerId, false);
                long size = fi.size();
                // we may not have the last entry in the cache
                if (size > lastEntry*8) {
                    ByteBuffer bb = ByteBuffer.allocate(ledgerCache.getPageSize());
                    long position = size - ledgerCache.getPageSize();
                    if (position < 0) {
                        position = 0;
                    }
                    fi.read(bb, position);
                    bb.flip();
                    long startingEntryId = position/8;
                    for(int i = ledgerCache.getEntriesPerPage()-1; i >= 0; i--) {
                        if (bb.getLong(i*8) != 0) {
                            if (lastEntry < startingEntryId+i) {
                                lastEntry = startingEntryId+i;
                            }
                            break;
                        }
                    }
                }
            } finally {
                if (fi != null) {
                    fi.release();
                }
            }
            entryId = lastEntry;
        }

        offset = ledgerCache.getEntryOffset(ledgerId, entryId);
        if (offset == 0) {
            throw new Bookie.NoEntryException(ledgerId, entryId);
        }
        return ByteBuffer.wrap(entryLogger.readEntry(ledgerId, entryId, offset));
    }
    void close() {
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/LedgerEntryPage.java,true,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

import java.io.IOException;
import java.nio.ByteBuffer;

/**
 * This is a page in the LedgerCache. It holds the locations
 * (entrylogfile, offset) for entry ids.
 */
public class LedgerEntryPage {
    private final int pageSize;
    private final int entriesPerPage;
    private long ledger = -1;
    private long firstEntry = -1;
    private final ByteBuffer page;
    private boolean clean = true;
    private boolean pinned = false;
    private int useCount;
    private int version;

    public LedgerEntryPage(int pageSize, int entriesPerPage) {
        this.pageSize = pageSize;
        this.entriesPerPage = entriesPerPage;
        page = ByteBuffer.allocateDirect(pageSize);
    }

    @Override
    public String toString() {
        StringBuilder sb = new StringBuilder();
        sb.append(getLedger());
        sb.append('@');
        sb.append(getFirstEntry());
        sb.append(clean ? " clean " : " dirty ");
        sb.append(useCount);
        return sb.toString();
    }
    synchronized public void usePage() {
        useCount++;
    }
    synchronized public void pin() {
        pinned = true;
    }
    synchronized public void unpin() {
        pinned = false;
    }
    synchronized public boolean isPinned() {
        return pinned;
    }
    synchronized public void releasePage() {
        useCount--;
        if (useCount < 0) {
            throw new IllegalStateException("Use count has gone below 0");
        }
    }
    synchronized private void checkPage() {
        if (useCount <= 0) {
            throw new IllegalStateException("Page not marked in use");
        }
    }
    @Override
    public boolean equals(Object other) {
        LedgerEntryPage otherLEP = (LedgerEntryPage) other;
        return otherLEP.getLedger() == getLedger() && otherLEP.getFirstEntry() == getFirstEntry();
    }
    @Override
    public int hashCode() {
        return (int)getLedger() ^ (int)(getFirstEntry());
    }
    void setClean(int versionOfCleaning) {
        this.clean = (versionOfCleaning == version);
    }
    boolean isClean() {
        return clean;
    }
    public void setOffset(long offset, int position) {
        checkPage();
        version++;
        this.clean = false;
        page.putLong(position, offset);
    }
    public long getOffset(int position) {
        checkPage();
        return page.getLong(position);
    }
    static final byte zeroPage[] = new byte[64*1024];
    public void zeroPage() {
        checkPage();
        page.clear();
        page.put(zeroPage, 0, page.remaining());
        clean = true;
    }
    public void readPage(FileInfo fi) throws IOException {
        checkPage();
        page.clear();
        while(page.remaining() != 0) {
            if (fi.read(page, getFirstEntry()*8) <= 0) {
                throw new IOException("Short page read of ledger " + getLedger() + " tried to get " + page.capacity() + " from position " + getFirstEntry()*8 + " still need " + page.remaining());
            }
        }
        clean = true;
    }
    public ByteBuffer getPageToWrite() {
        checkPage();
        page.clear();
        return page;
    }
    void setLedger(long ledger) {
        this.ledger = ledger;
    }
    long getLedger() {
        return ledger;
    }
    int getVersion() {
        return version;
    }
    void setFirstEntry(long firstEntry) {
        if (firstEntry % entriesPerPage != 0) {
            throw new IllegalArgumentException(firstEntry + " is not a multiple of " + entriesPerPage);
        }
        this.firstEntry = firstEntry;
    }
    long getFirstEntry() {
        return firstEntry;
    }
    public boolean inUse() {
        return useCount > 0;
    }
    public long getLastEntry() {
        for(int i = entriesPerPage - 1; i >= 0; i--) {
            if (getOffset(i*8) > 0) {
                return i + firstEntry;
            }
        }
        return 0;
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/MarkerFileChannel.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

import java.io.IOException;
import java.nio.ByteBuffer;
import java.nio.MappedByteBuffer;
import java.nio.channels.FileChannel;
import java.nio.channels.FileLock;
import java.nio.channels.ReadableByteChannel;
import java.nio.channels.WritableByteChannel;

/**
 * This class is just a stub that can be used in collections with
 * FileChannels
 */
public class MarkerFileChannel extends FileChannel {

    @Override
    public void force(boolean metaData) throws IOException {
        // TODO Auto-generated method stub

    }

    @Override
    public FileLock lock(long position, long size, boolean shared)
            throws IOException {
        // TODO Auto-generated method stub
        return null;
    }

    @Override
    public MappedByteBuffer map(MapMode mode, long position, long size)
            throws IOException {
        // TODO Auto-generated method stub
        return null;
    }

    @Override
    public long position() throws IOException {
        // TODO Auto-generated method stub
        return 0;
    }

    @Override
    public FileChannel position(long newPosition) throws IOException {
        // TODO Auto-generated method stub
        return null;
    }

    @Override
    public int read(ByteBuffer dst) throws IOException {
        // TODO Auto-generated method stub
        return 0;
    }

    @Override
    public int read(ByteBuffer dst, long position) throws IOException {
        // TODO Auto-generated method stub
        return 0;
    }

    @Override
    public long read(ByteBuffer[] dsts, int offset, int length)
            throws IOException {
        // TODO Auto-generated method stub
        return 0;
    }

    @Override
    public long size() throws IOException {
        // TODO Auto-generated method stub
        return 0;
    }

    @Override
    public long transferFrom(ReadableByteChannel src, long position, long count)
            throws IOException {
        // TODO Auto-generated method stub
        return 0;
    }

    @Override
    public long transferTo(long position, long count, WritableByteChannel target)
            throws IOException {
        // TODO Auto-generated method stub
        return 0;
    }

    @Override
    public FileChannel truncate(long size) throws IOException {
        // TODO Auto-generated method stub
        return null;
    }

    @Override
    public FileLock tryLock(long position, long size, boolean shared)
            throws IOException {
        // TODO Auto-generated method stub
        return null;
    }

    @Override
    public int write(ByteBuffer src) throws IOException {
        // TODO Auto-generated method stub
        return 0;
    }

    @Override
    public int write(ByteBuffer src, long position) throws IOException {
        // TODO Auto-generated method stub
        return 0;
    }

    @Override
    public long write(ByteBuffer[] srcs, int offset, int length)
            throws IOException {
        // TODO Auto-generated method stub
        return 0;
    }

    @Override
    protected void implCloseChannel() throws IOException {
        // TODO Auto-generated method stub

    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/AsyncCallback.java,false,"package org.apache.bookkeeper.client;

import java.util.Enumeration;

/**
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements. See the NOTICE file distributed with this
 * work for additional information regarding copyright ownership. The ASF
 * licenses this file to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 * License for the specific language governing permissions and limitations under
 * the License.
 */

public interface AsyncCallback {
    public interface AddCallback {
        /**
         * Callback declaration
         *
         * @param rc
         *          return code
         * @param lh
         *          ledger handle
         * @param entryId
         *          entry identifier
         * @param ctx
         *          control object
         */
        void addComplete(int rc, LedgerHandle lh, long entryId, Object ctx);
    }

    public interface CloseCallback {
        /**
         * Callback definition
         *
         * @param rc
         *          return code
         * @param lh
         *          ledger handle
         * @param ctx
         *          control object
         */
        void closeComplete(int rc, LedgerHandle lh, Object ctx);
    }

    public interface CreateCallback {
        /**
         * Declaration of callback method
         *
         * @param rc
         *          return status
         * @param lh
         *          ledger handle
         * @param ctx
         *          control object
         */

        void createComplete(int rc, LedgerHandle lh, Object ctx);
    }

    public interface OpenCallback {
        /**
         * Callback for asynchronous call to open ledger
         *
         * @param rc
         *          Return code
         * @param lh
         *          ledger handle
         * @param ctx
         *          control object
         */

        public void openComplete(int rc, LedgerHandle lh, Object ctx);

    }

    public interface ReadCallback {
        /**
         * Callback declaration
         *
         * @param rc
         *          return code
         * @param lh
         *          ledger handle
         * @param seq
         *          sequence of entries
         * @param ctx
         *          control object
         */

        void readComplete(int rc, LedgerHandle lh, Enumeration<LedgerEntry> seq,
                          Object ctx);
    }

    public interface DeleteCallback {
        /**
         * Callback definition for delete operations
         *
         * @param rc
         *          return code
         * @param ctx
         *          control object
         */
        void deleteComplete(int rc, Object ctx);
    }

    public interface ReadLastConfirmedCallback {
        /**
         * Callback definition for bookie recover operations
         *
         * @param rc
         *          return code
         * @param ctx
         *          control object
         */
        void readLastConfirmedComplete(int rc, long lastConfirmed, Object ctx);
    }

    public interface RecoverCallback {
        /**
         * Callback definition for bookie recover operations
         *
         * @param rc
         *          return code
         * @param ctx
         *          control object
         */
        void recoverComplete(int rc, Object ctx);
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/BKException.java,true,"package org.apache.bookkeeper.client;

/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

import java.lang.Exception;

/**
 * Class the enumerates all the possible error conditions
 *
 */

@SuppressWarnings("serial")
public abstract class BKException extends Exception {

    private int code;

    BKException(int code) {
        this.code = code;
    }

    /**
     * Create an exception from an error code
     * @param code return error code
     * @return correponding exception
     */
    public static BKException create(int code) {
        switch (code) {
        case Code.ReadException:
            return new BKReadException();
        case Code.QuorumException:
            return new BKQuorumException();
        case Code.NoBookieAvailableException:
            return new BKBookieException();
        case Code.DigestNotInitializedException:
            return new BKDigestNotInitializedException();
        case Code.DigestMatchException:
            return new BKDigestMatchException();
        case Code.NotEnoughBookiesException:
            return new BKNotEnoughBookiesException();
        case Code.NoSuchLedgerExistsException:
            return new BKNoSuchLedgerExistsException();
        case Code.BookieHandleNotAvailableException:
            return new BKBookieHandleNotAvailableException();
        case Code.ZKException:
            return new ZKException();
        case Code.LedgerRecoveryException:
            return new BKLedgerRecoveryException();
        case Code.LedgerClosedException:
            return new BKLedgerClosedException();
        case Code.WriteException:
            return new BKWriteException();
        case Code.NoSuchEntryException:
            return new BKNoSuchEntryException();
        case Code.IncorrectParameterException:
            return new BKIncorrectParameterException();
        case Code.InterruptedException:
            return new BKInterruptedException();
        case Code.ProtocolVersionException:
            return new BKProtocolVersionException();
        case Code.LedgerFencedException:
            return new BKLedgerFencedException();
        default:
            return new BKIllegalOpException();
        }
    }

    /**
     * List of return codes
     *
     */
    public interface Code {
        int OK = 0;
        int ReadException = -1;
        int QuorumException = -2;
        int NoBookieAvailableException = -3;
        int DigestNotInitializedException = -4;
        int DigestMatchException = -5;
        int NotEnoughBookiesException = -6;
        int NoSuchLedgerExistsException = -7;
        int BookieHandleNotAvailableException = -8;
        int ZKException = -9;
        int LedgerRecoveryException = -10;
        int LedgerClosedException = -11;
        int WriteException = -12;
        int NoSuchEntryException = -13;
        int IncorrectParameterException = -14;
        int InterruptedException = -15;
        int ProtocolVersionException = -16;

        int IllegalOpException = -100;
        int LedgerFencedException = -101;
    }

    public void setCode(int code) {
        this.code = code;
    }

    public int getCode() {
        return this.code;
    }

    public static String getMessage(int code) {
        switch (code) {
        case Code.OK:
            return "No problem";
        case Code.ReadException:
            return "Error while reading ledger";
        case Code.QuorumException:
            return "Invalid quorum size on ensemble size";
        case Code.NoBookieAvailableException:
            return "Invalid quorum size on ensemble size";
        case Code.DigestNotInitializedException:
            return "Digest engine not initialized";
        case Code.DigestMatchException:
            return "Entry digest does not match";
        case Code.NotEnoughBookiesException:
            return "Not enough non-faulty bookies available";
        case Code.NoSuchLedgerExistsException:
            return "No such ledger exists";
        case Code.BookieHandleNotAvailableException:
            return "Bookie handle is not available";
        case Code.ZKException:
            return "Error while using ZooKeeper";
        case Code.LedgerRecoveryException:
            return "Error while recovering ledger";
        case Code.LedgerClosedException:
            return "Attempt to write to a closed ledger";
        case Code.WriteException:
            return "Write failed on bookie";
        case Code.NoSuchEntryException:
            return "No such entry";
        case Code.IncorrectParameterException:
            return "Incorrect parameter input";
        case Code.InterruptedException:
            return "Interrupted while waiting for permit";
        case Code.ProtocolVersionException:
            return "Bookie protocol version on server is incompatible with client";
        case Code.LedgerFencedException:
            return "Ledger has been fenced off. Some other client must have opened it to read";
        default:
            return "Invalid operation";
        }
    }

    public static class BKReadException extends BKException {
        public BKReadException() {
            super(Code.ReadException);
        }
    }

    public static class BKNoSuchEntryException extends BKException {
        public BKNoSuchEntryException() {
            super(Code.NoSuchEntryException);
        }
    }

    public static class BKQuorumException extends BKException {
        public BKQuorumException() {
            super(Code.QuorumException);
        }
    }

    public static class BKBookieException extends BKException {
        public BKBookieException() {
            super(Code.NoBookieAvailableException);
        }
    }

    public static class BKDigestNotInitializedException extends BKException {
        public BKDigestNotInitializedException() {
            super(Code.DigestNotInitializedException);
        }
    }

    public static class BKDigestMatchException extends BKException {
        public BKDigestMatchException() {
            super(Code.DigestMatchException);
        }
    }

    public static class BKIllegalOpException extends BKException {
        public BKIllegalOpException() {
            super(Code.IllegalOpException);
        }
    }

    public static class BKNotEnoughBookiesException extends BKException {
        public BKNotEnoughBookiesException() {
            super(Code.NotEnoughBookiesException);
        }
    }

    public static class BKWriteException extends BKException {
        public BKWriteException() {
            super(Code.WriteException);
        }
    }

    public static class BKProtocolVersionException extends BKException {
        public BKProtocolVersionException() {
            super(Code.ProtocolVersionException);
        }
    }

    public static class BKNoSuchLedgerExistsException extends BKException {
        public BKNoSuchLedgerExistsException() {
            super(Code.NoSuchLedgerExistsException);
        }
    }

    public static class BKBookieHandleNotAvailableException extends BKException {
        public BKBookieHandleNotAvailableException() {
            super(Code.BookieHandleNotAvailableException);
        }
    }

    public static class ZKException extends BKException {
        public ZKException() {
            super(Code.ZKException);
        }
    }

    public static class BKLedgerRecoveryException extends BKException {
        public BKLedgerRecoveryException() {
            super(Code.LedgerRecoveryException);
        }
    }

    public static class BKLedgerClosedException extends BKException {
        public BKLedgerClosedException() {
            super(Code.LedgerClosedException);
        }
    }

    public static class BKIncorrectParameterException extends BKException {
        public BKIncorrectParameterException() {
            super(Code.IncorrectParameterException);
        }
    }

    public static class BKInterruptedException extends BKException {
        public BKInterruptedException() {
            super(Code.InterruptedException);
        }
    }

    public static class BKLedgerFencedException extends BKException {
        public BKLedgerFencedException() {
            super(Code.LedgerFencedException);
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/BookieWatcher.java,true,"package org.apache.bookkeeper.client;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.IOException;
import java.net.InetSocketAddress;
import java.util.ArrayList;
import java.util.Collections;
import java.util.HashSet;
import java.util.List;
import java.util.Set;
import java.util.concurrent.Executors;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;
import org.apache.bookkeeper.client.BKException.BKNotEnoughBookiesException;
import org.apache.bookkeeper.util.SafeRunnable;
import org.apache.bookkeeper.util.StringUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.WatchedEvent;
import org.apache.zookeeper.Watcher;
import org.apache.zookeeper.AsyncCallback.ChildrenCallback;
import org.apache.zookeeper.KeeperException.Code;

/**
 * This class is responsible for maintaining a consistent view of what bookies
 * are available by reading Zookeeper (and setting watches on the bookie nodes).
 * When a bookie fails, the other parts of the code turn to this class to find a
 * replacement
 *
 */
class BookieWatcher implements Watcher, ChildrenCallback {
    static final Logger logger = LoggerFactory.getLogger(BookieWatcher.class);

    public static final String BOOKIE_REGISTRATION_PATH = "/ledgers/available";
    static final Set<InetSocketAddress> EMPTY_SET = new HashSet<InetSocketAddress>();
    public static int ZK_CONNECT_BACKOFF_SEC = 1;

    BookKeeper bk;
    ScheduledExecutorService scheduler;

    Set<InetSocketAddress> knownBookies = new HashSet<InetSocketAddress>();

    SafeRunnable reReadTask = new SafeRunnable() {
        @Override
        public void safeRun() {
            readBookies();
        }
    };

    public BookieWatcher(BookKeeper bk) {
        this.bk = bk;
        this.scheduler = Executors.newSingleThreadScheduledExecutor();
    }

    public void halt() {
        scheduler.shutdown();
    }

    public void readBookies() {
        readBookies(this);
    }

    public void readBookies(ChildrenCallback callback) {
        bk.getZkHandle().getChildren(BOOKIE_REGISTRATION_PATH, this, callback, null);
    }

    @Override
    public void process(WatchedEvent event) {
        readBookies();
    }

    @Override
    public void processResult(int rc, String path, Object ctx, List<String> children) {

        if (rc != KeeperException.Code.OK.intValue()) {
            //logger.error("Error while reading bookies", KeeperException.create(Code.get(rc), path));
            // try the read after a second again
            scheduler.schedule(reReadTask, ZK_CONNECT_BACKOFF_SEC, TimeUnit.SECONDS);
            return;
        }

        // Read the bookie addresses into a set for efficient lookup
        Set<InetSocketAddress> newBookieAddrs = new HashSet<InetSocketAddress>();
        for (String bookieAddrString : children) {
            InetSocketAddress bookieAddr;
            try {
                bookieAddr = StringUtils.parseAddr(bookieAddrString);
            } catch (IOException e) {
                logger.error("Could not parse bookie address: " + bookieAddrString + ", ignoring this bookie");
                continue;
            }
            newBookieAddrs.add(bookieAddr);
        }

        synchronized (this) {
            knownBookies = newBookieAddrs;
        }
    }

    /**
     * Blocks until bookies are read from zookeeper, used in the {@link BookKeeper} constructor.
     * @throws InterruptedException
     * @throws KeeperException
     */
    public void readBookiesBlocking() throws InterruptedException, KeeperException {
        final LinkedBlockingQueue<Integer> queue = new LinkedBlockingQueue<Integer>();
        readBookies(new ChildrenCallback() {
            public void processResult(int rc, String path, Object ctx, List<String> children) {
                try {
                    BookieWatcher.this.processResult(rc, path, ctx, children);
                    queue.put(rc);
                } catch (InterruptedException e) {
                    logger.error("Interruped when trying to read bookies in a blocking fashion");
                    throw new RuntimeException(e);
                }
            }
        });
        int rc = queue.take();

        if (rc != KeeperException.Code.OK.intValue()) {
            throw KeeperException.create(Code.get(rc));
        }
    }

    /**
     * Wrapper over the {@link #getAdditionalBookies(Set, int)} method when there is no exclusion list (or exisiting bookies)
     * @param numBookiesNeeded
     * @return
     * @throws BKNotEnoughBookiesException
     */
    public ArrayList<InetSocketAddress> getNewBookies(int numBookiesNeeded) throws BKNotEnoughBookiesException {
        return getAdditionalBookies(EMPTY_SET, numBookiesNeeded);
    }

    /**
     * Wrapper over the {@link #getAdditionalBookies(Set, int)} method when you just need 1 extra bookie
     * @param existingBookies
     * @return
     * @throws BKNotEnoughBookiesException
     */
    public InetSocketAddress getAdditionalBookie(List<InetSocketAddress> existingBookies)
            throws BKNotEnoughBookiesException {
        return getAdditionalBookies(new HashSet<InetSocketAddress>(existingBookies), 1).get(0);
    }

    /**
     * Returns additional bookies given an exclusion list and how many are needed
     * @param existingBookies
     * @param numAdditionalBookiesNeeded
     * @return
     * @throws BKNotEnoughBookiesException
     */
    public ArrayList<InetSocketAddress> getAdditionalBookies(Set<InetSocketAddress> existingBookies,
            int numAdditionalBookiesNeeded) throws BKNotEnoughBookiesException {

        ArrayList<InetSocketAddress> newBookies = new ArrayList<InetSocketAddress>();

        if (numAdditionalBookiesNeeded <= 0) {
            return newBookies;
        }

        List<InetSocketAddress> allBookies;

        synchronized (this) {
            allBookies = new ArrayList<InetSocketAddress>(knownBookies);
        }

        Collections.shuffle(allBookies);

        for (InetSocketAddress bookie : allBookies) {
            if (existingBookies.contains(bookie)) {
                continue;
            }

            newBookies.add(bookie);
            numAdditionalBookiesNeeded--;

            if (numAdditionalBookiesNeeded == 0) {
                return newBookies;
            }
        }

        throw new BKNotEnoughBookiesException();
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/BookKeeper.java,true,"package org.apache.bookkeeper.client;

/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

import java.io.IOException;
import java.util.concurrent.Executors;

import org.apache.bookkeeper.meta.LedgerManager;
import org.apache.bookkeeper.meta.LedgerManagerFactory;
import org.apache.bookkeeper.conf.ClientConfiguration;
import org.apache.bookkeeper.client.AsyncCallback.CreateCallback;
import org.apache.bookkeeper.client.AsyncCallback.DeleteCallback;
import org.apache.bookkeeper.client.AsyncCallback.OpenCallback;
import org.apache.bookkeeper.client.BKException.Code;
import org.apache.bookkeeper.proto.BookieClient;
import org.apache.bookkeeper.util.OrderedSafeExecutor;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.WatchedEvent;
import org.apache.zookeeper.Watcher;
import org.apache.zookeeper.ZooKeeper;
import org.jboss.netty.channel.socket.ClientSocketChannelFactory;
import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory;

/**
 * BookKeeper client. We assume there is one single writer to a ledger at any
 * time.
 *
 * There are four possible operations: start a new ledger, write to a ledger,
 * read from a ledger and delete a ledger.
 *
 * The exceptions resulting from synchronous calls and error code resulting from
 * asynchronous calls can be found in the class {@link BKException}.
 *
 *
 */

public class BookKeeper {

    static final Logger LOG = LoggerFactory.getLogger(BookKeeper.class);

    ZooKeeper zk = null;
    // whether the zk handle is one we created, or is owned by whoever
    // instantiated us
    boolean ownZKHandle = false;

    ClientSocketChannelFactory channelFactory;
    // whether the socket factory is one we created, or is owned by whoever
    // instantiated us
    boolean ownChannelFactory = false;

    BookieClient bookieClient;
    BookieWatcher bookieWatcher;

    OrderedSafeExecutor callbackWorker = new OrderedSafeExecutor(Runtime
            .getRuntime().availableProcessors());
    OrderedSafeExecutor mainWorkerPool = new OrderedSafeExecutor(Runtime
            .getRuntime().availableProcessors());

    // Ledger manager responsible for how to store ledger meta data
    final LedgerManager ledgerManager;

    ClientConfiguration conf;

    /**
     * Create a bookkeeper client. A zookeeper client and a client socket factory
     * will be instantiated as part of this constructor.
     *
     * @param servers
     *          A list of one of more servers on which zookeeper is running. The
     *          client assumes that the running bookies have been registered with
     *          zookeeper under the path
     *          {@link BookieWatcher#BOOKIE_REGISTRATION_PATH}
     * @throws IOException
     * @throws InterruptedException
     * @throws KeeperException
     */
    public BookKeeper(String servers) throws IOException, InterruptedException,
        KeeperException {
        this(new ClientConfiguration().setZkServers(servers));
    }

    /**
     * Create a bookkeeper client using a configuration object.
     * A zookeeper client and a client socket factory will be 
     * instantiated as part of this constructor.
     *
     * @param conf
     *          Client Configuration object
     * @throws IOException
     * @throws InterruptedException
     * @throws KeeperException
     */
    public BookKeeper(ClientConfiguration conf) throws IOException, InterruptedException,
        KeeperException {
        this(conf, new ZooKeeper(conf.getZkServers(), conf.getZkTimeout(), new Watcher() {
            @Override
            public void process(WatchedEvent event) {
                // TODO: handle session disconnects and expires
                if (LOG.isDebugEnabled()) {
                    LOG.debug("Process: " + event.getType() + " " + event.getPath());
                }
            }
        }), new NioClientSocketChannelFactory(Executors.newCachedThreadPool(),
                                              Executors.newCachedThreadPool()));

        ownZKHandle = true;
        ownChannelFactory = true;
     }

    /**
     * Create a bookkeeper client but use the passed in zookeeper client instead
     * of instantiating one.
     *
     * @param conf
     *          Client Configuration object
     *          {@link ClientConfiguration}
     * @param zk
     *          Zookeeper client instance connected to the zookeeper with which
     *          the bookies have registered
     * @throws IOException
     * @throws InterruptedException
     * @throws KeeperException
     */
    public BookKeeper(ClientConfiguration conf, ZooKeeper zk)
        throws IOException, InterruptedException, KeeperException {
        this(conf, zk, new NioClientSocketChannelFactory(Executors.newCachedThreadPool(),
                Executors.newCachedThreadPool()));
        ownChannelFactory = true;
    }

    /**
     * Create a bookkeeper client but use the passed in zookeeper client and
     * client socket channel factory instead of instantiating those.
     *
     * @param conf
     *          Client Configuration Object
     *          {@link ClientConfiguration}
     * @param zk
     *          Zookeeper client instance connected to the zookeeper with which
     *          the bookies have registered
     * @param channelFactory
     *          A factory that will be used to create connections to the bookies
     * @throws IOException
     * @throws InterruptedException
     * @throws KeeperException
     */
    public BookKeeper(ClientConfiguration conf, ZooKeeper zk, ClientSocketChannelFactory channelFactory)
            throws IOException, InterruptedException, KeeperException {
        if (zk == null || channelFactory == null) {
            throw new NullPointerException();
        }
        this.conf = conf;
        this.zk = zk;
        this.channelFactory = channelFactory;
        bookieWatcher = new BookieWatcher(this);
        bookieWatcher.readBookiesBlocking();
        bookieClient = new BookieClient(conf, channelFactory, mainWorkerPool);
        // intialize ledger meta manager
        ledgerManager = LedgerManagerFactory.newLedgerManager(conf, zk);
    }

    LedgerManager getLedgerManager() {
        return ledgerManager;
    }

    /**
     * There are 2 digest types that can be used for verification. The CRC32 is
     * cheap to compute but does not protect against byzantine bookies (i.e., a
     * bookie might report fake bytes and a matching CRC32). The MAC code is more
     * expensive to compute, but is protected by a password, i.e., a bookie can't
     * report fake bytes with a mathching MAC unless it knows the password
     */
    public enum DigestType {
        MAC, CRC32
    };

    ZooKeeper getZkHandle() {
        return zk;
    }

    protected ClientConfiguration getConf() {
        return conf;
    }

    /**
     * Get the BookieClient, currently used for doing bookie recovery.
     *
     * @return BookieClient for the BookKeeper instance.
     */
    BookieClient getBookieClient() {
        return bookieClient;
    }

    /**
     * Creates a new ledger asynchronously. To create a ledger, we need to specify
     * the ensemble size, the quorum size, the digest type, a password, a callback
     * implementation, and an optional control object. The ensemble size is how
     * many bookies the entries should be striped among and the quorum size is the
     * degree of replication of each entry. The digest type is either a MAC or a
     * CRC. Note that the CRC option is not able to protect a client against a
     * bookie that replaces an entry. The password is used not only to
     * authenticate access to a ledger, but also to verify entries in ledgers.
     *
     * @param ensSize
     *          ensemble size
     * @param qSize
     *          quorum size
     * @param digestType
     *          digest type, either MAC or CRC32
     * @param passwd
     *          password
     * @param cb
     *          createCallback implementation
     * @param ctx
     *          optional control object
     */
    public void asyncCreateLedger(int ensSize, int qSize, DigestType digestType,
                                  byte[] passwd, CreateCallback cb, Object ctx) {

        new LedgerCreateOp(this, ensSize, qSize, digestType, passwd, cb, ctx)
        .initiate();

    }


    /**
     * Creates a new ledger. Default of 3 servers, and quorum of 2 servers.
     *
     * @param digestType
     *          digest type, either MAC or CRC32
     * @param passwd
     *          password
     * @return a handle to the newly created ledger
     * @throws InterruptedException
     * @throws BKException
     */
    public LedgerHandle createLedger(DigestType digestType, byte passwd[])
            throws BKException, InterruptedException {
        return createLedger(3, 2, digestType, passwd);
    }

    /**
     * Synchronous call to create ledger. Parameters match those of
     * {@link #asyncCreateLedger(int, int, DigestType, byte[], 
     *                           AsyncCallback.CreateCallback, Object)}
     *
     * @param ensSize
     * @param qSize
     * @param digestType
     * @param passwd
     * @return a handle to the newly created ledger
     * @throws InterruptedException
     * @throws BKException
     */
    public LedgerHandle createLedger(int ensSize, int qSize,
                                     DigestType digestType, byte passwd[]) 
            throws InterruptedException, BKException {
        SyncCounter counter = new SyncCounter();
        counter.inc();
        /*
         * Calls asynchronous version
         */
        asyncCreateLedger(ensSize, qSize, digestType, passwd, 
                          new SyncCreateCallback(), counter);

        /*
         * Wait
         */
        counter.block(0);
        if (counter.getLh() == null) {
            LOG.error("ZooKeeper error: " + counter.getrc());
            throw BKException.create(Code.ZKException);
        }

        return counter.getLh();
    }

    /**
     * Open existing ledger asynchronously for reading.
     * 
     * Opening a ledger with this method invokes fencing and recovery on the ledger 
     * if the ledger has not been closed. Fencing will block all other clients from 
     * writing to the ledger. Recovery will make sure that the ledger is closed 
     * before reading from it. 
     *
     * Recovery also makes sure that any entries which reached one bookie, but not a 
     * quorum, will be replicated to a quorum of bookies. This occurs in cases were
     * the writer of a ledger crashes after sending a write request to one bookie but
     * before being able to send it to the rest of the bookies in the quorum. 
     *
     * If the ledger is already closed, neither fencing nor recovery will be applied.
     * 
     * @see LedgerHandle#asyncClose
     *
     * @param lId
     *          ledger identifier
     * @param digestType
     *          digest type, either MAC or CRC32
     * @param passwd
     *          password
     * @param ctx
     *          optional control object
     */
    public void asyncOpenLedger(long lId, DigestType digestType, byte passwd[],
                                OpenCallback cb, Object ctx) {
        new LedgerOpenOp(this, lId, digestType, passwd, cb, ctx).initiate();
    }

    /**
     * Open existing ledger asynchronously for reading, but it does not try to
     * recover the ledger if it is not yet closed. The application needs to use
     * it carefully, since the writer might have crashed and ledger will remain
     * unsealed forever if there is no external mechanism to detect the failure
     * of the writer and the ledger is not open in a safe manner, invoking the
     * recovery procedure.
     * 
     * Opening a ledger without recovery does not fence the ledger. As such, other
     * clients can continue to write to the ledger. 
     *
     * This method returns a read only ledger handle. It will not be possible 
     * to add entries to the ledger. Any attempt to add entries will throw an 
     * exception.
     * 
     * Reads from the returned ledger will only be able to read entries up until
     * the lastConfirmedEntry at the point in time at which the ledger was opened.
     *
     * @param lId
     *          ledger identifier
     * @param digestType
     *          digest type, either MAC or CRC32
     * @param passwd
     *          password
     * @param ctx
     *          optional control object
     */
    public void asyncOpenLedgerNoRecovery(long lId, DigestType digestType, byte passwd[],
                                          OpenCallback cb, Object ctx) {
        new LedgerOpenOp(this, lId, digestType, passwd, cb, ctx).initiateWithoutRecovery();
    }


    /**
     * Synchronous open ledger call
     *
     * @see #asyncOpenLedger
     * @param lId
     *          ledger identifier
     * @param digestType
     *          digest type, either MAC or CRC32
     * @param passwd
     *          password
     * @return a handle to the open ledger
     * @throws InterruptedException
     * @throws BKException
     */

    public LedgerHandle openLedger(long lId, DigestType digestType, byte passwd[])
            throws BKException, InterruptedException {
        SyncCounter counter = new SyncCounter();
        counter.inc();

        /*
         * Calls async open ledger
         */
        asyncOpenLedger(lId, digestType, passwd, new SyncOpenCallback(), counter);

        /*
         * Wait
         */
        counter.block(0);
        if (counter.getrc() != BKException.Code.OK)
            throw BKException.create(counter.getrc());

        return counter.getLh();
    }

    /**
     * Synchronous, unsafe open ledger call
     *
     * @see #asyncOpenLedgerNoRecovery
     * @param lId
     *          ledger identifier
     * @param digestType
     *          digest type, either MAC or CRC32
     * @param passwd
     *          password
     * @return a handle to the open ledger
     * @throws InterruptedException
     * @throws BKException
     */

    public LedgerHandle openLedgerNoRecovery(long lId, DigestType digestType, byte passwd[])
            throws BKException, InterruptedException {
        SyncCounter counter = new SyncCounter();
        counter.inc();

        /*
         * Calls async open ledger
         */
        asyncOpenLedgerNoRecovery(lId, digestType, passwd,
                                  new SyncOpenCallback(), counter);

        /*
         * Wait
         */
        counter.block(0);
        if (counter.getrc() != BKException.Code.OK)
            throw BKException.create(counter.getrc());

        return counter.getLh();
    }

    /**
     * Deletes a ledger asynchronously.
     *
     * @param lId
     *            ledger Id
     * @param cb
     *            deleteCallback implementation
     * @param ctx
     *            optional control object
     */
    public void asyncDeleteLedger(long lId, DeleteCallback cb, Object ctx) {
        new LedgerDeleteOp(this, lId, cb, ctx).initiate();
    }


    /**
     * Synchronous call to delete a ledger. Parameters match those of
     * {@link #asyncDeleteLedger(long, AsyncCallback.DeleteCallback, Object)}
     *
     * @param lId
     *            ledgerId
     * @throws InterruptedException
     * @throws BKException
     */
    public void deleteLedger(long lId) throws InterruptedException, BKException {
        SyncCounter counter = new SyncCounter();
        counter.inc();
        // Call asynchronous version
        asyncDeleteLedger(lId, new SyncDeleteCallback(), counter);
        // Wait
        counter.block(0);
        if (counter.getrc() != KeeperException.Code.OK.intValue()) {
            LOG.error("ZooKeeper error deleting ledger node: " + counter.getrc());
            throw BKException.create(Code.ZKException);
        }
    }

    /**
     * Shuts down client.
     *
     */
    public void close() throws InterruptedException, BKException {
        bookieClient.close();
        ledgerManager.close();
        bookieWatcher.halt();
        if (ownChannelFactory) {
            channelFactory.releaseExternalResources();
        }
        if (ownZKHandle) {
            zk.close();
        }
        callbackWorker.shutdown();
        mainWorkerPool.shutdown();
    }

    private static class SyncCreateCallback implements CreateCallback {
        /**
         * Create callback implementation for synchronous create call.
         *
         * @param rc
         *          return code
         * @param lh
         *          ledger handle object
         * @param ctx
         *          optional control object
         */
        public void createComplete(int rc, LedgerHandle lh, Object ctx) {
            SyncCounter counter = (SyncCounter) ctx;
            counter.setLh(lh);
            counter.setrc(rc);
            counter.dec();
        }
    }

    private static class SyncOpenCallback implements OpenCallback {
        /**
         * Callback method for synchronous open operation
         *
         * @param rc
         *          return code
         * @param lh
         *          ledger handle
         * @param ctx
         *          optional control object
         */
        public void openComplete(int rc, LedgerHandle lh, Object ctx) {
            SyncCounter counter = (SyncCounter) ctx;
            counter.setLh(lh);
            
            LOG.debug("Open complete: " + rc);
            
            counter.setrc(rc);
            counter.dec();
        }
    }

    private static class SyncDeleteCallback implements DeleteCallback {
        /**
         * Delete callback implementation for synchronous delete call.
         *
         * @param rc
         *            return code
         * @param ctx
         *            optional control object
         */
        public void deleteComplete(int rc, Object ctx) {
            SyncCounter counter = (SyncCounter) ctx;
            counter.setrc(rc);
            counter.dec();
        }
    }



}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/BookKeeperAdmin.java,true,"package org.apache.bookkeeper.client;

/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

import java.io.IOException;
import java.net.InetSocketAddress;
import java.util.ArrayList;
import java.util.Enumeration;
import java.util.HashMap;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.Random;

import org.apache.bookkeeper.conf.ClientConfiguration;
import org.apache.bookkeeper.client.AsyncCallback.OpenCallback;
import org.apache.bookkeeper.client.AsyncCallback.ReadCallback;
import org.apache.bookkeeper.client.AsyncCallback.RecoverCallback;
import org.apache.bookkeeper.client.BookKeeper.DigestType;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.MultiCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.Processor;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.WriteCallback;
import org.apache.bookkeeper.proto.BookieProtocol;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.apache.zookeeper.AsyncCallback;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.WatchedEvent;
import org.apache.zookeeper.Watcher;
import org.apache.zookeeper.ZooKeeper;
import org.apache.zookeeper.KeeperException.Code;
import org.apache.zookeeper.data.Stat;
import org.jboss.netty.buffer.ChannelBuffer;

/**
 * Admin client for BookKeeper clusters
 */
public class BookKeeperAdmin {
    private static Logger LOG = LoggerFactory.getLogger(BookKeeperAdmin.class);

    static final String COLON = ":";

    // ZK client instance
    private ZooKeeper zk;
    // ZK ledgers related String constants
    static final String BOOKIES_PATH = BookieWatcher.BOOKIE_REGISTRATION_PATH;

    // BookKeeper client instance
    private BookKeeper bkc;

    /*
     * Random number generator used to choose an available bookie server to
     * replicate data from a dead bookie.
     */
    private Random rand = new Random();

    /*
     * For now, assume that all ledgers were created with the same DigestType
     * and password. In the future, this admin tool will need to know for each
     * ledger, what was the DigestType and password used to create it before it
     * can open it. These values will come from System properties, though hard
     * coded defaults are defined here.
     */
    private DigestType DIGEST_TYPE;
    private byte[] PASSWD;

    
    /**
     * Constructor that takes in a ZooKeeper servers connect string so we know
     * how to connect to ZooKeeper to retrieve information about the BookKeeper
     * cluster. We need this before we can do any type of admin operations on
     * the BookKeeper cluster.
     *
     * @param zkServers
     *            Comma separated list of hostname:port pairs for the ZooKeeper
     *            servers cluster.
     * @throws IOException
     *             throws this exception if there is an error instantiating the
     *             ZooKeeper client.
     * @throws InterruptedException
     *             Throws this exception if there is an error instantiating the
     *             BookKeeper client.
     * @throws KeeperException
     *             Throws this exception if there is an error instantiating the
     *             BookKeeper client.
     */
    public BookKeeperAdmin(String zkServers) throws IOException, InterruptedException, KeeperException {
        this(new ClientConfiguration().setZkServers(zkServers));
    }

    /**
     * Constructor that takes in a configuration object so we know
     * how to connect to ZooKeeper to retrieve information about the BookKeeper
     * cluster. We need this before we can do any type of admin operations on
     * the BookKeeper cluster.
     *
     * @param conf
     *           Client Configuration Object
     * @throws IOException
     *             throws this exception if there is an error instantiating the
     *             ZooKeeper client.
     * @throws InterruptedException
     *             Throws this exception if there is an error instantiating the
     *             BookKeeper client.
     * @throws KeeperException
     *             Throws this exception if there is an error instantiating the
     *             BookKeeper client.
     */
    public BookKeeperAdmin(ClientConfiguration conf) throws IOException, InterruptedException, KeeperException {
        // Create the ZooKeeper client instance
        zk = new ZooKeeper(conf.getZkServers(), conf.getZkTimeout(), new Watcher() {
            @Override
            public void process(WatchedEvent event) {
                if (LOG.isDebugEnabled()) {
                    LOG.debug("Process: " + event.getType() + " " + event.getPath());
                }
            }
        });

        // Create the BookKeeper client instance
        bkc = new BookKeeper(conf);
        DIGEST_TYPE = conf.getBookieRecoveryDigestType();
        PASSWD = conf.getBookieRecoveryPasswd();
    }

    /**
     * Gracefully release resources that this client uses.
     *
     * @throws InterruptedException
     *             if there is an error shutting down the clients that this
     *             class uses.
     */
    public void close() throws InterruptedException, BKException {
        bkc.close();
        zk.close();
    }

    /**
     * Method to get the input ledger's digest type. For now, this is just a
     * placeholder function since there is no way we can get this information
     * easily. In the future, BookKeeper should store this ledger metadata
     * somewhere such that an admin tool can access it.
     *
     * @param ledgerId
     *            LedgerId we are retrieving the digestType for.
     * @return DigestType for the input ledger
     */
    private DigestType getLedgerDigestType(long ledgerId) {
        return DIGEST_TYPE;
    }

    /**
     * Method to get the input ledger's password. For now, this is just a
     * placeholder function since there is no way we can get this information
     * easily. In the future, BookKeeper should store this ledger metadata
     * somewhere such that an admin tool can access it.
     *
     * @param ledgerId
     *            LedgerId we are retrieving the password for.
     * @return Password for the input ledger
     */
    private byte[] getLedgerPasswd(long ledgerId) {
        return PASSWD;
    }

    // Object used for calling async methods and waiting for them to complete.
    class SyncObject {
        boolean value;
        int rc;

        public SyncObject() {
            value = false;
            rc = BKException.Code.OK;
        }
    }

    /**
     * Synchronous method to rebuild and recover the ledger fragments data that
     * was stored on the source bookie. That bookie could have failed completely
     * and now the ledger data that was stored on it is under replicated. An
     * optional destination bookie server could be given if we want to copy all
     * of the ledger fragments data on the failed source bookie to it.
     * Otherwise, we will just randomly distribute the ledger fragments to the
     * active set of bookies, perhaps based on load. All ZooKeeper ledger
     * metadata will be updated to point to the new bookie(s) that contain the
     * replicated ledger fragments.
     *
     * @param bookieSrc
     *            Source bookie that had a failure. We want to replicate the
     *            ledger fragments that were stored there.
     * @param bookieDest
     *            Optional destination bookie that if passed, we will copy all
     *            of the ledger fragments from the source bookie over to it.
     */
    public void recoverBookieData(final InetSocketAddress bookieSrc, final InetSocketAddress bookieDest)
            throws InterruptedException, BKException {
        SyncObject sync = new SyncObject();
        // Call the async method to recover bookie data.
        asyncRecoverBookieData(bookieSrc, bookieDest, new RecoverCallback() {
            @Override
            public void recoverComplete(int rc, Object ctx) {
                LOG.info("Recover bookie operation completed with rc: " + rc);
                SyncObject syncObj = (SyncObject) ctx;
                synchronized (syncObj) {
                    syncObj.rc = rc;
                    syncObj.value = true;
                    syncObj.notify();
                }
            }
        }, sync);

        // Wait for the async method to complete.
        synchronized (sync) {
            while (sync.value == false) {
                sync.wait();
            }
        }
        if (sync.rc != BKException.Code.OK) {
            throw BKException.create(sync.rc);
        }
    }

    /**
     * Async method to rebuild and recover the ledger fragments data that was
     * stored on the source bookie. That bookie could have failed completely and
     * now the ledger data that was stored on it is under replicated. An
     * optional destination bookie server could be given if we want to copy all
     * of the ledger fragments data on the failed source bookie to it.
     * Otherwise, we will just randomly distribute the ledger fragments to the
     * active set of bookies, perhaps based on load. All ZooKeeper ledger
     * metadata will be updated to point to the new bookie(s) that contain the
     * replicated ledger fragments.
     *
     * @param bookieSrc
     *            Source bookie that had a failure. We want to replicate the
     *            ledger fragments that were stored there.
     * @param bookieDest
     *            Optional destination bookie that if passed, we will copy all
     *            of the ledger fragments from the source bookie over to it.
     * @param cb
     *            RecoverCallback to invoke once all of the data on the dead
     *            bookie has been recovered and replicated.
     * @param context
     *            Context for the RecoverCallback to call.
     */
    public void asyncRecoverBookieData(final InetSocketAddress bookieSrc, final InetSocketAddress bookieDest,
                                       final RecoverCallback cb, final Object context) {
        // Sync ZK to make sure we're reading the latest bookie data.
        zk.sync(BOOKIES_PATH, new AsyncCallback.VoidCallback() {
            @Override
            public void processResult(int rc, String path, Object ctx) {
                if (rc != Code.OK.intValue()) {
                    LOG.error("ZK error syncing: ", KeeperException.create(KeeperException.Code.get(rc), path));
                    cb.recoverComplete(BKException.Code.ZKException, context);
                    return;
                }
                getAvailableBookies(bookieSrc, bookieDest, cb, context);
            };
        }, null);
    }

    /**
     * This method asynchronously gets the set of available Bookies that the
     * dead input bookie's data will be copied over into. If the user passed in
     * a specific destination bookie, then just use that one. Otherwise, we'll
     * randomly pick one of the other available bookies to use for each ledger
     * fragment we are replicating.
     *
     * @param bookieSrc
     *            Source bookie that had a failure. We want to replicate the
     *            ledger fragments that were stored there.
     * @param bookieDest
     *            Optional destination bookie that if passed, we will copy all
     *            of the ledger fragments from the source bookie over to it.
     * @param cb
     *            RecoverCallback to invoke once all of the data on the dead
     *            bookie has been recovered and replicated.
     * @param context
     *            Context for the RecoverCallback to call.
     */
    private void getAvailableBookies(final InetSocketAddress bookieSrc, final InetSocketAddress bookieDest,
                                     final RecoverCallback cb, final Object context) {
        final List<InetSocketAddress> availableBookies = new LinkedList<InetSocketAddress>();
        if (bookieDest != null) {
            availableBookies.add(bookieDest);
            // Now poll ZK to get the active ledgers
            getActiveLedgers(bookieSrc, bookieDest, cb, context, availableBookies);
        } else {
            zk.getChildren(BOOKIES_PATH, null, new AsyncCallback.ChildrenCallback() {
                @Override
                public void processResult(int rc, String path, Object ctx, List<String> children) {
                    if (rc != Code.OK.intValue()) {
                        LOG.error("ZK error getting bookie nodes: ", KeeperException.create(KeeperException.Code
                                  .get(rc), path));
                        cb.recoverComplete(BKException.Code.ZKException, context);
                        return;
                    }
                    for (String bookieNode : children) {
                        String parts[] = bookieNode.split(COLON);
                        if (parts.length < 2) {
                            LOG.error("Bookie Node retrieved from ZK has invalid name format: " + bookieNode);
                            cb.recoverComplete(BKException.Code.ZKException, context);
                            return;
                        }
                        availableBookies.add(new InetSocketAddress(parts[0], Integer.parseInt(parts[1])));
                    }
                    // Now poll ZK to get the active ledgers
                    getActiveLedgers(bookieSrc, bookieDest, cb, context, availableBookies);
                }
            }, null);
        }
    }

    /**
     * This method asynchronously polls ZK to get the current set of active
     * ledgers. From this, we can open each ledger and look at the metadata to
     * determine if any of the ledger fragments for it were stored at the dead
     * input bookie.
     *
     * @param bookieSrc
     *            Source bookie that had a failure. We want to replicate the
     *            ledger fragments that were stored there.
     * @param bookieDest
     *            Optional destination bookie that if passed, we will copy all
     *            of the ledger fragments from the source bookie over to it.
     * @param cb
     *            RecoverCallback to invoke once all of the data on the dead
     *            bookie has been recovered and replicated.
     * @param context
     *            Context for the RecoverCallback to call.
     * @param availableBookies
     *            List of Bookie Servers that are available to use for
     *            replicating data on the failed bookie. This could contain a
     *            single bookie server if the user explicitly chose a bookie
     *            server to replicate data to.
     */
    private void getActiveLedgers(final InetSocketAddress bookieSrc, final InetSocketAddress bookieDest,
                                  final RecoverCallback cb, final Object context, final List<InetSocketAddress> availableBookies) {
        // Wrapper class around the RecoverCallback so it can be used
        // as the final VoidCallback to process ledgers
        class RecoverCallbackWrapper implements AsyncCallback.VoidCallback {
            final RecoverCallback cb;

            RecoverCallbackWrapper(RecoverCallback cb) {
                this.cb = cb;
            }

            @Override
            public void processResult(int rc, String path, Object ctx) {
                cb.recoverComplete(rc, ctx);
            }
        }

        Processor<Long> ledgerProcessor = new Processor<Long>() {
            @Override
            public void process(Long ledgerId, AsyncCallback.VoidCallback iterCallback) {
                recoverLedger(bookieSrc, ledgerId, iterCallback, availableBookies);
            }
        };
        bkc.getLedgerManager().asyncProcessLedgers(
            ledgerProcessor, new RecoverCallbackWrapper(cb),
            context, BKException.Code.OK, BKException.Code.LedgerRecoveryException);
    }

    /**
     * Get a new random bookie, but ensure that it isn't one that is already
     * in the ensemble for the ledger.
     */
    private InetSocketAddress getNewBookie(final List<InetSocketAddress> bookiesAlreadyInEnsemble, 
                                           final List<InetSocketAddress> availableBookies) 
            throws BKException.BKNotEnoughBookiesException {
        ArrayList<InetSocketAddress> candidates = new ArrayList<InetSocketAddress>();
        candidates.addAll(availableBookies);
        candidates.removeAll(bookiesAlreadyInEnsemble);
        if (candidates.size() == 0) {
            throw new BKException.BKNotEnoughBookiesException();
        }
        return candidates.get(rand.nextInt(candidates.size()));
    }

    /**
     * This method asynchronously recovers a given ledger if any of the ledger
     * entries were stored on the failed bookie.
     *
     * @param bookieSrc
     *            Source bookie that had a failure. We want to replicate the
     *            ledger fragments that were stored there.
     * @param lId
     *            Ledger id we want to recover.
     * @param ledgerIterCb
     *            IterationCallback to invoke once we've recovered the current
     *            ledger.
     * @param availableBookies
     *            List of Bookie Servers that are available to use for
     *            replicating data on the failed bookie. This could contain a
     *            single bookie server if the user explicitly chose a bookie
     *            server to replicate data to.
     */
    private void recoverLedger(final InetSocketAddress bookieSrc, final long lId,
                               final AsyncCallback.VoidCallback ledgerIterCb, final List<InetSocketAddress> availableBookies) {
        if (LOG.isDebugEnabled()) {
            LOG.debug("Recovering ledger : " + lId);
        }
        /*
         * For the current ledger, open it to retrieve the LedgerHandle. This
         * will contain the LedgerMetadata indicating which bookie servers the
         * ledger fragments are stored on. Check if any of the ledger fragments
         * for the current ledger are stored on the input dead bookie.
         */
        DigestType digestType = getLedgerDigestType(lId);
        byte[] passwd = getLedgerPasswd(lId);
        bkc.asyncOpenLedgerNoRecovery(lId, digestType, passwd, new OpenCallback() {
            @Override
            public void openComplete(int rc, final LedgerHandle lh, Object ctx) {
                if (rc != Code.OK.intValue()) {
                    LOG.error("BK error opening ledger: " + lId, BKException.create(rc));
                    ledgerIterCb.processResult(rc, null, null);
                    return;
                }
                /*
                 * This List stores the ledger fragments to recover indexed by
                 * the start entry ID for the range. The ensembles TreeMap is
                 * keyed off this.
                 */
                final List<Long> ledgerFragmentsToRecover = new LinkedList<Long>();
                /*
                 * This Map will store the start and end entry ID values for
                 * each of the ledger fragment ranges. The only exception is the
                 * current active fragment since it has no end yet. In the event
                 * of a bookie failure, a new ensemble is created so the current
                 * ensemble should not contain the dead bookie we are trying to
                 * recover.
                 */
                Map<Long, Long> ledgerFragmentsRange = new HashMap<Long, Long>();
                Long curEntryId = null;
                for (Map.Entry<Long, ArrayList<InetSocketAddress>> entry : lh.getLedgerMetadata().getEnsembles()
                         .entrySet()) {
                    if (curEntryId != null)
                        ledgerFragmentsRange.put(curEntryId, entry.getKey() - 1);
                    curEntryId = entry.getKey();
                    if (entry.getValue().contains(bookieSrc)) {
                        /*
                         * Current ledger fragment has entries stored on the
                         * dead bookie so we'll need to recover them.
                         */
                        ledgerFragmentsToRecover.add(entry.getKey());
                    }
                }
                /*
                 * See if this current ledger contains any ledger fragment that
                 * needs to be re-replicated. If not, then just invoke the
                 * multiCallback and return.
                 */
                if (ledgerFragmentsToRecover.size() == 0) {
                    ledgerIterCb.processResult(BKException.Code.OK, null, null);
                    return;
                }

                /*
                 * Multicallback for ledger. Once all fragments for the ledger have been recovered
                 * trigger the ledgerIterCb
                 */
                MultiCallback ledgerFragmentsMcb
                    = new MultiCallback(ledgerFragmentsToRecover.size(), ledgerIterCb, null,
                                        BKException.Code.OK, BKException.Code.LedgerRecoveryException);
                /*
                 * Now recover all of the necessary ledger fragments
                 * asynchronously using a MultiCallback for every fragment.
                 */
                for (final Long startEntryId : ledgerFragmentsToRecover) {
                    Long endEntryId = ledgerFragmentsRange.get(startEntryId);
                    InetSocketAddress newBookie = null;
                    try {
                        newBookie = getNewBookie(lh.getLedgerMetadata().getEnsembles().get(startEntryId),
                                                 availableBookies);
                    } catch (BKException.BKNotEnoughBookiesException bke) {
                        ledgerFragmentsMcb.processResult(BKException.Code.NotEnoughBookiesException, 
                                                         null, null);
                        continue;
                    }
                    
                    if (LOG.isDebugEnabled()) {
                        LOG.debug("Replicating fragment from [" + startEntryId 
                                  + "," + endEntryId + "] of ledger " + lh.getId()
                                  + " to " + newBookie);
                    }

                    try {
                        SingleFragmentCallback cb = new SingleFragmentCallback(
                                                                               ledgerFragmentsMcb, lh, startEntryId, bookieSrc, newBookie);
                        recoverLedgerFragment(bookieSrc, lh, startEntryId, endEntryId, cb, newBookie);
                    } catch(InterruptedException e) {
                        Thread.currentThread().interrupt();
                        return;
                    }
                }
            }
            }, null);
    }

    /**
     * This method asynchronously recovers a ledger fragment which is a
     * contiguous portion of a ledger that was stored in an ensemble that
     * included the failed bookie.
     *
     * @param bookieSrc
     *            Source bookie that had a failure. We want to replicate the
     *            ledger fragments that were stored there.
     * @param lh
     *            LedgerHandle for the ledger
     * @param startEntryId
     *            Start entry Id for the ledger fragment
     * @param endEntryId
     *            End entry Id for the ledger fragment
     * @param ledgerFragmentMcb
     *            MultiCallback to invoke once we've recovered the current
     *            ledger fragment.
     * @param newBookie
     *            New bookie we want to use to recover and replicate the ledger
     *            entries that were stored on the failed bookie.
     */
    private void recoverLedgerFragment(final InetSocketAddress bookieSrc, final LedgerHandle lh,
                                       final Long startEntryId, final Long endEntryId, final SingleFragmentCallback cb,
                                       final InetSocketAddress newBookie) throws InterruptedException {
        if (endEntryId == null) {
            /*
             * Ideally this should never happen if bookie failure is taken care
             * of properly. Nothing we can do though in this case.
             */
            LOG.warn("Dead bookie (" + bookieSrc + ") is still part of the current active ensemble for ledgerId: "
                     + lh.getId());
            cb.processResult(BKException.Code.OK, null, null);
            return;
        }

        ArrayList<InetSocketAddress> curEnsemble = lh.getLedgerMetadata().getEnsembles().get(startEntryId);
        int bookieIndex = 0;
        for (int i = 0; i < curEnsemble.size(); i++) {
            if (curEnsemble.get(i).equals(bookieSrc)) {
                bookieIndex = i;
                break;
            }
        }
        /*
         * Loop through all entries in the current ledger fragment range and
         * find the ones that were stored on the dead bookie.
         */
        List<Long> entriesToReplicate = new LinkedList<Long>();
        for (long i = startEntryId; i <= endEntryId; i++) {
            if (lh.getDistributionSchedule().getReplicaIndex(i, bookieIndex) >= 0) {
                /*
                 * Current entry is stored on the dead bookie so we'll need to
                 * read it and replicate it to a new bookie.
                 */
                entriesToReplicate.add(i);
            }
        }
        /*
         * Now asynchronously replicate all of the entries for the ledger
         * fragment that were on the dead bookie.
         */
        MultiCallback ledgerFragmentEntryMcb =
            new MultiCallback(entriesToReplicate.size(), cb, null,
                              BKException.Code.OK, BKException.Code.LedgerRecoveryException);
        for (final Long entryId : entriesToReplicate) {
            recoverLedgerFragmentEntry(entryId, lh, ledgerFragmentEntryMcb, newBookie);
        }
    }

    /**
     * This method asynchronously recovers a specific ledger entry by reading
     * the values via the BookKeeper Client (which would read it from the other
     * replicas) and then writing it to the chosen new bookie.
     *
     * @param entryId
     *            Ledger Entry ID to recover.
     * @param lh
     *            LedgerHandle for the ledger
     * @param ledgerFragmentEntryMcb
     *            MultiCallback to invoke once we've recovered the current
     *            ledger entry.
     * @param newBookie
     *            New bookie we want to use to recover and replicate the ledger
     *            entries that were stored on the failed bookie.
     */
    private void recoverLedgerFragmentEntry(final Long entryId, final LedgerHandle lh,
                                            final MultiCallback ledgerFragmentEntryMcb, 
                                            final InetSocketAddress newBookie) throws InterruptedException {
        /*
         * Read the ledger entry using the LedgerHandle. This will allow us to
         * read the entry from one of the other replicated bookies other than
         * the dead one.
         */
        lh.asyncReadEntries(entryId, entryId, new ReadCallback() {
            @Override
            public void readComplete(int rc, LedgerHandle lh, Enumeration<LedgerEntry> seq, Object ctx) {
                if (rc != Code.OK.intValue()) {
                    LOG.error("BK error reading ledger entry: " + entryId, BKException.create(rc));
                    ledgerFragmentEntryMcb.processResult(rc, null, null);
                    return;
                }
                /*
                 * Now that we've read the ledger entry, write it to the new
                 * bookie we've selected.
                 */
                LedgerEntry entry = seq.nextElement();
                byte[] data = entry.getEntry();
                ChannelBuffer toSend = lh.getDigestManager().computeDigestAndPackageForSending(entryId,
                                       lh.getLastAddConfirmed(), entry.getLength(), data, 0, data.length);
                bkc.getBookieClient().addEntry(newBookie, lh.getId(), lh.getLedgerKey(), entryId, toSend,
                new WriteCallback() {
                    @Override
                    public void writeComplete(int rc, long ledgerId, long entryId, InetSocketAddress addr,
                    Object ctx) {
                        if (rc != Code.OK.intValue()) {
                            LOG.error("BK error writing entry for ledgerId: " + ledgerId + ", entryId: "
                                      + entryId + ", bookie: " + addr, BKException.create(rc));
                        } else {
                            if (LOG.isDebugEnabled()) {
                                LOG.debug("Success writing ledger id " +ledgerId + ", entry id "
                                          + entryId + " to a new bookie " + addr + "!");
                            }
                        }
                        /*
                         * Pass the return code result up the chain with
                         * the parent callback.
                         */
                        ledgerFragmentEntryMcb.processResult(rc, null, null);
                    }
                }, null, BookieProtocol.FLAG_RECOVERY_ADD);
            }
        }, null);
    }

    /*
     * Callback for recovery of a single ledger fragment.
     * Once the fragment has had all entries replicated, update the ensemble 
     * in zookeeper.
     * Once finished propogate callback up to ledgerFragmentsMcb which should
     * be a multicallback responsible for all fragments in a single ledger
     */
    class SingleFragmentCallback implements AsyncCallback.VoidCallback {
        final MultiCallback ledgerFragmentsMcb;
        final LedgerHandle lh;
        final long fragmentStartId;
        final InetSocketAddress oldBookie;
        final InetSocketAddress newBookie;

        SingleFragmentCallback(MultiCallback ledgerFragmentsMcb, LedgerHandle lh, 
                               long fragmentStartId,
                               InetSocketAddress oldBookie,
                               InetSocketAddress newBookie) {
            this.ledgerFragmentsMcb = ledgerFragmentsMcb;
            this.lh = lh;
            this.fragmentStartId = fragmentStartId;
            this.newBookie = newBookie;
            this.oldBookie = oldBookie;
        }
        
        @Override
        public void processResult(int rc, String path, Object ctx) {
            if (rc != Code.OK.intValue()) {
                LOG.error("BK error replicating ledger fragments for ledger: " + lh.getId(), 
                          BKException.create(rc));
                ledgerFragmentsMcb.processResult(rc, null, null);
                return;
            }
            /*
             * Update the ledger metadata's ensemble info to point
             * to the new bookie.
             */
            ArrayList<InetSocketAddress> ensemble = lh.getLedgerMetadata().getEnsembles().get(
                    fragmentStartId);
            int deadBookieIndex = ensemble.indexOf(oldBookie);
            ensemble.remove(deadBookieIndex);
            ensemble.add(deadBookieIndex, newBookie);
            
            
            lh.writeLedgerConfig(new WriteCb(), null);
        }
        
        private class WriteCb implements AsyncCallback.StatCallback {
            @Override
            public void processResult(int rc, String path, Object ctx, Stat stat) {
                if (rc == Code.BADVERSION.intValue()) {
                    LOG.warn("Two fragments attempted update at once; ledger id: " + lh.getId() 
                             + " startid: " + fragmentStartId);
                    // try again, the previous success (with which this has conflicted)
                    // will have updated the stat
                    lh.writeLedgerConfig(new WriteCb(), null);
                    return;
                } else if (rc != Code.OK.intValue()) {
                    LOG.error("ZK error updating ledger config metadata for ledgerId: " + lh.getId(),
                              KeeperException.create(KeeperException.Code.get(rc), path));
                } else {
                    lh.getLedgerMetadata().updateZnodeStatus(stat);
                    LOG.info("Updated ZK for ledgerId: (" + lh.getId() + " : " + fragmentStartId 
                             + ") to point ledger fragments from old dead bookie: (" + oldBookie
                             + ") to new bookie: (" + newBookie + ")");
                }
                /*
                 * Pass the return code result up the chain with
                 * the parent callback.
                 */
                ledgerFragmentsMcb.processResult(rc, null, null);
            }
        };
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/CRC32DigestManager.java,false,"package org.apache.bookkeeper.client;

/*
* Licensed to the Apache Software Foundation (ASF) under one
* or more contributor license agreements.  See the NOTICE file
* distributed with this work for additional information
* regarding copyright ownership.  The ASF licenses this file
* to you under the Apache License, Version 2.0 (the
* "License"); you may not use this file except in compliance
* with the License.  You may obtain a copy of the License at
*
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*/


import java.nio.ByteBuffer;
import java.util.zip.CRC32;

class CRC32DigestManager extends DigestManager {
    CRC32 crc = new CRC32();

    public CRC32DigestManager(long ledgerId) {
        super(ledgerId);
    }

    @Override
    int getMacCodeLength() {
        return 8;
    }

    @Override
    byte[] getValueAndReset() {
        byte[] value = new byte[8];
        ByteBuffer buf = ByteBuffer.wrap(value);
        buf.putLong(crc.getValue());
        crc.reset();
        return value;
    }

    @Override
    void update(byte[] data, int offset, int length) {
        crc.update(data, offset, length);
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/DigestManager.java,true,"package org.apache.bookkeeper.client;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.nio.ByteBuffer;
import java.security.GeneralSecurityException;

import org.apache.bookkeeper.client.BKException.BKDigestMatchException;
import org.apache.bookkeeper.client.BookKeeper.DigestType;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.buffer.ChannelBuffer;
import org.jboss.netty.buffer.ChannelBufferInputStream;
import org.jboss.netty.buffer.ChannelBuffers;

/**
 * This class takes an entry, attaches a digest to it and packages it with relevant
 * data so that it can be shipped to the bookie. On the return side, it also
 * gets a packet, checks that the digest matches, and extracts the original entry
 * for the packet. Currently 2 types of digests are supported: MAC (based on SHA-1) and CRC32
 */

abstract class DigestManager {
    static final Logger logger = LoggerFactory.getLogger(DigestManager.class);

    static final int METADATA_LENGTH = 32;

    long ledgerId;

    abstract int getMacCodeLength();

    void update(byte[] data) {
        update(data, 0, data.length);
    }

    abstract void update(byte[] data, int offset, int length);
    abstract byte[] getValueAndReset();

    final int macCodeLength;

    public DigestManager(long ledgerId) {
        this.ledgerId = ledgerId;
        macCodeLength = getMacCodeLength();
    }

    static DigestManager instantiate(long ledgerId, byte[] passwd, DigestType digestType) throws GeneralSecurityException {
        switch(digestType) {
        case MAC:
            return new MacDigestManager(ledgerId, passwd);
        case CRC32:
            return new CRC32DigestManager(ledgerId);
        default:
            throw new GeneralSecurityException("Unknown checksum type: " + digestType);
        }
    }

    /**
     * Computes the digest for an entry and put bytes together for sending.
     *
     * @param entryId
     * @param lastAddConfirmed
     * @param length
     * @param data
     * @return
     */

    public ChannelBuffer computeDigestAndPackageForSending(long entryId, long lastAddConfirmed, long length, byte[] data, int doffset, int dlength) {

        byte[] bufferArray = new byte[METADATA_LENGTH + macCodeLength];
        ByteBuffer buffer = ByteBuffer.wrap(bufferArray);
        buffer.putLong(ledgerId);
        buffer.putLong(entryId);
        buffer.putLong(lastAddConfirmed);
        buffer.putLong(length);
        buffer.flip();

        update(buffer.array(), 0, METADATA_LENGTH);
        update(data, doffset, dlength);
        byte[] digest = getValueAndReset();

        buffer.limit(buffer.capacity());
        buffer.position(METADATA_LENGTH);
        buffer.put(digest);
        buffer.flip();

        return ChannelBuffers.wrappedBuffer(ChannelBuffers.wrappedBuffer(buffer), ChannelBuffers.wrappedBuffer(data, doffset, dlength));
    }

    private void verifyDigest(ChannelBuffer dataReceived) throws BKDigestMatchException {
        verifyDigest(-1, dataReceived, true);
    }

    private void verifyDigest(long entryId, ChannelBuffer dataReceived) throws BKDigestMatchException {
        verifyDigest(entryId, dataReceived, false);
    }

    private void verifyDigest(long entryId, ChannelBuffer dataReceived, boolean skipEntryIdCheck)
            throws BKDigestMatchException {

        ByteBuffer dataReceivedBuffer = dataReceived.toByteBuffer();
        byte[] digest;

        update(dataReceivedBuffer.array(), dataReceivedBuffer.position(), METADATA_LENGTH);

        int offset = METADATA_LENGTH + macCodeLength;
        update(dataReceivedBuffer.array(), dataReceivedBuffer.position() + offset, dataReceived.readableBytes() - offset);
        digest = getValueAndReset();

        for (int i = 0; i < digest.length; i++) {
            if (digest[i] != dataReceived.getByte(METADATA_LENGTH + i)) {
                logger.error("Mac mismatch for ledger-id: " + ledgerId + ", entry-id: " + entryId);
                throw new BKDigestMatchException();
            }
        }

        long actualLedgerId = dataReceived.readLong();
        long actualEntryId = dataReceived.readLong();

        if (actualLedgerId != ledgerId) {
            logger.error("Ledger-id mismatch in authenticated message, expected: " + ledgerId + " , actual: "
                         + actualLedgerId);
            throw new BKDigestMatchException();
        }

        if (!skipEntryIdCheck && actualEntryId != entryId) {
            logger.error("Entry-id mismatch in authenticated message, expected: " + entryId + " , actual: "
                         + actualEntryId);
            throw new BKDigestMatchException();
        }

    }

    /**
     * Verify that the digest matches and returns the data in the entry.
     *
     * @param entryId
     * @param dataReceived
     * @return
     * @throws BKDigestMatchException
     */
    ChannelBufferInputStream verifyDigestAndReturnData(long entryId, ChannelBuffer dataReceived)
            throws BKDigestMatchException {
        verifyDigest(entryId, dataReceived);
        dataReceived.readerIndex(METADATA_LENGTH + macCodeLength);
        return new ChannelBufferInputStream(dataReceived);
    }

    static class RecoveryData {
        long lastAddConfirmed;
        long entryId;

        public RecoveryData(long lastAddConfirmed, long entryId) {
            this.lastAddConfirmed = lastAddConfirmed;
            this.entryId = entryId;
        }

    }

    RecoveryData verifyDigestAndReturnLastConfirmed(ChannelBuffer dataReceived) throws BKDigestMatchException {
        verifyDigest(dataReceived);
        dataReceived.readerIndex(8);

        long entryId = dataReceived.readLong();
        long lastAddConfirmed = dataReceived.readLong();
        long length = dataReceived.readLong();
        return new RecoveryData(lastAddConfirmed, entryId);

    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/DistributionSchedule.java,true,"package org.apache.bookkeeper.client;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * This interface determins how entries are distributed among bookies.
 *
 * Every entry gets replicated to some number of replicas. The first replica for
 * an entry is given a replicaIndex of 0, and so on. To distribute write load,
 * not all entries go to all bookies. Given an entry-id and replica index, an
 * {@link DistributionSchedule} determines which bookie that replica should go
 * to.
 */

interface DistributionSchedule {

    /**
     *
     * @param entryId
     * @param replicaIndex
     * @return index of bookie that should get this replica
     */
    public int getBookieIndex(long entryId, int replicaIndex);

    /**
     *
     * @param entryId
     * @param bookieIndex
     * @return -1 if the given bookie index is not a replica for the given
     *         entryId
     */
    public int getReplicaIndex(long entryId, int bookieIndex);

    /**
     * Specifies whether its ok to proceed with recovery given that we have
     * heard back from the given bookie index. These calls will be a made in a
     * sequence and an implementation of this interface should accumulate
     * history about which bookie indexes we have heard from. Once this method
     * has returned true, it wont be called again on the same instance
     *
     * @param bookieIndexHeardFrom
     * @return true if its ok to proceed with recovery
     */
    public boolean canProceedWithRecovery(int bookieIndexHeardFrom);
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/LedgerCreateOp.java,true,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.client;

import java.io.IOException;
import java.net.InetSocketAddress;
import java.security.GeneralSecurityException;
import java.util.ArrayList;
import org.apache.bookkeeper.client.AsyncCallback.CreateCallback;
import org.apache.bookkeeper.client.BKException.BKNotEnoughBookiesException;
import org.apache.bookkeeper.client.BookKeeper.DigestType;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.AsyncCallback.StatCallback;
import org.apache.zookeeper.data.Stat;

/**
 * Encapsulates asynchronous ledger create operation
 *
 */
class LedgerCreateOp implements GenericCallback<String>, StatCallback {

    static final Logger LOG = LoggerFactory.getLogger(LedgerCreateOp.class);

    CreateCallback cb;
    LedgerMetadata metadata;
    LedgerHandle lh;
    Object ctx;
    byte[] passwd;
    BookKeeper bk;
    DigestType digestType;

    /**
     * Constructor
     *
     * @param bk
     *       BookKeeper object
     * @param ensembleSize
     *       ensemble size
     * @param quorumSize
     *       quorum size
     * @param digestType
     *       digest type, either MAC or CRC32
     * @param passwd
     *       passowrd
     * @param cb
     *       callback implementation
     * @param ctx
     *       optional control object
     */

    LedgerCreateOp(BookKeeper bk, int ensembleSize, int quorumSize, DigestType digestType, byte[] passwd, CreateCallback cb, Object ctx) {
        this.bk = bk;
        this.metadata = new LedgerMetadata(ensembleSize, quorumSize);
        this.digestType = digestType;
        this.passwd = passwd;
        this.cb = cb;
        this.ctx = ctx;
    }

    /**
     * Initiates the operation
     */
    public void initiate() {
        bk.getLedgerManager().newLedgerPath(this);
    }

    /**
     * Callback when created ledger path.
     */
    @Override
    public void operationComplete(int rc, String ledgerPath) {

        if (rc != KeeperException.Code.OK.intValue()) {
            LOG.error("Could not create node for ledger",
                      KeeperException.create(KeeperException.Code.get(rc), ledgerPath));
            cb.createComplete(BKException.Code.ZKException, null, this.ctx);
            return;
        }

        /*
         * Extract ledger id.
         */
        long ledgerId;
        try {
            ledgerId = bk.getLedgerManager().getLedgerId(ledgerPath);
        } catch (IOException e) {
            LOG.error("Could not extract ledger-id from path:" + ledgerPath, e);
            cb.createComplete(BKException.Code.ZKException, null, this.ctx);
            return;
        }

        /*
         * Adding bookies to ledger handle
         */

        ArrayList<InetSocketAddress> ensemble;
        try {
            ensemble = bk.bookieWatcher.getNewBookies(metadata.ensembleSize);
        } catch (BKNotEnoughBookiesException e) {
            LOG.error("Not enough bookies to create ledger" + ledgerId);
            cb.createComplete(e.getCode(), null, this.ctx);
            return;
        }

        /*
         * Add ensemble to the configuration
         */
        metadata.addEnsemble(new Long(0), ensemble);
        try {
            lh = new LedgerHandle(bk, ledgerId, metadata, digestType, passwd);
        } catch (GeneralSecurityException e) {
            LOG.error("Security exception while creating ledger: " + ledgerId, e);
            cb.createComplete(BKException.Code.DigestNotInitializedException, null, this.ctx);
            return;
        } catch (NumberFormatException e) {
            LOG.error("Incorrectly entered parameter throttle: " + bk.getConf().getThrottleValue(), e);
            cb.createComplete(BKException.Code.IncorrectParameterException, null, this.ctx);
            return;
        }

        lh.writeLedgerConfig(this, null);

    }

    /**
     * Implements ZooKeeper stat callback.
     *
     * @see org.apache.zookeeper.AsyncCallback.StatCallback#processResult(int, String, Object, Stat)
     */
    public void processResult(int rc, String path, Object ctx, Stat stat) {
        metadata.znodeVersion = stat.getVersion();
        cb.createComplete(rc, lh, this.ctx);
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/LedgerDeleteOp.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.client;

import org.apache.bookkeeper.client.AsyncCallback.DeleteCallback;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.apache.zookeeper.AsyncCallback.VoidCallback;

/**
 * Encapsulates asynchronous ledger delete operation
 *
 */
class LedgerDeleteOp implements VoidCallback {

    static final Logger LOG = LoggerFactory.getLogger(LedgerDeleteOp.class);

    BookKeeper bk;
    long ledgerId;
    DeleteCallback cb;
    Object ctx;

    /**
     * Constructor
     *
     * @param bk
     *            BookKeeper object
     * @param ledgerId
     *            ledger Id
     * @param cb
     *            callback implementation
     * @param ctx
     *            optional control object
     */
    LedgerDeleteOp(BookKeeper bk, long ledgerId, DeleteCallback cb, Object ctx) {
        this.bk = bk;
        this.ledgerId = ledgerId;
        this.cb = cb;
        this.ctx = ctx;
    }

    /**
     * Initiates the operation
     */
    public void initiate() {
        // Asynchronously delete the ledger node in ZK.
        // When this completes, it will invoke the callback method below.

        bk.getZkHandle().delete(bk.getLedgerManager().getLedgerPath(ledgerId), -1, this, null);
    }

    /**
     * Implements ZooKeeper Void Callback.
     *
     * @see org.apache.zookeeper.AsyncCallback.VoidCallback#processResult(int,
     *      java.lang.String, java.lang.Object)
     */
    public void processResult(int rc, String path, Object ctx) {
        cb.deleteComplete(rc, this.ctx);
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/LedgerEntry.java,false,"package org.apache.bookkeeper.client;

/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

import java.io.IOException;
import java.io.InputStream;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.buffer.ChannelBufferInputStream;

/**
 * Ledger entry. Its a simple tuple containing the ledger id, the entry-id, and
 * the entry content.
 *
 */

public class LedgerEntry {
    Logger LOG = LoggerFactory.getLogger(LedgerEntry.class);

    long ledgerId;
    long entryId;
    long length;
    ChannelBufferInputStream entryDataStream;

    int nextReplicaIndexToReadFrom = 0;

    LedgerEntry(long lId, long eId) {
        this.ledgerId = lId;
        this.entryId = eId;
    }

    public long getLedgerId() {
        return ledgerId;
    }

    public long getEntryId() {
        return entryId;
    }

    public long getLength() {
        return length;
    }

    public byte[] getEntry() {
        try {
            // In general, you can't rely on the available() method of an input
            // stream, but ChannelBufferInputStream is backed by a byte[] so it
            // accurately knows the # bytes available
            byte[] ret = new byte[entryDataStream.available()];
            entryDataStream.readFully(ret);
            return ret;
        } catch (IOException e) {
            // The channelbufferinput stream doesnt really throw the
            // ioexceptions, it just has to be in the signature because
            // InputStream says so. Hence this code, should never be reached.
            LOG.error("Unexpected IOException while reading from channel buffer", e);
            return new byte[0];
        }
    }

    public InputStream getEntryInputStream() {
        return entryDataStream;
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/LedgerHandle.java,true,"package org.apache.bookkeeper.client;

/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */
import java.io.IOException;
import java.net.InetSocketAddress;
import java.security.GeneralSecurityException;
import java.util.ArrayDeque;
import java.util.ArrayList;
import java.util.Enumeration;
import java.util.Queue;
import java.util.concurrent.Semaphore;

import org.apache.bookkeeper.client.AsyncCallback.ReadLastConfirmedCallback;
import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.client.AsyncCallback.AddCallback;
import org.apache.bookkeeper.client.AsyncCallback.CloseCallback;
import org.apache.bookkeeper.client.AsyncCallback.ReadCallback;
import org.apache.bookkeeper.client.BKException.BKNotEnoughBookiesException;
import org.apache.bookkeeper.client.BookKeeper.DigestType;
import org.apache.bookkeeper.client.LedgerMetadata;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;
import org.apache.bookkeeper.util.SafeRunnable;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.AsyncCallback.StatCallback;
import org.apache.zookeeper.AsyncCallback.DataCallback;
import org.apache.zookeeper.data.Stat;
import org.jboss.netty.buffer.ChannelBuffer;

/**
 * Ledger handle contains ledger metadata and is used to access the read and
 * write operations to a ledger.
 */
public class LedgerHandle {
    final static Logger LOG = LoggerFactory.getLogger(LedgerHandle.class);
    final static long LAST_ADD_CONFIRMED = -1;

    final byte[] ledgerKey;
    LedgerMetadata metadata;
    final BookKeeper bk;
    final long ledgerId;
    long lastAddPushed;
    long lastAddConfirmed;
    long length;
    final DigestManager macManager;
    final DistributionSchedule distributionSchedule;

    final Semaphore opCounterSem;
    private final Integer throttling;

    final Queue<PendingAddOp> pendingAddOps = new ArrayDeque<PendingAddOp>();

    LedgerHandle(BookKeeper bk, long ledgerId, LedgerMetadata metadata,
                 DigestType digestType, byte[] password)
            throws GeneralSecurityException, NumberFormatException {
        this.bk = bk;
        this.metadata = metadata;

        if (metadata.isClosed()) {
            lastAddConfirmed = lastAddPushed = metadata.close;
            length = metadata.length;
        } else {
            lastAddConfirmed = lastAddPushed = -1;
            length = 0;
        }

        this.ledgerId = ledgerId;

        this.throttling = bk.getConf().getThrottleValue();
        this.opCounterSem = new Semaphore(throttling);

        macManager = DigestManager.instantiate(ledgerId, password, digestType);
        this.ledgerKey = MacDigestManager.genDigest("ledger", password);
        distributionSchedule = new RoundRobinDistributionSchedule(
            metadata.quorumSize, metadata.ensembleSize);
    }

    /**
     * Get the id of the current ledger
     *
     * @return the id of the ledger
     */
    public long getId() {
        return ledgerId;
    }

    /**
     * Get the last confirmed entry id on this ledger
     *
     * @return the last confirmed entry id
     */
    public long getLastAddConfirmed() {
        return lastAddConfirmed;
    }

    /**
     * Get the entry id of the last entry that has been enqueued for addition (but
     * may not have possibly been persited to the ledger)
     *
     * @return the id of the last entry pushed
     */
    public long getLastAddPushed() {
        return lastAddPushed;
    }

    /**
     * Get the Ledger's key/password.
     *
     * @return byte array for the ledger's key/password.
     */
    public byte[] getLedgerKey() {
        return ledgerKey;
    }

    /**
     * Get the LedgerMetadata
     *
     * @return LedgerMetadata for the LedgerHandle
     */
    LedgerMetadata getLedgerMetadata() {
        return metadata;
    }

    /**
     * Get the DigestManager
     *
     * @return DigestManager for the LedgerHandle
     */
    DigestManager getDigestManager() {
        return macManager;
    }

    /**
     * Return total number of available slots.
     *
     * @return int    available slots
     */
    Semaphore getAvailablePermits() {
        return this.opCounterSem;
    }

    /**
     *  Add to the length of the ledger in bytes.
     *
     * @param delta
     * @return
     */
    long addToLength(long delta) {
        this.length += delta;
        return this.length;
    }

    /**
     * Returns the length of the ledger in bytes.
     *
     * @return the length of the ledger in bytes
     */
    public long getLength() {
        return this.length;
    }

    /**
     * Get the Distribution Schedule
     *
     * @return DistributionSchedule for the LedgerHandle
     */
    DistributionSchedule getDistributionSchedule() {
        return distributionSchedule;
    }

    void writeLedgerConfig(StatCallback callback, Object ctx) {
        if (LOG.isDebugEnabled()) {
            LOG.debug("Writing metadata to ZooKeeper: " + this.ledgerId + ", " + metadata.getZnodeVersion());
        }

        bk.getZkHandle().setData(bk.getLedgerManager().getLedgerPath(ledgerId),
                                 metadata.serialize(), metadata.getZnodeVersion(),
                                 callback, ctx);
    }

    /**
     * Close this ledger synchronously.
     * @see #asyncClose
     */
    public void close() 
            throws InterruptedException, BKException {
        SyncCounter counter = new SyncCounter();
        counter.inc();

        asyncClose(new SyncCloseCallback(), counter);

        counter.block(0);
        if (counter.getrc() != BKException.Code.OK) {
            throw BKException.create(counter.getrc());
        }
    }

    /**
     * Asynchronous close, any adds in flight will return errors.
     * 
     * Closing a ledger will ensure that all clients agree on what the last entry 
     * of the ledger is. This ensures that, once the ledger has been closed, all 
     * reads from the ledger will return the same set of entries. 
     * 
     * @param cb
     *          callback implementation
     * @param ctx
     *          control object
     * @throws InterruptedException
     */
    public void asyncClose(CloseCallback cb, Object ctx) {
        asyncCloseInternal(cb, ctx, BKException.Code.LedgerClosedException);
    }

    /**
     * Same as public version of asyncClose except that this one takes an
     * additional parameter which is the return code to hand to all the pending
     * add ops
     *
     * @param cb
     * @param ctx
     * @param rc
     */
    void asyncCloseInternal(final CloseCallback cb, final Object ctx, final int rc) {
 
        bk.mainWorkerPool.submitOrdered(ledgerId, new SafeRunnable() {

            @Override
            public void safeRun() {
                metadata.length = length;
                // Close operation is idempotent, so no need to check if we are
                // already closed

                metadata.close(lastAddConfirmed);
                errorOutPendingAdds(rc);
                lastAddPushed = lastAddConfirmed;

                if (LOG.isDebugEnabled()) {
                    LOG.debug("Closing ledger: " + ledgerId + " at entryId: "
                              + metadata.close + " with this many bytes: " + metadata.length);
                }

                writeLedgerConfig(new StatCallback() {
                    @Override
                    public void processResult(int rc, String path, Object subctx,
                                              Stat stat) {
                        if (rc != KeeperException.Code.OK.intValue()) {
                            LOG.warn("Conditional write failed: " + KeeperException.Code.get(rc));
                            cb.closeComplete(BKException.Code.ZKException, LedgerHandle.this,
                                             ctx);
                        } else {
                            metadata.updateZnodeStatus(stat);
                            cb.closeComplete(BKException.Code.OK, LedgerHandle.this, ctx);
                        }
                    }
                }, null);

            }
        });
    }

    /**
     * Read a sequence of entries synchronously.
     *
     * @param firstEntry
     *          id of first entry of sequence (included)
     * @param lastEntry
     *          id of last entry of sequence (included)
     *
     */
    public Enumeration<LedgerEntry> readEntries(long firstEntry, long lastEntry)
            throws InterruptedException, BKException {
        SyncCounter counter = new SyncCounter();
        counter.inc();

        asyncReadEntries(firstEntry, lastEntry, new SyncReadCallback(), counter);

        counter.block(0);
        if (counter.getrc() != BKException.Code.OK) {
            throw BKException.create(counter.getrc());
        }

        return counter.getSequence();
    }

    /**
     * Read a sequence of entries asynchronously.
     *
     * @param firstEntry
     *          id of first entry of sequence
     * @param lastEntry
     *          id of last entry of sequence
     * @param cb
     *          object implementing read callback interface
     * @param ctx
     *          control object
     */
    public void asyncReadEntries(long firstEntry, long lastEntry,
                                 ReadCallback cb, Object ctx) {
        // Little sanity check
        if (firstEntry < 0 || lastEntry > lastAddConfirmed
                || firstEntry > lastEntry) {
            cb.readComplete(BKException.Code.ReadException, this, null, ctx);
            return;
        }

        try {
            new PendingReadOp(this, firstEntry, lastEntry, cb, ctx).initiate();
        } catch (InterruptedException e) {
            cb.readComplete(BKException.Code.InterruptedException, this, null, ctx);
        }
    }

    /**
     * Add entry synchronously to an open ledger.
     *
     * @param data
     *         array of bytes to be written to the ledger
     */
    public void addEntry(byte[] data) throws InterruptedException, BKException {
        addEntry(data, 0, data.length);
    }

    /**
     * Add entry synchronously to an open ledger.
     *
     * @param data
     *         array of bytes to be written to the ledger
     * @param offset
     *          offset from which to take bytes from data
     * @param length
     *          number of bytes to take from data
     */
    public void addEntry(byte[] data, int offset, int length)
            throws InterruptedException, BKException {
        LOG.debug("Adding entry " + data);
        SyncCounter counter = new SyncCounter();
        counter.inc();

        asyncAddEntry(data, offset, length, new SyncAddCallback(), counter);
        counter.block(0);
        
        if (counter.getrc() != BKException.Code.OK) {
            throw BKException.create(counter.getrc());
        }

        if(counter.getrc() != BKException.Code.OK) {
            throw BKException.create(counter.getrc());
        }
    }

    /**
     * Add entry asynchronously to an open ledger.
     *
     * @param data
     *          array of bytes to be written
     * @param cb
     *          object implementing callbackinterface
     * @param ctx
     *          some control object
     */
    public void asyncAddEntry(final byte[] data, final AddCallback cb,
                              final Object ctx) {
        asyncAddEntry(data, 0, data.length, cb, ctx);
    }

    /**
     * Add entry asynchronously to an open ledger, using an offset and range.
     *
     * @param data
     *          array of bytes to be written
     * @param offset
     *          offset from which to take bytes from data
     * @param length
     *          number of bytes to take from data
     * @param cb
     *          object implementing callbackinterface
     * @param ctx
     *          some control object
     * @throws ArrayIndexOutOfBoundsException if offset or length is negative or
     *          offset and length sum to a value higher than the length of data.
     */
    public void asyncAddEntry(final byte[] data, final int offset, final int length,
                              final AddCallback cb, final Object ctx) {
        PendingAddOp op = new PendingAddOp(LedgerHandle.this, cb, ctx);
        doAsyncAddEntry(op, data, offset, length, cb, ctx);
    }

    /**
     * Make a recovery add entry request. Recovery adds can add to a ledger even if
     * it has been fenced.
     *
     * This is only valid for bookie and ledger recovery, which may need to replicate
     * entries to a quorum of bookies to ensure data safety.
     *
     * Normal client should never call this method.
     */
    void asyncRecoveryAddEntry(final byte[] data, final int offset, final int length,
                               final AddCallback cb, final Object ctx) {
        PendingAddOp op = new PendingAddOp(LedgerHandle.this, cb, ctx).enableRecoveryAdd();
        doAsyncAddEntry(op, data, offset, length, cb, ctx);
    }

    private void doAsyncAddEntry(final PendingAddOp op, final byte[] data, final int offset, final int length,
                                 final AddCallback cb, final Object ctx) {
        if (offset < 0 || length < 0
                || (offset + length) > data.length) {
            throw new ArrayIndexOutOfBoundsException(
                "Invalid values for offset("+offset
                +") or length("+length+")");
        }
        try {
            opCounterSem.acquire();
        } catch (InterruptedException e) {
            cb.addComplete(BKException.Code.InterruptedException,
                           LedgerHandle.this, -1, ctx);
        }

        try {
            bk.mainWorkerPool.submitOrdered(ledgerId, new SafeRunnable() {
                @Override
                public void safeRun() {
                    if (metadata.isClosed()) {
                        LOG.warn("Attempt to add to closed ledger: " + ledgerId);
                        LedgerHandle.this.opCounterSem.release();
                        cb.addComplete(BKException.Code.LedgerClosedException,
                                       LedgerHandle.this, -1, ctx);
                        return;
                    }

                    long entryId = ++lastAddPushed;
                    long currentLength = addToLength(length);
                    op.setEntryId(entryId);
                    pendingAddOps.add(op);
                    ChannelBuffer toSend = macManager.computeDigestAndPackageForSending(
                                               entryId, lastAddConfirmed, currentLength, data, offset, length);
                    op.initiate(toSend);
                }
            });
        } catch (RuntimeException e) {
            opCounterSem.release();
            throw e;
        }
    }

    /**
     * Obtains last confirmed write from a quorum of bookies.
     *
     * @param cb
     * @param ctx
     */

    public void asyncReadLastConfirmed(ReadLastConfirmedCallback cb, Object ctx) {
        new ReadLastConfirmedOp(this, cb, ctx).initiate();
    }


    /**
     * Context objects for synchronous call to read last confirmed.
     */
    class LastConfirmedCtx {
        long response;
        int rc;

        LastConfirmedCtx() {
            this.response = -1;
        }

        void setLastConfirmed(long lastConfirmed) {
            this.response = lastConfirmed;
        }

        long getlastConfirmed() {
            return this.response;
        }

        void setRC(int rc) {
            this.rc = rc;
        }

        int getRC() {
            return this.rc;
        }

        boolean ready() {
            return (this.response != -1);
        }
    }

    public long readLastConfirmed()
            throws InterruptedException, BKException {
        LastConfirmedCtx ctx = new LastConfirmedCtx();
        asyncReadLastConfirmed(new SyncReadLastConfirmedCallback(), ctx);
        synchronized(ctx) {
            while(!ctx.ready()) {
                ctx.wait();
            }
        }

        if(ctx.getRC() != BKException.Code.OK) throw BKException.create(ctx.getRC());
        return ctx.getlastConfirmed();
    }

    // close the ledger and send fails to all the adds in the pipeline
    void handleUnrecoverableErrorDuringAdd(int rc) {
        asyncCloseInternal(NoopCloseCallback.instance, null, rc);
    }

    void errorOutPendingAdds(int rc) {
        PendingAddOp pendingAddOp;
        while ((pendingAddOp = pendingAddOps.poll()) != null) {
            pendingAddOp.submitCallback(rc);
        }
    }

    void sendAddSuccessCallbacks() {
        // Start from the head of the queue and proceed while there are
        // entries that have had all their responses come back
        PendingAddOp pendingAddOp;
        while ((pendingAddOp = pendingAddOps.peek()) != null) {
            if (pendingAddOp.numResponsesPending != 0) {
                return;
            }
            pendingAddOps.remove();
            lastAddConfirmed = pendingAddOp.entryId;
            pendingAddOp.submitCallback(BKException.Code.OK);
        }

    }

    void handleBookieFailure(InetSocketAddress addr, final int bookieIndex) {
        InetSocketAddress newBookie;

        if (LOG.isDebugEnabled()) {
            LOG.debug("Handling failure of bookie: " + addr + " index: "
                      + bookieIndex);
        }

        try {
            newBookie = bk.bookieWatcher
                        .getAdditionalBookie(metadata.currentEnsemble);
        } catch (BKNotEnoughBookiesException e) {
            LOG
            .error("Could not get additional bookie to remake ensemble, closing ledger: "
                   + ledgerId);
            handleUnrecoverableErrorDuringAdd(e.getCode());
            return;
        }

        final ArrayList<InetSocketAddress> newEnsemble = new ArrayList<InetSocketAddress>(
            metadata.currentEnsemble);
        newEnsemble.set(bookieIndex, newBookie);

        if (LOG.isDebugEnabled()) {
            LOG.debug("Changing ensemble from: " + metadata.currentEnsemble + " to: "
                      + newEnsemble + " for ledger: " + ledgerId + " starting at entry: "
                      + (lastAddConfirmed + 1));
        }

        metadata.addEnsemble(lastAddConfirmed + 1, newEnsemble);

        writeLedgerConfig(new StatCallback() {
            @Override
            public void processResult(final int rc, String path, Object ctx, final Stat stat) {

                bk.mainWorkerPool.submitOrdered(ledgerId, new SafeRunnable() {
                    @Override
                    public void safeRun() {
                        if (rc != KeeperException.Code.OK.intValue()) {
                            LOG
                            .error("Could not persist ledger metadata while changing ensemble to: "
                                   + newEnsemble + " , closing ledger");
                            handleUnrecoverableErrorDuringAdd(BKException.Code.ZKException);
                            return;
                        }

                        metadata.updateZnodeStatus(stat);
                        for (PendingAddOp pendingAddOp : pendingAddOps) {
                            pendingAddOp.unsetSuccessAndSendWriteRequest(bookieIndex);
                        }
                    }
                });

            }
        }, null);

    }

    void rereadMetadata(final GenericCallback<Void> cb) {
        bk.getZkHandle().getData(bk.getLedgerManager().getLedgerPath(ledgerId), false,
                new DataCallback() {
                    public void processResult(int rc, String path,
                                              Object ctx, byte[] data, Stat stat) {
                        if (rc != KeeperException.Code.OK.intValue()) {
                            LOG.error("Error reading metadata from ledger, code =" + rc);
                            cb.operationComplete(BKException.Code.ZKException, null);
                            return;
                        }
                        
                        try {
                            metadata = LedgerMetadata.parseConfig(data, stat.getVersion());
                        } catch (IOException e) {
                            LOG.error("Error parsing ledger metadata for ledger", e);
                            cb.operationComplete(BKException.Code.ZKException, null);
                        }
                        cb.operationComplete(BKException.Code.OK, null);
                    }
                }, null);
    }

    void recover(final GenericCallback<Void> cb) {
        if (metadata.isClosed()) {
            lastAddConfirmed = lastAddPushed = metadata.close;
            length = metadata.length;

            // We are already closed, nothing to do
            cb.operationComplete(BKException.Code.OK, null);
            return;
        }

        metadata.markLedgerInRecovery();

        writeLedgerConfig(new StatCallback() {
            @Override
            public void processResult(final int rc, String path, Object ctx, Stat stat) {
                if (rc == KeeperException.Code.BadVersion) {
                    rereadMetadata(new GenericCallback<Void>() {
                            @Override
                            public void operationComplete(int rc, Void result) {
                                if (rc != BKException.Code.OK) {
                                    cb.operationComplete(rc, null);
                                } else {
                                    recover(cb);
                                }
                            }
                        });
                } else if (rc == KeeperException.Code.OK.intValue()) {
                    metadata.znodeVersion = stat.getVersion();
                    new LedgerRecoveryOp(LedgerHandle.this, cb).initiate();
                } else {
                    LOG.error("Error writing ledger config " +  rc 
                              + " path = " + path);
                    cb.operationComplete(BKException.Code.ZKException, null);
                }
            }
        }, null);
    }

    static class NoopCloseCallback implements CloseCallback {
        static NoopCloseCallback instance = new NoopCloseCallback();

        @Override
        public void closeComplete(int rc, LedgerHandle lh, Object ctx) {
            if (rc != KeeperException.Code.OK.intValue()) {
                LOG.warn("Close failed: " + BKException.getMessage(rc));
            }
            // noop
        }
    }
    
    private static class SyncReadCallback implements ReadCallback {
        /**
         * Implementation of callback interface for synchronous read method.
         *
         * @param rc
         *          return code
         * @param leder
         *          ledger identifier
         * @param seq
         *          sequence of entries
         * @param ctx
         *          control object
         */
        public void readComplete(int rc, LedgerHandle lh,
                                 Enumeration<LedgerEntry> seq, Object ctx) {
            
            SyncCounter counter = (SyncCounter) ctx;
            synchronized (counter) {
                counter.setSequence(seq);
                counter.setrc(rc);
                counter.dec();
                counter.notify();
            }
        }
    }

    private static class SyncAddCallback implements AddCallback {
        /**
         * Implementation of callback interface for synchronous read method.
         *
         * @param rc
         *          return code
         * @param leder
         *          ledger identifier
         * @param entry
         *          entry identifier
         * @param ctx
         *          control object
         */
        public void addComplete(int rc, LedgerHandle lh, long entry, Object ctx) {
            SyncCounter counter = (SyncCounter) ctx;
            
            counter.setrc(rc);
            counter.dec();
        }
    }

    private static class SyncReadLastConfirmedCallback implements ReadLastConfirmedCallback {
        /**
         * Implementation of  callback interface for synchronous read last confirmed method.
         */
        public void readLastConfirmedComplete(int rc, long lastConfirmed, Object ctx) {
            LastConfirmedCtx lcCtx = (LastConfirmedCtx) ctx;
            
            synchronized(lcCtx) {
                lcCtx.setRC(rc);
                lcCtx.setLastConfirmed(lastConfirmed);
                lcCtx.notify();
            }
        }
    }

    private static class SyncCloseCallback implements CloseCallback {
        /**
         * Close callback method
         *
         * @param rc
         * @param lh
         * @param ctx
         */
        public void closeComplete(int rc, LedgerHandle lh, Object ctx) {
            SyncCounter counter = (SyncCounter) ctx;
            counter.setrc(rc);
            synchronized (counter) {
                counter.dec();
                counter.notify();
            }
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/LedgerMetadata.java,true,"package org.apache.bookkeeper.client;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.IOException;
import java.net.InetSocketAddress;
import java.util.ArrayList;
import java.util.Map;
import java.util.SortedMap;
import java.util.TreeMap;

import org.apache.bookkeeper.util.StringUtils;
import org.apache.zookeeper.data.Stat;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * This class encapsulates all the ledger metadata that is persistently stored
 * in zookeeper. It provides parsing and serialization methods of such metadata.
 *
 */
public class LedgerMetadata {
    static final Logger LOG = LoggerFactory.getLogger(LedgerMetadata.class);

    private static final String closed = "CLOSED";
    private static final String lSplitter = "\n";
    private static final String tSplitter = "\t";

    // can't use -1 for NOTCLOSED because that is reserved for a closed, empty
    // ledger
    public static final int NOTCLOSED = -101;
    public static final int IN_RECOVERY = -102;

    public static final int LOWEST_COMPAT_METADATA_FORMAT_VERSION = 0;
    public static final int CURRENT_METADATA_FORMAT_VERSION = 1;
    public static final String VERSION_KEY = "BookieMetadataFormatVersion";

    int metadataFormatVersion = 0;

    int ensembleSize;
    int quorumSize;
    long length;
    long close;
    private SortedMap<Long, ArrayList<InetSocketAddress>> ensembles = new TreeMap<Long, ArrayList<InetSocketAddress>>();
    ArrayList<InetSocketAddress> currentEnsemble;
    volatile int znodeVersion = -1;
    
    public LedgerMetadata(int ensembleSize, int quorumSize) {
        this.ensembleSize = ensembleSize;
        this.quorumSize = quorumSize;

        /*
         * It is set in PendingReadOp.readEntryComplete, and
         * we read it in LedgerRecoveryOp.readComplete.
         */
        this.length = 0;
        this.close = NOTCLOSED;
        this.metadataFormatVersion = CURRENT_METADATA_FORMAT_VERSION;
    };

    private LedgerMetadata() {
        this(0, 0);
    }

    /**
     * Get the Map of bookie ensembles for the various ledger fragments
     * that make up the ledger.
     *
     * @return SortedMap of Ledger Fragments and the corresponding
     * bookie ensembles that store the entries.
     */
    public SortedMap<Long, ArrayList<InetSocketAddress>> getEnsembles() {
        return ensembles;
    }

    boolean isClosed() {
        return close != NOTCLOSED 
            && close != IN_RECOVERY;
    }
    
    void markLedgerInRecovery() {
        close = IN_RECOVERY;
    }

    void close(long entryId) {
        close = entryId;
    }

    void addEnsemble(long startEntryId, ArrayList<InetSocketAddress> ensemble) {
        assert ensembles.isEmpty() || startEntryId >= ensembles.lastKey();

        ensembles.put(startEntryId, ensemble);
        currentEnsemble = ensemble;
    }

    ArrayList<InetSocketAddress> getEnsemble(long entryId) {
        // the head map cannot be empty, since we insert an ensemble for
        // entry-id 0, right when we start
        return ensembles.get(ensembles.headMap(entryId + 1).lastKey());
    }

    /**
     * the entry id > the given entry-id at which the next ensemble change takes
     * place ( -1 if no further ensemble changes)
     *
     * @param entryId
     * @return
     */
    long getNextEnsembleChange(long entryId) {
        SortedMap<Long, ArrayList<InetSocketAddress>> tailMap = ensembles.tailMap(entryId + 1);

        if (tailMap.isEmpty()) {
            return -1;
        } else {
            return tailMap.firstKey();
        }
    }

    /**
     * Generates a byte array of this object
     *
     * @return the metadata serialized into a byte array
     */
    public byte[] serialize() {
        StringBuilder s = new StringBuilder();
        s.append(VERSION_KEY).append(tSplitter).append(metadataFormatVersion).append(lSplitter);
        s.append(quorumSize).append(lSplitter).append(ensembleSize).append(lSplitter).append(length);

        for (Map.Entry<Long, ArrayList<InetSocketAddress>> entry : ensembles.entrySet()) {
            s.append(lSplitter).append(entry.getKey());
            for (InetSocketAddress addr : entry.getValue()) {
                s.append(tSplitter);
                StringUtils.addrToString(s, addr);
            }
        }

        if (close != NOTCLOSED) {
            s.append(lSplitter).append(close).append(tSplitter).append(closed);
        }

        if (LOG.isDebugEnabled()) {
            LOG.debug("Serialized config: " + s.toString());
        }

        return s.toString().getBytes();
    }

    /**
     * Parses a given byte array and transforms into a LedgerConfig object
     *
     * @param array
     *            byte array to parse
     * @return LedgerConfig
     * @throws IOException
     *             if the given byte[] cannot be parsed
     */

    static LedgerMetadata parseConfig(byte[] bytes, int version) throws IOException {

        LedgerMetadata lc = new LedgerMetadata();
        String config = new String(bytes);

        if (LOG.isDebugEnabled()) {
            LOG.debug("Parsing Config: " + config);
        }
        
        String lines[] = config.split(lSplitter);
        
        try {
            int i = 0;
            if (lines[0].startsWith(VERSION_KEY)) {
                String parts[] = lines[0].split(tSplitter);
                lc.metadataFormatVersion = new Integer(parts[1]);
                i++;
            } else {
                lc.metadataFormatVersion = 0;
            }
            
            if (lc.metadataFormatVersion < LOWEST_COMPAT_METADATA_FORMAT_VERSION
                || lc.metadataFormatVersion > CURRENT_METADATA_FORMAT_VERSION) {
                throw new IOException("Metadata version not compatible. Expected between "
                        + LOWEST_COMPAT_METADATA_FORMAT_VERSION + " and " + CURRENT_METADATA_FORMAT_VERSION
                        + ", but got " + lc.metadataFormatVersion);
            }
            if ((lines.length+i) < 2) {
                throw new IOException("Quorum size or ensemble size absent from config: " + config);
            }

            lc.znodeVersion = version;
            lc.quorumSize = new Integer(lines[i++]);
            lc.ensembleSize = new Integer(lines[i++]);
            lc.length = new Long(lines[i++]);

            for (; i < lines.length; i++) {
                String parts[] = lines[i].split(tSplitter);

                if (parts[1].equals(closed)) {
                    lc.close = new Long(parts[0]);
                    break;
                }

                ArrayList<InetSocketAddress> addrs = new ArrayList<InetSocketAddress>();
                for (int j = 1; j < parts.length; j++) {
                    addrs.add(StringUtils.parseAddr(parts[j]));
                }
                lc.addEnsemble(new Long(parts[0]), addrs);
            }
        } catch (NumberFormatException e) {
            throw new IOException(e);
        }
        return lc;
    }
    

    /**
     * Updates the status of this metadata in ZooKeeper.
     * 
     * @param stat
     */
    public void updateZnodeStatus(Stat stat) {
        this.znodeVersion = stat.getVersion();
    }
    
    /**
     * Returns the last znode version.
     * 
     * @return int znode version
     */
    public int getZnodeVersion() {
        return this.znodeVersion;
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/LedgerOpenOp.java,true,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.client;

import java.io.IOException;
import java.security.GeneralSecurityException;
import org.apache.bookkeeper.client.AsyncCallback.OpenCallback;
import org.apache.bookkeeper.client.AsyncCallback.ReadLastConfirmedCallback;
import org.apache.bookkeeper.client.BookKeeper.DigestType;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.AsyncCallback.DataCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;
import org.apache.zookeeper.data.Stat;

/**
 * Encapsulates the ledger open operation
 *
 */
class LedgerOpenOp implements DataCallback {
    static final Logger LOG = LoggerFactory.getLogger(LedgerOpenOp.class);

    final BookKeeper bk;
    final long ledgerId;
    final OpenCallback cb;
    final Object ctx;
    LedgerHandle lh;
    final byte[] passwd;
    final DigestType digestType;
    boolean doRecovery;

    /**
     * Constructor.
     *
     * @param bk
     * @param ledgerId
     * @param digestType
     * @param passwd
     * @param cb
     * @param ctx
     */
    public LedgerOpenOp(BookKeeper bk, long ledgerId, DigestType digestType, byte[] passwd, 
                        OpenCallback cb, Object ctx) {
        this.bk = bk;
        this.ledgerId = ledgerId;
        this.passwd = passwd;
        this.cb = cb;
        this.ctx = ctx;
        this.digestType = digestType;

        this.doRecovery = true;
    }

    /**
     * Inititates the ledger open operation
     */
    public void initiate() {
        /**
         * Asynchronously read the ledger metadata node.
         */

        bk.getZkHandle().getData(bk.getLedgerManager().getLedgerPath(ledgerId), false, this, ctx);
    }

    /**
     * Inititates the ledger open operation without recovery
     */
    public void initiateWithoutRecovery() {
        this.doRecovery = false;
        initiate();
    }

    /**
     * Implements ZooKeeper data callback.
     * @see org.apache.zookeeper.AsyncCallback.DataCallback#processResult(int, String, Object, byte[], Stat)
     */
    public void processResult(int rc, String path, Object ctx, byte[] data, Stat stat) {

        if (rc == KeeperException.Code.NONODE.intValue()) {
            if (LOG.isDebugEnabled()) {
                LOG.debug("No such ledger: " + ledgerId, KeeperException.create(KeeperException.Code.get(rc), path));
            }
            cb.openComplete(BKException.Code.NoSuchLedgerExistsException, null, this.ctx);
            return;
        }
        if (rc != KeeperException.Code.OK.intValue()) {
            LOG.error("Could not read metadata for ledger: " + ledgerId, KeeperException.create(KeeperException.Code
                      .get(rc), path));
            cb.openComplete(BKException.Code.ZKException, null, this.ctx);
            return;
        }

        LedgerMetadata metadata;
        try {
            metadata = LedgerMetadata.parseConfig(data, stat.getVersion());
        } catch (IOException e) {
            LOG.error("Could not parse ledger metadata for ledger: " + ledgerId, e);
            cb.openComplete(BKException.Code.ZKException, null, this.ctx);
            return;
        }

        try {
            lh = new ReadOnlyLedgerHandle(bk, ledgerId, metadata, digestType, passwd);
        } catch (GeneralSecurityException e) {
            LOG.error("Security exception while opening ledger: " + ledgerId, e);
            cb.openComplete(BKException.Code.DigestNotInitializedException, null, this.ctx);
            return;
        } catch (NumberFormatException e) {
            LOG.error("Incorrectly entered parameter throttle: " + bk.getConf().getThrottleValue(), e);
            cb.openComplete(BKException.Code.IncorrectParameterException, null, this.ctx);
            return;
        }

        if (metadata.isClosed()) {
            // Ledger was closed properly
            cb.openComplete(BKException.Code.OK, lh, this.ctx);
            return;
        }

        if (doRecovery) {
            lh.recover(new GenericCallback<Void>() {
                    @Override
                    public void operationComplete(int rc, Void result) {
                        if (rc != BKException.Code.OK) {
                            cb.openComplete(BKException.Code.LedgerRecoveryException, null, LedgerOpenOp.this.ctx);
                        } else {
                            cb.openComplete(BKException.Code.OK, lh, LedgerOpenOp.this.ctx);
                        }
                    }
                });
        } else {
            lh.asyncReadLastConfirmed(new ReadLastConfirmedCallback() {

                @Override
                public void readLastConfirmedComplete(int rc,
                        long lastConfirmed, Object ctx) {
                    if (rc != BKException.Code.OK) {
                        cb.openComplete(BKException.Code.ReadException, null, LedgerOpenOp.this.ctx);
                    } else {
                        lh.lastAddConfirmed = lh.lastAddPushed = lastConfirmed;
                        cb.openComplete(BKException.Code.OK, lh, LedgerOpenOp.this.ctx);
                    }
                }
                
            }, null);
            
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/LedgerRecoveryOp.java,true,"package org.apache.bookkeeper.client;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.util.Enumeration;

import org.apache.bookkeeper.client.AsyncCallback.AddCallback;
import org.apache.bookkeeper.client.AsyncCallback.CloseCallback;
import org.apache.bookkeeper.client.AsyncCallback.ReadCallback;
import org.apache.bookkeeper.client.BKException.BKDigestMatchException;
import org.apache.bookkeeper.client.LedgerHandle.NoopCloseCallback;
import org.apache.bookkeeper.client.DigestManager.RecoveryData;
import org.apache.bookkeeper.proto.BookieProtocol;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.ReadEntryCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;

import org.apache.zookeeper.KeeperException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.buffer.ChannelBuffer;

/**
 * This class encapsulated the ledger recovery operation. It first does a read
 * with entry-id of -1 (LedgerHandle.LAST_ADD_CONFIRMED) to all bookies. Then
 * starting from the last confirmed entry (from hints in the ledger entries),
 * it reads forward until it is not able to find a particular entry. It closes
 * the ledger at that entry.
 *
 */
class LedgerRecoveryOp implements ReadEntryCallback, ReadCallback, AddCallback {
    static final Logger LOG = LoggerFactory.getLogger(LedgerRecoveryOp.class);
    LedgerHandle lh;
    int numResponsesPending;
    boolean proceedingWithRecovery = false;
    long maxAddPushed = -1;
    long maxAddConfirmed = -1;
    long maxLength = 0;

    GenericCallback<Void> cb;

    public LedgerRecoveryOp(LedgerHandle lh, GenericCallback<Void> cb) {
        this.cb = cb;
        this.lh = lh;
        numResponsesPending = lh.metadata.ensembleSize;
    }

    public void initiate() {
        /** 
         * Enable fencing on this op. When the read request reaches the bookies
         * server it will fence off the ledger, stopping any subsequent operation
         * from writing to it.
         */
        int flags = BookieProtocol.FLAG_DO_FENCING;
        for (int i = 0; i < lh.metadata.currentEnsemble.size(); i++) {
            lh.bk.bookieClient.readEntry(lh.metadata.currentEnsemble.get(i), lh.ledgerId, 
                                         LedgerHandle.LAST_ADD_CONFIRMED, this, i, flags);
        }
    }

    public synchronized void readEntryComplete(final int rc, final long ledgerId, final long entryId,
            final ChannelBuffer buffer, final Object ctx) {

        // Already proceeding with recovery, nothing to do
        if (proceedingWithRecovery) {
            return;
        }

        int bookieIndex = (Integer) ctx;

        numResponsesPending--;

        boolean heardValidResponse = false;

        if (rc == BKException.Code.OK) {
            try {
                RecoveryData recoveryData = lh.macManager.verifyDigestAndReturnLastConfirmed(buffer);
                maxAddConfirmed = Math.max(maxAddConfirmed, recoveryData.lastAddConfirmed);
                maxAddPushed = Math.max(maxAddPushed, recoveryData.entryId);
                heardValidResponse = true;
            } catch (BKDigestMatchException e) {
                // Too bad, this bookie didnt give us a valid answer, we
                // still might be able to recover though so continue
                LOG.error("Mac mismatch while reading last entry from bookie: "
                          + lh.metadata.currentEnsemble.get(bookieIndex));
            }
        }

        if (rc == BKException.Code.NoSuchLedgerExistsException || rc == BKException.Code.NoSuchEntryException) {
            // this still counts as a valid response, e.g., if the
            // client
            // crashed without writing any entry
            heardValidResponse = true;
        }

        // other return codes dont count as valid responses
        if (heardValidResponse && lh.distributionSchedule.canProceedWithRecovery(bookieIndex)) {
            proceedingWithRecovery = true;
            lh.lastAddPushed = lh.lastAddConfirmed = maxAddConfirmed;
            lh.length = maxLength;
            doRecoveryRead();
            return;
        }

        if (numResponsesPending == 0) {
            // Have got all responses back but was still not enough to
            // start
            // recovery, just fail the operation
            LOG.error("While recovering ledger: " + ledgerId + " did not hear success responses from all quorums");
            cb.operationComplete(BKException.Code.LedgerRecoveryException, null);
        }

    }

    /**
     * Try to read past the last confirmed.
     */
    private void doRecoveryRead() {
        lh.lastAddConfirmed++;
        lh.asyncReadEntries(lh.lastAddConfirmed, lh.lastAddConfirmed, this, null);
    }

    @Override
    public void readComplete(int rc, LedgerHandle lh, Enumeration<LedgerEntry> seq, Object ctx) {
        // get back to prev value
        lh.lastAddConfirmed--;
        if (rc == BKException.Code.OK) {
            LedgerEntry entry = seq.nextElement();
            byte[] data = entry.getEntry();

            /*
             * We will add this entry again to make sure it is written to enough
             * replicas. We subtract the length of the data itself, since it will
             * be added again when processing the call to add it.
             */
            lh.length = entry.getLength() - (long) data.length;
            lh.asyncRecoveryAddEntry(data, 0, data.length, this, null);

            return;
        }

        if (rc == BKException.Code.NoSuchEntryException || rc == BKException.Code.NoSuchLedgerExistsException) {
            lh.asyncCloseInternal(new CloseCallback() {
                @Override
                public void closeComplete(int rc, LedgerHandle lh, Object ctx) {
                    if (rc != KeeperException.Code.OK.intValue()) {
                        LOG.warn("Close failed: " + BKException.getMessage(rc));
                        cb.operationComplete(BKException.Code.ZKException, null);
                    } else {
                        cb.operationComplete(BKException.Code.OK, null);
                        LOG.debug("After closing length is: " + lh.getLength()); 
                    }
                } 
                
                }, null, BKException.Code.LedgerClosedException);
            return;
        }

        // otherwise, some other error, we can't handle
        LOG.error("Failure " + BKException.getMessage(rc) + " while reading entry: " + lh.lastAddConfirmed + 1
                  + " ledger: " + lh.ledgerId + " while recovering ledger");
        cb.operationComplete(rc, null);
        return;
    }

    @Override
    public void addComplete(int rc, LedgerHandle lh, long entryId, Object ctx) {
        if (rc != BKException.Code.OK) {
            // Give up, we can't recover from this error

            LOG.error("Failure " + BKException.getMessage(rc) + " while writing entry: " + lh.lastAddConfirmed + 1
                      + " ledger: " + lh.ledgerId + " while recovering ledger");
            cb.operationComplete(rc, null);
            return;
        }

        doRecoveryRead();

    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/MacDigestManager.java,true,"package org.apache.bookkeeper.client;

/*
* Licensed to the Apache Software Foundation (ASF) under one
* or more contributor license agreements.  See the NOTICE file
* distributed with this work for additional information
* regarding copyright ownership.  The ASF licenses this file
* to you under the Apache License, Version 2.0 (the
* "License"); you may not use this file except in compliance
* with the License.  You may obtain a copy of the License at
*
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*/

import java.security.GeneralSecurityException;
import java.security.MessageDigest;
import java.security.NoSuchAlgorithmException;

import javax.crypto.Mac;
import javax.crypto.spec.SecretKeySpec;

class MacDigestManager extends DigestManager {
    public static String DIGEST_ALGORITHM = "SHA-1";
    public static String KEY_ALGORITHM = "HmacSHA1";
    Mac mac;

    public MacDigestManager(long ledgerId, byte[] passwd) throws GeneralSecurityException {
        super(ledgerId);
        byte[] macKey = genDigest("mac", passwd);
        SecretKeySpec keySpec = new SecretKeySpec(macKey, KEY_ALGORITHM);
        mac = Mac.getInstance(KEY_ALGORITHM);
        mac.init(keySpec);


    }

    static byte[] genDigest(String pad, byte[] passwd) throws NoSuchAlgorithmException {
        MessageDigest digest = MessageDigest.getInstance(DIGEST_ALGORITHM);
        digest.update(pad.getBytes());
        digest.update(passwd);
        return digest.digest();
    }

    @Override
    int getMacCodeLength() {
        return 20;
    }


    @Override
    byte[] getValueAndReset() {
        return mac.doFinal();
    }

    @Override
    void update(byte[] data, int offset, int length) {
        mac.update(data, offset, length);
    }


}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/PendingAddOp.java,true,"package org.apache.bookkeeper.client;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.net.InetSocketAddress;
import org.apache.bookkeeper.client.AsyncCallback.AddCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.WriteCallback;
import org.apache.bookkeeper.proto.BookieProtocol;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.buffer.ChannelBuffer;

/**
 * This represents a pending add operation. When it has got success from all
 * bookies, it sees if its at the head of the pending adds queue, and if yes,
 * sends ack back to the application. If a bookie fails, a replacement is made
 * and placed at the same position in the ensemble. The pending adds are then
 * rereplicated.
 *
 *
 */
class PendingAddOp implements WriteCallback {
    final static Logger LOG = LoggerFactory.getLogger(PendingAddOp.class);

    ChannelBuffer toSend;
    AddCallback cb;
    Object ctx;
    long entryId;
    boolean[] successesSoFar;
    int numResponsesPending;
    LedgerHandle lh;
    boolean isRecoveryAdd = false;

    PendingAddOp(LedgerHandle lh, AddCallback cb, Object ctx) {
        this.lh = lh;
        this.cb = cb;
        this.ctx = ctx;
        this.entryId = -1;
        
        successesSoFar = new boolean[lh.metadata.quorumSize];
        numResponsesPending = successesSoFar.length;
    }

    /** 
     * Enable the recovery add flag for this operation.
     * @see LedgerHandle#asyncRecoveryAddEntry
     */
    PendingAddOp enableRecoveryAdd() {
        isRecoveryAdd = true;
        return this;
    }

    void setEntryId(long entryId) {
        this.entryId = entryId;
    }

    void sendWriteRequest(int bookieIndex, int arrayIndex) {
        int flags = isRecoveryAdd ? BookieProtocol.FLAG_RECOVERY_ADD : BookieProtocol.FLAG_NONE;

        lh.bk.bookieClient.addEntry(lh.metadata.currentEnsemble.get(bookieIndex), lh.ledgerId, lh.ledgerKey, entryId, toSend,
                this, arrayIndex, flags);
    }

    void unsetSuccessAndSendWriteRequest(int bookieIndex) {
        if (toSend == null) {
            // this addOp hasn't yet had its mac computed. When the mac is
            // computed, its write requests will be sent, so no need to send it
            // now
            return;
        }

        int replicaIndex = lh.distributionSchedule.getReplicaIndex(entryId, bookieIndex);
        if (replicaIndex < 0) {
            if (LOG.isDebugEnabled()) {
                LOG.debug("Leaving unchanged, ledger: " + lh.ledgerId + " entry: " + entryId + " bookie index: "
                          + bookieIndex);
            }
            return;
        }

        if (LOG.isDebugEnabled()) {
            LOG.debug("Unsetting success for ledger: " + lh.ledgerId + " entry: " + entryId + " bookie index: "
                      + bookieIndex);
        }

        // if we had already heard a success from this array index, need to
        // increment our number of responses that are pending, since we are
        // going to unset this success
        if (successesSoFar[replicaIndex]) {
            successesSoFar[replicaIndex] = false;
            numResponsesPending++;
        }

        sendWriteRequest(bookieIndex, replicaIndex);
    }

    void initiate(ChannelBuffer toSend) {
        this.toSend = toSend;
        for (int i = 0; i < successesSoFar.length; i++) {
            int bookieIndex = lh.distributionSchedule.getBookieIndex(entryId, i);
            sendWriteRequest(bookieIndex, i);
        }
    }

    @Override
    public void writeComplete(int rc, long ledgerId, long entryId, InetSocketAddress addr, Object ctx) {

        Integer replicaIndex = (Integer) ctx;
        int bookieIndex = lh.distributionSchedule.getBookieIndex(entryId, replicaIndex);

        if (!lh.metadata.currentEnsemble.get(bookieIndex).equals(addr)) {
            // ensemble has already changed, failure of this addr is immaterial
            LOG.warn("Write did not succeed: " + ledgerId + ", " + entryId + ". But we have already fixed it.");
            return;
        }

        switch (rc) {
        case BKException.Code.OK:
            // continue
            break;
        case BKException.Code.LedgerFencedException:
            LOG.warn("Fencing exception on write: " + ledgerId + ", " + entryId);
            lh.handleUnrecoverableErrorDuringAdd(rc);
            return;
        default:
            LOG.warn("Write did not succeed: " + ledgerId + ", " + entryId);
            lh.handleBookieFailure(addr, bookieIndex);
            return;
        }


        if (!successesSoFar[replicaIndex]) {
            successesSoFar[replicaIndex] = true;
            numResponsesPending--;

            // do some quick checks to see if some adds may have finished. All
            // this will be checked under locks again
            if (numResponsesPending == 0 && lh.pendingAddOps.peek() == this) {
                lh.sendAddSuccessCallbacks();
            }
        }
    }

    void submitCallback(final int rc) {
        cb.addComplete(rc, lh, entryId, ctx);
        lh.opCounterSem.release();
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/PendingReadOp.java,true,"package org.apache.bookkeeper.client;

/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

import java.net.InetSocketAddress;
import java.nio.ByteBuffer;
import java.util.ArrayDeque;
import java.util.ArrayList;
import java.util.Enumeration;
import java.util.NoSuchElementException;
import java.util.Queue;
import org.apache.bookkeeper.client.AsyncCallback.ReadCallback;
import org.apache.bookkeeper.client.BKException.BKDigestMatchException;
import org.apache.bookkeeper.proto.BookieProtocol;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.ReadEntryCallback;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.buffer.ChannelBuffer;
import org.jboss.netty.buffer.ChannelBufferInputStream;

import java.io.IOException;

/**
 * Sequence of entries of a ledger that represents a pending read operation.
 * When all the data read has come back, the application callback is called.
 * This class could be improved because we could start pushing data to the
 * application as soon as it arrives rather than waiting for the whole thing.
 *
 */

class PendingReadOp implements Enumeration<LedgerEntry>, ReadEntryCallback {
    Logger LOG = LoggerFactory.getLogger(PendingReadOp.class);

    Queue<LedgerEntry> seq;
    ReadCallback cb;
    Object ctx;
    LedgerHandle lh;
    long numPendingReads;
    long startEntryId;
    long endEntryId;

    PendingReadOp(LedgerHandle lh, long startEntryId, long endEntryId, ReadCallback cb, Object ctx) {

        seq = new ArrayDeque<LedgerEntry>((int) (endEntryId - startEntryId));
        this.cb = cb;
        this.ctx = ctx;
        this.lh = lh;
        this.startEntryId = startEntryId;
        this.endEntryId = endEntryId;
        numPendingReads = endEntryId - startEntryId + 1;
    }

    public void initiate() throws InterruptedException {
        long nextEnsembleChange = startEntryId, i = startEntryId;

        ArrayList<InetSocketAddress> ensemble = null;
        do {

            if(LOG.isDebugEnabled()) {
                LOG.debug("Acquiring lock: " + i);
            }

            lh.opCounterSem.acquire();

            if (i == nextEnsembleChange) {
                ensemble = lh.metadata.getEnsemble(i);
                nextEnsembleChange = lh.metadata.getNextEnsembleChange(i);
            }
            LedgerEntry entry = new LedgerEntry(lh.ledgerId, i);
            seq.add(entry);
            i++;
            sendRead(ensemble, entry, BKException.Code.ReadException);

        } while (i <= endEntryId);
    }

    void sendRead(ArrayList<InetSocketAddress> ensemble, LedgerEntry entry, int lastErrorCode) {
        if (entry.nextReplicaIndexToReadFrom >= lh.metadata.quorumSize) {
            // we are done, the read has failed from all replicas, just fail the
            // read
            submitCallback(lastErrorCode);
            return;
        }

        int bookieIndex = lh.distributionSchedule.getBookieIndex(entry.entryId, entry.nextReplicaIndexToReadFrom);
        entry.nextReplicaIndexToReadFrom++;
        lh.bk.bookieClient.readEntry(ensemble.get(bookieIndex), lh.ledgerId, entry.entryId, 
                                     this, entry, BookieProtocol.FLAG_NONE);
    }

    void logErrorAndReattemptRead(LedgerEntry entry, String errMsg, int rc) {
        ArrayList<InetSocketAddress> ensemble = lh.metadata.getEnsemble(entry.entryId);
        int bookeIndex = lh.distributionSchedule.getBookieIndex(entry.entryId, entry.nextReplicaIndexToReadFrom - 1);
        LOG.error(errMsg + " while reading entry: " + entry.entryId + " ledgerId: " + lh.ledgerId + " from bookie: "
                  + ensemble.get(bookeIndex));
        sendRead(ensemble, entry, rc);
        return;
    }

    @Override
    public void readEntryComplete(int rc, long ledgerId, final long entryId, final ChannelBuffer buffer, Object ctx) {
        final LedgerEntry entry = (LedgerEntry) ctx;

        if (rc != BKException.Code.OK) {
            logErrorAndReattemptRead(entry, "Error: " + BKException.getMessage(rc), rc);
            return;
        }

        ChannelBufferInputStream is;
        try {
            is = lh.macManager.verifyDigestAndReturnData(entryId, buffer);
        } catch (BKDigestMatchException e) {
            logErrorAndReattemptRead(entry, "Mac mismatch", BKException.Code.DigestMatchException);
            return;
        }

        entry.entryDataStream = is;

        /*
         * The length is a long and it is the last field of the metadata of an entry.
         * Consequently, we have to subtract 8 from METADATA_LENGTH to get the length.
         */
        entry.length = buffer.getLong(DigestManager.METADATA_LENGTH - 8);

        numPendingReads--;
        if (numPendingReads == 0) {
            submitCallback(BKException.Code.OK);
        }

        if(LOG.isDebugEnabled()) {
            LOG.debug("Releasing lock: " + entryId);
        }

        lh.opCounterSem.release();

        if(numPendingReads < 0)
            LOG.error("Read too many values");
    }

    private void submitCallback(int code) {
        cb.readComplete(code, lh, PendingReadOp.this, PendingReadOp.this.ctx);
    }
    public boolean hasMoreElements() {
        return !seq.isEmpty();
    }

    public LedgerEntry nextElement() throws NoSuchElementException {
        return seq.remove();
    }

    public int size() {
        return seq.size();
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/ReadLastConfirmedOp.java,true,"package org.apache.bookkeeper.client;
/* Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.util.EnumSet;
import org.apache.bookkeeper.client.AsyncCallback.ReadLastConfirmedCallback;
import org.apache.bookkeeper.client.BKException.BKDigestMatchException;
import org.apache.bookkeeper.client.DigestManager.RecoveryData;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.ReadEntryCallback;
import org.apache.bookkeeper.proto.BookieProtocol;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.buffer.ChannelBuffer;

/**
 * This class encapsulated the read last confirmed operation.
 *
 */
class ReadLastConfirmedOp implements ReadEntryCallback {
    static final Logger LOG = LoggerFactory.getLogger(LedgerRecoveryOp.class);
    LedgerHandle lh;
    Object ctx;
    int numResponsesPending;
    int validResponses;
    long maxAddConfirmed;
    long maxLength = 0;
    volatile boolean completed = false;

    ReadLastConfirmedCallback cb;

    public ReadLastConfirmedOp(LedgerHandle lh, ReadLastConfirmedCallback cb, Object ctx) {
        this.cb = cb;
        this.ctx = ctx;
        this.lh = lh;
        this.validResponses = 0;
        this.numResponsesPending = lh.metadata.ensembleSize;
    }

    public void initiate() {
        for (int i = 0; i < lh.metadata.currentEnsemble.size(); i++) {
            lh.bk.bookieClient.readEntry(lh.metadata.currentEnsemble.get(i), lh.ledgerId, LedgerHandle.LAST_ADD_CONFIRMED, 
                                         this, i, BookieProtocol.FLAG_NONE);
        }
    }

    public synchronized void readEntryComplete(final int rc, final long ledgerId, final long entryId,
            final ChannelBuffer buffer, final Object ctx) {
        int bookieIndex = (Integer) ctx;

        numResponsesPending--;

        if (rc == BKException.Code.OK) {
            try {
                RecoveryData recoveryData = lh.macManager.verifyDigestAndReturnLastConfirmed(buffer);
                maxAddConfirmed = Math.max(maxAddConfirmed, recoveryData.lastAddConfirmed);
                validResponses++;
            } catch (BKDigestMatchException e) {
                // Too bad, this bookie didn't give us a valid answer, we
                // still might be able to recover though so continue
                LOG.error("Mac mismatch while reading last entry from bookie: "
                          + lh.metadata.currentEnsemble.get(bookieIndex));
            }
        }

        if (rc == BKException.Code.NoSuchLedgerExistsException || rc == BKException.Code.NoSuchEntryException) {
            // this still counts as a valid response, e.g., if the client crashed without writing any entry
            validResponses++;
        }

        // other return codes dont count as valid responses
        if ((validResponses >= lh.metadata.quorumSize) &&
                !completed) {
            completed = true;
            if (LOG.isDebugEnabled()) {
                LOG.debug("Read Complete with enough validResponses");
            }
            cb.readLastConfirmedComplete(BKException.Code.OK, maxAddConfirmed, this.ctx);
            return;
        }

        if (numResponsesPending == 0 && !completed) {
            completed = true;
            // Have got all responses back but was still not enough, just fail the operation
            LOG.error("While readLastConfirmed ledger: " + ledgerId + " did not hear success responses from all quorums");
            cb.readLastConfirmedComplete(BKException.Code.LedgerRecoveryException, maxAddConfirmed, this.ctx);
        }

    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/ReadOnlyLedgerHandle.java,true,"package org.apache.bookkeeper.client;

/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

import org.apache.bookkeeper.client.AsyncCallback.AddCallback;
import org.apache.bookkeeper.client.AsyncCallback.CloseCallback;
import org.apache.bookkeeper.client.BookKeeper.DigestType;
import java.security.GeneralSecurityException;


/**
 * Read only ledger handle. This ledger handle allows you to 
 * read from a ledger but not to write to it. It overrides all 
 * the public write operations from LedgerHandle.
 * It should be returned for BookKeeper#openLedger operations.
 */
class ReadOnlyLedgerHandle extends LedgerHandle {
    ReadOnlyLedgerHandle(BookKeeper bk, long ledgerId, LedgerMetadata metadata,
                         DigestType digestType, byte[] password)
            throws GeneralSecurityException, NumberFormatException {
        super(bk, ledgerId, metadata, digestType, password);
    }

    @Override
    public void close() 
            throws InterruptedException, BKException {
        // noop
    }

    @Override
    public void asyncClose(CloseCallback cb, Object ctx) {
        cb.closeComplete(BKException.Code.OK, this, ctx);
    }
    
    @Override
    public void addEntry(byte[] data) throws InterruptedException, BKException {
        addEntry(data, 0, data.length);
    }
    
    @Override
    public void addEntry(byte[] data, int offset, int length)
            throws InterruptedException, BKException {
        LOG.error("Tried to add entry on a Read-Only ledger handle, ledgerid=" + ledgerId);        
        throw BKException.create(BKException.Code.IllegalOpException);
    }

    @Override
    public void asyncAddEntry(final byte[] data, final AddCallback cb,
                              final Object ctx) {
        asyncAddEntry(data, 0, data.length, cb, ctx);
    }

    @Override
    public void asyncAddEntry(final byte[] data, final int offset, final int length,
                              final AddCallback cb, final Object ctx) {
        LOG.error("Tried to add entry on a Read-Only ledger handle, ledgerid=" + ledgerId);
        cb.addComplete(BKException.Code.IllegalOpException, this, -1, ctx);
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/RoundRobinDistributionSchedule.java,true,"package org.apache.bookkeeper.client;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import org.apache.bookkeeper.util.MathUtils;

/**
 * A specific {@link DistributionSchedule} that places entries in round-robin
 * fashion. For ensemble size 3, and quorum size 2, Entry 0 goes to bookie 0 and
 * 1, entry 1 goes to bookie 1 and 2, and entry 2 goes to bookie 2 and 0, and so
 * on.
 *
 */
class RoundRobinDistributionSchedule implements DistributionSchedule {
    int quorumSize;
    int ensembleSize;

    // covered[i] is true if the quorum starting at bookie index i has been
    // covered by a recovery reply
    boolean[] covered = null;
    int numQuorumsUncovered;

    public RoundRobinDistributionSchedule(int quorumSize, int ensembleSize) {
        this.quorumSize = quorumSize;
        this.ensembleSize = ensembleSize;
    }

    @Override
    public int getBookieIndex(long entryId, int replicaIndex) {
        return (int) ((entryId + replicaIndex) % ensembleSize);
    }

    @Override
    public int getReplicaIndex(long entryId, int bookieIndex) {
        // NOTE: Java's % operator returns the sign of the dividend and is hence
        // not always positive

        int replicaIndex = MathUtils.signSafeMod(bookieIndex - entryId, ensembleSize);

        return replicaIndex < quorumSize ? replicaIndex : -1;

    }

    public synchronized boolean canProceedWithRecovery(int bookieIndexHeardFrom) {
        if (covered == null) {
            covered = new boolean[ensembleSize];
            numQuorumsUncovered = ensembleSize;
        }

        if (numQuorumsUncovered == 0) {
            return true;
        }

        for (int i = 0; i < quorumSize; i++) {
            int quorumStartIndex = MathUtils.signSafeMod(bookieIndexHeardFrom - i, ensembleSize);
            if (!covered[quorumStartIndex]) {
                covered[quorumStartIndex] = true;
                numQuorumsUncovered--;

                if (numQuorumsUncovered == 0) {
                    return true;
                }
            }

        }

        return false;

    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/SyncCounter.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.client;

import java.util.Enumeration;

/**
 * Implements objects to help with the synchronization of asynchronous calls
 *
 */

class SyncCounter {
    int i;
    int rc;
    int total;
    Enumeration<LedgerEntry> seq = null;
    LedgerHandle lh = null;

    synchronized void inc() {
        i++;
        total++;
    }

    synchronized void dec() {
        i--;
        notifyAll();
    }

    synchronized void block(int limit) throws InterruptedException {
        while (i > limit) {
            int prev = i;
            wait();
            if (i == prev) {
                break;
            }
        }
    }

    synchronized int total() {
        return total;
    }

    void setrc(int rc) {
        this.rc = rc;
    }

    int getrc() {
        return rc;
    }

    void setSequence(Enumeration<LedgerEntry> seq) {
        this.seq = seq;
    }

    Enumeration<LedgerEntry> getSequence() {
        return seq;
    }

    void setLh(LedgerHandle lh) {
        this.lh = lh;
    }

    LedgerHandle getLh() {
        return lh;
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/conf/AbstractConfiguration.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.conf;

import java.net.URL;

import org.apache.commons.configuration.CompositeConfiguration;
import org.apache.commons.configuration.Configuration;
import org.apache.commons.configuration.ConfigurationException;
import org.apache.commons.configuration.PropertiesConfiguration;
import org.apache.commons.configuration.SystemConfiguration;

import org.apache.bookkeeper.meta.LedgerManagerFactory;

/**
 * Abstract configuration
 */
public abstract class AbstractConfiguration extends CompositeConfiguration {

    // Ledger Manager
    protected final static String LEDGER_MANAGER_TYPE = "ledgerManagerType";
    protected final static String ZK_LEDGERS_ROOT_PATH = "zkLedgersRootPath";

    protected AbstractConfiguration() {
        super();
        // add configuration for system properties
        addConfiguration(new SystemConfiguration());
    }

    /**
     * You can load configurations in precedence order. The first one takes
     * precedence over any loaded later.
     *
     * @param confURL
     *          Configuration URL
     */
    public void loadConf(URL confURL) throws ConfigurationException {
        Configuration loadedConf = new PropertiesConfiguration(confURL);
        addConfiguration(loadedConf);
    }

    /**
     * You can load configuration from other configuration
     *
     * @param baseConf
     *          Other Configuration
     */
    public void loadConf(AbstractConfiguration baseConf) {
        addConfiguration(baseConf); 
    }

    /**
     * Set Ledger Manager Type.
     *
     * @param lmType
     *          Ledger Manager Type
     * @return void
     */
    public void setLedgerManagerType(String lmType) {
        setProperty(LEDGER_MANAGER_TYPE, lmType); 
    }

    /**
     * Get Ledger Manager Type.
     *
     * @return ledger manager type
     * @throws ConfigurationException
     */
    public String getLedgerManagerType() {
        return getString(LEDGER_MANAGER_TYPE);
    }

    /**
     * Set Zk Ledgers Root Path.
     *
     * @param zkLedgersPath zk ledgers root path
     */
    public void setZkLedgersRootPath(String zkLedgersPath) {
        setProperty(ZK_LEDGERS_ROOT_PATH, zkLedgersPath);
    }

    /**
     * Get Zk Ledgers Root Path.
     *
     * @return zk ledgers root path
     */
    public String getZkLedgersRootPath() {
        return getString(ZK_LEDGERS_ROOT_PATH, "/ledgers");
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/conf/ClientConfiguration.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.conf;

import org.apache.bookkeeper.client.BookKeeper.DigestType;

/**
 * Configuration settings for client side
 */
public class ClientConfiguration extends AbstractConfiguration {

    // Zookeeper Parameters
    protected final static String ZK_TIMEOUT = "zkTimeout";
    protected final static String ZK_SERVERS = "zkServers";

    // Throttle value
    protected final static String THROTTLE = "throttle";

    // Digest Type
    protected final static String DIGEST_TYPE = "digestType";
    // Passwd
    protected final static String PASSWD = "passwd";

    // NIO Parameters
    protected final static String CLIENT_TCP_NODELAY = "clientTcpNoDelay";

    /**
     * Construct a default client-side configuration
     */
    public ClientConfiguration() {
        super();
    }

    /**
     * Construct a client-side configuration using a base configuration
     *
     * @param conf
     *          Base configuration
     */
    public ClientConfiguration(AbstractConfiguration conf) {
        super();
        loadConf(conf);
    }

    /**
     * Get throttle value
     *
     * @return throttle value
     * @see #setThrottleValue
     */
    public int getThrottleValue() {
        return this.getInt(THROTTLE, 5000);
    }

    /**
     * Set throttle value.
     *
     * Since BookKeeper process requests in asynchrous way, it will holds 
     * those pending request in queue. You may easily run it out of memory
     * if producing too many requests than the capability of bookie servers can handle.
     * To prevent that from happeding, you can set a throttle value here.
     *
     * @param throttle
     *          Throttle Value
     * @return client configuration
     */
    public ClientConfiguration setThrottleValue(int throttle) {
        this.setProperty(THROTTLE, Integer.toString(throttle));
        return this;
    }

    /**
     * Get digest type used in bookkeeper admin
     *
     * @return digest type
     * @see #setBookieRecoveryDigestType
     */
    public DigestType getBookieRecoveryDigestType() {
        return DigestType.valueOf(this.getString(DIGEST_TYPE, DigestType.CRC32.toString()));
    }

    /**
     * Set digest type used in bookkeeper admin.
     *
     * Digest Type and Passwd used to open ledgers for admin tool
     * For now, assume that all ledgers were created with the same DigestType
     * and password. In the future, this admin tool will need to know for each
     * ledger, what was the DigestType and password used to create it before it
     * can open it. These values will come from System properties, though fixed
     * defaults are defined here.
     *
     * @param digestType
     *          Digest Type
     * @return client configuration
     */
    public ClientConfiguration setBookieRecoveryDigestType(DigestType digestType) {
        this.setProperty(DIGEST_TYPE, digestType.toString());
        return this;
    }

    /**
     * Get passwd used in bookkeeper admin
     *
     * @return password
     * @see #setBookieRecoveryPasswd
     */
    public byte[] getBookieRecoveryPasswd() {
        return this.getString(PASSWD, "").getBytes();
    }

    /**
     * Set passwd used in bookkeeper admin.
     *
     * Digest Type and Passwd used to open ledgers for admin tool
     * For now, assume that all ledgers were created with the same DigestType
     * and password. In the future, this admin tool will need to know for each
     * ledger, what was the DigestType and password used to create it before it
     * can open it. These values will come from System properties, though fixed
     * defaults are defined here.
     *
     * @param passwd
     *          Password
     * @return client configuration
     */
    public ClientConfiguration setBookieRecoveryPasswd(byte[] passwd) {
        setProperty(PASSWD, new String(passwd));
        return this;
    }

    /**
     * Is tcp connection no delay.
     *
     * @return tcp socket nodelay setting
     * @see #setClientTcpNoDelay
     */
    public boolean getClientTcpNoDelay() {
        return getBoolean(CLIENT_TCP_NODELAY, true);
    }

    /**
     * Set socket nodelay setting.
     *
     * This settings is used to enabled/disabled Nagle's algorithm, which is a means of
     * improving the efficiency of TCP/IP networks by reducing the number of packets
     * that need to be sent over the network. If you are sending many small messages, 
     * such that more than one can fit in a single IP packet, setting client.tcpnodelay
     * to false to enable Nagle algorithm can provide better performance.
     * <br>
     * Default value is true.
     *
     * @param noDelay
     *          NoDelay setting
     * @return client configuration
     */
    public ClientConfiguration setClientTcpNoDelay(boolean noDelay) {
        setProperty(CLIENT_TCP_NODELAY, Boolean.toString(noDelay));
        return this;
    }

    /**
     * Get zookeeper servers to connect
     *
     * @return zookeeper servers
     */
    public String getZkServers() {
        return getString(ZK_SERVERS, "localhost");
    }

    /**
     * Set zookeeper servers to connect
     *
     * @param zkServers
     *          ZooKeeper servers to connect
     */
    public ClientConfiguration setZkServers(String zkServers) {
        setProperty(ZK_SERVERS, zkServers);
        return this;
    }

    /**
     * Get zookeeper timeout
     *
     * @return zookeeper client timeout
     */
    public int getZkTimeout() {
        return getInt(ZK_TIMEOUT, 10000);
    }

    /**
     * Set zookeeper timeout
     *
     * @param zkTimeout
     *          ZooKeeper client timeout
     * @return client configuration
     */
    public ClientConfiguration setZkTimeout(int zkTimeout) {
        setProperty(ZK_TIMEOUT, Integer.toString(zkTimeout));
        return this;
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/conf/ServerConfiguration.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.conf;

import java.io.File;

import org.apache.commons.lang.StringUtils;

/**
 * Configuration manages server-side settings
 */
public class ServerConfiguration extends AbstractConfiguration {
    // Entry Log Parameters
    protected final static String ENTRY_LOG_SIZE_LIMIT = "logSizeLimit";

    // Gc Parameters
    protected final static String GC_WAIT_TIME = "gcWaitTime";
    // Sync Parameters
    protected final static String FLUSH_INTERVAL = "flushInterval";
    // Bookie death watch interval
    protected final static String DEATH_WATCH_INTERVAL = "bookieDeathWatchInterval";
    // Ledger Cache Parameters
    protected final static String OPEN_FILE_LIMIT = "openFileLimit";
    protected final static String PAGE_LIMIT = "pageLimit";
    protected final static String PAGE_SIZE = "pageSize";
    // Journal Parameters
    protected final static String MAX_JOURNAL_SIZE = "journalMaxSizeMB";
    protected final static String MAX_BACKUP_JOURNALS = "journalMaxBackups";
    // Bookie Parameters
    protected final static String BOOKIE_PORT = "bookiePort";
    protected final static String JOURNAL_DIR = "journalDirectory";
    protected final static String LEDGER_DIRS = "ledgerDirectories";
    // NIO Parameters
    protected final static String SERVER_TCP_NODELAY = "serverTcpNoDelay";
    // Zookeeper Parameters
    protected final static String ZK_TIMEOUT = "zkTimeout";
    protected final static String ZK_SERVERS = "zkServers";

    // separator for ledger dir
    protected final static String SEP = ",";

    /**
     * Construct a default configuration object
     */
    public ServerConfiguration() {
        super();
    }

    /**
     * Construct a configuration based on other configuration
     *
     * @param conf
     *          Other configuration
     */
    public ServerConfiguration(AbstractConfiguration conf) {
        super();
        loadConf(conf);
    }

    /**
     * Get entry logger size limitation
     *
     * @return entry logger size limitation
     */
    public long getEntryLogSizeLimit() {
        return this.getLong(ENTRY_LOG_SIZE_LIMIT, 2 * 1024 * 1024 * 1024L);
    }

    /**
     * Set entry logger size limitation
     *
     * @param logSizeLimit
     *          new log size limitation
     */
    public ServerConfiguration setEntryLogSizeLimit(long logSizeLimit) {
        this.setProperty(ENTRY_LOG_SIZE_LIMIT, Long.toString(logSizeLimit));
        return this;
    }

    /**
     * Get Garbage collection wait time
     *
     * @return gc wait time
     */
    public long getGcWaitTime() {
        return this.getLong(GC_WAIT_TIME, 1000);
    }

    /**
     * Set garbage collection wait time
     *
     * @param gcWaitTime
     *          gc wait time
     * @return server configuration
     */
    public ServerConfiguration setGcWaitTime(long gcWaitTime) {
        this.setProperty(GC_WAIT_TIME, Long.toString(gcWaitTime));
        return this;
    }

    /**
     * Get flush interval
     *
     * @return flush interval
     */
    public int getFlushInterval() {
        return this.getInt(FLUSH_INTERVAL, 100);
    }

    /**
     * Set flush interval
     *
     * @param flushInterval
     *          Flush Interval
     * @return server configuration
     */
    public ServerConfiguration setFlushInterval(int flushInterval) {
        this.setProperty(FLUSH_INTERVAL, Integer.toString(flushInterval));
        return this;
    }

    /**
     * Get bookie death watch interval
     *
     * @return watch interval
     */
    public int getDeathWatchInterval() {
        return this.getInt(DEATH_WATCH_INTERVAL, 1000);
    }

    /**
     * Get open file limit
     *
     * @return max number of files to open
     */
    public int getOpenFileLimit() {
        return this.getInt(OPEN_FILE_LIMIT, 900);
    }

    /**
     * Get limitation number of index pages in ledger cache
     *
     * @return max number of index pages in ledger cache
     */
    public int getPageLimit() {
        return this.getInt(PAGE_LIMIT, -1);
    }

    /**
     * Get page size
     *
     * @return page size in ledger cache
     */
    public int getPageSize() {
        return this.getInt(PAGE_SIZE, 8192);
    }

    /**
     * Max journal file size
     *
     * @return max journal file size
     */
    public long getMaxJournalSize() {
        return this.getLong(MAX_JOURNAL_SIZE, 2 * 1024);
    }

    /**
     * Set new max journal file size
     *
     * @param maxJournalSize
     *          new max journal file size
     * @return server configuration
     */
    public ServerConfiguration setMaxJournalSize(long maxJournalSize) {
        this.setProperty(MAX_JOURNAL_SIZE, Long.toString(maxJournalSize));
        return this;
    }

    /**
     * Max number of older journal files kept
     *
     * @return max number of older journal files to kept
     */
    public int getMaxBackupJournals() {
        return this.getInt(MAX_BACKUP_JOURNALS, 5);
    }

    /**
     * Set max number of older journal files to kept
     *
     * @param maxBackupJournals
     *          Max number of older journal files
     * @return server configuration
     */
    public ServerConfiguration setMaxBackupJournals(int maxBackupJournals) {
        this.setProperty(MAX_BACKUP_JOURNALS, Integer.toString(maxBackupJournals));
        return this;
    }

    /**
     * Get bookie port that bookie server listen on
     *
     * @return bookie port
     */
    public int getBookiePort() {
        return this.getInt(BOOKIE_PORT, 3181);
    }

    /**
     * Set new bookie port that bookie server listen on
     *
     * @param port
     *          Port to listen on
     * @return server configuration
     */
    public ServerConfiguration setBookiePort(int port) {
        this.setProperty(BOOKIE_PORT, Integer.toString(port));
        return this;
    }

    /**
     * Get dir name to store journal files
     *
     * @return journal dir name
     */
    public String getJournalDirName() {
        return this.getString(JOURNAL_DIR, "/tmp/bk-txn");
    }

    /**
     * Set dir name to store journal files
     *
     * @param journalDir
     *          Dir to store journal files
     * @return server configuration
     */
    public ServerConfiguration setJournalDirName(String journalDir) {
        this.setProperty(JOURNAL_DIR, journalDir);
        return this;
    }

    /**
     * Get dir to store journal files
     *
     * @return journal dir, if no journal dir provided return null
     */
    public File getJournalDir() {
        String journalDirName = getJournalDirName();
        if (null == journalDirName) {
            return null;
        }
        return new File(journalDirName);
    }

    /**
     * Get dir names to store ledger data
     *
     * @return ledger dir names, if not provided return null
     */
    public String[] getLedgerDirNames() {
        String ledgerDirs = this.getString(LEDGER_DIRS, "/tmp/bk-data");
        if (null == ledgerDirs) {
            return null;
        }
        return ledgerDirs.split(SEP);
    }

    /**
     * Set dir names to store ledger data
     *
     * @param ledgerDirs
     *          Dir names to store ledger data
     * @return server configuration
     */
    public ServerConfiguration setLedgerDirNames(String[] ledgerDirs) {
        if (null == ledgerDirs) {
            return this;
        }
        this.setProperty(LEDGER_DIRS, StringUtils.join(ledgerDirs, SEP));
        return this;
    }

    /**
     * Get dirs that stores ledger data
     *
     * @return ledger dirs
     */
    public File[] getLedgerDirs() {
        String[] ledgerDirNames = getLedgerDirNames();
        if (null == ledgerDirNames) {
            return null;
        }
        File[] ledgerDirs = new File[ledgerDirNames.length];
        for (int i = 0; i < ledgerDirNames.length; i++) {
            ledgerDirs[i] = new File(ledgerDirNames[i]);
        }
        return ledgerDirs;
    }

    /**
     * Is tcp connection no delay.
     *
     * @return tcp socket nodelay setting
     */
    public boolean getServerTcpNoDelay() {
        return getBoolean(SERVER_TCP_NODELAY, true);
    }

    /**
     * Set socket nodelay setting
     *
     * @param noDelay
     *          NoDelay setting
     * @return server configuration
     */
    public ServerConfiguration setServerTcpNoDelay(boolean noDelay) {
        setProperty(SERVER_TCP_NODELAY, Boolean.toString(noDelay));
        return this;
    }

    /**
     * Get zookeeper servers to connect
     *
     * @return zookeeper servers
     */
    public String getZkServers() {
        return getString(ZK_SERVERS, null);
    }

    /**
     * Set zookeeper servers to connect
     *
     * @param zkServers
     *          ZooKeeper servers to connect
     */
    public ServerConfiguration setZkServers(String zkServers) {
        setProperty(ZK_SERVERS, zkServers);
        return this;
    }

    /**
     * Get zookeeper timeout
     *
     * @return zookeeper server timeout
     */
    public int getZkTimeout() {
        return getInt(ZK_TIMEOUT, 10000);
    }

    /**
     * Set zookeeper timeout
     *
     * @param zkTimeout
     *          ZooKeeper server timeout
     * @return server configuration
     */
    public ServerConfiguration setZkTimeout(int zkTimeout) {
        setProperty(ZK_TIMEOUT, Integer.toString(zkTimeout));
        return this;
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/meta/AbstractZkLedgerManager.java,true,"package org.apache.bookkeeper.meta;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.IOException;
import java.util.HashSet;
import java.util.List;

import org.apache.bookkeeper.conf.AbstractConfiguration;
import org.apache.bookkeeper.meta.LedgerManager;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.MultiCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.Processor;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.zookeeper.AsyncCallback;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.KeeperException.Code;
import org.apache.zookeeper.ZooKeeper;

/**
 * Abstract ledger manager based on zookeeper, which provides common methods such as query zk nodes.
 */
abstract class AbstractZkLedgerManager implements LedgerManager {

    static Logger LOG = LoggerFactory.getLogger(AbstractZkLedgerManager.class);

    // Ledger Node Prefix
    static public final String LEDGER_NODE_PREFIX = "L";
    static final String AVAILABLE_NODE = "available";

    protected final AbstractConfiguration conf;
    protected final ZooKeeper zk;
    protected final String ledgerRootPath;

    /**
     * ZooKeeper-based Ledger Manager Constructor
     *
     * @param conf
     *          Configuration object
     * @param zk
     *          ZooKeeper Client Handle
     * @param ledgerRootPath
     *          ZooKeeper Path to store ledger metadata
     */
    protected AbstractZkLedgerManager(AbstractConfiguration conf, ZooKeeper zk,
                                      String ledgerRootPath) {
        this.conf = conf;
        this.zk = zk;
        this.ledgerRootPath = ledgerRootPath;
    }

    /**
     * Get all the ledgers in a single zk node
     *
     * @param nodePath
     *          Zookeeper node path
     * @param getLedgersCallback
     *          callback function to process ledgers in a single node
     */
    protected void asyncGetLedgersInSingleNode(final String nodePath, final GenericCallback<HashSet<Long>> getLedgersCallback) {
        // First sync ZK to make sure we're reading the latest active/available ledger nodes.
        zk.sync(nodePath, new AsyncCallback.VoidCallback() {
            @Override
            public void processResult(int rc, String path, Object ctx) {
                if (LOG.isDebugEnabled()) {
                    LOG.debug("Sync node path " + path + " return : " + rc);
                }
                if (rc != Code.OK.intValue()) {
                    LOG.error("ZK error syncing the ledgers node when getting children: ", KeeperException
                            .create(KeeperException.Code.get(rc), path));
                    getLedgersCallback.operationComplete(rc, null);
                    return;
                }
                // Sync has completed successfully so now we can poll ZK
                // and read in the latest set of active ledger nodes.
                doAsyncGetLedgersInSingleNode(nodePath, getLedgersCallback);
            }
        }, null);
    }

    private void doAsyncGetLedgersInSingleNode(final String nodePath,
                                               final GenericCallback<HashSet<Long>> getLedgersCallback) {
        zk.getChildren(nodePath, false, new AsyncCallback.ChildrenCallback() {
            @Override
            public void processResult(int rc, String path, Object ctx, List<String> ledgerNodes) {
                if (rc != Code.OK.intValue()) {
                    LOG.error("Error polling ZK for the available ledger nodes: ", KeeperException
                            .create(KeeperException.Code.get(rc), path));
                    getLedgersCallback.operationComplete(rc, null);
                    return;
                }
                if (LOG.isDebugEnabled()) {
                    LOG.debug("Retrieved current set of ledger nodes: " + ledgerNodes);
                }
                // Convert the ZK retrieved ledger nodes to a HashSet for easier comparisons.
                HashSet<Long> allActiveLedgers = new HashSet<Long>(ledgerNodes.size(), 1.0f);
                for (String ledgerNode : ledgerNodes) {
                    if (isSpecialZnode(ledgerNode)) {
                        continue;
                    }
                    try {
                        // convert the node path to ledger id according to different ledger manager implementation
                        allActiveLedgers.add(getLedgerId(path + "/" + ledgerNode));
                    } catch (IOException ie) {
                        LOG.warn("Error extracting ledgerId from ZK ledger node: " + ledgerNode);
                        // This is a pretty bad error as it indicates a ledger node in ZK
                        // has an incorrect format. For now just continue and consider
                        // this as a non-existent ledger.
                        continue;
                    }
                }

                getLedgersCallback.operationComplete(rc, allActiveLedgers);

            }
        }, null);
    }

    private class GetLedgersCtx {
        int rc;
        HashSet<Long> ledgers = null;
    }

    /**
     * Get all the ledgers in a single zk node
     *
     * @param nodePath
     *          Zookeeper node path
     * @throws IOException
     * @throws InterruptedException
     */
    protected HashSet<Long> getLedgersInSingleNode(final String nodePath)
        throws IOException, InterruptedException {
        final GetLedgersCtx ctx = new GetLedgersCtx();
        if (LOG.isDebugEnabled()) {
            LOG.debug("Try to get ledgers of node : " + nodePath);
        }
        synchronized (ctx) {
            asyncGetLedgersInSingleNode(nodePath, new GenericCallback<HashSet<Long>>() {
                @Override
                public void operationComplete(int rc, HashSet<Long> zkActiveLedgers) {
                    synchronized (ctx) {
                        if (Code.OK.intValue() == rc) {
                            ctx.ledgers = zkActiveLedgers;
                        }
                        ctx.rc = rc;
                        ctx.notifyAll();
                    }
                }
            });
            ctx.wait();
        }
        if (Code.OK.intValue() != ctx.rc && null != ctx.ledgers) {
            throw new IOException("Error on getting ledgers from node " + nodePath);
        }
        return ctx.ledgers;
    }

    /**
     * Process ledgers in a single zk node.
     *
     * <p>
     * for each ledger found in this zk node, processor#process(ledgerId) will be triggerred
     * to process a specific ledger. after all ledgers has been processed, the finalCb will
     * be called with provided context object. The RC passed to finalCb is decided by :
     * <ul>
     * <li> All ledgers are processed successfully, successRc will be passed.
     * <li> Either ledger is processed failed, failureRc will be passed.
     * </ul>
     * </p>
     *
     * @param path
     *          Zk node path to store ledgers
     * @param processor
     *          Processor provided to process ledger
     * @param finalCb
     *          Callback object when all ledgers are processed
     * @param ctx
     *          Context object passed to finalCb
     * @param successRc
     *          RC passed to finalCb when all ledgers are processed successfully
     * @param failureRc
     *          RC passed to finalCb when either ledger is processed failed
     */
    protected void asyncProcessLedgersInSingleNode(
            final String path, final Processor<Long> processor,
            final AsyncCallback.VoidCallback finalCb, final Object ctx,
            final int successRc, final int failureRc) {
        asyncGetLedgersInSingleNode(path, new GenericCallback<HashSet<Long>>() {
            @Override
            public void operationComplete(int rc, HashSet<Long> zkActiveLedgers) {
                if (Code.OK.intValue() != rc) {
                    finalCb.processResult(failureRc, null, ctx);
                    return;
                }

                if (LOG.isDebugEnabled()) {
                    LOG.debug("Processing ledgers : " + zkActiveLedgers);
                }

                // no ledgers found, return directly
                if (zkActiveLedgers.size() == 0) {
                    finalCb.processResult(successRc, null, ctx);
                    return;
                }

                MultiCallback mcb = new MultiCallback(zkActiveLedgers.size(), finalCb, ctx,
                                                      successRc, failureRc);
                // start loop over all ledgers
                for (Long ledger : zkActiveLedgers) {
                    processor.process(ledger, mcb);
                }
            }
        });
    }

    /**
     * Whether the znode a special znode
     *
     * @param znode
     *          Znode Name
     * @return true  if the znode is a special znode otherwise false
     */
    protected boolean isSpecialZnode(String znode) {
        if (AVAILABLE_NODE.equals(znode) 
            || LedgerLayout.LAYOUT_ZNODE.equals(znode)) {
            return true;
        }
        return false;
    }

    @Override
    public void close() {
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/meta/FlatLedgerManager.java,true,"package org.apache.bookkeeper.meta;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.IOException;
import java.util.concurrent.ConcurrentMap;
import java.util.concurrent.ConcurrentHashMap;
import java.util.HashSet;

import org.apache.bookkeeper.conf.AbstractConfiguration;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.Processor;
import org.apache.bookkeeper.util.StringUtils;
import org.apache.bookkeeper.util.ZkUtils;
import org.apache.zookeeper.AsyncCallback;
import org.apache.zookeeper.AsyncCallback.StringCallback;
import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.KeeperException.Code;
import org.apache.zookeeper.ZooDefs.Ids;
import org.apache.zookeeper.ZooKeeper;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Manage all ledgers in a single zk node.
 *
 * <p>
 * All ledgers' metadata are put in a single zk node, created using zk sequential node.
 * Each ledger node is prefixed with 'L'.
 * </p>
 * <p>
 * All actived ledgers found in bookie server side is managed in a hash map.
 * </p>
 * <p>
 * Garbage collection in FlatLedgerManager is procssed as below:
 * <ul>
 * <li> fetch all existed ledgers from zookeeper, said <b>zkActiveLedgers</b>
 * <li> fetch all active ledgers from bookie server, said <b>bkActiveLedgers</b>
 * <li> loop over <b>bkActiveLedgers</b> to find those ledgers aren't existed in
 * <b>zkActiveLedgers</b>, do garbage collection on them.
 * </ul>
 * </p>
 */
class FlatLedgerManager extends AbstractZkLedgerManager {

    static final Logger LOG = LoggerFactory.getLogger(FlatLedgerManager.class);
    public static final String NAME = "flat";
    public static final int CUR_VERSION = 1;

    // path prefix to store ledger znodes
    private final String ledgerPrefix;
    // hash map to store all active ledger ids
    private ConcurrentMap<Long, Boolean> activeLedgers;

    /**
     * Constructor
     *
     * @param conf
     *          Configuration object
     * @param zk
     *          ZooKeeper Client Handle
     * @param ledgerRootPath
     *          ZooKeeper Path to store ledger metadata
     * @throws IOException when version is not compatible
     */
    public FlatLedgerManager(AbstractConfiguration conf, ZooKeeper zk,
                             String ledgerRootPath, int layoutVersion)
        throws IOException {
        super(conf, zk, ledgerRootPath);

        if (layoutVersion != CUR_VERSION) {
            throw new IOException("Incompatible layout version found : " 
                                  + layoutVersion);
        }

        ledgerPrefix = ledgerRootPath + "/" + LEDGER_NODE_PREFIX;
        activeLedgers = new ConcurrentHashMap<Long, Boolean>();
    }

    @Override
    public void newLedgerPath(final GenericCallback<String> cb) {
        StringCallback scb = new StringCallback() {
            @Override
            public void processResult(int rc, String path, Object ctx,
                    String name) {
                if (Code.OK.intValue() != rc) {
                    cb.operationComplete(rc, null);
                } else {
                    cb.operationComplete(rc, name);
                }
            }
        };
        ZkUtils.createFullPathOptimistic(zk, ledgerPrefix, new byte[0],
            Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT_SEQUENTIAL, scb, null);
    }

    @Override
    public String getLedgerPath(long ledgerId) {
        StringBuilder sb = new StringBuilder();
        sb.append(ledgerPrefix)
          .append(StringUtils.getZKStringId(ledgerId));
        return sb.toString();
    }

    @Override
    public long getLedgerId(String nodeName) throws IOException {
        long ledgerId;
        try {
            String parts[] = nodeName.split(ledgerPrefix);
            ledgerId = Long.parseLong(parts[parts.length - 1]);
        } catch (NumberFormatException e) {
            throw new IOException(e);
        }
        return ledgerId;
    }

    @Override
    public void asyncProcessLedgers(final Processor<Long> processor,
                                    final AsyncCallback.VoidCallback finalCb, final Object ctx,
                                    final int successRc, final int failureRc) {
        asyncProcessLedgersInSingleNode(ledgerRootPath, processor, finalCb, ctx, successRc, failureRc);
    }

    @Override
    public void addActiveLedger(long ledgerId, boolean active) {
        activeLedgers.put(ledgerId, active);
    }

    @Override
    public void removeActiveLedger(long ledgerId) {
        activeLedgers.remove(ledgerId);
    }

    @Override
    public boolean containsActiveLedger(long ledgerId) {
        return activeLedgers.containsKey(ledgerId);
    }

    @Override
    public void garbageCollectLedgers(GarbageCollector gc) {
        try {
            HashSet<Long> zkActiveLedgers = getLedgersInSingleNode(ledgerRootPath);
            ConcurrentMap<Long, Boolean> bkActiveLedgers = activeLedgers;
            if (LOG.isDebugEnabled()) {
                LOG.debug("All active ledgers from ZK: " + zkActiveLedgers);
                LOG.debug("Current active ledgers from Bookie: " + bkActiveLedgers.keySet());
            }
            doGc(gc, bkActiveLedgers, zkActiveLedgers);
        } catch (IOException ie) {
            LOG.warn("Error during garbage collecting ledgers from " + ledgerRootPath, ie);
        } catch (InterruptedException inte) {
            LOG.warn("Interrupted during garbage collecting ledgers from " + ledgerRootPath, inte);
        }
    }

    /**
     * Do garbage collecting comparing hosted ledgers and zk ledgers
     *
     * @param gc
     *          Garbage collector to do garbage collection when found inactive/deleted ledgers
     * @param bkActiveLedgers
     *          Active ledgers hosted in bookie server
     * @param zkAllLedgers
     *          All ledgers stored in zookeeper
     */
    void doGc(GarbageCollector gc, ConcurrentMap<Long, Boolean> bkActiveLedgers, HashSet<Long> zkAllLedgers) {
        // remove any active ledgers that doesn't exist in zk
        for (Long bkLid : bkActiveLedgers.keySet()) {
            if (!zkAllLedgers.contains(bkLid)) {
                // remove it from current active ledger
                bkActiveLedgers.remove(bkLid);
                gc.gc(bkLid);
            }
        }
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/meta/HierarchicalLedgerManager.java,true,"package org.apache.bookkeeper.meta;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.IOException;
import java.util.concurrent.ConcurrentMap;
import java.util.concurrent.ConcurrentSkipListMap;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.HashSet;
import java.util.List;

import org.apache.bookkeeper.conf.AbstractConfiguration;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.Processor;
import org.apache.bookkeeper.util.StringUtils;
import org.apache.bookkeeper.util.ZkUtils;
import org.apache.zookeeper.AsyncCallback;
import org.apache.zookeeper.AsyncCallback.StringCallback;
import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.KeeperException.Code;
import org.apache.zookeeper.ZooDefs.Ids;
import org.apache.zookeeper.ZooKeeper;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Hierarchical Ledger Manager which manages ledger meta in zookeeper using 2-level hierarchical znodes.
 *
 * <p>
 * Hierarchical Ledger Manager first obtain a global unique id from zookeeper using a EPHEMERAL_SEQUENTIAL
 * znode <i>(ledgersRootPath)/ledgers/idgen/ID-</i>.
 * Since zookeeper sequential counter has a format of %10d -- that is 10 digits with 0 (zero) padding, i.e.
 * "&lt;path&gt;0000000001", HierarchicalLedgerManager splits the generated id into 3 parts (2-4-4):
 * <pre>&lt;level1 (2 digits)&gt;&lt;level2 (4 digits)&gt;&lt;level3 (4 digits)&gt;</pre>
 * These 3 parts are used to form the actual ledger node path used to store ledger metadata:
 * <pre>(ledgersRootPath)/level1/level2/L(level3)</pre>
 * E.g Ledger 0000000001 is split into 3 parts <i>00</i>, <i>0000</i>, <i>0001</i>, which is stored in
 * <i>(ledgersRootPath)/00/0000/L0001</i>. So each znode could have at most 10000 ledgers, which avoids
 * failed to get children list of a too big znode during garbage collection.
 * <p>
 * All actived ledgers found in bookie server is managed in a sorted map, which ease us to pick
 * up all actived ledgers belongs to (level1, level2).
 * </p>
 * <p>
 * Garbage collection in HierarchicalLedgerManager is processed node by node as below:
 * <ul>
 * fetching all level1 nodes, by calling zk#getChildren(ledgerRootPath).
 * <ul>
 * for each level1 node, fetching their level2 nodes, by calling zk#getChildren(ledgerRootPath + "/" + level1)
 * <li> fetch all existed ledgers from zookeeper in level1/level2 node, said <b>zkActiveLedgers</b>
 * <li> fetch all active ledgers from bookie server in level1/level2, said <b>bkActiveLedgers</b>
 * <li> loop over <b>bkActiveLedgers</b> to find those ledgers aren't existed in <b>zkActiveLedgers</b>, do garbage collection on them.
 * </ul>
 * </ul>
 * Since garbage collection is running in background, HierarchicalLedgerManager did gc on single hash
 * node at a time to avoid consuming too much resources.
 * </p>
 */
class HierarchicalLedgerManager extends AbstractZkLedgerManager {

    static final Logger LOG = LoggerFactory.getLogger(HierarchicalLedgerManager.class);
    public static final String NAME = "hierarchical";

    public static final int CUR_VERSION = 1;

    static final String IDGENERATION_PREFIX = "/idgen/ID-";
    private static final String MAX_ID_SUFFIX = "9999";
    private static final String MIN_ID_SUFFIX = "0000";

    // Path to generate global id
    private final String idGenPath;
    // A sorted map to stored all active ledger ids
    private ConcurrentSkipListMap<Long, Boolean> activeLedgers;

    // we use this to prevent long stack chains from building up in callbacks
    ScheduledExecutorService scheduler;

    /**
     * Constructor
     *
     * @param conf
     *          Configuration object
     * @param zk
     *          ZooKeeper Client Handle
     * @param ledgerRootPath
     *          ZooKeeper Path to store ledger metadata
     * @throws IOException when version is not compatible
     */
    public HierarchicalLedgerManager(AbstractConfiguration conf, ZooKeeper zk,
                                     String ledgerRootPath, int layoutVersion)
        throws IOException {
        super(conf, zk, ledgerRootPath);

        if (layoutVersion != CUR_VERSION) {
            throw new IOException("Incompatible layout version found : " 
                                  + layoutVersion);
        }

        this.idGenPath = ledgerRootPath + IDGENERATION_PREFIX;
        this.activeLedgers = new ConcurrentSkipListMap<Long, Boolean>();
        this.scheduler = Executors.newSingleThreadScheduledExecutor();
        if (LOG.isDebugEnabled()) {
            LOG.debug("Using HierarchicalLedgerManager with root path : " + ledgerRootPath);
        }
    }

    @Override
    public void close() {
        try {
            scheduler.shutdown();
        } catch (Exception e) {
            LOG.warn("Error when closing HierarchicalLedgerManager : ", e);
        }
        super.close();
    }

    @Override
    public void newLedgerPath(final GenericCallback<String> ledgerCb) {
        ZkUtils.createFullPathOptimistic(zk, idGenPath, new byte[0], Ids.OPEN_ACL_UNSAFE,
            CreateMode.EPHEMERAL_SEQUENTIAL, new StringCallback() {
            @Override
            public void processResult(int rc, String path, Object ctx, final String idPathName) {
                if (rc != KeeperException.Code.OK.intValue()) {
                    LOG.error("Could not generate new ledger id",
                              KeeperException.create(KeeperException.Code.get(rc), path));
                    ledgerCb.operationComplete(rc, null);
                    return;
                }
                /*
                 * Extract ledger id from gen path
                 */
                long ledgerId;
                try {
                    ledgerId = getLedgerIdFromGenPath(idPathName);
                } catch (IOException e) {
                    LOG.error("Could not extract ledger-id from id gen path:" + path, e);
                    ledgerCb.operationComplete(KeeperException.Code.SYSTEMERROR.intValue(), null);
                    return;
                }
                StringCallback scb = new StringCallback() {
                    @Override
                    public void processResult(int rc, String path,
                            Object ctx, String name) {
                        if (rc != KeeperException.Code.OK.intValue()) {
                            ledgerCb.operationComplete(rc, null);
                        } else {
                            ledgerCb.operationComplete(rc, name);
                        }
                    }
                };
                String ledgerPath = getLedgerPath(ledgerId);
                ZkUtils.createFullPathOptimistic(zk, ledgerPath, new byte[0],
                    Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT, scb, null);
                // delete the znode for id generation
                scheduler.submit(new Runnable() {
                    @Override
                    public void run() {
                        zk.delete(idPathName, -1, new AsyncCallback.VoidCallback() {
                            @Override
                            public void processResult(int rc, String path, Object ctx) {
                                if (rc != KeeperException.Code.OK.intValue()) {
                                    LOG.warn("Exception during deleting znode for id generation : ",
                                             KeeperException.create(KeeperException.Code.get(rc), path));
                                } else {
                                    if (LOG.isDebugEnabled()) {
                                        LOG.debug("Deleting znode for id generation : " + idPathName);
                                    }
                                }
                            }
                        }, null);
                    }
                });
            }
        }, null);
    }

    // get ledger id from generation path
    private long getLedgerIdFromGenPath(String nodeName) throws IOException {
        long ledgerId;
        try {
            String parts[] = nodeName.split(IDGENERATION_PREFIX);
            ledgerId = Long.parseLong(parts[parts.length - 1]);
        } catch (NumberFormatException e) {
            throw new IOException(e);
        }
        return ledgerId;
    }

    @Override
    public String getLedgerPath(long ledgerId) {
        String ledgerIdStr = StringUtils.getZKStringId(ledgerId);
        // do 2-4-4 split
        StringBuilder sb = new StringBuilder();
        sb.append(ledgerRootPath).append("/")
          .append(ledgerIdStr.substring(0, 2)).append("/")
          .append(ledgerIdStr.substring(2, 6)).append("/")
          .append(LEDGER_NODE_PREFIX).append(ledgerIdStr.substring(6, 10));
        return sb.toString();
    }

    @Override
    public long getLedgerId(String pathName) throws IOException {
        if (!pathName.startsWith(ledgerRootPath)) {
            throw new IOException("it is not a valid hashed path name : " + pathName);
        }
        String hierarchicalPath = pathName.substring(ledgerRootPath.length() + 1);
        String[] hierarchicalParts = hierarchicalPath.split("/");
        if (hierarchicalParts.length != 3) {
            throw new IOException("it is not a valid hierarchical path name : " + pathName);
        }
        hierarchicalParts[2] =
            hierarchicalParts[2].substring(LEDGER_NODE_PREFIX.length());
        return getLedgerId(hierarchicalParts);
    }

    // get ledger from all level nodes
    private long getLedgerId(String...levelNodes) throws IOException {
        try {
            StringBuilder sb = new StringBuilder();
            for (String node : levelNodes) {
                sb.append(node);
            }
            return Long.parseLong(sb.toString());
        } catch (NumberFormatException e) {
            throw new IOException(e);
        }
    }

    //
    // Active Ledger Manager
    //

    /**
     * Get the smallest cache id in a specified node /level1/level2
     *
     * @param level1
     *          1st level node name
     * @param level2
     *          2nd level node name
     * @return the smallest ledger id
     */
    private long getStartLedgerIdByLevel(String level1, String level2) throws IOException {
        return getLedgerId(level1, level2, MIN_ID_SUFFIX);
    }

    /**
     * Get the largest cache id in a specified node /level1/level2
     *
     * @param level1
     *          1st level node name
     * @param level2
     *          2nd level node name
     * @return the largest ledger id
     */
    private long getEndLedgerIdByLevel(String level1, String level2) throws IOException {
        return getLedgerId(level1, level2, MAX_ID_SUFFIX);
    }

    @Override
    public void asyncProcessLedgers(final Processor<Long> processor,
                                    final AsyncCallback.VoidCallback finalCb, final Object context,
                                    final int successRc, final int failureRc) {
        // process 1st level nodes
        asyncProcessLevelNodes(ledgerRootPath, new Processor<String>() {
            @Override
            public void process(final String l1Node, final AsyncCallback.VoidCallback cb1) {
                if (isSpecialZnode(l1Node)) {
                    cb1.processResult(successRc, null, context);
                    return;
                }
                final String l1NodePath = ledgerRootPath + "/" + l1Node;
                // process level1 path, after all children of level1 process
                // it callback to continue processing next level1 node
                asyncProcessLevelNodes(l1NodePath, new Processor<String>() {
                    @Override
                    public void process(String l2Node, AsyncCallback.VoidCallback cb2) {
                        // process level1/level2 path
                        String l2NodePath = ledgerRootPath + "/" + l1Node + "/" + l2Node;
                        // process each ledger
                        // after all ledger are processed, cb2 will be call to continue processing next level2 node
                        asyncProcessLedgersInSingleNode(l2NodePath, processor, cb2,
                                                        context, successRc, failureRc);
                    }
                }, cb1, context, successRc, failureRc);
            }
        }, finalCb, context, successRc, failureRc);
    }

    /**
     * Process hash nodes in a given path
     */
    private void asyncProcessLevelNodes(
        final String path, final Processor<String> processor,
        final AsyncCallback.VoidCallback finalCb, final Object context,
        final int successRc, final int failureRc) {
        zk.sync(path, new AsyncCallback.VoidCallback() {
            @Override
            public void processResult(int rc, String path, Object ctx) {
                if (rc != Code.OK.intValue()) {
                    LOG.error("Error syncing path " + path + " when getting its chidren: ",
                              KeeperException.create(KeeperException.Code.get(rc), path));
                    finalCb.processResult(failureRc, null, context);
                    return;
                }

                zk.getChildren(path, false, new AsyncCallback.ChildrenCallback() {
                    @Override
                    public void processResult(int rc, String path, Object ctx,
                                              List<String> levelNodes) {
                        if (rc != Code.OK.intValue()) {
                            LOG.error("Error polling hash nodes of " + path,
                                      KeeperException.create(KeeperException.Code.get(rc), path));
                            finalCb.processResult(failureRc, null, context);
                            return;
                        }
                        AsyncListProcessor<String> listProcessor =
                                new AsyncListProcessor<String>(scheduler);
                        // process its children
                        listProcessor.process(levelNodes, processor, finalCb,
                                              context, successRc, failureRc);
                    }
                }, null);
            }
        }, null);
    }

    @Override
    public void addActiveLedger(long ledgerId, boolean active) {
        activeLedgers.put(ledgerId, active);
    }

    @Override
    public void removeActiveLedger(long ledgerId) {
        activeLedgers.remove(ledgerId);
    }

    @Override
    public boolean containsActiveLedger(long ledgerId) {
        return activeLedgers.containsKey(ledgerId);
    }

    @Override
    public void garbageCollectLedgers(GarbageCollector gc) {
        try {
            List<String> l1Nodes = zk.getChildren(ledgerRootPath, null);
            for (String l1Node : l1Nodes) {
                if (isSpecialZnode(l1Node)) {
                    continue;
                }
                try {
                    List<String> l2Nodes = zk.getChildren(ledgerRootPath + "/" + l1Node, null);
                    for (String l2Node : l2Nodes) {
                        doGcByLevel(gc, l1Node, l2Node);
                    }
                } catch (Exception e) {
                    LOG.warn("Exception during garbage collecting ledgers for " + l1Node
                           + " of " + ledgerRootPath);
                }
            }
        } catch (Exception e) {
            LOG.warn("Exception during garbage collecting inactive/deleted ledgers");
        }
    }

    /**
     * Garbage collection a single node level1/level2
     *
     * @param gc
     *          Garbage collector
     * @param level1
     *          1st level node name
     * @param level2
     *          2nd level node name
     * @throws IOException
     * @throws InterruptedException
     */
    void doGcByLevel(GarbageCollector gc, final String level1, final String level2)
        throws IOException, InterruptedException {

        StringBuilder nodeBuilder = new StringBuilder();
        nodeBuilder.append(ledgerRootPath).append("/")
                   .append(level1).append("/").append(level2);
        String nodePath = nodeBuilder.toString();

        HashSet<Long> zkActiveLedgers = getLedgersInSingleNode(nodePath);
        // get hosted ledgers in /level1/level2
        long startLedgerId = getStartLedgerIdByLevel(level1, level2);
        long endLedgerId = getEndLedgerIdByLevel(level1, level2);
        ConcurrentMap<Long, Boolean> bkActiveLedgers =
            activeLedgers.subMap(startLedgerId, true, endLedgerId, true);
        if (LOG.isDebugEnabled()) {
            LOG.debug("All active ledgers from ZK for hash node "
                      + level1 + "/" + level2 + " : " + zkActiveLedgers);
            LOG.debug("Current active ledgers from Bookie for hash node "
                      + level1 + "/" + level2 + " : " + bkActiveLedgers);
        }

        doGc(gc, bkActiveLedgers, zkActiveLedgers);
    }

    /**
     * Do garbage collecting comparing hosted ledgers and zk ledgers
     *
     * @param gc
     *          Garbage collector
     * @param bkActiveLedgers
     *          Active ledgers hosted in bookie server
     * @param zkAllLedgers
     *          All ledgers stored in zookeeper
     */
    void doGc(GarbageCollector gc, ConcurrentMap<Long, Boolean> bkActiveLedgers,
              HashSet<Long> zkAllLedgers) {
        // remove any active ledgers that doesn't exist in zk
        for (Long lid : bkActiveLedgers.keySet()) {
            if (!zkAllLedgers.contains(lid)) {
                // remove it from current active ledger
                bkActiveLedgers.remove(lid);
                gc.gc(lid);
            }
        }
    }

    /**
     * Process list one by one in asynchronize way. Process will be stopped immediately
     * when error occurred.
     */
    private static class AsyncListProcessor<T> {
        // use this to prevent long stack chains from building up in callbacks
        ScheduledExecutorService scheduler;

        /**
         * Constructor
         *
         * @param scheduler
         *          Executor used to prevent long stack chains
         */
        public AsyncListProcessor(ScheduledExecutorService scheduler) {
            this.scheduler = scheduler;
        }

        /**
         * Process list of items
         *
         * @param data
         *          List of data to process
         * @param processor
         *          Callback to process element of list when success
         * @param finalCb
         *          Final callback to be called after all elements in the list are processed
         * @param contxt
         *          Context of final callback
         * @param successRc
         *          RC passed to final callback on success
         * @param failureRc
         *          RC passed to final callback on failure
         */
        public void process(final List<T> data, final Processor<T> processor,
                            final AsyncCallback.VoidCallback finalCb, final Object context,
                            final int successRc, final int failureRc) {
            if (data == null || data.size() == 0) {
                finalCb.processResult(successRc, null, context);
                return;
            }
            final int size = data.size();
            final AtomicInteger current = new AtomicInteger(0);
            AsyncCallback.VoidCallback stubCallback = new AsyncCallback.VoidCallback() {
                @Override
                public void processResult(int rc, String path, Object ctx) {
                    if (rc != successRc) {
                        // terminal immediately
                        finalCb.processResult(failureRc, null, context);
                        return;
                    }
                    // process next element
                    int next = current.incrementAndGet();
                    if (next >= size) { // reach the end of list
                        finalCb.processResult(successRc, null, context);
                        return;
                    }
                    final T dataToProcess = data.get(next);
                    final AsyncCallback.VoidCallback stub = this;
                    scheduler.submit(new Runnable() {
                        @Override
                        public final void run() {
                            processor.process(dataToProcess, stub);
                        }
                    });
                }
            };
            T firstElement = data.get(0);
            processor.process(firstElement, stubCallback);
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/meta/LedgerLayout.java,true,"package org.apache.bookkeeper.meta;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.IOException;

import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.ZooDefs.Ids;
import org.apache.zookeeper.data.Stat;
import org.apache.zookeeper.ZooKeeper;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.commons.configuration.ConfigurationException;

import org.apache.bookkeeper.conf.AbstractConfiguration;
import org.apache.bookkeeper.meta.FlatLedgerManager;

/**
 * This class encapsulates ledger layout information that is persistently stored
 * in zookeeper. It provides parsing and serialization methods of such information.
 *
 */
class LedgerLayout {
    static final Logger LOG = LoggerFactory.getLogger(LedgerLayout.class);

    // Znode name to store layout information
    public static final String LAYOUT_ZNODE = "LAYOUT";
    // version of ledger layout metadata
    public static final int LAYOUT_FORMAT_VERSION = 1;

    /**
     * Read ledger layout from zookeeper
     *
     * @param zk            ZooKeeper Client
     * @param ledgersRoot   Root of the ledger namespace to check
     * @return ledger layout, or null if none set in zookeeper
     */
    public static LedgerLayout readLayout(final ZooKeeper zk, final String ledgersRoot)
            throws IOException, KeeperException {
        String ledgersLayout = ledgersRoot + "/" + LAYOUT_ZNODE;

        try {
            LedgerLayout layout;

            try {
                byte[] layoutData = zk.getData(ledgersLayout, false, null);
                layout = parseLayout(layoutData);
            } catch (KeeperException.NoNodeException nne) {
                return null;
            }
            
            return layout;
        } catch (InterruptedException ie) {
            throw new IOException(ie);
        }
    }

    static final String splitter = ":";
    static final String lSplitter = "\n";

    // ledger manager class
    private String managerType;
    // ledger manager version
    private int managerVersion;

    // layout version of how to store layout information
    private int layoutFormatVersion = LAYOUT_FORMAT_VERSION;

    /**
     * Ledger Layout Constructor
     *
     * @param type
     *          Ledger Manager Type
     * @param managerVersion
     *          Ledger Manager Version
     * @param layoutFormatVersion
     *          Ledger Layout Format Version
     */
    public LedgerLayout(String managerType, int managerVersion) {
        this.managerType = managerType;
        this.managerVersion = managerVersion;
    }

    public String getManagerType() {
        return this.managerType;
    }

    public int getManagerVersion() {
        return this.managerVersion;
    }

    /**
     * Store the ledger layout into zookeeper
     */
    public void store(final ZooKeeper zk, String ledgersRoot) 
            throws IOException, KeeperException, InterruptedException {
        String ledgersLayout = ledgersRoot + "/" + LAYOUT_ZNODE;
        zk.create(ledgersLayout, serialize(), 
                  Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
    }

    /**
     * Generates a byte array based on the LedgerLayout object.
     *
     * @return byte[]
     */
    private byte[] serialize() throws IOException {
        StringBuilder sb = new StringBuilder();
        sb.append(layoutFormatVersion).append(lSplitter)
            .append(managerType).append(splitter).append(managerVersion);

        if (LOG.isDebugEnabled()) {
            LOG.debug("Serialized layout info: " + sb.toString());
        }

        return sb.toString().getBytes("UTF-8");
    }

    /**
     * Parses a given byte array and transforms into a LedgerLayout object
     *
     * @param bytes
     *          byte array to parse
     * @param znodeVersion
     *          version of znode
     * @return LedgerLayout
     * @throws IOException
     *             if the given byte[] cannot be parsed
     */
    private static LedgerLayout parseLayout(byte[] bytes) throws IOException {
        String layout = new String(bytes, "UTF-8");

        if (LOG.isDebugEnabled()) {
            LOG.debug("Parsing Layout: " + layout);
        }

        String lines[] = layout.split(lSplitter);

        try {
            int layoutFormatVersion = new Integer(lines[0]);
            if (LAYOUT_FORMAT_VERSION != layoutFormatVersion) {
                throw new IOException("Metadata version not compatible. Expected " 
                        + LAYOUT_FORMAT_VERSION + ", but got " + layoutFormatVersion);
            }

            if (lines.length < 2) {
                throw new IOException("Ledger manager and its version absent from layout: " + layout);
            }

            String[] parts = lines[1].split(splitter);
            if (parts.length != 2) {
                throw new IOException("Invalid Ledger Manager defined in layout : " + layout);
            }
            // ledger manager class
            String managerType = parts[0];
            // ledger manager version
            int managerVersion = new Integer(parts[1]);
            return new LedgerLayout(managerType, managerVersion);
        } catch (NumberFormatException e) {
            throw new IOException(e);
        }
    }

    @Override
    public boolean equals(Object obj) {
        if (null == obj) {
            return false;
        }
        if (!(obj instanceof LedgerLayout)) {
            return false;
        }
        LedgerLayout other = (LedgerLayout)obj;
        return managerType.equals(other.managerType) &&
            managerVersion == other.managerVersion;
    }

    @Override
    public String toString() {
        StringBuilder sb = new StringBuilder();
        sb.append("LV").append(layoutFormatVersion).append(":")
            .append(",Type:").append(managerType).append(":")
            .append(managerVersion);
        return sb.toString();
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/meta/LedgerManager.java,true,"package org.apache.bookkeeper.meta;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.IOException;

import org.apache.zookeeper.AsyncCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.Processor;

/**
 * LedgerManager takes responsibility of ledger management
 *
 * <ul>
 * <li>How to store ledger meta (e.g. in ZooKeeper or other key/value store)
 * <li>How to manager active ledgers (so know how to do garbage collection)
 * <li>How to garbage collect inactive/deleted ledgers
 * </ul>
 */
public interface LedgerManager {

    /**
     * Get the path that is used to store ledger metadata
     *
     * @param ledgerId
     *          Ledger ID
     * @return ledger node path
     */
    public String getLedgerPath(long ledgerId);

    /**
     * Get ledger id from its ledger path
     *
     * @param ledgerPath
     *          Ledger path to store metadata
     * @return ledger id
     * @throws IOException when the ledger path is invalid
     */
    public long getLedgerId(String ledgerPath) throws IOException;

    /**
     * Create a new zk ledger path.
     *
     * @param cb
     *        Callback when getting new zk ledger path to create.
     */
    public abstract void newLedgerPath(GenericCallback<String> cb);

    /**
     * Loop to process all ledgers.
     * <p>
     * <ul>
     * After all ledgers were processed, finalCb will be triggerred:
     * <li> if all ledgers are processed done with OK, success rc will be passed to finalCb.
     * <li> if some ledgers are prcoessed failed, failure rc will be passed to finalCb.
     * </ul>
     * </p>
     *
     * @param processor
     *          Ledger Processor to process a specific ledger
     * @param finalCb
     *          Callback triggered after all ledgers are processed
     * @param context
     *          Context of final callback
     * @param successRc
     *          Success RC code passed to finalCb when callback
     * @param failureRc
     *          Failure RC code passed to finalCb when exceptions occured.
     */
    public void asyncProcessLedgers(Processor<Long> processor, AsyncCallback.VoidCallback finalCb,
                                    Object context, int successRc, int failureRc);

    /**
     * Add active ledger
     *
     * @param ledgerId
     *          Ledger ID
     * @param active
     *          Status of ledger
     */
    public void addActiveLedger(long ledgerId, boolean active);

    /**
     * Remove active ledger
     *
     * @param ledgerId
     *          Ledger ID
     */
    public void removeActiveLedger(long ledgerId);

    /**
     * Is Ledger ledgerId in active ledgers set
     *
     * @param ledgerId
     *          Ledger ID
     * @return true if the ledger is in active ledgers set, otherwise return false
     */
    public boolean containsActiveLedger(long ledgerId);

    /**
     * Garbage Collector which handles ledger deletion in server side
     */
    public static interface GarbageCollector {
        /**
         * garbage collecting a specific ledger
         *
         * @param ledgerId
         *          Ledger ID to be garbage collected
         */
        public void gc(long ledgerId);
    }

    /**
     * Garbage collecting all inactive/deleted ledgers
     * <p>
     * GarbageCollector#gc is triggered each time we found a ledger could be garbage collected.
     * After method finished, all those inactive ledgers should be garbage collected.
     * </p>
     *
     * @param gc garbage collector
     */
    public void garbageCollectLedgers(GarbageCollector gc);

    /**
     * Close ledger manager
     */
    public void close();
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/meta/LedgerManagerFactory.java,false,"package org.apache.bookkeeper.meta;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.IOException;

import org.apache.bookkeeper.conf.AbstractConfiguration;
import org.apache.zookeeper.ZooKeeper;
import org.apache.zookeeper.KeeperException;

/**
 * <code>LedgerManagerFactory</code> takes responsibility of creating new ledger manager.
 */
public class LedgerManagerFactory {
    /**
     * Create new Ledger Manager.
     *
     * @param conf
     *          Configuration Object.
     * @param zk
     *          ZooKeeper Client Handle, talk to zk to know which ledger manager is used.
     * @return new ledger manager
     * @throws IOException
     */
    public static LedgerManager newLedgerManager(
        final AbstractConfiguration conf, final ZooKeeper zk)
            throws IOException, KeeperException, InterruptedException {
        String lmType = conf.getLedgerManagerType();
        String ledgerRootPath = conf.getZkLedgersRootPath();
            
        if (null == ledgerRootPath || ledgerRootPath.length() == 0) {
            throw new IOException("Empty Ledger Root Path.");
        }
        
        // if zk is null, return the default ledger manager
        if (zk == null) {
            return new FlatLedgerManager(conf, zk, 
                    ledgerRootPath, FlatLedgerManager.CUR_VERSION);
        }

        // check that the configured ledger manager is
        // compatible with the existing layout
        LedgerLayout layout = LedgerLayout.readLayout(zk, ledgerRootPath);
        if (layout == null) { // no existing layout
            if (lmType == null 
                || lmType.equals(FlatLedgerManager.NAME)) {
                layout = new LedgerLayout(FlatLedgerManager.NAME, 
                                          FlatLedgerManager.CUR_VERSION);
            } else if (lmType.equals(HierarchicalLedgerManager.NAME)) {
                layout = new LedgerLayout(HierarchicalLedgerManager.NAME, 
                                          HierarchicalLedgerManager.CUR_VERSION);
            } else {
                throw new IOException("Unknown ledger manager type " + lmType);
            }
            try {
                layout.store(zk, ledgerRootPath);
            } catch (KeeperException.NodeExistsException nee) {
                LedgerLayout layout2 = LedgerLayout.readLayout(zk, ledgerRootPath);
                if (!layout2.equals(layout)) {
                    throw new IOException("Contention writing to layout to zookeeper, "
                            + " other layout " + layout2 + " is incompatible with our "
                            + "layout " + layout);
                }
            }
        } else if (lmType != null && !layout.getManagerType().equals(lmType)) {
            throw new IOException("Configured layout " + lmType
                    + " does not match existing layout " + layout.getManagerType());
        }

        // create the ledger manager
        if (FlatLedgerManager.NAME.equals(layout.getManagerType())) {
            return new FlatLedgerManager(conf, zk, ledgerRootPath, 
                                         layout.getManagerVersion());
        } else if (HierarchicalLedgerManager.NAME.equals(layout.getManagerType())) {
            return new HierarchicalLedgerManager(conf, zk, ledgerRootPath,
                                                 layout.getManagerVersion());
        } else {
            throw new IOException("Unknown ledger manager type: " + lmType);
        }
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/proto/BookieClient.java,true,"package org.apache.bookkeeper.proto;

/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

import java.io.IOException;
import java.net.InetSocketAddress;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.Executors;
import java.util.concurrent.atomic.AtomicLong;
import org.apache.bookkeeper.conf.ClientConfiguration;
import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.ReadEntryCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.WriteCallback;
import org.apache.bookkeeper.util.OrderedSafeExecutor;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.buffer.ChannelBuffer;
import org.jboss.netty.buffer.ChannelBuffers;
import org.jboss.netty.channel.socket.ClientSocketChannelFactory;
import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory;

/**
 * Implements the client-side part of the BookKeeper protocol.
 *
 */
public class BookieClient {
    static final Logger LOG = LoggerFactory.getLogger(BookieClient.class);

    // This is global state that should be across all BookieClients
    AtomicLong totalBytesOutstanding = new AtomicLong();

    OrderedSafeExecutor executor;
    ClientSocketChannelFactory channelFactory;
    ConcurrentHashMap<InetSocketAddress, PerChannelBookieClient> channels = new ConcurrentHashMap<InetSocketAddress, PerChannelBookieClient>();

    private final ClientConfiguration conf;

    public BookieClient(ClientConfiguration conf, ClientSocketChannelFactory channelFactory, OrderedSafeExecutor executor) {
        this.conf = conf;
        this.channelFactory = channelFactory;
        this.executor = executor;
    }

    public PerChannelBookieClient lookupClient(InetSocketAddress addr) {
        PerChannelBookieClient channel = channels.get(addr);

        if (channel == null) {
            channel = new PerChannelBookieClient(executor, channelFactory, addr, totalBytesOutstanding);
            PerChannelBookieClient prevChannel = channels.putIfAbsent(addr, channel);
            if (prevChannel != null) {
                channel = prevChannel;
            }
        }

        return channel;
    }

    public void addEntry(final InetSocketAddress addr, final long ledgerId, final byte[] masterKey, final long entryId,
            final ChannelBuffer toSend, final WriteCallback cb, final Object ctx, final int options) {
        final PerChannelBookieClient client = lookupClient(addr);

        client.connectIfNeededAndDoOp(new GenericCallback<Void>() {
            @Override
            public void operationComplete(int rc, Void result) {
                if (rc != BKException.Code.OK) {
                    cb.writeComplete(rc, ledgerId, entryId, addr, ctx);
                    return;
                }
                client.addEntry(ledgerId, masterKey, entryId, toSend, cb, ctx, options);
            }
        });
    }

    public void readEntry(final InetSocketAddress addr, final long ledgerId, final long entryId,
                          final ReadEntryCallback cb, final Object ctx, final int options) {
        final PerChannelBookieClient client = lookupClient(addr);

        client.connectIfNeededAndDoOp(new GenericCallback<Void>() {
            @Override
            public void operationComplete(int rc, Void result) {

                if (rc != BKException.Code.OK) {
                    cb.readEntryComplete(rc, ledgerId, entryId, null, ctx);
                    return;
                }
                client.readEntry(ledgerId, entryId, cb, ctx, options);
            }
        });
    }

    public void close() {
        for (PerChannelBookieClient channel: channels.values()) {
            channel.close();
        }
    }

    private static class Counter {
        int i;
        int total;

        synchronized void inc() {
            i++;
            total++;
        }

        synchronized void dec() {
            i--;
            notifyAll();
        }

        synchronized void wait(int limit) throws InterruptedException {
            while (i > limit) {
                wait();
            }
        }

        synchronized int total() {
            return total;
        }
    }

    /**
     * @param args
     * @throws IOException
     * @throws NumberFormatException
     * @throws InterruptedException
     */
    public static void main(String[] args) throws NumberFormatException, IOException, InterruptedException {
        if (args.length != 3) {
            System.err.println("USAGE: BookieClient bookieHost port ledger#");
            return;
        }
        WriteCallback cb = new WriteCallback() {

            public void writeComplete(int rc, long ledger, long entry, InetSocketAddress addr, Object ctx) {
                Counter counter = (Counter) ctx;
                counter.dec();
                if (rc != 0) {
                    System.out.println("rc = " + rc + " for " + entry + "@" + ledger);
                }
            }
        };
        Counter counter = new Counter();
        byte hello[] = "hello".getBytes();
        long ledger = Long.parseLong(args[2]);
        ClientSocketChannelFactory channelFactory = new NioClientSocketChannelFactory(Executors.newCachedThreadPool(), Executors
                .newCachedThreadPool());
        OrderedSafeExecutor executor = new OrderedSafeExecutor(1);
        BookieClient bc = new BookieClient(new ClientConfiguration(), channelFactory, executor);
        InetSocketAddress addr = new InetSocketAddress(args[0], Integer.parseInt(args[1]));

        for (int i = 0; i < 100000; i++) {
            counter.inc();
            bc.addEntry(addr, ledger, new byte[0], i, ChannelBuffers.wrappedBuffer(hello), cb, counter, 0);
        }
        counter.wait(0);
        System.out.println("Total = " + counter.total());
        channelFactory.releaseExternalResources();
        executor.shutdown();
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/proto/BookieProtocol.java,true,"package org.apache.bookkeeper.proto;

/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

/**
 * The packets of the Bookie protocol all have a 4-byte integer indicating the
 * type of request or response at the very beginning of the packet followed by a
 * payload.
 *
 */
public interface BookieProtocol {

    /**
     * Lowest protocol version which will work with the bookie.
     */
    public static final byte LOWEST_COMPAT_PROTOCOL_VERSION = 0;

    /**
     * Current version of the protocol, which client will use. 
     */
    public static final byte CURRENT_PROTOCOL_VERSION = 1;

    /** 
     * The first int of a packet is the header.
     * It contains the version, opCode and flags.
     * The initial versions of BK didn't have this structure
     * and just had an int representing the opCode as the 
     * first int. This handles that case also. 
     */
    static class PacketHeader {
        final byte version;
        final byte opCode;
        final short flags;

        public PacketHeader(byte version, byte opCode, short flags) {
            this.version = version;
            this.opCode = opCode;
            this.flags = flags;
        }
        
        int toInt() {
            if (version == 0) {
                return (int)opCode;
            } else {
                return ((version & 0xFF) << 24) 
                    | ((opCode & 0xFF) << 16)
                    | (flags & 0xFFFF);
            }
        }

        static PacketHeader fromInt(int i) {
            byte version = (byte)(i >> 24); 
            byte opCode = 0;
            short flags = 0;
            if (version == 0) {
                opCode = (byte)i;
            } else {
                opCode = (byte)((i >> 16) & 0xFF);
                flags = (short)(i & 0xFFFF);
            }
            return new PacketHeader(version, opCode, flags);
        }

        byte getVersion() {
            return version;
        }

        byte getOpCode() {
            return opCode;
        }

        short getFlags() {
            return flags;
        }
    }

    /**
     * The Add entry request payload will be a ledger entry exactly as it should
     * be logged. The response payload will be a 4-byte integer that has the
     * error code followed by the 8-byte ledger number and 8-byte entry number
     * of the entry written.
     */
    public static final byte ADDENTRY = 1;
    /**
     * The Read entry request payload will be the ledger number and entry number
     * to read. (The ledger number is an 8-byte integer and the entry number is
     * a 8-byte integer.) The response payload will be a 4-byte integer
     * representing an error code and a ledger entry if the error code is EOK,
     * otherwise it will be the 8-byte ledger number and the 4-byte entry number
     * requested. (Note that the first sixteen bytes of the entry happen to be
     * the ledger number and entry number as well.)
     */
    public static final byte READENTRY = 2;

    /**
     * The error code that indicates success
     */
    public static final int EOK = 0;
    /**
     * The error code that indicates that the ledger does not exist
     */
    public static final int ENOLEDGER = 1;
    /**
     * The error code that indicates that the requested entry does not exist
     */
    public static final int ENOENTRY = 2;
    /**
     * The error code that indicates an invalid request type
     */
    public static final int EBADREQ = 100;
    /**
     * General error occurred at the server
     */
    public static final int EIO = 101;

    /**
     * Unauthorized access to ledger
     */
    public static final int EUA = 102;

    /**
     * The server version is incompatible with the client
     */
    public static final int EBADVERSION = 103;

    /**
     * Attempt to write to fenced ledger
     */
    public static final int EFENCED = 104;


    public static final short FLAG_NONE = 0x0;
    public static final short FLAG_DO_FENCING = 0x0001;
    public static final short FLAG_RECOVERY_ADD = 0x0002;
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/proto/BookieServer.java,true,"package org.apache.bookkeeper.proto;

/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

import java.io.File;
import java.io.IOException;
import java.net.InetAddress;
import java.net.InetSocketAddress;
import java.net.MalformedURLException;
import java.net.UnknownHostException;
import java.nio.ByteBuffer;

import org.apache.zookeeper.KeeperException;

import org.apache.bookkeeper.bookie.Bookie;
import org.apache.bookkeeper.bookie.BookieException;
import org.apache.bookkeeper.conf.ServerConfiguration;
import org.apache.bookkeeper.proto.NIOServerFactory.Cnxn;
import static org.apache.bookkeeper.proto.BookieProtocol.PacketHeader;
import org.apache.commons.configuration.ConfigurationException;
import org.apache.commons.cli.BasicParser;
import org.apache.commons.cli.Options;
import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.HelpFormatter;
import org.apache.commons.cli.ParseException;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Implements the server-side part of the BookKeeper protocol.
 *
 */
public class BookieServer implements NIOServerFactory.PacketProcessor, BookkeeperInternalCallbacks.WriteCallback {
    final ServerConfiguration conf;
    NIOServerFactory nioServerFactory;
    private volatile boolean running = false;
    Bookie bookie;
    DeathWatcher deathWatcher;
    static Logger LOG = LoggerFactory.getLogger(BookieServer.class);

    public BookieServer(ServerConfiguration conf) 
            throws IOException, KeeperException, InterruptedException {
        this.conf = conf;
        this.bookie = new Bookie(conf);
    }

    public void start() throws IOException {
        nioServerFactory = new NIOServerFactory(conf, this);
        running = true;
        deathWatcher = new DeathWatcher(conf);
        deathWatcher.start();
    }

    public InetSocketAddress getLocalAddress() {
        try {
            return new InetSocketAddress(InetAddress.getLocalHost().getHostAddress(), conf.getBookiePort());
        } catch (UnknownHostException uhe) {
            return nioServerFactory.getLocalAddress();
        }
    }

    public synchronized void shutdown() throws InterruptedException {
        if (!running) {
            return;
        }
        nioServerFactory.shutdown();
        bookie.shutdown();
        running = false;
    }

    public boolean isRunning() {
        return bookie.isRunning() && nioServerFactory.isRunning() && running;
    }

    /**
     * Whether bookie is running?
     *
     * @return true if bookie is running, otherwise return false
     */
    public boolean isBookieRunning() {
        return bookie.isRunning();
    }

    /**
     * Whether nio server is running?
     *
     * @return true if nio server is running, otherwise return false
     */
    public boolean isNioServerRunning() {
        return nioServerFactory.isRunning();
    }

    public void join() throws InterruptedException {
        nioServerFactory.join();
    }

    /**
     * A thread to watch whether bookie & nioserver is still alive
     */
    class DeathWatcher extends Thread {

        final int watchInterval;

        DeathWatcher(ServerConfiguration conf) {
            watchInterval = conf.getDeathWatchInterval();
        }

        @Override
        public void run() {
            while(true) {
                try {
                    Thread.sleep(watchInterval);
                } catch (InterruptedException ie) {
                    // do nothing
                }
                if (!isBookieRunning() || !isNioServerRunning()) {
                    try {
                        shutdown();
                    } catch (InterruptedException ie) {
                        System.exit(-1);
                    }
                    break;
                }
            }
        }
    }

    static final Options bkOpts = new Options();
    static {
        bkOpts.addOption("c", "conf", true, "Configuration for Bookie Server");
        bkOpts.addOption("h", "help", false, "Print help message");
    }

    /**
     * Print usage
     */
    private static void printUsage() {
        HelpFormatter hf = new HelpFormatter();
        hf.printHelp("BookieServer [options]\n\tor\n"
                   + "BookieServer <bookie_port> <zk_servers> <journal_dir> <ledger_dir [ledger_dir]>", bkOpts);
    }

    private static void loadConfFile(ServerConfiguration conf, String confFile)
        throws IllegalArgumentException {
        try {
            conf.loadConf(new File(confFile).toURI().toURL());
        } catch (MalformedURLException e) {
            LOG.error("Could not open configuration file: " + confFile, e);
            throw new IllegalArgumentException();
        } catch (ConfigurationException e) {
            LOG.error("Malformed configuration file: " + confFile, e);
            throw new IllegalArgumentException();
        }
        LOG.info("Using configuration file " + confFile);
    }

    private static ServerConfiguration parseArgs(String[] args)
        throws IllegalArgumentException {
        try {
            BasicParser parser = new BasicParser();
            CommandLine cmdLine = parser.parse(bkOpts, args);

            if (cmdLine.hasOption('h')) {
                throw new IllegalArgumentException();
            }

            ServerConfiguration conf = new ServerConfiguration();
            String[] leftArgs = cmdLine.getArgs();

            if (cmdLine.hasOption('c')) {
                if (null != leftArgs && leftArgs.length > 0) {
                    throw new IllegalArgumentException();
                }
                String confFile = cmdLine.getOptionValue("c");
                loadConfFile(conf, confFile);
                return conf;
            }

            if (leftArgs.length < 4) {
                throw new IllegalArgumentException();
            }

            // command line arguments overwrite settings in configuration file
            conf.setBookiePort(Integer.parseInt(leftArgs[0]));
            conf.setZkServers(leftArgs[1]);
            conf.setJournalDirName(leftArgs[2]);
            String[] ledgerDirNames = new String[leftArgs.length - 3];
            System.arraycopy(leftArgs, 3, ledgerDirNames, 0, ledgerDirNames.length);
            conf.setLedgerDirNames(ledgerDirNames);

            return conf;
        } catch (ParseException e) {
            LOG.error("Error parsing command line arguments : ", e);
            throw new IllegalArgumentException(e);
        }
    }

    /**
     * @param args
     * @throws IOException
     * @throws InterruptedException
     */
    public static void main(String[] args) 
            throws IOException, KeeperException, InterruptedException {
        ServerConfiguration conf = null;
        try {
            conf = parseArgs(args);
        } catch (IllegalArgumentException iae) {
            LOG.error("Error parsing command line arguments : ", iae);
            System.err.println(iae.getMessage());
            printUsage();
            throw iae;
        }

        StringBuilder sb = new StringBuilder();
        String[] ledgerDirNames = conf.getLedgerDirNames();
        for (int i = 0; i < ledgerDirNames.length; i++) {
            if (i != 0) {
                sb.append(',');
            }
            sb.append(ledgerDirNames[i]);
        }

        String hello = String.format(
                           "Hello, I'm your bookie, listening on port %1$s. ZKServers are on %2$s. Journals are in %3$s. Ledgers are stored in %4$s.",
                           conf.getBookiePort(), conf.getZkServers(),
                           conf.getJournalDirName(), sb);
        LOG.info(hello);
        final BookieServer bs = new BookieServer(conf);
        bs.start();
        Runtime.getRuntime().addShutdownHook(new Thread() {
            @Override
            public void run() {
                try {
                    bs.shutdown();
                    LOG.info("Shut down bookie server successfully");
                } catch (InterruptedException ie) {
                    LOG.warn("Exception when shutting down bookie server : ", ie);
                }
            }
        });
        LOG.info("Register shutdown hook successfully");
        bs.join();
    }

    public void processPacket(ByteBuffer packet, Cnxn src) {
        PacketHeader h = PacketHeader.fromInt(packet.getInt());

        // packet format is different between ADDENTRY and READENTRY
        long ledgerId = -1;
        long entryId = -1;
        byte[] masterKey = null;
        switch (h.getOpCode()) {
        case BookieProtocol.ADDENTRY:
            // first read master key
            masterKey = new byte[20];
            packet.get(masterKey, 0, 20);
            // !! fall thru to read ledger id and entry id
        case BookieProtocol.READENTRY:
            ByteBuffer bb = packet.duplicate();
            ledgerId = bb.getLong();
            entryId = bb.getLong();
            break;
        }

        if (h.getVersion() < BookieProtocol.LOWEST_COMPAT_PROTOCOL_VERSION
            || h.getVersion() > BookieProtocol.CURRENT_PROTOCOL_VERSION) {
            LOG.error("Invalid protocol version, expected something between "
                      + BookieProtocol.LOWEST_COMPAT_PROTOCOL_VERSION 
                      + " & " + BookieProtocol.CURRENT_PROTOCOL_VERSION
                    + ". got " + h.getVersion());
            src.sendResponse(buildResponse(BookieProtocol.EBADVERSION, 
                                           h.getVersion(), h.getOpCode(), ledgerId, entryId));
            return;
        }
        short flags = h.getFlags();
        switch (h.getOpCode()) {
        case BookieProtocol.ADDENTRY:
            try {
                // LOG.debug("Master key: " + new String(masterKey));
                if ((flags & BookieProtocol.FLAG_RECOVERY_ADD) == BookieProtocol.FLAG_RECOVERY_ADD) {
                    bookie.recoveryAddEntry(packet.slice(), this, src, masterKey);
                } else {
                    bookie.addEntry(packet.slice(), this, src, masterKey);
                }
            } catch (IOException e) {
                LOG.error("Error writing " + entryId + "@" + ledgerId, e);
                src.sendResponse(buildResponse(BookieProtocol.EIO, h.getVersion(), h.getOpCode(), ledgerId, entryId));
            } catch (BookieException.LedgerFencedException lfe) {
                LOG.error("Attempt to write to fenced ledger", lfe);
                src.sendResponse(buildResponse(BookieProtocol.EFENCED, h.getVersion(), h.getOpCode(), ledgerId, entryId));
            } catch (BookieException e) {
                LOG.error("Unauthorized access to ledger " + ledgerId, e);
                src.sendResponse(buildResponse(BookieProtocol.EUA, h.getVersion(), h.getOpCode(), ledgerId, entryId));
            }
            break;
        case BookieProtocol.READENTRY:
            ByteBuffer[] rsp = new ByteBuffer[2];
            LOG.debug("Received new read request: " + ledgerId + ", " + entryId);
            int errorCode = BookieProtocol.EIO;
            try {
                if ((flags & BookieProtocol.FLAG_DO_FENCING) == BookieProtocol.FLAG_DO_FENCING) {
                    LOG.warn("Ledger " + ledgerId + " fenced by " + src.getPeerName());
                    bookie.fenceLedger(ledgerId);
                }
                rsp[1] = bookie.readEntry(ledgerId, entryId);
                LOG.debug("##### Read entry ##### " + rsp[1].remaining());
                errorCode = BookieProtocol.EOK;
            } catch (Bookie.NoLedgerException e) {
                if (LOG.isTraceEnabled()) {
                    LOG.error("Error reading " + entryId + "@" + ledgerId, e);
                }
                errorCode = BookieProtocol.ENOLEDGER;
            } catch (Bookie.NoEntryException e) {
                if (LOG.isTraceEnabled()) {
                    LOG.error("Error reading " + entryId + "@" + ledgerId, e);
                }
                errorCode = BookieProtocol.ENOENTRY;
            } catch (IOException e) {
                if (LOG.isTraceEnabled()) {
                    LOG.error("Error reading " + entryId + "@" + ledgerId, e);
                }
                errorCode = BookieProtocol.EIO;
            }
            rsp[0] = buildResponse(errorCode, h.getVersion(), h.getOpCode(), ledgerId, entryId);

            if (LOG.isTraceEnabled()) {
                LOG.trace("Read entry rc = " + errorCode + " for " + entryId + "@" + ledgerId);
            }
            if (rsp[1] == null) {
                // We haven't filled in entry data, so we have to send back
                // the ledger and entry ids here
                rsp[1] = ByteBuffer.allocate(16);
                rsp[1].putLong(ledgerId);
                rsp[1].putLong(entryId);
                rsp[1].flip();
            }
            LOG.debug("Sending response for: " + entryId + ", " + new String(rsp[1].array()));
            src.sendResponse(rsp);
            break;
        default: 
            src.sendResponse(buildResponse(BookieProtocol.EBADREQ, h.getVersion(), h.getOpCode(), ledgerId, entryId));
        }
    }
    
    private ByteBuffer buildResponse(int errorCode, byte version, byte opCode, long ledgerId, long entryId) {
        ByteBuffer rsp = ByteBuffer.allocate(24);
        rsp.putInt(new PacketHeader(version, 
                                    opCode, (short)0).toInt());
        rsp.putInt(errorCode);
        rsp.putLong(ledgerId);
        rsp.putLong(entryId);

        rsp.flip();
        return rsp;
    }

    public void writeComplete(int rc, long ledgerId, long entryId, InetSocketAddress addr, Object ctx) {
        Cnxn src = (Cnxn) ctx;
        ByteBuffer bb = ByteBuffer.allocate(24);
        bb.putInt(new PacketHeader(BookieProtocol.CURRENT_PROTOCOL_VERSION, 
                                   BookieProtocol.ADDENTRY, (short)0).toInt());
        bb.putInt(rc);
        bb.putLong(ledgerId);
        bb.putLong(entryId);
        bb.flip();
        if (LOG.isTraceEnabled()) {
            LOG.trace("Add entry rc = " + rc + " for " + entryId + "@" + ledgerId);
        }
        src.sendResponse(new ByteBuffer[] { bb });
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/proto/BookkeeperInternalCallbacks.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.proto;

import java.net.InetSocketAddress;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.atomic.AtomicInteger;

import org.apache.zookeeper.AsyncCallback;
import org.jboss.netty.buffer.ChannelBuffer;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Declaration of a callback interfaces used in bookkeeper client library but
 * not exposed to the client application.
 */

public class BookkeeperInternalCallbacks {

    static final Logger LOG = LoggerFactory.getLogger(BookkeeperInternalCallbacks.class);

    /**
     * Callback for calls from BookieClient objects. Such calls are for replies
     * of write operations (operations to add an entry to a ledger).
     *
     */

    public interface WriteCallback {
        void writeComplete(int rc, long ledgerId, long entryId, InetSocketAddress addr, Object ctx);
    }

    public interface GenericCallback<T> {
        void operationComplete(int rc, T result);
    }

    /**
     * Declaration of a callback implementation for calls from BookieClient objects.
     * Such calls are for replies of read operations (operations to read an entry
     * from a ledger).
     *
     */

    public interface ReadEntryCallback {
        void readEntryComplete(int rc, long ledgerId, long entryId, ChannelBuffer buffer, Object ctx);
    }

    /**
     * This is a multi callback object that waits for all of
     * the multiple async operations to complete. If any fail, then we invoke
     * the final callback with a provided failureRc
     */
    public static class MultiCallback implements AsyncCallback.VoidCallback {
        // Number of expected callbacks
        final int expected;
        final int failureRc;
        final int successRc;
        // Final callback and the corresponding context to invoke
        final AsyncCallback.VoidCallback cb;
        final Object context;
        // This keeps track of how many operations have completed
        final AtomicInteger done = new AtomicInteger();
        // List of the exceptions from operations that completed unsuccessfully
        final LinkedBlockingQueue<Integer> exceptions = new LinkedBlockingQueue<Integer>();

        public MultiCallback(int expected, AsyncCallback.VoidCallback cb, Object context, int successRc, int failureRc) {
            this.expected = expected;
            this.cb = cb;
            this.context = context;
            this.failureRc = failureRc;
            this.successRc = successRc;
            if (expected == 0) {
                cb.processResult(successRc, null, context);
            }
        }

        private void tick() {
            if (done.incrementAndGet() == expected) {
                if (exceptions.isEmpty()) {
                    cb.processResult(successRc, null, context);
                } else {
                    cb.processResult(failureRc, null, context);
                }
            }
        }

        @Override
        public void processResult(int rc, String path, Object ctx) {
            if (rc != successRc) {
                LOG.error("Error in mutil callback : " + rc);
                exceptions.add(rc);
            }
            tick();
        }

    }

    /**
     * Processor to process a specific element
     */
    public static interface Processor<T> {
        /**
         * Process a specific element
         *
         * @param data
         *          data to process
         * @param iterationCallback
         *          Callback to invoke when process has been done.
         */
        public void process(T data, AsyncCallback.VoidCallback cb);
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/proto/NIOServerFactory.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.bookkeeper.proto;

import java.io.IOException;
import java.net.InetSocketAddress;
import java.nio.ByteBuffer;
import java.nio.channels.CancelledKeyException;
import java.nio.channels.Channel;
import java.nio.channels.SelectionKey;
import java.nio.channels.Selector;
import java.nio.channels.ServerSocketChannel;
import java.nio.channels.SocketChannel;
import java.util.ArrayList;
import java.util.Collections;
import java.util.HashSet;
import java.util.Iterator;
import java.util.Set;
import java.util.concurrent.LinkedBlockingQueue;

import org.apache.bookkeeper.conf.ServerConfiguration;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * This class handles communication with clients using NIO. There is one Cnxn
 * per client, but only one thread doing the communication.
 */
public class NIOServerFactory extends Thread {

    public interface PacketProcessor {
        public void processPacket(ByteBuffer packet, Cnxn src);
    }

    ServerStats stats = new ServerStats();

    Logger LOG = LoggerFactory.getLogger(NIOServerFactory.class);

    ServerSocketChannel ss;

    Selector selector = Selector.open();

    /**
     * We use this buffer to do efficient socket I/O. Since there is a single
     * sender thread per NIOServerCnxn instance, we can use a member variable to
     * only allocate it once.
     */
    ByteBuffer directBuffer = ByteBuffer.allocateDirect(64 * 1024);

    HashSet<Cnxn> cnxns = new HashSet<Cnxn>();

    int outstandingLimit = 2000;

    PacketProcessor processor;

    long minLatency = 99999999;

    ServerConfiguration conf;

    public NIOServerFactory(ServerConfiguration conf, PacketProcessor processor) throws IOException {
        super("NIOServerFactory");
        setDaemon(true);
        this.processor = processor;
        this.conf = conf;
        this.ss = ServerSocketChannel.open();
        ss.socket().bind(new InetSocketAddress(conf.getBookiePort()));
        ss.configureBlocking(false);
        ss.register(selector, SelectionKey.OP_ACCEPT);
        start();
    }

    public InetSocketAddress getLocalAddress() {
        return (InetSocketAddress) ss.socket().getLocalSocketAddress();
    }

    private void addCnxn(Cnxn cnxn) {
        synchronized (cnxns) {
            cnxns.add(cnxn);
        }
    }

    public boolean isRunning() {
        return !ss.socket().isClosed();
    }

    @Override
    public void run() {
        while (!ss.socket().isClosed()) {
            try {
                selector.select(1000);
                Set<SelectionKey> selected;
                synchronized (this) {
                    selected = selector.selectedKeys();
                }
                ArrayList<SelectionKey> selectedList = new ArrayList<SelectionKey>(selected);
                Collections.shuffle(selectedList);
                for (SelectionKey k : selectedList) {
                    if ((k.readyOps() & SelectionKey.OP_ACCEPT) != 0) {
                        SocketChannel sc = ((ServerSocketChannel) k.channel()).accept();
                        sc.configureBlocking(false);
                        SelectionKey sk = sc.register(selector, SelectionKey.OP_READ);
                        Cnxn cnxn = new Cnxn(sc, sk);
                        sk.attach(cnxn);
                        addCnxn(cnxn);
                    } else if ((k.readyOps() & (SelectionKey.OP_READ | SelectionKey.OP_WRITE)) != 0) {
                        Cnxn c = (Cnxn) k.attachment();
                        c.doIO(k);
                    }
                }
                selected.clear();
            } catch (Exception e) {
                LOG.warn("Exception in server socket loop: " + ss.socket().getInetAddress(), e);
            }
        }
        LOG.info("NIOServerCnxn factory exitedloop.");
        clear();
    }

    /**
     * clear all the connections in the selector
     *
     */
    synchronized public void clear() {
        selector.wakeup();
        synchronized (cnxns) {
            // got to clear all the connections that we have in the selector
            for (Iterator<Cnxn> it = cnxns.iterator(); it.hasNext();) {
                Cnxn cnxn = it.next();
                it.remove();
                try {
                    cnxn.close();
                } catch (Exception e) {
                    // Do nothing.
                }
            }
        }

    }

    public void shutdown() {
        try {
            ss.close();
            clear();
            this.interrupt();
            this.join();
        } catch (InterruptedException e) {
            LOG.warn("Interrupted", e);
        } catch (Exception e) {
            LOG.error("Unexpected exception", e);
        }
    }

    /**
     * The buffer will cause the connection to be close when we do a send.
     */
    static final ByteBuffer closeConn = ByteBuffer.allocate(0);

    public class Cnxn {

        private SocketChannel sock;

        private SelectionKey sk;

        boolean initialized;

        ByteBuffer lenBuffer = ByteBuffer.allocate(4);

        ByteBuffer incomingBuffer = lenBuffer;

        LinkedBlockingQueue<ByteBuffer> outgoingBuffers = new LinkedBlockingQueue<ByteBuffer>();

        int sessionTimeout;

        int packetsSent;

        int packetsReceived;

        void doIO(SelectionKey k) throws InterruptedException {
            try {
                if (sock == null) {
                    return;
                }
                if (k.isReadable()) {
                    int rc = sock.read(incomingBuffer);
                    if (rc < 0) {
                        throw new IOException("Read error");
                    }
                    if (incomingBuffer.remaining() == 0) {
                        incomingBuffer.flip();
                        if (incomingBuffer == lenBuffer) {
                            readLength(k);
                        } else {
                            cnxnStats.packetsReceived++;
                            stats.incrementPacketsReceived();
                            try {
                                readRequest();
                            } finally {
                                lenBuffer.clear();
                                incomingBuffer = lenBuffer;
                            }
                        }
                    }
                }
                if (k.isWritable()) {
                    if (outgoingBuffers.size() > 0) {
                        // ZooLog.logTraceMessage(LOG,
                        // ZooLog.CLIENT_DATA_PACKET_TRACE_MASK,
                        // "sk " + k + " is valid: " +
                        // k.isValid());

                        /*
                         * This is going to reset the buffer position to 0 and
                         * the limit to the size of the buffer, so that we can
                         * fill it with data from the non-direct buffers that we
                         * need to send.
                         */
                        directBuffer.clear();

                        for (ByteBuffer b : outgoingBuffers) {
                            if (directBuffer.remaining() < b.remaining()) {
                                /*
                                 * When we call put later, if the directBuffer
                                 * is to small to hold everything, nothing will
                                 * be copied, so we've got to slice the buffer
                                 * if it's too big.
                                 */
                                b = (ByteBuffer) b.slice().limit(directBuffer.remaining());
                            }
                            /*
                             * put() is going to modify the positions of both
                             * buffers, put we don't want to change the position
                             * of the source buffers (we'll do that after the
                             * send, if needed), so we save and reset the
                             * position after the copy
                             */
                            int p = b.position();
                            directBuffer.put(b);
                            b.position(p);
                            if (directBuffer.remaining() == 0) {
                                break;
                            }
                        }
                        /*
                         * Do the flip: limit becomes position, position gets
                         * set to 0. This sets us up for the write.
                         */
                        directBuffer.flip();

                        int sent = sock.write(directBuffer);
                        ByteBuffer bb;

                        // Remove the buffers that we have sent
                        while (outgoingBuffers.size() > 0) {
                            bb = outgoingBuffers.peek();
                            if (bb == closeConn) {
                                throw new IOException("closing");
                            }
                            int left = bb.remaining() - sent;
                            if (left > 0) {
                                /*
                                 * We only partially sent this buffer, so we
                                 * update the position and exit the loop.
                                 */
                                bb.position(bb.position() + sent);
                                break;
                            }
                            cnxnStats.packetsSent++;
                            /* We've sent the whole buffer, so drop the buffer */
                            sent -= bb.remaining();
                            ServerStats.getInstance().incrementPacketsSent();
                            outgoingBuffers.remove();
                        }
                        // ZooLog.logTraceMessage(LOG,
                        // ZooLog.CLIENT_DATA_PACKET_TRACE_MASK, "after send,
                        // outgoingBuffers.size() = " + outgoingBuffers.size());
                    }
                    synchronized (this) {
                        if (outgoingBuffers.size() == 0) {
                            if (!initialized && (sk.interestOps() & SelectionKey.OP_READ) == 0) {
                                throw new IOException("Responded to info probe");
                            }
                            sk.interestOps(sk.interestOps() & (~SelectionKey.OP_WRITE));
                        } else {
                            sk.interestOps(sk.interestOps() | SelectionKey.OP_WRITE);
                        }
                    }
                }
            } catch (CancelledKeyException e) {
                close();
            } catch (IOException e) {
                // LOG.error("FIXMSG",e);
                close();
            }
        }

        private void readRequest() throws IOException {
            incomingBuffer = incomingBuffer.slice();
            processor.processPacket(incomingBuffer, this);
        }

        public void disableRecv() {
            sk.interestOps(sk.interestOps() & (~SelectionKey.OP_READ));
        }

        public void enableRecv() {
            if (sk.isValid()) {
                int interest = sk.interestOps();
                if ((interest & SelectionKey.OP_READ) == 0) {
                    sk.interestOps(interest | SelectionKey.OP_READ);
                }
            }
        }

        private void readLength(SelectionKey k) throws IOException {
            // Read the length, now get the buffer
            int len = lenBuffer.getInt();
            if (len < 0 || len > 0xfffff) {
                throw new IOException("Len error " + len);
            }
            incomingBuffer = ByteBuffer.allocate(len);
        }

        /**
         * The number of requests that have been submitted but not yet responded
         * to.
         */
        int outstandingRequests;

        /*
         * (non-Javadoc)
         *
         * @see org.apache.zookeeper.server.ServerCnxnIface#getSessionTimeout()
         */
        public int getSessionTimeout() {
            return sessionTimeout;
        }

        String peerName = null;

        public Cnxn(SocketChannel sock, SelectionKey sk) throws IOException {
            this.sock = sock;
            this.sk = sk;
            sock.socket().setTcpNoDelay(conf.getServerTcpNoDelay());
            sock.socket().setSoLinger(true, 2);
            sk.interestOps(SelectionKey.OP_READ);
            if (LOG.isTraceEnabled()) {
                peerName = sock.socket().toString();
            }

            lenBuffer.clear();
            incomingBuffer = lenBuffer;
        }

        @Override
        public String toString() {
            return "NIOServerCnxn object with sock = " + sock + " and sk = " + sk;
        }

        public String getPeerName() {
            if (peerName == null) {
                peerName = sock.socket().toString();
            }
            return peerName;
        }

        boolean closed;

        /*
         * (non-Javadoc)
         *
         * @see org.apache.zookeeper.server.ServerCnxnIface#close()
         */
        public void close() {
            if (closed) {
                return;
            }
            closed = true;
            synchronized (cnxns) {
                cnxns.remove(this);
            }
            LOG.debug("close  NIOServerCnxn: " + sock);
            try {
                /*
                 * The following sequence of code is stupid! You would think
                 * that only sock.close() is needed, but alas, it doesn't work
                 * that way. If you just do sock.close() there are cases where
                 * the socket doesn't actually close...
                 */
                sock.socket().shutdownOutput();
            } catch (IOException e) {
                // This is a relatively common exception that we can't avoid
            }
            try {
                sock.socket().shutdownInput();
            } catch (IOException e) {
            }
            try {
                sock.socket().close();
            } catch (IOException e) {
                LOG.error("FIXMSG", e);
            }
            try {
                sock.close();
                // XXX The next line doesn't seem to be needed, but some posts
                // to forums suggest that it is needed. Keep in mind if errors
                // in
                // this section arise.
                // factory.selector.wakeup();
            } catch (IOException e) {
                LOG.error("FIXMSG", e);
            }
            sock = null;
            if (sk != null) {
                try {
                    // need to cancel this selection key from the selector
                    sk.cancel();
                } catch (Exception e) {
                }
            }
        }

        private void makeWritable(SelectionKey sk) {
            try {
                selector.wakeup();
                if (sk.isValid()) {
                    sk.interestOps(sk.interestOps() | SelectionKey.OP_WRITE);
                }
            } catch (RuntimeException e) {
                LOG.error("Problem setting writable", e);
                throw e;
            }
        }

        private void sendBuffers(ByteBuffer bb[]) {
            ByteBuffer len = ByteBuffer.allocate(4);
            int total = 0;
            for (int i = 0; i < bb.length; i++) {
                if (bb[i] != null) {
                    total += bb[i].remaining();
                }
            }
            if (LOG.isTraceEnabled()) {
                LOG.debug("Sending response of size " + total + " to " + peerName);
            }
            len.putInt(total);
            len.flip();
            outgoingBuffers.add(len);
            for (int i = 0; i < bb.length; i++) {
                if (bb[i] != null) {
                    outgoingBuffers.add(bb[i]);
                }
            }
            makeWritable(sk);
        }

        synchronized public void sendResponse(ByteBuffer... bb) {
            if (closed) {
                return;
            }
            sendBuffers(bb);
            synchronized (NIOServerFactory.this) {
                outstandingRequests--;
                // check throttling
                if (outstandingRequests < outstandingLimit) {
                    sk.selector().wakeup();
                    enableRecv();
                }
            }
        }

        public InetSocketAddress getRemoteAddress() {
            return (InetSocketAddress) sock.socket().getRemoteSocketAddress();
        }

        private class CnxnStats {
            long packetsReceived;

            long packetsSent;

            /**
             * The number of requests that have been submitted but not yet
             * responded to.
             */
            public long getOutstandingRequests() {
                return outstandingRequests;
            }

            public long getPacketsReceived() {
                return packetsReceived;
            }

            public long getPacketsSent() {
                return packetsSent;
            }

            @Override
            public String toString() {
                StringBuilder sb = new StringBuilder();
                Channel channel = sk.channel();
                if (channel instanceof SocketChannel) {
                    sb.append(" ").append(((SocketChannel) channel).socket().getRemoteSocketAddress()).append("[")
                    .append(Integer.toHexString(sk.interestOps())).append("](queued=").append(
                        getOutstandingRequests()).append(",recved=").append(getPacketsReceived()).append(
                            ",sent=").append(getPacketsSent()).append(")\n");
                }
                return sb.toString();
            }
        }

        private CnxnStats cnxnStats = new CnxnStats();

        public CnxnStats getStats() {
            return cnxnStats;
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/proto/PerChannelBookieClient.java,true,"package org.apache.bookkeeper.proto;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.IOException;
import java.net.InetSocketAddress;
import java.util.ArrayDeque;
import java.util.Queue;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.Semaphore;
import java.util.concurrent.atomic.AtomicLong;

import org.apache.bookkeeper.conf.ClientConfiguration;
import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.WriteCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.ReadEntryCallback;
import static org.apache.bookkeeper.proto.BookieProtocol.PacketHeader;
import org.apache.bookkeeper.util.OrderedSafeExecutor;
import org.apache.bookkeeper.util.SafeRunnable;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.bootstrap.ClientBootstrap;
import org.jboss.netty.buffer.ChannelBuffer;
import org.jboss.netty.buffer.ChannelBuffers;
import org.jboss.netty.channel.Channel;
import org.jboss.netty.channel.ChannelFactory;
import org.jboss.netty.channel.ChannelFuture;
import org.jboss.netty.channel.ChannelFutureListener;
import org.jboss.netty.channel.ChannelHandlerContext;
import org.jboss.netty.channel.ChannelPipeline;
import org.jboss.netty.channel.ChannelPipelineCoverage;
import org.jboss.netty.channel.ChannelPipelineFactory;
import org.jboss.netty.channel.ChannelStateEvent;
import org.jboss.netty.channel.Channels;
import org.jboss.netty.channel.ExceptionEvent;
import org.jboss.netty.channel.MessageEvent;
import org.jboss.netty.channel.SimpleChannelHandler;
import org.jboss.netty.channel.socket.ClientSocketChannelFactory;
import org.jboss.netty.handler.codec.frame.CorruptedFrameException;
import org.jboss.netty.handler.codec.frame.LengthFieldBasedFrameDecoder;
import org.jboss.netty.handler.codec.frame.TooLongFrameException;

/**
 * This class manages all details of connection to a particular bookie. It also
 * has reconnect logic if a connection to a bookie fails.
 *
 */

@ChannelPipelineCoverage("one")
public class PerChannelBookieClient extends SimpleChannelHandler implements ChannelPipelineFactory {

    static final Logger LOG = LoggerFactory.getLogger(PerChannelBookieClient.class);

    static final long maxMemory = Runtime.getRuntime().maxMemory() / 5;
    public static int MAX_FRAME_LENGTH = 2 * 1024 * 1024; // 2M

    InetSocketAddress addr;
    Semaphore opCounterSem = new Semaphore(2000);
    AtomicLong totalBytesOutstanding;
    ClientSocketChannelFactory channelFactory;
    OrderedSafeExecutor executor;

    ConcurrentHashMap<CompletionKey, AddCompletion> addCompletions = new ConcurrentHashMap<CompletionKey, AddCompletion>();
    ConcurrentHashMap<CompletionKey, ReadCompletion> readCompletions = new ConcurrentHashMap<CompletionKey, ReadCompletion>();

    /**
     * The following member variables do not need to be concurrent, or volatile
     * because they are always updated under a lock
     */
    Queue<GenericCallback<Void>> pendingOps = new ArrayDeque<GenericCallback<Void>>();
    Channel channel = null;

    private enum ConnectionState {
        DISCONNECTED, CONNECTING, CONNECTED
            };

    private ConnectionState state;
    private final ClientConfiguration conf;

    public PerChannelBookieClient(OrderedSafeExecutor executor, ClientSocketChannelFactory channelFactory,
                                  InetSocketAddress addr, AtomicLong totalBytesOutstanding) {
        this(new ClientConfiguration(), executor, channelFactory, addr, totalBytesOutstanding);
    }
            
    public PerChannelBookieClient(ClientConfiguration conf, OrderedSafeExecutor executor, ClientSocketChannelFactory channelFactory,
                                  InetSocketAddress addr, AtomicLong totalBytesOutstanding) {
        this.conf = conf;
        this.addr = addr;
        this.executor = executor;
        this.totalBytesOutstanding = totalBytesOutstanding;
        this.channelFactory = channelFactory;
        this.state = ConnectionState.DISCONNECTED;
    }

    synchronized private void connect() {
        if (state == ConnectionState.CONNECTING) {
            return;
        } 
        // Start the connection attempt to the input server host.
        state = ConnectionState.CONNECTING;

        if (LOG.isDebugEnabled())
            LOG.debug("Connecting to bookie: " + addr);

        // Set up the ClientBootStrap so we can create a new Channel connection
        // to the bookie.
        ClientBootstrap bootstrap = new ClientBootstrap(channelFactory);
        bootstrap.setPipelineFactory(this);
        bootstrap.setOption("tcpNoDelay", conf.getClientTcpNoDelay());
        bootstrap.setOption("keepAlive", true);

        ChannelFuture future = bootstrap.connect(addr);

        future.addListener(new ChannelFutureListener() {
            @Override
            public void operationComplete(ChannelFuture future) throws Exception {
                int rc;
                Queue<GenericCallback<Void>> oldPendingOps;

                synchronized (PerChannelBookieClient.this) {

                    if (future.isSuccess()) {
                        LOG.info("Successfully connected to bookie: " + addr);
                        rc = BKException.Code.OK;
                        channel = future.getChannel();
                        state = ConnectionState.CONNECTED;
                    } else {
                        LOG.error("Could not connect to bookie: " + addr);
                        rc = BKException.Code.BookieHandleNotAvailableException;
                        channel = null;
                        state = ConnectionState.DISCONNECTED;
                    }

                    PerChannelBookieClient.this.channel = channel;

                    // trick to not do operations under the lock, take the list
                    // of pending ops and assign it to a new variable, while
                    // emptying the pending ops by just assigning it to a new
                    // list
                    oldPendingOps = pendingOps;
                    pendingOps = new ArrayDeque<GenericCallback<Void>>();
                }

                for (GenericCallback<Void> pendingOp : oldPendingOps) {
                    pendingOp.operationComplete(rc, null);
                }
            }
        });
    }

    void connectIfNeededAndDoOp(GenericCallback<Void> op) {
        boolean doOpNow;

        // common case without lock first
        if (channel != null && state == ConnectionState.CONNECTED) {
            doOpNow = true;
        } else {

            synchronized (this) {
                // check again under lock
                if (channel != null && state == ConnectionState.CONNECTED) {
                    doOpNow = true;
                } else {

                    // if reached here, channel is either null (first connection
                    // attempt),
                    // or the channel is disconnected
                    doOpNow = false;

                    // connection attempt is still in progress, queue up this
                    // op. Op will be executed when connection attempt either
                    // fails
                    // or
                    // succeeds
                    pendingOps.add(op);

                    connect();
                }
            }
        }

        if (doOpNow) {
            op.operationComplete(BKException.Code.OK, null);
        }

    }

    /**
     * This method should be called only after connection has been checked for
     * {@link #connectIfNeededAndDoOp(GenericCallback)}
     *
     * @param ledgerId
     * @param masterKey
     * @param entryId
     * @param lastConfirmed
     * @param macCode
     * @param data
     * @param cb
     * @param ctx
     */
    void addEntry(final long ledgerId, byte[] masterKey, final long entryId, ChannelBuffer toSend, WriteCallback cb,
                  Object ctx, final int options) {
        final int entrySize = toSend.readableBytes();

        // if (totalBytesOutstanding.get() > maxMemory) {
        // // TODO: how to throttle, throw an exception, or call the callback?
        // // Maybe this should be done at the layer above?
        // }

        final CompletionKey completionKey = new CompletionKey(ledgerId, entryId);

        addCompletions.put(completionKey, new AddCompletion(cb, entrySize, ctx));

        int totalHeaderSize = 4 // for the length of the packet
                              + 4 // for the type of request
                              + masterKey.length; // for the master key

        ChannelBuffer header = channel.getConfig().getBufferFactory().getBuffer(totalHeaderSize);
        header.writeInt(totalHeaderSize - 4 + entrySize);
        header.writeInt(new PacketHeader(BookieProtocol.CURRENT_PROTOCOL_VERSION, 
                                         BookieProtocol.ADDENTRY, (short)options).toInt());
        header.writeBytes(masterKey);

        ChannelBuffer wrappedBuffer = ChannelBuffers.wrappedBuffer(header, toSend);

        ChannelFuture future = channel.write(wrappedBuffer);
        future.addListener(new ChannelFutureListener() {
            @Override
            public void operationComplete(ChannelFuture future) throws Exception {
                if (future.isSuccess()) {
                    if (LOG.isDebugEnabled()) {
                        LOG.debug("Successfully wrote request for adding entry: " + entryId + " ledger-id: " + ledgerId
                                  + " bookie: " + channel.getRemoteAddress() + " entry length: " + entrySize);
                    }
                    // totalBytesOutstanding.addAndGet(entrySize);
                } else {
                    errorOutAddKey(completionKey);
                }
            }
        });

    }

    public void readEntry(final long ledgerId, final long entryId, ReadEntryCallback cb, Object ctx, final int options) {
        final CompletionKey key = new CompletionKey(ledgerId, entryId);
        readCompletions.put(key, new ReadCompletion(cb, ctx));

        int totalHeaderSize = 4 // for the length of the packet
                              + 4 // for request type
                              + 8 // for ledgerId
                              + 8; // for entryId

        ChannelBuffer tmpEntry = channel.getConfig().getBufferFactory().getBuffer(totalHeaderSize);
        tmpEntry.writeInt(totalHeaderSize - 4);

        tmpEntry.writeInt(new PacketHeader(BookieProtocol.CURRENT_PROTOCOL_VERSION, 
                                           BookieProtocol.READENTRY, (short)options).toInt());
        tmpEntry.writeLong(ledgerId);
        tmpEntry.writeLong(entryId);

        ChannelFuture future = channel.write(tmpEntry);
        future.addListener(new ChannelFutureListener() {
            @Override
            public void operationComplete(ChannelFuture future) throws Exception {
                if (future.isSuccess()) {
                    if (LOG.isDebugEnabled()) {
                        LOG.debug("Successfully wrote request for reading entry: " + entryId + " ledger-id: "
                                  + ledgerId + " bookie: " + channel.getRemoteAddress());
                    }
                } else {
                    errorOutReadKey(key);
                }
            }
        });

    }

    public void close() {
        if (channel != null) {
            channel.close().awaitUninterruptibly();
        }
    }

    void errorOutReadKey(final CompletionKey key) {
        executor.submitOrdered(key.ledgerId, new SafeRunnable() {
            @Override
            public void safeRun() {

                ReadCompletion readCompletion = readCompletions.remove(key);

                if (readCompletion != null) {
                    LOG.error("Could not write  request for reading entry: " + key.entryId + " ledger-id: "
                              + key.ledgerId + " bookie: " + channel.getRemoteAddress());

                    readCompletion.cb.readEntryComplete(BKException.Code.BookieHandleNotAvailableException,
                                                        key.ledgerId, key.entryId, null, readCompletion.ctx);
                }
            }

        });
    }

    void errorOutAddKey(final CompletionKey key) {
        executor.submitOrdered(key.ledgerId, new SafeRunnable() {
            @Override
            public void safeRun() {

                AddCompletion addCompletion = addCompletions.remove(key);

                if (addCompletion != null) {
                    String bAddress = "null";
                    if(channel != null)
                        bAddress = channel.getRemoteAddress().toString();
                    LOG.error("Could not write request for adding entry: " + key.entryId + " ledger-id: "
                              + key.ledgerId + " bookie: " + bAddress);

                    addCompletion.cb.writeComplete(BKException.Code.BookieHandleNotAvailableException, key.ledgerId,
                                                   key.entryId, addr, addCompletion.ctx);
                    LOG.error("Invoked callback method: " + key.entryId);
                }
            }

        });

    }

    /**
     * Errors out pending entries. We call this method from one thread to avoid
     * concurrent executions to QuorumOpMonitor (implements callbacks). It seems
     * simpler to call it from BookieHandle instead of calling directly from
     * here.
     */

    void errorOutOutstandingEntries() {

        // DO NOT rewrite these using Map.Entry iterations. We want to iterate
        // on keys and see if we are successfully able to remove the key from
        // the map. Because the add and the read methods also do the same thing
        // in case they get a write failure on the socket. The one who
        // successfully removes the key from the map is the one responsible for
        // calling the application callback.

        for (CompletionKey key : addCompletions.keySet()) {
            errorOutAddKey(key);
        }

        for (CompletionKey key : readCompletions.keySet()) {
            errorOutReadKey(key);
        }
    }

    /**
     * In the netty pipeline, we need to split packets based on length, so we
     * use the {@link LengthFieldBasedFrameDecoder}. Other than that all actions
     * are carried out in this class, e.g., making sense of received messages,
     * prepending the length to outgoing packets etc.
     */
    @Override
    public ChannelPipeline getPipeline() throws Exception {
        ChannelPipeline pipeline = Channels.pipeline();
        pipeline.addLast("lengthbasedframedecoder", new LengthFieldBasedFrameDecoder(MAX_FRAME_LENGTH, 0, 4, 0, 4));
        pipeline.addLast("mainhandler", this);
        return pipeline;
    }

    /**
     * If our channel has disconnected, we just error out the pending entries
     */
    @Override
    public void channelDisconnected(ChannelHandlerContext ctx, ChannelStateEvent e) throws Exception {
        LOG.info("Disconnected from bookie: " + addr);
        errorOutOutstandingEntries();
        channel.close();

        state = ConnectionState.DISCONNECTED;

        // we don't want to reconnect right away. If someone sends a request to
        // this address, we will reconnect.
    }

    /**
     * Called by netty when an exception happens in one of the netty threads
     * (mostly due to what we do in the netty threads)
     */
    @Override
    public void exceptionCaught(ChannelHandlerContext ctx, ExceptionEvent e) throws Exception {
        Throwable t = e.getCause();
        if (t instanceof CorruptedFrameException || t instanceof TooLongFrameException) {
            LOG.error("Corrupted fram received from bookie: "
                      + e.getChannel().getRemoteAddress());
            return;
        }
        if (t instanceof IOException) {
            // these are thrown when a bookie fails, logging them just pollutes
            // the logs (the failure is logged from the listeners on the write
            // operation), so I'll just ignore it here.
            return;
        }

        LOG.error("Unexpected exception caught by bookie client channel handler", t);
        // Since we are a library, cant terminate App here, can we?
    }

    /**
     * Called by netty when a message is received on a channel
     */
    @Override
    public void messageReceived(ChannelHandlerContext ctx, MessageEvent e) throws Exception {
        if (!(e.getMessage() instanceof ChannelBuffer)) {
            ctx.sendUpstream(e);
            return;
        }

        final ChannelBuffer buffer = (ChannelBuffer) e.getMessage();
        final int type, rc;
        final long ledgerId, entryId;
        final PacketHeader header;

        try {
            header = PacketHeader.fromInt(buffer.readInt());
            rc = buffer.readInt();
            ledgerId = buffer.readLong();
            entryId = buffer.readLong();
        } catch (IndexOutOfBoundsException ex) {
            LOG.error("Unparseable response from bookie: " + addr, ex);
            return;
        }

        executor.submitOrdered(ledgerId, new SafeRunnable() {
            @Override
            public void safeRun() {
                switch (header.getOpCode()) {
                case BookieProtocol.ADDENTRY:
                    handleAddResponse(ledgerId, entryId, rc);
                    break;
                case BookieProtocol.READENTRY:
                    handleReadResponse(ledgerId, entryId, rc, buffer);
                    break;
                default:
                    LOG.error("Unexpected response, type: " + header.getOpCode() 
                              + " received from bookie: " + addr + " , ignoring");
                }
            }
        });
    }

    void handleAddResponse(long ledgerId, long entryId, int rc) {
        if (LOG.isDebugEnabled()) {
            LOG.debug("Got response for add request from bookie: " + addr + " for ledger: " + ledgerId + " entry: "
                      + entryId + " rc: " + rc);
        }

        // convert to BKException code because thats what the uppper
        // layers expect. This is UGLY, there should just be one set of
        // error codes.
        switch (rc) {
        case BookieProtocol.EOK:
            rc = BKException.Code.OK;
            break;
        case BookieProtocol.EBADVERSION:
            rc = BKException.Code.ProtocolVersionException;
            break;
        case BookieProtocol.EFENCED:
            rc = BKException.Code.LedgerFencedException;
            break;
        default: 
            LOG.error("Add for ledger: " + ledgerId + ", entry: " + entryId + " failed on bookie: " + addr
                      + " with code: " + rc);
            rc = BKException.Code.WriteException;
            break;
        }

        AddCompletion ac;
        ac = addCompletions.remove(new CompletionKey(ledgerId, entryId));
        if (ac == null) {
            LOG.error("Unexpected add response received from bookie: " + addr + " for ledger: " + ledgerId
                      + ", entry: " + entryId + " , ignoring");
            return;
        }

        // totalBytesOutstanding.addAndGet(-ac.size);

        ac.cb.writeComplete(rc, ledgerId, entryId, addr, ac.ctx);

    }

    void handleReadResponse(long ledgerId, long entryId, int rc, ChannelBuffer buffer) {
        if (LOG.isDebugEnabled()) {
            LOG.debug("Got response for read request from bookie: " + addr + " for ledger: " + ledgerId + " entry: "
                      + entryId + " rc: " + rc + "entry length: " + buffer.readableBytes());
        }

        // convert to BKException code because thats what the uppper
        // layers expect. This is UGLY, there should just be one set of
        // error codes.
        if (rc == BookieProtocol.EOK) {
            rc = BKException.Code.OK;
        } else if (rc == BookieProtocol.ENOENTRY || rc == BookieProtocol.ENOLEDGER) {
            rc = BKException.Code.NoSuchEntryException;
        } else if (rc == BookieProtocol.EBADVERSION) {
            rc = BKException.Code.ProtocolVersionException;
        } else {
            LOG.error("Read for ledger: " + ledgerId + ", entry: " + entryId + " failed on bookie: " + addr
                      + " with code: " + rc);
            rc = BKException.Code.ReadException;
        }

        CompletionKey key = new CompletionKey(ledgerId, entryId);
        ReadCompletion readCompletion = readCompletions.remove(key);

        if (readCompletion == null) {
            /*
             * This is a special case. When recovering a ledger, a client
             * submits a read request with id -1, and receives a response with a
             * different entry id.
             */
            readCompletion = readCompletions.remove(new CompletionKey(ledgerId, -1));
        }

        if (readCompletion == null) {
            LOG.error("Unexpected read response received from bookie: " + addr + " for ledger: " + ledgerId
                      + ", entry: " + entryId + " , ignoring");
            return;
        }

        readCompletion.cb.readEntryComplete(rc, ledgerId, entryId, buffer.slice(), readCompletion.ctx);
    }

    /**
     * Boiler-plate wrapper classes follow
     *
     */
    // visible for testing
    static class ReadCompletion {
        final ReadEntryCallback cb;
        final Object ctx;

        public ReadCompletion(ReadEntryCallback cb, Object ctx) {
            this.cb = cb;
            this.ctx = ctx;
        }
    }

    // visible for testing
    static class AddCompletion {
        final WriteCallback cb;
        //final long size;
        final Object ctx;

        public AddCompletion(WriteCallback cb, long size, Object ctx) {
            this.cb = cb;
            //this.size = size;
            this.ctx = ctx;
        }
    }
    
    // visable for testing
    static class CompletionKey {
        long ledgerId;
        long entryId;

        CompletionKey(long ledgerId, long entryId) {
            this.ledgerId = ledgerId;
            this.entryId = entryId;
        }

        @Override
        public boolean equals(Object obj) {
            if (!(obj instanceof CompletionKey) || obj == null) {
                return false;
            }
            CompletionKey that = (CompletionKey) obj;
            return this.ledgerId == that.ledgerId && this.entryId == that.entryId;
        }

        @Override
        public int hashCode() {
            return ((int) ledgerId << 16) ^ ((int) entryId);
        }

        public String toString() {
            return String.format("LedgerEntry(%d, %d)", ledgerId, entryId);
        }
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/proto/ServerStats.java,true,"/*
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.bookkeeper.proto;

public class ServerStats {
    private static ServerStats instance = new ServerStats();
    private long packetsSent;
    private long packetsReceived;
    private long maxLatency;
    private long minLatency = Long.MAX_VALUE;
    private long totalLatency = 0;
    private long count = 0;

    public interface Provider {
        public long getOutstandingRequests();

        public long getLastProcessedZxid();
    }

    private Provider provider = null;
    private Object mutex = new Object();

    static public ServerStats getInstance() {
        return instance;
    }

    static public void registerAsConcrete() {
        setInstance(new ServerStats());
    }

    static synchronized public void unregister() {
        instance = null;
    }

    static synchronized protected void setInstance(ServerStats newInstance) {
        assert instance == null;
        instance = newInstance;
    }

    protected ServerStats() {
    }

    // getters
    synchronized public long getMinLatency() {
        return (minLatency == Long.MAX_VALUE) ? 0 : minLatency;
    }

    synchronized public long getAvgLatency() {
        if (count != 0)
            return totalLatency / count;
        return 0;
    }

    synchronized public long getMaxLatency() {
        return maxLatency;
    }

    public long getOutstandingRequests() {
        synchronized (mutex) {
            return (provider != null) ? provider.getOutstandingRequests() : -1;
        }
    }

    public long getLastProcessedZxid() {
        synchronized (mutex) {
            return (provider != null) ? provider.getLastProcessedZxid() : -1;
        }
    }

    synchronized public long getPacketsReceived() {
        return packetsReceived;
    }

    synchronized public long getPacketsSent() {
        return packetsSent;
    }

    public String getServerState() {
        return "standalone";
    }

    @Override
    public String toString() {
        StringBuilder sb = new StringBuilder();
        sb.append("Latency min/avg/max: " + getMinLatency() + "/" + getAvgLatency() + "/" + getMaxLatency() + "\n");
        sb.append("Received: " + getPacketsReceived() + "\n");
        sb.append("Sent: " + getPacketsSent() + "\n");
        if (provider != null) {
            sb.append("Outstanding: " + getOutstandingRequests() + "\n");
            sb.append("Zxid: 0x" + Long.toHexString(getLastProcessedZxid()) + "\n");
        }
        sb.append("Mode: " + getServerState() + "\n");
        return sb.toString();
    }

    // mutators
    public void setStatsProvider(Provider zk) {
        synchronized (mutex) {
            provider = zk;
        }
    }

    synchronized void updateLatency(long requestCreateTime) {
        long latency = System.currentTimeMillis() - requestCreateTime;
        totalLatency += latency;
        count++;
        if (latency < minLatency) {
            minLatency = latency;
        }
        if (latency > maxLatency) {
            maxLatency = latency;
        }
    }

    synchronized public void resetLatency() {
        totalLatency = count = maxLatency = 0;
        minLatency = Long.MAX_VALUE;
    }

    synchronized public void resetMaxLatency() {
        maxLatency = getMinLatency();
    }

    synchronized public void incrementPacketsReceived() {
        packetsReceived++;
    }

    synchronized public void incrementPacketsSent() {
        packetsSent++;
    }

    synchronized public void resetRequestCounters() {
        packetsReceived = packetsSent = 0;
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/streaming/LedgerInputStream.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */
package org.apache.bookkeeper.streaming;

import java.io.IOException;
import java.io.InputStream;
import java.nio.ByteBuffer;
import java.util.Enumeration;

import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.client.LedgerEntry;
import org.apache.bookkeeper.client.LedgerHandle;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class LedgerInputStream extends InputStream {
    Logger LOG = LoggerFactory.getLogger(LedgerInputStream.class);
    private LedgerHandle lh;
    private ByteBuffer bytebuff;
    byte[] bbytes;
    long lastEntry = 0;
    int increment = 50;
    int defaultSize = 1024 * 1024; // 1MB default size
    Enumeration<LedgerEntry> ledgerSeq = null;

    /**
     * construct a outputstream from a ledger handle
     *
     * @param lh
     *            ledger handle
     * @throws {@link BKException}, {@link InterruptedException}
     */
    public LedgerInputStream(LedgerHandle lh) throws BKException, InterruptedException {
        this.lh = lh;
        bbytes = new byte[defaultSize];
        this.bytebuff = ByteBuffer.wrap(bbytes);
        this.bytebuff.position(this.bytebuff.limit());
        lastEntry = Math.min(lh.getLastAddConfirmed(), increment);
        ledgerSeq = lh.readEntries(0, lastEntry);
    }

    /**
     * construct a outputstream from a ledger handle
     *
     * @param lh
     *            the ledger handle
     * @param size
     *            the size of the buffer
     * @throws {@link BKException}, {@link InterruptedException}
     */
    public LedgerInputStream(LedgerHandle lh, int size) throws BKException, InterruptedException {
        this.lh = lh;
        bbytes = new byte[size];
        this.bytebuff = ByteBuffer.wrap(bbytes);
        this.bytebuff.position(this.bytebuff.limit());
        lastEntry = Math.min(lh.getLastAddConfirmed(), increment);
        ledgerSeq = lh.readEntries(0, lastEntry);
    }

    /**
     * Method close currently doesn't do anything. The application
     * is supposed to open and close the ledger handle backing up
     * a stream ({@link LedgerHandle}).
     */
    @Override
    public void close() {
        // do nothing
        // let the application
        // close the ledger
    }

    /**
     * refill the buffer, we need to read more bytes
     *
     * @return if we can refill or not
     */
    private synchronized boolean refill() throws IOException {
        bytebuff.clear();
        if (!ledgerSeq.hasMoreElements() && lastEntry >= lh.getLastAddConfirmed()) {
            return false;
        }
        if (!ledgerSeq.hasMoreElements()) {
            // do refill
            long last = Math.min(lastEntry + increment, lh.getLastAddConfirmed());
            try {
                ledgerSeq = lh.readEntries(lastEntry + 1, last);
            } catch (BKException bk) {
                IOException ie = new IOException(bk.getMessage());
                ie.initCause(bk);
                throw ie;
            } catch (InterruptedException ie) {
                Thread.currentThread().interrupt();
            }
            lastEntry = last;
        }
        LedgerEntry le = ledgerSeq.nextElement();
        bbytes = le.getEntry();
        bytebuff = ByteBuffer.wrap(bbytes);
        return true;
    }

    @Override
    public synchronized int read() throws IOException {
        boolean toread = true;
        if (bytebuff.remaining() == 0) {
            // their are no remaining bytes
            toread = refill();
        }
        if (toread) {
            int ret = 0xFF & bytebuff.get();
            return ret;
        }
        return -1;
    }

    @Override
    public synchronized int read(byte[] b) throws IOException {
        // be smart ... just copy the bytes
        // once and return the size
        // user will call it again
        boolean toread = true;
        if (bytebuff.remaining() == 0) {
            toread = refill();
        }
        if (toread) {
            int bcopied = bytebuff.remaining();
            int tocopy = Math.min(bcopied, b.length);
            // cannot used gets because of
            // the underflow/overflow exceptions
            System.arraycopy(bbytes, bytebuff.position(), b, 0, tocopy);
            bytebuff.position(bytebuff.position() + tocopy);
            return tocopy;
        }
        return -1;
    }

    @Override
    public synchronized int read(byte[] b, int off, int len) throws IOException {
        // again dont need ot fully
        // fill b, just return
        // what we have and let the application call read
        // again
        boolean toread = true;
        if (bytebuff.remaining() == 0) {
            toread = refill();
        }
        if (toread) {
            int bcopied = bytebuff.remaining();
            int tocopy = Math.min(bcopied, len);
            System.arraycopy(bbytes, bytebuff.position(), b, off, tocopy);
            bytebuff.position(bytebuff.position() + tocopy);
            return tocopy;
        }
        return -1;
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/streaming/LedgerOutputStream.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */
package org.apache.bookkeeper.streaming;

import java.io.IOException;
import java.io.OutputStream;
import java.nio.ByteBuffer;

import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.client.LedgerHandle;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * this class provides a streaming api to get an output stream from a ledger
 * handle and write to it as a stream of bytes. This is built on top of
 * ledgerhandle api and uses a buffer to cache the data written to it and writes
 * out the entry to the ledger.
 */
public class LedgerOutputStream extends OutputStream {
    Logger LOG = LoggerFactory.getLogger(LedgerOutputStream.class);
    private LedgerHandle lh;
    private ByteBuffer bytebuff;
    byte[] bbytes;
    int defaultSize = 1024 * 1024; // 1MB default size

    /**
     * construct a outputstream from a ledger handle
     *
     * @param lh
     *            ledger handle
     */
    public LedgerOutputStream(LedgerHandle lh) {
        this.lh = lh;
        bbytes = new byte[defaultSize];
        this.bytebuff = ByteBuffer.wrap(bbytes);
    }

    /**
     * construct a outputstream from a ledger handle
     *
     * @param lh
     *            the ledger handle
     * @param size
     *            the size of the buffer
     */
    public LedgerOutputStream(LedgerHandle lh, int size) {
        this.lh = lh;
        bbytes = new byte[size];
        this.bytebuff = ByteBuffer.wrap(bbytes);
    }

    @Override
    public void close() {
        // flush everything
        // we have
        flush();
    }

    @Override
    public synchronized void flush() {
        // lets flush all the data
        // into the ledger entry
        if (bytebuff.position() > 0) {
            // copy the bytes into
            // a new byte buffer and send it out
            byte[] b = new byte[bytebuff.position()];
            LOG.info("Comment: flushing with params " + " " + bytebuff.position());
            System.arraycopy(bbytes, 0, b, 0, bytebuff.position());
            try {
                lh.addEntry(b);
            } catch (InterruptedException ie) {
                LOG.warn("Interrupted while flusing " + ie);
                Thread.currentThread().interrupt();
            } catch (BKException bke) {
                LOG.warn("BookKeeper exception ", bke);
            }
        }
    }

    /**
     * make space for len bytes to be written to the buffer.
     *
     * @param len
     * @return if true then we can make space for len if false we cannot
     */
    private boolean makeSpace(int len) {
        if (bytebuff.remaining() < len) {
            flush();
            bytebuff.clear();
            if (bytebuff.capacity() < len) {
                return false;
            }
        }
        return true;
    }

    @Override
    public synchronized void write(byte[] b) {
        if (makeSpace(b.length)) {
            bytebuff.put(b);
        } else {
            try {
                lh.addEntry(b);
            } catch (InterruptedException ie) {
                LOG.warn("Interrupted while writing", ie);
                Thread.currentThread().interrupt();
            } catch (BKException bke) {
                LOG.warn("BookKeeper exception", bke);
            }
        }
    }

    @Override
    public synchronized void write(byte[] b, int off, int len) {
        if (!makeSpace(len)) {
            // lets try making the buffer bigger
            bbytes = new byte[len];
            bytebuff = ByteBuffer.wrap(bbytes);
        }
        bytebuff.put(b, off, len);
    }

    @Override
    public synchronized void write(int b) throws IOException {
        makeSpace(1);
        byte oneB = (byte) (b & 0xFF);
        bytebuff.put(oneB);
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/tools/BookKeeperTools.java,true,"package org.apache.bookkeeper.tools;

/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

import java.io.IOException;
import org.apache.zookeeper.KeeperException;
import java.net.InetSocketAddress;

import org.apache.bookkeeper.client.BookKeeperAdmin;
import org.apache.bookkeeper.client.BKException;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Provides Admin Tools to manage the BookKeeper cluster.
 *
 */
public class BookKeeperTools {
    private static Logger LOG = LoggerFactory.getLogger(BookKeeperTools.class);

    /**
     * Main method so we can invoke the bookie recovery via command line.
     *
     * @param args
     *            Arguments to BookKeeperTools. 2 are required and the third is
     *            optional. The first is a comma separated list of ZK server
     *            host:port pairs. The second is the host:port socket address
     *            for the bookie we are trying to recover. The third is the
     *            host:port socket address of the optional destination bookie
     *            server we want to replicate the data over to.
     * @throws InterruptedException
     * @throws IOException
     * @throws KeeperException
     * @throws BKException
     */
    public static void main(String[] args) 
            throws InterruptedException, IOException, KeeperException, BKException {
        // Validate the inputs
        if (args.length < 2) {
            System.err.println("USAGE: BookKeeperTools zkServers bookieSrc [bookieDest]");
            return;
        }
        // Parse out the input arguments
        String zkServers = args[0];
        String bookieSrcString[] = args[1].split(":");
        if (bookieSrcString.length < 2) {
            System.err.println("BookieSrc inputted has invalid name format (host:port expected): " + bookieSrcString);
            return;
        }
        final InetSocketAddress bookieSrc = new InetSocketAddress(bookieSrcString[0], Integer
                .parseInt(bookieSrcString[1]));
        InetSocketAddress bookieDest = null;
        if (args.length < 3) {
            String bookieDestString[] = args[2].split(":");
            if (bookieDestString.length < 2) {
                System.err.println("BookieDest inputted has invalid name format (host:port expected): "
                                   + bookieDestString);
                return;
            }
            bookieDest = new InetSocketAddress(bookieDestString[0], Integer.parseInt(bookieDestString[1]));
        }

        // Create the BookKeeperTools instance and perform the bookie recovery
        // synchronously.
        BookKeeperAdmin bkTools = new BookKeeperAdmin(zkServers);
        bkTools.recoverBookieData(bookieSrc, bookieDest);

        // Shutdown the resources used in the BookKeeperTools instance.
        bkTools.close();
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/util/LocalBookKeeper.java,true,"package org.apache.bookkeeper.util;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.BufferedReader;
import java.io.File;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.OutputStream;
import java.net.InetAddress;
import java.net.InetSocketAddress;
import java.net.Socket;

import org.apache.bookkeeper.conf.ServerConfiguration;
import org.apache.bookkeeper.proto.BookieServer;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.WatchedEvent;
import org.apache.zookeeper.Watcher;
import org.apache.zookeeper.ZooKeeper;
import org.apache.zookeeper.ZooDefs.Ids;
import org.apache.zookeeper.server.NIOServerCnxnFactory;
import org.apache.zookeeper.server.ZooKeeperServer;

public class LocalBookKeeper {
    protected static final Logger LOG = LoggerFactory.getLogger(LocalBookKeeper.class);
    public static final int CONNECTION_TIMEOUT = 30000;

    int numberOfBookies;

    public LocalBookKeeper() {
        numberOfBookies = 3;
    }

    public LocalBookKeeper(int numberOfBookies) {
        this();
        this.numberOfBookies = numberOfBookies;
        LOG.info("Running " + this.numberOfBookies + " bookie(s).");
    }

    private final String HOSTPORT = "127.0.0.1:2181";
    NIOServerCnxnFactory serverFactory;
    ZooKeeperServer zks;
    ZooKeeper zkc;
    int ZooKeeperDefaultPort = 2181;
    File ZkTmpDir;

    //BookKeeper variables
    File tmpDirs[];
    BookieServer bs[];
    ServerConfiguration bsConfs[];
    Integer initialPort = 5000;

    /**
     * @param args
     */

    private void runZookeeper(int maxCC) throws IOException {
        // create a ZooKeeper server(dataDir, dataLogDir, port)
        LOG.info("Starting ZK server");
        //ServerStats.registerAsConcrete();
        //ClientBase.setupTestEnv();
        ZkTmpDir = File.createTempFile("zookeeper", "test");
        ZkTmpDir.delete();
        ZkTmpDir.mkdir();

        try {
            zks = new ZooKeeperServer(ZkTmpDir, ZkTmpDir, ZooKeeperDefaultPort);
            serverFactory =  new NIOServerCnxnFactory();
            serverFactory.configure(new InetSocketAddress(ZooKeeperDefaultPort), maxCC);
            serverFactory.startup(zks);
        } catch (Exception e) {
            // TODO Auto-generated catch block
            LOG.error("Exception while instantiating ZooKeeper", e);
        }

        boolean b = waitForServerUp(HOSTPORT, CONNECTION_TIMEOUT);
        LOG.debug("ZooKeeper server up: " + b);
    }

    private void initializeZookeper() {
        LOG.info("Instantiate ZK Client");
        //initialize the zk client with values
        try {
            zkc = new ZooKeeper("127.0.0.1", ZooKeeperDefaultPort, new emptyWatcher());
            zkc.create("/ledgers", new byte[0], Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
            zkc.create("/ledgers/available", new byte[0], Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
            // No need to create an entry for each requested bookie anymore as the
            // BookieServers will register themselves with ZooKeeper on startup.
        } catch (KeeperException e) {
            // TODO Auto-generated catch block
            LOG.error("Exception while creating znodes", e);
        } catch (InterruptedException e) {
            // TODO Auto-generated catch block
            LOG.error("Interrupted while creating znodes", e);
        } catch (IOException e) {
            // TODO Auto-generated catch block
            LOG.error("Exception while creating znodes", e);
        }
    }
    private void runBookies(ServerConfiguration baseConf) 
            throws IOException, KeeperException, InterruptedException {
        LOG.info("Starting Bookie(s)");
        // Create Bookie Servers (B1, B2, B3)

        tmpDirs = new File[numberOfBookies];
        bs = new BookieServer[numberOfBookies];
        bsConfs = new ServerConfiguration[numberOfBookies];

        for(int i = 0; i < numberOfBookies; i++) {
            tmpDirs[i] = File.createTempFile("bookie" + Integer.toString(i), "test");
            tmpDirs[i].delete();
            tmpDirs[i].mkdir();

            bsConfs[i] = new ServerConfiguration(baseConf);
            // override settings
            bsConfs[i].setBookiePort(initialPort + i);
            bsConfs[i].setZkServers(InetAddress.getLocalHost().getHostAddress() + ":"
                                  + ZooKeeperDefaultPort);
            bsConfs[i].setJournalDirName(tmpDirs[i].getPath());
            bsConfs[i].setLedgerDirNames(new String[] { tmpDirs[i].getPath() });

            bs[i] = new BookieServer(bsConfs[i]);
            bs[i].start();
        }
    }

    public static void main(String[] args)
            throws IOException, KeeperException, InterruptedException {
        if(args.length < 1) {
            usage();
            System.exit(-1);
        }
        LocalBookKeeper lb = new LocalBookKeeper(Integer.parseInt(args[0]));

        ServerConfiguration conf = new ServerConfiguration();
        if (args.length >= 2) {
            String confFile = args[1];
            try {
                conf.loadConf(new File(confFile).toURI().toURL());
                LOG.info("Using configuration file " + confFile);
            } catch (Exception e) {
                // load conf failed
                LOG.warn("Error loading configuration file " + confFile, e);
            }
        }

        lb.runZookeeper(1000);
        lb.initializeZookeper();
        lb.runBookies(conf);
        while (true) {
            Thread.sleep(5000);
        }
    }

    private static void usage() {
        System.err.println("Usage: LocalBookKeeper number-of-bookies");
    }

    /*	User for testing purposes, void */
    class emptyWatcher implements Watcher {
        public void process(WatchedEvent event) {}
    }

    public static boolean waitForServerUp(String hp, long timeout) {
        long start = System.currentTimeMillis();
        String split[] = hp.split(":");
        String host = split[0];
        int port = Integer.parseInt(split[1]);
        while (true) {
            try {
                Socket sock = new Socket(host, port);
                BufferedReader reader = null;
                try {
                    OutputStream outstream = sock.getOutputStream();
                    outstream.write("stat".getBytes());
                    outstream.flush();

                    reader =
                        new BufferedReader(
                        new InputStreamReader(sock.getInputStream()));
                    String line = reader.readLine();
                    if (line != null && line.startsWith("Zookeeper version:")) {
                        LOG.info("Server UP");
                        return true;
                    }
                } finally {
                    sock.close();
                    if (reader != null) {
                        reader.close();
                    }
                }
            } catch (IOException e) {
                // ignore as this is expected
                LOG.info("server " + hp + " not up " + e);
            }

            if (System.currentTimeMillis() > start + timeout) {
                break;
            }
            try {
                Thread.sleep(250);
            } catch (InterruptedException e) {
                // ignore
            }
        }
        return false;
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/util/Main.java,true,"package org.apache.bookkeeper.util;

/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

import java.io.IOException;

import org.apache.bookkeeper.proto.BookieClient;
import org.apache.bookkeeper.proto.BookieServer;

public class Main {

    static void usage() {
        System.err.println("USAGE: bookeeper client|bookie");
    }

    /**
     * @param args
     * @throws InterruptedException
     * @throws IOException
     */
    public static void main(String[] args) throws Exception {
        if (args.length < 1 || !(args[0].equals("client") || args[0].equals("bookie"))) {
            usage();
            return;
        }
        String newArgs[] = new String[args.length - 1];
        System.arraycopy(args, 1, newArgs, 0, newArgs.length);
        if (args[0].equals("bookie")) {
            BookieServer.main(newArgs);
        } else {
            BookieClient.main(newArgs);
        }
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/util/MathUtils.java,true,"package org.apache.bookkeeper.util;


/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * Provides misc math functions that dont come standard
 */
public class MathUtils {

    public static int signSafeMod(long dividend, int divisor) {
        int mod = (int) (dividend % divisor);

        if (mod < 0) {
            mod += divisor;
        }

        return mod;

    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/util/OrderedSafeExecutor.java,true,"package org.apache.bookkeeper.util;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.util.Random;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;

/**
 * This class provides 2 things over the java {@link ScheduledExecutorService}.
 *
 * 1. It takes {@link SafeRunnable objects} instead of plain Runnable objects.
 * This means that exceptions in scheduled tasks wont go unnoticed and will be
 * logged.
 *
 * 2. It supports submitting tasks with an ordering key, so that tasks submitted
 * with the same key will always be executed in order, but tasks across
 * different keys can be unordered. This retains parallelism while retaining the
 * basic amount of ordering we want (e.g. , per ledger handle). Ordering is
 * achieved by hashing the key objects to threads by their {@link #hashCode()}
 * method.
 *
 */
public class OrderedSafeExecutor {
    ExecutorService threads[];
    Random rand = new Random();

    public OrderedSafeExecutor(int numThreads) {
        if (numThreads <= 0) {
            throw new IllegalArgumentException();
        }

        threads = new ExecutorService[numThreads];
        for (int i = 0; i < numThreads; i++) {
            threads[i] = Executors.newSingleThreadExecutor();
        }
    }

    ExecutorService chooseThread() {
        // skip random # generation in this special case
        if (threads.length == 1) {
            return threads[0];
        }

        return threads[rand.nextInt(threads.length)];

    }

    ExecutorService chooseThread(Object orderingKey) {
        // skip hashcode generation in this special case
        if (threads.length == 1) {
            return threads[0];
        }

        return threads[MathUtils.signSafeMod(orderingKey.hashCode(), threads.length)];

    }

    /**
     * schedules a one time action to execute
     */
    public void submit(SafeRunnable r) {
        chooseThread().submit(r);
    }

    /**
     * schedules a one time action to execute with an ordering guarantee on the key
     * @param orderingKey
     * @param r
     */
    public void submitOrdered(Object orderingKey, SafeRunnable r) {
        chooseThread(orderingKey).submit(r);
    }

    public void shutdown() {
        for (int i = 0; i < threads.length; i++) {
            threads[i].shutdown();
        }
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/util/SafeRunnable.java,false,"package org.apache.bookkeeper.util;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public abstract class SafeRunnable implements Runnable {

    static final Logger logger = LoggerFactory.getLogger(SafeRunnable.class);

    @Override
    public void run() {
        try {
            safeRun();
        } catch(Throwable t) {
            logger.error("Unexpected throwable caught ", t);
        }
    }

    public abstract void safeRun();

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/util/StringUtils.java,true,"package org.apache.bookkeeper.util;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.IOException;
import java.net.InetSocketAddress;

/**
 * Provided utilites for parsing network addresses, ledger-id from node paths
 * etc.
 *
 */
public class StringUtils {

    /**
     * Parses address into IP and port.
     *
     * @param addr
     *            String
     */

    public static InetSocketAddress parseAddr(String s) throws IOException {

        String parts[] = s.split(":");
        if (parts.length != 2) {
            throw new IOException(s + " does not have the form host:port");
        }
        int port;
        try {
            port = Integer.parseInt(parts[1]);
        } catch (NumberFormatException e) {
            throw new IOException(s + " does not have the form host:port");
        }

        InetSocketAddress addr = new InetSocketAddress(parts[0], port);
        return addr;
    }

    public static StringBuilder addrToString(StringBuilder sb, InetSocketAddress addr) {
        return sb.append(addr.getAddress().getHostAddress()).append(":").append(addr.getPort());
    }

    /**
     * Formats ledger ID according to ZooKeeper rules
     *
     * @param id
     *            znode id
     */
    public static String getZKStringId(long id) {
        return String.format("%010d", id);
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/util/ZkUtils.java,true,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.util;

import java.io.File;
import java.util.List;

import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.AsyncCallback;
import org.apache.zookeeper.AsyncCallback.StringCallback;
import org.apache.zookeeper.KeeperException.Code;
import org.apache.zookeeper.data.ACL;
import org.apache.zookeeper.ZooKeeper;

/**
 * Provided utilites for zookeeper access, etc.
 */
public class ZkUtils {

    /**
     * Create zookeeper path recursively
     *
     * @param zk
     *          Zookeeper client
     * @param originalPath
     *          Zookeeper full path
     * @param data
     *          Zookeeper data
     * @param acl
     *          Acl of the zk path
     * @param createMode
     *          Create mode of zk path
     * @param callback
     *          Callback
     * @param ctx
     *          Context object
     */
    public static void createFullPathOptimistic(
        final ZooKeeper zk, final String originalPath, final byte[] data,
        final List<ACL> acl, final CreateMode createMode,
        final AsyncCallback.StringCallback callback, final Object ctx) {

        zk.create(originalPath, data, acl, createMode, new StringCallback() {
            @Override
            public void processResult(int rc, String path, Object ctx, String name) {

                if (rc != Code.NONODE.intValue()) {
                    callback.processResult(rc, path, ctx, name);
                    return;
                }

                // Since I got a nonode, it means that my parents don't exist
                // create mode is persistent since ephemeral nodes can't be
                // parents
                createFullPathOptimistic(zk, new File(originalPath).getParent().replace("\\", "/"), new byte[0], acl,
                        CreateMode.PERSISTENT, new StringCallback() {

                            @Override
                            public void processResult(int rc, String path, Object ctx, String name) {
                                if (rc == Code.OK.intValue() || rc == Code.NODEEXISTS.intValue()) {
                                    // succeeded in creating the parent, now
                                    // create the original path
                                    createFullPathOptimistic(zk, originalPath, data, acl, createMode, callback,
                                            ctx);
                                } else {
                                    callback.processResult(rc, path, ctx, name);
                                }
                            }
                        }, ctx);
            }
        }, ctx);

    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/HedwigClient.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client;

import org.apache.hedwig.client.api.Client;
import org.apache.hedwig.client.api.Publisher;
import org.apache.hedwig.client.api.Subscriber;
import org.apache.hedwig.client.netty.HedwigClientImpl;

import org.apache.hedwig.client.conf.ClientConfiguration;
import org.jboss.netty.channel.ChannelFactory;

/**
 * Hedwig client uses as starting point for all communications with the Hedwig service.
 * 
 * @see Publisher
 * @see Subscriber
 */
public class HedwigClient implements Client {
    private final Client impl;

    /**
     * Construct a hedwig client object. The configuration object
     * should be an instance of a class which implements ClientConfiguration.
     *
     * @param cfg The client configuration.
     */
    public HedwigClient(ClientConfiguration cfg) {
        impl = HedwigClientImpl.create(cfg);
    }

    /**
     * Construct a hedwig client object, using a preexisting socket factory.
     * This is useful if you need to create many hedwig client instances.
     *
     * @param cfg The client configuration
     * @param socketFactory A netty socket factory.
     */
    public HedwigClient(ClientConfiguration cfg, ChannelFactory socketFactory) {
        impl = HedwigClientImpl.create(cfg, socketFactory);
    }

    @Override
    public Publisher getPublisher() {
        return impl.getPublisher();
    }

    @Override
    public Subscriber getSubscriber() {
        return impl.getSubscriber();
    }

    @Override
    public void close() {
        impl.close();
    }
}"
hedwig-client/src/main/java/org/apache/hedwig/client/api/Client.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.api;

/**
 * Interface defining the client API for Hedwig
 */
public interface Client {
    /**
     * Retrieve the Publisher object for the client.
     * This object can be used to publish messages to a topic on Hedwig.
     * @see Publisher
     */
    public Publisher getPublisher();
    
    /**
     * Retrieve the Subscriber object for the client.
     * This object can be used to subscribe for messages from a topic.
     * @see Subscriber
     */
    public Subscriber getSubscriber();

    /**
     * Close the client and free all associated resources.
     */
    public void close();
}"
hedwig-client/src/main/java/org/apache/hedwig/client/api/MessageHandler.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.api;

import com.google.protobuf.ByteString;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.util.Callback;

/**
 * Interface to define the client handler logic to deliver messages it is
 * subscribed to.
 *
 */
public interface MessageHandler {

    /**
     * Delivers a message which has been published for topic. 
     *
     * @param topic
     *            The topic name where the message came from.
     * @param subscriberId
     *            ID of the subscriber.
     * @param msg
     *            The message object to deliver.
     * @param callback
     *            Callback to invoke when the message delivery has been done.
     * @param context
     *            Calling context that the Callback needs since this is done
     *            asynchronously.
     */
    public void deliver(ByteString topic, ByteString subscriberId, Message msg, Callback<Void> callback, Object context);

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/api/Publisher.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.api;

import com.google.protobuf.ByteString;
import org.apache.hedwig.exceptions.PubSubException.CouldNotConnectException;
import org.apache.hedwig.exceptions.PubSubException.ServiceDownException;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.util.Callback;

/**
 * Interface to define the client Publisher API.
 *
 */
public interface Publisher {

    /**
     * Publishes a message on the given topic.
     *
     * @param topic
     *            Topic name to publish on
     * @param msg
     *            Message object to serialize and publish
     * @throws CouldNotConnectException
     *             If we are not able to connect to the server host
     * @throws ServiceDownException
     *             If we are unable to publish the message to the topic.
     */
    public void publish(ByteString topic, Message msg) throws CouldNotConnectException, ServiceDownException;

    /**
     * Publishes a message asynchronously on the given topic.
     *
     * @param topic
     *            Topic name to publish on
     * @param msg
     *            Message object to serialize and publish
     * @param callback
     *            Callback to invoke when the publish to the server has actually
     *            gone through. This will have to deal with error conditions on
     *            the async publish request.
     * @param context
     *            Calling context that the Callback needs since this is done
     *            asynchronously.
     */
    public void asyncPublish(ByteString topic, Message msg, Callback<Void> callback, Object context);

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/api/Subscriber.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.api;

import java.util.List;

import com.google.protobuf.ByteString;
import org.apache.hedwig.client.exceptions.InvalidSubscriberIdException;
import org.apache.hedwig.exceptions.PubSubException.ClientAlreadySubscribedException;
import org.apache.hedwig.exceptions.PubSubException.ClientNotSubscribedException;
import org.apache.hedwig.exceptions.PubSubException.CouldNotConnectException;
import org.apache.hedwig.exceptions.PubSubException.ServiceDownException;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach;
import org.apache.hedwig.util.Callback;

/**
 * Interface to define the client Subscriber API.
 *
 */
public interface Subscriber {

    /**
     * Subscribe to the given topic for the inputted subscriberId.
     *
     * @param topic
     *            Topic name of the subscription
     * @param subscriberId
     *            ID of the subscriber
     * @param mode
     *            Whether to prohibit, tolerate, or require an existing
     *            subscription.
     * @throws CouldNotConnectException
     *             If we are not able to connect to the server host
     * @throws ClientAlreadySubscribedException
     *             If client is already subscribed to the topic
     * @throws ServiceDownException
     *             If unable to subscribe to topic
     * @throws InvalidSubscriberIdException
     *             If the subscriberId is not valid. We may want to set aside
     *             certain formats of subscriberId's for different purposes.
     *             e.g. local vs. hub subscriber
     */
    public void subscribe(ByteString topic, ByteString subscriberId, CreateOrAttach mode)
            throws CouldNotConnectException, ClientAlreadySubscribedException, ServiceDownException,
        InvalidSubscriberIdException;

    /**
     * Subscribe to the given topic asynchronously for the inputted subscriberId
     * disregarding if the topic has been created yet or not.
     *
     * @param topic
     *            Topic name of the subscription
     * @param subscriberId
     *            ID of the subscriber
     * @param mode
     *            Whether to prohibit, tolerate, or require an existing
     *            subscription.
     * @param callback
     *            Callback to invoke when the subscribe request to the server
     *            has actually gone through. This will have to deal with error
     *            conditions on the async subscribe request.
     * @param context
     *            Calling context that the Callback needs since this is done
     *            asynchronously.
     */
    public void asyncSubscribe(ByteString topic, ByteString subscriberId, CreateOrAttach mode, Callback<Void> callback,
                               Object context);

    /**
     * Unsubscribe from a topic that the subscriberId user has previously
     * subscribed to.
     *
     * @param topic
     *            Topic name of the subscription
     * @param subscriberId
     *            ID of the subscriber
     * @throws CouldNotConnectException
     *             If we are not able to connect to the server host
     * @throws ClientNotSubscribedException
     *             If the client is not currently subscribed to the topic
     * @throws ServiceDownException
     *             If the server was down and unable to complete the request
     * @throws InvalidSubscriberIdException
     *             If the subscriberId is not valid. We may want to set aside
     *             certain formats of subscriberId's for different purposes.
     *             e.g. local vs. hub subscriber
     */
    public void unsubscribe(ByteString topic, ByteString subscriberId) throws CouldNotConnectException,
        ClientNotSubscribedException, ServiceDownException, InvalidSubscriberIdException;

    /**
     * Unsubscribe from a topic asynchronously that the subscriberId user has
     * previously subscribed to.
     *
     * @param topic
     *            Topic name of the subscription
     * @param subscriberId
     *            ID of the subscriber
     * @param callback
     *            Callback to invoke when the unsubscribe request to the server
     *            has actually gone through. This will have to deal with error
     *            conditions on the async unsubscribe request.
     * @param context
     *            Calling context that the Callback needs since this is done
     *            asynchronously.
     */
    public void asyncUnsubscribe(ByteString topic, ByteString subscriberId, Callback<Void> callback, Object context);

    /**
     * Manually send a consume message to the server for the given inputs.
     *
     * @param topic
     *            Topic name of the subscription
     * @param subscriberId
     *            ID of the subscriber
     * @param messageSeqId
     *            Message Sequence ID for the latest message that the client app
     *            has successfully consumed. All messages up to that point will
     *            also be considered as consumed.
     * @throws ClientNotSubscribedException
     *             If the client is not currently subscribed to the topic based
     *             on the client's local state.
     */
    public void consume(ByteString topic, ByteString subscriberId, MessageSeqId messageSeqId)
            throws ClientNotSubscribedException;

    /**
     * Checks if the subscriberId client is currently subscribed to the given
     * topic.
     *
     * @param topic
     *            Topic name of the subscription.
     * @param subscriberId
     *            ID of the subscriber
     * @throws CouldNotConnectException
     *             If we are not able to connect to the server host
     * @throws ServiceDownException
     *             If there is an error checking the server if the client has a
     *             subscription
     * @return Boolean indicating if the client has a subscription or not.
     */
    public boolean hasSubscription(ByteString topic, ByteString subscriberId) throws CouldNotConnectException,
        ServiceDownException;

    /**
     * Fills the input List with the subscriptions this subscriberId client is
     * subscribed to.
     *
     * @param subscriberId
     *            ID of the subscriber
     * @return List filled with subscription name (topic) strings.
     * @throws CouldNotConnectException
     *             If we are not able to connect to the server host
     * @throws ServiceDownException
     *             If there is an error retrieving the list of topics
     */
    public List<ByteString> getSubscriptionList(ByteString subscriberId) throws CouldNotConnectException,
        ServiceDownException;

    /**
     * Begin delivery of messages from the server to us for this topic and
     * subscriberId.
     *
     * @param topic
     *            Topic name of the subscription
     * @param subscriberId
     *            ID of the subscriber
     * @param messageHandler
     *            Message Handler that will consume the subscribed messages
     * @throws ClientNotSubscribedException
     *             If the client is not currently subscribed to the topic
     */
    public void startDelivery(ByteString topic, ByteString subscriberId, MessageHandler messageHandler)
            throws ClientNotSubscribedException;

    /**
     * Stop delivery of messages for this topic and subscriberId.
     *
     * @param topic
     *            Topic name of the subscription
     * @param subscriberId
     *            ID of the subscriber
     * @throws ClientNotSubscribedException
     *             If the client is not currently subscribed to the topic
     */
    public void stopDelivery(ByteString topic, ByteString subscriberId) throws ClientNotSubscribedException;

    /**
     * Closes all of the client side cached data for this subscription without
     * actually sending an unsubscribe request to the server. This will close
     * the subscribe channel synchronously (if it exists) for the topic.
     *
     * @param topic
     *            Topic name of the subscription
     * @param subscriberId
     *            ID of the subscriber
     * @throws ServiceDownException
     *             If the subscribe channel was not able to be closed
     *             successfully
     */
    public void closeSubscription(ByteString topic, ByteString subscriberId) throws ServiceDownException;

    /**
     * Closes all of the client side cached data for this subscription without
     * actually sending an unsubscribe request to the server. This will close
     * the subscribe channel asynchronously (if it exists) for the topic.
     *
     * @param topic
     *            Topic name of the subscription
     * @param subscriberId
     *            ID of the subscriber
     * @param callback
     *            Callback to invoke when the subscribe channel has been closed.
     * @param context
     *            Calling context that the Callback needs since this is done
     *            asynchronously.
     */
    public void asyncCloseSubscription(ByteString topic, ByteString subscriberId, Callback<Void> callback,
                                       Object context);

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/benchmark/BenchmarkPublisher.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.benchmark;

import com.google.protobuf.ByteString;
import org.apache.hedwig.client.api.MessageHandler;
import org.apache.hedwig.client.api.Publisher;
import org.apache.hedwig.client.api.Subscriber;
import org.apache.hedwig.client.benchmark.BenchmarkUtils.BenchmarkCallback;
import org.apache.hedwig.client.benchmark.BenchmarkUtils.ThroughputLatencyAggregator;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach;
import org.apache.hedwig.util.Callback;

public class BenchmarkPublisher extends BenchmarkWorker {
    Publisher publisher;
    Subscriber subscriber;
    int msgSize;
    int nParallel;
    double rate;

    public BenchmarkPublisher(int numTopics, int numMessages, int numRegions, int startTopicLabel, int partitionIndex,
                              int numPartitions, Publisher publisher, Subscriber subscriber, int msgSize, int nParallel, int rate) {
        super(numTopics, numMessages, numRegions, startTopicLabel, partitionIndex, numPartitions);
        this.publisher = publisher;
        this.msgSize = msgSize;
        this.subscriber = subscriber;
        this.nParallel = nParallel;

        this.rate = rate / (numRegions * numPartitions + 0.0);
    }

    public void warmup(int nWarmup) throws Exception {
        ByteString topic = ByteString.copyFromUtf8("warmup" + partitionIndex);
        ByteString subId = ByteString.copyFromUtf8("sub");
        subscriber.subscribe(topic, subId, CreateOrAttach.CREATE_OR_ATTACH);

        subscriber.startDelivery(topic, subId, new MessageHandler() {
            @Override
            public void deliver(ByteString topic, ByteString subscriberId, Message msg, Callback<Void> callback,
            Object context) {
                // noop
                callback.operationFinished(context, null);
            }
        });

        // picking constants arbitarily for warmup phase
        ThroughputLatencyAggregator agg = new ThroughputLatencyAggregator("acked pubs", nWarmup, 100);
        Message msg = getMsg(1024);
        for (int i = 0; i < nWarmup; i++) {
            publisher.asyncPublish(topic, msg, new BenchmarkCallback(agg), null);
        }

        if (agg.tpAgg.queue.take() > 0) {
            throw new RuntimeException("Warmup publishes failed!");
        }

    }

    public Message getMsg(int size) {
        StringBuilder sb = new StringBuilder();
        for (int i = 0; i < size; i++) {
            sb.append('a');
        }
        final ByteString body = ByteString.copyFromUtf8(sb.toString());
        Message msg = Message.newBuilder().setBody(body).build();
        return msg;
    }

    public Void call() throws Exception {
        Message msg = getMsg(msgSize);

        // Single warmup for every topic
        int myPublishCount = 0;
        for (int i = 0; i < numTopics; i++) {
            if (!HedwigBenchmark.amIResponsibleForTopic(startTopicLabel + i, partitionIndex, numPartitions)) {
                continue;
            }
            ByteString topic = ByteString.copyFromUtf8(HedwigBenchmark.TOPIC_PREFIX + (startTopicLabel + i));
            publisher.publish(topic, msg);
            myPublishCount++;
        }

        long startTime = System.currentTimeMillis();
        int myPublishLimit = numMessages / numRegions / numPartitions - myPublishCount;
        myPublishCount = 0;
        ThroughputLatencyAggregator agg = new ThroughputLatencyAggregator("acked pubs", myPublishLimit, nParallel);

        int topicLabel = 0;

        while (myPublishCount < myPublishLimit) {
            int topicNum = startTopicLabel + topicLabel;
            topicLabel = (topicLabel + 1) % numTopics;

            if (!HedwigBenchmark.amIResponsibleForTopic(topicNum, partitionIndex, numPartitions)) {
                continue;
            }

            ByteString topic = ByteString.copyFromUtf8(HedwigBenchmark.TOPIC_PREFIX + topicNum);

            if (rate > 0) {
                long delay = startTime + (long) (1000 * myPublishCount / rate) - System.currentTimeMillis();
                if (delay > 0)
                    Thread.sleep(delay);
            }
            publisher.asyncPublish(topic, msg, new BenchmarkCallback(agg), null);
            myPublishCount++;
        }

        System.out.println("Finished unacked pubs: tput = " + BenchmarkUtils.calcTp(myPublishLimit, startTime)
                           + " ops/s");
        // Wait till the benchmark test has completed
        agg.tpAgg.queue.take();
        System.out.println(agg.summarize(startTime));
        return null;
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/benchmark/BenchmarkSubscriber.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.benchmark;

import java.util.HashMap;
import java.util.Map;
import java.util.concurrent.Callable;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.protobuf.ByteString;
import org.apache.hedwig.client.api.MessageHandler;
import org.apache.hedwig.client.api.Subscriber;
import org.apache.hedwig.client.benchmark.BenchmarkUtils.BenchmarkCallback;
import org.apache.hedwig.client.benchmark.BenchmarkUtils.ThroughputAggregator;
import org.apache.hedwig.client.benchmark.BenchmarkUtils.ThroughputLatencyAggregator;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach;
import org.apache.hedwig.util.Callback;

public class BenchmarkSubscriber extends BenchmarkWorker implements Callable<Void> {
    static final Logger logger = LoggerFactory.getLogger(BenchmarkSubscriber.class);
    Subscriber subscriber;
    ByteString subId;


    public BenchmarkSubscriber(int numTopics, int numMessages, int numRegions,
                               int startTopicLabel, int partitionIndex, int numPartitions, Subscriber subscriber, ByteString subId) {
        super(numTopics, numMessages, numRegions, startTopicLabel, partitionIndex, numPartitions);
        this.subscriber = subscriber;
        this.subId = subId;
    }

    public void warmup(int numWarmup) throws InterruptedException {
        /*
         * multiplying the number of ops by numParitions because we end up
         * skipping many because of the partitioning logic
         */
        multiSub("warmup", "warmup", 0, numWarmup, numWarmup * numPartitions);
    }

    public Void call() throws Exception {

        final ThroughputAggregator agg = new ThroughputAggregator("recvs", numMessages);
        final Map<String, Long> lastSeqIdSeenMap = new HashMap<String, Long>();

        for (int i = startTopicLabel; i < startTopicLabel + numTopics; i++) {

            if (!HedwigBenchmark.amIResponsibleForTopic(i, partitionIndex, numPartitions)) {
                continue;
            }

            final String topic = HedwigBenchmark.TOPIC_PREFIX + i;

            subscriber.subscribe(ByteString.copyFromUtf8(topic), subId, CreateOrAttach.CREATE_OR_ATTACH);
            subscriber.startDelivery(ByteString.copyFromUtf8(topic), subId, new MessageHandler() {

                @Override
                public void deliver(ByteString thisTopic, ByteString subscriberId, Message msg,
                Callback<Void> callback, Object context) {
                    if (logger.isDebugEnabled())
                        logger.debug("Got message from src-region: " + msg.getSrcRegion() + " with seq-id: "
                                     + msg.getMsgId());

                    String mapKey = topic + msg.getSrcRegion().toStringUtf8();
                    Long lastSeqIdSeen = lastSeqIdSeenMap.get(mapKey);
                    if (lastSeqIdSeen == null) {
                        lastSeqIdSeen = (long) 0;
                    }

                    if (getSrcSeqId(msg) <= lastSeqIdSeen) {
                        logger.info("Redelivery of message, src-region: " + msg.getSrcRegion() + "seq-id: "
                                    + msg.getMsgId());
                    } else {
                        agg.ding(false);
                    }

                    callback.operationFinished(context, null);
                }
            });
        }
        System.out.println("Finished subscribing to topics and now waiting for messages to come in...");
        // Wait till the benchmark test has completed
        agg.queue.take();
        System.out.println(agg.summarize(agg.earliest.get()));
        return null;
    }

    long getSrcSeqId(Message msg) {
        if (msg.getMsgId().getRemoteComponentsCount() == 0) {
            return msg.getMsgId().getLocalComponent();
        }

        for (RegionSpecificSeqId rseqId : msg.getMsgId().getRemoteComponentsList()) {
            if (rseqId.getRegion().equals(msg.getSrcRegion()))
                return rseqId.getSeqId();
        }

        return msg.getMsgId().getLocalComponent();
    }

    void multiSub(String label, String topicPrefix, int start, final int npar, final int count)
            throws InterruptedException {
        long startTime = System.currentTimeMillis();
        ThroughputLatencyAggregator agg = new ThroughputLatencyAggregator(label, count / numPartitions, npar);
        int end = start + count;
        for (int i = start; i < end; ++i) {
            if (!HedwigBenchmark.amIResponsibleForTopic(i, partitionIndex, numPartitions)) {
                continue;
            }
            subscriber.asyncSubscribe(ByteString.copyFromUtf8(topicPrefix + i), subId, CreateOrAttach.CREATE_OR_ATTACH,
                                      new BenchmarkCallback(agg), null);
        }
        // Wait till the benchmark test has completed
        agg.tpAgg.queue.take();
        if (count > 1)
            System.out.println(agg.summarize(startTime));
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/benchmark/BenchmarkUtils.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.benchmark;

import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.Semaphore;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicLong;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.util.Callback;

public class BenchmarkUtils {
    static final Logger logger = LoggerFactory.getLogger(BenchmarkUtils.class);

    public static double calcTp(final int count, long startTime) {
        return 1000. * count / (System.currentTimeMillis() - startTime);
    }

    /**
     * Stats aggregator for callback (round-trip) operations. Measures both
     * throughput and latency.
     */
    public static class ThroughputLatencyAggregator {
        int numBuckets;
        final ThroughputAggregator tpAgg;
        final Semaphore outstanding;
        final AtomicLong sum = new AtomicLong();

        final AtomicLong[] latencyBuckets;

        // bucket[i] is count of number of operations that took >= i ms and <
        // (i+1) ms.

        public ThroughputLatencyAggregator(String label, int count, int limit) throws InterruptedException {
            numBuckets = Integer.getInteger("numBuckets", 101);
            latencyBuckets = new AtomicLong[numBuckets];
            tpAgg = new ThroughputAggregator(label, count);
            outstanding = new Semaphore(limit);
            for (int i = 0; i < numBuckets; i++) {
                latencyBuckets[i] = new AtomicLong();
            }
        }

        public void reportLatency(long latency) {
            sum.addAndGet(latency);

            int bucketIndex;
            if (latency >= numBuckets) {
                bucketIndex = (int) numBuckets - 1;
            } else {
                bucketIndex = (int) latency;
            }
            latencyBuckets[bucketIndex].incrementAndGet();
        }

        private String getPercentile(double percentile) {
            int numInliersNeeded = (int) (percentile / 100 * tpAgg.count);
            int numInliersFound = 0;
            for (int i = 0; i < numBuckets - 1; i++) {
                numInliersFound += latencyBuckets[i].intValue();
                if (numInliersFound > numInliersNeeded) {
                    return i + "";
                }
            }
            return " >= " + (numBuckets - 1);
        }

        public String summarize(long startTime) {
            double percentile = Double.parseDouble(System.getProperty("percentile", "99.9"));
            return tpAgg.summarize(startTime) + ", avg latency = " + sum.get() / tpAgg.count + ", " + percentile
                   + "%ile latency = " + getPercentile(percentile);
        }
    }

    /**
     * Stats aggregator for non-callback (single-shot) operations. Measures just
     * throughput.
     */
    public static class ThroughputAggregator {
        final String label;
        final int count;
        final AtomicInteger done = new AtomicInteger();
        final AtomicLong earliest = new AtomicLong();
        final AtomicInteger numFailed = new AtomicInteger();
        final LinkedBlockingQueue<Integer> queue = new LinkedBlockingQueue<Integer>();

        public ThroughputAggregator(final String label, final int count) {
            this.label = label;
            this.count = count;
            if (count == 0)
                queue.add(0);
            if (Boolean.getBoolean("progress")) {
                new Thread(new Runnable() {
                    @Override
                    public void run() {
                        try {
                            for (int doneSnap = 0, prev = 0; doneSnap < count; prev = doneSnap, doneSnap = done.get()) {
                                if (doneSnap > prev) {
                                    System.out.println(label + " progress: " + doneSnap + " of " + count);
                                }
                                Thread.sleep(1000);
                            }
                        } catch (Exception ex) {
                            throw new RuntimeException(ex);
                        }
                    }
                }).start();
            }
        }

        public void ding(boolean failed) {
            int snapDone = done.incrementAndGet();
            earliest.compareAndSet(0, System.currentTimeMillis());
            if (failed)
                numFailed.incrementAndGet();
            if (logger.isDebugEnabled())
                logger.debug(label + " " + (failed ? "failed" : "succeeded") + ", done so far = " + snapDone);
            if (snapDone == count) {
                queue.add(numFailed.get());
            }
        }

        public String summarize(long startTime) {
            return "Finished " + label + ": count = " + done.get() + ", tput = " + calcTp(count, startTime)
                   + " ops/s, numFailed = " + numFailed;
        }
    }

    public static class BenchmarkCallback implements Callback<Void> {

        final ThroughputLatencyAggregator agg;
        final long startTime;

        public BenchmarkCallback(ThroughputLatencyAggregator agg) throws InterruptedException {
            this.agg = agg;
            agg.outstanding.acquire();
            // Must set the start time *after* taking acquiring on outstanding.
            startTime = System.currentTimeMillis();
        }

        private void finish(boolean failed) {
            agg.reportLatency(System.currentTimeMillis() - startTime);
            agg.tpAgg.ding(failed);
            agg.outstanding.release();
        }

        @Override
        public void operationFinished(Object ctx, Void resultOfOperation) {
            finish(false);
        }

        @Override
        public void operationFailed(Object ctx, PubSubException exception) {
            finish(true);
        }
    };

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/benchmark/BenchmarkWorker.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.benchmark;

public class BenchmarkWorker {
    int numTopics;
    int numMessages;
    int numRegions;
    int startTopicLabel;
    int partitionIndex;
    int numPartitions;

    public BenchmarkWorker(int numTopics, int numMessages, int numRegions,
                           int startTopicLabel, int partitionIndex, int numPartitions) {
        this.numTopics = numTopics;
        this.numMessages = numMessages;
        this.numRegions = numRegions;
        this.startTopicLabel = startTopicLabel;
        this.partitionIndex = partitionIndex;
        this.numPartitions = numPartitions;

        if (numMessages % (numTopics * numRegions) != 0) {
            throw new RuntimeException("Number of messages not equally divisible among regions and topics");
        }

        if (numTopics % numPartitions != 0) {
            throw new RuntimeException("Number of topics not equally divisible among partitions");
        }

    }
}
"
hedwig-client/src/main/java/org/apache/hedwig/client/benchmark/HedwigBenchmark.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.benchmark;

import java.io.File;
import java.util.concurrent.Callable;

import org.apache.commons.configuration.ConfigurationException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.logging.InternalLoggerFactory;
import org.jboss.netty.logging.Log4JLoggerFactory;

import com.google.protobuf.ByteString;
import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.HedwigClient;
import org.apache.hedwig.client.api.Publisher;
import org.apache.hedwig.client.api.Subscriber;

public class HedwigBenchmark implements Callable<Void> {
    protected static final Logger logger = LoggerFactory.getLogger(HedwigBenchmark.class);

    static final String TOPIC_PREFIX = "topic";

    private final HedwigClient client;
    private final Publisher publisher;
    private final Subscriber subscriber;

    public HedwigBenchmark(ClientConfiguration cfg) {
        client = new HedwigClient(cfg);
        publisher = client.getPublisher();
        subscriber = client.getSubscriber();
    }

    static boolean amIResponsibleForTopic(int topicNum, int partitionIndex, int numPartitions) {
        return topicNum % numPartitions == partitionIndex;
    }

    @Override
    public Void call() throws Exception {

        //
        // Parameters.
        //

        // What program to run: pub, sub (subscription benchmark), recv.
        final String mode = System.getProperty("mode","");

        // Number of requests to make (publishes or subscribes).
        int numTopics = Integer.getInteger("nTopics", 50);
        int numMessages = Integer.getInteger("nMsgs", 1000);
        int numRegions = Integer.getInteger("nRegions", 1);
        int startTopicLabel = Integer.getInteger("startTopicLabel", 0);
        int partitionIndex = Integer.getInteger("partitionIndex", 0);
        int numPartitions = Integer.getInteger("nPartitions", 1);

        int replicaIndex = Integer.getInteger("replicaIndex", 0);

        int rate = Integer.getInteger("rate", 0);
        int nParallel = Integer.getInteger("npar", 100);
        int msgSize = Integer.getInteger("msgSize", 1024);

        // Number of warmup subscriptions to make.
        final int nWarmups = Integer.getInteger("nwarmups", 1000);

        if (mode.equals("sub")) {
            BenchmarkSubscriber benchmarkSub = new BenchmarkSubscriber(numTopics, 0, 1, startTopicLabel, 0, 1,
                    subscriber, ByteString.copyFromUtf8("mySub"));

            benchmarkSub.warmup(nWarmups);
            benchmarkSub.call();

        } else if (mode.equals("recv")) {

            BenchmarkSubscriber benchmarkSub = new BenchmarkSubscriber(numTopics, numMessages, numRegions,
                    startTopicLabel, partitionIndex, numPartitions, subscriber, ByteString.copyFromUtf8("sub-"
                            + replicaIndex));

            benchmarkSub.call();

        } else if (mode.equals("pub")) {
            // Offered load in msgs/second.
            BenchmarkPublisher benchmarkPub = new BenchmarkPublisher(numTopics, numMessages, numRegions,
                    startTopicLabel, partitionIndex, numPartitions, publisher, subscriber, msgSize, nParallel, rate);
            benchmarkPub.warmup(nWarmups);
            benchmarkPub.call();

        } else {
            throw new Exception("unknown mode: " + mode);
        }

        return null;
    }

    public static void main(String[] args) throws Exception {
        ClientConfiguration cfg = new ClientConfiguration();
        if (args.length > 0) {
            String confFile = args[0];
            try {
                cfg.loadConf(new File(confFile).toURI().toURL());
            } catch (ConfigurationException e) {
                throw new RuntimeException(e);
            }
        }

        InternalLoggerFactory.setDefaultFactory(new Log4JLoggerFactory());

        HedwigBenchmark app = new HedwigBenchmark(cfg);
        app.call();
        System.exit(0);
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/conf/ClientConfiguration.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.conf;

import java.net.InetSocketAddress;

import org.apache.commons.configuration.ConfigurationException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.hedwig.conf.AbstractConfiguration;
import org.apache.hedwig.util.HedwigSocketAddress;

public class ClientConfiguration extends AbstractConfiguration {
    Logger logger = LoggerFactory.getLogger(ClientConfiguration.class);

    // Protected member variables for configuration parameter names
    protected static final String DEFAULT_SERVER_HOST = "default_server_host";
    protected static final String MAX_MESSAGE_SIZE = "max_message_size";
    protected static final String MAX_SERVER_REDIRECTS = "max_server_redirects";
    protected static final String AUTO_SEND_CONSUME_MESSAGE_ENABLED = "auto_send_consume_message_enabled";
    protected static final String CONSUMED_MESSAGES_BUFFER_SIZE = "consumed_messages_buffer_size";
    protected static final String MESSAGE_CONSUME_RETRY_WAIT_TIME = "message_consume_retry_wait_time";
    protected static final String SUBSCRIBE_RECONNECT_RETRY_WAIT_TIME = "subscribe_reconnect_retry_wait_time";
    protected static final String MAX_OUTSTANDING_MESSAGES = "max_outstanding_messages";
    protected static final String SERVER_ACK_RESPONSE_TIMEOUT = "server_ack_response_timeout";
    protected static final String TIMEOUT_THREAD_RUN_INTERVAL = "timeout_thread_run_interval";
    protected static final String SSL_ENABLED = "ssl_enabled";

    // Singletons we want to instantiate only once per ClientConfiguration
    protected HedwigSocketAddress myDefaultServerAddress = null;

    // Getters for the various Client Configuration parameters.
    // This should point to the default server host, or the VIP fronting all of
    // the server hubs. This will return the HedwigSocketAddress which
    // encapsulates both the regular and SSL port connection to the server host.
    protected HedwigSocketAddress getDefaultServerHedwigSocketAddress() {
        if (myDefaultServerAddress == null)
            myDefaultServerAddress = new HedwigSocketAddress(conf.getString(DEFAULT_SERVER_HOST, "localhost:4080:9876"));
        return myDefaultServerAddress;
    }

    // This will get the default server InetSocketAddress based on if SSL is
    // enabled or not.
    public InetSocketAddress getDefaultServerHost() {
        if (isSSLEnabled())
            return getDefaultServerHedwigSocketAddress().getSSLSocketAddress();
        else
            return getDefaultServerHedwigSocketAddress().getSocketAddress();
    }

    public int getMaximumMessageSize() {
        return conf.getInt(MAX_MESSAGE_SIZE, 2 * 1024 * 1024);
    }

    // This parameter is for setting the maximum number of server redirects to
    // allow before we consider it as an error condition. This is to stop
    // infinite redirect loops in case there is a problem with the hub servers
    // topic mastership.
    public int getMaximumServerRedirects() {
        return conf.getInt(MAX_SERVER_REDIRECTS, 2);
    }

    // This parameter is a boolean flag indicating if the client library should
    // automatically send the consume message to the server based on the
    // configured amount of messages consumed by the client app. The client app
    // could choose to override this behavior and instead, manually send the
    // consume message to the server via the client library using its own
    // logic and policy.
    public boolean isAutoSendConsumeMessageEnabled() {
        return conf.getBoolean(AUTO_SEND_CONSUME_MESSAGE_ENABLED, true);
    }

    // This parameter is to set how many consumed messages we'll buffer up
    // before we send the Consume message to the server indicating that all
    // of the messages up to that point have been successfully consumed by
    // the client.
    public int getConsumedMessagesBufferSize() {
        return conf.getInt(CONSUMED_MESSAGES_BUFFER_SIZE, 5);
    }

    // This parameter is used to determine how long we wait before retrying the
    // client app's MessageHandler to consume a subscribed messages sent to us
    // from the server. The time to wait is in milliseconds.
    public long getMessageConsumeRetryWaitTime() {
        return conf.getLong(MESSAGE_CONSUME_RETRY_WAIT_TIME, 10000);
    }

    // This parameter is used to determine how long we wait before retrying the
    // Subscribe Reconnect request. This is done when the connection to a server
    // disconnects and we attempt to connect to it. We'll keep on trying but
    // in case the server(s) is down for a longer time, we want to throttle
    // how often we do the subscribe reconnect request. The time to wait is in
    // milliseconds.
    public long getSubscribeReconnectRetryWaitTime() {
        return conf.getLong(SUBSCRIBE_RECONNECT_RETRY_WAIT_TIME, 10000);
    }

    // This parameter is for setting the maximum number of outstanding messages
    // the client app can be consuming at a time for topic subscription before
    // we throttle things and stop reading from the Netty Channel.
    public int getMaximumOutstandingMessages() {
        return conf.getInt(MAX_OUTSTANDING_MESSAGES, 10);
    }

    // This parameter is used to determine how long we wait (in milliseconds)
    // before we time out outstanding PubSubRequests that were written to the
    // server successfully but haven't yet received the ack response.
    public long getServerAckResponseTimeout() {
        return conf.getLong(SERVER_ACK_RESPONSE_TIMEOUT, 30000);
    }

    // This parameter is used to determine how often we run the server ack
    // response timeout cleaner thread (in milliseconds).
    public long getTimeoutThreadRunInterval() {
        return conf.getLong(TIMEOUT_THREAD_RUN_INTERVAL, 60000);
    }

    // This parameter is a boolean flag indicating if communication with the
    // server should be done via SSL for encryption. This is needed for
    // cross-colo hub clients listening to non-local servers.
    public boolean isSSLEnabled() {
        return conf.getBoolean(SSL_ENABLED, false);
    }

    // Validate that the configuration properties are valid.
    public void validate() throws ConfigurationException {
        if (isSSLEnabled() && getDefaultServerHedwigSocketAddress().getSSLSocketAddress() == null) {
            throw new ConfigurationException("SSL is enabled but a default server SSL port not given!");
        }
        // Add other validation checks here
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/data/MessageConsumeData.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.data;

import com.google.protobuf.ByteString;
import org.apache.hedwig.protocol.PubSubProtocol.Message;

/**
 * Wrapper class to store all of the data points needed to encapsulate Message
 * Consumption in the Subscribe flow for consuming a message sent from the
 * server for a given TopicSubscriber. This will be used as the Context in the
 * VoidCallback for the MessageHandlers once they've completed consuming the
 * message.
 *
 */
public class MessageConsumeData {

    // Member variables
    public final ByteString topic;
    public final ByteString subscriberId;
    // This is the Message sent from the server for Subscribes for consumption
    // by the client.
    public final Message msg;

    // Constructor
    public MessageConsumeData(final ByteString topic, final ByteString subscriberId, final Message msg) {
        this.topic = topic;
        this.subscriberId = subscriberId;
        this.msg = msg;
    }

    @Override
    public String toString() {
        StringBuilder sb = new StringBuilder();
        if (topic != null)
            sb.append("Topic: " + topic.toStringUtf8());
        if (subscriberId != null)
            sb.append(PubSubData.COMMA).append("SubscriberId: " + subscriberId.toStringUtf8());
        if (msg != null)
            sb.append(PubSubData.COMMA).append("Message: " + msg);
        return sb.toString();
    }
}
"
hedwig-client/src/main/java/org/apache/hedwig/client/data/PubSubData.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.data;

import java.util.List;

import com.google.protobuf.ByteString;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.OperationType;
import org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach;
import org.apache.hedwig.util.Callback;

/**
 * Wrapper class to store all of the data points needed to encapsulate all
 * PubSub type of request operations the client will do. This includes knowing
 * all of the information needed if we need to redo the publish/subscribe
 * request in case of a server redirect. This will be used for all sync/async
 * calls, and for all the known types of request messages to send to the server
 * hubs: Publish, Subscribe, Unsubscribe, and Consume.
 *
 */
public class PubSubData {
    // Static string constants
    protected static final String COMMA = ", ";

    // Member variables needed during object construction time.
    public final ByteString topic;
    public final Message msg;
    public final ByteString subscriberId;
    // Enum to indicate what type of operation this PubSub request data object
    // is for.
    public final OperationType operationType;
    // Enum for subscribe requests to indicate if this is a CREATE, ATTACH, or
    // CREATE_OR_ATTACH subscription request. For non-subscribe requests,
    // this will be null.
    public final CreateOrAttach createOrAttach;
    // These two variables are not final since we might override them
    // in the case of a Subscribe reconnect.
    public Callback<Void> callback;
    public Object context;

    // Member variables used after object has been constructed.
    // List of all servers we've sent the PubSubRequest to successfully.
    // This is to keep track of redirected servers that responded back to us.
    public List<ByteString> triedServers;
    // List of all servers that we've tried to connect or write to but
    // was unsuccessful. We'll retry sending the PubSubRequest but will
    // quit if we're trying to connect or write to a server that we've
    // attempted to previously.
    public List<ByteString> connectFailedServers;
    public List<ByteString> writeFailedServers;
    // Boolean to the hub server indicating if it should claim ownership
    // of the topic the PubSubRequest is for. This is mainly used after
    // a server redirect. Defaults to false.
    public boolean shouldClaim = false;
    // TxnID for the PubSubData if it was sent as a PubSubRequest to the hub
    // server. This is used in the WriteCallback in case of failure. We want
    // to remove it from the ResponseHandler.txn2PubSubData map since the
    // failed PubSubRequest will not get an ack response from the server.
    // This is set later in the PubSub flows only when we write the actual
    // request. Therefore it is not an argument in the constructor.
    public long txnId;
    // Time in milliseconds using the System.currentTimeMillis() call when the
    // PubSubRequest was written on the netty Channel to the server.
    public long requestWriteTime;
    // For synchronous calls, this variable is used to know when the background
    // async process for it has completed, set in the VoidCallback.
    public boolean isDone = false;

    // Constructor for all types of PubSub request data to send to the server
    public PubSubData(final ByteString topic, final Message msg, final ByteString subscriberId,
                      final OperationType operationType, final CreateOrAttach createOrAttach, final Callback<Void> callback,
                      final Object context) {
        this.topic = topic;
        this.msg = msg;
        this.subscriberId = subscriberId;
        this.operationType = operationType;
        this.createOrAttach = createOrAttach;
        this.callback = callback;
        this.context = context;
    }

    // Clear all of the stored servers we've contacted or attempted to in this
    // request.
    public void clearServersList() {
        if (triedServers != null)
            triedServers.clear();
        if (connectFailedServers != null)
            connectFailedServers.clear();
        if (writeFailedServers != null)
            writeFailedServers.clear();
    }

    @Override
    public String toString() {
        StringBuilder sb = new StringBuilder();
        if (topic != null)
            sb.append("Topic: " + topic.toStringUtf8());
        if (msg != null)
            sb.append(COMMA).append("Message: " + msg);
        if (subscriberId != null)
            sb.append(COMMA).append("SubscriberId: " + subscriberId.toStringUtf8());
        if (operationType != null)
            sb.append(COMMA).append("Operation Type: " + operationType.toString());
        if (createOrAttach != null)
            sb.append(COMMA).append("Create Or Attach: " + createOrAttach.toString());
        if (triedServers != null && triedServers.size() > 0) {
            sb.append(COMMA).append("Tried Servers: ");
            for (ByteString triedServer : triedServers) {
                sb.append(triedServer.toStringUtf8()).append(COMMA);
            }
        }
        if (connectFailedServers != null && connectFailedServers.size() > 0) {
            sb.append(COMMA).append("Connect Failed Servers: ");
            for (ByteString connectFailedServer : connectFailedServers) {
                sb.append(connectFailedServer.toStringUtf8()).append(COMMA);
            }
        }
        if (writeFailedServers != null && writeFailedServers.size() > 0) {
            sb.append(COMMA).append("Write Failed Servers: ");
            for (ByteString writeFailedServer : writeFailedServers) {
                sb.append(writeFailedServer.toStringUtf8()).append(COMMA);
            }
        }
        sb.append(COMMA).append("Should Claim: " + shouldClaim);
        if (txnId != 0)
            sb.append(COMMA).append("TxnID: " + txnId);
        if (requestWriteTime != 0)
            sb.append(COMMA).append("Request Write Time: " + requestWriteTime);
        sb.append(COMMA).append("Is Done: " + isDone);
        return sb.toString();
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/data/TopicSubscriber.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.data;

import org.apache.commons.lang.builder.HashCodeBuilder;

import com.google.protobuf.ByteString;

/**
 * Wrapper class object for the Topic + SubscriberId combination. Since the
 * Subscribe flows always use the Topic + SubscriberId as the logical entity,
 * we'll create a simple class to encapsulate that.
 *
 */
public class TopicSubscriber {
    private final ByteString topic;
    private final ByteString subscriberId;
    private final int hashCode;

    public TopicSubscriber(final ByteString topic, final ByteString subscriberId) {
        this.topic = topic;
        this.subscriberId = subscriberId;
        hashCode = new HashCodeBuilder().append(topic).append(subscriberId).toHashCode();
    }

    @Override
    public boolean equals(final Object o) {
        if (o == this)
            return true;
        if (!(o instanceof TopicSubscriber))
            return false;
        final TopicSubscriber obj = (TopicSubscriber) o;
        return topic.equals(obj.topic) && subscriberId.equals(obj.subscriberId);
    }

    @Override
    public int hashCode() {
        return hashCode;
    }

    @Override
    public String toString() {
        StringBuilder sb = new StringBuilder();
        if (topic != null)
            sb.append("Topic: " + topic.toStringUtf8());
        if (subscriberId != null)
            sb.append(PubSubData.COMMA).append("SubscriberId: " + subscriberId.toStringUtf8());
        return sb.toString();
    }

    public ByteString getTopic() {
        return topic;
    }

    public ByteString getSubscriberId() {
        return subscriberId;
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/exceptions/InvalidSubscriberIdException.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.exceptions;

/**
 * This is a Hedwig client side exception when the local client wants to do
 * subscribe type of operations. Currently, to distinguish between local and hub
 * subscribers, the subscriberId will have a specific format.
 */
public class InvalidSubscriberIdException extends Exception {

    private static final long serialVersionUID = 873259807218723523L;

    public InvalidSubscriberIdException(String message) {
        super(message);
    }

    public InvalidSubscriberIdException(String message, Throwable t) {
        super(message, t);
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/exceptions/ServerRedirectLoopException.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.exceptions;

/**
 * This is a Hedwig client side exception when the PubSubRequest is being
 * redirected to a server where the request has already been sent to previously.
 * To avoid having a cyclical redirect loop, this condition is checked for
 * and this exception will be thrown to the client caller.
 */
public class ServerRedirectLoopException extends Exception {

    private static final long serialVersionUID = 98723508723152897L;

    public ServerRedirectLoopException(String message) {
        super(message);
    }

    public ServerRedirectLoopException(String message, Throwable t) {
        super(message, t);
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/exceptions/TooManyServerRedirectsException.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.exceptions;

/**
 * This is a Hedwig client side exception when there have been too many server
 * redirects during a publish/subscribe call. We only allow a certain number of
 * server redirects to find the topic master. If we have exceeded this
 * configured amount, the publish/subscribe will fail with this exception.
 *
 */
public class TooManyServerRedirectsException extends Exception {

    private static final long serialVersionUID = 2341192937965635310L;

    public TooManyServerRedirectsException(String message) {
        super(message);
    }

    public TooManyServerRedirectsException(String message, Throwable t) {
        super(message, t);
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/handlers/MessageConsumeCallback.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.handlers;

import java.util.TimerTask;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;

import org.apache.hedwig.client.data.MessageConsumeData;
import org.apache.hedwig.client.data.TopicSubscriber;
import org.apache.hedwig.client.netty.HedwigClientImpl;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.util.Callback;

/**
 * This is the Callback used by the MessageHandlers on the client app when
 * they've finished consuming a subscription message sent from the server
 * asynchronously. This callback back to the client libs will be stateless so we
 * can use a singleton for the class. The object context used should be the
 * MessageConsumeData type. That will contain all of the information needed to
 * call the message consume logic in the client lib ResponseHandler.
 *
 */
public class MessageConsumeCallback implements Callback<Void> {

    private static Logger logger = LoggerFactory.getLogger(MessageConsumeCallback.class);

    private final HedwigClientImpl client;

    public MessageConsumeCallback(HedwigClientImpl client) {
        this.client = client;
    }

    class MessageConsumeRetryTask extends TimerTask {
        private final MessageConsumeData messageConsumeData;
        private final TopicSubscriber topicSubscriber;

        public MessageConsumeRetryTask(MessageConsumeData messageConsumeData, TopicSubscriber topicSubscriber) {
            this.messageConsumeData = messageConsumeData;
            this.topicSubscriber = topicSubscriber;
        }

        @Override
        public void run() {
            // Try to consume the message again
            Channel topicSubscriberChannel = client.getSubscriber().getChannelForTopic(topicSubscriber);
            HedwigClientImpl.getResponseHandlerFromChannel(topicSubscriberChannel).getSubscribeResponseHandler()
            .asyncMessageConsume(messageConsumeData.msg);
        }
    }

    public void operationFinished(Object ctx, Void resultOfOperation) {
        MessageConsumeData messageConsumeData = (MessageConsumeData) ctx;
        TopicSubscriber topicSubscriber = new TopicSubscriber(messageConsumeData.topic, messageConsumeData.subscriberId);
        // Message has been successfully consumed by the client app so callback
        // to the ResponseHandler indicating that the message is consumed.
        Channel topicSubscriberChannel = client.getSubscriber().getChannelForTopic(topicSubscriber);
        HedwigClientImpl.getResponseHandlerFromChannel(topicSubscriberChannel).getSubscribeResponseHandler()
        .messageConsumed(messageConsumeData.msg);
    }

    public void operationFailed(Object ctx, PubSubException exception) {
        // Message has NOT been successfully consumed by the client app so
        // callback to the ResponseHandler to try the async MessageHandler
        // Consume logic again.
        MessageConsumeData messageConsumeData = (MessageConsumeData) ctx;
        TopicSubscriber topicSubscriber = new TopicSubscriber(messageConsumeData.topic, messageConsumeData.subscriberId);
        logger.error("Message was not consumed successfully by client MessageHandler: " + messageConsumeData);

        // Sleep a pre-configured amount of time (in milliseconds) before we
        // do the retry. In the future, we can have more dynamic logic on
        // what duration to sleep based on how many times we've retried, or
        // perhaps what the last amount of time we slept was. We could stick
        // some of this meta-data into the MessageConsumeData when we retry.
        client.getClientTimer().schedule(new MessageConsumeRetryTask(messageConsumeData, topicSubscriber),
                                         client.getConfiguration().getMessageConsumeRetryWaitTime());
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/handlers/PublishResponseHandler.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.handlers;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;

import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.client.netty.HedwigClientImpl;
import org.apache.hedwig.client.netty.ResponseHandler;
import org.apache.hedwig.exceptions.PubSubException.ServiceDownException;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse;

public class PublishResponseHandler {

    private static Logger logger = LoggerFactory.getLogger(PublishResponseHandler.class);

    private final ResponseHandler responseHandler;

    public PublishResponseHandler(ResponseHandler responseHandler) {
        this.responseHandler = responseHandler;
    }

    // Main method to handle Publish Response messages from the server.
    public void handlePublishResponse(PubSubResponse response, PubSubData pubSubData, Channel channel) throws Exception {
        if (logger.isDebugEnabled())
            logger.debug("Handling a Publish response: " + response + ", pubSubData: " + pubSubData + ", host: "
                         + HedwigClientImpl.getHostFromChannel(channel));
        switch (response.getStatusCode()) {
        case SUCCESS:
            // Response was success so invoke the callback's operationFinished
            // method.
            pubSubData.callback.operationFinished(pubSubData.context, null);
            break;
        case SERVICE_DOWN:
            // Response was service down failure so just invoke the callback's
            // operationFailed method.
            pubSubData.callback.operationFailed(pubSubData.context, new ServiceDownException(
                                                    "Server responded with a SERVICE_DOWN status"));
            break;
        case NOT_RESPONSIBLE_FOR_TOPIC:
            // Redirect response so we'll need to repost the original Publish
            // Request
            responseHandler.handleRedirectResponse(response, pubSubData, channel);
            break;
        default:
            // Consider all other status codes as errors, operation failed
            // cases.
            logger.error("Unexpected error response from server for PubSubResponse: " + response);
            pubSubData.callback.operationFailed(pubSubData.context, new ServiceDownException(
                                                    "Server responded with a status code of: " + response.getStatusCode()));
            break;
        }
    }
}
"
hedwig-client/src/main/java/org/apache/hedwig/client/handlers/PubSubCallback.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.handlers;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.util.Callback;

/**
 * This class is used when we are doing synchronous type of operations. All
 * underlying client ops in Hedwig are async so this is just a way to make the
 * async calls synchronous.
 *
 */
public class PubSubCallback implements Callback<Void> {

    private static Logger logger = LoggerFactory.getLogger(PubSubCallback.class);

    // Private member variables
    private PubSubData pubSubData;
    // Boolean indicator to see if the sync PubSub call was successful or not.
    private boolean isCallSuccessful;
    // For sync callbacks, we'd like to know what the PubSubException is thrown
    // on failure. This is so we can have a handle to the exception and rethrow
    // it later.
    private PubSubException failureException;

    // Constructor
    public PubSubCallback(PubSubData pubSubData) {
        this.pubSubData = pubSubData;
    }

    public void operationFinished(Object ctx, Void resultOfOperation) {
        if (logger.isDebugEnabled())
            logger.debug("PubSub call succeeded for pubSubData: " + pubSubData);
        // Wake up the main sync PubSub thread that is waiting for us to
        // complete.
        synchronized (pubSubData) {
            isCallSuccessful = true;
            pubSubData.isDone = true;
            pubSubData.notify();
        }
    }

    public void operationFailed(Object ctx, PubSubException exception) {
        if (logger.isDebugEnabled())
            logger.debug("PubSub call failed with exception: " + exception + ", pubSubData: " + pubSubData);
        // Wake up the main sync PubSub thread that is waiting for us to
        // complete.
        synchronized (pubSubData) {
            isCallSuccessful = false;
            failureException = exception;
            pubSubData.isDone = true;
            pubSubData.notify();
        }
    }

    // Public getter to determine if the PubSub callback is successful or not
    // based on the PubSub ack response from the server.
    public boolean getIsCallSuccessful() {
        return isCallSuccessful;
    }

    // Public getter to retrieve what the PubSubException was that occurred when
    // the operation failed.
    public PubSubException getFailureException() {
        return failureException;
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/handlers/SubscribeReconnectCallback.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.handlers;

import java.util.TimerTask;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.hedwig.client.api.MessageHandler;
import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.client.netty.HedwigClientImpl;
import org.apache.hedwig.client.netty.HedwigSubscriber;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.ClientNotSubscribedException;
import org.apache.hedwig.util.Callback;

/**
 * This class is used when a Subscribe channel gets disconnected and we attempt
 * to re-establish the connection. Once the connection to the server host for
 * the topic is completed, we need to restart delivery for that topic if that
 * was the case before the original channel got disconnected. This async
 * callback will be the hook for this.
 *
 */
public class SubscribeReconnectCallback implements Callback<Void> {

    private static Logger logger = LoggerFactory.getLogger(SubscribeReconnectCallback.class);

    // Private member variables
    private final PubSubData origSubData;
    private final HedwigClientImpl client;
    private final HedwigSubscriber sub;
    private final ClientConfiguration cfg;
    private final MessageHandler messageHandler;

    // Constructor
    public SubscribeReconnectCallback(PubSubData origSubData, HedwigClientImpl client, MessageHandler messageHandler) {
        this.origSubData = origSubData;
        this.client = client;
        this.sub = client.getSubscriber();
        this.cfg = client.getConfiguration();
        this.messageHandler = messageHandler;
    }

    class SubscribeReconnectRetryTask extends TimerTask {
        @Override
        public void run() {
            if (logger.isDebugEnabled())
                logger.debug("Retrying subscribe reconnect request for origSubData: " + origSubData);
            // Clear out all of the servers we've contacted or attempted to from
            // this request.
            origSubData.clearServersList();
            client.doConnect(origSubData, cfg.getDefaultServerHost());
        }
    }

    public void operationFinished(Object ctx, Void resultOfOperation) {
        if (logger.isDebugEnabled())
            logger.debug("Subscribe reconnect succeeded for origSubData: " + origSubData);
        // Now we want to restart delivery for the subscription channel only
        // if delivery was started at the time the original subscribe channel
        // was disconnected.
        if (messageHandler != null) {
            try {
                sub.startDelivery(origSubData.topic, origSubData.subscriberId, messageHandler);
            } catch (ClientNotSubscribedException e) {
                // This exception should never be thrown here but just in case,
                // log an error and just keep retrying the subscribe request.
                logger.error("Subscribe was successful but error starting delivery for topic: "
                             + origSubData.topic.toStringUtf8() + ", subscriberId: "
                             + origSubData.subscriberId.toStringUtf8(), e);
                retrySubscribeRequest();
            }
        }
    }

    public void operationFailed(Object ctx, PubSubException exception) {
        // If the subscribe reconnect fails, just keep retrying the subscribe
        // request. There isn't a way to flag to the application layer that
        // a topic subscription has failed. So instead, we'll just keep
        // retrying in the background until success.
        logger.error("Subscribe reconnect failed with error: " + exception.getMessage());
        retrySubscribeRequest();
    }

    private void retrySubscribeRequest() {
        // If the client has stopped, there is no need to proceed with any
        // callback logic here.
        if (client.hasStopped())
            return;

        // Retry the subscribe request but only after waiting for a
        // preconfigured amount of time.
        client.getClientTimer().schedule(new SubscribeReconnectRetryTask(),
                                         client.getConfiguration().getSubscribeReconnectRetryWaitTime());
    }
}
"
hedwig-client/src/main/java/org/apache/hedwig/client/handlers/SubscribeResponseHandler.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.handlers;

import java.util.Collections;
import java.util.concurrent.ConcurrentHashMap;
import java.util.LinkedList;
import java.util.Queue;
import java.util.Set;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;

import org.apache.hedwig.client.api.MessageHandler;
import org.apache.hedwig.client.data.MessageConsumeData;
import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.client.data.TopicSubscriber;
import org.apache.hedwig.client.netty.HedwigClientImpl;
import org.apache.hedwig.client.netty.ResponseHandler;
import org.apache.hedwig.exceptions.PubSubException.ClientAlreadySubscribedException;
import org.apache.hedwig.exceptions.PubSubException.ServiceDownException;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse;
import org.apache.hedwig.protocol.PubSubProtocol.StatusCode;

public class SubscribeResponseHandler {

    private static Logger logger = LoggerFactory.getLogger(SubscribeResponseHandler.class);

    private final ResponseHandler responseHandler;

    // Member variables used when this ResponseHandler is for a Subscribe
    // channel. We need to be able to consume messages sent back to us from
    // the server, and to also recreate the Channel connection if it ever goes
    // down. For that, we need to store the original PubSubData for the
    // subscribe request, and also the MessageHandler that was registered when
    // delivery of messages started for the subscription.
    private PubSubData origSubData;
    private Channel subscribeChannel;
    private MessageHandler messageHandler;
    // Counter for the number of consumed messages so far to buffer up before we
    // send the Consume message back to the server along with the last/largest
    // message seq ID seen so far in that batch.
    private int numConsumedMessagesInBuffer = 0;
    private MessageSeqId lastMessageSeqId;
    // Queue used for subscribes when the MessageHandler hasn't been registered
    // yet but we've already received subscription messages from the server.
    // This will be lazily created as needed.
    private Queue<Message> subscribeMsgQueue;
    // Set to store all of the outstanding subscribed messages that are pending
    // to be consumed by the client app's MessageHandler. If this ever grows too
    // big (e.g. problem at the client end for message consumption), we can
    // throttle things by temporarily setting the Subscribe Netty Channel
    // to not be readable. When the Set has shrunk sufficiently, we can turn the
    // channel back on to read new messages.
    private Set<Message> outstandingMsgSet;

    public SubscribeResponseHandler(ResponseHandler responseHandler) {
        this.responseHandler = responseHandler;
    }

    // Public getter to retrieve the original PubSubData used for the Subscribe
    // request.
    public PubSubData getOrigSubData() {
        return origSubData;
    }

    // Main method to handle Subscribe responses from the server that we sent
    // a Subscribe Request to.
    public void handleSubscribeResponse(PubSubResponse response, PubSubData pubSubData, Channel channel)
            throws Exception {
        // If this was not a successful response to the Subscribe request, we
        // won't be using the Netty Channel created so just close it.
        if (!response.getStatusCode().equals(StatusCode.SUCCESS)) {
            HedwigClientImpl.getResponseHandlerFromChannel(channel).channelClosedExplicitly = true;
            channel.close();
        }

        if (logger.isDebugEnabled())
            logger.debug("Handling a Subscribe response: " + response + ", pubSubData: " + pubSubData + ", host: "
                         + HedwigClientImpl.getHostFromChannel(channel));
        switch (response.getStatusCode()) {
        case SUCCESS:
            // For successful Subscribe requests, store this Channel locally
            // and set it to not be readable initially.
            // This way we won't be delivering messages for this topic
            // subscription until the client explicitly says so.
            subscribeChannel = channel;
            subscribeChannel.setReadable(false);
            // Store the original PubSubData used to create this successful
            // Subscribe request.
            origSubData = pubSubData;
            // Store the mapping for the TopicSubscriber to the Channel.
            // This is so we can control the starting and stopping of
            // message deliveries from the server on that Channel. Store
            // this only on a successful ack response from the server.
            TopicSubscriber topicSubscriber = new TopicSubscriber(pubSubData.topic, pubSubData.subscriberId);
            responseHandler.getSubscriber().setChannelForTopic(topicSubscriber, channel);
            // Lazily create the Set (from a concurrent hashmap) to keep track
            // of outstanding Messages to be consumed by the client app. At this
            // stage, delivery for that topic hasn't started yet so creation of 
            // this Set should be thread safe. We'll create the Set with an initial
            // capacity equal to the configured parameter for the maximum number of
            // outstanding messages to allow. The load factor will be set to
            // 1.0f which means we'll only rehash and allocate more space if
            // we ever exceed the initial capacity. That should be okay
            // because when that happens, things are slow already and piling
            // up on the client app side to consume messages.
            
            outstandingMsgSet = Collections.newSetFromMap(new ConcurrentHashMap<Message,Boolean>(
                responseHandler.getConfiguration().getMaximumOutstandingMessages(), 1.0f));
            
            // Response was success so invoke the callback's operationFinished
            // method.
            pubSubData.callback.operationFinished(pubSubData.context, null);
            break;
        case CLIENT_ALREADY_SUBSCRIBED:
            // For Subscribe requests, the server says that the client is
            // already subscribed to it.
            pubSubData.callback.operationFailed(pubSubData.context, new ClientAlreadySubscribedException(
                                                    "Client is already subscribed for topic: " + pubSubData.topic.toStringUtf8() + ", subscriberId: "
                                                    + pubSubData.subscriberId.toStringUtf8()));
            break;
        case SERVICE_DOWN:
            // Response was service down failure so just invoke the callback's
            // operationFailed method.
            pubSubData.callback.operationFailed(pubSubData.context, new ServiceDownException(
                                                    "Server responded with a SERVICE_DOWN status"));
            break;
        case NOT_RESPONSIBLE_FOR_TOPIC:
            // Redirect response so we'll need to repost the original Subscribe
            // Request
            responseHandler.handleRedirectResponse(response, pubSubData, channel);
            break;
        default:
            // Consider all other status codes as errors, operation failed
            // cases.
            logger.error("Unexpected error response from server for PubSubResponse: " + response);
            pubSubData.callback.operationFailed(pubSubData.context, new ServiceDownException(
                                                    "Server responded with a status code of: " + response.getStatusCode()));
            break;
        }
    }

    // Main method to handle consuming a message for a topic that the client is
    // subscribed to.
    public void handleSubscribeMessage(PubSubResponse response) {
        if (logger.isDebugEnabled())
            logger.debug("Handling a Subscribe message in response: " + response + ", topic: "
                         + origSubData.topic.toStringUtf8() + ", subscriberId: " + origSubData.subscriberId.toStringUtf8());
        Message message = response.getMessage();

        synchronized (this) {
            // Consume the message asynchronously that the client is subscribed
            // to. Do this only if delivery for the subscription has started and
            // a MessageHandler has been registered for the TopicSubscriber.
            if (messageHandler != null) {
                asyncMessageConsume(message);
            } else {
                // MessageHandler has not yet been registered so queue up these
                // messages for the Topic Subscription. Make the initial lazy
                // creation of the message queue thread safe just so we don't
                // run into a race condition where two simultaneous threads process
                // a received message and both try to create a new instance of
                // the message queue. Performance overhead should be okay
                // because the delivery of the topic has not even started yet
                // so these messages are not consumed and just buffered up here.
                if (subscribeMsgQueue == null)
                    subscribeMsgQueue = new LinkedList<Message>();
                if (logger.isDebugEnabled())
                    logger
                    .debug("Message has arrived but Subscribe channel does not have a registered MessageHandler yet so queueing up the message: "
                           + message);
                subscribeMsgQueue.add(message);
            }
        }
    }

    /**
     * Method called when a message arrives for a subscribe Channel and we want
     * to consume it asynchronously via the registered MessageHandler (should
     * not be null when called here).
     *
     * @param message
     *            Message from Subscribe Channel we want to consume.
     */
    protected void asyncMessageConsume(Message message) {
        if (logger.isDebugEnabled())
            logger.debug("Call the client app's MessageHandler asynchronously to consume the message: " + message
                         + ", topic: " + origSubData.topic.toStringUtf8() + ", subscriberId: "
                         + origSubData.subscriberId.toStringUtf8());
        // Add this "pending to be consumed" message to the outstandingMsgSet.
        outstandingMsgSet.add(message);
        // Check if we've exceeded the max size for the outstanding message set.
        if (outstandingMsgSet.size() >= responseHandler.getConfiguration().getMaximumOutstandingMessages()
                && subscribeChannel.isReadable()) {
            // Too many outstanding messages so throttle it by setting the Netty
            // Channel to not be readable.
            if (logger.isDebugEnabled())
                logger.debug("Too many outstanding messages (" + outstandingMsgSet.size()
                             + ") so throttling the subscribe netty Channel");
            subscribeChannel.setReadable(false);
        }
        MessageConsumeData messageConsumeData = new MessageConsumeData(origSubData.topic, origSubData.subscriberId,
                message);
        messageHandler.deliver(origSubData.topic, origSubData.subscriberId, message, responseHandler.getClient()
                .getConsumeCallback(), messageConsumeData);
    }

    /**
     * Method called when the client app's MessageHandler has asynchronously
     * completed consuming a subscribed message sent from the server. The
     * contract with the client app is that messages sent to the handler to be
     * consumed will have the callback response done in the same order. So if we
     * asynchronously call the MessageHandler to consume messages #1-5, that
     * should call the messageConsumed method here via the VoidCallback in the
     * same order. To make this thread safe, since multiple outstanding messages
     * could be consumed by the client app and then called back to here, make
     * this method synchronized.
     *
     * @param message
     *            Message sent from server for topic subscription that has been
     *            consumed by the client.
     */
    protected synchronized void messageConsumed(Message message) {
        if (logger.isDebugEnabled())
            logger.debug("Message has been successfully consumed by the client app for message: " + message
                         + ", topic: " + origSubData.topic.toStringUtf8() + ", subscriberId: "
                         + origSubData.subscriberId.toStringUtf8());
        // Update the consumed messages buffer variables
        if (responseHandler.getConfiguration().isAutoSendConsumeMessageEnabled()) {
            // Update these variables only if we are auto-sending consume
            // messages to the server. Otherwise the onus is on the client app
            // to call the Subscriber consume API to let the server know which
            // messages it has successfully consumed.
            numConsumedMessagesInBuffer++;
            lastMessageSeqId = message.getMsgId();
        }
        // Remove this consumed message from the outstanding Message Set.
        outstandingMsgSet.remove(message);

        // For consume response to server, there is a config param on how many
        // messages to consume and buffer up before sending the consume request.
        // We just need to keep a count of the number of messages consumed
        // and the largest/latest msg ID seen so far in this batch. Messages
        // should be delivered in order and without gaps. Do this only if
        // auto-sending of consume messages is enabled.
        if (responseHandler.getConfiguration().isAutoSendConsumeMessageEnabled()
                && numConsumedMessagesInBuffer >= responseHandler.getConfiguration().getConsumedMessagesBufferSize()) {
            // Send the consume request and reset the consumed messages buffer
            // variables. We will use the same Channel created from the
            // subscribe request for the TopicSubscriber.
            if (logger.isDebugEnabled())
                logger
                .debug("Consumed message buffer limit reached so send the Consume Request to the server with lastMessageSeqId: "
                       + lastMessageSeqId);
            responseHandler.getSubscriber().doConsume(origSubData, subscribeChannel, lastMessageSeqId);
            numConsumedMessagesInBuffer = 0;
            lastMessageSeqId = null;
        }

        // Check if we throttled message consumption previously when the
        // outstanding message limit was reached. For now, only turn the
        // delivery back on if there are no more outstanding messages to
        // consume. We could make this a configurable parameter if needed.
        if (!subscribeChannel.isReadable() && outstandingMsgSet.size() == 0) {
            if (logger.isDebugEnabled())
                logger
                .debug("Message consumption has caught up so okay to turn off throttling of messages on the subscribe channel for topic: "
                       + origSubData.topic.toStringUtf8()
                       + ", subscriberId: "
                       + origSubData.subscriberId.toStringUtf8());
            subscribeChannel.setReadable(true);
        }
    }

    /**
     * Setter used for Subscribe flows when delivery for the subscription is
     * started. This is used to register the MessageHandler needed to consumer
     * the subscribed messages for the topic.
     *
     * @param messageHandler
     *            MessageHandler to register for this ResponseHandler instance.
     */
    public void setMessageHandler(MessageHandler messageHandler) {
        if (logger.isDebugEnabled())
            logger.debug("Setting the messageHandler for topic: " + origSubData.topic.toStringUtf8()
                         + ", subscriberId: " + origSubData.subscriberId.toStringUtf8());
        synchronized (this) {
            this.messageHandler = messageHandler;
            // Once the MessageHandler is registered, see if we have any queued up
            // subscription messages sent to us already from the server. If so,
            // consume those first. Do this only if the MessageHandler registered is
            // not null (since that would be the HedwigSubscriber.stopDelivery
            // call).
            if (messageHandler != null && subscribeMsgQueue != null && subscribeMsgQueue.size() > 0) {
                if (logger.isDebugEnabled())
                    logger.debug("Consuming " + subscribeMsgQueue.size() + " queued up messages for topic: "
                                 + origSubData.topic.toStringUtf8() + ", subscriberId: "
                                 + origSubData.subscriberId.toStringUtf8());
                for (Message message : subscribeMsgQueue) {
                    asyncMessageConsume(message);
                }
                // Now we can remove the queued up messages since they are all
                // consumed.
                subscribeMsgQueue.clear();
            }
        }
    }

    /**
     * Getter for the MessageHandler that is set for this subscribe channel.
     *
     * @return The MessageHandler for consuming messages
     */
    public MessageHandler getMessageHandler() {
        return messageHandler;
    }
}
"
hedwig-client/src/main/java/org/apache/hedwig/client/handlers/UnsubscribeResponseHandler.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.handlers;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;

import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.client.netty.HedwigClientImpl;
import org.apache.hedwig.client.netty.ResponseHandler;
import org.apache.hedwig.exceptions.PubSubException.ClientNotSubscribedException;
import org.apache.hedwig.exceptions.PubSubException.ServiceDownException;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse;

public class UnsubscribeResponseHandler {

    private static Logger logger = LoggerFactory.getLogger(UnsubscribeResponseHandler.class);

    private final ResponseHandler responseHandler;

    public UnsubscribeResponseHandler(ResponseHandler responseHandler) {
        this.responseHandler = responseHandler;
    }

    // Main method to handle Unsubscribe Response messages from the server.
    public void handleUnsubscribeResponse(PubSubResponse response, PubSubData pubSubData, Channel channel)
            throws Exception {
        if (logger.isDebugEnabled())
            logger.debug("Handling an Unsubscribe response: " + response + ", pubSubData: " + pubSubData + ", host: "
                         + HedwigClientImpl.getHostFromChannel(channel));
        switch (response.getStatusCode()) {
        case SUCCESS:
            // For successful Unsubscribe requests, we can now safely close the
            // Subscribe Channel and any cached data for that TopicSubscriber.
            responseHandler.getSubscriber().closeSubscription(pubSubData.topic, pubSubData.subscriberId);
            // Response was success so invoke the callback's operationFinished
            // method.
            pubSubData.callback.operationFinished(pubSubData.context, null);
            break;
        case CLIENT_NOT_SUBSCRIBED:
            // For Unsubscribe requests, the server says that the client was
            // never subscribed to the topic.
            pubSubData.callback.operationFailed(pubSubData.context, new ClientNotSubscribedException(
                                                    "Client was never subscribed to topic: " + pubSubData.topic.toStringUtf8() + ", subscriberId: "
                                                    + pubSubData.subscriberId.toStringUtf8()));
            break;
        case SERVICE_DOWN:
            // Response was service down failure so just invoke the callback's
            // operationFailed method.
            pubSubData.callback.operationFailed(pubSubData.context, new ServiceDownException(
                                                    "Server responded with a SERVICE_DOWN status"));
            break;
        case NOT_RESPONSIBLE_FOR_TOPIC:
            // Redirect response so we'll need to repost the original
            // Unsubscribe Request
            responseHandler.handleRedirectResponse(response, pubSubData, channel);
            break;
        default:
            // Consider all other status codes as errors, operation failed
            // cases.
            logger.error("Unexpected error response from server for PubSubResponse: " + response);
            pubSubData.callback.operationFailed(pubSubData.context, new ServiceDownException(
                                                    "Server responded with a status code of: " + response.getStatusCode()));
            break;
        }
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/ClientChannelPipelineFactory.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty;

import org.jboss.netty.channel.ChannelPipeline;
import org.jboss.netty.channel.ChannelPipelineFactory;
import org.jboss.netty.channel.Channels;
import org.jboss.netty.handler.codec.frame.LengthFieldBasedFrameDecoder;
import org.jboss.netty.handler.codec.frame.LengthFieldPrepender;
import org.jboss.netty.handler.codec.protobuf.ProtobufDecoder;
import org.jboss.netty.handler.codec.protobuf.ProtobufEncoder;
import org.jboss.netty.handler.ssl.SslHandler;

import org.apache.hedwig.protocol.PubSubProtocol;

public class ClientChannelPipelineFactory implements ChannelPipelineFactory {

    private HedwigClientImpl client;

    public ClientChannelPipelineFactory(HedwigClientImpl client) {
        this.client = client;
    }

    // Retrieve a ChannelPipeline from the factory.
    public ChannelPipeline getPipeline() throws Exception {
        // Create a new ChannelPipline using the factory method from the
        // Channels helper class.
        ChannelPipeline pipeline = Channels.pipeline();
        if (client.getSslFactory() != null) {
            pipeline.addLast("ssl", new SslHandler(client.getSslFactory().getEngine()));
        }
        pipeline.addLast("lengthbaseddecoder", new LengthFieldBasedFrameDecoder(client.getConfiguration()
                         .getMaximumMessageSize(), 0, 4, 0, 4));
        pipeline.addLast("lengthprepender", new LengthFieldPrepender(4));

        pipeline.addLast("protobufdecoder", new ProtobufDecoder(PubSubProtocol.PubSubResponse.getDefaultInstance()));
        pipeline.addLast("protobufencoder", new ProtobufEncoder());

        pipeline.addLast("responsehandler", new ResponseHandler(client));
        return pipeline;
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/ConnectCallback.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty;

import java.net.InetSocketAddress;
import java.util.LinkedList;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.ChannelFuture;
import org.jboss.netty.channel.ChannelFutureListener;

import com.google.protobuf.ByteString;
import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.exceptions.PubSubException.CouldNotConnectException;
import org.apache.hedwig.protocol.PubSubProtocol.OperationType;
import org.apache.hedwig.util.HedwigSocketAddress;

public class ConnectCallback implements ChannelFutureListener {

    private static Logger logger = LoggerFactory.getLogger(ConnectCallback.class);

    // Private member variables
    private PubSubData pubSubData;
    private InetSocketAddress host;
    private final HedwigClientImpl client;
    private final HedwigPublisher pub;
    private final HedwigSubscriber sub;
    private final ClientConfiguration cfg;

    // Constructor
    public ConnectCallback(PubSubData pubSubData, InetSocketAddress host, HedwigClientImpl client) {
        super();
        this.pubSubData = pubSubData;
        this.host = host;
        this.client = client;
        this.pub = client.getPublisher();
        this.sub = client.getSubscriber();
        this.cfg = client.getConfiguration();
    }

    public void operationComplete(ChannelFuture future) throws Exception {
        // If the client has stopped, there is no need to proceed with any
        // callback logic here.
        if (client.hasStopped())
            return;

        // Check if the connection to the server was done successfully.
        if (!future.isSuccess()) {
            logger.error("Error connecting to host: " + host);
            // If we were not able to connect to the host, it could be down.
            ByteString hostString = ByteString.copyFromUtf8(HedwigSocketAddress.sockAddrStr(host));
            if (pubSubData.connectFailedServers != null && pubSubData.connectFailedServers.contains(hostString)) {
                // We've already tried to connect to this host before so just
                // invoke the operationFailed callback.
                logger.error("Error connecting to host more than once so just invoke the operationFailed callback!");
                pubSubData.callback.operationFailed(pubSubData.context, new CouldNotConnectException(
                                                        "Could not connect to host: " + host));
            } else {
                if (logger.isDebugEnabled())
                    logger.debug("Try to connect to server: " + host + " again for pubSubData: " + pubSubData);
                // Keep track of this current server that we failed to connect
                // to but retry the request on the default server host/VIP.
                // The topic2Host mapping might need to be updated.
                if (pubSubData.connectFailedServers == null)
                    pubSubData.connectFailedServers = new LinkedList<ByteString>();
                pubSubData.connectFailedServers.add(hostString);
                client.doConnect(pubSubData, cfg.getDefaultServerHost());
            }
            // Finished with failure logic so just return.
            return;
        }

        // Now that we have connected successfully to the server, see what type
        // of PubSub request this was.
        if (logger.isDebugEnabled())
            logger.debug("Connection to host: " + host + " was successful for pubSubData: " + pubSubData);
        if (pubSubData.operationType.equals(OperationType.PUBLISH)) {
            // Publish Request so store this Channel connection in the
            // HedwigPublisher Map (if it doesn't exist yet) and then
            // do the publish on the cached channel mapped to the host.
            // Note that due to race concurrency situations, it is
            // possible that the cached channel is not the same one
            // as the channel established here. If that is the case,
            // this channel will be closed but we'll always publish on the
            // cached channel in the HedwigPublisher.host2Channel map.
            pub.storeHost2ChannelMapping(future.getChannel());
            pub.doPublish(pubSubData, pub.host2Channel.get(HedwigClientImpl.getHostFromChannel(future.getChannel())));
        } else if (pubSubData.operationType.equals(OperationType.UNSUBSCRIBE)) {
            // Unsubscribe Request so store this Channel connection in the
            // HedwigPublisher Map (if it doesn't exist yet) and then do the
            // unsubscribe. Unsubscribe requests will share and reuse
            // the netty Channel connections that Publish requests use.
            pub.storeHost2ChannelMapping(future.getChannel());
            sub.doSubUnsub(pubSubData, pub.host2Channel.get(HedwigClientImpl.getHostFromChannel(future.getChannel())));
        } else {
            // Subscribe Request. We do not store the Channel connection yet for
            // Subscribes here. This will be done only when we've found the
            // right server topic master. That is only determined when we
            // receive a successful server ack response to the Subscribe
            // request (handled in ResponseHandler). There is no need to store
            // the Unsubscribe channel connection as we won't use it again.
            sub.doSubUnsub(pubSubData, future.getChannel());
        }
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/HedwigClientImpl.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty;

import java.net.InetSocketAddress;
import java.util.LinkedList;
import java.util.List;
import java.util.Timer;
import java.util.TimerTask;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ConcurrentMap;
import java.util.concurrent.Executors;
import java.util.concurrent.atomic.AtomicLong;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.bootstrap.ClientBootstrap;
import org.jboss.netty.channel.Channel;
import org.jboss.netty.channel.ChannelFactory;
import org.jboss.netty.channel.ChannelFuture;
import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory;

import com.google.protobuf.ByteString;
import org.apache.hedwig.client.api.Client;
import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.client.handlers.MessageConsumeCallback;
import org.apache.hedwig.client.ssl.SslClientContextFactory;
import org.apache.hedwig.exceptions.PubSubException.UncertainStateException;

/**
 * This is a top level Hedwig Client class that encapsulates the common
 * functionality needed for both Publish and Subscribe operations.
 *
 */
public class HedwigClientImpl implements Client {

    private static final Logger logger = LoggerFactory.getLogger(HedwigClientImpl.class);

    // Global counter used for generating unique transaction ID's for
    // publish and subscribe requests
    protected final AtomicLong globalCounter = new AtomicLong();
    // Static String constants
    protected static final String COLON = ":";

    // The Netty socket factory for making connections to the server.
    protected final ChannelFactory socketFactory;
    // Whether the socket factory is one we created or is owned by whoever
    // instantiated us.
    protected boolean ownChannelFactory = false;

    // PipelineFactory to create netty client channels to the appropriate server
    private ClientChannelPipelineFactory pipelineFactory;

    // Concurrent Map to store the mapping from the Topic to the Host.
    // This could change over time since servers can drop mastership of topics
    // for load balancing or failover. If a server host ever goes down, we'd
    // also want to remove all topic mappings the host was responsible for.
    // The second Map is used as the inverted version of the first one.
    protected final ConcurrentMap<ByteString, InetSocketAddress> topic2Host = new ConcurrentHashMap<ByteString, InetSocketAddress>();
    private final ConcurrentMap<InetSocketAddress, List<ByteString>> host2Topics = new ConcurrentHashMap<InetSocketAddress, List<ByteString>>();

    // Each client instantiation will have a Timer for running recurring
    // threads. One such timer task thread to is to timeout long running
    // PubSubRequests that are waiting for an ack response from the server.
    private final Timer clientTimer = new Timer(true);

    // Boolean indicating if the client is running or has stopped.
    // Once we stop the client, we should sidestep all of the connect,
    // write callback and channel disconnected logic.
    private boolean isStopped = false;

    private HedwigSubscriber sub;
    private final HedwigPublisher pub;
    private final ClientConfiguration cfg;
    private final MessageConsumeCallback consumeCb;
    private SslClientContextFactory sslFactory = null;

    public static Client create(ClientConfiguration cfg) {
        return new HedwigClientImpl(cfg);
    }

    public static Client create(ClientConfiguration cfg, ChannelFactory socketFactory) {
        return new HedwigClientImpl(cfg, socketFactory);
    }

    // Base constructor that takes in a Configuration object.
    // This will create its own client socket channel factory.
    protected HedwigClientImpl(ClientConfiguration cfg) {
        this(cfg, new NioClientSocketChannelFactory(Executors.newCachedThreadPool(), Executors.newCachedThreadPool()));
        ownChannelFactory = true;
    }

    // Constructor that takes in a Configuration object and a ChannelFactory
    // that has already been instantiated by the caller.
    protected HedwigClientImpl(ClientConfiguration cfg, ChannelFactory socketFactory) {
        this.cfg = cfg;
        this.socketFactory = socketFactory;
        pub = new HedwigPublisher(this);
        sub = new HedwigSubscriber(this);
        pipelineFactory = new ClientChannelPipelineFactory(this);
        consumeCb = new MessageConsumeCallback(this);
        if (cfg.isSSLEnabled()) {
            sslFactory = new SslClientContextFactory(cfg);
        }
        // Schedule all of the client timer tasks. Currently we only have the
        // Request Timeout task.
        clientTimer.schedule(new PubSubRequestTimeoutTask(), 0, cfg.getTimeoutThreadRunInterval());
    }

    // Public getters for the various components of a client.
    public ClientConfiguration getConfiguration() {
        return cfg;
    }

    public HedwigSubscriber getSubscriber() {
        return sub;
    }

    // Protected method to set the subscriber. This is needed currently for hub
    // versions of the client subscriber.
    protected void setSubscriber(HedwigSubscriber sub) {
        this.sub = sub;
    }

    public HedwigPublisher getPublisher() {
        return pub;
    }

    public MessageConsumeCallback getConsumeCallback() {
        return consumeCb;
    }

    public SslClientContextFactory getSslFactory() {
        return sslFactory;
    }

    // We need to deal with the possible problem of a PubSub request being
    // written to successfully to the server host but for some reason, the
    // ack message back never comes. What could happen is that the VoidCallback
    // stored in the ResponseHandler.txn2PublishData map will never be called.
    // We should have a configured timeout so if that passes from the time a
    // write was successfully done to the server, we can fail this async PubSub
    // transaction. The caller could possibly redo the transaction if needed at
    // a later time. Creating a timeout cleaner TimerTask to do this here.
    class PubSubRequestTimeoutTask extends TimerTask {
        /**
         * Implement the TimerTask's abstract run method.
         */
        @Override
        public void run() {
            if (logger.isDebugEnabled())
                logger.debug("Running the PubSubRequest Timeout Task");
            // Loop through all outstanding PubSubData requests and check if
            // the requestWriteTime has timed out compared to the current time.
            long curTime = System.currentTimeMillis();
            long timeoutInterval = cfg.getServerAckResponseTimeout();

            // First check the ResponseHandlers associated with cached
            // channels in HedwigPublisher.host2Channel. This stores the
            // channels used for Publish and Unsubscribe requests.
            for (Channel channel : pub.host2Channel.values()) {
                ResponseHandler responseHandler = getResponseHandlerFromChannel(channel);
                for (PubSubData pubSubData : responseHandler.txn2PubSubData.values()) {
                    checkPubSubDataToTimeOut(pubSubData, responseHandler, curTime, timeoutInterval);
                }
            }
            // Now do the same for the cached channels in
            // HedwigSubscriber.topicSubscriber2Channel. This stores the
            // channels used exclusively for Subscribe requests.
            for (Channel channel : sub.topicSubscriber2Channel.values()) {
                ResponseHandler responseHandler = getResponseHandlerFromChannel(channel);
                for (PubSubData pubSubData : responseHandler.txn2PubSubData.values()) {
                    checkPubSubDataToTimeOut(pubSubData, responseHandler, curTime, timeoutInterval);
                }
            }
        }

        private void checkPubSubDataToTimeOut(PubSubData pubSubData, ResponseHandler responseHandler, long curTime,
                                              long timeoutInterval) {
            if (curTime > pubSubData.requestWriteTime + timeoutInterval) {
                // Current PubSubRequest has timed out so remove it from the
                // ResponseHandler's map and invoke the VoidCallback's
                // operationFailed method.
                logger.error("Current PubSubRequest has timed out for pubSubData: " + pubSubData);
                responseHandler.txn2PubSubData.remove(pubSubData.txnId);
                pubSubData.callback.operationFailed(pubSubData.context, new UncertainStateException(
                                                        "Server ack response never received so PubSubRequest has timed out!"));
            }
        }
    }

    // When we are done with the client, this is a clean way to gracefully close
    // all channels/sockets created by the client and to also release all
    // resources used by netty.
    public void close() {
        logger.info("Stopping the client!");
        // Set the client boolean flag to indicate the client has stopped.
        isStopped = true;
        // Stop the timer and all timer task threads.
        clientTimer.cancel();
        // Close all of the open Channels.
        for (Channel channel : pub.host2Channel.values()) {
            getResponseHandlerFromChannel(channel).channelClosedExplicitly = true;
            channel.close().awaitUninterruptibly();
        }
        for (Channel channel : sub.topicSubscriber2Channel.values()) {
            getResponseHandlerFromChannel(channel).channelClosedExplicitly = true;
            channel.close().awaitUninterruptibly();
        }
        // Clear out all Maps.
        topic2Host.clear();
        host2Topics.clear();
        pub.host2Channel.clear();
        sub.topicSubscriber2Channel.clear();
        // Release resources used by the ChannelFactory on the client if we are
        // the owner that created it.
        if (ownChannelFactory) {
            socketFactory.releaseExternalResources();
        }
        logger.info("Completed stopping the client!");
    }

    /**
     * This is a helper method to do the connect attempt to the server given the
     * inputted host/port. This can be used to connect to the default server
     * host/port which is the VIP. That will pick a server in the cluster at
     * random to connect to for the initial PubSub attempt (with redirect logic
     * being done at the server side). Additionally, this could be called after
     * the client makes an initial PubSub attempt at a server, and is redirected
     * to the one that is responsible for the topic. Once the connect to the
     * server is done, we will perform the corresponding PubSub write on that
     * channel.
     *
     * @param pubSubData
     *            PubSub call's data wrapper object
     * @param serverHost
     *            Input server host to connect to of type InetSocketAddress
     */
    public void doConnect(PubSubData pubSubData, InetSocketAddress serverHost) {
        if (logger.isDebugEnabled())
            logger.debug("Connecting to host: " + serverHost + " with pubSubData: " + pubSubData);
        // Set up the ClientBootStrap so we can create a new Channel connection
        // to the server.
        ClientBootstrap bootstrap = new ClientBootstrap(socketFactory);
        bootstrap.setPipelineFactory(pipelineFactory);
        bootstrap.setOption("tcpNoDelay", true);
        bootstrap.setOption("keepAlive", true);

        // Start the connection attempt to the input server host.
        ChannelFuture future = bootstrap.connect(serverHost);
        future.addListener(new ConnectCallback(pubSubData, serverHost, this));
    }

    /**
     * Helper method to store the topic2Host mapping in the HedwigClient cache
     * map. This method is assumed to be called when we've done a successful
     * connection to the correct server topic master.
     *
     * @param pubSubData
     *            PubSub wrapper data
     * @param channel
     *            Netty Channel
     */
    protected void storeTopic2HostMapping(PubSubData pubSubData, Channel channel) {
        // Retrieve the server host that we've connected to and store the
        // mapping from the topic to this host. For all other non-redirected
        // server statuses, we consider that as a successful connection to the
        // correct topic master.
        InetSocketAddress host = getHostFromChannel(channel);
        if (topic2Host.containsKey(pubSubData.topic) && topic2Host.get(pubSubData.topic).equals(host)) {
            // Entry in map exists for the topic but it is the same as the
            // current host. In this case there is nothing to do.
            return;
        }

        // Store the relevant mappings for this topic and host combination.
        if (logger.isDebugEnabled())
            logger.debug("Storing info for topic: " + pubSubData.topic.toStringUtf8() + ", old host: "
                         + topic2Host.get(pubSubData.topic) + ", new host: " + host);
        topic2Host.put(pubSubData.topic, host);
        if (host2Topics.containsKey(host)) {
            host2Topics.get(host).add(pubSubData.topic);
        } else {
            LinkedList<ByteString> topicsList = new LinkedList<ByteString>();
            topicsList.add(pubSubData.topic);
            host2Topics.put(host, topicsList);
        }
    }

    /**
     * Helper static method to get the String Hostname:Port from a netty
     * Channel. Assumption is that the netty Channel was originally created with
     * an InetSocketAddress. This is true with the Hedwig netty implementation.
     *
     * @param channel
     *            Netty channel to extract the hostname and port from.
     * @return String representation of the Hostname:Port from the Netty Channel
     */
    public static InetSocketAddress getHostFromChannel(Channel channel) {
        return (InetSocketAddress) channel.getRemoteAddress();
    }

    /**
     * Helper static method to get the ResponseHandler instance from a Channel
     * via the ChannelPipeline it is associated with. The assumption is that the
     * last ChannelHandler tied to the ChannelPipeline is the ResponseHandler.
     *
     * @param channel
     *            Channel we are retrieving the ResponseHandler instance for
     * @return ResponseHandler Instance tied to the Channel's Pipeline
     */
    public static ResponseHandler getResponseHandlerFromChannel(Channel channel) {
        return (ResponseHandler) channel.getPipeline().getLast();
    }

    // Public getter for entries in the topic2Host Map.
    public InetSocketAddress getHostForTopic(ByteString topic) {
        return topic2Host.get(topic);
    }

    // If a server host goes down or the channel to it gets disconnected,
    // we want to clear out all relevant cached information. We'll
    // need to remove all of the topic mappings that the host was
    // responsible for.
    public void clearAllTopicsForHost(InetSocketAddress host) {
        if (logger.isDebugEnabled())
            logger.debug("Clearing all topics for host: " + host);
        // For each of the topics that the host was responsible for,
        // remove it from the topic2Host mapping.
        if (host2Topics.containsKey(host)) {
            for (ByteString topic : host2Topics.get(host)) {
                if (logger.isDebugEnabled())
                    logger.debug("Removing mapping for topic: " + topic.toStringUtf8() + " from host: " + host);
                topic2Host.remove(topic);
            }
            // Now it is safe to remove the host2Topics mapping entry.
            host2Topics.remove(host);
        }
    }

    // Public getter to see if the client has been stopped.
    public boolean hasStopped() {
        return isStopped;
    }

    // Public getter to get the client's Timer object.
    // This is so we can reuse this and not have to create multiple Timer
    // objects.
    public Timer getClientTimer() {
        return clientTimer;
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/HedwigPublisher.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty;

import java.net.InetSocketAddress;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ConcurrentMap;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;
import org.jboss.netty.channel.ChannelFuture;

import com.google.protobuf.ByteString;
import org.apache.hedwig.client.api.Publisher;
import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.client.handlers.PubSubCallback;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.CouldNotConnectException;
import org.apache.hedwig.exceptions.PubSubException.ServiceDownException;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.OperationType;
import org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;
import org.apache.hedwig.protocol.PubSubProtocol.PublishRequest;
import org.apache.hedwig.util.Callback;

/**
 * This is the Hedwig Netty specific implementation of the Publisher interface.
 *
 */
public class HedwigPublisher implements Publisher {

    private static Logger logger = LoggerFactory.getLogger(HedwigPublisher.class);

    // Concurrent Map to store the mappings for a given Host (Hostname:Port) to
    // the Channel that has been established for it previously. This channel
    // will be used whenever we publish on a topic that the server is the master
    // of currently. The channels used here will only be used for publish and
    // unsubscribe requests.
    protected final ConcurrentMap<InetSocketAddress, Channel> host2Channel = new ConcurrentHashMap<InetSocketAddress, Channel>();

    private final HedwigClientImpl client;
    private final ClientConfiguration cfg;

    protected HedwigPublisher(HedwigClientImpl client) {
        this.client = client;
        this.cfg = client.getConfiguration();
    }

    public void publish(ByteString topic, Message msg) throws CouldNotConnectException, ServiceDownException {
        if (logger.isDebugEnabled())
            logger.debug("Calling a sync publish for topic: " + topic.toStringUtf8() + ", msg: " + msg);
        PubSubData pubSubData = new PubSubData(topic, msg, null, OperationType.PUBLISH, null, null, null);
        synchronized (pubSubData) {
            PubSubCallback pubSubCallback = new PubSubCallback(pubSubData);
            asyncPublish(topic, msg, pubSubCallback, null);
            try {
                while (!pubSubData.isDone)
                    pubSubData.wait();
            } catch (InterruptedException e) {
                throw new ServiceDownException("Interrupted Exception while waiting for async publish call");
            }
            // Check from the PubSubCallback if it was successful or not.
            if (!pubSubCallback.getIsCallSuccessful()) {
                // See what the exception was that was thrown when the operation
                // failed.
                PubSubException failureException = pubSubCallback.getFailureException();
                if (failureException == null) {
                    // This should not happen as the operation failed but a null
                    // PubSubException was passed. Log a warning message but
                    // throw a generic ServiceDownException.
                    logger.error("Sync Publish operation failed but no PubSubException was passed!");
                    throw new ServiceDownException("Server ack response to publish request is not successful");
                }
                // For the expected exceptions that could occur, just rethrow
                // them.
                else if (failureException instanceof CouldNotConnectException) {
                    throw (CouldNotConnectException) failureException;
                } else if (failureException instanceof ServiceDownException) {
                    throw (ServiceDownException) failureException;
                } else {
                    // For other types of PubSubExceptions, just throw a generic
                    // ServiceDownException but log a warning message.
                    logger.error("Unexpected exception type when a sync publish operation failed: " + failureException);
                    throw new ServiceDownException("Server ack response to publish request is not successful");
                }
            }
        }
    }

    public void asyncPublish(ByteString topic, Message msg, Callback<Void> callback, Object context) {
        if (logger.isDebugEnabled())
            logger.debug("Calling an async publish for topic: " + topic.toStringUtf8() + ", msg: " + msg);
        // Check if we already have a Channel connection set up to the server
        // for the given Topic.
        PubSubData pubSubData = new PubSubData(topic, msg, null, OperationType.PUBLISH, null, callback, context);
        if (client.topic2Host.containsKey(topic)) {
            InetSocketAddress host = client.topic2Host.get(topic);
            if (host2Channel.containsKey(host)) {
                // We already have the Channel connection for the server host so
                // do the publish directly. We will deal with redirect logic
                // later on if that server is no longer the current host for
                // the topic.
                doPublish(pubSubData, host2Channel.get(host));
            } else {
                // We have a mapping for the topic to host but don't have a
                // Channel for that server. This can happen if the Channel
                // is disconnected for some reason. Do the connect then to
                // the specified server host to create a new Channel connection.
                client.doConnect(pubSubData, host);
            }
        } else {
            // Server host for the given topic is not known yet so use the
            // default server host/port as defined in the configs. This should
            // point to the server VIP which would redirect to a random server
            // (which might not be the server hosting the topic).
            InetSocketAddress host = cfg.getDefaultServerHost();
            if (host2Channel.containsKey(host)) {
                // if there is a channel to default server, use it!
                doPublish(pubSubData, host2Channel.get(host));
                return;
            }
            client.doConnect(pubSubData, host);
        }
    }

    /**
     * This is a helper method to write the actual publish message once the
     * client is connected to the server and a Channel is available.
     *
     * @param pubSubData
     *            Publish call's data wrapper object
     * @param channel
     *            Netty I/O channel for communication between the client and
     *            server
     */
    protected void doPublish(PubSubData pubSubData, Channel channel) {
        // Create a PubSubRequest
        PubSubRequest.Builder pubsubRequestBuilder = PubSubRequest.newBuilder();
        pubsubRequestBuilder.setProtocolVersion(ProtocolVersion.VERSION_ONE);
        pubsubRequestBuilder.setType(OperationType.PUBLISH);
        if (pubSubData.triedServers != null && pubSubData.triedServers.size() > 0) {
            pubsubRequestBuilder.addAllTriedServers(pubSubData.triedServers);
        }
        long txnId = client.globalCounter.incrementAndGet();
        pubsubRequestBuilder.setTxnId(txnId);
        pubsubRequestBuilder.setShouldClaim(pubSubData.shouldClaim);
        pubsubRequestBuilder.setTopic(pubSubData.topic);

        // Now create the PublishRequest
        PublishRequest.Builder publishRequestBuilder = PublishRequest.newBuilder();

        publishRequestBuilder.setMsg(pubSubData.msg);

        // Set the PublishRequest into the outer PubSubRequest
        pubsubRequestBuilder.setPublishRequest(publishRequestBuilder);

        // Update the PubSubData with the txnId and the requestWriteTime
        pubSubData.txnId = txnId;
        pubSubData.requestWriteTime = System.currentTimeMillis();

        // Before we do the write, store this information into the
        // ResponseHandler so when the server responds, we know what
        // appropriate Callback Data to invoke for the given txn ID.
        HedwigClientImpl.getResponseHandlerFromChannel(channel).txn2PubSubData.put(txnId, pubSubData);

        // Finally, write the Publish request through the Channel.
        if (logger.isDebugEnabled())
            logger.debug("Writing a Publish request to host: " + HedwigClientImpl.getHostFromChannel(channel)
                         + " for pubSubData: " + pubSubData);
        ChannelFuture future = channel.write(pubsubRequestBuilder.build());
        future.addListener(new WriteCallback(pubSubData, client));
    }

    // Synchronized method to store the host2Channel mapping (if it doesn't
    // exist yet). Retrieve the hostname info from the Channel created via the
    // RemoteAddress tied to it.
    protected synchronized void storeHost2ChannelMapping(Channel channel) {
        InetSocketAddress host = HedwigClientImpl.getHostFromChannel(channel);
        if (!host2Channel.containsKey(host)) {
            if (logger.isDebugEnabled())
                logger.debug("Storing a new Channel mapping for host: " + host);
            host2Channel.put(host, channel);
        } else {
            // If we've reached here, that means we already have a Channel
            // mapping for the given host. This should ideally not happen
            // and it means we are creating another Channel to a server host
            // to publish on when we could have used an existing one. This could
            // happen due to a race condition if initially multiple concurrent
            // threads are publishing on the same topic and no Channel exists
            // currently to the server. We are not synchronizing this initial
            // creation of Channels to a given host for performance.
            // Another possible way to have redundant Channels created is if
            // a new topic is being published to, we connect to the default
            // server host which should be a VIP that redirects to a "real"
            // server host. Since we don't know beforehand what is the full
            // set of server hosts, we could be redirected to a server that
            // we already have a channel connection to from a prior existing
            // topic. Close these redundant channels as they won't be used.
            if (logger.isDebugEnabled())
                logger.debug("Channel mapping to host: " + host + " already exists so no need to store it.");
            HedwigClientImpl.getResponseHandlerFromChannel(channel).channelClosedExplicitly = true;
            channel.close();
        }
    }

    // Public getter for entries in the host2Channel Map.
    // This is used for classes that need this information but are not in the
    // same classpath.
    public Channel getChannelForHost(InetSocketAddress host) {
        return host2Channel.get(host);
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/HedwigSubscriber.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty;

import java.net.InetSocketAddress;
import java.util.List;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ConcurrentMap;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;
import org.jboss.netty.channel.ChannelFuture;
import org.jboss.netty.channel.ChannelFutureListener;

import com.google.protobuf.ByteString;
import org.apache.hedwig.client.api.MessageHandler;
import org.apache.hedwig.client.api.Subscriber;
import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.client.data.TopicSubscriber;
import org.apache.hedwig.client.exceptions.InvalidSubscriberIdException;
import org.apache.hedwig.client.handlers.PubSubCallback;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.ClientAlreadySubscribedException;
import org.apache.hedwig.exceptions.PubSubException.ClientNotSubscribedException;
import org.apache.hedwig.exceptions.PubSubException.CouldNotConnectException;
import org.apache.hedwig.exceptions.PubSubException.ServiceDownException;
import org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.OperationType;
import org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;
import org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest;
import org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest;
import org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach;
import org.apache.hedwig.protoextensions.SubscriptionStateUtils;
import org.apache.hedwig.util.Callback;

/**
 * This is the Hedwig Netty specific implementation of the Subscriber interface.
 *
 */
public class HedwigSubscriber implements Subscriber {

    private static Logger logger = LoggerFactory.getLogger(HedwigSubscriber.class);

    // Concurrent Map to store the cached Channel connections on the client side
    // to a server host for a given Topic + SubscriberId combination. For each
    // TopicSubscriber, we want a unique Channel connection to the server for
    // it. We can also get the ResponseHandler tied to the Channel via the
    // Channel Pipeline.
    protected final ConcurrentMap<TopicSubscriber, Channel> topicSubscriber2Channel = new ConcurrentHashMap<TopicSubscriber, Channel>();

    protected final HedwigClientImpl client;
    protected final ClientConfiguration cfg;

    public HedwigSubscriber(HedwigClientImpl client) {
        this.client = client;
        this.cfg = client.getConfiguration();
    }

    // Private method that holds the common logic for doing synchronous
    // Subscribe or Unsubscribe requests. This is for code reuse since these
    // two flows are very similar. The assumption is that the input
    // OperationType is either SUBSCRIBE or UNSUBSCRIBE.
    private void subUnsub(ByteString topic, ByteString subscriberId, OperationType operationType,
                          CreateOrAttach createOrAttach) throws CouldNotConnectException, ClientAlreadySubscribedException,
        ClientNotSubscribedException, ServiceDownException {
        if (logger.isDebugEnabled())
            logger.debug("Calling a sync subUnsub request for topic: " + topic.toStringUtf8() + ", subscriberId: "
                         + subscriberId.toStringUtf8() + ", operationType: " + operationType + ", createOrAttach: "
                         + createOrAttach);
        PubSubData pubSubData = new PubSubData(topic, null, subscriberId, operationType, createOrAttach, null, null);
        synchronized (pubSubData) {
            PubSubCallback pubSubCallback = new PubSubCallback(pubSubData);
            asyncSubUnsub(topic, subscriberId, pubSubCallback, null, operationType, createOrAttach);
            try {
                while (!pubSubData.isDone)
                    pubSubData.wait();
            } catch (InterruptedException e) {
                throw new ServiceDownException("Interrupted Exception while waiting for async subUnsub call");
            }
            // Check from the PubSubCallback if it was successful or not.
            if (!pubSubCallback.getIsCallSuccessful()) {
                // See what the exception was that was thrown when the operation
                // failed.
                PubSubException failureException = pubSubCallback.getFailureException();
                if (failureException == null) {
                    // This should not happen as the operation failed but a null
                    // PubSubException was passed. Log a warning message but
                    // throw a generic ServiceDownException.
                    logger.error("Sync SubUnsub operation failed but no PubSubException was passed!");
                    throw new ServiceDownException("Server ack response to SubUnsub request is not successful");
                }
                // For the expected exceptions that could occur, just rethrow
                // them.
                else if (failureException instanceof CouldNotConnectException)
                    throw (CouldNotConnectException) failureException;
                else if (failureException instanceof ClientAlreadySubscribedException)
                    throw (ClientAlreadySubscribedException) failureException;
                else if (failureException instanceof ClientNotSubscribedException)
                    throw (ClientNotSubscribedException) failureException;
                else if (failureException instanceof ServiceDownException)
                    throw (ServiceDownException) failureException;
                else {
                    logger.error("Unexpected PubSubException thrown: " + failureException.toString());
                    // Throw a generic ServiceDownException but wrap the
                    // original PubSubException within it.
                    throw new ServiceDownException(failureException);
                }
            }
        }
    }

    // Private method that holds the common logic for doing asynchronous
    // Subscribe or Unsubscribe requests. This is for code reuse since these two
    // flows are very similar. The assumption is that the input OperationType is
    // either SUBSCRIBE or UNSUBSCRIBE.
    private void asyncSubUnsub(ByteString topic, ByteString subscriberId, Callback<Void> callback, Object context,
                               OperationType operationType, CreateOrAttach createOrAttach) {
        if (logger.isDebugEnabled())
            logger.debug("Calling an async subUnsub request for topic: " + topic.toStringUtf8() + ", subscriberId: "
                         + subscriberId.toStringUtf8() + ", operationType: " + operationType + ", createOrAttach: "
                         + createOrAttach);
        // Check if we know which server host is the master for the topic we are
        // subscribing to.
        PubSubData pubSubData = new PubSubData(topic, null, subscriberId, operationType, createOrAttach, callback,
                                               context);
        if (client.topic2Host.containsKey(topic)) {
            InetSocketAddress host = client.topic2Host.get(topic);
            if (operationType.equals(OperationType.UNSUBSCRIBE) && client.getPublisher().host2Channel.containsKey(host)) {
                // For unsubscribes, we can reuse the channel connections to the
                // server host that are cached for publishes. For publish and
                // unsubscribe flows, we will thus use the same Channels and
                // will cache and store them during the ConnectCallback.
                doSubUnsub(pubSubData, client.getPublisher().host2Channel.get(host));
            } else {
                // We know which server host is the master for the topic so
                // connect to that first. For subscribes, we want a new channel
                // connection each time for the TopicSubscriber. If the
                // TopicSubscriber is already connected and subscribed,
                // we assume the server will respond with an appropriate status
                // indicating this. For unsubscribes, it is possible that the
                // client is subscribed to the topic already but does not
                // have a Channel connection yet to the server host. e.g. Client
                // goes down and comes back up but client side soft state memory
                // does not have the netty Channel connection anymore.
                client.doConnect(pubSubData, host);
            }
        } else {
            // Server host for the given topic is not known yet so use the
            // default server host/port as defined in the configs. This should
            // point to the server VIP which would redirect to a random server
            // (which might not be the server hosting the topic).
            client.doConnect(pubSubData, cfg.getDefaultServerHost());
        }
    }

    public void subscribe(ByteString topic, ByteString subscriberId, CreateOrAttach mode)
            throws CouldNotConnectException, ClientAlreadySubscribedException, ServiceDownException,
        InvalidSubscriberIdException {
        subscribe(topic, subscriberId, mode, false);
    }

    protected void subscribe(ByteString topic, ByteString subscriberId, CreateOrAttach mode, boolean isHub)
            throws CouldNotConnectException, ClientAlreadySubscribedException, ServiceDownException,
        InvalidSubscriberIdException {
        // Validate that the format of the subscriberId is valid either as a
        // local or hub subscriber.
        if (!isValidSubscriberId(subscriberId, isHub)) {
            throw new InvalidSubscriberIdException("SubscriberId passed is not valid: " + subscriberId.toStringUtf8()
                                                   + ", isHub: " + isHub);
        }
        try {
            subUnsub(topic, subscriberId, OperationType.SUBSCRIBE, mode);
        } catch (ClientNotSubscribedException e) {
            logger.error("Unexpected Exception thrown: " + e.toString());
            // This exception should never be thrown here. But just in case,
            // throw a generic ServiceDownException but wrap the original
            // Exception within it.
            throw new ServiceDownException(e);
        }
    }

    public void asyncSubscribe(ByteString topic, ByteString subscriberId, CreateOrAttach mode, Callback<Void> callback,
                               Object context) {
        asyncSubscribe(topic, subscriberId, mode, callback, context, false);
    }

    protected void asyncSubscribe(ByteString topic, ByteString subscriberId, CreateOrAttach mode,
                                  Callback<Void> callback, Object context, boolean isHub) {
        // Validate that the format of the subscriberId is valid either as a
        // local or hub subscriber.
        if (!isValidSubscriberId(subscriberId, isHub)) {
            callback.operationFailed(context, new ServiceDownException(new InvalidSubscriberIdException(
                                         "SubscriberId passed is not valid: " + subscriberId.toStringUtf8() + ", isHub: " + isHub)));
            return;
        }
        asyncSubUnsub(topic, subscriberId, callback, context, OperationType.SUBSCRIBE, mode);
    }

    public void unsubscribe(ByteString topic, ByteString subscriberId) throws CouldNotConnectException,
        ClientNotSubscribedException, ServiceDownException, InvalidSubscriberIdException {
        unsubscribe(topic, subscriberId, false);
    }

    protected void unsubscribe(ByteString topic, ByteString subscriberId, boolean isHub)
            throws CouldNotConnectException, ClientNotSubscribedException, ServiceDownException,
        InvalidSubscriberIdException {
        // Validate that the format of the subscriberId is valid either as a
        // local or hub subscriber.
        if (!isValidSubscriberId(subscriberId, isHub)) {
            throw new InvalidSubscriberIdException("SubscriberId passed is not valid: " + subscriberId.toStringUtf8()
                                                   + ", isHub: " + isHub);
        }
        // Synchronously close the subscription on the client side. Even
        // if the unsubscribe request to the server errors out, we won't be
        // delivering messages for this subscription to the client. The client
        // can later retry the unsubscribe request to the server so they are
        // "fully" unsubscribed from the given topic.
        closeSubscription(topic, subscriberId);
        try {
            subUnsub(topic, subscriberId, OperationType.UNSUBSCRIBE, null);
        } catch (ClientAlreadySubscribedException e) {
            logger.error("Unexpected Exception thrown: " + e.toString());
            // This exception should never be thrown here. But just in case,
            // throw a generic ServiceDownException but wrap the original
            // Exception within it.
            throw new ServiceDownException(e);
        }
    }

    public void asyncUnsubscribe(final ByteString topic, final ByteString subscriberId, final Callback<Void> callback,
                                 final Object context) {
        asyncUnsubscribe(topic, subscriberId, callback, context, false);
    }

    protected void asyncUnsubscribe(final ByteString topic, final ByteString subscriberId,
                                    final Callback<Void> callback, final Object context, boolean isHub) {
        // Validate that the format of the subscriberId is valid either as a
        // local or hub subscriber.
        if (!isValidSubscriberId(subscriberId, isHub)) {
            callback.operationFailed(context, new ServiceDownException(new InvalidSubscriberIdException(
                                         "SubscriberId passed is not valid: " + subscriberId.toStringUtf8() + ", isHub: " + isHub)));
            return;
        }
        // Asynchronously close the subscription. On the callback to that
        // operation once it completes, post the async unsubscribe request.
        asyncCloseSubscription(topic, subscriberId, new Callback<Void>() {
            @Override
            public void operationFinished(Object ctx, Void resultOfOperation) {
                asyncSubUnsub(topic, subscriberId, callback, context, OperationType.UNSUBSCRIBE, null);
            }

            @Override
            public void operationFailed(Object ctx, PubSubException exception) {
                callback.operationFailed(context, exception);
            }
        }, null);
    }

    // This is a helper method to determine if a subscriberId is valid as either
    // a hub or local subscriber
    private boolean isValidSubscriberId(ByteString subscriberId, boolean isHub) {
        if ((isHub && !SubscriptionStateUtils.isHubSubscriber(subscriberId))
                || (!isHub && SubscriptionStateUtils.isHubSubscriber(subscriberId)))
            return false;
        else
            return true;
    }

    public void consume(ByteString topic, ByteString subscriberId, MessageSeqId messageSeqId)
            throws ClientNotSubscribedException {
        if (logger.isDebugEnabled())
            logger.debug("Calling consume for topic: " + topic.toStringUtf8() + ", subscriberId: "
                         + subscriberId.toStringUtf8() + ", messageSeqId: " + messageSeqId);
        TopicSubscriber topicSubscriber = new TopicSubscriber(topic, subscriberId);
        // Check that this topic subscription on the client side exists.
        if (!topicSubscriber2Channel.containsKey(topicSubscriber)) {
            throw new ClientNotSubscribedException(
                "Cannot send consume message since client is not subscribed to topic: " + topic.toStringUtf8()
                + ", subscriberId: " + subscriberId.toStringUtf8());
        }
        PubSubData pubSubData = new PubSubData(topic, null, subscriberId, OperationType.CONSUME, null, null, null);
        // Send the consume message to the server using the same subscribe
        // channel that the topic subscription uses.
        doConsume(pubSubData, topicSubscriber2Channel.get(topicSubscriber), messageSeqId);
    }

    /**
     * This is a helper method to write the actual subscribe/unsubscribe message
     * once the client is connected to the server and a Channel is available.
     *
     * @param pubSubData
     *            Subscribe/Unsubscribe call's data wrapper object. We assume
     *            that the operationType field is either SUBSCRIBE or
     *            UNSUBSCRIBE.
     * @param channel
     *            Netty I/O channel for communication between the client and
     *            server
     */
    protected void doSubUnsub(PubSubData pubSubData, Channel channel) {
        // Create a PubSubRequest
        PubSubRequest.Builder pubsubRequestBuilder = PubSubRequest.newBuilder();
        pubsubRequestBuilder.setProtocolVersion(ProtocolVersion.VERSION_ONE);
        pubsubRequestBuilder.setType(pubSubData.operationType);
        if (pubSubData.triedServers != null && pubSubData.triedServers.size() > 0) {
            pubsubRequestBuilder.addAllTriedServers(pubSubData.triedServers);
        }
        long txnId = client.globalCounter.incrementAndGet();
        pubsubRequestBuilder.setTxnId(txnId);
        pubsubRequestBuilder.setShouldClaim(pubSubData.shouldClaim);
        pubsubRequestBuilder.setTopic(pubSubData.topic);

        // Create either the Subscribe or Unsubscribe Request
        if (pubSubData.operationType.equals(OperationType.SUBSCRIBE)) {
            // Create the SubscribeRequest
            SubscribeRequest.Builder subscribeRequestBuilder = SubscribeRequest.newBuilder();
            subscribeRequestBuilder.setSubscriberId(pubSubData.subscriberId);
            subscribeRequestBuilder.setCreateOrAttach(pubSubData.createOrAttach);
            // For now, all subscribes should wait for all cross-regional
            // subscriptions to be established before returning.
            subscribeRequestBuilder.setSynchronous(true);

            // Set the SubscribeRequest into the outer PubSubRequest
            pubsubRequestBuilder.setSubscribeRequest(subscribeRequestBuilder);
        } else {
            // Create the UnSubscribeRequest
            UnsubscribeRequest.Builder unsubscribeRequestBuilder = UnsubscribeRequest.newBuilder();
            unsubscribeRequestBuilder.setSubscriberId(pubSubData.subscriberId);

            // Set the UnsubscribeRequest into the outer PubSubRequest
            pubsubRequestBuilder.setUnsubscribeRequest(unsubscribeRequestBuilder);
        }

        // Update the PubSubData with the txnId and the requestWriteTime
        pubSubData.txnId = txnId;
        pubSubData.requestWriteTime = System.currentTimeMillis();

        // Before we do the write, store this information into the
        // ResponseHandler so when the server responds, we know what
        // appropriate Callback Data to invoke for the given txn ID.
        HedwigClientImpl.getResponseHandlerFromChannel(channel).txn2PubSubData.put(txnId, pubSubData);

        // Finally, write the Subscribe request through the Channel.
        if (logger.isDebugEnabled())
            logger.debug("Writing a SubUnsub request to host: " + HedwigClientImpl.getHostFromChannel(channel)
                         + " for pubSubData: " + pubSubData);
        ChannelFuture future = channel.write(pubsubRequestBuilder.build());
        future.addListener(new WriteCallback(pubSubData, client));
    }

    /**
     * This is a helper method to write a consume message to the server after a
     * subscribe Channel connection is made to the server and messages are being
     * consumed by the client.
     *
     * @param pubSubData
     *            Consume call's data wrapper object. We assume that the
     *            operationType field is CONSUME.
     * @param channel
     *            Netty I/O channel for communication between the client and
     *            server
     * @param messageSeqId
     *            Message Seq ID for the latest/last message the client has
     *            consumed.
     */
    public void doConsume(final PubSubData pubSubData, final Channel channel, final MessageSeqId messageSeqId) {
        // Create a PubSubRequest
        PubSubRequest.Builder pubsubRequestBuilder = PubSubRequest.newBuilder();
        pubsubRequestBuilder.setProtocolVersion(ProtocolVersion.VERSION_ONE);
        pubsubRequestBuilder.setType(OperationType.CONSUME);
        long txnId = client.globalCounter.incrementAndGet();
        pubsubRequestBuilder.setTxnId(txnId);
        pubsubRequestBuilder.setTopic(pubSubData.topic);

        // Create the ConsumeRequest
        ConsumeRequest.Builder consumeRequestBuilder = ConsumeRequest.newBuilder();
        consumeRequestBuilder.setSubscriberId(pubSubData.subscriberId);
        consumeRequestBuilder.setMsgId(messageSeqId);

        // Set the ConsumeRequest into the outer PubSubRequest
        pubsubRequestBuilder.setConsumeRequest(consumeRequestBuilder);

        // For Consume requests, we will send them from the client in a fire and
        // forget manner. We are not expecting the server to send back an ack
        // response so no need to register this in the ResponseHandler. There
        // are no callbacks to invoke since this isn't a client initiated
        // action. Instead, just have a future listener that will log an error
        // message if there was a problem writing the consume request.
        if (logger.isDebugEnabled())
            logger.debug("Writing a Consume request to host: " + HedwigClientImpl.getHostFromChannel(channel)
                         + " with messageSeqId: " + messageSeqId + " for pubSubData: " + pubSubData);
        ChannelFuture future = channel.write(pubsubRequestBuilder.build());
        future.addListener(new ChannelFutureListener() {
            @Override
            public void operationComplete(ChannelFuture future) throws Exception {
                if (!future.isSuccess()) {
                    logger.error("Error writing a Consume request to host: " + HedwigClientImpl.getHostFromChannel(channel)
                                 + " with messageSeqId: " + messageSeqId + " for pubSubData: " + pubSubData);
                }
            }
        });

    }

    public boolean hasSubscription(ByteString topic, ByteString subscriberId) throws CouldNotConnectException,
        ServiceDownException {
        // The subscription type of info should be stored on the server end, not
        // the client side. Eventually, the server will have the Subscription
        // Manager part that ties into Zookeeper to manage this info.
        // Commenting out these type of API's related to that here for now until
        // this data is available on the server. Will figure out what the
        // correct way to contact the server to get this info is then.
        // The client side just has soft memory state for client subscription
        // information.
        return topicSubscriber2Channel.containsKey(new TopicSubscriber(topic, subscriberId));
    }

    public List<ByteString> getSubscriptionList(ByteString subscriberId) throws CouldNotConnectException,
        ServiceDownException {
        // Same as the previous hasSubscription method, this data should reside
        // on the server end, not the client side.
        return null;
    }

    public void startDelivery(final ByteString topic, final ByteString subscriberId, MessageHandler messageHandler)
            throws ClientNotSubscribedException {
        if (logger.isDebugEnabled())
            logger.debug("Starting delivery for topic: " + topic.toStringUtf8() + ", subscriberId: "
                         + subscriberId.toStringUtf8());
        TopicSubscriber topicSubscriber = new TopicSubscriber(topic, subscriberId);
        // Make sure we know about this topic subscription on the client side
        // exists. The assumption is that the client should have in memory the
        // Channel created for the TopicSubscriber once the server has sent
        // an ack response to the initial subscribe request.
        if (!topicSubscriber2Channel.containsKey(topicSubscriber)) {
            logger.error("Client is not yet subscribed to topic: " + topic.toStringUtf8() + ", subscriberId: "
                         + subscriberId.toStringUtf8());
            throw new ClientNotSubscribedException("Client is not yet subscribed to topic: " + topic.toStringUtf8()
                                                   + ", subscriberId: " + subscriberId.toStringUtf8());
        }

        // Register the MessageHandler with the subscribe Channel's
        // Response Handler.
        Channel topicSubscriberChannel = topicSubscriber2Channel.get(topicSubscriber);
        HedwigClientImpl.getResponseHandlerFromChannel(topicSubscriberChannel).getSubscribeResponseHandler()
        .setMessageHandler(messageHandler);
        // Now make the TopicSubscriber Channel readable (it is set to not be
        // readable when the initial subscription is done). Note that this is an
        // asynchronous call. If this fails (not likely), the futureListener
        // will just log an error message for now.
        ChannelFuture future = topicSubscriberChannel.setReadable(true);
        future.addListener(new ChannelFutureListener() {
            @Override
            public void operationComplete(ChannelFuture future) throws Exception {
                if (!future.isSuccess()) {
                    logger.error("Unable to make subscriber Channel readable in startDelivery call for topic: "
                                 + topic.toStringUtf8() + ", subscriberId: " + subscriberId.toStringUtf8());
                }
            }
        });
    }

    public void stopDelivery(final ByteString topic, final ByteString subscriberId) throws ClientNotSubscribedException {
        if (logger.isDebugEnabled())
            logger.debug("Stopping delivery for topic: " + topic.toStringUtf8() + ", subscriberId: "
                         + subscriberId.toStringUtf8());
        TopicSubscriber topicSubscriber = new TopicSubscriber(topic, subscriberId);
        // Make sure we know that this topic subscription on the client side
        // exists. The assumption is that the client should have in memory the
        // Channel created for the TopicSubscriber once the server has sent
        // an ack response to the initial subscribe request.
        if (!topicSubscriber2Channel.containsKey(topicSubscriber)) {
            logger.error("Client is not yet subscribed to topic: " + topic.toStringUtf8() + ", subscriberId: "
                         + subscriberId.toStringUtf8());
            throw new ClientNotSubscribedException("Client is not yet subscribed to topic: " + topic.toStringUtf8()
                                                   + ", subscriberId: " + subscriberId.toStringUtf8());
        }

        // Unregister the MessageHandler for the subscribe Channel's
        // Response Handler.
        Channel topicSubscriberChannel = topicSubscriber2Channel.get(topicSubscriber);
        HedwigClientImpl.getResponseHandlerFromChannel(topicSubscriberChannel).getSubscribeResponseHandler()
        .setMessageHandler(null);
        // Now make the TopicSubscriber channel not-readable. This will buffer
        // up messages if any are sent from the server. Note that this is an
        // asynchronous call. If this fails (not likely), the futureListener
        // will just log an error message for now.
        ChannelFuture future = topicSubscriberChannel.setReadable(false);
        future.addListener(new ChannelFutureListener() {
            @Override
            public void operationComplete(ChannelFuture future) throws Exception {
                if (!future.isSuccess()) {
                    logger.error("Unable to make subscriber Channel not readable in stopDelivery call for topic: "
                                 + topic.toStringUtf8() + ", subscriberId: " + subscriberId.toStringUtf8());
                }
            }
        });
    }

    public void closeSubscription(ByteString topic, ByteString subscriberId) throws ServiceDownException {
        PubSubData pubSubData = new PubSubData(topic, null, subscriberId, null, null, null, null);
        synchronized (pubSubData) {
            PubSubCallback pubSubCallback = new PubSubCallback(pubSubData);
            asyncCloseSubscription(topic, subscriberId, pubSubCallback, null);
            try {
                while (!pubSubData.isDone)
                    pubSubData.wait();
            } catch (InterruptedException e) {
                throw new ServiceDownException("Interrupted Exception while waiting for asyncCloseSubscription call");
            }
            // Check from the PubSubCallback if it was successful or not.
            if (!pubSubCallback.getIsCallSuccessful()) {
                throw new ServiceDownException("Exception while trying to close the subscription for topic: "
                                               + topic.toStringUtf8() + ", subscriberId: " + subscriberId.toStringUtf8());
            }
        }
    }

    public void asyncCloseSubscription(final ByteString topic, final ByteString subscriberId,
                                       final Callback<Void> callback, final Object context) {
        if (logger.isDebugEnabled())
            logger.debug("Closing subscription asynchronously for topic: " + topic.toStringUtf8() + ", subscriberId: "
                         + subscriberId.toStringUtf8());
        TopicSubscriber topicSubscriber = new TopicSubscriber(topic, subscriberId);
        if (topicSubscriber2Channel.containsKey(topicSubscriber)) {
            // Remove all cached references for the TopicSubscriber
            Channel channel = topicSubscriber2Channel.get(topicSubscriber);
            topicSubscriber2Channel.remove(topicSubscriber);
            // Close the subscribe channel asynchronously.
            HedwigClientImpl.getResponseHandlerFromChannel(channel).channelClosedExplicitly = true;
            ChannelFuture future = channel.close();
            future.addListener(new ChannelFutureListener() {
                @Override
                public void operationComplete(ChannelFuture future) throws Exception {
                    if (!future.isSuccess()) {
                        logger.error("Failed to close the subscription channel for topic: " + topic.toStringUtf8()
                                     + ", subscriberId: " + subscriberId.toStringUtf8());
                        callback.operationFailed(context, new ServiceDownException(
                                                     "Failed to close the subscription channel for topic: " + topic.toStringUtf8()
                                                     + ", subscriberId: " + subscriberId.toStringUtf8()));
                    } else {
                        callback.operationFinished(context, null);
                    }
                }
            });
        } else {
            logger.warn("Trying to close a subscription when we don't have a subscribe channel cached for topic: "
                        + topic.toStringUtf8() + ", subscriberId: " + subscriberId.toStringUtf8());
            callback.operationFinished(context, null);
        }
    }

    // Public getter and setters for entries in the topic2Host Map.
    // This is used for classes that need this information but are not in the
    // same classpath.
    public Channel getChannelForTopic(TopicSubscriber topic) {
        return topicSubscriber2Channel.get(topic);
    }

    public void setChannelForTopic(TopicSubscriber topic, Channel channel) {
        topicSubscriber2Channel.put(topic, channel);
    }

    public void removeChannelForTopic(TopicSubscriber topic) {
        topicSubscriber2Channel.remove(topic);
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/ResponseHandler.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty;

import java.net.InetSocketAddress;
import java.util.LinkedList;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ConcurrentMap;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;
import org.jboss.netty.channel.ChannelHandlerContext;
import org.jboss.netty.channel.ChannelPipelineCoverage;
import org.jboss.netty.channel.ChannelStateEvent;
import org.jboss.netty.channel.ExceptionEvent;
import org.jboss.netty.channel.MessageEvent;
import org.jboss.netty.channel.SimpleChannelHandler;
import org.jboss.netty.handler.ssl.SslHandler;

import com.google.protobuf.ByteString;
import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.client.exceptions.ServerRedirectLoopException;
import org.apache.hedwig.client.exceptions.TooManyServerRedirectsException;
import org.apache.hedwig.client.handlers.PublishResponseHandler;
import org.apache.hedwig.client.handlers.SubscribeReconnectCallback;
import org.apache.hedwig.client.handlers.SubscribeResponseHandler;
import org.apache.hedwig.client.handlers.UnsubscribeResponseHandler;
import org.apache.hedwig.exceptions.PubSubException.ServiceDownException;
import org.apache.hedwig.exceptions.PubSubException.UncertainStateException;
import org.apache.hedwig.protocol.PubSubProtocol.OperationType;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse;
import org.apache.hedwig.protocol.PubSubProtocol.StatusCode;
import org.apache.hedwig.util.HedwigSocketAddress;

@ChannelPipelineCoverage("all")
public class ResponseHandler extends SimpleChannelHandler {

    private static Logger logger = LoggerFactory.getLogger(ResponseHandler.class);

    // Concurrent Map to store for each async PubSub request, the txn ID
    // and the corresponding PubSub call's data which stores the VoidCallback to
    // invoke when we receive a PubSub ack response from the server.
    // This is specific to this instance of the ResponseHandler which is
    // tied to a specific netty Channel Pipeline.
    protected final ConcurrentMap<Long, PubSubData> txn2PubSubData = new ConcurrentHashMap<Long, PubSubData>();

    // Boolean indicating if we closed the channel this ResponseHandler is
    // attached to explicitly or not. If so, we do not need to do the
    // channel disconnected logic here.
    public boolean channelClosedExplicitly = false;

    private final HedwigClientImpl client;
    private final HedwigPublisher pub;
    private final HedwigSubscriber sub;
    private final ClientConfiguration cfg;

    private final PublishResponseHandler pubHandler;
    private final SubscribeResponseHandler subHandler;
    private final UnsubscribeResponseHandler unsubHandler;

    public ResponseHandler(HedwigClientImpl client) {
        this.client = client;
        this.sub = client.getSubscriber();
        this.pub = client.getPublisher();
        this.cfg = client.getConfiguration();
        this.pubHandler = new PublishResponseHandler(this);
        this.subHandler = new SubscribeResponseHandler(this);
        this.unsubHandler = new UnsubscribeResponseHandler(this);
    }

    // Public getters needed for the private members
    public HedwigClientImpl getClient() {
        return client;
    }

    public HedwigSubscriber getSubscriber() {
        return sub;
    }

    public ClientConfiguration getConfiguration() {
        return cfg;
    }

    public SubscribeResponseHandler getSubscribeResponseHandler() {
        return subHandler;
    }

    @Override
    public void messageReceived(ChannelHandlerContext ctx, MessageEvent e) throws Exception {
        // If the Message is not a PubSubResponse, just send it upstream and let
        // something else handle it.
        if (!(e.getMessage() instanceof PubSubResponse)) {
            ctx.sendUpstream(e);
        }
        // Retrieve the PubSubResponse from the Message that was sent by the
        // server.
        PubSubResponse response = (PubSubResponse) e.getMessage();
        if (logger.isDebugEnabled())
            logger.debug("Response received from host: " + HedwigClientImpl.getHostFromChannel(ctx.getChannel())
                         + ", response: " + response);

        // Determine if this PubSubResponse is an ack response for a PubSub
        // Request or if it is a message being pushed to the client subscriber.
        if (response.hasMessage()) {
            // Subscribed messages being pushed to the client so handle/consume
            // it and return.
            subHandler.handleSubscribeMessage(response);
            return;
        }

        // Response is an ack to a prior PubSubRequest so first retrieve the
        // PubSub data for this txn.
        PubSubData pubSubData = txn2PubSubData.containsKey(response.getTxnId()) ? txn2PubSubData.get(response
                                .getTxnId()) : null;
        // Validate that the PubSub data for this txn is stored. If not, just
        // log an error message and return since we don't know how to handle
        // this.
        if (pubSubData == null) {
            logger.error("PubSub Data was not found for PubSubResponse: " + response);
            return;
        }

        // Now that we've retrieved the PubSubData for this specific Txn ID, we
        // can remove it from the Map.
        txn2PubSubData.remove(response.getTxnId());

        // Store the topic2Host mapping if this wasn't a server redirect. We'll
        // assume that if the server was able to have an open Channel connection
        // to the client, and responded with an ack message other than the
        // NOT_RESPONSIBLE_FOR_TOPIC one, it is the correct topic master.
        if (!response.getStatusCode().equals(StatusCode.NOT_RESPONSIBLE_FOR_TOPIC)) {
            client.storeTopic2HostMapping(pubSubData, ctx.getChannel());
        }

        // Depending on the operation type, call the appropriate handler.
        switch (pubSubData.operationType) {
        case PUBLISH:
            pubHandler.handlePublishResponse(response, pubSubData, ctx.getChannel());
            break;
        case SUBSCRIBE:
            subHandler.handleSubscribeResponse(response, pubSubData, ctx.getChannel());
            break;
        case UNSUBSCRIBE:
            unsubHandler.handleUnsubscribeResponse(response, pubSubData, ctx.getChannel());
            break;
        default:
            // The above are the only expected PubSubResponse messages received
            // from the server for the various client side requests made.
            logger.error("Response received from server is for an unhandled operation type, txnId: "
                         + response.getTxnId() + ", operationType: " + pubSubData.operationType);
        }
    }

    /**
     * Logic to repost a PubSubRequest when the server responds with a redirect
     * indicating they are not the topic master.
     *
     * @param response
     *            PubSubResponse from the server for the redirect
     * @param pubSubData
     *            PubSubData for the original PubSubRequest made
     * @param channel
     *            Channel Channel we used to make the original PubSubRequest
     * @throws Exception
     *             Throws an exception if there was an error in doing the
     *             redirect repost of the PubSubRequest
     */
    public void handleRedirectResponse(PubSubResponse response, PubSubData pubSubData, Channel channel)
            throws Exception {
        if (logger.isDebugEnabled())
            logger.debug("Handling a redirect from host: " + HedwigClientImpl.getHostFromChannel(channel) + ", response: "
                         + response + ", pubSubData: " + pubSubData);
        // In this case, the PubSub request was done to a server that is not
        // responsible for the topic. First make sure that we haven't
        // exceeded the maximum number of server redirects.
        int curNumServerRedirects = (pubSubData.triedServers == null) ? 0 : pubSubData.triedServers.size();
        if (curNumServerRedirects >= cfg.getMaximumServerRedirects()) {
            // We've already exceeded the maximum number of server redirects
            // so consider this as an error condition for the client.
            // Invoke the operationFailed callback and just return.
            if (logger.isDebugEnabled())
                logger.debug("Exceeded the number of server redirects (" + curNumServerRedirects + ") so error out.");
            pubSubData.callback.operationFailed(pubSubData.context, new ServiceDownException(
                                                    new TooManyServerRedirectsException("Already reached max number of redirects: "
                                                            + curNumServerRedirects)));
            return;
        }

        // We will redirect and try to connect to the correct server
        // stored in the StatusMsg of the response. First store the
        // server that we sent the PubSub request to for the topic.
        ByteString triedServer = ByteString.copyFromUtf8(HedwigSocketAddress.sockAddrStr(HedwigClientImpl
                                 .getHostFromChannel(channel)));
        if (pubSubData.triedServers == null)
            pubSubData.triedServers = new LinkedList<ByteString>();
        pubSubData.shouldClaim = true;
        pubSubData.triedServers.add(triedServer);

        // Now get the redirected server host (expected format is
        // Hostname:Port:SSLPort) from the server's response message. If one is
        // not given for some reason, then redirect to the default server
        // host/VIP to repost the request.
        String statusMsg = response.getStatusMsg();
        InetSocketAddress redirectedHost;
        if (statusMsg != null && statusMsg.length() > 0) {
            if (cfg.isSSLEnabled()) {
                redirectedHost = new HedwigSocketAddress(statusMsg).getSSLSocketAddress();
            } else {
                redirectedHost = new HedwigSocketAddress(statusMsg).getSocketAddress();
            }
        } else {
            redirectedHost = cfg.getDefaultServerHost();
        }

        // Make sure the redirected server is not one we've already attempted
        // already before in this PubSub request.
        if (pubSubData.triedServers.contains(ByteString.copyFromUtf8(HedwigSocketAddress.sockAddrStr(redirectedHost)))) {
            logger.error("We've already sent this PubSubRequest before to redirectedHost: " + redirectedHost
                         + ", pubSubData: " + pubSubData);
            pubSubData.callback.operationFailed(pubSubData.context, new ServiceDownException(
                                                    new ServerRedirectLoopException("Already made the request before to redirected host: "
                                                            + redirectedHost)));
            return;
        }

        // Check if we already have a Channel open to the redirected server
        // host.
        boolean redirectedHostChannelExists = pub.host2Channel.containsKey(redirectedHost) ? true : false;
        if (pubSubData.operationType.equals(OperationType.SUBSCRIBE) || !redirectedHostChannelExists) {
            // We don't have an existing channel to the redirected host OR this
            // is a redirected Subscribe request. For Subscribe requests, we
            // always want to create a new unique Channel connection to the
            // topic master server for the TopicSubscriber.
            client.doConnect(pubSubData, redirectedHost);
        } else {
            // For Publish and Unsubscribe requests, we can just post the
            // request again directly on the existing cached redirected host
            // channel.
            if (pubSubData.operationType.equals(OperationType.PUBLISH)) {
                pub.doPublish(pubSubData, pub.host2Channel.get(redirectedHost));
            } else if (pubSubData.operationType.equals(OperationType.UNSUBSCRIBE)) {
                sub.doSubUnsub(pubSubData, pub.host2Channel.get(redirectedHost));
            }
        }
    }

    // Logic to deal with what happens when a Channel to a server host is
    // disconnected.
    @Override
    public void channelDisconnected(ChannelHandlerContext ctx, ChannelStateEvent e) throws Exception {
        // If this channel was closed explicitly by the client code,
        // we do not need to do any of this logic. This could happen
        // for redundant Publish channels created or redirected subscribe
        // channels that are not used anymore or when we shutdown the
        // client and manually close all of the open channels.
        // Also don't do any of the disconnect logic if the client has stopped.
        if (channelClosedExplicitly || client.hasStopped())
            return;

        // Make sure the host retrieved is not null as there could be some weird
        // channel disconnect events happening during a client shutdown.
        // If it is, just return as there shouldn't be anything we need to do.
        InetSocketAddress host = HedwigClientImpl.getHostFromChannel(ctx.getChannel());
        logger.warn("Channel was disconnected to host: " + host);
        if (host == null)
            return;

        // If this Channel was used for Publish and Unsubscribe flows, just
        // remove it from the HewdigPublisher's host2Channel map. We will
        // re-establish a Channel connection to that server when the next
        // publish/unsubscribe request to a topic that the server owns occurs.
        PubSubData origSubData = subHandler.getOrigSubData();

        // Now determine what type of operation this channel was used for.
        if (origSubData == null) {
            // Only remove the Channel from the mapping if this current
            // disconnected channel is the same as the cached entry.
            // Due to race concurrency situations, it is possible to
            // create multiple channels to the same host for publish
            // and unsubscribe requests.
            if (pub.host2Channel.containsKey(host) && pub.host2Channel.get(host).equals(ctx.getChannel())) {
                if (logger.isDebugEnabled())
                    logger.debug("Disconnected channel for host: " + host
                                 + " was for Publish/Unsubscribe requests so remove all references to it.");
                pub.host2Channel.remove(host);
                client.clearAllTopicsForHost(host);
            }
        } else {
            // Subscribe channel disconnected so first close and clear all
            // cached Channel data set up for this topic subscription.
            sub.closeSubscription(origSubData.topic, origSubData.subscriberId);
            client.clearAllTopicsForHost(host);
            // Since the connection to the server host that was responsible
            // for the topic died, we are not sure about the state of that
            // server. Resend the original subscribe request data to the default
            // server host/VIP. Also clear out all of the servers we've
            // contacted or attempted to from this request as we are starting a
            // "fresh" subscribe request.
            origSubData.clearServersList();
            // Set a new type of VoidCallback for this async call. We need this
            // hook so after the subscribe reconnect has completed, delivery for
            // that topic subscriber should also be restarted (if it was that
            // case before the channel disconnect).
            origSubData.callback = new SubscribeReconnectCallback(origSubData, client, subHandler.getMessageHandler());
            origSubData.context = null;
            if (logger.isDebugEnabled())
                logger.debug("Disconnected subscribe channel so reconnect with origSubData: " + origSubData);
            client.doConnect(origSubData, cfg.getDefaultServerHost());
        }

        // Finally, all of the PubSubRequests that are still waiting for an ack
        // response from the server need to be removed and timed out. Invoke the
        // operationFailed callbacks on all of them. Use the
        // UncertainStateException since the server did receive the request but
        // we're not sure of the state of the request since the ack response was
        // never received.
        for (PubSubData pubSubData : txn2PubSubData.values()) {
            if (logger.isDebugEnabled())
                logger.debug("Channel disconnected so invoking the operationFailed callback for pubSubData: "
                             + pubSubData);
            pubSubData.callback.operationFailed(pubSubData.context, new UncertainStateException(
                                                    "Server ack response never received before server connection disconnected!"));
        }
        txn2PubSubData.clear();
    }

    // Logic to deal with what happens when a Channel to a server host is
    // connected. This is needed if the client is using an SSL port to
    // communicate with the server. If so, we need to do the SSL handshake here
    // when the channel is first connected.
    @Override
    public void channelConnected(ChannelHandlerContext ctx, ChannelStateEvent e) throws Exception {
        // No need to initiate the SSL handshake if we are closing this channel
        // explicitly or the client has been stopped.
        if (cfg.isSSLEnabled() && !channelClosedExplicitly && !client.hasStopped()) {
            if (logger.isDebugEnabled()) {
                logger.debug("Initiating the SSL handshake");
            }
            ctx.getPipeline().get(SslHandler.class).handshake(e.getChannel());
        }
    }

    @Override
    public void exceptionCaught(ChannelHandlerContext ctx, ExceptionEvent e) {
        logger.error("Exception caught on client channel", e.getCause());
        e.getChannel().close();
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/WriteCallback.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty;

import java.net.InetSocketAddress;
import java.util.LinkedList;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.ChannelFuture;
import org.jboss.netty.channel.ChannelFutureListener;

import com.google.protobuf.ByteString;
import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.exceptions.PubSubException.ServiceDownException;
import org.apache.hedwig.util.HedwigSocketAddress;

public class WriteCallback implements ChannelFutureListener {

    private static Logger logger = LoggerFactory.getLogger(WriteCallback.class);

    // Private member variables
    private PubSubData pubSubData;
    private final HedwigClientImpl client;
    private final ClientConfiguration cfg;

    // Constructor
    public WriteCallback(PubSubData pubSubData, HedwigClientImpl client) {
        super();
        this.pubSubData = pubSubData;
        this.client = client;
        this.cfg = client.getConfiguration();
    }

    public void operationComplete(ChannelFuture future) throws Exception {
        // If the client has stopped, there is no need to proceed
        // with any callback logic here.
        if (client.hasStopped())
            return;

        // When the write operation to the server is done, we just need to check
        // if it was successful or not.
        InetSocketAddress host = HedwigClientImpl.getHostFromChannel(future.getChannel());
        if (!future.isSuccess()) {
            logger.error("Error writing on channel to host: " + host);
            // On a write failure for a PubSubRequest, we also want to remove
            // the saved txnId to PubSubData in the ResponseHandler. These
            // requests will not receive an ack response from the server
            // so there is no point storing that information there anymore.
            HedwigClientImpl.getResponseHandlerFromChannel(future.getChannel()).txn2PubSubData.remove(pubSubData.txnId);

            // If we were not able to write on the channel to the server host,
            // the host could have died or something is wrong with the channel
            // connection where we can connect to the host, but not write to it.
            ByteString hostString = (host == null) ? null : ByteString.copyFromUtf8(HedwigSocketAddress.sockAddrStr(host));
            if (pubSubData.writeFailedServers != null && pubSubData.writeFailedServers.contains(hostString)) {
                // We've already tried to write to this server previously and
                // failed, so invoke the operationFailed callback.
                logger.error("Error writing to host more than once so just invoke the operationFailed callback!");
                pubSubData.callback.operationFailed(pubSubData.context, new ServiceDownException(
                                                        "Error while writing message to server: " + hostString));
            } else {
                if (logger.isDebugEnabled())
                    logger.debug("Try to send the PubSubRequest again to the default server host/VIP for pubSubData: "
                                 + pubSubData);
                // Keep track of this current server that we failed to write to
                // but retry the request on the default server host/VIP.
                if (pubSubData.writeFailedServers == null)
                    pubSubData.writeFailedServers = new LinkedList<ByteString>();
                pubSubData.writeFailedServers.add(hostString);
                client.doConnect(pubSubData, cfg.getDefaultServerHost());
            }
        } else {
            // Now that the write to the server is done, we have to wait for it
            // to respond. The ResponseHandler will take care of the ack
            // response from the server before we can determine if the async
            // PubSub call has really completed successfully or not.
            if (logger.isDebugEnabled())
                logger.debug("Successfully wrote to host: " + host + " for pubSubData: " + pubSubData);
        }
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/ssl/SslClientContextFactory.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.ssl;

import javax.net.ssl.SSLContext;

import org.apache.hedwig.client.conf.ClientConfiguration;

public class SslClientContextFactory extends SslContextFactory {

    public SslClientContextFactory(ClientConfiguration cfg) {
        try {
            // Create the SSL context.
            ctx = SSLContext.getInstance("TLS");
            ctx.init(null, getTrustManagers(), null);
        } catch (Exception ex) {
            throw new RuntimeException(ex);
        }
    }

    @Override
    protected boolean isClient() {
        return true;
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/ssl/SslContextFactory.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.ssl;

import java.security.cert.CertificateException;
import java.security.cert.X509Certificate;

import javax.net.ssl.SSLContext;
import javax.net.ssl.SSLEngine;
import javax.net.ssl.TrustManager;
import javax.net.ssl.X509TrustManager;

public abstract class SslContextFactory {

    protected SSLContext ctx;

    public SSLContext getContext() {
        return ctx;
    }

    protected abstract boolean isClient();

    public SSLEngine getEngine() {
        SSLEngine engine = ctx.createSSLEngine();
        engine.setUseClientMode(isClient());
        return engine;
    }

    protected TrustManager[] getTrustManagers() {
        return new TrustManager[] { new X509TrustManager() {
                // Always trust, even if invalid.

                @Override
                public X509Certificate[] getAcceptedIssuers() {
                    return new X509Certificate[0];
                }

                @Override
                public void checkServerTrusted(X509Certificate[] chain, String authType) throws CertificateException {
                    // Always trust.
                }

                @Override
                public void checkClientTrusted(X509Certificate[] chain, String authType) throws CertificateException {
                    // Always trust.
                }
            }
        };
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/conf/AbstractConfiguration.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.conf;

import java.net.URL;

import org.apache.commons.configuration.CompositeConfiguration;
import org.apache.commons.configuration.Configuration;
import org.apache.commons.configuration.ConfigurationException;
import org.apache.commons.configuration.PropertiesConfiguration;

public abstract class AbstractConfiguration {
    protected CompositeConfiguration conf;

    protected AbstractConfiguration() {
        conf = new CompositeConfiguration();
    }

    /**
     * Return real configuration object
     *
     * @return configuration
     */
    public Configuration getConf() {
        return conf;
    }

    /**
     * You can load configurations in precedence order. The first one takes
     * precedence over any loaded later.
     *
     * @param confURL
     */
    public void loadConf(URL confURL) throws ConfigurationException {
        Configuration loadedConf = new PropertiesConfiguration(confURL);
        conf.addConfiguration(loadedConf);

    }
}
"
hedwig-client/src/main/java/org/apache/hedwig/util/Callback.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.util;

import org.apache.hedwig.exceptions.PubSubException;

/**
 * This class is used for callbacks for asynchronous operations
 *
 */
public interface Callback<T> {

    /**
     * This method is called when the asynchronous operation finishes
     *
     * @param ctx
     * @param resultOfOperation
     */
    public abstract void operationFinished(Object ctx, T resultOfOperation);

    /**
     * This method is called when the operation failed due to some reason. The
     * reason for failure is passed in.
     *
     * @param ctx
     *            The context for the callback
     * @param exception
     *            The reason for the failure of the scan
     */
    public abstract void operationFailed(Object ctx, PubSubException exception);

}
"
hedwig-client/src/main/java/org/apache/hedwig/util/CallbackUtils.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.util;

import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.atomic.AtomicInteger;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.CompositeException;

public class CallbackUtils {

    /**
     * A callback that waits for all of a number of events to fire. If any fail,
     * then fail the final callback with a composite exception.
     *
     * TODO: change this to use any Exception and make CompositeException
     * generic, not a PubSubException.
     *
     * @param expected
     *            Number of expected callbacks.
     * @param cb
     *            The final callback to call.
     * @param ctx
     * @param logger
     *            May be null.
     * @param successMsg
     *            If not null, then this is logged on success.
     * @param failureMsg
     *            If not null, then this is logged on failure.
     * @param eagerErrorHandler
     *            If not null, then this will be executed after the first
     *            failure (but before the final failure callback). Useful for
     *            releasing resources, etc. as soon as we know the composite
     *            operation is doomed.
     * @return the generated callback
     */
    public static Callback<Void> multiCallback(final int expected, final Callback<Void> cb, final Object ctx,
            final Logger logger, final String successMsg, final String failureMsg,
            Runnable eagerErrorHandler) {
        if (expected == 0) {
            cb.operationFinished(ctx, null);
            return null;
        } else {
            return new Callback<Void>() {

                final AtomicInteger done = new AtomicInteger();
                final LinkedBlockingQueue<PubSubException> exceptions = new LinkedBlockingQueue<PubSubException>();

                private void tick() {
                    if (done.incrementAndGet() == expected) {
                        if (exceptions.isEmpty()) {
                            cb.operationFinished(ctx, null);
                        } else {
                            cb.operationFailed(ctx, new CompositeException(exceptions));
                        }
                    }
                }

                @Override
                public void operationFailed(Object ctx, PubSubException exception) {
                    if (logger != null && failureMsg != null)
                        logger.error(failureMsg, exception);
                    exceptions.add(exception);
                    tick();
                }

                @Override
                public void operationFinished(Object ctx, Void resultOfOperation) {
                    if (logger != null && successMsg != null)
                        logger.info(successMsg);
                    tick();
                }

            };
        }
    }

    /**
     * A callback that waits for all of a number of events to fire. If any fail,
     * then fail the final callback with a composite exception.
     */
    public static Callback<Void> multiCallback(int expected, Callback<Void> cb, Object ctx) {
        return multiCallback(expected, cb, ctx, null, null, null, null);
    }

    /**
     * A callback that waits for all of a number of events to fire. If any fail,
     * then fail the final callback with a composite exception.
     */
    public static Callback<Void> multinCallback(int expected, Callback<Void> cb, Object ctx, Runnable eagerErrorHandler) {
        return multiCallback(expected, cb, ctx, null, null, null, eagerErrorHandler);
    }

    private static Callback<Void> nop = new Callback<Void>() {

        @Override
        public void operationFailed(Object ctx, PubSubException exception) {
        }

        @Override
        public void operationFinished(Object ctx, Void resultOfOperation) {
        }

    };

    /**
     * A do-nothing callback.
     */
    public static Callback<Void> nop() {
        return nop;
    }

    /**
     * Logs what happened before continuing the callback chain.
     */
    public static <T> Callback<T> logger(final Logger logger, final String successMsg,
                                         final String failureMsg, final Callback<T> cont) {
        return new Callback<T>() {

            @Override
            public void operationFailed(Object ctx, PubSubException exception) {
                logger.error(failureMsg, exception);
                if (cont != null)
                    cont.operationFailed(ctx, exception);
            }

            @Override
            public void operationFinished(Object ctx, T resultOfOperation) {
                logger.info(successMsg);
                if (cont != null)
                    cont.operationFinished(ctx, resultOfOperation);
            }

        };
    }

    /**
     * Logs what happened (no continuation).
     */
    public static Callback<Void> logger(Logger logger, String successMsg, String failureMsg) {
        return logger(logger, successMsg, failureMsg, nop());
    }

    /**
     * Return a Callback<Void> that just calls the given Callback cb with the
     * bound result.
     */
    public static <T> Callback<Void> curry(final Callback<T> cb, final T result) {
        return new Callback<Void>() {

            @Override
            public void operationFailed(Object ctx, PubSubException exception) {
                cb.operationFailed(ctx, exception);
            }

            @Override
            public void operationFinished(Object ctx, Void resultOfOperation) {
                cb.operationFinished(ctx, result);
            }

        };
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/util/ConcurrencyUtils.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.util;

import java.util.concurrent.BlockingQueue;
import java.util.concurrent.CyclicBarrier;

public class ConcurrencyUtils {

    public static <T, U extends T, V extends BlockingQueue<T>> void put(V queue, U value) {
        try {
            queue.put(value);
        } catch (Exception ex) {
            throw new RuntimeException(ex);
        }
    }

    public static <T> T take(BlockingQueue<T> queue) {
        try {
            return queue.take();
        } catch (Exception ex) {
            throw new RuntimeException(ex);
        }
    }

    public static void await(CyclicBarrier barrier) {
        try {
            barrier.await();
        } catch (Exception ex) {
            throw new RuntimeException(ex);
        }
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/util/Either.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.util;

public class Either<T, U> {

    private T x;
    private U y;

    private Either(T x, U y) {
        this.x = x;
        this.y = y;
    }

    public static <T, U> Either<T, U> of(T x, U y) {
        return new Either<T, U>(x, y);
    }

    public static <T, U> Either<T, U> left(T x) {
        return new Either<T, U>(x, null);
    }

    public static <T, U> Either<T, U> right(U y) {
        return new Either<T, U>(null, y);
    }

    public T left() {
        return x;
    }

    public U right() {
        return y;
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/util/FileUtils.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.util;

import java.io.File;
import java.io.IOException;
import java.util.LinkedList;
import java.util.List;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class FileUtils {

    static DirDeleterThred dirDeleterThread;
    static Logger log = LoggerFactory.getLogger(FileUtils.class);

    static {
        dirDeleterThread = new DirDeleterThred();
        Runtime.getRuntime().addShutdownHook(dirDeleterThread);
    }

    public static File createTempDirectory(String prefix) throws IOException {
        return createTempDirectory(prefix, null);
    }

    public static File createTempDirectory(String prefix, String suffix) throws IOException {
        File tempDir = File.createTempFile(prefix, suffix);
        if (!tempDir.delete()) {
            throw new IOException("Could not delete temp file: " + tempDir.getAbsolutePath());
        }

        if (!tempDir.mkdir()) {
            throw new IOException("Could not create temp directory: " + tempDir.getAbsolutePath());
        }

        dirDeleterThread.addDirToDelete(tempDir);
        return tempDir;

    }

    static class DirDeleterThred extends Thread {
        List<File> dirsToDelete = new LinkedList<File>();

        public synchronized void addDirToDelete(File dir) {
            dirsToDelete.add(dir);
        }

        @Override
        public void run() {
            synchronized (this) {
                for (File dir : dirsToDelete) {
                    deleteDirectory(dir);
                }
            }
        }

        protected void deleteDirectory(File dir) {
            if (dir.isFile()) {
                if (!dir.delete()) {
                    log.error("Could not delete " + dir.getAbsolutePath());
                }
                return;
            }

            File[] files = dir.listFiles();
            if (files == null) {
                return;
            }

            for (File f : files) {
                deleteDirectory(f);
            }

            if (!dir.delete()) {
                log.error("Could not delete directory: " + dir.getAbsolutePath());
            }

        }

    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/util/HedwigSocketAddress.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.util;

import java.net.InetSocketAddress;

/**
 * This is a data wrapper class that is basically an InetSocketAddress with one
 * extra piece of information for the SSL port (optional). This is used by
 * Hedwig so we can encapsulate both regular and SSL port information in one
 * data structure. Hedwig hub servers can be configured to listen on the
 * standard regular port and additionally on an optional SSL port. The String
 * representation of a HedwigSocketAddress is: <hostname>:<port>:<SSL
 * port(optional)>
 */
public class HedwigSocketAddress {

    // Member fields that make up this class.
    private final String hostname;
    private final int port;
    private final int sslPort;

    private final InetSocketAddress socketAddress;
    private final InetSocketAddress sslSocketAddress;

    // Constants used by this class.
    public static final String COLON = ":";
    private static final int NO_SSL_PORT = -1;

    // Constructor that takes in both a regular and SSL port.
    public HedwigSocketAddress(String hostname, int port, int sslPort) {
        this.hostname = hostname;
        this.port = port;
        this.sslPort = sslPort;
        socketAddress = new InetSocketAddress(hostname, port);
        if (sslPort != NO_SSL_PORT)
            sslSocketAddress = new InetSocketAddress(hostname, sslPort);
        else
            sslSocketAddress = null;
    }

    // Constructor that only takes in a regular port.
    public HedwigSocketAddress(String hostname, int port) {
        this(hostname, port, NO_SSL_PORT);
    }

    // Constructor from a String "serialized" version of this class.
    public HedwigSocketAddress(String addr) {
        String[] parts = addr.split(COLON);
        this.hostname = parts[0];
        this.port = Integer.parseInt(parts[1]);
        if (parts.length > 2)
            this.sslPort = Integer.parseInt(parts[2]);
        else
            this.sslPort = NO_SSL_PORT;
        socketAddress = new InetSocketAddress(hostname, port);
        if (sslPort != NO_SSL_PORT)
            sslSocketAddress = new InetSocketAddress(hostname, sslPort);
        else
            sslSocketAddress = null;
    }

    // Public getters
    public String getHostname() {
        return hostname;
    }

    public int getPort() {
        return port;
    }

    public int getSSLPort() {
        return sslPort;
    }

    // Method to return an InetSocketAddress for the regular port.
    public InetSocketAddress getSocketAddress() {
        return socketAddress;
    }

    // Method to return an InetSocketAddress for the SSL port.
    // Note that if no SSL port (or an invalid value) was passed
    // during object creation, this call will throw an IllegalArgumentException
    // (runtime exception).
    public InetSocketAddress getSSLSocketAddress() {
        return sslSocketAddress;
    }

    // Method to determine if this object instance is SSL enabled or not
    // (contains a valid SSL port).
    public boolean isSSLEnabled() {
        return sslPort != NO_SSL_PORT;
    }

    // Return the String "serialized" version of this object.
    @Override
    public String toString() {
        StringBuilder sb = new StringBuilder();
        sb.append(hostname).append(COLON).append(port).append(COLON).append(sslPort);
        return sb.toString();
    }

    // Implement an equals method comparing two HedwigSocketAddress objects.
    @Override
    public boolean equals(Object obj) {
        if (!(obj instanceof HedwigSocketAddress))
            return false;
        HedwigSocketAddress that = (HedwigSocketAddress) obj;
        return (this.hostname.equals(that.hostname) && (this.port == that.port) && (this.sslPort == that.sslPort));
    }

    // Static helper method to return the string representation for an
    // InetSocketAddress. The HedwigClient can only operate in SSL or non-SSL
    // mode. So the server hosts it connects to will just be an
    // InetSocketAddress instead of a HedwigSocketAddress. This utility method
    // can be used so we can store these server hosts as strings (ByteStrings)
    // in various places (e.g. list of server hosts we've connected to
    // or wrote to unsuccessfully).
    public static String sockAddrStr(InetSocketAddress addr) {
        return addr.getAddress().getHostAddress() + ":" + addr.getPort();
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/util/Option.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.util;

public class Option<T> {

    private T x;

    public static <T> Option<T> of(T x) {
        return new Option<T>(x);
    }

    public static <T> Option<T> of() {
        return new Option<T>();
    }

    public Option() {
    }

    public Option(T x) {
        this.x = x;
    }

    public T get() {
        return x;
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/util/Pair.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.util;

public class Pair<T, U> {

    private T x;
    private U y;

    public Pair(T x, U y) {
        this.x = x;
        this.y = y;
    }

    public static <T, U> Pair<T, U> of(T x, U y) {
        return new Pair<T, U>(x, y);
    }

    public T first() {
        return x;
    }

    public U second() {
        return y;
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/util/PathUtils.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.util;

import java.io.File;
import java.util.ArrayList;
import java.util.List;

public class PathUtils {

    /** Generate all prefixes for a path. "/a/b/c" -> ["/a","/a/b","/a/b/c"] */
    public static List<String> prefixes(String path) {
        List<String> prefixes = new ArrayList<String>();
        String prefix = "";
        for (String comp : path.split("/+")) {
            // Skip the first (empty) path component.
            if (!comp.equals("")) {
                prefix += "/" + comp;
                prefixes.add(prefix);
            }
        }
        return prefixes;
    }

    /** Return true iff prefix is a prefix of path. */
    public static boolean isPrefix(String prefix, String path) {
        String[] as = prefix.split("/+"), bs = path.split("/+");
        if (as.length > bs.length)
            return false;
        for (int i = 0; i < as.length; i++)
            if (!as[i].equals(bs[i]))
                return false;
        return true;
    }

    /** Like File.getParent but always uses the / separator. */
    public static String parent(String path) {
        return new File(path).getParent().replace("\\", "/");
    }

}
"
hedwig-protocol/src/main/java/org/apache/hedwig/exceptions/PubSubException.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.exceptions;

import java.util.Collection;

import org.apache.hedwig.protocol.PubSubProtocol.StatusCode;

@SuppressWarnings("serial")
public abstract class PubSubException extends Exception {
    protected StatusCode code;

    protected PubSubException(StatusCode code, String msg) {
        super(msg);
        this.code = code;
    }

    protected PubSubException(StatusCode code, Exception e) {
        super(e);
        this.code = code;
    }

    public static PubSubException create(StatusCode code, String msg) {
        if (code == StatusCode.CLIENT_ALREADY_SUBSCRIBED) {
            return new ClientAlreadySubscribedException(msg);
        } else if (code == StatusCode.CLIENT_NOT_SUBSCRIBED) {
            return new ClientNotSubscribedException(msg);
        } else if (code == StatusCode.MALFORMED_REQUEST) {
            return new MalformedRequestException(msg);
        } else if (code == StatusCode.NO_SUCH_TOPIC) {
            return new NoSuchTopicException(msg);
        } else if (code == StatusCode.NOT_RESPONSIBLE_FOR_TOPIC) {
            return new ServerNotResponsibleForTopicException(msg);
        } else if (code == StatusCode.SERVICE_DOWN) {
            return new ServiceDownException(msg);
        } else if (code == StatusCode.COULD_NOT_CONNECT) {
            return new CouldNotConnectException(msg);
        }
        /*
         * Insert new ones here
         */
        else if (code == StatusCode.UNCERTAIN_STATE) {
            return new UncertainStateException(msg);
        }
        // Finally the catch all exception (for unexpected error conditions)
        else {
            return new UnexpectedConditionException("Unknow status code:" + code.getNumber() + ", msg: " + msg);
        }
    }

    public StatusCode getCode() {
        return code;
    }

    public static class ClientAlreadySubscribedException extends PubSubException {
        public ClientAlreadySubscribedException(String msg) {
            super(StatusCode.CLIENT_ALREADY_SUBSCRIBED, msg);
        }
    }

    public static class ClientNotSubscribedException extends PubSubException {
        public ClientNotSubscribedException(String msg) {
            super(StatusCode.CLIENT_NOT_SUBSCRIBED, msg);
        }
    }

    public static class MalformedRequestException extends PubSubException {
        public MalformedRequestException(String msg) {
            super(StatusCode.MALFORMED_REQUEST, msg);
        }
    }

    public static class NoSuchTopicException extends PubSubException {
        public NoSuchTopicException(String msg) {
            super(StatusCode.NO_SUCH_TOPIC, msg);
        }
    }

    public static class ServerNotResponsibleForTopicException extends PubSubException {
        // Note the exception message serves as the name of the responsible host
        public ServerNotResponsibleForTopicException(String responsibleHost) {
            super(StatusCode.NOT_RESPONSIBLE_FOR_TOPIC, responsibleHost);
        }
    }

    public static class TopicBusyException extends PubSubException {
        public TopicBusyException(String msg) {
            super(StatusCode.TOPIC_BUSY, msg);
        }
    }

    public static class ServiceDownException extends PubSubException {
        public ServiceDownException(String msg) {
            super(StatusCode.SERVICE_DOWN, msg);
        }

        public ServiceDownException(Exception e) {
            super(StatusCode.SERVICE_DOWN, e);
        }
    }

    public static class CouldNotConnectException extends PubSubException {
        public CouldNotConnectException(String msg) {
            super(StatusCode.COULD_NOT_CONNECT, msg);
        }
    }

    /*
     * Insert new ones here
     */
    public static class UncertainStateException extends PubSubException {
        public UncertainStateException(String msg) {
            super(StatusCode.UNCERTAIN_STATE, msg);
        }
    }

    // The catch all exception (for unexpected error conditions)
    public static class UnexpectedConditionException extends PubSubException {
        public UnexpectedConditionException(String msg) {
            super(StatusCode.UNEXPECTED_CONDITION, msg);
        }
    }

    // The composite exception (for concurrent operations).
    public static class CompositeException extends PubSubException {
        private final Collection<PubSubException> exceptions;
        public CompositeException(Collection<PubSubException> exceptions) {
            super(StatusCode.COMPOSITE, "composite exception");
            this.exceptions = exceptions;
        }
        public Collection<PubSubException> getExceptions() {
            return exceptions;
        }
        @Override
        public String toString() {
            StringBuilder builder = new StringBuilder();
            builder.append(super.toString()).append('\n');
            for (PubSubException exception : exceptions)
                builder.append(exception).append('\n');
            return builder.toString();
        }
    }

    public static class ClientNotSubscribedRuntimeException extends RuntimeException {
    }

}
"
hedwig-protocol/src/main/java/org/apache/hedwig/protocol/PubSubProtocol.java,true,"// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: src/main/protobuf/PubSubProtocol.proto

package org.apache.hedwig.protocol;

public final class PubSubProtocol {
    private PubSubProtocol() {}
    public static void registerAllExtensions(
        com.google.protobuf.ExtensionRegistry registry) {
    }
    public enum ProtocolVersion
    implements com.google.protobuf.ProtocolMessageEnum {
        VERSION_ONE(0, 1),
        ;


        public final int getNumber() {
            return value;
        }

        public static ProtocolVersion valueOf(int value) {
            switch (value) {
            case 1:
                return VERSION_ONE;
            default:
                return null;
            }
        }

        public static com.google.protobuf.Internal.EnumLiteMap<ProtocolVersion>
        internalGetValueMap() {
            return internalValueMap;
        }
        private static com.google.protobuf.Internal.EnumLiteMap<ProtocolVersion>
        internalValueMap =
        new com.google.protobuf.Internal.EnumLiteMap<ProtocolVersion>() {
            public ProtocolVersion findValueByNumber(int number) {
                return ProtocolVersion.valueOf(number)
                       ;
            }
        };

        public final com.google.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
            return getDescriptor().getValues().get(index);
        }
        public final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
            return getDescriptor();
        }
        public static final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
            return org.apache.hedwig.protocol.PubSubProtocol.getDescriptor().getEnumTypes().get(0);
        }

        private static final ProtocolVersion[] VALUES = {
            VERSION_ONE,
        };
        public static ProtocolVersion valueOf(
            com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
            if (desc.getType() != getDescriptor()) {
                throw new java.lang.IllegalArgumentException(
                    "EnumValueDescriptor is not for this type.");
            }
            return VALUES[desc.getIndex()];
        }
        private final int index;
        private final int value;
        private ProtocolVersion(int index, int value) {
            this.index = index;
            this.value = value;
        }

        static {
            org.apache.hedwig.protocol.PubSubProtocol.getDescriptor();
        }

        // @@protoc_insertion_point(enum_scope:Hedwig.ProtocolVersion)
    }

    public enum OperationType
    implements com.google.protobuf.ProtocolMessageEnum {
        PUBLISH(0, 0),
        SUBSCRIBE(1, 1),
        CONSUME(2, 2),
        UNSUBSCRIBE(3, 3),
        START_DELIVERY(4, 4),
        STOP_DELIVERY(5, 5),
        ;


        public final int getNumber() {
            return value;
        }

        public static OperationType valueOf(int value) {
            switch (value) {
            case 0:
                return PUBLISH;
            case 1:
                return SUBSCRIBE;
            case 2:
                return CONSUME;
            case 3:
                return UNSUBSCRIBE;
            case 4:
                return START_DELIVERY;
            case 5:
                return STOP_DELIVERY;
            default:
                return null;
            }
        }

        public static com.google.protobuf.Internal.EnumLiteMap<OperationType>
        internalGetValueMap() {
            return internalValueMap;
        }
        private static com.google.protobuf.Internal.EnumLiteMap<OperationType>
        internalValueMap =
        new com.google.protobuf.Internal.EnumLiteMap<OperationType>() {
            public OperationType findValueByNumber(int number) {
                return OperationType.valueOf(number)
                       ;
            }
        };

        public final com.google.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
            return getDescriptor().getValues().get(index);
        }
        public final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
            return getDescriptor();
        }
        public static final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
            return org.apache.hedwig.protocol.PubSubProtocol.getDescriptor().getEnumTypes().get(1);
        }

        private static final OperationType[] VALUES = {
            PUBLISH, SUBSCRIBE, CONSUME, UNSUBSCRIBE, START_DELIVERY, STOP_DELIVERY,
        };
        public static OperationType valueOf(
            com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
            if (desc.getType() != getDescriptor()) {
                throw new java.lang.IllegalArgumentException(
                    "EnumValueDescriptor is not for this type.");
            }
            return VALUES[desc.getIndex()];
        }
        private final int index;
        private final int value;
        private OperationType(int index, int value) {
            this.index = index;
            this.value = value;
        }

        static {
            org.apache.hedwig.protocol.PubSubProtocol.getDescriptor();
        }

        // @@protoc_insertion_point(enum_scope:Hedwig.OperationType)
    }

    public enum StatusCode
    implements com.google.protobuf.ProtocolMessageEnum {
        SUCCESS(0, 0),
        MALFORMED_REQUEST(1, 401),
        NO_SUCH_TOPIC(2, 402),
        CLIENT_ALREADY_SUBSCRIBED(3, 403),
        CLIENT_NOT_SUBSCRIBED(4, 404),
        COULD_NOT_CONNECT(5, 405),
        TOPIC_BUSY(6, 406),
        NOT_RESPONSIBLE_FOR_TOPIC(7, 501),
        SERVICE_DOWN(8, 502),
        UNCERTAIN_STATE(9, 503),
        UNEXPECTED_CONDITION(10, 600),
        COMPOSITE(11, 700),
        ;


        public final int getNumber() {
            return value;
        }

        public static StatusCode valueOf(int value) {
            switch (value) {
            case 0:
                return SUCCESS;
            case 401:
                return MALFORMED_REQUEST;
            case 402:
                return NO_SUCH_TOPIC;
            case 403:
                return CLIENT_ALREADY_SUBSCRIBED;
            case 404:
                return CLIENT_NOT_SUBSCRIBED;
            case 405:
                return COULD_NOT_CONNECT;
            case 406:
                return TOPIC_BUSY;
            case 501:
                return NOT_RESPONSIBLE_FOR_TOPIC;
            case 502:
                return SERVICE_DOWN;
            case 503:
                return UNCERTAIN_STATE;
            case 600:
                return UNEXPECTED_CONDITION;
            case 700:
                return COMPOSITE;
            default:
                return null;
            }
        }

        public static com.google.protobuf.Internal.EnumLiteMap<StatusCode>
        internalGetValueMap() {
            return internalValueMap;
        }
        private static com.google.protobuf.Internal.EnumLiteMap<StatusCode>
        internalValueMap =
        new com.google.protobuf.Internal.EnumLiteMap<StatusCode>() {
            public StatusCode findValueByNumber(int number) {
                return StatusCode.valueOf(number)
                       ;
            }
        };

        public final com.google.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
            return getDescriptor().getValues().get(index);
        }
        public final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
            return getDescriptor();
        }
        public static final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
            return org.apache.hedwig.protocol.PubSubProtocol.getDescriptor().getEnumTypes().get(2);
        }

        private static final StatusCode[] VALUES = {
            SUCCESS, MALFORMED_REQUEST, NO_SUCH_TOPIC, CLIENT_ALREADY_SUBSCRIBED, CLIENT_NOT_SUBSCRIBED, COULD_NOT_CONNECT, TOPIC_BUSY, NOT_RESPONSIBLE_FOR_TOPIC, SERVICE_DOWN, UNCERTAIN_STATE, UNEXPECTED_CONDITION, COMPOSITE,
        };
        public static StatusCode valueOf(
            com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
            if (desc.getType() != getDescriptor()) {
                throw new java.lang.IllegalArgumentException(
                    "EnumValueDescriptor is not for this type.");
            }
            return VALUES[desc.getIndex()];
        }
        private final int index;
        private final int value;
        private StatusCode(int index, int value) {
            this.index = index;
            this.value = value;
        }

        static {
            org.apache.hedwig.protocol.PubSubProtocol.getDescriptor();
        }

        // @@protoc_insertion_point(enum_scope:Hedwig.StatusCode)
    }

    public static final class Message extends
        com.google.protobuf.GeneratedMessage {
        // Use Message.newBuilder() to construct.
        private Message() {
            initFields();
        }
        private Message(boolean noInit) {}

        private static final Message defaultInstance;
        public static Message getDefaultInstance() {
            return defaultInstance;
        }

        public Message getDefaultInstanceForType() {
            return defaultInstance;
        }

        public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
            return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_Message_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
            return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_Message_fieldAccessorTable;
        }

        // required bytes body = 1;
        public static final int BODY_FIELD_NUMBER = 1;
        private boolean hasBody;
        private com.google.protobuf.ByteString body_ = com.google.protobuf.ByteString.EMPTY;
        public boolean hasBody() {
            return hasBody;
        }
        public com.google.protobuf.ByteString getBody() {
            return body_;
        }

        // optional bytes srcRegion = 2;
        public static final int SRCREGION_FIELD_NUMBER = 2;
        private boolean hasSrcRegion;
        private com.google.protobuf.ByteString srcRegion_ = com.google.protobuf.ByteString.EMPTY;
        public boolean hasSrcRegion() {
            return hasSrcRegion;
        }
        public com.google.protobuf.ByteString getSrcRegion() {
            return srcRegion_;
        }

        // optional .Hedwig.MessageSeqId msgId = 3;
        public static final int MSGID_FIELD_NUMBER = 3;
        private boolean hasMsgId;
        private org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId msgId_;
        public boolean hasMsgId() {
            return hasMsgId;
        }
        public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId getMsgId() {
            return msgId_;
        }

        private void initFields() {
            msgId_ = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance();
        }
        public final boolean isInitialized() {
            if (!hasBody) return false;
            if (hasMsgId()) {
                if (!getMsgId().isInitialized()) return false;
            }
            return true;
        }

        public void writeTo(com.google.protobuf.CodedOutputStream output)
                throws java.io.IOException {
            getSerializedSize();
            if (hasBody()) {
                output.writeBytes(1, getBody());
            }
            if (hasSrcRegion()) {
                output.writeBytes(2, getSrcRegion());
            }
            if (hasMsgId()) {
                output.writeMessage(3, getMsgId());
            }
            getUnknownFields().writeTo(output);
        }

        private int memoizedSerializedSize = -1;
        public int getSerializedSize() {
            int size = memoizedSerializedSize;
            if (size != -1) return size;

            size = 0;
            if (hasBody()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeBytesSize(1, getBody());
            }
            if (hasSrcRegion()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeBytesSize(2, getSrcRegion());
            }
            if (hasMsgId()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeMessageSize(3, getMsgId());
            }
            size += getUnknownFields().getSerializedSize();
            memoizedSerializedSize = size;
            return size;
        }

        public static org.apache.hedwig.protocol.PubSubProtocol.Message parseFrom(
            com.google.protobuf.ByteString data)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.Message parseFrom(
            com.google.protobuf.ByteString data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.Message parseFrom(byte[] data)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.Message parseFrom(
            byte[] data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.Message parseFrom(java.io.InputStream input)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.Message parseFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.Message parseDelimitedFrom(java.io.InputStream input)
                throws java.io.IOException {
            Builder builder = newBuilder();
            if (builder.mergeDelimitedFrom(input)) {
                return builder.buildParsed();
            } else {
                return null;
            }
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.Message parseDelimitedFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            Builder builder = newBuilder();
            if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
                return builder.buildParsed();
            } else {
                return null;
            }
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.Message parseFrom(
            com.google.protobuf.CodedInputStream input)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.Message parseFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input, extensionRegistry)
                   .buildParsed();
        }

        public static Builder newBuilder() {
            return Builder.create();
        }
        public Builder newBuilderForType() {
            return newBuilder();
        }
        public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.Message prototype) {
            return newBuilder().mergeFrom(prototype);
        }
        public Builder toBuilder() {
            return newBuilder(this);
        }

        public static final class Builder extends
            com.google.protobuf.GeneratedMessage.Builder<Builder> {
            private org.apache.hedwig.protocol.PubSubProtocol.Message result;

            // Construct using org.apache.hedwig.protocol.PubSubProtocol.Message.newBuilder()
            private Builder() {}

            private static Builder create() {
                Builder builder = new Builder();
                builder.result = new org.apache.hedwig.protocol.PubSubProtocol.Message();
                return builder;
            }

            protected org.apache.hedwig.protocol.PubSubProtocol.Message internalGetResult() {
                return result;
            }

            public Builder clear() {
                if (result == null) {
                    throw new IllegalStateException(
                        "Cannot call clear() after build().");
                }
                result = new org.apache.hedwig.protocol.PubSubProtocol.Message();
                return this;
            }

            public Builder clone() {
                return create().mergeFrom(result);
            }

            public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
                return org.apache.hedwig.protocol.PubSubProtocol.Message.getDescriptor();
            }

            public org.apache.hedwig.protocol.PubSubProtocol.Message getDefaultInstanceForType() {
                return org.apache.hedwig.protocol.PubSubProtocol.Message.getDefaultInstance();
            }

            public boolean isInitialized() {
                return result.isInitialized();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.Message build() {
                if (result != null && !isInitialized()) {
                    throw newUninitializedMessageException(result);
                }
                return buildPartial();
            }

            private org.apache.hedwig.protocol.PubSubProtocol.Message buildParsed()
                    throws com.google.protobuf.InvalidProtocolBufferException {
                if (!isInitialized()) {
                    throw newUninitializedMessageException(
                        result).asInvalidProtocolBufferException();
                }
                return buildPartial();
            }

            public org.apache.hedwig.protocol.PubSubProtocol.Message buildPartial() {
                if (result == null) {
                    throw new IllegalStateException(
                        "build() has already been called on this Builder.");
                }
                org.apache.hedwig.protocol.PubSubProtocol.Message returnMe = result;
                result = null;
                return returnMe;
            }

            public Builder mergeFrom(com.google.protobuf.Message other) {
                if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.Message) {
                    return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.Message)other);
                } else {
                    super.mergeFrom(other);
                    return this;
                }
            }

            public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.Message other) {
                if (other == org.apache.hedwig.protocol.PubSubProtocol.Message.getDefaultInstance()) return this;
                if (other.hasBody()) {
                    setBody(other.getBody());
                }
                if (other.hasSrcRegion()) {
                    setSrcRegion(other.getSrcRegion());
                }
                if (other.hasMsgId()) {
                    mergeMsgId(other.getMsgId());
                }
                this.mergeUnknownFields(other.getUnknownFields());
                return this;
            }

            public Builder mergeFrom(
                com.google.protobuf.CodedInputStream input,
                com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                    throws java.io.IOException {
                com.google.protobuf.UnknownFieldSet.Builder unknownFields =
                    com.google.protobuf.UnknownFieldSet.newBuilder(
                        this.getUnknownFields());
                while (true) {
                    int tag = input.readTag();
                    switch (tag) {
                    case 0:
                        this.setUnknownFields(unknownFields.build());
                        return this;
                    default: {
                        if (!parseUnknownField(input, unknownFields,
                                               extensionRegistry, tag)) {
                            this.setUnknownFields(unknownFields.build());
                            return this;
                        }
                        break;
                    }
                    case 10: {
                        setBody(input.readBytes());
                        break;
                    }
                    case 18: {
                        setSrcRegion(input.readBytes());
                        break;
                    }
                    case 26: {
                        org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.newBuilder();
                        if (hasMsgId()) {
                            subBuilder.mergeFrom(getMsgId());
                        }
                        input.readMessage(subBuilder, extensionRegistry);
                        setMsgId(subBuilder.buildPartial());
                        break;
                    }
                    }
                }
            }


            // required bytes body = 1;
            public boolean hasBody() {
                return result.hasBody();
            }
            public com.google.protobuf.ByteString getBody() {
                return result.getBody();
            }
            public Builder setBody(com.google.protobuf.ByteString value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.hasBody = true;
                result.body_ = value;
                return this;
            }
            public Builder clearBody() {
                result.hasBody = false;
                result.body_ = getDefaultInstance().getBody();
                return this;
            }

            // optional bytes srcRegion = 2;
            public boolean hasSrcRegion() {
                return result.hasSrcRegion();
            }
            public com.google.protobuf.ByteString getSrcRegion() {
                return result.getSrcRegion();
            }
            public Builder setSrcRegion(com.google.protobuf.ByteString value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.hasSrcRegion = true;
                result.srcRegion_ = value;
                return this;
            }
            public Builder clearSrcRegion() {
                result.hasSrcRegion = false;
                result.srcRegion_ = getDefaultInstance().getSrcRegion();
                return this;
            }

            // optional .Hedwig.MessageSeqId msgId = 3;
            public boolean hasMsgId() {
                return result.hasMsgId();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId getMsgId() {
                return result.getMsgId();
            }
            public Builder setMsgId(org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.hasMsgId = true;
                result.msgId_ = value;
                return this;
            }
            public Builder setMsgId(org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder builderForValue) {
                result.hasMsgId = true;
                result.msgId_ = builderForValue.build();
                return this;
            }
            public Builder mergeMsgId(org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId value) {
                if (result.hasMsgId() &&
                        result.msgId_ != org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance()) {
                    result.msgId_ =
                        org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.newBuilder(result.msgId_).mergeFrom(value).buildPartial();
                } else {
                    result.msgId_ = value;
                }
                result.hasMsgId = true;
                return this;
            }
            public Builder clearMsgId() {
                result.hasMsgId = false;
                result.msgId_ = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance();
                return this;
            }

            // @@protoc_insertion_point(builder_scope:Hedwig.Message)
        }

        static {
            defaultInstance = new Message(true);
            org.apache.hedwig.protocol.PubSubProtocol.internalForceInit();
            defaultInstance.initFields();
        }

        // @@protoc_insertion_point(class_scope:Hedwig.Message)
    }

    public static final class RegionSpecificSeqId extends
        com.google.protobuf.GeneratedMessage {
        // Use RegionSpecificSeqId.newBuilder() to construct.
        private RegionSpecificSeqId() {
            initFields();
        }
        private RegionSpecificSeqId(boolean noInit) {}

        private static final RegionSpecificSeqId defaultInstance;
        public static RegionSpecificSeqId getDefaultInstance() {
            return defaultInstance;
        }

        public RegionSpecificSeqId getDefaultInstanceForType() {
            return defaultInstance;
        }

        public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
            return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_RegionSpecificSeqId_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
            return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_RegionSpecificSeqId_fieldAccessorTable;
        }

        // required bytes region = 1;
        public static final int REGION_FIELD_NUMBER = 1;
        private boolean hasRegion;
        private com.google.protobuf.ByteString region_ = com.google.protobuf.ByteString.EMPTY;
        public boolean hasRegion() {
            return hasRegion;
        }
        public com.google.protobuf.ByteString getRegion() {
            return region_;
        }

        // required uint64 seqId = 2;
        public static final int SEQID_FIELD_NUMBER = 2;
        private boolean hasSeqId;
        private long seqId_ = 0L;
        public boolean hasSeqId() {
            return hasSeqId;
        }
        public long getSeqId() {
            return seqId_;
        }

        private void initFields() {
        }
        public final boolean isInitialized() {
            if (!hasRegion) return false;
            if (!hasSeqId) return false;
            return true;
        }

        public void writeTo(com.google.protobuf.CodedOutputStream output)
                throws java.io.IOException {
            getSerializedSize();
            if (hasRegion()) {
                output.writeBytes(1, getRegion());
            }
            if (hasSeqId()) {
                output.writeUInt64(2, getSeqId());
            }
            getUnknownFields().writeTo(output);
        }

        private int memoizedSerializedSize = -1;
        public int getSerializedSize() {
            int size = memoizedSerializedSize;
            if (size != -1) return size;

            size = 0;
            if (hasRegion()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeBytesSize(1, getRegion());
            }
            if (hasSeqId()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeUInt64Size(2, getSeqId());
            }
            size += getUnknownFields().getSerializedSize();
            memoizedSerializedSize = size;
            return size;
        }

        public static org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId parseFrom(
            com.google.protobuf.ByteString data)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId parseFrom(
            com.google.protobuf.ByteString data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId parseFrom(byte[] data)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId parseFrom(
            byte[] data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId parseFrom(java.io.InputStream input)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId parseFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId parseDelimitedFrom(java.io.InputStream input)
                throws java.io.IOException {
            Builder builder = newBuilder();
            if (builder.mergeDelimitedFrom(input)) {
                return builder.buildParsed();
            } else {
                return null;
            }
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId parseDelimitedFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            Builder builder = newBuilder();
            if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
                return builder.buildParsed();
            } else {
                return null;
            }
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId parseFrom(
            com.google.protobuf.CodedInputStream input)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId parseFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input, extensionRegistry)
                   .buildParsed();
        }

        public static Builder newBuilder() {
            return Builder.create();
        }
        public Builder newBuilderForType() {
            return newBuilder();
        }
        public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId prototype) {
            return newBuilder().mergeFrom(prototype);
        }
        public Builder toBuilder() {
            return newBuilder(this);
        }

        public static final class Builder extends
            com.google.protobuf.GeneratedMessage.Builder<Builder> {
            private org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId result;

            // Construct using org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId.newBuilder()
            private Builder() {}

            private static Builder create() {
                Builder builder = new Builder();
                builder.result = new org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId();
                return builder;
            }

            protected org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId internalGetResult() {
                return result;
            }

            public Builder clear() {
                if (result == null) {
                    throw new IllegalStateException(
                        "Cannot call clear() after build().");
                }
                result = new org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId();
                return this;
            }

            public Builder clone() {
                return create().mergeFrom(result);
            }

            public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
                return org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId.getDescriptor();
            }

            public org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId getDefaultInstanceForType() {
                return org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId.getDefaultInstance();
            }

            public boolean isInitialized() {
                return result.isInitialized();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId build() {
                if (result != null && !isInitialized()) {
                    throw newUninitializedMessageException(result);
                }
                return buildPartial();
            }

            private org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId buildParsed()
                    throws com.google.protobuf.InvalidProtocolBufferException {
                if (!isInitialized()) {
                    throw newUninitializedMessageException(
                        result).asInvalidProtocolBufferException();
                }
                return buildPartial();
            }

            public org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId buildPartial() {
                if (result == null) {
                    throw new IllegalStateException(
                        "build() has already been called on this Builder.");
                }
                org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId returnMe = result;
                result = null;
                return returnMe;
            }

            public Builder mergeFrom(com.google.protobuf.Message other) {
                if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId) {
                    return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId)other);
                } else {
                    super.mergeFrom(other);
                    return this;
                }
            }

            public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId other) {
                if (other == org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId.getDefaultInstance()) return this;
                if (other.hasRegion()) {
                    setRegion(other.getRegion());
                }
                if (other.hasSeqId()) {
                    setSeqId(other.getSeqId());
                }
                this.mergeUnknownFields(other.getUnknownFields());
                return this;
            }

            public Builder mergeFrom(
                com.google.protobuf.CodedInputStream input,
                com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                    throws java.io.IOException {
                com.google.protobuf.UnknownFieldSet.Builder unknownFields =
                    com.google.protobuf.UnknownFieldSet.newBuilder(
                        this.getUnknownFields());
                while (true) {
                    int tag = input.readTag();
                    switch (tag) {
                    case 0:
                        this.setUnknownFields(unknownFields.build());
                        return this;
                    default: {
                        if (!parseUnknownField(input, unknownFields,
                                               extensionRegistry, tag)) {
                            this.setUnknownFields(unknownFields.build());
                            return this;
                        }
                        break;
                    }
                    case 10: {
                        setRegion(input.readBytes());
                        break;
                    }
                    case 16: {
                        setSeqId(input.readUInt64());
                        break;
                    }
                    }
                }
            }


            // required bytes region = 1;
            public boolean hasRegion() {
                return result.hasRegion();
            }
            public com.google.protobuf.ByteString getRegion() {
                return result.getRegion();
            }
            public Builder setRegion(com.google.protobuf.ByteString value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.hasRegion = true;
                result.region_ = value;
                return this;
            }
            public Builder clearRegion() {
                result.hasRegion = false;
                result.region_ = getDefaultInstance().getRegion();
                return this;
            }

            // required uint64 seqId = 2;
            public boolean hasSeqId() {
                return result.hasSeqId();
            }
            public long getSeqId() {
                return result.getSeqId();
            }
            public Builder setSeqId(long value) {
                result.hasSeqId = true;
                result.seqId_ = value;
                return this;
            }
            public Builder clearSeqId() {
                result.hasSeqId = false;
                result.seqId_ = 0L;
                return this;
            }

            // @@protoc_insertion_point(builder_scope:Hedwig.RegionSpecificSeqId)
        }

        static {
            defaultInstance = new RegionSpecificSeqId(true);
            org.apache.hedwig.protocol.PubSubProtocol.internalForceInit();
            defaultInstance.initFields();
        }

        // @@protoc_insertion_point(class_scope:Hedwig.RegionSpecificSeqId)
    }

    public static final class MessageSeqId extends
        com.google.protobuf.GeneratedMessage {
        // Use MessageSeqId.newBuilder() to construct.
        private MessageSeqId() {
            initFields();
        }
        private MessageSeqId(boolean noInit) {}

        private static final MessageSeqId defaultInstance;
        public static MessageSeqId getDefaultInstance() {
            return defaultInstance;
        }

        public MessageSeqId getDefaultInstanceForType() {
            return defaultInstance;
        }

        public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
            return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_MessageSeqId_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
            return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_MessageSeqId_fieldAccessorTable;
        }

        // optional uint64 localComponent = 1;
        public static final int LOCALCOMPONENT_FIELD_NUMBER = 1;
        private boolean hasLocalComponent;
        private long localComponent_ = 0L;
        public boolean hasLocalComponent() {
            return hasLocalComponent;
        }
        public long getLocalComponent() {
            return localComponent_;
        }

        // repeated .Hedwig.RegionSpecificSeqId remoteComponents = 2;
        public static final int REMOTECOMPONENTS_FIELD_NUMBER = 2;
        private java.util.List<org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId> remoteComponents_ =
            java.util.Collections.emptyList();
        public java.util.List<org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId> getRemoteComponentsList() {
            return remoteComponents_;
        }
        public int getRemoteComponentsCount() {
            return remoteComponents_.size();
        }
        public org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId getRemoteComponents(int index) {
            return remoteComponents_.get(index);
        }

        private void initFields() {
        }
        public final boolean isInitialized() {
            for (org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId element : getRemoteComponentsList()) {
                if (!element.isInitialized()) return false;
            }
            return true;
        }

        public void writeTo(com.google.protobuf.CodedOutputStream output)
                throws java.io.IOException {
            getSerializedSize();
            if (hasLocalComponent()) {
                output.writeUInt64(1, getLocalComponent());
            }
            for (org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId element : getRemoteComponentsList()) {
                output.writeMessage(2, element);
            }
            getUnknownFields().writeTo(output);
        }

        private int memoizedSerializedSize = -1;
        public int getSerializedSize() {
            int size = memoizedSerializedSize;
            if (size != -1) return size;

            size = 0;
            if (hasLocalComponent()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeUInt64Size(1, getLocalComponent());
            }
            for (org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId element : getRemoteComponentsList()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeMessageSize(2, element);
            }
            size += getUnknownFields().getSerializedSize();
            memoizedSerializedSize = size;
            return size;
        }

        public static org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId parseFrom(
            com.google.protobuf.ByteString data)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId parseFrom(
            com.google.protobuf.ByteString data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId parseFrom(byte[] data)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId parseFrom(
            byte[] data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId parseFrom(java.io.InputStream input)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId parseFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId parseDelimitedFrom(java.io.InputStream input)
                throws java.io.IOException {
            Builder builder = newBuilder();
            if (builder.mergeDelimitedFrom(input)) {
                return builder.buildParsed();
            } else {
                return null;
            }
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId parseDelimitedFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            Builder builder = newBuilder();
            if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
                return builder.buildParsed();
            } else {
                return null;
            }
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId parseFrom(
            com.google.protobuf.CodedInputStream input)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId parseFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input, extensionRegistry)
                   .buildParsed();
        }

        public static Builder newBuilder() {
            return Builder.create();
        }
        public Builder newBuilderForType() {
            return newBuilder();
        }
        public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId prototype) {
            return newBuilder().mergeFrom(prototype);
        }
        public Builder toBuilder() {
            return newBuilder(this);
        }

        public static final class Builder extends
            com.google.protobuf.GeneratedMessage.Builder<Builder> {
            private org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId result;

            // Construct using org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.newBuilder()
            private Builder() {}

            private static Builder create() {
                Builder builder = new Builder();
                builder.result = new org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId();
                return builder;
            }

            protected org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId internalGetResult() {
                return result;
            }

            public Builder clear() {
                if (result == null) {
                    throw new IllegalStateException(
                        "Cannot call clear() after build().");
                }
                result = new org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId();
                return this;
            }

            public Builder clone() {
                return create().mergeFrom(result);
            }

            public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
                return org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDescriptor();
            }

            public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId getDefaultInstanceForType() {
                return org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance();
            }

            public boolean isInitialized() {
                return result.isInitialized();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId build() {
                if (result != null && !isInitialized()) {
                    throw newUninitializedMessageException(result);
                }
                return buildPartial();
            }

            private org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId buildParsed()
                    throws com.google.protobuf.InvalidProtocolBufferException {
                if (!isInitialized()) {
                    throw newUninitializedMessageException(
                        result).asInvalidProtocolBufferException();
                }
                return buildPartial();
            }

            public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId buildPartial() {
                if (result == null) {
                    throw new IllegalStateException(
                        "build() has already been called on this Builder.");
                }
                if (result.remoteComponents_ != java.util.Collections.EMPTY_LIST) {
                    result.remoteComponents_ =
                        java.util.Collections.unmodifiableList(result.remoteComponents_);
                }
                org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId returnMe = result;
                result = null;
                return returnMe;
            }

            public Builder mergeFrom(com.google.protobuf.Message other) {
                if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId) {
                    return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId)other);
                } else {
                    super.mergeFrom(other);
                    return this;
                }
            }

            public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId other) {
                if (other == org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance()) return this;
                if (other.hasLocalComponent()) {
                    setLocalComponent(other.getLocalComponent());
                }
                if (!other.remoteComponents_.isEmpty()) {
                    if (result.remoteComponents_.isEmpty()) {
                        result.remoteComponents_ = new java.util.ArrayList<org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId>();
                    }
                    result.remoteComponents_.addAll(other.remoteComponents_);
                }
                this.mergeUnknownFields(other.getUnknownFields());
                return this;
            }

            public Builder mergeFrom(
                com.google.protobuf.CodedInputStream input,
                com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                    throws java.io.IOException {
                com.google.protobuf.UnknownFieldSet.Builder unknownFields =
                    com.google.protobuf.UnknownFieldSet.newBuilder(
                        this.getUnknownFields());
                while (true) {
                    int tag = input.readTag();
                    switch (tag) {
                    case 0:
                        this.setUnknownFields(unknownFields.build());
                        return this;
                    default: {
                        if (!parseUnknownField(input, unknownFields,
                                               extensionRegistry, tag)) {
                            this.setUnknownFields(unknownFields.build());
                            return this;
                        }
                        break;
                    }
                    case 8: {
                        setLocalComponent(input.readUInt64());
                        break;
                    }
                    case 18: {
                        org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId.newBuilder();
                        input.readMessage(subBuilder, extensionRegistry);
                        addRemoteComponents(subBuilder.buildPartial());
                        break;
                    }
                    }
                }
            }


            // optional uint64 localComponent = 1;
            public boolean hasLocalComponent() {
                return result.hasLocalComponent();
            }
            public long getLocalComponent() {
                return result.getLocalComponent();
            }
            public Builder setLocalComponent(long value) {
                result.hasLocalComponent = true;
                result.localComponent_ = value;
                return this;
            }
            public Builder clearLocalComponent() {
                result.hasLocalComponent = false;
                result.localComponent_ = 0L;
                return this;
            }

            // repeated .Hedwig.RegionSpecificSeqId remoteComponents = 2;
            public java.util.List<org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId> getRemoteComponentsList() {
                return java.util.Collections.unmodifiableList(result.remoteComponents_);
            }
            public int getRemoteComponentsCount() {
                return result.getRemoteComponentsCount();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId getRemoteComponents(int index) {
                return result.getRemoteComponents(index);
            }
            public Builder setRemoteComponents(int index, org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.remoteComponents_.set(index, value);
                return this;
            }
            public Builder setRemoteComponents(int index, org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId.Builder builderForValue) {
                result.remoteComponents_.set(index, builderForValue.build());
                return this;
            }
            public Builder addRemoteComponents(org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                if (result.remoteComponents_.isEmpty()) {
                    result.remoteComponents_ = new java.util.ArrayList<org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId>();
                }
                result.remoteComponents_.add(value);
                return this;
            }
            public Builder addRemoteComponents(org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId.Builder builderForValue) {
                if (result.remoteComponents_.isEmpty()) {
                    result.remoteComponents_ = new java.util.ArrayList<org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId>();
                }
                result.remoteComponents_.add(builderForValue.build());
                return this;
            }
            public Builder addAllRemoteComponents(
                java.lang.Iterable<? extends org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId> values) {
                if (result.remoteComponents_.isEmpty()) {
                    result.remoteComponents_ = new java.util.ArrayList<org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId>();
                }
                super.addAll(values, result.remoteComponents_);
                return this;
            }
            public Builder clearRemoteComponents() {
                result.remoteComponents_ = java.util.Collections.emptyList();
                return this;
            }

            // @@protoc_insertion_point(builder_scope:Hedwig.MessageSeqId)
        }

        static {
            defaultInstance = new MessageSeqId(true);
            org.apache.hedwig.protocol.PubSubProtocol.internalForceInit();
            defaultInstance.initFields();
        }

        // @@protoc_insertion_point(class_scope:Hedwig.MessageSeqId)
    }

    public static final class PubSubRequest extends
        com.google.protobuf.GeneratedMessage {
        // Use PubSubRequest.newBuilder() to construct.
        private PubSubRequest() {
            initFields();
        }
        private PubSubRequest(boolean noInit) {}

        private static final PubSubRequest defaultInstance;
        public static PubSubRequest getDefaultInstance() {
            return defaultInstance;
        }

        public PubSubRequest getDefaultInstanceForType() {
            return defaultInstance;
        }

        public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
            return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_PubSubRequest_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
            return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_PubSubRequest_fieldAccessorTable;
        }

        // required .Hedwig.ProtocolVersion protocolVersion = 1;
        public static final int PROTOCOLVERSION_FIELD_NUMBER = 1;
        private boolean hasProtocolVersion;
        private org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion protocolVersion_;
        public boolean hasProtocolVersion() {
            return hasProtocolVersion;
        }
        public org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion getProtocolVersion() {
            return protocolVersion_;
        }

        // required .Hedwig.OperationType type = 2;
        public static final int TYPE_FIELD_NUMBER = 2;
        private boolean hasType;
        private org.apache.hedwig.protocol.PubSubProtocol.OperationType type_;
        public boolean hasType() {
            return hasType;
        }
        public org.apache.hedwig.protocol.PubSubProtocol.OperationType getType() {
            return type_;
        }

        // repeated bytes triedServers = 3;
        public static final int TRIEDSERVERS_FIELD_NUMBER = 3;
        private java.util.List<com.google.protobuf.ByteString> triedServers_ =
            java.util.Collections.emptyList();
        public java.util.List<com.google.protobuf.ByteString> getTriedServersList() {
            return triedServers_;
        }
        public int getTriedServersCount() {
            return triedServers_.size();
        }
        public com.google.protobuf.ByteString getTriedServers(int index) {
            return triedServers_.get(index);
        }

        // required uint64 txnId = 4;
        public static final int TXNID_FIELD_NUMBER = 4;
        private boolean hasTxnId;
        private long txnId_ = 0L;
        public boolean hasTxnId() {
            return hasTxnId;
        }
        public long getTxnId() {
            return txnId_;
        }

        // optional bool shouldClaim = 5;
        public static final int SHOULDCLAIM_FIELD_NUMBER = 5;
        private boolean hasShouldClaim;
        private boolean shouldClaim_ = false;
        public boolean hasShouldClaim() {
            return hasShouldClaim;
        }
        public boolean getShouldClaim() {
            return shouldClaim_;
        }

        // required bytes topic = 6;
        public static final int TOPIC_FIELD_NUMBER = 6;
        private boolean hasTopic;
        private com.google.protobuf.ByteString topic_ = com.google.protobuf.ByteString.EMPTY;
        public boolean hasTopic() {
            return hasTopic;
        }
        public com.google.protobuf.ByteString getTopic() {
            return topic_;
        }

        // optional .Hedwig.PublishRequest publishRequest = 52;
        public static final int PUBLISHREQUEST_FIELD_NUMBER = 52;
        private boolean hasPublishRequest;
        private org.apache.hedwig.protocol.PubSubProtocol.PublishRequest publishRequest_;
        public boolean hasPublishRequest() {
            return hasPublishRequest;
        }
        public org.apache.hedwig.protocol.PubSubProtocol.PublishRequest getPublishRequest() {
            return publishRequest_;
        }

        // optional .Hedwig.SubscribeRequest subscribeRequest = 53;
        public static final int SUBSCRIBEREQUEST_FIELD_NUMBER = 53;
        private boolean hasSubscribeRequest;
        private org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest subscribeRequest_;
        public boolean hasSubscribeRequest() {
            return hasSubscribeRequest;
        }
        public org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest getSubscribeRequest() {
            return subscribeRequest_;
        }

        // optional .Hedwig.ConsumeRequest consumeRequest = 54;
        public static final int CONSUMEREQUEST_FIELD_NUMBER = 54;
        private boolean hasConsumeRequest;
        private org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest consumeRequest_;
        public boolean hasConsumeRequest() {
            return hasConsumeRequest;
        }
        public org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest getConsumeRequest() {
            return consumeRequest_;
        }

        // optional .Hedwig.UnsubscribeRequest unsubscribeRequest = 55;
        public static final int UNSUBSCRIBEREQUEST_FIELD_NUMBER = 55;
        private boolean hasUnsubscribeRequest;
        private org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest unsubscribeRequest_;
        public boolean hasUnsubscribeRequest() {
            return hasUnsubscribeRequest;
        }
        public org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest getUnsubscribeRequest() {
            return unsubscribeRequest_;
        }

        // optional .Hedwig.StopDeliveryRequest stopDeliveryRequest = 56;
        public static final int STOPDELIVERYREQUEST_FIELD_NUMBER = 56;
        private boolean hasStopDeliveryRequest;
        private org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest stopDeliveryRequest_;
        public boolean hasStopDeliveryRequest() {
            return hasStopDeliveryRequest;
        }
        public org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest getStopDeliveryRequest() {
            return stopDeliveryRequest_;
        }

        // optional .Hedwig.StartDeliveryRequest startDeliveryRequest = 57;
        public static final int STARTDELIVERYREQUEST_FIELD_NUMBER = 57;
        private boolean hasStartDeliveryRequest;
        private org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest startDeliveryRequest_;
        public boolean hasStartDeliveryRequest() {
            return hasStartDeliveryRequest;
        }
        public org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest getStartDeliveryRequest() {
            return startDeliveryRequest_;
        }

        private void initFields() {
            protocolVersion_ = org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion.VERSION_ONE;
            type_ = org.apache.hedwig.protocol.PubSubProtocol.OperationType.PUBLISH;
            publishRequest_ = org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.getDefaultInstance();
            subscribeRequest_ = org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.getDefaultInstance();
            consumeRequest_ = org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.getDefaultInstance();
            unsubscribeRequest_ = org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.getDefaultInstance();
            stopDeliveryRequest_ = org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.getDefaultInstance();
            startDeliveryRequest_ = org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.getDefaultInstance();
        }
        public final boolean isInitialized() {
            if (!hasProtocolVersion) return false;
            if (!hasType) return false;
            if (!hasTxnId) return false;
            if (!hasTopic) return false;
            if (hasPublishRequest()) {
                if (!getPublishRequest().isInitialized()) return false;
            }
            if (hasSubscribeRequest()) {
                if (!getSubscribeRequest().isInitialized()) return false;
            }
            if (hasConsumeRequest()) {
                if (!getConsumeRequest().isInitialized()) return false;
            }
            if (hasUnsubscribeRequest()) {
                if (!getUnsubscribeRequest().isInitialized()) return false;
            }
            if (hasStopDeliveryRequest()) {
                if (!getStopDeliveryRequest().isInitialized()) return false;
            }
            if (hasStartDeliveryRequest()) {
                if (!getStartDeliveryRequest().isInitialized()) return false;
            }
            return true;
        }

        public void writeTo(com.google.protobuf.CodedOutputStream output)
                throws java.io.IOException {
            getSerializedSize();
            if (hasProtocolVersion()) {
                output.writeEnum(1, getProtocolVersion().getNumber());
            }
            if (hasType()) {
                output.writeEnum(2, getType().getNumber());
            }
            for (com.google.protobuf.ByteString element : getTriedServersList()) {
                output.writeBytes(3, element);
            }
            if (hasTxnId()) {
                output.writeUInt64(4, getTxnId());
            }
            if (hasShouldClaim()) {
                output.writeBool(5, getShouldClaim());
            }
            if (hasTopic()) {
                output.writeBytes(6, getTopic());
            }
            if (hasPublishRequest()) {
                output.writeMessage(52, getPublishRequest());
            }
            if (hasSubscribeRequest()) {
                output.writeMessage(53, getSubscribeRequest());
            }
            if (hasConsumeRequest()) {
                output.writeMessage(54, getConsumeRequest());
            }
            if (hasUnsubscribeRequest()) {
                output.writeMessage(55, getUnsubscribeRequest());
            }
            if (hasStopDeliveryRequest()) {
                output.writeMessage(56, getStopDeliveryRequest());
            }
            if (hasStartDeliveryRequest()) {
                output.writeMessage(57, getStartDeliveryRequest());
            }
            getUnknownFields().writeTo(output);
        }

        private int memoizedSerializedSize = -1;
        public int getSerializedSize() {
            int size = memoizedSerializedSize;
            if (size != -1) return size;

            size = 0;
            if (hasProtocolVersion()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeEnumSize(1, getProtocolVersion().getNumber());
            }
            if (hasType()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeEnumSize(2, getType().getNumber());
            }
            {
                int dataSize = 0;
                for (com.google.protobuf.ByteString element : getTriedServersList()) {
                    dataSize += com.google.protobuf.CodedOutputStream
                                .computeBytesSizeNoTag(element);
                }
                size += dataSize;
                size += 1 * getTriedServersList().size();
            }
            if (hasTxnId()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeUInt64Size(4, getTxnId());
            }
            if (hasShouldClaim()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeBoolSize(5, getShouldClaim());
            }
            if (hasTopic()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeBytesSize(6, getTopic());
            }
            if (hasPublishRequest()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeMessageSize(52, getPublishRequest());
            }
            if (hasSubscribeRequest()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeMessageSize(53, getSubscribeRequest());
            }
            if (hasConsumeRequest()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeMessageSize(54, getConsumeRequest());
            }
            if (hasUnsubscribeRequest()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeMessageSize(55, getUnsubscribeRequest());
            }
            if (hasStopDeliveryRequest()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeMessageSize(56, getStopDeliveryRequest());
            }
            if (hasStartDeliveryRequest()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeMessageSize(57, getStartDeliveryRequest());
            }
            size += getUnknownFields().getSerializedSize();
            memoizedSerializedSize = size;
            return size;
        }

        public static org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest parseFrom(
            com.google.protobuf.ByteString data)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest parseFrom(
            com.google.protobuf.ByteString data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest parseFrom(byte[] data)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest parseFrom(
            byte[] data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest parseFrom(java.io.InputStream input)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest parseFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest parseDelimitedFrom(java.io.InputStream input)
                throws java.io.IOException {
            Builder builder = newBuilder();
            if (builder.mergeDelimitedFrom(input)) {
                return builder.buildParsed();
            } else {
                return null;
            }
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest parseDelimitedFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            Builder builder = newBuilder();
            if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
                return builder.buildParsed();
            } else {
                return null;
            }
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest parseFrom(
            com.google.protobuf.CodedInputStream input)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest parseFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input, extensionRegistry)
                   .buildParsed();
        }

        public static Builder newBuilder() {
            return Builder.create();
        }
        public Builder newBuilderForType() {
            return newBuilder();
        }
        public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest prototype) {
            return newBuilder().mergeFrom(prototype);
        }
        public Builder toBuilder() {
            return newBuilder(this);
        }

        public static final class Builder extends
            com.google.protobuf.GeneratedMessage.Builder<Builder> {
            private org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest result;

            // Construct using org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest.newBuilder()
            private Builder() {}

            private static Builder create() {
                Builder builder = new Builder();
                builder.result = new org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest();
                return builder;
            }

            protected org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest internalGetResult() {
                return result;
            }

            public Builder clear() {
                if (result == null) {
                    throw new IllegalStateException(
                        "Cannot call clear() after build().");
                }
                result = new org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest();
                return this;
            }

            public Builder clone() {
                return create().mergeFrom(result);
            }

            public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
                return org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest.getDescriptor();
            }

            public org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest getDefaultInstanceForType() {
                return org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest.getDefaultInstance();
            }

            public boolean isInitialized() {
                return result.isInitialized();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest build() {
                if (result != null && !isInitialized()) {
                    throw newUninitializedMessageException(result);
                }
                return buildPartial();
            }

            private org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest buildParsed()
                    throws com.google.protobuf.InvalidProtocolBufferException {
                if (!isInitialized()) {
                    throw newUninitializedMessageException(
                        result).asInvalidProtocolBufferException();
                }
                return buildPartial();
            }

            public org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest buildPartial() {
                if (result == null) {
                    throw new IllegalStateException(
                        "build() has already been called on this Builder.");
                }
                if (result.triedServers_ != java.util.Collections.EMPTY_LIST) {
                    result.triedServers_ =
                        java.util.Collections.unmodifiableList(result.triedServers_);
                }
                org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest returnMe = result;
                result = null;
                return returnMe;
            }

            public Builder mergeFrom(com.google.protobuf.Message other) {
                if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest) {
                    return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest)other);
                } else {
                    super.mergeFrom(other);
                    return this;
                }
            }

            public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest other) {
                if (other == org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest.getDefaultInstance()) return this;
                if (other.hasProtocolVersion()) {
                    setProtocolVersion(other.getProtocolVersion());
                }
                if (other.hasType()) {
                    setType(other.getType());
                }
                if (!other.triedServers_.isEmpty()) {
                    if (result.triedServers_.isEmpty()) {
                        result.triedServers_ = new java.util.ArrayList<com.google.protobuf.ByteString>();
                    }
                    result.triedServers_.addAll(other.triedServers_);
                }
                if (other.hasTxnId()) {
                    setTxnId(other.getTxnId());
                }
                if (other.hasShouldClaim()) {
                    setShouldClaim(other.getShouldClaim());
                }
                if (other.hasTopic()) {
                    setTopic(other.getTopic());
                }
                if (other.hasPublishRequest()) {
                    mergePublishRequest(other.getPublishRequest());
                }
                if (other.hasSubscribeRequest()) {
                    mergeSubscribeRequest(other.getSubscribeRequest());
                }
                if (other.hasConsumeRequest()) {
                    mergeConsumeRequest(other.getConsumeRequest());
                }
                if (other.hasUnsubscribeRequest()) {
                    mergeUnsubscribeRequest(other.getUnsubscribeRequest());
                }
                if (other.hasStopDeliveryRequest()) {
                    mergeStopDeliveryRequest(other.getStopDeliveryRequest());
                }
                if (other.hasStartDeliveryRequest()) {
                    mergeStartDeliveryRequest(other.getStartDeliveryRequest());
                }
                this.mergeUnknownFields(other.getUnknownFields());
                return this;
            }

            public Builder mergeFrom(
                com.google.protobuf.CodedInputStream input,
                com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                    throws java.io.IOException {
                com.google.protobuf.UnknownFieldSet.Builder unknownFields =
                    com.google.protobuf.UnknownFieldSet.newBuilder(
                        this.getUnknownFields());
                while (true) {
                    int tag = input.readTag();
                    switch (tag) {
                    case 0:
                        this.setUnknownFields(unknownFields.build());
                        return this;
                    default: {
                        if (!parseUnknownField(input, unknownFields,
                                               extensionRegistry, tag)) {
                            this.setUnknownFields(unknownFields.build());
                            return this;
                        }
                        break;
                    }
                    case 8: {
                        int rawValue = input.readEnum();
                        org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion value = org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion.valueOf(rawValue);
                        if (value == null) {
                            unknownFields.mergeVarintField(1, rawValue);
                        } else {
                            setProtocolVersion(value);
                        }
                        break;
                    }
                    case 16: {
                        int rawValue = input.readEnum();
                        org.apache.hedwig.protocol.PubSubProtocol.OperationType value = org.apache.hedwig.protocol.PubSubProtocol.OperationType.valueOf(rawValue);
                        if (value == null) {
                            unknownFields.mergeVarintField(2, rawValue);
                        } else {
                            setType(value);
                        }
                        break;
                    }
                    case 26: {
                        addTriedServers(input.readBytes());
                        break;
                    }
                    case 32: {
                        setTxnId(input.readUInt64());
                        break;
                    }
                    case 40: {
                        setShouldClaim(input.readBool());
                        break;
                    }
                    case 50: {
                        setTopic(input.readBytes());
                        break;
                    }
                    case 418: {
                        org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.newBuilder();
                        if (hasPublishRequest()) {
                            subBuilder.mergeFrom(getPublishRequest());
                        }
                        input.readMessage(subBuilder, extensionRegistry);
                        setPublishRequest(subBuilder.buildPartial());
                        break;
                    }
                    case 426: {
                        org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.newBuilder();
                        if (hasSubscribeRequest()) {
                            subBuilder.mergeFrom(getSubscribeRequest());
                        }
                        input.readMessage(subBuilder, extensionRegistry);
                        setSubscribeRequest(subBuilder.buildPartial());
                        break;
                    }
                    case 434: {
                        org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.newBuilder();
                        if (hasConsumeRequest()) {
                            subBuilder.mergeFrom(getConsumeRequest());
                        }
                        input.readMessage(subBuilder, extensionRegistry);
                        setConsumeRequest(subBuilder.buildPartial());
                        break;
                    }
                    case 442: {
                        org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.newBuilder();
                        if (hasUnsubscribeRequest()) {
                            subBuilder.mergeFrom(getUnsubscribeRequest());
                        }
                        input.readMessage(subBuilder, extensionRegistry);
                        setUnsubscribeRequest(subBuilder.buildPartial());
                        break;
                    }
                    case 450: {
                        org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.newBuilder();
                        if (hasStopDeliveryRequest()) {
                            subBuilder.mergeFrom(getStopDeliveryRequest());
                        }
                        input.readMessage(subBuilder, extensionRegistry);
                        setStopDeliveryRequest(subBuilder.buildPartial());
                        break;
                    }
                    case 458: {
                        org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.newBuilder();
                        if (hasStartDeliveryRequest()) {
                            subBuilder.mergeFrom(getStartDeliveryRequest());
                        }
                        input.readMessage(subBuilder, extensionRegistry);
                        setStartDeliveryRequest(subBuilder.buildPartial());
                        break;
                    }
                    }
                }
            }


            // required .Hedwig.ProtocolVersion protocolVersion = 1;
            public boolean hasProtocolVersion() {
                return result.hasProtocolVersion();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion getProtocolVersion() {
                return result.getProtocolVersion();
            }
            public Builder setProtocolVersion(org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.hasProtocolVersion = true;
                result.protocolVersion_ = value;
                return this;
            }
            public Builder clearProtocolVersion() {
                result.hasProtocolVersion = false;
                result.protocolVersion_ = org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion.VERSION_ONE;
                return this;
            }

            // required .Hedwig.OperationType type = 2;
            public boolean hasType() {
                return result.hasType();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.OperationType getType() {
                return result.getType();
            }
            public Builder setType(org.apache.hedwig.protocol.PubSubProtocol.OperationType value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.hasType = true;
                result.type_ = value;
                return this;
            }
            public Builder clearType() {
                result.hasType = false;
                result.type_ = org.apache.hedwig.protocol.PubSubProtocol.OperationType.PUBLISH;
                return this;
            }

            // repeated bytes triedServers = 3;
            public java.util.List<com.google.protobuf.ByteString> getTriedServersList() {
                return java.util.Collections.unmodifiableList(result.triedServers_);
            }
            public int getTriedServersCount() {
                return result.getTriedServersCount();
            }
            public com.google.protobuf.ByteString getTriedServers(int index) {
                return result.getTriedServers(index);
            }
            public Builder setTriedServers(int index, com.google.protobuf.ByteString value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.triedServers_.set(index, value);
                return this;
            }
            public Builder addTriedServers(com.google.protobuf.ByteString value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                if (result.triedServers_.isEmpty()) {
                    result.triedServers_ = new java.util.ArrayList<com.google.protobuf.ByteString>();
                }
                result.triedServers_.add(value);
                return this;
            }
            public Builder addAllTriedServers(
                java.lang.Iterable<? extends com.google.protobuf.ByteString> values) {
                if (result.triedServers_.isEmpty()) {
                    result.triedServers_ = new java.util.ArrayList<com.google.protobuf.ByteString>();
                }
                super.addAll(values, result.triedServers_);
                return this;
            }
            public Builder clearTriedServers() {
                result.triedServers_ = java.util.Collections.emptyList();
                return this;
            }

            // required uint64 txnId = 4;
            public boolean hasTxnId() {
                return result.hasTxnId();
            }
            public long getTxnId() {
                return result.getTxnId();
            }
            public Builder setTxnId(long value) {
                result.hasTxnId = true;
                result.txnId_ = value;
                return this;
            }
            public Builder clearTxnId() {
                result.hasTxnId = false;
                result.txnId_ = 0L;
                return this;
            }

            // optional bool shouldClaim = 5;
            public boolean hasShouldClaim() {
                return result.hasShouldClaim();
            }
            public boolean getShouldClaim() {
                return result.getShouldClaim();
            }
            public Builder setShouldClaim(boolean value) {
                result.hasShouldClaim = true;
                result.shouldClaim_ = value;
                return this;
            }
            public Builder clearShouldClaim() {
                result.hasShouldClaim = false;
                result.shouldClaim_ = false;
                return this;
            }

            // required bytes topic = 6;
            public boolean hasTopic() {
                return result.hasTopic();
            }
            public com.google.protobuf.ByteString getTopic() {
                return result.getTopic();
            }
            public Builder setTopic(com.google.protobuf.ByteString value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.hasTopic = true;
                result.topic_ = value;
                return this;
            }
            public Builder clearTopic() {
                result.hasTopic = false;
                result.topic_ = getDefaultInstance().getTopic();
                return this;
            }

            // optional .Hedwig.PublishRequest publishRequest = 52;
            public boolean hasPublishRequest() {
                return result.hasPublishRequest();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.PublishRequest getPublishRequest() {
                return result.getPublishRequest();
            }
            public Builder setPublishRequest(org.apache.hedwig.protocol.PubSubProtocol.PublishRequest value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.hasPublishRequest = true;
                result.publishRequest_ = value;
                return this;
            }
            public Builder setPublishRequest(org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.Builder builderForValue) {
                result.hasPublishRequest = true;
                result.publishRequest_ = builderForValue.build();
                return this;
            }
            public Builder mergePublishRequest(org.apache.hedwig.protocol.PubSubProtocol.PublishRequest value) {
                if (result.hasPublishRequest() &&
                        result.publishRequest_ != org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.getDefaultInstance()) {
                    result.publishRequest_ =
                        org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.newBuilder(result.publishRequest_).mergeFrom(value).buildPartial();
                } else {
                    result.publishRequest_ = value;
                }
                result.hasPublishRequest = true;
                return this;
            }
            public Builder clearPublishRequest() {
                result.hasPublishRequest = false;
                result.publishRequest_ = org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.getDefaultInstance();
                return this;
            }

            // optional .Hedwig.SubscribeRequest subscribeRequest = 53;
            public boolean hasSubscribeRequest() {
                return result.hasSubscribeRequest();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest getSubscribeRequest() {
                return result.getSubscribeRequest();
            }
            public Builder setSubscribeRequest(org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.hasSubscribeRequest = true;
                result.subscribeRequest_ = value;
                return this;
            }
            public Builder setSubscribeRequest(org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.Builder builderForValue) {
                result.hasSubscribeRequest = true;
                result.subscribeRequest_ = builderForValue.build();
                return this;
            }
            public Builder mergeSubscribeRequest(org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest value) {
                if (result.hasSubscribeRequest() &&
                        result.subscribeRequest_ != org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.getDefaultInstance()) {
                    result.subscribeRequest_ =
                        org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.newBuilder(result.subscribeRequest_).mergeFrom(value).buildPartial();
                } else {
                    result.subscribeRequest_ = value;
                }
                result.hasSubscribeRequest = true;
                return this;
            }
            public Builder clearSubscribeRequest() {
                result.hasSubscribeRequest = false;
                result.subscribeRequest_ = org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.getDefaultInstance();
                return this;
            }

            // optional .Hedwig.ConsumeRequest consumeRequest = 54;
            public boolean hasConsumeRequest() {
                return result.hasConsumeRequest();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest getConsumeRequest() {
                return result.getConsumeRequest();
            }
            public Builder setConsumeRequest(org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.hasConsumeRequest = true;
                result.consumeRequest_ = value;
                return this;
            }
            public Builder setConsumeRequest(org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.Builder builderForValue) {
                result.hasConsumeRequest = true;
                result.consumeRequest_ = builderForValue.build();
                return this;
            }
            public Builder mergeConsumeRequest(org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest value) {
                if (result.hasConsumeRequest() &&
                        result.consumeRequest_ != org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.getDefaultInstance()) {
                    result.consumeRequest_ =
                        org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.newBuilder(result.consumeRequest_).mergeFrom(value).buildPartial();
                } else {
                    result.consumeRequest_ = value;
                }
                result.hasConsumeRequest = true;
                return this;
            }
            public Builder clearConsumeRequest() {
                result.hasConsumeRequest = false;
                result.consumeRequest_ = org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.getDefaultInstance();
                return this;
            }

            // optional .Hedwig.UnsubscribeRequest unsubscribeRequest = 55;
            public boolean hasUnsubscribeRequest() {
                return result.hasUnsubscribeRequest();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest getUnsubscribeRequest() {
                return result.getUnsubscribeRequest();
            }
            public Builder setUnsubscribeRequest(org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.hasUnsubscribeRequest = true;
                result.unsubscribeRequest_ = value;
                return this;
            }
            public Builder setUnsubscribeRequest(org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.Builder builderForValue) {
                result.hasUnsubscribeRequest = true;
                result.unsubscribeRequest_ = builderForValue.build();
                return this;
            }
            public Builder mergeUnsubscribeRequest(org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest value) {
                if (result.hasUnsubscribeRequest() &&
                        result.unsubscribeRequest_ != org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.getDefaultInstance()) {
                    result.unsubscribeRequest_ =
                        org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.newBuilder(result.unsubscribeRequest_).mergeFrom(value).buildPartial();
                } else {
                    result.unsubscribeRequest_ = value;
                }
                result.hasUnsubscribeRequest = true;
                return this;
            }
            public Builder clearUnsubscribeRequest() {
                result.hasUnsubscribeRequest = false;
                result.unsubscribeRequest_ = org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.getDefaultInstance();
                return this;
            }

            // optional .Hedwig.StopDeliveryRequest stopDeliveryRequest = 56;
            public boolean hasStopDeliveryRequest() {
                return result.hasStopDeliveryRequest();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest getStopDeliveryRequest() {
                return result.getStopDeliveryRequest();
            }
            public Builder setStopDeliveryRequest(org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.hasStopDeliveryRequest = true;
                result.stopDeliveryRequest_ = value;
                return this;
            }
            public Builder setStopDeliveryRequest(org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.Builder builderForValue) {
                result.hasStopDeliveryRequest = true;
                result.stopDeliveryRequest_ = builderForValue.build();
                return this;
            }
            public Builder mergeStopDeliveryRequest(org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest value) {
                if (result.hasStopDeliveryRequest() &&
                        result.stopDeliveryRequest_ != org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.getDefaultInstance()) {
                    result.stopDeliveryRequest_ =
                        org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.newBuilder(result.stopDeliveryRequest_).mergeFrom(value).buildPartial();
                } else {
                    result.stopDeliveryRequest_ = value;
                }
                result.hasStopDeliveryRequest = true;
                return this;
            }
            public Builder clearStopDeliveryRequest() {
                result.hasStopDeliveryRequest = false;
                result.stopDeliveryRequest_ = org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.getDefaultInstance();
                return this;
            }

            // optional .Hedwig.StartDeliveryRequest startDeliveryRequest = 57;
            public boolean hasStartDeliveryRequest() {
                return result.hasStartDeliveryRequest();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest getStartDeliveryRequest() {
                return result.getStartDeliveryRequest();
            }
            public Builder setStartDeliveryRequest(org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.hasStartDeliveryRequest = true;
                result.startDeliveryRequest_ = value;
                return this;
            }
            public Builder setStartDeliveryRequest(org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.Builder builderForValue) {
                result.hasStartDeliveryRequest = true;
                result.startDeliveryRequest_ = builderForValue.build();
                return this;
            }
            public Builder mergeStartDeliveryRequest(org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest value) {
                if (result.hasStartDeliveryRequest() &&
                        result.startDeliveryRequest_ != org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.getDefaultInstance()) {
                    result.startDeliveryRequest_ =
                        org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.newBuilder(result.startDeliveryRequest_).mergeFrom(value).buildPartial();
                } else {
                    result.startDeliveryRequest_ = value;
                }
                result.hasStartDeliveryRequest = true;
                return this;
            }
            public Builder clearStartDeliveryRequest() {
                result.hasStartDeliveryRequest = false;
                result.startDeliveryRequest_ = org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.getDefaultInstance();
                return this;
            }

            // @@protoc_insertion_point(builder_scope:Hedwig.PubSubRequest)
        }

        static {
            defaultInstance = new PubSubRequest(true);
            org.apache.hedwig.protocol.PubSubProtocol.internalForceInit();
            defaultInstance.initFields();
        }

        // @@protoc_insertion_point(class_scope:Hedwig.PubSubRequest)
    }

    public static final class PublishRequest extends
        com.google.protobuf.GeneratedMessage {
        // Use PublishRequest.newBuilder() to construct.
        private PublishRequest() {
            initFields();
        }
        private PublishRequest(boolean noInit) {}

        private static final PublishRequest defaultInstance;
        public static PublishRequest getDefaultInstance() {
            return defaultInstance;
        }

        public PublishRequest getDefaultInstanceForType() {
            return defaultInstance;
        }

        public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
            return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_PublishRequest_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
            return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_PublishRequest_fieldAccessorTable;
        }

        // required .Hedwig.Message msg = 2;
        public static final int MSG_FIELD_NUMBER = 2;
        private boolean hasMsg;
        private org.apache.hedwig.protocol.PubSubProtocol.Message msg_;
        public boolean hasMsg() {
            return hasMsg;
        }
        public org.apache.hedwig.protocol.PubSubProtocol.Message getMsg() {
            return msg_;
        }

        private void initFields() {
            msg_ = org.apache.hedwig.protocol.PubSubProtocol.Message.getDefaultInstance();
        }
        public final boolean isInitialized() {
            if (!hasMsg) return false;
            if (!getMsg().isInitialized()) return false;
            return true;
        }

        public void writeTo(com.google.protobuf.CodedOutputStream output)
                throws java.io.IOException {
            getSerializedSize();
            if (hasMsg()) {
                output.writeMessage(2, getMsg());
            }
            getUnknownFields().writeTo(output);
        }

        private int memoizedSerializedSize = -1;
        public int getSerializedSize() {
            int size = memoizedSerializedSize;
            if (size != -1) return size;

            size = 0;
            if (hasMsg()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeMessageSize(2, getMsg());
            }
            size += getUnknownFields().getSerializedSize();
            memoizedSerializedSize = size;
            return size;
        }

        public static org.apache.hedwig.protocol.PubSubProtocol.PublishRequest parseFrom(
            com.google.protobuf.ByteString data)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.PublishRequest parseFrom(
            com.google.protobuf.ByteString data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.PublishRequest parseFrom(byte[] data)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.PublishRequest parseFrom(
            byte[] data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.PublishRequest parseFrom(java.io.InputStream input)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.PublishRequest parseFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.PublishRequest parseDelimitedFrom(java.io.InputStream input)
                throws java.io.IOException {
            Builder builder = newBuilder();
            if (builder.mergeDelimitedFrom(input)) {
                return builder.buildParsed();
            } else {
                return null;
            }
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.PublishRequest parseDelimitedFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            Builder builder = newBuilder();
            if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
                return builder.buildParsed();
            } else {
                return null;
            }
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.PublishRequest parseFrom(
            com.google.protobuf.CodedInputStream input)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.PublishRequest parseFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input, extensionRegistry)
                   .buildParsed();
        }

        public static Builder newBuilder() {
            return Builder.create();
        }
        public Builder newBuilderForType() {
            return newBuilder();
        }
        public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.PublishRequest prototype) {
            return newBuilder().mergeFrom(prototype);
        }
        public Builder toBuilder() {
            return newBuilder(this);
        }

        public static final class Builder extends
            com.google.protobuf.GeneratedMessage.Builder<Builder> {
            private org.apache.hedwig.protocol.PubSubProtocol.PublishRequest result;

            // Construct using org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.newBuilder()
            private Builder() {}

            private static Builder create() {
                Builder builder = new Builder();
                builder.result = new org.apache.hedwig.protocol.PubSubProtocol.PublishRequest();
                return builder;
            }

            protected org.apache.hedwig.protocol.PubSubProtocol.PublishRequest internalGetResult() {
                return result;
            }

            public Builder clear() {
                if (result == null) {
                    throw new IllegalStateException(
                        "Cannot call clear() after build().");
                }
                result = new org.apache.hedwig.protocol.PubSubProtocol.PublishRequest();
                return this;
            }

            public Builder clone() {
                return create().mergeFrom(result);
            }

            public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
                return org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.getDescriptor();
            }

            public org.apache.hedwig.protocol.PubSubProtocol.PublishRequest getDefaultInstanceForType() {
                return org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.getDefaultInstance();
            }

            public boolean isInitialized() {
                return result.isInitialized();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.PublishRequest build() {
                if (result != null && !isInitialized()) {
                    throw newUninitializedMessageException(result);
                }
                return buildPartial();
            }

            private org.apache.hedwig.protocol.PubSubProtocol.PublishRequest buildParsed()
                    throws com.google.protobuf.InvalidProtocolBufferException {
                if (!isInitialized()) {
                    throw newUninitializedMessageException(
                        result).asInvalidProtocolBufferException();
                }
                return buildPartial();
            }

            public org.apache.hedwig.protocol.PubSubProtocol.PublishRequest buildPartial() {
                if (result == null) {
                    throw new IllegalStateException(
                        "build() has already been called on this Builder.");
                }
                org.apache.hedwig.protocol.PubSubProtocol.PublishRequest returnMe = result;
                result = null;
                return returnMe;
            }

            public Builder mergeFrom(com.google.protobuf.Message other) {
                if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.PublishRequest) {
                    return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.PublishRequest)other);
                } else {
                    super.mergeFrom(other);
                    return this;
                }
            }

            public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.PublishRequest other) {
                if (other == org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.getDefaultInstance()) return this;
                if (other.hasMsg()) {
                    mergeMsg(other.getMsg());
                }
                this.mergeUnknownFields(other.getUnknownFields());
                return this;
            }

            public Builder mergeFrom(
                com.google.protobuf.CodedInputStream input,
                com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                    throws java.io.IOException {
                com.google.protobuf.UnknownFieldSet.Builder unknownFields =
                    com.google.protobuf.UnknownFieldSet.newBuilder(
                        this.getUnknownFields());
                while (true) {
                    int tag = input.readTag();
                    switch (tag) {
                    case 0:
                        this.setUnknownFields(unknownFields.build());
                        return this;
                    default: {
                        if (!parseUnknownField(input, unknownFields,
                                               extensionRegistry, tag)) {
                            this.setUnknownFields(unknownFields.build());
                            return this;
                        }
                        break;
                    }
                    case 18: {
                        org.apache.hedwig.protocol.PubSubProtocol.Message.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.Message.newBuilder();
                        if (hasMsg()) {
                            subBuilder.mergeFrom(getMsg());
                        }
                        input.readMessage(subBuilder, extensionRegistry);
                        setMsg(subBuilder.buildPartial());
                        break;
                    }
                    }
                }
            }


            // required .Hedwig.Message msg = 2;
            public boolean hasMsg() {
                return result.hasMsg();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.Message getMsg() {
                return result.getMsg();
            }
            public Builder setMsg(org.apache.hedwig.protocol.PubSubProtocol.Message value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.hasMsg = true;
                result.msg_ = value;
                return this;
            }
            public Builder setMsg(org.apache.hedwig.protocol.PubSubProtocol.Message.Builder builderForValue) {
                result.hasMsg = true;
                result.msg_ = builderForValue.build();
                return this;
            }
            public Builder mergeMsg(org.apache.hedwig.protocol.PubSubProtocol.Message value) {
                if (result.hasMsg() &&
                        result.msg_ != org.apache.hedwig.protocol.PubSubProtocol.Message.getDefaultInstance()) {
                    result.msg_ =
                        org.apache.hedwig.protocol.PubSubProtocol.Message.newBuilder(result.msg_).mergeFrom(value).buildPartial();
                } else {
                    result.msg_ = value;
                }
                result.hasMsg = true;
                return this;
            }
            public Builder clearMsg() {
                result.hasMsg = false;
                result.msg_ = org.apache.hedwig.protocol.PubSubProtocol.Message.getDefaultInstance();
                return this;
            }

            // @@protoc_insertion_point(builder_scope:Hedwig.PublishRequest)
        }

        static {
            defaultInstance = new PublishRequest(true);
            org.apache.hedwig.protocol.PubSubProtocol.internalForceInit();
            defaultInstance.initFields();
        }

        // @@protoc_insertion_point(class_scope:Hedwig.PublishRequest)
    }

    public static final class SubscribeRequest extends
        com.google.protobuf.GeneratedMessage {
        // Use SubscribeRequest.newBuilder() to construct.
        private SubscribeRequest() {
            initFields();
        }
        private SubscribeRequest(boolean noInit) {}

        private static final SubscribeRequest defaultInstance;
        public static SubscribeRequest getDefaultInstance() {
            return defaultInstance;
        }

        public SubscribeRequest getDefaultInstanceForType() {
            return defaultInstance;
        }

        public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
            return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscribeRequest_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
            return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscribeRequest_fieldAccessorTable;
        }

        public enum CreateOrAttach
        implements com.google.protobuf.ProtocolMessageEnum {
            CREATE(0, 0),
            ATTACH(1, 1),
            CREATE_OR_ATTACH(2, 2),
            ;


            public final int getNumber() {
                return value;
            }

            public static CreateOrAttach valueOf(int value) {
                switch (value) {
                case 0:
                    return CREATE;
                case 1:
                    return ATTACH;
                case 2:
                    return CREATE_OR_ATTACH;
                default:
                    return null;
                }
            }

            public static com.google.protobuf.Internal.EnumLiteMap<CreateOrAttach>
            internalGetValueMap() {
                return internalValueMap;
            }
            private static com.google.protobuf.Internal.EnumLiteMap<CreateOrAttach>
            internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<CreateOrAttach>() {
                public CreateOrAttach findValueByNumber(int number) {
                    return CreateOrAttach.valueOf(number)
                           ;
                }
            };

            public final com.google.protobuf.Descriptors.EnumValueDescriptor
            getValueDescriptor() {
                return getDescriptor().getValues().get(index);
            }
            public final com.google.protobuf.Descriptors.EnumDescriptor
            getDescriptorForType() {
                return getDescriptor();
            }
            public static final com.google.protobuf.Descriptors.EnumDescriptor
            getDescriptor() {
                return org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.getDescriptor().getEnumTypes().get(0);
            }

            private static final CreateOrAttach[] VALUES = {
                CREATE, ATTACH, CREATE_OR_ATTACH,
            };
            public static CreateOrAttach valueOf(
                com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
                if (desc.getType() != getDescriptor()) {
                    throw new java.lang.IllegalArgumentException(
                        "EnumValueDescriptor is not for this type.");
                }
                return VALUES[desc.getIndex()];
            }
            private final int index;
            private final int value;
            private CreateOrAttach(int index, int value) {
                this.index = index;
                this.value = value;
            }

            static {
                org.apache.hedwig.protocol.PubSubProtocol.getDescriptor();
            }

            // @@protoc_insertion_point(enum_scope:Hedwig.SubscribeRequest.CreateOrAttach)
        }

        // required bytes subscriberId = 2;
        public static final int SUBSCRIBERID_FIELD_NUMBER = 2;
        private boolean hasSubscriberId;
        private com.google.protobuf.ByteString subscriberId_ = com.google.protobuf.ByteString.EMPTY;
        public boolean hasSubscriberId() {
            return hasSubscriberId;
        }
        public com.google.protobuf.ByteString getSubscriberId() {
            return subscriberId_;
        }

        // optional .Hedwig.SubscribeRequest.CreateOrAttach createOrAttach = 3 [default = CREATE_OR_ATTACH];
        public static final int CREATEORATTACH_FIELD_NUMBER = 3;
        private boolean hasCreateOrAttach;
        private org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach createOrAttach_;
        public boolean hasCreateOrAttach() {
            return hasCreateOrAttach;
        }
        public org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach getCreateOrAttach() {
            return createOrAttach_;
        }

        // optional bool synchronous = 4 [default = false];
        public static final int SYNCHRONOUS_FIELD_NUMBER = 4;
        private boolean hasSynchronous;
        private boolean synchronous_ = false;
        public boolean hasSynchronous() {
            return hasSynchronous;
        }
        public boolean getSynchronous() {
            return synchronous_;
        }

        private void initFields() {
            createOrAttach_ = org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach.CREATE_OR_ATTACH;
        }
        public final boolean isInitialized() {
            if (!hasSubscriberId) return false;
            return true;
        }

        public void writeTo(com.google.protobuf.CodedOutputStream output)
                throws java.io.IOException {
            getSerializedSize();
            if (hasSubscriberId()) {
                output.writeBytes(2, getSubscriberId());
            }
            if (hasCreateOrAttach()) {
                output.writeEnum(3, getCreateOrAttach().getNumber());
            }
            if (hasSynchronous()) {
                output.writeBool(4, getSynchronous());
            }
            getUnknownFields().writeTo(output);
        }

        private int memoizedSerializedSize = -1;
        public int getSerializedSize() {
            int size = memoizedSerializedSize;
            if (size != -1) return size;

            size = 0;
            if (hasSubscriberId()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeBytesSize(2, getSubscriberId());
            }
            if (hasCreateOrAttach()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeEnumSize(3, getCreateOrAttach().getNumber());
            }
            if (hasSynchronous()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeBoolSize(4, getSynchronous());
            }
            size += getUnknownFields().getSerializedSize();
            memoizedSerializedSize = size;
            return size;
        }

        public static org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest parseFrom(
            com.google.protobuf.ByteString data)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest parseFrom(
            com.google.protobuf.ByteString data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest parseFrom(byte[] data)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest parseFrom(
            byte[] data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest parseFrom(java.io.InputStream input)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest parseFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest parseDelimitedFrom(java.io.InputStream input)
                throws java.io.IOException {
            Builder builder = newBuilder();
            if (builder.mergeDelimitedFrom(input)) {
                return builder.buildParsed();
            } else {
                return null;
            }
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest parseDelimitedFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            Builder builder = newBuilder();
            if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
                return builder.buildParsed();
            } else {
                return null;
            }
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest parseFrom(
            com.google.protobuf.CodedInputStream input)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest parseFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input, extensionRegistry)
                   .buildParsed();
        }

        public static Builder newBuilder() {
            return Builder.create();
        }
        public Builder newBuilderForType() {
            return newBuilder();
        }
        public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest prototype) {
            return newBuilder().mergeFrom(prototype);
        }
        public Builder toBuilder() {
            return newBuilder(this);
        }

        public static final class Builder extends
            com.google.protobuf.GeneratedMessage.Builder<Builder> {
            private org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest result;

            // Construct using org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.newBuilder()
            private Builder() {}

            private static Builder create() {
                Builder builder = new Builder();
                builder.result = new org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest();
                return builder;
            }

            protected org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest internalGetResult() {
                return result;
            }

            public Builder clear() {
                if (result == null) {
                    throw new IllegalStateException(
                        "Cannot call clear() after build().");
                }
                result = new org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest();
                return this;
            }

            public Builder clone() {
                return create().mergeFrom(result);
            }

            public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
                return org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.getDescriptor();
            }

            public org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest getDefaultInstanceForType() {
                return org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.getDefaultInstance();
            }

            public boolean isInitialized() {
                return result.isInitialized();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest build() {
                if (result != null && !isInitialized()) {
                    throw newUninitializedMessageException(result);
                }
                return buildPartial();
            }

            private org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest buildParsed()
                    throws com.google.protobuf.InvalidProtocolBufferException {
                if (!isInitialized()) {
                    throw newUninitializedMessageException(
                        result).asInvalidProtocolBufferException();
                }
                return buildPartial();
            }

            public org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest buildPartial() {
                if (result == null) {
                    throw new IllegalStateException(
                        "build() has already been called on this Builder.");
                }
                org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest returnMe = result;
                result = null;
                return returnMe;
            }

            public Builder mergeFrom(com.google.protobuf.Message other) {
                if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest) {
                    return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest)other);
                } else {
                    super.mergeFrom(other);
                    return this;
                }
            }

            public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest other) {
                if (other == org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.getDefaultInstance()) return this;
                if (other.hasSubscriberId()) {
                    setSubscriberId(other.getSubscriberId());
                }
                if (other.hasCreateOrAttach()) {
                    setCreateOrAttach(other.getCreateOrAttach());
                }
                if (other.hasSynchronous()) {
                    setSynchronous(other.getSynchronous());
                }
                this.mergeUnknownFields(other.getUnknownFields());
                return this;
            }

            public Builder mergeFrom(
                com.google.protobuf.CodedInputStream input,
                com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                    throws java.io.IOException {
                com.google.protobuf.UnknownFieldSet.Builder unknownFields =
                    com.google.protobuf.UnknownFieldSet.newBuilder(
                        this.getUnknownFields());
                while (true) {
                    int tag = input.readTag();
                    switch (tag) {
                    case 0:
                        this.setUnknownFields(unknownFields.build());
                        return this;
                    default: {
                        if (!parseUnknownField(input, unknownFields,
                                               extensionRegistry, tag)) {
                            this.setUnknownFields(unknownFields.build());
                            return this;
                        }
                        break;
                    }
                    case 18: {
                        setSubscriberId(input.readBytes());
                        break;
                    }
                    case 24: {
                        int rawValue = input.readEnum();
                        org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach value = org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach.valueOf(rawValue);
                        if (value == null) {
                            unknownFields.mergeVarintField(3, rawValue);
                        } else {
                            setCreateOrAttach(value);
                        }
                        break;
                    }
                    case 32: {
                        setSynchronous(input.readBool());
                        break;
                    }
                    }
                }
            }


            // required bytes subscriberId = 2;
            public boolean hasSubscriberId() {
                return result.hasSubscriberId();
            }
            public com.google.protobuf.ByteString getSubscriberId() {
                return result.getSubscriberId();
            }
            public Builder setSubscriberId(com.google.protobuf.ByteString value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.hasSubscriberId = true;
                result.subscriberId_ = value;
                return this;
            }
            public Builder clearSubscriberId() {
                result.hasSubscriberId = false;
                result.subscriberId_ = getDefaultInstance().getSubscriberId();
                return this;
            }

            // optional .Hedwig.SubscribeRequest.CreateOrAttach createOrAttach = 3 [default = CREATE_OR_ATTACH];
            public boolean hasCreateOrAttach() {
                return result.hasCreateOrAttach();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach getCreateOrAttach() {
                return result.getCreateOrAttach();
            }
            public Builder setCreateOrAttach(org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.hasCreateOrAttach = true;
                result.createOrAttach_ = value;
                return this;
            }
            public Builder clearCreateOrAttach() {
                result.hasCreateOrAttach = false;
                result.createOrAttach_ = org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach.CREATE_OR_ATTACH;
                return this;
            }

            // optional bool synchronous = 4 [default = false];
            public boolean hasSynchronous() {
                return result.hasSynchronous();
            }
            public boolean getSynchronous() {
                return result.getSynchronous();
            }
            public Builder setSynchronous(boolean value) {
                result.hasSynchronous = true;
                result.synchronous_ = value;
                return this;
            }
            public Builder clearSynchronous() {
                result.hasSynchronous = false;
                result.synchronous_ = false;
                return this;
            }

            // @@protoc_insertion_point(builder_scope:Hedwig.SubscribeRequest)
        }

        static {
            defaultInstance = new SubscribeRequest(true);
            org.apache.hedwig.protocol.PubSubProtocol.internalForceInit();
            defaultInstance.initFields();
        }

        // @@protoc_insertion_point(class_scope:Hedwig.SubscribeRequest)
    }

    public static final class ConsumeRequest extends
        com.google.protobuf.GeneratedMessage {
        // Use ConsumeRequest.newBuilder() to construct.
        private ConsumeRequest() {
            initFields();
        }
        private ConsumeRequest(boolean noInit) {}

        private static final ConsumeRequest defaultInstance;
        public static ConsumeRequest getDefaultInstance() {
            return defaultInstance;
        }

        public ConsumeRequest getDefaultInstanceForType() {
            return defaultInstance;
        }

        public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
            return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_ConsumeRequest_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
            return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_ConsumeRequest_fieldAccessorTable;
        }

        // required bytes subscriberId = 2;
        public static final int SUBSCRIBERID_FIELD_NUMBER = 2;
        private boolean hasSubscriberId;
        private com.google.protobuf.ByteString subscriberId_ = com.google.protobuf.ByteString.EMPTY;
        public boolean hasSubscriberId() {
            return hasSubscriberId;
        }
        public com.google.protobuf.ByteString getSubscriberId() {
            return subscriberId_;
        }

        // required .Hedwig.MessageSeqId msgId = 3;
        public static final int MSGID_FIELD_NUMBER = 3;
        private boolean hasMsgId;
        private org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId msgId_;
        public boolean hasMsgId() {
            return hasMsgId;
        }
        public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId getMsgId() {
            return msgId_;
        }

        private void initFields() {
            msgId_ = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance();
        }
        public final boolean isInitialized() {
            if (!hasSubscriberId) return false;
            if (!hasMsgId) return false;
            if (!getMsgId().isInitialized()) return false;
            return true;
        }

        public void writeTo(com.google.protobuf.CodedOutputStream output)
                throws java.io.IOException {
            getSerializedSize();
            if (hasSubscriberId()) {
                output.writeBytes(2, getSubscriberId());
            }
            if (hasMsgId()) {
                output.writeMessage(3, getMsgId());
            }
            getUnknownFields().writeTo(output);
        }

        private int memoizedSerializedSize = -1;
        public int getSerializedSize() {
            int size = memoizedSerializedSize;
            if (size != -1) return size;

            size = 0;
            if (hasSubscriberId()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeBytesSize(2, getSubscriberId());
            }
            if (hasMsgId()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeMessageSize(3, getMsgId());
            }
            size += getUnknownFields().getSerializedSize();
            memoizedSerializedSize = size;
            return size;
        }

        public static org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest parseFrom(
            com.google.protobuf.ByteString data)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest parseFrom(
            com.google.protobuf.ByteString data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest parseFrom(byte[] data)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest parseFrom(
            byte[] data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest parseFrom(java.io.InputStream input)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest parseFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest parseDelimitedFrom(java.io.InputStream input)
                throws java.io.IOException {
            Builder builder = newBuilder();
            if (builder.mergeDelimitedFrom(input)) {
                return builder.buildParsed();
            } else {
                return null;
            }
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest parseDelimitedFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            Builder builder = newBuilder();
            if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
                return builder.buildParsed();
            } else {
                return null;
            }
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest parseFrom(
            com.google.protobuf.CodedInputStream input)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest parseFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input, extensionRegistry)
                   .buildParsed();
        }

        public static Builder newBuilder() {
            return Builder.create();
        }
        public Builder newBuilderForType() {
            return newBuilder();
        }
        public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest prototype) {
            return newBuilder().mergeFrom(prototype);
        }
        public Builder toBuilder() {
            return newBuilder(this);
        }

        public static final class Builder extends
            com.google.protobuf.GeneratedMessage.Builder<Builder> {
            private org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest result;

            // Construct using org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.newBuilder()
            private Builder() {}

            private static Builder create() {
                Builder builder = new Builder();
                builder.result = new org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest();
                return builder;
            }

            protected org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest internalGetResult() {
                return result;
            }

            public Builder clear() {
                if (result == null) {
                    throw new IllegalStateException(
                        "Cannot call clear() after build().");
                }
                result = new org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest();
                return this;
            }

            public Builder clone() {
                return create().mergeFrom(result);
            }

            public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
                return org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.getDescriptor();
            }

            public org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest getDefaultInstanceForType() {
                return org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.getDefaultInstance();
            }

            public boolean isInitialized() {
                return result.isInitialized();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest build() {
                if (result != null && !isInitialized()) {
                    throw newUninitializedMessageException(result);
                }
                return buildPartial();
            }

            private org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest buildParsed()
                    throws com.google.protobuf.InvalidProtocolBufferException {
                if (!isInitialized()) {
                    throw newUninitializedMessageException(
                        result).asInvalidProtocolBufferException();
                }
                return buildPartial();
            }

            public org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest buildPartial() {
                if (result == null) {
                    throw new IllegalStateException(
                        "build() has already been called on this Builder.");
                }
                org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest returnMe = result;
                result = null;
                return returnMe;
            }

            public Builder mergeFrom(com.google.protobuf.Message other) {
                if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest) {
                    return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest)other);
                } else {
                    super.mergeFrom(other);
                    return this;
                }
            }

            public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest other) {
                if (other == org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.getDefaultInstance()) return this;
                if (other.hasSubscriberId()) {
                    setSubscriberId(other.getSubscriberId());
                }
                if (other.hasMsgId()) {
                    mergeMsgId(other.getMsgId());
                }
                this.mergeUnknownFields(other.getUnknownFields());
                return this;
            }

            public Builder mergeFrom(
                com.google.protobuf.CodedInputStream input,
                com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                    throws java.io.IOException {
                com.google.protobuf.UnknownFieldSet.Builder unknownFields =
                    com.google.protobuf.UnknownFieldSet.newBuilder(
                        this.getUnknownFields());
                while (true) {
                    int tag = input.readTag();
                    switch (tag) {
                    case 0:
                        this.setUnknownFields(unknownFields.build());
                        return this;
                    default: {
                        if (!parseUnknownField(input, unknownFields,
                                               extensionRegistry, tag)) {
                            this.setUnknownFields(unknownFields.build());
                            return this;
                        }
                        break;
                    }
                    case 18: {
                        setSubscriberId(input.readBytes());
                        break;
                    }
                    case 26: {
                        org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.newBuilder();
                        if (hasMsgId()) {
                            subBuilder.mergeFrom(getMsgId());
                        }
                        input.readMessage(subBuilder, extensionRegistry);
                        setMsgId(subBuilder.buildPartial());
                        break;
                    }
                    }
                }
            }


            // required bytes subscriberId = 2;
            public boolean hasSubscriberId() {
                return result.hasSubscriberId();
            }
            public com.google.protobuf.ByteString getSubscriberId() {
                return result.getSubscriberId();
            }
            public Builder setSubscriberId(com.google.protobuf.ByteString value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.hasSubscriberId = true;
                result.subscriberId_ = value;
                return this;
            }
            public Builder clearSubscriberId() {
                result.hasSubscriberId = false;
                result.subscriberId_ = getDefaultInstance().getSubscriberId();
                return this;
            }

            // required .Hedwig.MessageSeqId msgId = 3;
            public boolean hasMsgId() {
                return result.hasMsgId();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId getMsgId() {
                return result.getMsgId();
            }
            public Builder setMsgId(org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.hasMsgId = true;
                result.msgId_ = value;
                return this;
            }
            public Builder setMsgId(org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder builderForValue) {
                result.hasMsgId = true;
                result.msgId_ = builderForValue.build();
                return this;
            }
            public Builder mergeMsgId(org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId value) {
                if (result.hasMsgId() &&
                        result.msgId_ != org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance()) {
                    result.msgId_ =
                        org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.newBuilder(result.msgId_).mergeFrom(value).buildPartial();
                } else {
                    result.msgId_ = value;
                }
                result.hasMsgId = true;
                return this;
            }
            public Builder clearMsgId() {
                result.hasMsgId = false;
                result.msgId_ = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance();
                return this;
            }

            // @@protoc_insertion_point(builder_scope:Hedwig.ConsumeRequest)
        }

        static {
            defaultInstance = new ConsumeRequest(true);
            org.apache.hedwig.protocol.PubSubProtocol.internalForceInit();
            defaultInstance.initFields();
        }

        // @@protoc_insertion_point(class_scope:Hedwig.ConsumeRequest)
    }

    public static final class UnsubscribeRequest extends
        com.google.protobuf.GeneratedMessage {
        // Use UnsubscribeRequest.newBuilder() to construct.
        private UnsubscribeRequest() {
            initFields();
        }
        private UnsubscribeRequest(boolean noInit) {}

        private static final UnsubscribeRequest defaultInstance;
        public static UnsubscribeRequest getDefaultInstance() {
            return defaultInstance;
        }

        public UnsubscribeRequest getDefaultInstanceForType() {
            return defaultInstance;
        }

        public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
            return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_UnsubscribeRequest_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
            return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_UnsubscribeRequest_fieldAccessorTable;
        }

        // required bytes subscriberId = 2;
        public static final int SUBSCRIBERID_FIELD_NUMBER = 2;
        private boolean hasSubscriberId;
        private com.google.protobuf.ByteString subscriberId_ = com.google.protobuf.ByteString.EMPTY;
        public boolean hasSubscriberId() {
            return hasSubscriberId;
        }
        public com.google.protobuf.ByteString getSubscriberId() {
            return subscriberId_;
        }

        private void initFields() {
        }
        public final boolean isInitialized() {
            if (!hasSubscriberId) return false;
            return true;
        }

        public void writeTo(com.google.protobuf.CodedOutputStream output)
                throws java.io.IOException {
            getSerializedSize();
            if (hasSubscriberId()) {
                output.writeBytes(2, getSubscriberId());
            }
            getUnknownFields().writeTo(output);
        }

        private int memoizedSerializedSize = -1;
        public int getSerializedSize() {
            int size = memoizedSerializedSize;
            if (size != -1) return size;

            size = 0;
            if (hasSubscriberId()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeBytesSize(2, getSubscriberId());
            }
            size += getUnknownFields().getSerializedSize();
            memoizedSerializedSize = size;
            return size;
        }

        public static org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest parseFrom(
            com.google.protobuf.ByteString data)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest parseFrom(
            com.google.protobuf.ByteString data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest parseFrom(byte[] data)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest parseFrom(
            byte[] data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest parseFrom(java.io.InputStream input)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest parseFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest parseDelimitedFrom(java.io.InputStream input)
                throws java.io.IOException {
            Builder builder = newBuilder();
            if (builder.mergeDelimitedFrom(input)) {
                return builder.buildParsed();
            } else {
                return null;
            }
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest parseDelimitedFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            Builder builder = newBuilder();
            if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
                return builder.buildParsed();
            } else {
                return null;
            }
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest parseFrom(
            com.google.protobuf.CodedInputStream input)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest parseFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input, extensionRegistry)
                   .buildParsed();
        }

        public static Builder newBuilder() {
            return Builder.create();
        }
        public Builder newBuilderForType() {
            return newBuilder();
        }
        public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest prototype) {
            return newBuilder().mergeFrom(prototype);
        }
        public Builder toBuilder() {
            return newBuilder(this);
        }

        public static final class Builder extends
            com.google.protobuf.GeneratedMessage.Builder<Builder> {
            private org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest result;

            // Construct using org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.newBuilder()
            private Builder() {}

            private static Builder create() {
                Builder builder = new Builder();
                builder.result = new org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest();
                return builder;
            }

            protected org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest internalGetResult() {
                return result;
            }

            public Builder clear() {
                if (result == null) {
                    throw new IllegalStateException(
                        "Cannot call clear() after build().");
                }
                result = new org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest();
                return this;
            }

            public Builder clone() {
                return create().mergeFrom(result);
            }

            public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
                return org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.getDescriptor();
            }

            public org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest getDefaultInstanceForType() {
                return org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.getDefaultInstance();
            }

            public boolean isInitialized() {
                return result.isInitialized();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest build() {
                if (result != null && !isInitialized()) {
                    throw newUninitializedMessageException(result);
                }
                return buildPartial();
            }

            private org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest buildParsed()
                    throws com.google.protobuf.InvalidProtocolBufferException {
                if (!isInitialized()) {
                    throw newUninitializedMessageException(
                        result).asInvalidProtocolBufferException();
                }
                return buildPartial();
            }

            public org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest buildPartial() {
                if (result == null) {
                    throw new IllegalStateException(
                        "build() has already been called on this Builder.");
                }
                org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest returnMe = result;
                result = null;
                return returnMe;
            }

            public Builder mergeFrom(com.google.protobuf.Message other) {
                if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest) {
                    return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest)other);
                } else {
                    super.mergeFrom(other);
                    return this;
                }
            }

            public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest other) {
                if (other == org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.getDefaultInstance()) return this;
                if (other.hasSubscriberId()) {
                    setSubscriberId(other.getSubscriberId());
                }
                this.mergeUnknownFields(other.getUnknownFields());
                return this;
            }

            public Builder mergeFrom(
                com.google.protobuf.CodedInputStream input,
                com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                    throws java.io.IOException {
                com.google.protobuf.UnknownFieldSet.Builder unknownFields =
                    com.google.protobuf.UnknownFieldSet.newBuilder(
                        this.getUnknownFields());
                while (true) {
                    int tag = input.readTag();
                    switch (tag) {
                    case 0:
                        this.setUnknownFields(unknownFields.build());
                        return this;
                    default: {
                        if (!parseUnknownField(input, unknownFields,
                                               extensionRegistry, tag)) {
                            this.setUnknownFields(unknownFields.build());
                            return this;
                        }
                        break;
                    }
                    case 18: {
                        setSubscriberId(input.readBytes());
                        break;
                    }
                    }
                }
            }


            // required bytes subscriberId = 2;
            public boolean hasSubscriberId() {
                return result.hasSubscriberId();
            }
            public com.google.protobuf.ByteString getSubscriberId() {
                return result.getSubscriberId();
            }
            public Builder setSubscriberId(com.google.protobuf.ByteString value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.hasSubscriberId = true;
                result.subscriberId_ = value;
                return this;
            }
            public Builder clearSubscriberId() {
                result.hasSubscriberId = false;
                result.subscriberId_ = getDefaultInstance().getSubscriberId();
                return this;
            }

            // @@protoc_insertion_point(builder_scope:Hedwig.UnsubscribeRequest)
        }

        static {
            defaultInstance = new UnsubscribeRequest(true);
            org.apache.hedwig.protocol.PubSubProtocol.internalForceInit();
            defaultInstance.initFields();
        }

        // @@protoc_insertion_point(class_scope:Hedwig.UnsubscribeRequest)
    }

    public static final class StopDeliveryRequest extends
        com.google.protobuf.GeneratedMessage {
        // Use StopDeliveryRequest.newBuilder() to construct.
        private StopDeliveryRequest() {
            initFields();
        }
        private StopDeliveryRequest(boolean noInit) {}

        private static final StopDeliveryRequest defaultInstance;
        public static StopDeliveryRequest getDefaultInstance() {
            return defaultInstance;
        }

        public StopDeliveryRequest getDefaultInstanceForType() {
            return defaultInstance;
        }

        public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
            return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_StopDeliveryRequest_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
            return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_StopDeliveryRequest_fieldAccessorTable;
        }

        // required bytes subscriberId = 2;
        public static final int SUBSCRIBERID_FIELD_NUMBER = 2;
        private boolean hasSubscriberId;
        private com.google.protobuf.ByteString subscriberId_ = com.google.protobuf.ByteString.EMPTY;
        public boolean hasSubscriberId() {
            return hasSubscriberId;
        }
        public com.google.protobuf.ByteString getSubscriberId() {
            return subscriberId_;
        }

        private void initFields() {
        }
        public final boolean isInitialized() {
            if (!hasSubscriberId) return false;
            return true;
        }

        public void writeTo(com.google.protobuf.CodedOutputStream output)
                throws java.io.IOException {
            getSerializedSize();
            if (hasSubscriberId()) {
                output.writeBytes(2, getSubscriberId());
            }
            getUnknownFields().writeTo(output);
        }

        private int memoizedSerializedSize = -1;
        public int getSerializedSize() {
            int size = memoizedSerializedSize;
            if (size != -1) return size;

            size = 0;
            if (hasSubscriberId()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeBytesSize(2, getSubscriberId());
            }
            size += getUnknownFields().getSerializedSize();
            memoizedSerializedSize = size;
            return size;
        }

        public static org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest parseFrom(
            com.google.protobuf.ByteString data)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest parseFrom(
            com.google.protobuf.ByteString data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest parseFrom(byte[] data)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest parseFrom(
            byte[] data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest parseFrom(java.io.InputStream input)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest parseFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest parseDelimitedFrom(java.io.InputStream input)
                throws java.io.IOException {
            Builder builder = newBuilder();
            if (builder.mergeDelimitedFrom(input)) {
                return builder.buildParsed();
            } else {
                return null;
            }
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest parseDelimitedFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            Builder builder = newBuilder();
            if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
                return builder.buildParsed();
            } else {
                return null;
            }
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest parseFrom(
            com.google.protobuf.CodedInputStream input)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest parseFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input, extensionRegistry)
                   .buildParsed();
        }

        public static Builder newBuilder() {
            return Builder.create();
        }
        public Builder newBuilderForType() {
            return newBuilder();
        }
        public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest prototype) {
            return newBuilder().mergeFrom(prototype);
        }
        public Builder toBuilder() {
            return newBuilder(this);
        }

        public static final class Builder extends
            com.google.protobuf.GeneratedMessage.Builder<Builder> {
            private org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest result;

            // Construct using org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.newBuilder()
            private Builder() {}

            private static Builder create() {
                Builder builder = new Builder();
                builder.result = new org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest();
                return builder;
            }

            protected org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest internalGetResult() {
                return result;
            }

            public Builder clear() {
                if (result == null) {
                    throw new IllegalStateException(
                        "Cannot call clear() after build().");
                }
                result = new org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest();
                return this;
            }

            public Builder clone() {
                return create().mergeFrom(result);
            }

            public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
                return org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.getDescriptor();
            }

            public org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest getDefaultInstanceForType() {
                return org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.getDefaultInstance();
            }

            public boolean isInitialized() {
                return result.isInitialized();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest build() {
                if (result != null && !isInitialized()) {
                    throw newUninitializedMessageException(result);
                }
                return buildPartial();
            }

            private org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest buildParsed()
                    throws com.google.protobuf.InvalidProtocolBufferException {
                if (!isInitialized()) {
                    throw newUninitializedMessageException(
                        result).asInvalidProtocolBufferException();
                }
                return buildPartial();
            }

            public org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest buildPartial() {
                if (result == null) {
                    throw new IllegalStateException(
                        "build() has already been called on this Builder.");
                }
                org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest returnMe = result;
                result = null;
                return returnMe;
            }

            public Builder mergeFrom(com.google.protobuf.Message other) {
                if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest) {
                    return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest)other);
                } else {
                    super.mergeFrom(other);
                    return this;
                }
            }

            public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest other) {
                if (other == org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.getDefaultInstance()) return this;
                if (other.hasSubscriberId()) {
                    setSubscriberId(other.getSubscriberId());
                }
                this.mergeUnknownFields(other.getUnknownFields());
                return this;
            }

            public Builder mergeFrom(
                com.google.protobuf.CodedInputStream input,
                com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                    throws java.io.IOException {
                com.google.protobuf.UnknownFieldSet.Builder unknownFields =
                    com.google.protobuf.UnknownFieldSet.newBuilder(
                        this.getUnknownFields());
                while (true) {
                    int tag = input.readTag();
                    switch (tag) {
                    case 0:
                        this.setUnknownFields(unknownFields.build());
                        return this;
                    default: {
                        if (!parseUnknownField(input, unknownFields,
                                               extensionRegistry, tag)) {
                            this.setUnknownFields(unknownFields.build());
                            return this;
                        }
                        break;
                    }
                    case 18: {
                        setSubscriberId(input.readBytes());
                        break;
                    }
                    }
                }
            }


            // required bytes subscriberId = 2;
            public boolean hasSubscriberId() {
                return result.hasSubscriberId();
            }
            public com.google.protobuf.ByteString getSubscriberId() {
                return result.getSubscriberId();
            }
            public Builder setSubscriberId(com.google.protobuf.ByteString value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.hasSubscriberId = true;
                result.subscriberId_ = value;
                return this;
            }
            public Builder clearSubscriberId() {
                result.hasSubscriberId = false;
                result.subscriberId_ = getDefaultInstance().getSubscriberId();
                return this;
            }

            // @@protoc_insertion_point(builder_scope:Hedwig.StopDeliveryRequest)
        }

        static {
            defaultInstance = new StopDeliveryRequest(true);
            org.apache.hedwig.protocol.PubSubProtocol.internalForceInit();
            defaultInstance.initFields();
        }

        // @@protoc_insertion_point(class_scope:Hedwig.StopDeliveryRequest)
    }

    public static final class StartDeliveryRequest extends
        com.google.protobuf.GeneratedMessage {
        // Use StartDeliveryRequest.newBuilder() to construct.
        private StartDeliveryRequest() {
            initFields();
        }
        private StartDeliveryRequest(boolean noInit) {}

        private static final StartDeliveryRequest defaultInstance;
        public static StartDeliveryRequest getDefaultInstance() {
            return defaultInstance;
        }

        public StartDeliveryRequest getDefaultInstanceForType() {
            return defaultInstance;
        }

        public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
            return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_StartDeliveryRequest_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
            return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_StartDeliveryRequest_fieldAccessorTable;
        }

        // required bytes subscriberId = 2;
        public static final int SUBSCRIBERID_FIELD_NUMBER = 2;
        private boolean hasSubscriberId;
        private com.google.protobuf.ByteString subscriberId_ = com.google.protobuf.ByteString.EMPTY;
        public boolean hasSubscriberId() {
            return hasSubscriberId;
        }
        public com.google.protobuf.ByteString getSubscriberId() {
            return subscriberId_;
        }

        private void initFields() {
        }
        public final boolean isInitialized() {
            if (!hasSubscriberId) return false;
            return true;
        }

        public void writeTo(com.google.protobuf.CodedOutputStream output)
                throws java.io.IOException {
            getSerializedSize();
            if (hasSubscriberId()) {
                output.writeBytes(2, getSubscriberId());
            }
            getUnknownFields().writeTo(output);
        }

        private int memoizedSerializedSize = -1;
        public int getSerializedSize() {
            int size = memoizedSerializedSize;
            if (size != -1) return size;

            size = 0;
            if (hasSubscriberId()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeBytesSize(2, getSubscriberId());
            }
            size += getUnknownFields().getSerializedSize();
            memoizedSerializedSize = size;
            return size;
        }

        public static org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest parseFrom(
            com.google.protobuf.ByteString data)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest parseFrom(
            com.google.protobuf.ByteString data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest parseFrom(byte[] data)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest parseFrom(
            byte[] data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest parseFrom(java.io.InputStream input)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest parseFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest parseDelimitedFrom(java.io.InputStream input)
                throws java.io.IOException {
            Builder builder = newBuilder();
            if (builder.mergeDelimitedFrom(input)) {
                return builder.buildParsed();
            } else {
                return null;
            }
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest parseDelimitedFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            Builder builder = newBuilder();
            if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
                return builder.buildParsed();
            } else {
                return null;
            }
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest parseFrom(
            com.google.protobuf.CodedInputStream input)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest parseFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input, extensionRegistry)
                   .buildParsed();
        }

        public static Builder newBuilder() {
            return Builder.create();
        }
        public Builder newBuilderForType() {
            return newBuilder();
        }
        public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest prototype) {
            return newBuilder().mergeFrom(prototype);
        }
        public Builder toBuilder() {
            return newBuilder(this);
        }

        public static final class Builder extends
            com.google.protobuf.GeneratedMessage.Builder<Builder> {
            private org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest result;

            // Construct using org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.newBuilder()
            private Builder() {}

            private static Builder create() {
                Builder builder = new Builder();
                builder.result = new org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest();
                return builder;
            }

            protected org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest internalGetResult() {
                return result;
            }

            public Builder clear() {
                if (result == null) {
                    throw new IllegalStateException(
                        "Cannot call clear() after build().");
                }
                result = new org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest();
                return this;
            }

            public Builder clone() {
                return create().mergeFrom(result);
            }

            public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
                return org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.getDescriptor();
            }

            public org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest getDefaultInstanceForType() {
                return org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.getDefaultInstance();
            }

            public boolean isInitialized() {
                return result.isInitialized();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest build() {
                if (result != null && !isInitialized()) {
                    throw newUninitializedMessageException(result);
                }
                return buildPartial();
            }

            private org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest buildParsed()
                    throws com.google.protobuf.InvalidProtocolBufferException {
                if (!isInitialized()) {
                    throw newUninitializedMessageException(
                        result).asInvalidProtocolBufferException();
                }
                return buildPartial();
            }

            public org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest buildPartial() {
                if (result == null) {
                    throw new IllegalStateException(
                        "build() has already been called on this Builder.");
                }
                org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest returnMe = result;
                result = null;
                return returnMe;
            }

            public Builder mergeFrom(com.google.protobuf.Message other) {
                if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest) {
                    return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest)other);
                } else {
                    super.mergeFrom(other);
                    return this;
                }
            }

            public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest other) {
                if (other == org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.getDefaultInstance()) return this;
                if (other.hasSubscriberId()) {
                    setSubscriberId(other.getSubscriberId());
                }
                this.mergeUnknownFields(other.getUnknownFields());
                return this;
            }

            public Builder mergeFrom(
                com.google.protobuf.CodedInputStream input,
                com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                    throws java.io.IOException {
                com.google.protobuf.UnknownFieldSet.Builder unknownFields =
                    com.google.protobuf.UnknownFieldSet.newBuilder(
                        this.getUnknownFields());
                while (true) {
                    int tag = input.readTag();
                    switch (tag) {
                    case 0:
                        this.setUnknownFields(unknownFields.build());
                        return this;
                    default: {
                        if (!parseUnknownField(input, unknownFields,
                                               extensionRegistry, tag)) {
                            this.setUnknownFields(unknownFields.build());
                            return this;
                        }
                        break;
                    }
                    case 18: {
                        setSubscriberId(input.readBytes());
                        break;
                    }
                    }
                }
            }


            // required bytes subscriberId = 2;
            public boolean hasSubscriberId() {
                return result.hasSubscriberId();
            }
            public com.google.protobuf.ByteString getSubscriberId() {
                return result.getSubscriberId();
            }
            public Builder setSubscriberId(com.google.protobuf.ByteString value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.hasSubscriberId = true;
                result.subscriberId_ = value;
                return this;
            }
            public Builder clearSubscriberId() {
                result.hasSubscriberId = false;
                result.subscriberId_ = getDefaultInstance().getSubscriberId();
                return this;
            }

            // @@protoc_insertion_point(builder_scope:Hedwig.StartDeliveryRequest)
        }

        static {
            defaultInstance = new StartDeliveryRequest(true);
            org.apache.hedwig.protocol.PubSubProtocol.internalForceInit();
            defaultInstance.initFields();
        }

        // @@protoc_insertion_point(class_scope:Hedwig.StartDeliveryRequest)
    }

    public static final class PubSubResponse extends
        com.google.protobuf.GeneratedMessage {
        // Use PubSubResponse.newBuilder() to construct.
        private PubSubResponse() {
            initFields();
        }
        private PubSubResponse(boolean noInit) {}

        private static final PubSubResponse defaultInstance;
        public static PubSubResponse getDefaultInstance() {
            return defaultInstance;
        }

        public PubSubResponse getDefaultInstanceForType() {
            return defaultInstance;
        }

        public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
            return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_PubSubResponse_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
            return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_PubSubResponse_fieldAccessorTable;
        }

        // required .Hedwig.ProtocolVersion protocolVersion = 1;
        public static final int PROTOCOLVERSION_FIELD_NUMBER = 1;
        private boolean hasProtocolVersion;
        private org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion protocolVersion_;
        public boolean hasProtocolVersion() {
            return hasProtocolVersion;
        }
        public org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion getProtocolVersion() {
            return protocolVersion_;
        }

        // required .Hedwig.StatusCode statusCode = 2;
        public static final int STATUSCODE_FIELD_NUMBER = 2;
        private boolean hasStatusCode;
        private org.apache.hedwig.protocol.PubSubProtocol.StatusCode statusCode_;
        public boolean hasStatusCode() {
            return hasStatusCode;
        }
        public org.apache.hedwig.protocol.PubSubProtocol.StatusCode getStatusCode() {
            return statusCode_;
        }

        // required uint64 txnId = 3;
        public static final int TXNID_FIELD_NUMBER = 3;
        private boolean hasTxnId;
        private long txnId_ = 0L;
        public boolean hasTxnId() {
            return hasTxnId;
        }
        public long getTxnId() {
            return txnId_;
        }

        // optional string statusMsg = 4;
        public static final int STATUSMSG_FIELD_NUMBER = 4;
        private boolean hasStatusMsg;
        private java.lang.String statusMsg_ = "";
        public boolean hasStatusMsg() {
            return hasStatusMsg;
        }
        public java.lang.String getStatusMsg() {
            return statusMsg_;
        }

        // optional .Hedwig.Message message = 5;
        public static final int MESSAGE_FIELD_NUMBER = 5;
        private boolean hasMessage;
        private org.apache.hedwig.protocol.PubSubProtocol.Message message_;
        public boolean hasMessage() {
            return hasMessage;
        }
        public org.apache.hedwig.protocol.PubSubProtocol.Message getMessage() {
            return message_;
        }

        // optional bytes topic = 6;
        public static final int TOPIC_FIELD_NUMBER = 6;
        private boolean hasTopic;
        private com.google.protobuf.ByteString topic_ = com.google.protobuf.ByteString.EMPTY;
        public boolean hasTopic() {
            return hasTopic;
        }
        public com.google.protobuf.ByteString getTopic() {
            return topic_;
        }

        // optional bytes subscriberId = 7;
        public static final int SUBSCRIBERID_FIELD_NUMBER = 7;
        private boolean hasSubscriberId;
        private com.google.protobuf.ByteString subscriberId_ = com.google.protobuf.ByteString.EMPTY;
        public boolean hasSubscriberId() {
            return hasSubscriberId;
        }
        public com.google.protobuf.ByteString getSubscriberId() {
            return subscriberId_;
        }

        private void initFields() {
            protocolVersion_ = org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion.VERSION_ONE;
            statusCode_ = org.apache.hedwig.protocol.PubSubProtocol.StatusCode.SUCCESS;
            message_ = org.apache.hedwig.protocol.PubSubProtocol.Message.getDefaultInstance();
        }
        public final boolean isInitialized() {
            if (!hasProtocolVersion) return false;
            if (!hasStatusCode) return false;
            if (!hasTxnId) return false;
            if (hasMessage()) {
                if (!getMessage().isInitialized()) return false;
            }
            return true;
        }

        public void writeTo(com.google.protobuf.CodedOutputStream output)
                throws java.io.IOException {
            getSerializedSize();
            if (hasProtocolVersion()) {
                output.writeEnum(1, getProtocolVersion().getNumber());
            }
            if (hasStatusCode()) {
                output.writeEnum(2, getStatusCode().getNumber());
            }
            if (hasTxnId()) {
                output.writeUInt64(3, getTxnId());
            }
            if (hasStatusMsg()) {
                output.writeString(4, getStatusMsg());
            }
            if (hasMessage()) {
                output.writeMessage(5, getMessage());
            }
            if (hasTopic()) {
                output.writeBytes(6, getTopic());
            }
            if (hasSubscriberId()) {
                output.writeBytes(7, getSubscriberId());
            }
            getUnknownFields().writeTo(output);
        }

        private int memoizedSerializedSize = -1;
        public int getSerializedSize() {
            int size = memoizedSerializedSize;
            if (size != -1) return size;

            size = 0;
            if (hasProtocolVersion()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeEnumSize(1, getProtocolVersion().getNumber());
            }
            if (hasStatusCode()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeEnumSize(2, getStatusCode().getNumber());
            }
            if (hasTxnId()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeUInt64Size(3, getTxnId());
            }
            if (hasStatusMsg()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeStringSize(4, getStatusMsg());
            }
            if (hasMessage()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeMessageSize(5, getMessage());
            }
            if (hasTopic()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeBytesSize(6, getTopic());
            }
            if (hasSubscriberId()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeBytesSize(7, getSubscriberId());
            }
            size += getUnknownFields().getSerializedSize();
            memoizedSerializedSize = size;
            return size;
        }

        public static org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse parseFrom(
            com.google.protobuf.ByteString data)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse parseFrom(
            com.google.protobuf.ByteString data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse parseFrom(byte[] data)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse parseFrom(
            byte[] data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse parseFrom(java.io.InputStream input)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse parseFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse parseDelimitedFrom(java.io.InputStream input)
                throws java.io.IOException {
            Builder builder = newBuilder();
            if (builder.mergeDelimitedFrom(input)) {
                return builder.buildParsed();
            } else {
                return null;
            }
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse parseDelimitedFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            Builder builder = newBuilder();
            if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
                return builder.buildParsed();
            } else {
                return null;
            }
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse parseFrom(
            com.google.protobuf.CodedInputStream input)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse parseFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input, extensionRegistry)
                   .buildParsed();
        }

        public static Builder newBuilder() {
            return Builder.create();
        }
        public Builder newBuilderForType() {
            return newBuilder();
        }
        public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse prototype) {
            return newBuilder().mergeFrom(prototype);
        }
        public Builder toBuilder() {
            return newBuilder(this);
        }

        public static final class Builder extends
            com.google.protobuf.GeneratedMessage.Builder<Builder> {
            private org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse result;

            // Construct using org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse.newBuilder()
            private Builder() {}

            private static Builder create() {
                Builder builder = new Builder();
                builder.result = new org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse();
                return builder;
            }

            protected org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse internalGetResult() {
                return result;
            }

            public Builder clear() {
                if (result == null) {
                    throw new IllegalStateException(
                        "Cannot call clear() after build().");
                }
                result = new org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse();
                return this;
            }

            public Builder clone() {
                return create().mergeFrom(result);
            }

            public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
                return org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse.getDescriptor();
            }

            public org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse getDefaultInstanceForType() {
                return org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse.getDefaultInstance();
            }

            public boolean isInitialized() {
                return result.isInitialized();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse build() {
                if (result != null && !isInitialized()) {
                    throw newUninitializedMessageException(result);
                }
                return buildPartial();
            }

            private org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse buildParsed()
                    throws com.google.protobuf.InvalidProtocolBufferException {
                if (!isInitialized()) {
                    throw newUninitializedMessageException(
                        result).asInvalidProtocolBufferException();
                }
                return buildPartial();
            }

            public org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse buildPartial() {
                if (result == null) {
                    throw new IllegalStateException(
                        "build() has already been called on this Builder.");
                }
                org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse returnMe = result;
                result = null;
                return returnMe;
            }

            public Builder mergeFrom(com.google.protobuf.Message other) {
                if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse) {
                    return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse)other);
                } else {
                    super.mergeFrom(other);
                    return this;
                }
            }

            public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse other) {
                if (other == org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse.getDefaultInstance()) return this;
                if (other.hasProtocolVersion()) {
                    setProtocolVersion(other.getProtocolVersion());
                }
                if (other.hasStatusCode()) {
                    setStatusCode(other.getStatusCode());
                }
                if (other.hasTxnId()) {
                    setTxnId(other.getTxnId());
                }
                if (other.hasStatusMsg()) {
                    setStatusMsg(other.getStatusMsg());
                }
                if (other.hasMessage()) {
                    mergeMessage(other.getMessage());
                }
                if (other.hasTopic()) {
                    setTopic(other.getTopic());
                }
                if (other.hasSubscriberId()) {
                    setSubscriberId(other.getSubscriberId());
                }
                this.mergeUnknownFields(other.getUnknownFields());
                return this;
            }

            public Builder mergeFrom(
                com.google.protobuf.CodedInputStream input,
                com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                    throws java.io.IOException {
                com.google.protobuf.UnknownFieldSet.Builder unknownFields =
                    com.google.protobuf.UnknownFieldSet.newBuilder(
                        this.getUnknownFields());
                while (true) {
                    int tag = input.readTag();
                    switch (tag) {
                    case 0:
                        this.setUnknownFields(unknownFields.build());
                        return this;
                    default: {
                        if (!parseUnknownField(input, unknownFields,
                                               extensionRegistry, tag)) {
                            this.setUnknownFields(unknownFields.build());
                            return this;
                        }
                        break;
                    }
                    case 8: {
                        int rawValue = input.readEnum();
                        org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion value = org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion.valueOf(rawValue);
                        if (value == null) {
                            unknownFields.mergeVarintField(1, rawValue);
                        } else {
                            setProtocolVersion(value);
                        }
                        break;
                    }
                    case 16: {
                        int rawValue = input.readEnum();
                        org.apache.hedwig.protocol.PubSubProtocol.StatusCode value = org.apache.hedwig.protocol.PubSubProtocol.StatusCode.valueOf(rawValue);
                        if (value == null) {
                            unknownFields.mergeVarintField(2, rawValue);
                        } else {
                            setStatusCode(value);
                        }
                        break;
                    }
                    case 24: {
                        setTxnId(input.readUInt64());
                        break;
                    }
                    case 34: {
                        setStatusMsg(input.readString());
                        break;
                    }
                    case 42: {
                        org.apache.hedwig.protocol.PubSubProtocol.Message.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.Message.newBuilder();
                        if (hasMessage()) {
                            subBuilder.mergeFrom(getMessage());
                        }
                        input.readMessage(subBuilder, extensionRegistry);
                        setMessage(subBuilder.buildPartial());
                        break;
                    }
                    case 50: {
                        setTopic(input.readBytes());
                        break;
                    }
                    case 58: {
                        setSubscriberId(input.readBytes());
                        break;
                    }
                    }
                }
            }


            // required .Hedwig.ProtocolVersion protocolVersion = 1;
            public boolean hasProtocolVersion() {
                return result.hasProtocolVersion();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion getProtocolVersion() {
                return result.getProtocolVersion();
            }
            public Builder setProtocolVersion(org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.hasProtocolVersion = true;
                result.protocolVersion_ = value;
                return this;
            }
            public Builder clearProtocolVersion() {
                result.hasProtocolVersion = false;
                result.protocolVersion_ = org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion.VERSION_ONE;
                return this;
            }

            // required .Hedwig.StatusCode statusCode = 2;
            public boolean hasStatusCode() {
                return result.hasStatusCode();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.StatusCode getStatusCode() {
                return result.getStatusCode();
            }
            public Builder setStatusCode(org.apache.hedwig.protocol.PubSubProtocol.StatusCode value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.hasStatusCode = true;
                result.statusCode_ = value;
                return this;
            }
            public Builder clearStatusCode() {
                result.hasStatusCode = false;
                result.statusCode_ = org.apache.hedwig.protocol.PubSubProtocol.StatusCode.SUCCESS;
                return this;
            }

            // required uint64 txnId = 3;
            public boolean hasTxnId() {
                return result.hasTxnId();
            }
            public long getTxnId() {
                return result.getTxnId();
            }
            public Builder setTxnId(long value) {
                result.hasTxnId = true;
                result.txnId_ = value;
                return this;
            }
            public Builder clearTxnId() {
                result.hasTxnId = false;
                result.txnId_ = 0L;
                return this;
            }

            // optional string statusMsg = 4;
            public boolean hasStatusMsg() {
                return result.hasStatusMsg();
            }
            public java.lang.String getStatusMsg() {
                return result.getStatusMsg();
            }
            public Builder setStatusMsg(java.lang.String value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.hasStatusMsg = true;
                result.statusMsg_ = value;
                return this;
            }
            public Builder clearStatusMsg() {
                result.hasStatusMsg = false;
                result.statusMsg_ = getDefaultInstance().getStatusMsg();
                return this;
            }

            // optional .Hedwig.Message message = 5;
            public boolean hasMessage() {
                return result.hasMessage();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.Message getMessage() {
                return result.getMessage();
            }
            public Builder setMessage(org.apache.hedwig.protocol.PubSubProtocol.Message value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.hasMessage = true;
                result.message_ = value;
                return this;
            }
            public Builder setMessage(org.apache.hedwig.protocol.PubSubProtocol.Message.Builder builderForValue) {
                result.hasMessage = true;
                result.message_ = builderForValue.build();
                return this;
            }
            public Builder mergeMessage(org.apache.hedwig.protocol.PubSubProtocol.Message value) {
                if (result.hasMessage() &&
                        result.message_ != org.apache.hedwig.protocol.PubSubProtocol.Message.getDefaultInstance()) {
                    result.message_ =
                        org.apache.hedwig.protocol.PubSubProtocol.Message.newBuilder(result.message_).mergeFrom(value).buildPartial();
                } else {
                    result.message_ = value;
                }
                result.hasMessage = true;
                return this;
            }
            public Builder clearMessage() {
                result.hasMessage = false;
                result.message_ = org.apache.hedwig.protocol.PubSubProtocol.Message.getDefaultInstance();
                return this;
            }

            // optional bytes topic = 6;
            public boolean hasTopic() {
                return result.hasTopic();
            }
            public com.google.protobuf.ByteString getTopic() {
                return result.getTopic();
            }
            public Builder setTopic(com.google.protobuf.ByteString value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.hasTopic = true;
                result.topic_ = value;
                return this;
            }
            public Builder clearTopic() {
                result.hasTopic = false;
                result.topic_ = getDefaultInstance().getTopic();
                return this;
            }

            // optional bytes subscriberId = 7;
            public boolean hasSubscriberId() {
                return result.hasSubscriberId();
            }
            public com.google.protobuf.ByteString getSubscriberId() {
                return result.getSubscriberId();
            }
            public Builder setSubscriberId(com.google.protobuf.ByteString value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.hasSubscriberId = true;
                result.subscriberId_ = value;
                return this;
            }
            public Builder clearSubscriberId() {
                result.hasSubscriberId = false;
                result.subscriberId_ = getDefaultInstance().getSubscriberId();
                return this;
            }

            // @@protoc_insertion_point(builder_scope:Hedwig.PubSubResponse)
        }

        static {
            defaultInstance = new PubSubResponse(true);
            org.apache.hedwig.protocol.PubSubProtocol.internalForceInit();
            defaultInstance.initFields();
        }

        // @@protoc_insertion_point(class_scope:Hedwig.PubSubResponse)
    }

    public static final class SubscriptionState extends
        com.google.protobuf.GeneratedMessage {
        // Use SubscriptionState.newBuilder() to construct.
        private SubscriptionState() {
            initFields();
        }
        private SubscriptionState(boolean noInit) {}

        private static final SubscriptionState defaultInstance;
        public static SubscriptionState getDefaultInstance() {
            return defaultInstance;
        }

        public SubscriptionState getDefaultInstanceForType() {
            return defaultInstance;
        }

        public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
            return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscriptionState_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
            return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscriptionState_fieldAccessorTable;
        }

        // required .Hedwig.MessageSeqId msgId = 1;
        public static final int MSGID_FIELD_NUMBER = 1;
        private boolean hasMsgId;
        private org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId msgId_;
        public boolean hasMsgId() {
            return hasMsgId;
        }
        public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId getMsgId() {
            return msgId_;
        }

        private void initFields() {
            msgId_ = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance();
        }
        public final boolean isInitialized() {
            if (!hasMsgId) return false;
            if (!getMsgId().isInitialized()) return false;
            return true;
        }

        public void writeTo(com.google.protobuf.CodedOutputStream output)
                throws java.io.IOException {
            getSerializedSize();
            if (hasMsgId()) {
                output.writeMessage(1, getMsgId());
            }
            getUnknownFields().writeTo(output);
        }

        private int memoizedSerializedSize = -1;
        public int getSerializedSize() {
            int size = memoizedSerializedSize;
            if (size != -1) return size;

            size = 0;
            if (hasMsgId()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeMessageSize(1, getMsgId());
            }
            size += getUnknownFields().getSerializedSize();
            memoizedSerializedSize = size;
            return size;
        }

        public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState parseFrom(
            com.google.protobuf.ByteString data)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState parseFrom(
            com.google.protobuf.ByteString data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState parseFrom(byte[] data)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState parseFrom(
            byte[] data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState parseFrom(java.io.InputStream input)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState parseFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState parseDelimitedFrom(java.io.InputStream input)
                throws java.io.IOException {
            Builder builder = newBuilder();
            if (builder.mergeDelimitedFrom(input)) {
                return builder.buildParsed();
            } else {
                return null;
            }
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState parseDelimitedFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            Builder builder = newBuilder();
            if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
                return builder.buildParsed();
            } else {
                return null;
            }
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState parseFrom(
            com.google.protobuf.CodedInputStream input)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState parseFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input, extensionRegistry)
                   .buildParsed();
        }

        public static Builder newBuilder() {
            return Builder.create();
        }
        public Builder newBuilderForType() {
            return newBuilder();
        }
        public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState prototype) {
            return newBuilder().mergeFrom(prototype);
        }
        public Builder toBuilder() {
            return newBuilder(this);
        }

        public static final class Builder extends
            com.google.protobuf.GeneratedMessage.Builder<Builder> {
            private org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState result;

            // Construct using org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState.newBuilder()
            private Builder() {}

            private static Builder create() {
                Builder builder = new Builder();
                builder.result = new org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState();
                return builder;
            }

            protected org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState internalGetResult() {
                return result;
            }

            public Builder clear() {
                if (result == null) {
                    throw new IllegalStateException(
                        "Cannot call clear() after build().");
                }
                result = new org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState();
                return this;
            }

            public Builder clone() {
                return create().mergeFrom(result);
            }

            public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
                return org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState.getDescriptor();
            }

            public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState getDefaultInstanceForType() {
                return org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState.getDefaultInstance();
            }

            public boolean isInitialized() {
                return result.isInitialized();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState build() {
                if (result != null && !isInitialized()) {
                    throw newUninitializedMessageException(result);
                }
                return buildPartial();
            }

            private org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState buildParsed()
                    throws com.google.protobuf.InvalidProtocolBufferException {
                if (!isInitialized()) {
                    throw newUninitializedMessageException(
                        result).asInvalidProtocolBufferException();
                }
                return buildPartial();
            }

            public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState buildPartial() {
                if (result == null) {
                    throw new IllegalStateException(
                        "build() has already been called on this Builder.");
                }
                org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState returnMe = result;
                result = null;
                return returnMe;
            }

            public Builder mergeFrom(com.google.protobuf.Message other) {
                if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState) {
                    return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState)other);
                } else {
                    super.mergeFrom(other);
                    return this;
                }
            }

            public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState other) {
                if (other == org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState.getDefaultInstance()) return this;
                if (other.hasMsgId()) {
                    mergeMsgId(other.getMsgId());
                }
                this.mergeUnknownFields(other.getUnknownFields());
                return this;
            }

            public Builder mergeFrom(
                com.google.protobuf.CodedInputStream input,
                com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                    throws java.io.IOException {
                com.google.protobuf.UnknownFieldSet.Builder unknownFields =
                    com.google.protobuf.UnknownFieldSet.newBuilder(
                        this.getUnknownFields());
                while (true) {
                    int tag = input.readTag();
                    switch (tag) {
                    case 0:
                        this.setUnknownFields(unknownFields.build());
                        return this;
                    default: {
                        if (!parseUnknownField(input, unknownFields,
                                               extensionRegistry, tag)) {
                            this.setUnknownFields(unknownFields.build());
                            return this;
                        }
                        break;
                    }
                    case 10: {
                        org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.newBuilder();
                        if (hasMsgId()) {
                            subBuilder.mergeFrom(getMsgId());
                        }
                        input.readMessage(subBuilder, extensionRegistry);
                        setMsgId(subBuilder.buildPartial());
                        break;
                    }
                    }
                }
            }


            // required .Hedwig.MessageSeqId msgId = 1;
            public boolean hasMsgId() {
                return result.hasMsgId();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId getMsgId() {
                return result.getMsgId();
            }
            public Builder setMsgId(org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.hasMsgId = true;
                result.msgId_ = value;
                return this;
            }
            public Builder setMsgId(org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder builderForValue) {
                result.hasMsgId = true;
                result.msgId_ = builderForValue.build();
                return this;
            }
            public Builder mergeMsgId(org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId value) {
                if (result.hasMsgId() &&
                        result.msgId_ != org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance()) {
                    result.msgId_ =
                        org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.newBuilder(result.msgId_).mergeFrom(value).buildPartial();
                } else {
                    result.msgId_ = value;
                }
                result.hasMsgId = true;
                return this;
            }
            public Builder clearMsgId() {
                result.hasMsgId = false;
                result.msgId_ = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance();
                return this;
            }

            // @@protoc_insertion_point(builder_scope:Hedwig.SubscriptionState)
        }

        static {
            defaultInstance = new SubscriptionState(true);
            org.apache.hedwig.protocol.PubSubProtocol.internalForceInit();
            defaultInstance.initFields();
        }

        // @@protoc_insertion_point(class_scope:Hedwig.SubscriptionState)
    }

    public static final class LedgerRange extends
        com.google.protobuf.GeneratedMessage {
        // Use LedgerRange.newBuilder() to construct.
        private LedgerRange() {
            initFields();
        }
        private LedgerRange(boolean noInit) {}

        private static final LedgerRange defaultInstance;
        public static LedgerRange getDefaultInstance() {
            return defaultInstance;
        }

        public LedgerRange getDefaultInstanceForType() {
            return defaultInstance;
        }

        public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
            return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_LedgerRange_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
            return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_LedgerRange_fieldAccessorTable;
        }

        // required uint64 ledgerId = 1;
        public static final int LEDGERID_FIELD_NUMBER = 1;
        private boolean hasLedgerId;
        private long ledgerId_ = 0L;
        public boolean hasLedgerId() {
            return hasLedgerId;
        }
        public long getLedgerId() {
            return ledgerId_;
        }

        // optional .Hedwig.MessageSeqId endSeqIdIncluded = 2;
        public static final int ENDSEQIDINCLUDED_FIELD_NUMBER = 2;
        private boolean hasEndSeqIdIncluded;
        private org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId endSeqIdIncluded_;
        public boolean hasEndSeqIdIncluded() {
            return hasEndSeqIdIncluded;
        }
        public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId getEndSeqIdIncluded() {
            return endSeqIdIncluded_;
        }

        private void initFields() {
            endSeqIdIncluded_ = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance();
        }
        public final boolean isInitialized() {
            if (!hasLedgerId) return false;
            if (hasEndSeqIdIncluded()) {
                if (!getEndSeqIdIncluded().isInitialized()) return false;
            }
            return true;
        }

        public void writeTo(com.google.protobuf.CodedOutputStream output)
                throws java.io.IOException {
            getSerializedSize();
            if (hasLedgerId()) {
                output.writeUInt64(1, getLedgerId());
            }
            if (hasEndSeqIdIncluded()) {
                output.writeMessage(2, getEndSeqIdIncluded());
            }
            getUnknownFields().writeTo(output);
        }

        private int memoizedSerializedSize = -1;
        public int getSerializedSize() {
            int size = memoizedSerializedSize;
            if (size != -1) return size;

            size = 0;
            if (hasLedgerId()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeUInt64Size(1, getLedgerId());
            }
            if (hasEndSeqIdIncluded()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeMessageSize(2, getEndSeqIdIncluded());
            }
            size += getUnknownFields().getSerializedSize();
            memoizedSerializedSize = size;
            return size;
        }

        public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRange parseFrom(
            com.google.protobuf.ByteString data)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRange parseFrom(
            com.google.protobuf.ByteString data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRange parseFrom(byte[] data)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRange parseFrom(
            byte[] data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRange parseFrom(java.io.InputStream input)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRange parseFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRange parseDelimitedFrom(java.io.InputStream input)
                throws java.io.IOException {
            Builder builder = newBuilder();
            if (builder.mergeDelimitedFrom(input)) {
                return builder.buildParsed();
            } else {
                return null;
            }
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRange parseDelimitedFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            Builder builder = newBuilder();
            if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
                return builder.buildParsed();
            } else {
                return null;
            }
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRange parseFrom(
            com.google.protobuf.CodedInputStream input)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRange parseFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input, extensionRegistry)
                   .buildParsed();
        }

        public static Builder newBuilder() {
            return Builder.create();
        }
        public Builder newBuilderForType() {
            return newBuilder();
        }
        public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.LedgerRange prototype) {
            return newBuilder().mergeFrom(prototype);
        }
        public Builder toBuilder() {
            return newBuilder(this);
        }

        public static final class Builder extends
            com.google.protobuf.GeneratedMessage.Builder<Builder> {
            private org.apache.hedwig.protocol.PubSubProtocol.LedgerRange result;

            // Construct using org.apache.hedwig.protocol.PubSubProtocol.LedgerRange.newBuilder()
            private Builder() {}

            private static Builder create() {
                Builder builder = new Builder();
                builder.result = new org.apache.hedwig.protocol.PubSubProtocol.LedgerRange();
                return builder;
            }

            protected org.apache.hedwig.protocol.PubSubProtocol.LedgerRange internalGetResult() {
                return result;
            }

            public Builder clear() {
                if (result == null) {
                    throw new IllegalStateException(
                        "Cannot call clear() after build().");
                }
                result = new org.apache.hedwig.protocol.PubSubProtocol.LedgerRange();
                return this;
            }

            public Builder clone() {
                return create().mergeFrom(result);
            }

            public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
                return org.apache.hedwig.protocol.PubSubProtocol.LedgerRange.getDescriptor();
            }

            public org.apache.hedwig.protocol.PubSubProtocol.LedgerRange getDefaultInstanceForType() {
                return org.apache.hedwig.protocol.PubSubProtocol.LedgerRange.getDefaultInstance();
            }

            public boolean isInitialized() {
                return result.isInitialized();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.LedgerRange build() {
                if (result != null && !isInitialized()) {
                    throw newUninitializedMessageException(result);
                }
                return buildPartial();
            }

            private org.apache.hedwig.protocol.PubSubProtocol.LedgerRange buildParsed()
                    throws com.google.protobuf.InvalidProtocolBufferException {
                if (!isInitialized()) {
                    throw newUninitializedMessageException(
                        result).asInvalidProtocolBufferException();
                }
                return buildPartial();
            }

            public org.apache.hedwig.protocol.PubSubProtocol.LedgerRange buildPartial() {
                if (result == null) {
                    throw new IllegalStateException(
                        "build() has already been called on this Builder.");
                }
                org.apache.hedwig.protocol.PubSubProtocol.LedgerRange returnMe = result;
                result = null;
                return returnMe;
            }

            public Builder mergeFrom(com.google.protobuf.Message other) {
                if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.LedgerRange) {
                    return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.LedgerRange)other);
                } else {
                    super.mergeFrom(other);
                    return this;
                }
            }

            public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.LedgerRange other) {
                if (other == org.apache.hedwig.protocol.PubSubProtocol.LedgerRange.getDefaultInstance()) return this;
                if (other.hasLedgerId()) {
                    setLedgerId(other.getLedgerId());
                }
                if (other.hasEndSeqIdIncluded()) {
                    mergeEndSeqIdIncluded(other.getEndSeqIdIncluded());
                }
                this.mergeUnknownFields(other.getUnknownFields());
                return this;
            }

            public Builder mergeFrom(
                com.google.protobuf.CodedInputStream input,
                com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                    throws java.io.IOException {
                com.google.protobuf.UnknownFieldSet.Builder unknownFields =
                    com.google.protobuf.UnknownFieldSet.newBuilder(
                        this.getUnknownFields());
                while (true) {
                    int tag = input.readTag();
                    switch (tag) {
                    case 0:
                        this.setUnknownFields(unknownFields.build());
                        return this;
                    default: {
                        if (!parseUnknownField(input, unknownFields,
                                               extensionRegistry, tag)) {
                            this.setUnknownFields(unknownFields.build());
                            return this;
                        }
                        break;
                    }
                    case 8: {
                        setLedgerId(input.readUInt64());
                        break;
                    }
                    case 18: {
                        org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.newBuilder();
                        if (hasEndSeqIdIncluded()) {
                            subBuilder.mergeFrom(getEndSeqIdIncluded());
                        }
                        input.readMessage(subBuilder, extensionRegistry);
                        setEndSeqIdIncluded(subBuilder.buildPartial());
                        break;
                    }
                    }
                }
            }


            // required uint64 ledgerId = 1;
            public boolean hasLedgerId() {
                return result.hasLedgerId();
            }
            public long getLedgerId() {
                return result.getLedgerId();
            }
            public Builder setLedgerId(long value) {
                result.hasLedgerId = true;
                result.ledgerId_ = value;
                return this;
            }
            public Builder clearLedgerId() {
                result.hasLedgerId = false;
                result.ledgerId_ = 0L;
                return this;
            }

            // optional .Hedwig.MessageSeqId endSeqIdIncluded = 2;
            public boolean hasEndSeqIdIncluded() {
                return result.hasEndSeqIdIncluded();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId getEndSeqIdIncluded() {
                return result.getEndSeqIdIncluded();
            }
            public Builder setEndSeqIdIncluded(org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.hasEndSeqIdIncluded = true;
                result.endSeqIdIncluded_ = value;
                return this;
            }
            public Builder setEndSeqIdIncluded(org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder builderForValue) {
                result.hasEndSeqIdIncluded = true;
                result.endSeqIdIncluded_ = builderForValue.build();
                return this;
            }
            public Builder mergeEndSeqIdIncluded(org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId value) {
                if (result.hasEndSeqIdIncluded() &&
                        result.endSeqIdIncluded_ != org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance()) {
                    result.endSeqIdIncluded_ =
                        org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.newBuilder(result.endSeqIdIncluded_).mergeFrom(value).buildPartial();
                } else {
                    result.endSeqIdIncluded_ = value;
                }
                result.hasEndSeqIdIncluded = true;
                return this;
            }
            public Builder clearEndSeqIdIncluded() {
                result.hasEndSeqIdIncluded = false;
                result.endSeqIdIncluded_ = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance();
                return this;
            }

            // @@protoc_insertion_point(builder_scope:Hedwig.LedgerRange)
        }

        static {
            defaultInstance = new LedgerRange(true);
            org.apache.hedwig.protocol.PubSubProtocol.internalForceInit();
            defaultInstance.initFields();
        }

        // @@protoc_insertion_point(class_scope:Hedwig.LedgerRange)
    }

    public static final class LedgerRanges extends
        com.google.protobuf.GeneratedMessage {
        // Use LedgerRanges.newBuilder() to construct.
        private LedgerRanges() {
            initFields();
        }
        private LedgerRanges(boolean noInit) {}

        private static final LedgerRanges defaultInstance;
        public static LedgerRanges getDefaultInstance() {
            return defaultInstance;
        }

        public LedgerRanges getDefaultInstanceForType() {
            return defaultInstance;
        }

        public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
            return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_LedgerRanges_descriptor;
        }

        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
            return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_LedgerRanges_fieldAccessorTable;
        }

        // repeated .Hedwig.LedgerRange ranges = 1;
        public static final int RANGES_FIELD_NUMBER = 1;
        private java.util.List<org.apache.hedwig.protocol.PubSubProtocol.LedgerRange> ranges_ =
            java.util.Collections.emptyList();
        public java.util.List<org.apache.hedwig.protocol.PubSubProtocol.LedgerRange> getRangesList() {
            return ranges_;
        }
        public int getRangesCount() {
            return ranges_.size();
        }
        public org.apache.hedwig.protocol.PubSubProtocol.LedgerRange getRanges(int index) {
            return ranges_.get(index);
        }

        private void initFields() {
        }
        public final boolean isInitialized() {
            for (org.apache.hedwig.protocol.PubSubProtocol.LedgerRange element : getRangesList()) {
                if (!element.isInitialized()) return false;
            }
            return true;
        }

        public void writeTo(com.google.protobuf.CodedOutputStream output)
                throws java.io.IOException {
            getSerializedSize();
            for (org.apache.hedwig.protocol.PubSubProtocol.LedgerRange element : getRangesList()) {
                output.writeMessage(1, element);
            }
            getUnknownFields().writeTo(output);
        }

        private int memoizedSerializedSize = -1;
        public int getSerializedSize() {
            int size = memoizedSerializedSize;
            if (size != -1) return size;

            size = 0;
            for (org.apache.hedwig.protocol.PubSubProtocol.LedgerRange element : getRangesList()) {
                size += com.google.protobuf.CodedOutputStream
                        .computeMessageSize(1, element);
            }
            size += getUnknownFields().getSerializedSize();
            memoizedSerializedSize = size;
            return size;
        }

        public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges parseFrom(
            com.google.protobuf.ByteString data)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges parseFrom(
            com.google.protobuf.ByteString data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges parseFrom(byte[] data)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges parseFrom(
            byte[] data,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws com.google.protobuf.InvalidProtocolBufferException {
            return newBuilder().mergeFrom(data, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges parseFrom(java.io.InputStream input)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges parseFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input, extensionRegistry)
                   .buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges parseDelimitedFrom(java.io.InputStream input)
                throws java.io.IOException {
            Builder builder = newBuilder();
            if (builder.mergeDelimitedFrom(input)) {
                return builder.buildParsed();
            } else {
                return null;
            }
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges parseDelimitedFrom(
            java.io.InputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            Builder builder = newBuilder();
            if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
                return builder.buildParsed();
            } else {
                return null;
            }
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges parseFrom(
            com.google.protobuf.CodedInputStream input)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input).buildParsed();
        }
        public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges parseFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                throws java.io.IOException {
            return newBuilder().mergeFrom(input, extensionRegistry)
                   .buildParsed();
        }

        public static Builder newBuilder() {
            return Builder.create();
        }
        public Builder newBuilderForType() {
            return newBuilder();
        }
        public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges prototype) {
            return newBuilder().mergeFrom(prototype);
        }
        public Builder toBuilder() {
            return newBuilder(this);
        }

        public static final class Builder extends
            com.google.protobuf.GeneratedMessage.Builder<Builder> {
            private org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges result;

            // Construct using org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges.newBuilder()
            private Builder() {}

            private static Builder create() {
                Builder builder = new Builder();
                builder.result = new org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges();
                return builder;
            }

            protected org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges internalGetResult() {
                return result;
            }

            public Builder clear() {
                if (result == null) {
                    throw new IllegalStateException(
                        "Cannot call clear() after build().");
                }
                result = new org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges();
                return this;
            }

            public Builder clone() {
                return create().mergeFrom(result);
            }

            public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
                return org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges.getDescriptor();
            }

            public org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges getDefaultInstanceForType() {
                return org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges.getDefaultInstance();
            }

            public boolean isInitialized() {
                return result.isInitialized();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges build() {
                if (result != null && !isInitialized()) {
                    throw newUninitializedMessageException(result);
                }
                return buildPartial();
            }

            private org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges buildParsed()
                    throws com.google.protobuf.InvalidProtocolBufferException {
                if (!isInitialized()) {
                    throw newUninitializedMessageException(
                        result).asInvalidProtocolBufferException();
                }
                return buildPartial();
            }

            public org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges buildPartial() {
                if (result == null) {
                    throw new IllegalStateException(
                        "build() has already been called on this Builder.");
                }
                if (result.ranges_ != java.util.Collections.EMPTY_LIST) {
                    result.ranges_ =
                        java.util.Collections.unmodifiableList(result.ranges_);
                }
                org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges returnMe = result;
                result = null;
                return returnMe;
            }

            public Builder mergeFrom(com.google.protobuf.Message other) {
                if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges) {
                    return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges)other);
                } else {
                    super.mergeFrom(other);
                    return this;
                }
            }

            public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges other) {
                if (other == org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges.getDefaultInstance()) return this;
                if (!other.ranges_.isEmpty()) {
                    if (result.ranges_.isEmpty()) {
                        result.ranges_ = new java.util.ArrayList<org.apache.hedwig.protocol.PubSubProtocol.LedgerRange>();
                    }
                    result.ranges_.addAll(other.ranges_);
                }
                this.mergeUnknownFields(other.getUnknownFields());
                return this;
            }

            public Builder mergeFrom(
                com.google.protobuf.CodedInputStream input,
                com.google.protobuf.ExtensionRegistryLite extensionRegistry)
                    throws java.io.IOException {
                com.google.protobuf.UnknownFieldSet.Builder unknownFields =
                    com.google.protobuf.UnknownFieldSet.newBuilder(
                        this.getUnknownFields());
                while (true) {
                    int tag = input.readTag();
                    switch (tag) {
                    case 0:
                        this.setUnknownFields(unknownFields.build());
                        return this;
                    default: {
                        if (!parseUnknownField(input, unknownFields,
                                               extensionRegistry, tag)) {
                            this.setUnknownFields(unknownFields.build());
                            return this;
                        }
                        break;
                    }
                    case 10: {
                        org.apache.hedwig.protocol.PubSubProtocol.LedgerRange.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.LedgerRange.newBuilder();
                        input.readMessage(subBuilder, extensionRegistry);
                        addRanges(subBuilder.buildPartial());
                        break;
                    }
                    }
                }
            }


            // repeated .Hedwig.LedgerRange ranges = 1;
            public java.util.List<org.apache.hedwig.protocol.PubSubProtocol.LedgerRange> getRangesList() {
                return java.util.Collections.unmodifiableList(result.ranges_);
            }
            public int getRangesCount() {
                return result.getRangesCount();
            }
            public org.apache.hedwig.protocol.PubSubProtocol.LedgerRange getRanges(int index) {
                return result.getRanges(index);
            }
            public Builder setRanges(int index, org.apache.hedwig.protocol.PubSubProtocol.LedgerRange value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                result.ranges_.set(index, value);
                return this;
            }
            public Builder setRanges(int index, org.apache.hedwig.protocol.PubSubProtocol.LedgerRange.Builder builderForValue) {
                result.ranges_.set(index, builderForValue.build());
                return this;
            }
            public Builder addRanges(org.apache.hedwig.protocol.PubSubProtocol.LedgerRange value) {
                if (value == null) {
                    throw new NullPointerException();
                }
                if (result.ranges_.isEmpty()) {
                    result.ranges_ = new java.util.ArrayList<org.apache.hedwig.protocol.PubSubProtocol.LedgerRange>();
                }
                result.ranges_.add(value);
                return this;
            }
            public Builder addRanges(org.apache.hedwig.protocol.PubSubProtocol.LedgerRange.Builder builderForValue) {
                if (result.ranges_.isEmpty()) {
                    result.ranges_ = new java.util.ArrayList<org.apache.hedwig.protocol.PubSubProtocol.LedgerRange>();
                }
                result.ranges_.add(builderForValue.build());
                return this;
            }
            public Builder addAllRanges(
                java.lang.Iterable<? extends org.apache.hedwig.protocol.PubSubProtocol.LedgerRange> values) {
                if (result.ranges_.isEmpty()) {
                    result.ranges_ = new java.util.ArrayList<org.apache.hedwig.protocol.PubSubProtocol.LedgerRange>();
                }
                super.addAll(values, result.ranges_);
                return this;
            }
            public Builder clearRanges() {
                result.ranges_ = java.util.Collections.emptyList();
                return this;
            }

            // @@protoc_insertion_point(builder_scope:Hedwig.LedgerRanges)
        }

        static {
            defaultInstance = new LedgerRanges(true);
            org.apache.hedwig.protocol.PubSubProtocol.internalForceInit();
            defaultInstance.initFields();
        }

        // @@protoc_insertion_point(class_scope:Hedwig.LedgerRanges)
    }

    private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_Message_descriptor;
    private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
    internal_static_Hedwig_Message_fieldAccessorTable;
    private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_RegionSpecificSeqId_descriptor;
    private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
    internal_static_Hedwig_RegionSpecificSeqId_fieldAccessorTable;
    private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_MessageSeqId_descriptor;
    private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
    internal_static_Hedwig_MessageSeqId_fieldAccessorTable;
    private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_PubSubRequest_descriptor;
    private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
    internal_static_Hedwig_PubSubRequest_fieldAccessorTable;
    private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_PublishRequest_descriptor;
    private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
    internal_static_Hedwig_PublishRequest_fieldAccessorTable;
    private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_SubscribeRequest_descriptor;
    private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
    internal_static_Hedwig_SubscribeRequest_fieldAccessorTable;
    private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_ConsumeRequest_descriptor;
    private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
    internal_static_Hedwig_ConsumeRequest_fieldAccessorTable;
    private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_UnsubscribeRequest_descriptor;
    private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
    internal_static_Hedwig_UnsubscribeRequest_fieldAccessorTable;
    private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_StopDeliveryRequest_descriptor;
    private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
    internal_static_Hedwig_StopDeliveryRequest_fieldAccessorTable;
    private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_StartDeliveryRequest_descriptor;
    private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
    internal_static_Hedwig_StartDeliveryRequest_fieldAccessorTable;
    private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_PubSubResponse_descriptor;
    private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
    internal_static_Hedwig_PubSubResponse_fieldAccessorTable;
    private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_SubscriptionState_descriptor;
    private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
    internal_static_Hedwig_SubscriptionState_fieldAccessorTable;
    private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_LedgerRange_descriptor;
    private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
    internal_static_Hedwig_LedgerRange_fieldAccessorTable;
    private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_LedgerRanges_descriptor;
    private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
    internal_static_Hedwig_LedgerRanges_fieldAccessorTable;

    public static com.google.protobuf.Descriptors.FileDescriptor
    getDescriptor() {
        return descriptor;
    }
    private static com.google.protobuf.Descriptors.FileDescriptor
    descriptor;
    static {
        java.lang.String[] descriptorData = {
            "\n&src/main/protobuf/PubSubProtocol.proto" +
            "\022\006Hedwig\"O\n\007Message\022\014\n\004body\030\001 \002(\014\022\021\n\tsrc" +
            "Region\030\002 \001(\014\022#\n\005msgId\030\003 \001(\0132\024.Hedwig.Mes" +
            "sageSeqId\"4\n\023RegionSpecificSeqId\022\016\n\006regi" +
            "on\030\001 \002(\014\022\r\n\005seqId\030\002 \002(\004\"]\n\014MessageSeqId\022" +
            "\026\n\016localComponent\030\001 \001(\004\0225\n\020remoteCompone" +
            "nts\030\002 \003(\0132\033.Hedwig.RegionSpecificSeqId\"\361" +
            "\003\n\rPubSubRequest\0220\n\017protocolVersion\030\001 \002(" +
            "\0162\027.Hedwig.ProtocolVersion\022#\n\004type\030\002 \002(\016" +
            "2\025.Hedwig.OperationType\022\024\n\014triedServers\030",
            "\003 \003(\014\022\r\n\005txnId\030\004 \002(\004\022\023\n\013shouldClaim\030\005 \001(" +
            "\010\022\r\n\005topic\030\006 \002(\014\022.\n\016publishRequest\0304 \001(\013" +
            "2\026.Hedwig.PublishRequest\0222\n\020subscribeReq" +
            "uest\0305 \001(\0132\030.Hedwig.SubscribeRequest\022.\n\016" +
            "consumeRequest\0306 \001(\0132\026.Hedwig.ConsumeReq" +
            "uest\0226\n\022unsubscribeRequest\0307 \001(\0132\032.Hedwi" +
            "g.UnsubscribeRequest\0228\n\023stopDeliveryRequ" +
            "est\0308 \001(\0132\033.Hedwig.StopDeliveryRequest\022:" +
            "\n\024startDeliveryRequest\0309 \001(\0132\034.Hedwig.St" +
            "artDeliveryRequest\".\n\016PublishRequest\022\034\n\003",
            "msg\030\002 \002(\0132\017.Hedwig.Message\"\327\001\n\020Subscribe" +
            "Request\022\024\n\014subscriberId\030\002 \002(\014\022Q\n\016createO" +
            "rAttach\030\003 \001(\0162\'.Hedwig.SubscribeRequest." +
            "CreateOrAttach:\020CREATE_OR_ATTACH\022\032\n\013sync" +
            "hronous\030\004 \001(\010:\005false\">\n\016CreateOrAttach\022\n" +
            "\n\006CREATE\020\000\022\n\n\006ATTACH\020\001\022\024\n\020CREATE_OR_ATTA" +
            "CH\020\002\"K\n\016ConsumeRequest\022\024\n\014subscriberId\030\002" +
            " \002(\014\022#\n\005msgId\030\003 \002(\0132\024.Hedwig.MessageSeqI" +
            "d\"*\n\022UnsubscribeRequest\022\024\n\014subscriberId\030" +
            "\002 \002(\014\"+\n\023StopDeliveryRequest\022\024\n\014subscrib",
            "erId\030\002 \002(\014\",\n\024StartDeliveryRequest\022\024\n\014su" +
            "bscriberId\030\002 \002(\014\"\323\001\n\016PubSubResponse\0220\n\017p" +
            "rotocolVersion\030\001 \002(\0162\027.Hedwig.ProtocolVe" +
            "rsion\022&\n\nstatusCode\030\002 \002(\0162\022.Hedwig.Statu" +
            "sCode\022\r\n\005txnId\030\003 \002(\004\022\021\n\tstatusMsg\030\004 \001(\t\022" +
            " \n\007message\030\005 \001(\0132\017.Hedwig.Message\022\r\n\005top" +
            "ic\030\006 \001(\014\022\024\n\014subscriberId\030\007 \001(\014\"8\n\021Subscr" +
            "iptionState\022#\n\005msgId\030\001 \002(\0132\024.Hedwig.Mess" +
            "ageSeqId\"O\n\013LedgerRange\022\020\n\010ledgerId\030\001 \002(" +
            "\004\022.\n\020endSeqIdIncluded\030\002 \001(\0132\024.Hedwig.Mes",
            "sageSeqId\"3\n\014LedgerRanges\022#\n\006ranges\030\001 \003(" +
            "\0132\023.Hedwig.LedgerRange*\"\n\017ProtocolVersio" +
            "n\022\017\n\013VERSION_ONE\020\001*p\n\rOperationType\022\013\n\007P" +
            "UBLISH\020\000\022\r\n\tSUBSCRIBE\020\001\022\013\n\007CONSUME\020\002\022\017\n\013" +
            "UNSUBSCRIBE\020\003\022\022\n\016START_DELIVERY\020\004\022\021\n\rSTO" +
            "P_DELIVERY\020\005*\236\002\n\nStatusCode\022\013\n\007SUCCESS\020\000" +
            "\022\026\n\021MALFORMED_REQUEST\020\221\003\022\022\n\rNO_SUCH_TOPI" +
            "C\020\222\003\022\036\n\031CLIENT_ALREADY_SUBSCRIBED\020\223\003\022\032\n\025" +
            "CLIENT_NOT_SUBSCRIBED\020\224\003\022\026\n\021COULD_NOT_CO" +
            "NNECT\020\225\003\022\017\n\nTOPIC_BUSY\020\226\003\022\036\n\031NOT_RESPONS",
            "IBLE_FOR_TOPIC\020\365\003\022\021\n\014SERVICE_DOWN\020\366\003\022\024\n\017" +
            "UNCERTAIN_STATE\020\367\003\022\031\n\024UNEXPECTED_CONDITI" +
            "ON\020\330\004\022\016\n\tCOMPOSITE\020\274\005B\036\n\032org.apache.hedw" +
            "ig.protocolH\001"
        };
        com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
        new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {
            public com.google.protobuf.ExtensionRegistry assignDescriptors(
            com.google.protobuf.Descriptors.FileDescriptor root) {
                descriptor = root;
                internal_static_Hedwig_Message_descriptor =
                    getDescriptor().getMessageTypes().get(0);
                internal_static_Hedwig_Message_fieldAccessorTable = new
                com.google.protobuf.GeneratedMessage.FieldAccessorTable(
                    internal_static_Hedwig_Message_descriptor,
                    new java.lang.String[] { "Body", "SrcRegion", "MsgId", },
                    org.apache.hedwig.protocol.PubSubProtocol.Message.class,
                    org.apache.hedwig.protocol.PubSubProtocol.Message.Builder.class);
                internal_static_Hedwig_RegionSpecificSeqId_descriptor =
                    getDescriptor().getMessageTypes().get(1);
                internal_static_Hedwig_RegionSpecificSeqId_fieldAccessorTable = new
                com.google.protobuf.GeneratedMessage.FieldAccessorTable(
                    internal_static_Hedwig_RegionSpecificSeqId_descriptor,
                    new java.lang.String[] { "Region", "SeqId", },
                    org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId.class,
                    org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId.Builder.class);
                internal_static_Hedwig_MessageSeqId_descriptor =
                    getDescriptor().getMessageTypes().get(2);
                internal_static_Hedwig_MessageSeqId_fieldAccessorTable = new
                com.google.protobuf.GeneratedMessage.FieldAccessorTable(
                    internal_static_Hedwig_MessageSeqId_descriptor,
                    new java.lang.String[] { "LocalComponent", "RemoteComponents", },
                    org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.class,
                    org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder.class);
                internal_static_Hedwig_PubSubRequest_descriptor =
                    getDescriptor().getMessageTypes().get(3);
                internal_static_Hedwig_PubSubRequest_fieldAccessorTable = new
                com.google.protobuf.GeneratedMessage.FieldAccessorTable(
                    internal_static_Hedwig_PubSubRequest_descriptor,
                    new java.lang.String[] { "ProtocolVersion", "Type", "TriedServers", "TxnId", "ShouldClaim", "Topic", "PublishRequest", "SubscribeRequest", "ConsumeRequest", "UnsubscribeRequest", "StopDeliveryRequest", "StartDeliveryRequest", },
                    org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest.class,
                    org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest.Builder.class);
                internal_static_Hedwig_PublishRequest_descriptor =
                    getDescriptor().getMessageTypes().get(4);
                internal_static_Hedwig_PublishRequest_fieldAccessorTable = new
                com.google.protobuf.GeneratedMessage.FieldAccessorTable(
                    internal_static_Hedwig_PublishRequest_descriptor,
                    new java.lang.String[] { "Msg", },
                    org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.class,
                    org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.Builder.class);
                internal_static_Hedwig_SubscribeRequest_descriptor =
                    getDescriptor().getMessageTypes().get(5);
                internal_static_Hedwig_SubscribeRequest_fieldAccessorTable = new
                com.google.protobuf.GeneratedMessage.FieldAccessorTable(
                    internal_static_Hedwig_SubscribeRequest_descriptor,
                    new java.lang.String[] { "SubscriberId", "CreateOrAttach", "Synchronous", },
                    org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.class,
                    org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.Builder.class);
                internal_static_Hedwig_ConsumeRequest_descriptor =
                    getDescriptor().getMessageTypes().get(6);
                internal_static_Hedwig_ConsumeRequest_fieldAccessorTable = new
                com.google.protobuf.GeneratedMessage.FieldAccessorTable(
                    internal_static_Hedwig_ConsumeRequest_descriptor,
                    new java.lang.String[] { "SubscriberId", "MsgId", },
                    org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.class,
                    org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.Builder.class);
                internal_static_Hedwig_UnsubscribeRequest_descriptor =
                    getDescriptor().getMessageTypes().get(7);
                internal_static_Hedwig_UnsubscribeRequest_fieldAccessorTable = new
                com.google.protobuf.GeneratedMessage.FieldAccessorTable(
                    internal_static_Hedwig_UnsubscribeRequest_descriptor,
                    new java.lang.String[] { "SubscriberId", },
                    org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.class,
                    org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.Builder.class);
                internal_static_Hedwig_StopDeliveryRequest_descriptor =
                    getDescriptor().getMessageTypes().get(8);
                internal_static_Hedwig_StopDeliveryRequest_fieldAccessorTable = new
                com.google.protobuf.GeneratedMessage.FieldAccessorTable(
                    internal_static_Hedwig_StopDeliveryRequest_descriptor,
                    new java.lang.String[] { "SubscriberId", },
                    org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.class,
                    org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.Builder.class);
                internal_static_Hedwig_StartDeliveryRequest_descriptor =
                    getDescriptor().getMessageTypes().get(9);
                internal_static_Hedwig_StartDeliveryRequest_fieldAccessorTable = new
                com.google.protobuf.GeneratedMessage.FieldAccessorTable(
                    internal_static_Hedwig_StartDeliveryRequest_descriptor,
                    new java.lang.String[] { "SubscriberId", },
                    org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.class,
                    org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.Builder.class);
                internal_static_Hedwig_PubSubResponse_descriptor =
                    getDescriptor().getMessageTypes().get(10);
                internal_static_Hedwig_PubSubResponse_fieldAccessorTable = new
                com.google.protobuf.GeneratedMessage.FieldAccessorTable(
                    internal_static_Hedwig_PubSubResponse_descriptor,
                    new java.lang.String[] { "ProtocolVersion", "StatusCode", "TxnId", "StatusMsg", "Message", "Topic", "SubscriberId", },
                    org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse.class,
                    org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse.Builder.class);
                internal_static_Hedwig_SubscriptionState_descriptor =
                    getDescriptor().getMessageTypes().get(11);
                internal_static_Hedwig_SubscriptionState_fieldAccessorTable = new
                com.google.protobuf.GeneratedMessage.FieldAccessorTable(
                    internal_static_Hedwig_SubscriptionState_descriptor,
                    new java.lang.String[] { "MsgId", },
                    org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState.class,
                    org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState.Builder.class);
                internal_static_Hedwig_LedgerRange_descriptor =
                    getDescriptor().getMessageTypes().get(12);
                internal_static_Hedwig_LedgerRange_fieldAccessorTable = new
                com.google.protobuf.GeneratedMessage.FieldAccessorTable(
                    internal_static_Hedwig_LedgerRange_descriptor,
                    new java.lang.String[] { "LedgerId", "EndSeqIdIncluded", },
                    org.apache.hedwig.protocol.PubSubProtocol.LedgerRange.class,
                    org.apache.hedwig.protocol.PubSubProtocol.LedgerRange.Builder.class);
                internal_static_Hedwig_LedgerRanges_descriptor =
                    getDescriptor().getMessageTypes().get(13);
                internal_static_Hedwig_LedgerRanges_fieldAccessorTable = new
                com.google.protobuf.GeneratedMessage.FieldAccessorTable(
                    internal_static_Hedwig_LedgerRanges_descriptor,
                    new java.lang.String[] { "Ranges", },
                    org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges.class,
                    org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges.Builder.class);
                return null;
            }
        };
        com.google.protobuf.Descriptors.FileDescriptor
        .internalBuildGeneratedFileFrom(descriptorData,
                                        new com.google.protobuf.Descriptors.FileDescriptor[] {
                                        }, assigner);
    }

    public static void internalForceInit() {}

    // @@protoc_insertion_point(outer_class_scope)
}
"
hedwig-protocol/src/main/java/org/apache/hedwig/protoextensions/MessageIdUtils.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.protoextensions;

import java.util.HashMap;
import java.util.List;
import java.util.Map;

import com.google.protobuf.ByteString;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.exceptions.PubSubException.UnexpectedConditionException;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId;

public class MessageIdUtils {

    public static String msgIdToReadableString(MessageSeqId seqId) {
        StringBuilder sb = new StringBuilder();
        sb.append("local:");
        sb.append(seqId.getLocalComponent());

        String separator = ";";
        for (RegionSpecificSeqId regionId : seqId.getRemoteComponentsList()) {
            sb.append(separator);
            sb.append(regionId.getRegion().toStringUtf8());
            sb.append(':');
            sb.append(regionId.getSeqId());
        }
        return sb.toString();
    }

    public static Map<ByteString, RegionSpecificSeqId> inMapForm(MessageSeqId msi) {
        Map<ByteString, RegionSpecificSeqId> map = new HashMap<ByteString, RegionSpecificSeqId>();

        for (RegionSpecificSeqId lmsid : msi.getRemoteComponentsList()) {
            map.put(lmsid.getRegion(), lmsid);
        }

        return map;
    }

    public static boolean areEqual(MessageSeqId m1, MessageSeqId m2) {

        if (m1.getLocalComponent() != m2.getLocalComponent()) {
            return false;
        }

        if (m1.getRemoteComponentsCount() != m2.getRemoteComponentsCount()) {
            return false;
        }

        Map<ByteString, RegionSpecificSeqId> m2map = inMapForm(m2);

        for (RegionSpecificSeqId lmsid1 : m1.getRemoteComponentsList()) {
            RegionSpecificSeqId lmsid2 = m2map.get(lmsid1.getRegion());
            if (lmsid2 == null) {
                return false;
            }
            if (lmsid1.getSeqId() != lmsid2.getSeqId()) {
                return false;
            }
        }

        return true;

    }

    public static Message mergeLocalSeqId(Message.Builder messageBuilder, long localSeqId) {
        MessageSeqId.Builder msidBuilder = MessageSeqId.newBuilder(messageBuilder.getMsgId());
        msidBuilder.setLocalComponent(localSeqId);
        messageBuilder.setMsgId(msidBuilder);
        return messageBuilder.build();
    }

    public static Message mergeLocalSeqId(Message orginalMessage, long localSeqId) {
        return mergeLocalSeqId(Message.newBuilder(orginalMessage), localSeqId);
    }

    /**
     * Compares two seq numbers represented as lists of longs.
     *
     * @param l1
     * @param l2
     * @return 1 if the l1 is greater, 0 if they are equal, -1 if l2 is greater
     * @throws UnexpectedConditionException
     *             If the lists are of unequal length
     */
    public static int compare(List<Long> l1, List<Long> l2) throws UnexpectedConditionException {
        if (l1.size() != l2.size()) {
            throw new UnexpectedConditionException("Seq-ids being compared have different sizes: " + l1.size()
                                                   + " and " + l2.size());
        }

        for (int i = 0; i < l1.size(); i++) {
            long v1 = l1.get(i);
            long v2 = l2.get(i);

            if (v1 == v2) {
                continue;
            }

            return v1 > v2 ? 1 : -1;
        }

        // All components equal
        return 0;
    }

    /**
     * Returns the element-wise vector maximum of the two vectors id1 and id2,
     * if we imagine them to be sparse representations of vectors.
     */
    public static void takeRegionMaximum(MessageSeqId.Builder newIdBuilder, MessageSeqId id1, MessageSeqId id2) {
        Map<ByteString, RegionSpecificSeqId> id2Map = MessageIdUtils.inMapForm(id2);

        for (RegionSpecificSeqId rrsid1 : id1.getRemoteComponentsList()) {
            ByteString region = rrsid1.getRegion();

            RegionSpecificSeqId rssid2 = id2Map.get(region);

            if (rssid2 == null) {
                newIdBuilder.addRemoteComponents(rrsid1);
                continue;
            }

            newIdBuilder.addRemoteComponents((rrsid1.getSeqId() > rssid2.getSeqId()) ? rrsid1 : rssid2);

            // remove from map
            id2Map.remove(region);
        }

        // now take the remaining components in the map and add them
        for (RegionSpecificSeqId rssid2 : id2Map.values()) {
            newIdBuilder.addRemoteComponents(rssid2);
        }

    }
}
"
hedwig-protocol/src/main/java/org/apache/hedwig/protoextensions/PubSubResponseUtils.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.protoextensions;

import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse;
import org.apache.hedwig.protocol.PubSubProtocol.StatusCode;

public class PubSubResponseUtils {

    /**
     * Change here if bumping up the version number that the server sends back
     */
    protected static ProtocolVersion serverVersion = ProtocolVersion.VERSION_ONE;

    static PubSubResponse.Builder getBasicBuilder(StatusCode status) {
        return PubSubResponse.newBuilder().setProtocolVersion(serverVersion).setStatusCode(status);
    }

    public static PubSubResponse getSuccessResponse(long txnId) {
        return getBasicBuilder(StatusCode.SUCCESS).setTxnId(txnId).build();
    }

    public static PubSubResponse getResponseForException(PubSubException e, long txnId) {
        return getBasicBuilder(e.getCode()).setStatusMsg(e.getMessage()).setTxnId(txnId).build();
    }
}
"
hedwig-protocol/src/main/java/org/apache/hedwig/protoextensions/SubscriptionStateUtils.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.protoextensions;

import com.google.protobuf.ByteString;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState;

public class SubscriptionStateUtils {

    // For now, to differentiate hub subscribers from local ones, the
    // subscriberId will be prepended with a hard-coded prefix. Local
    // subscribers will validate that the subscriberId used cannot start with
    // this prefix. This is only used internally by the hub subscribers.
    public static final String HUB_SUBSCRIBER_PREFIX = "__";

    public static String toString(SubscriptionState state) {
        StringBuilder sb = new StringBuilder();
        sb.append("consumeSeqId: " + MessageIdUtils.msgIdToReadableString(state.getMsgId()));
        return sb.toString();
    }

    public static boolean isHubSubscriber(ByteString subscriberId) {
        return subscriberId.toStringUtf8().startsWith(HUB_SUBSCRIBER_PREFIX);
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/benchmark/AbstractBenchmark.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.benchmark;

import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.Semaphore;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicLong;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.hedwig.util.ConcurrencyUtils;

public abstract class AbstractBenchmark {

    static final Logger logger = LoggerFactory.getLogger(AbstractBenchmark.class);

    AtomicLong totalLatency = new AtomicLong();
    LinkedBlockingQueue<Boolean> doneSignalQueue = new LinkedBlockingQueue<Boolean>();

    abstract void doOps(int numOps) throws Exception;
    abstract void tearDown() throws Exception;

    protected class AbstractCallback {
        AtomicInteger numDone = new AtomicInteger(0);
        Semaphore outstanding;
        int numOps;
        boolean logging;

        public AbstractCallback(Semaphore outstanding, int numOps) {
            this.outstanding = outstanding;
            this.numOps = numOps;
            logging = Boolean.getBoolean("progress");
        }

        public void handle(boolean success, Object ctx) {
            outstanding.release();

            if (!success) {
                ConcurrencyUtils.put(doneSignalQueue, false);
                return;
            }

            totalLatency.addAndGet(System.currentTimeMillis() - (Long)ctx);
            int numDoneInt = numDone.incrementAndGet();

            if (logging && numDoneInt % 10000 == 0) {
                logger.info("Finished " + numDoneInt + " ops");
            }

            if (numOps == numDoneInt) {
                ConcurrencyUtils.put(doneSignalQueue, true);
            }
        }
    }

    public void runPhase(String phase, int numOps) throws Exception {
        long startTime = System.currentTimeMillis();

        doOps(numOps);

        if (!doneSignalQueue.take()) {
            logger.error("One or more operations failed in phase: " + phase);
            throw new RuntimeException();
        } else {
            logger.info("Phase: " + phase + " Avg latency : " + totalLatency.get() / numOps + ", tput = " + (numOps * 1000/ (System.currentTimeMillis() - startTime)));
        }
    }





    public void run() throws Exception {

        int numWarmup = Integer.getInteger("nWarmup", 50000);
        runPhase("warmup", numWarmup);

        logger.info("Sleeping for 10 seconds");
        Thread.sleep(10000);
        //reset latency
        totalLatency.set(0);

        int numOps = Integer.getInteger("nOps", 400000);
        runPhase("real", numOps);

        tearDown();
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/benchmark/BookieBenchmark.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.benchmark;

import java.net.InetSocketAddress;
import java.nio.ByteBuffer;
import java.util.concurrent.Executors;
import java.util.concurrent.Semaphore;
import org.apache.bookkeeper.conf.ClientConfiguration;
import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.proto.BookieClient;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.WriteCallback;
import org.apache.bookkeeper.util.OrderedSafeExecutor;
import org.jboss.netty.buffer.ChannelBuffer;
import org.jboss.netty.buffer.ChannelBuffers;
import org.jboss.netty.channel.socket.ClientSocketChannelFactory;
import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory;

public class BookieBenchmark extends AbstractBenchmark {

    static final Logger logger = LoggerFactory.getLogger(BookkeeperBenchmark.class);

    BookieClient bkc;
    InetSocketAddress addr;
    ClientSocketChannelFactory channelFactory;
    OrderedSafeExecutor executor = new OrderedSafeExecutor(1);


    public BookieBenchmark(String bookieHostPort)  throws Exception {
        channelFactory = new NioClientSocketChannelFactory(Executors.newCachedThreadPool(), Executors.newCachedThreadPool());
        bkc = new BookieClient(new ClientConfiguration(), channelFactory, executor);
        String[] hostPort = bookieHostPort.split(":");
        addr = new InetSocketAddress(hostPort[0], Integer.parseInt(hostPort[1]));

    }


    @Override
    void doOps(final int numOps) throws Exception {
        int numOutstanding = Integer.getInteger("nPars",1000);
        final Semaphore outstanding = new Semaphore(numOutstanding);


        WriteCallback callback = new WriteCallback() {
            AbstractCallback handler = new AbstractCallback(outstanding, numOps);

            @Override
            public void writeComplete(int rc, long ledgerId, long entryId,
            InetSocketAddress addr, Object ctx) {
                handler.handle(rc == BKException.Code.OK, ctx);
            }
        };

        byte[] passwd = new byte[20];
        int size = Integer.getInteger("size", 1024);
        byte[] data = new byte[size];

        for (int i=0; i<numOps; i++) {
            outstanding.acquire();

            ByteBuffer buffer = ByteBuffer.allocate(44);
            long ledgerId = 1000;
            buffer.putLong(ledgerId);
            buffer.putLong(i);
            buffer.putLong(0);
            buffer.put(passwd);
            buffer.rewind();
            ChannelBuffer toSend = ChannelBuffers.wrappedBuffer(ChannelBuffers.wrappedBuffer(buffer.slice()), ChannelBuffers.wrappedBuffer(data));
            bkc.addEntry(addr, ledgerId, passwd, i, toSend, callback, System.currentTimeMillis(), 0);
        }

    }

    @Override
    public void tearDown() {
        bkc.close();
        channelFactory.releaseExternalResources();
        executor.shutdown();
    }


    public static void main(String[] args) throws Exception {
        BookieBenchmark benchmark = new BookieBenchmark(args[0]);
        benchmark.run();
    }


}
"
hedwig-server/src/main/java/org/apache/hedwig/server/benchmark/BookkeeperBenchmark.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.benchmark;

import java.util.Random;
import java.util.concurrent.Semaphore;
import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.client.BookKeeper;
import org.apache.bookkeeper.client.LedgerHandle;
import org.apache.bookkeeper.client.AsyncCallback.AddCallback;
import org.apache.bookkeeper.client.BookKeeper.DigestType;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class BookkeeperBenchmark extends AbstractBenchmark {

    static final Logger logger = LoggerFactory.getLogger(BookkeeperBenchmark.class);

    BookKeeper bk;
    LedgerHandle[] lh;

    public BookkeeperBenchmark(String zkHostPort) throws Exception {
        bk = new BookKeeper(zkHostPort);
        int numLedgers = Integer.getInteger("nLedgers",5);
        lh = new LedgerHandle[numLedgers];
        int quorumSize = Integer.getInteger("quorum", 2);
        int ensembleSize = Integer.getInteger("ensemble", 4);
        DigestType digestType = DigestType.valueOf(System.getProperty("digestType", "CRC32"));
        for (int i=0; i< numLedgers; i++) {
            lh[i] = bk.createLedger(ensembleSize, quorumSize, digestType, "blah".getBytes());
        }

    }


    @Override
    void doOps(final int numOps) throws Exception {
        int size = Integer.getInteger("size", 1024);
        byte[] msg = new byte[size];

        int numOutstanding = Integer.getInteger("nPars",1000);
        final Semaphore outstanding = new Semaphore(numOutstanding);

        AddCallback callback = new AddCallback() {
            AbstractCallback handler = new AbstractCallback(outstanding, numOps);


            @Override
            public void addComplete(int rc, LedgerHandle lh, long entryId, Object ctx) {
                handler.handle(rc == BKException.Code.OK, ctx);
            }

        };



        Random rand = new Random();

        for (int i=0; i<numOps; i++) {
            outstanding.acquire();
            lh[rand.nextInt(lh.length)].asyncAddEntry(msg, callback, System.currentTimeMillis());
        }


    }

    @Override
    public void tearDown() throws Exception {
        bk.close();
    }


    public static void main(String[] args) throws Exception {
        BookkeeperBenchmark benchmark = new BookkeeperBenchmark(args[0]);
        benchmark.run();
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/benchmark/FakeBookie.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.benchmark;

import java.net.InetSocketAddress;
import java.util.concurrent.Executors;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.bootstrap.ServerBootstrap;
import org.jboss.netty.buffer.ChannelBuffer;
import org.jboss.netty.channel.ChannelHandlerContext;
import org.jboss.netty.channel.ChannelPipeline;
import org.jboss.netty.channel.ChannelPipelineCoverage;
import org.jboss.netty.channel.ChannelPipelineFactory;
import org.jboss.netty.channel.Channels;
import org.jboss.netty.channel.MessageEvent;
import org.jboss.netty.channel.SimpleChannelHandler;
import org.jboss.netty.channel.socket.ServerSocketChannelFactory;
import org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory;
import org.jboss.netty.handler.codec.frame.LengthFieldBasedFrameDecoder;
import org.jboss.netty.handler.codec.frame.LengthFieldPrepender;
import org.jboss.netty.logging.InternalLoggerFactory;
import org.jboss.netty.logging.Log4JLoggerFactory;

@ChannelPipelineCoverage("all")
public class FakeBookie extends SimpleChannelHandler implements
    ChannelPipelineFactory {
    static final Logger logger = LoggerFactory.getLogger(FakeBookie.class);
    ServerSocketChannelFactory serverChannelFactory = new NioServerSocketChannelFactory(
        Executors.newCachedThreadPool(), Executors.newCachedThreadPool());

    public FakeBookie(int port) {
        InternalLoggerFactory.setDefaultFactory(new Log4JLoggerFactory());
        ServerBootstrap bootstrap = new ServerBootstrap(serverChannelFactory);

        bootstrap.setPipelineFactory(this);
        bootstrap.setOption("child.tcpNoDelay", true);
        bootstrap.setOption("child.keepAlive", true);
        bootstrap.setOption("reuseAddress", true);

        logger.info("Going into receive loop");
        // Bind and start to accept incoming connections.
        bootstrap.bind(new InetSocketAddress(port));
    }

    @Override
    public ChannelPipeline getPipeline() throws Exception {
        ChannelPipeline pipeline = Channels.pipeline();
        pipeline.addLast("lengthbaseddecoder",
                         new LengthFieldBasedFrameDecoder(1024 * 1024, 0, 4, 0, 4));
        pipeline.addLast("lengthprepender", new LengthFieldPrepender(4));
        pipeline.addLast("main", this);
        return pipeline;
    }

    @Override
    public void messageReceived(ChannelHandlerContext ctx, MessageEvent e)
            throws Exception {
        if (!(e.getMessage() instanceof ChannelBuffer)) {
            ctx.sendUpstream(e);
            return;
        }

        ChannelBuffer buffer = (ChannelBuffer) e.getMessage();

        int type = buffer.readInt();
        buffer.readerIndex(24);
        long ledgerId = buffer.readLong();
        long entryId = buffer.readLong();

        ChannelBuffer outBuf = ctx.getChannel().getConfig().getBufferFactory()
                               .getBuffer(24);
        outBuf.writeInt(type);
        outBuf.writeInt(0); // rc
        outBuf.writeLong(ledgerId);
        outBuf.writeLong(entryId);
        e.getChannel().write(outBuf);

    }


    public static void main(String args[]) {
        new FakeBookie(Integer.parseInt(args[0]));
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/common/ByteStringInterner.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.common;

import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ConcurrentMap;

import com.google.protobuf.ByteString;

public class ByteStringInterner {
    // TODO: how to release references when strings are no longer used. weak
    // references?

    private static ConcurrentMap<ByteString, ByteString> map = new ConcurrentHashMap<ByteString, ByteString>();

    public static ByteString intern(ByteString in) {
        ByteString presentValueInMap = map.get(in);
        if (presentValueInMap != null) {
            return presentValueInMap;
        }

        presentValueInMap = map.putIfAbsent(in, in);
        if (presentValueInMap != null) {
            return presentValueInMap;
        }

        return in;

    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/common/ServerConfiguration.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.common;

import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.InputStream;
import java.net.InetAddress;
import java.net.URL;
import java.net.UnknownHostException;
import java.util.Arrays;
import java.util.LinkedList;
import java.util.List;

import org.apache.commons.configuration.ConfigurationException;

import com.google.protobuf.ByteString;
import org.apache.hedwig.conf.AbstractConfiguration;
import org.apache.hedwig.util.HedwigSocketAddress;

public class ServerConfiguration extends AbstractConfiguration {
    protected final static String REGION = "region";
    protected final static String MAX_MESSAGE_SIZE = "max_message_size";
    protected final static String READAHEAD_COUNT = "readahead_count";
    protected final static String READAHEAD_SIZE = "readahead_size";
    protected final static String CACHE_SIZE = "cache_size";
    protected final static String SCAN_BACKOFF_MSEC = "scan_backoff_ms";
    protected final static String SERVER_PORT = "server_port";
    protected final static String SSL_SERVER_PORT = "ssl_server_port";
    protected final static String ZK_PREFIX = "zk_prefix";
    protected final static String ZK_HOST = "zk_host";
    protected final static String ZK_TIMEOUT = "zk_timeout";
    protected final static String READAHEAD_ENABLED = "readhead_enabled";
    protected final static String STANDALONE = "standalone";
    protected final static String REGIONS = "regions";
    protected final static String CERT_NAME = "cert_name";
    protected final static String CERT_PATH = "cert_path";
    protected final static String PASSWORD = "password";
    protected final static String SSL_ENABLED = "ssl_enabled";
    protected final static String CONSUME_INTERVAL = "consume_interval";
    protected final static String RETENTION_SECS = "retention_secs";
    protected final static String INTER_REGION_SSL_ENABLED = "inter_region_ssl_enabled";
    protected final static String MESSAGES_CONSUMED_THREAD_RUN_INTERVAL = "messages_consumed_thread_run_interval";
    protected final static String BK_ENSEMBLE_SIZE = "bk_ensemble_size";
    protected final static String BK_QUORUM_SIZE = "bk_quorum_size";

    // these are the derived attributes
    protected ByteString myRegionByteString = null;
    protected HedwigSocketAddress myServerAddress = null;
    protected List<String> regionList = null;

    // Although this method is not currently used, currently maintaining it like
    // this so that we can support on-the-fly changes in configuration
    protected void refreshDerivedAttributes() {
        refreshMyRegionByteString();
        refreshMyServerAddress();
        refreshRegionList();
    }

    @Override
    public void loadConf(URL confURL) throws ConfigurationException {
        super.loadConf(confURL);
        refreshDerivedAttributes();
    }

    public int getMaximumMessageSize() {
        return conf.getInt(MAX_MESSAGE_SIZE, 1258291); /* 1.2M */
    }

    public String getMyRegion() {
        return conf.getString(REGION, "standalone");
    }

    protected void refreshMyRegionByteString() {
        myRegionByteString = ByteString.copyFromUtf8(getMyRegion());
    }

    protected void refreshMyServerAddress() {
        try {
            // Use the raw IP address as the hostname
            myServerAddress = new HedwigSocketAddress(InetAddress.getLocalHost().getHostAddress(), getServerPort(),
                    getSSLServerPort());
        } catch (UnknownHostException e) {
            throw new RuntimeException(e);
        }
    }

    // The expected format for the regions parameter is Hostname:Port:SSLPort
    // with spaces in between each of the regions.
    protected void refreshRegionList() {
        String regions = conf.getString(REGIONS, "");
        if (regions.isEmpty()) {
            regionList = new LinkedList<String>();
        } else {
            regionList = Arrays.asList(regions.split(" "));
        }
    }

    public ByteString getMyRegionByteString() {
        if (myRegionByteString == null) {
            refreshMyRegionByteString();
        }
        return myRegionByteString;
    }

    public int getReadAheadCount() {
        return conf.getInt(READAHEAD_COUNT, 10);
    }

    public long getReadAheadSizeBytes() {
        return conf.getLong(READAHEAD_SIZE, 4 * 1024 * 1024); // 4M
    }

    public long getMaximumCacheSize() {
        // 2G or half of the maximum amount of memory the JVM uses
        return conf.getLong(CACHE_SIZE, Math.min(2 * 1024L * 1024L * 1024L, Runtime.getRuntime().maxMemory() / 2));
    }

    // After a scan of a log fails, how long before we retry (in msec)
    public long getScanBackoffPeriodMs() {
        return conf.getLong(SCAN_BACKOFF_MSEC, 1000);
    }

    public int getServerPort() {
        return conf.getInt(SERVER_PORT, 4080);
    }

    public int getSSLServerPort() {
        return conf.getInt(SSL_SERVER_PORT, 9876);
    }

    public String getZkPrefix() {
        return conf.getString(ZK_PREFIX, "/hedwig");
    }

    public StringBuilder getZkRegionPrefix(StringBuilder sb) {
        return sb.append(getZkPrefix()).append("/").append(getMyRegion());
    }

    public StringBuilder getZkTopicsPrefix(StringBuilder sb) {
        return getZkRegionPrefix(sb).append("/topics");
    }

    public StringBuilder getZkTopicPath(StringBuilder sb, ByteString topic) {
        return getZkTopicsPrefix(sb).append("/").append(topic.toStringUtf8());
    }

    public StringBuilder getZkHostsPrefix(StringBuilder sb) {
        return getZkRegionPrefix(sb).append("/hosts");
    }

    public HedwigSocketAddress getServerAddr() {
        if (myServerAddress == null) {
            refreshMyServerAddress();
        }
        return myServerAddress;
    }

    public String getZkHost() {
        return conf.getString(ZK_HOST, "localhost");
    }

    public int getZkTimeout() {
        return conf.getInt(ZK_TIMEOUT, 2000);
    }

    public boolean getReadAheadEnabled() {
        return conf.getBoolean(READAHEAD_ENABLED, true);
    }

    public boolean isStandalone() {
        return conf.getBoolean(STANDALONE, false);
    }

    public List<String> getRegions() {
        if (regionList == null) {
            refreshRegionList();
        }
        return regionList;
    }

    // This is the name of the SSL certificate if available as a resource.
    public String getCertName() {
        return conf.getString(CERT_NAME, "");
    }

    // This is the path to the SSL certificate if it is available as a file.
    public String getCertPath() {
        return conf.getString(CERT_PATH, "");
    }

    // This method return the SSL certificate as an InputStream based on if it
    // is configured to be available as a resource or as a file. If nothing is
    // configured correctly, then a ConfigurationException will be thrown as
    // we do not know how to obtain the SSL certificate stream.
    public InputStream getCertStream() throws FileNotFoundException, ConfigurationException {
        String certName = getCertName();
        String certPath = getCertPath();
        if (certName != null && !certName.isEmpty()) {
            return getClass().getResourceAsStream(certName);
        } else if (certPath != null && !certPath.isEmpty()) {
            return new FileInputStream(certPath);
        } else
            throw new ConfigurationException("SSL Certificate configuration does not have resource name or path set!");
    }

    public String getPassword() {
        return conf.getString(PASSWORD, "");
    }

    public boolean isSSLEnabled() {
        return conf.getBoolean(SSL_ENABLED, false);
    }

    public int getConsumeInterval() {
        return conf.getInt(CONSUME_INTERVAL, 50);
    }

    public int getRetentionSecs() {
        return conf.getInt(RETENTION_SECS, 0);
    }

    public boolean isInterRegionSSLEnabled() {
        return conf.getBoolean(INTER_REGION_SSL_ENABLED, false);
    }

    // This parameter is used to determine how often we run the
    // SubscriptionManager's Messages Consumed timer task thread (in
    // milliseconds).
    public int getMessagesConsumedThreadRunInterval() {
        return conf.getInt(MESSAGES_CONSUMED_THREAD_RUN_INTERVAL, 60000);
    }

    // This parameter is used when Bookkeeper is the persistence store
    // and indicates what the ensemble size is (i.e. how many bookie
    // servers to stripe the ledger entries across).
    public int getBkEnsembleSize() {
        return conf.getInt(BK_ENSEMBLE_SIZE, 3);
    }

    // This parameter is used when Bookkeeper is the persistence store
    // and indicates what the quorum size is (i.e. how many redundant
    // copies of each ledger entry is written).
    public int getBkQuorumSize() {
        return conf.getInt(BK_QUORUM_SIZE, 2);
    }

    /*
     * Is this a valid configuration that we can run with? This code might grow
     * over time.
     */
    public void validate() throws ConfigurationException {
        if (!getZkPrefix().startsWith("/")) {
            throw new ConfigurationException(ZK_PREFIX + " must start with a /");
        }
        // Validate that if Regions exist and inter-region communication is SSL
        // enabled, that the Regions correspond to valid HedwigSocketAddresses,
        // namely that SSL ports are present.
        if (isInterRegionSSLEnabled() && getRegions().size() > 0) {
            for (String hubString : getRegions()) {
                HedwigSocketAddress hub = new HedwigSocketAddress(hubString);
                if (hub.getSSLSocketAddress() == null)
                    throw new ConfigurationException("Region defined does not have required SSL port: " + hubString);
            }
        }
        // Validate that the Bookkeeper ensemble size >= quorum size.
        if (getBkEnsembleSize() < getBkQuorumSize()) {
            throw new ConfigurationException("BK ensemble size (" + getBkEnsembleSize()
                                             + ") is less than the quorum size (" + getBkQuorumSize() + ")");
        }

        // add other checks here
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/common/TerminateJVMExceptionHandler.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.common;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class TerminateJVMExceptionHandler implements Thread.UncaughtExceptionHandler {
    static Logger logger = LoggerFactory.getLogger(TerminateJVMExceptionHandler.class);

    @Override
    public void uncaughtException(Thread t, Throwable e) {
        logger.error("Uncaught exception in thread " + t.getName(), e);
        System.exit(1);
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/common/TopicOpQueuer.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.common;

import java.util.HashMap;
import java.util.LinkedList;
import java.util.Queue;
import java.util.concurrent.ScheduledExecutorService;

import com.google.protobuf.ByteString;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.util.Callback;

public class TopicOpQueuer {
    /**
     * Map from topic to the queue of operations for that topic.
     */
    protected HashMap<ByteString, Queue<Runnable>> topic2ops = new HashMap<ByteString, Queue<Runnable>>();

    protected final ScheduledExecutorService scheduler;

    public TopicOpQueuer(ScheduledExecutorService scheduler) {
        this.scheduler = scheduler;
    }

    public interface Op extends Runnable {
    }

    public abstract class AsynchronousOp<T> implements Op {
        final public ByteString topic;
        final public Callback<T> cb;
        final public Object ctx;

        public AsynchronousOp(final ByteString topic, final Callback<T> cb, Object ctx) {
            this.topic = topic;
            this.cb = new Callback<T>() {
                @Override
                public void operationFailed(Object ctx, PubSubException exception) {
                    cb.operationFailed(ctx, exception);
                    popAndRunNext(topic);
                }

                @Override
                public void operationFinished(Object ctx, T resultOfOperation) {
                    cb.operationFinished(ctx, resultOfOperation);
                    popAndRunNext(topic);
                }
            };
            this.ctx = ctx;
        }
    }

    public abstract class SynchronousOp implements Op {
        final public ByteString topic;

        public SynchronousOp(ByteString topic) {
            this.topic = topic;
        }

        @Override
        public final void run() {
            runInternal();
            popAndRunNext(topic);
        }

        protected abstract void runInternal();

    }

    protected synchronized void popAndRunNext(ByteString topic) {
        Queue<Runnable> ops = topic2ops.get(topic);
        if (!ops.isEmpty())
            ops.remove();
        if (!ops.isEmpty())
            scheduler.submit(ops.peek());
    }

    public void pushAndMaybeRun(ByteString topic, Op op) {
        int size;
        synchronized (this) {
            Queue<Runnable> ops = topic2ops.get(topic);
            if (ops == null) {
                ops = new LinkedList<Runnable>();
                topic2ops.put(topic, ops);
            }
            ops.add(op);
            size = ops.size();
        }
        if (size == 1)
            op.run();
    }

    public Runnable peek(ByteString topic) {
        return topic2ops.get(topic).peek();
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/common/UnexpectedError.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.common;

public class UnexpectedError extends Error {

    /**
     *
     */
    private static final long serialVersionUID = 1L;

    public UnexpectedError(String msg) {
        super(msg);
    }

    public UnexpectedError(Throwable cause) {
        super(cause);
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/delivery/ChannelEndPoint.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.delivery;

import java.util.HashMap;
import java.util.Map;

import org.jboss.netty.channel.Channel;
import org.jboss.netty.channel.ChannelFuture;
import org.jboss.netty.channel.ChannelFutureListener;

import org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse;
import org.apache.hedwig.server.common.UnexpectedError;

public class ChannelEndPoint implements DeliveryEndPoint, ChannelFutureListener {

    Channel channel;

    public Channel getChannel() {
        return channel;
    }

    Map<ChannelFuture, DeliveryCallback> callbacks = new HashMap<ChannelFuture, DeliveryCallback>();

    public ChannelEndPoint(Channel channel) {
        this.channel = channel;
    }

    public void close() {
        channel.close();
    }

    public void send(PubSubResponse response, DeliveryCallback callback) {
        ChannelFuture future = channel.write(response);
        callbacks.put(future, callback);
        future.addListener(this);
    }

    public void operationComplete(ChannelFuture future) throws Exception {
        DeliveryCallback callback = callbacks.get(future);
        callbacks.remove(future);

        if (callback == null) {
            throw new UnexpectedError("Could not locate callback for channel future");
        }

        if (future.isSuccess()) {
            callback.sendingFinished();
        } else {
            // treat all channel errors as permanent
            callback.permanentErrorOnSend();
        }

    }

    @Override
    public boolean equals(Object obj) {
        if (obj instanceof ChannelEndPoint) {
            ChannelEndPoint channelEndPoint = (ChannelEndPoint) obj;
            return channel.equals(channelEndPoint.channel);
        } else {
            return false;
        }
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/delivery/DeliveryCallback.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.delivery;

public interface DeliveryCallback {

    public void sendingFinished();

    public void transientErrorOnSend();

    public void permanentErrorOnSend();
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/delivery/DeliveryEndPoint.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.delivery;

import org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse;

public interface DeliveryEndPoint {

    public void send(PubSubResponse response, DeliveryCallback callback);

    public void close();

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/delivery/DeliveryManager.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.delivery;

import com.google.protobuf.ByteString;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.server.subscriptions.MessageFilter;

public interface DeliveryManager {
    public void startServingSubscription(ByteString topic, ByteString subscriberId, MessageSeqId seqIdToStartFrom,
                                         DeliveryEndPoint endPoint, MessageFilter filter, boolean isHubSubscriber);

    public void stopServingSubscriber(ByteString topic, ByteString subscriberId);
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/delivery/FIFODeliveryManager.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.delivery;

import java.util.HashMap;
import java.util.HashSet;
import java.util.Map;
import java.util.Queue;
import java.util.Set;
import java.util.SortedMap;
import java.util.TreeMap;
import java.util.concurrent.BlockingQueue;
import java.util.concurrent.ConcurrentLinkedQueue;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.TimeUnit;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.protobuf.ByteString;
import org.apache.hedwig.client.data.TopicSubscriber;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse;
import org.apache.hedwig.protocol.PubSubProtocol.StatusCode;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.common.UnexpectedError;
import org.apache.hedwig.server.persistence.Factory;
import org.apache.hedwig.server.persistence.MapMethods;
import org.apache.hedwig.server.persistence.PersistenceManager;
import org.apache.hedwig.server.persistence.ScanCallback;
import org.apache.hedwig.server.persistence.ScanRequest;
import org.apache.hedwig.server.subscriptions.MessageFilter;

public class FIFODeliveryManager implements Runnable, DeliveryManager {

    protected static final Logger logger = LoggerFactory.getLogger(FIFODeliveryManager.class);

    protected interface DeliveryManagerRequest {
        public void performRequest();
    }

    /**
     * the main queue that the single-threaded delivery manager works off of
     */
    BlockingQueue<DeliveryManagerRequest> requestQueue = new LinkedBlockingQueue<DeliveryManagerRequest>();

    /**
     * The queue of all subscriptions that are facing a transient error either
     * in scanning from the persistence manager, or in sending to the consumer
     */
    Queue<ActiveSubscriberState> retryQueue = new ConcurrentLinkedQueue<ActiveSubscriberState>();

    /**
     * Stores a mapping from topic to the delivery pointers on the topic. The
     * delivery pointers are stored in a sorted map from seq-id to the set of
     * subscribers at that seq-id
     */
    Map<ByteString, SortedMap<Long, Set<ActiveSubscriberState>>> perTopicDeliveryPtrs;

    /**
     * Mapping from delivery end point to the subscriber state that we are
     * serving at that end point. This prevents us e.g., from serving two
     * subscriptions to the same endpoint
     */
    Map<TopicSubscriber, ActiveSubscriberState> subscriberStates;

    private PersistenceManager persistenceMgr;

    private ServerConfiguration cfg;

    // Boolean indicating if this thread should continue running. This is used
    // when we want to stop the thread during a PubSubServer shutdown.
    protected boolean keepRunning = true;

    public FIFODeliveryManager(PersistenceManager persistenceMgr, ServerConfiguration cfg) {
        this.persistenceMgr = persistenceMgr;
        perTopicDeliveryPtrs = new HashMap<ByteString, SortedMap<Long, Set<ActiveSubscriberState>>>();
        subscriberStates = new HashMap<TopicSubscriber, ActiveSubscriberState>();
        new Thread(this, "DeliveryManagerThread").start();
        this.cfg = cfg;
    }

    /**
     * ===================================================================== Our
     * usual enqueue function, stop if error because of unbounded queue, should
     * never happen
     *
     */
    protected void enqueueWithoutFailure(DeliveryManagerRequest request) {
        if (!requestQueue.offer(request)) {
            throw new UnexpectedError("Could not enqueue object: " + request + " to delivery manager request queue.");
        }
    }

    /**
     * ====================================================================
     * Public interface of the delivery manager
     */

    /**
     * Tells the delivery manager to start sending out messages for a particular
     * subscription
     *
     * @param topic
     * @param subscriberId
     * @param seqIdToStartFrom
     *            Message sequence-id from where delivery should be started
     * @param endPoint
     *            The delivery end point to which send messages to
     * @param filter
     *            Only messages passing this filter should be sent to this
     *            subscriber
     * @param isHubSubscriber
     *            There are some seq-id intricacies. To a hub subscriber, we
     *            should send only a subset of the seq-id vector
     */
    public void startServingSubscription(ByteString topic, ByteString subscriberId, MessageSeqId seqIdToStartFrom,
                                         DeliveryEndPoint endPoint, MessageFilter filter, boolean isHubSubscriber) {

        ActiveSubscriberState subscriber = new ActiveSubscriberState(topic, subscriberId, seqIdToStartFrom
                .getLocalComponent() - 1, endPoint, filter, isHubSubscriber);

        enqueueWithoutFailure(subscriber);
    }

    public void stopServingSubscriber(ByteString topic, ByteString subscriberId) {
        ActiveSubscriberState subState = subscriberStates.get(new TopicSubscriber(topic, subscriberId));

        if (subState != null) {
            stopServingSubscriber(subState);
        }
    }

    /**
     * Due to some error or disconnection or unsusbcribe, someone asks us to
     * stop serving a particular endpoint
     *
     * @param endPoint
     */
    protected void stopServingSubscriber(ActiveSubscriberState subscriber) {
        enqueueWithoutFailure(new StopServingSubscriber(subscriber));
    }

    /**
     * Instructs the delivery manager to backoff on the given subscriber and
     * retry sending after some time
     *
     * @param subscriber
     */

    public void retryErroredSubscriberAfterDelay(ActiveSubscriberState subscriber) {

        subscriber.setLastScanErrorTime(System.currentTimeMillis());

        if (!retryQueue.offer(subscriber)) {
            throw new UnexpectedError("Could not enqueue to delivery manager retry queue");
        }
    }

    /**
     * Instructs the delivery manager to move the delivery pointer for a given
     * subscriber
     *
     * @param subscriber
     * @param prevSeqId
     * @param newSeqId
     */
    public void moveDeliveryPtrForward(ActiveSubscriberState subscriber, long prevSeqId, long newSeqId) {
        enqueueWithoutFailure(new DeliveryPtrMove(subscriber, prevSeqId, newSeqId));
    }

    /*
     * ==========================================================================
     * == End of public interface, internal machinery begins.
     */
    public void run() {
        while (keepRunning) {
            DeliveryManagerRequest request = null;

            try {
                // We use a timeout of 1 second, so that we can wake up once in
                // a while to check if there is something in the retry queue.
                request = requestQueue.poll(1, TimeUnit.SECONDS);
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
            }

            // First retry any subscriptions that had failed and need a retry
            retryErroredSubscribers();

            if (request == null) {
                continue;
            }

            request.performRequest();

        }
    }

    /**
     * Stop method which will enqueue a ShutdownDeliveryManagerRequest.
     */
    public void stop() {
        enqueueWithoutFailure(new ShutdownDeliveryManagerRequest());
    }

    protected void retryErroredSubscribers() {
        long lastInterestingFailureTime = System.currentTimeMillis() - cfg.getScanBackoffPeriodMs();
        ActiveSubscriberState subscriber;

        while ((subscriber = retryQueue.peek()) != null) {
            if (subscriber.getLastScanErrorTime() > lastInterestingFailureTime) {
                // Not enough time has elapsed yet, will retry later
                // Since the queue is fifo, no need to check later items
                return;
            }

            // retry now
            subscriber.deliverNextMessage();
            retryQueue.poll();
        }
    }

    protected void removeDeliveryPtr(ActiveSubscriberState subscriber, Long seqId, boolean isAbsenceOk,
                                     boolean pruneTopic) {

        assert seqId != null;

        // remove this subscriber from the delivery pointers data structure
        ByteString topic = subscriber.getTopic();
        SortedMap<Long, Set<ActiveSubscriberState>> deliveryPtrs = perTopicDeliveryPtrs.get(topic);

        if (deliveryPtrs == null && !isAbsenceOk) {
            throw new UnexpectedError("No delivery pointers found while disconnecting " + "channel for topic:" + topic);
        }

        if(null == deliveryPtrs) {
            return;
        }

        if (!MapMethods.removeFromMultiMap(deliveryPtrs, seqId, subscriber) && !isAbsenceOk) {

            throw new UnexpectedError("Could not find subscriber:" + subscriber + " at the expected delivery pointer");
        }

        if (pruneTopic && deliveryPtrs.isEmpty()) {
            perTopicDeliveryPtrs.remove(topic);
        }

    }

    protected long getMinimumSeqId(ByteString topic) {
        SortedMap<Long, Set<ActiveSubscriberState>> deliveryPtrs = perTopicDeliveryPtrs.get(topic);

        if (deliveryPtrs == null || deliveryPtrs.isEmpty()) {
            return Long.MAX_VALUE - 1;
        }
        return deliveryPtrs.firstKey();
    }

    protected void addDeliveryPtr(ActiveSubscriberState subscriber, Long seqId) {

        // If this topic doesn't exist in the per-topic delivery pointers table,
        // create an entry for it
        SortedMap<Long, Set<ActiveSubscriberState>> deliveryPtrs = MapMethods.getAfterInsertingIfAbsent(
                    perTopicDeliveryPtrs, subscriber.getTopic(), TreeMapLongToSetSubscriberFactory.instance);

        MapMethods.addToMultiMap(deliveryPtrs, seqId, subscriber, HashMapSubscriberFactory.instance);
    }

    public class ActiveSubscriberState implements ScanCallback, DeliveryCallback, DeliveryManagerRequest {
        ByteString topic;
        ByteString subscriberId;
        long lastLocalSeqIdDelivered;
        boolean connected = true;
        DeliveryEndPoint deliveryEndPoint;
        long lastScanErrorTime = -1;
        long localSeqIdDeliveringNow;
        long lastSeqIdCommunicatedExternally;
        // TODO make use of these variables
        MessageFilter filter;
        boolean isHubSubscriber;
        final static int SEQ_ID_SLACK = 10;

        public ActiveSubscriberState(ByteString topic, ByteString subscriberId, long lastLocalSeqIdDelivered,
                                     DeliveryEndPoint deliveryEndPoint, MessageFilter filter, boolean isHubSubscriber) {
            this.topic = topic;
            this.subscriberId = subscriberId;
            this.lastLocalSeqIdDelivered = lastLocalSeqIdDelivered;
            this.deliveryEndPoint = deliveryEndPoint;
            this.filter = filter;
            this.isHubSubscriber = isHubSubscriber;
        }

        public void setNotConnected() {
            this.connected = false;
            deliveryEndPoint.close();
        }

        public ByteString getTopic() {
            return topic;
        }

        public long getLastLocalSeqIdDelivered() {
            return lastLocalSeqIdDelivered;
        }

        public long getLastScanErrorTime() {
            return lastScanErrorTime;
        }

        public void setLastScanErrorTime(long lastScanErrorTime) {
            this.lastScanErrorTime = lastScanErrorTime;
        }

        protected boolean isConnected() {
            return connected;
        }

        public void deliverNextMessage() {

            if (!isConnected()) {
                return;
            }

            localSeqIdDeliveringNow = persistenceMgr.getSeqIdAfterSkipping(topic, lastLocalSeqIdDelivered, 1);

            ScanRequest scanRequest = new ScanRequest(topic, localSeqIdDeliveringNow,
                    /* callback= */this, /* ctx= */null);

            persistenceMgr.scanSingleMessage(scanRequest);
        }

        /**
         * ===============================================================
         * {@link ScanCallback} methods
         */

        public void messageScanned(Object ctx, Message message) {
            if (!connected) {
                return;
            }

            // We're using a simple all-to-all network topology, so no region
            // should ever need to forward messages to any other region.
            // Otherwise, with the current logic, messages will end up
            // ping-pong-ing back and forth between regions with subscriptions
            // to each other without termination (or in any other cyclic
            // configuration).
            if (isHubSubscriber && !message.getSrcRegion().equals(cfg.getMyRegionByteString())) {
                sendingFinished();
                return;
            }

            /**
             * The method below will invoke our sendingFinished() method when
             * done
             */
            PubSubResponse response = PubSubResponse.newBuilder().setProtocolVersion(ProtocolVersion.VERSION_ONE)
                                      .setStatusCode(StatusCode.SUCCESS).setTxnId(0).setMessage(message).build();

            deliveryEndPoint.send(response, //
                                  // callback =
                                  this);

        }

        public void scanFailed(Object ctx, Exception exception) {
            if (!connected) {
                return;
            }

            // wait for some time and then retry
            retryErroredSubscriberAfterDelay(this);
        }

        public void scanFinished(Object ctx, ReasonForFinish reason) {
            // no-op
        }

        /**
         * ===============================================================
         * {@link DeliveryCallback} methods
         */
        public void sendingFinished() {
            if (!connected) {
                return;
            }

            lastLocalSeqIdDelivered = localSeqIdDeliveringNow;

            if (lastLocalSeqIdDelivered > lastSeqIdCommunicatedExternally + SEQ_ID_SLACK) {
                // Note: The order of the next 2 statements is important. We should
                // submit a request to change our delivery pointer only *after* we
                // have actually changed it. Otherwise, there is a race condition
                // with removal of this channel, w.r.t, maintaining the deliveryPtrs
                // tree map.
                long prevId = lastSeqIdCommunicatedExternally;
                lastSeqIdCommunicatedExternally = lastLocalSeqIdDelivered;
                moveDeliveryPtrForward(this, prevId, lastLocalSeqIdDelivered);
            }
            deliverNextMessage();
        }

        public long getLastSeqIdCommunicatedExternally() {
            return lastSeqIdCommunicatedExternally;
        }


        public void permanentErrorOnSend() {
            stopServingSubscriber(this);
        }

        public void transientErrorOnSend() {
            retryErroredSubscriberAfterDelay(this);
        }

        /**
         * ===============================================================
         * {@link DeliveryManagerRequest} methods
         */
        public void performRequest() {

            // Put this subscriber in the channel to subscriber mapping
            ActiveSubscriberState prevSubscriber = subscriberStates.put(new TopicSubscriber(topic, subscriberId), this);

            if (prevSubscriber != null) {
                stopServingSubscriber(prevSubscriber);
            }

            lastSeqIdCommunicatedExternally = lastLocalSeqIdDelivered;
            addDeliveryPtr(this, lastLocalSeqIdDelivered);

            deliverNextMessage();
        };

        @Override
        public String toString() {
            StringBuilder sb = new StringBuilder();
            sb.append("Topic: ");
            sb.append(topic.toStringUtf8());
            sb.append("DeliveryPtr: ");
            sb.append(lastLocalSeqIdDelivered);
            return sb.toString();

        }
    }

    protected class StopServingSubscriber implements DeliveryManagerRequest {
        ActiveSubscriberState subscriber;

        public StopServingSubscriber(ActiveSubscriberState subscriber) {
            this.subscriber = subscriber;
        }

        @Override
        public void performRequest() {

            // This will automatically stop delivery, and disconnect the channel
            subscriber.setNotConnected();

            // if the subscriber has moved on, a move request for its delivery
            // pointer must be pending in the request queue. Note that the
            // subscriber first changes its delivery pointer and then submits a
            // request to move so this works.
            removeDeliveryPtr(subscriber, subscriber.getLastSeqIdCommunicatedExternally(), //
                              // isAbsenceOk=
                              true,
                              // pruneTopic=
                              true);
        }

    }

    protected class DeliveryPtrMove implements DeliveryManagerRequest {

        ActiveSubscriberState subscriber;
        Long oldSeqId;
        Long newSeqId;

        public DeliveryPtrMove(ActiveSubscriberState subscriber, Long oldSeqId, Long newSeqId) {
            this.subscriber = subscriber;
            this.oldSeqId = oldSeqId;
            this.newSeqId = newSeqId;
        }

        @Override
        public void performRequest() {
            ByteString topic = subscriber.getTopic();
            long prevMinSeqId = getMinimumSeqId(topic);

            if (subscriber.isConnected()) {
                removeDeliveryPtr(subscriber, oldSeqId, //
                                  // isAbsenceOk=
                                  false,
                                  // pruneTopic=
                                  false);

                addDeliveryPtr(subscriber, newSeqId);
            } else {
                removeDeliveryPtr(subscriber, oldSeqId, //
                                  // isAbsenceOk=
                                  true,
                                  // pruneTopic=
                                  true);
            }

            long nowMinSeqId = getMinimumSeqId(topic);

            if (nowMinSeqId > prevMinSeqId) {
                persistenceMgr.deliveredUntil(topic, nowMinSeqId);
            }
        }
    }

    protected class ShutdownDeliveryManagerRequest implements DeliveryManagerRequest {
        // This is a simple type of Request we will enqueue when the
        // PubSubServer is shut down and we want to stop the DeliveryManager
        // thread.
        public void performRequest() {
            keepRunning = false;
        }
    }

    /**
     * ====================================================================
     *
     * Dumb factories for our map methods
     */
    protected static class TreeMapLongToSetSubscriberFactory implements
        Factory<SortedMap<Long, Set<ActiveSubscriberState>>> {
        static TreeMapLongToSetSubscriberFactory instance = new TreeMapLongToSetSubscriberFactory();

        @Override
        public SortedMap<Long, Set<ActiveSubscriberState>> newInstance() {
            return new TreeMap<Long, Set<ActiveSubscriberState>>();
        }
    }

    protected static class HashMapSubscriberFactory implements Factory<Set<ActiveSubscriberState>> {
        static HashMapSubscriberFactory instance = new HashMapSubscriberFactory();

        @Override
        public Set<ActiveSubscriberState> newInstance() {
            return new HashSet<ActiveSubscriberState>();
        }
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/handlers/BaseHandler.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.handlers;

import org.jboss.netty.channel.Channel;

import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.ServerNotResponsibleForTopicException;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;
import org.apache.hedwig.protoextensions.PubSubResponseUtils;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.topics.TopicManager;
import org.apache.hedwig.util.Callback;
import org.apache.hedwig.util.HedwigSocketAddress;

public abstract class BaseHandler implements Handler {

    protected TopicManager topicMgr;
    protected ServerConfiguration cfg;

    protected BaseHandler(TopicManager tm, ServerConfiguration cfg) {
        this.topicMgr = tm;
        this.cfg = cfg;
    }


    public void handleRequest(final PubSubRequest request, final Channel channel) {
        topicMgr.getOwner(request.getTopic(), request.getShouldClaim(),
        new Callback<HedwigSocketAddress>() {
            @Override
            public void operationFailed(Object ctx, PubSubException exception) {
                channel.write(PubSubResponseUtils.getResponseForException(exception, request.getTxnId()));
            }

            @Override
            public void operationFinished(Object ctx, HedwigSocketAddress owner) {
                if (!owner.equals(cfg.getServerAddr())) {
                    channel.write(PubSubResponseUtils.getResponseForException(
                                      new ServerNotResponsibleForTopicException(owner.toString()), request.getTxnId()));
                    return;
                }
                handleRequestAtOwner(request, channel);
            }
        }, null);
    }

    public abstract void handleRequestAtOwner(PubSubRequest request, Channel channel);

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/handlers/ChannelDisconnectListener.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.handlers;

import org.jboss.netty.channel.Channel;

public interface ChannelDisconnectListener {

    /**
     * Act on a particular channel being disconnected
     * @param channel
     */
    public void channelDisconnected(Channel channel);
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/handlers/ConsumeHandler.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.handlers;

import org.jboss.netty.channel.Channel;

import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.netty.UmbrellaHandler;
import org.apache.hedwig.server.subscriptions.SubscriptionManager;
import org.apache.hedwig.server.topics.TopicManager;
import org.apache.hedwig.util.Callback;

public class ConsumeHandler extends BaseHandler {

    SubscriptionManager sm;
    Callback<Void> noopCallback = new NoopCallback<Void>();

    class NoopCallback<T> implements Callback<T> {
        @Override
        public void operationFailed(Object ctx, PubSubException exception) {
        }

        public void operationFinished(Object ctx, T resultOfOperation) {
        };
    }

    @Override
    public void handleRequestAtOwner(PubSubRequest request, Channel channel) {
        if (!request.hasConsumeRequest()) {
            UmbrellaHandler.sendErrorResponseToMalformedRequest(channel, request.getTxnId(),
                    "Missing consume request data");
            return;
        }

        ConsumeRequest consumeRequest = request.getConsumeRequest();

        sm.setConsumeSeqIdForSubscriber(request.getTopic(), consumeRequest.getSubscriberId(),
                                        consumeRequest.getMsgId(), noopCallback, null);

    }

    public ConsumeHandler(TopicManager tm, SubscriptionManager sm, ServerConfiguration cfg) {
        super(tm, cfg);
        this.sm = sm;
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/handlers/Handler.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.handlers;

import org.jboss.netty.channel.Channel;

import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;

public interface Handler {

    /**
     * Handle a request synchronously or asynchronously. After handling the
     * request, the appropriate response should be written on the given channel
     *
     * @param request
     *            The request to handle
     *
     * @param channel
     *            The channel on which to write the response
     */
    public void handleRequest(final PubSubRequest request, final Channel channel);
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/handlers/PublishHandler.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.handlers;

import org.jboss.netty.channel.Channel;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;
import org.apache.hedwig.protoextensions.PubSubResponseUtils;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.netty.UmbrellaHandler;
import org.apache.hedwig.server.persistence.PersistRequest;
import org.apache.hedwig.server.persistence.PersistenceManager;
import org.apache.hedwig.server.topics.TopicManager;
import org.apache.hedwig.util.Callback;

public class PublishHandler extends BaseHandler {

    private PersistenceManager persistenceMgr;

    public PublishHandler(TopicManager topicMgr, PersistenceManager persistenceMgr, ServerConfiguration cfg) {
        super(topicMgr, cfg);
        this.persistenceMgr = persistenceMgr;
    }

    @Override
    public void handleRequestAtOwner(final PubSubRequest request, final Channel channel) {
        if (!request.hasPublishRequest()) {
            UmbrellaHandler.sendErrorResponseToMalformedRequest(channel, request.getTxnId(),
                    "Missing publish request data");
            return;
        }

        Message msgToSerialize = Message.newBuilder(request.getPublishRequest().getMsg()).setSrcRegion(
                                     cfg.getMyRegionByteString()).build();

        PersistRequest persistRequest = new PersistRequest(request.getTopic(), msgToSerialize,
        new Callback<Long>() {
            @Override
            public void operationFailed(Object ctx, PubSubException exception) {
                channel.write(PubSubResponseUtils.getResponseForException(exception, request.getTxnId()));
            }

            @Override
            public void operationFinished(Object ctx, Long resultOfOperation) {
                channel.write(PubSubResponseUtils.getSuccessResponse(request.getTxnId()));
            }
        }, null);

        persistenceMgr.persistMessage(persistRequest);
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/handlers/SubscribeHandler.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.handlers;

import java.util.concurrent.ConcurrentHashMap;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;
import org.jboss.netty.channel.ChannelFutureListener;

import com.google.protobuf.ByteString;
import org.apache.hedwig.client.data.TopicSubscriber;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.ServerNotResponsibleForTopicException;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;
import org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest;
import org.apache.hedwig.protoextensions.PubSubResponseUtils;
import org.apache.hedwig.protoextensions.SubscriptionStateUtils;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.delivery.ChannelEndPoint;
import org.apache.hedwig.server.delivery.DeliveryManager;
import org.apache.hedwig.server.netty.UmbrellaHandler;
import org.apache.hedwig.server.persistence.PersistenceManager;
import org.apache.hedwig.server.subscriptions.SubscriptionManager;
import org.apache.hedwig.server.subscriptions.TrueFilter;
import org.apache.hedwig.server.topics.TopicManager;
import org.apache.hedwig.util.Callback;

public class SubscribeHandler extends BaseHandler implements ChannelDisconnectListener {
    static Logger logger = LoggerFactory.getLogger(SubscribeHandler.class);

    private DeliveryManager deliveryMgr;
    private PersistenceManager persistenceMgr;
    private SubscriptionManager subMgr;
    ConcurrentHashMap<TopicSubscriber, Channel> sub2Channel;
    ConcurrentHashMap<Channel, TopicSubscriber> channel2sub;

    public SubscribeHandler(TopicManager topicMgr, DeliveryManager deliveryManager, PersistenceManager persistenceMgr,
                            SubscriptionManager subMgr, ServerConfiguration cfg) {
        super(topicMgr, cfg);
        this.deliveryMgr = deliveryManager;
        this.persistenceMgr = persistenceMgr;
        this.subMgr = subMgr;
        sub2Channel = new ConcurrentHashMap<TopicSubscriber, Channel>();
        channel2sub = new ConcurrentHashMap<Channel, TopicSubscriber>();
    }

    public void channelDisconnected(Channel channel) {
        // Evils of synchronized programming: there is a race between a channel
        // getting disconnected, and us adding it to the maps when a subscribe
        // succeeds
        synchronized (channel) {
            TopicSubscriber topicSub = channel2sub.remove(channel);
            if (topicSub != null) {
                sub2Channel.remove(topicSub);
            }
        }
    }

    @Override
    public void handleRequestAtOwner(final PubSubRequest request, final Channel channel) {

        if (!request.hasSubscribeRequest()) {
            UmbrellaHandler.sendErrorResponseToMalformedRequest(channel, request.getTxnId(),
                    "Missing subscribe request data");
            return;
        }

        final ByteString topic = request.getTopic();

        MessageSeqId seqId;
        try {
            seqId = persistenceMgr.getCurrentSeqIdForTopic(topic);
        } catch (ServerNotResponsibleForTopicException e) {
            channel.write(PubSubResponseUtils.getResponseForException(e, request.getTxnId())).addListener(
                ChannelFutureListener.CLOSE);
            return;
        }

        final SubscribeRequest subRequest = request.getSubscribeRequest();
        final ByteString subscriberId = subRequest.getSubscriberId();

        MessageSeqId lastSeqIdPublished = MessageSeqId.newBuilder(seqId).setLocalComponent(seqId.getLocalComponent()).build();

        subMgr.serveSubscribeRequest(topic, subRequest, lastSeqIdPublished, new Callback<MessageSeqId>() {

            @Override
            public void operationFailed(Object ctx, PubSubException exception) {
                channel.write(PubSubResponseUtils.getResponseForException(exception, request.getTxnId())).addListener(
                    ChannelFutureListener.CLOSE);
            }

            @Override
            public void operationFinished(Object ctx, MessageSeqId resultOfOperation) {

                TopicSubscriber topicSub = new TopicSubscriber(topic, subscriberId);

                // race with channel getting disconnected while we are adding it
                // to the 2 maps
                synchronized (channel) {
                    if (!channel.isConnected()) {
                        // channel got disconnected while we were processing the
                        // subscribe request,
                        // nothing much we can do in this case
                        return;
                    }

                    if (null != sub2Channel.putIfAbsent(topicSub, channel)) {
                        // there was another channel mapped to this sub
                        PubSubException pse = new PubSubException.TopicBusyException(
                            "subscription for this topic, subscriberId is already being served on a different channel");
                        channel.write(PubSubResponseUtils.getResponseForException(pse, request.getTxnId()))
                        .addListener(ChannelFutureListener.CLOSE);
                        return;
                    } else {
                        // channel2sub is just a cache, so we can add to it
                        // without synchronization
                        channel2sub.put(channel, topicSub);
                    }
                }
                // First write success and then tell the delivery manager,
                // otherwise the first message might go out before the response
                // to the subscribe
                channel.write(PubSubResponseUtils.getSuccessResponse(request.getTxnId()));

                // want to start 1 ahead of the consume ptr
                MessageSeqId seqIdToStartFrom = MessageSeqId.newBuilder(resultOfOperation).setLocalComponent(
                                                    resultOfOperation.getLocalComponent() + 1).build();
                deliveryMgr.startServingSubscription(topic, subscriberId, seqIdToStartFrom,
                                                     new ChannelEndPoint(channel), TrueFilter.instance(), SubscriptionStateUtils
                                                     .isHubSubscriber(subRequest.getSubscriberId()));
            }
        }, null);

    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/handlers/UnsubscribeHandler.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.handlers;

import org.jboss.netty.channel.Channel;
import com.google.protobuf.ByteString;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;
import org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest;
import org.apache.hedwig.protoextensions.PubSubResponseUtils;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.delivery.DeliveryManager;
import org.apache.hedwig.server.netty.UmbrellaHandler;
import org.apache.hedwig.server.subscriptions.SubscriptionManager;
import org.apache.hedwig.server.topics.TopicManager;
import org.apache.hedwig.util.Callback;

public class UnsubscribeHandler extends BaseHandler {
    SubscriptionManager subMgr;
    DeliveryManager deliveryMgr;

    public UnsubscribeHandler(TopicManager tm, ServerConfiguration cfg, SubscriptionManager subMgr,
                              DeliveryManager deliveryMgr) {
        super(tm, cfg);
        this.subMgr = subMgr;
        this.deliveryMgr = deliveryMgr;
    }

    @Override
    public void handleRequestAtOwner(final PubSubRequest request, final Channel channel) {
        if (!request.hasUnsubscribeRequest()) {
            UmbrellaHandler.sendErrorResponseToMalformedRequest(channel, request.getTxnId(),
                    "Missing unsubscribe request data");
            return;
        }

        final UnsubscribeRequest unsubRequest = request.getUnsubscribeRequest();
        final ByteString topic = request.getTopic();
        final ByteString subscriberId = unsubRequest.getSubscriberId();

        subMgr.unsubscribe(topic, subscriberId, new Callback<Void>() {
            @Override
            public void operationFailed(Object ctx, PubSubException exception) {
                channel.write(PubSubResponseUtils.getResponseForException(exception, request.getTxnId()));
            }

            @Override
            public void operationFinished(Object ctx, Void resultOfOperation) {
                deliveryMgr.stopServingSubscriber(topic, subscriberId);
                channel.write(PubSubResponseUtils.getSuccessResponse(request.getTxnId()));

            }
        }, null);

    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/netty/PubSubServer.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.netty;

import java.io.File;
import java.io.IOException;
import java.net.InetSocketAddress;
import java.net.MalformedURLException;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.SynchronousQueue;
import java.util.concurrent.TimeUnit;

import org.apache.bookkeeper.conf.ClientConfiguration;
import org.apache.bookkeeper.client.BookKeeper;
import org.apache.bookkeeper.client.BKException;
import org.apache.commons.configuration.ConfigurationException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.WatchedEvent;
import org.apache.zookeeper.Watcher;
import org.apache.zookeeper.ZooKeeper;
import org.jboss.netty.bootstrap.ServerBootstrap;
import org.jboss.netty.channel.group.ChannelGroup;
import org.jboss.netty.channel.group.DefaultChannelGroup;
import org.jboss.netty.channel.socket.ClientSocketChannelFactory;
import org.jboss.netty.channel.socket.ServerSocketChannelFactory;
import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory;
import org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory;
import org.jboss.netty.logging.InternalLoggerFactory;
import org.jboss.netty.logging.Log4JLoggerFactory;

import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.protocol.PubSubProtocol.OperationType;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.common.TerminateJVMExceptionHandler;
import org.apache.hedwig.server.delivery.DeliveryManager;
import org.apache.hedwig.server.delivery.FIFODeliveryManager;
import org.apache.hedwig.server.handlers.ConsumeHandler;
import org.apache.hedwig.server.handlers.Handler;
import org.apache.hedwig.server.handlers.PublishHandler;
import org.apache.hedwig.server.handlers.SubscribeHandler;
import org.apache.hedwig.server.handlers.UnsubscribeHandler;
import org.apache.hedwig.server.persistence.BookkeeperPersistenceManager;
import org.apache.hedwig.server.persistence.LocalDBPersistenceManager;
import org.apache.hedwig.server.persistence.PersistenceManager;
import org.apache.hedwig.server.persistence.PersistenceManagerWithRangeScan;
import org.apache.hedwig.server.persistence.ReadAheadCache;
import org.apache.hedwig.server.regions.HedwigHubClientFactory;
import org.apache.hedwig.server.regions.RegionManager;
import org.apache.hedwig.server.ssl.SslServerContextFactory;
import org.apache.hedwig.server.subscriptions.AbstractSubscriptionManager;
import org.apache.hedwig.server.subscriptions.InMemorySubscriptionManager;
import org.apache.hedwig.server.subscriptions.SubscriptionManager;
import org.apache.hedwig.server.subscriptions.ZkSubscriptionManager;
import org.apache.hedwig.server.topics.TopicManager;
import org.apache.hedwig.server.topics.TrivialOwnAllTopicManager;
import org.apache.hedwig.server.topics.ZkTopicManager;
import org.apache.hedwig.util.ConcurrencyUtils;
import org.apache.hedwig.util.Either;
import org.apache.hedwig.zookeeper.SafeAsyncCallback;

public class PubSubServer {

    static Logger logger = LoggerFactory.getLogger(PubSubServer.class);

    // Netty related variables
    ServerSocketChannelFactory serverChannelFactory;
    ClientSocketChannelFactory clientChannelFactory;
    ServerConfiguration conf;
    ChannelGroup allChannels;

    // Manager components that make up the PubSubServer
    PersistenceManager pm;
    DeliveryManager dm;
    TopicManager tm;
    SubscriptionManager sm;
    RegionManager rm;

    ZooKeeper zk; // null if we are in standalone mode
    BookKeeper bk; // null if we are in standalone mode

    // we use this to prevent long stack chains from building up in callbacks
    ScheduledExecutorService scheduler;

    protected PersistenceManager instantiatePersistenceManager(TopicManager topicMgr) throws IOException,
        InterruptedException {

        PersistenceManagerWithRangeScan underlyingPM;

        if (conf.isStandalone()) {

            underlyingPM = LocalDBPersistenceManager.instance();

        } else {
            try {
                ClientConfiguration bkConf = new ClientConfiguration();
                bkConf.addConfiguration(conf.getConf());
                bk = new BookKeeper(bkConf, zk, clientChannelFactory);
            } catch (KeeperException e) {
                logger.error("Could not instantiate bookkeeper client", e);
                throw new IOException(e);
            }
            underlyingPM = new BookkeeperPersistenceManager(bk, zk, topicMgr, conf, scheduler);

        }

        PersistenceManager pm = underlyingPM;

        if (conf.getReadAheadEnabled()) {
            pm = new ReadAheadCache(underlyingPM, conf).start();
        }

        return pm;
    }

    protected SubscriptionManager instantiateSubscriptionManager(TopicManager tm, PersistenceManager pm) {
        if (conf.isStandalone()) {
            return new InMemorySubscriptionManager(tm, pm, conf, scheduler);
        } else {
            return new ZkSubscriptionManager(zk, tm, pm, conf, scheduler);
        }

    }

    protected RegionManager instantiateRegionManager(PersistenceManager pm, ScheduledExecutorService scheduler) {
        return new RegionManager(pm, conf, zk, scheduler, new HedwigHubClientFactory(conf, clientChannelFactory));
    }

    protected void instantiateZookeeperClient() throws Exception {
        if (!conf.isStandalone()) {
            final CountDownLatch signalZkReady = new CountDownLatch(1);

            zk = new ZooKeeper(conf.getZkHost(), conf.getZkTimeout(), new Watcher() {
                @Override
                public void process(WatchedEvent event) {
                    if(Event.KeeperState.SyncConnected.equals(event.getState())) {
                        signalZkReady.countDown();
                    }
                }
            });
            // wait until connection is effective
            if (!signalZkReady.await(conf.getZkTimeout()*2, TimeUnit.MILLISECONDS)) {
                logger.error("Could not establish connection with ZooKeeper after zk_timeout*2 = " +
                             conf.getZkTimeout()*2 + " ms. (Default value for zk_timeout is 2000).");
                throw new Exception("Could not establish connection with ZooKeeper after zk_timeout*2 = " +
                                    conf.getZkTimeout()*2 + " ms. (Default value for zk_timeout is 2000).");
            }
        }
    }

    protected TopicManager instantiateTopicManager() throws IOException {
        TopicManager tm;

        if (conf.isStandalone()) {
            tm = new TrivialOwnAllTopicManager(conf, scheduler);
        } else {
            try {
                tm = new ZkTopicManager(zk, conf, scheduler);
            } catch (PubSubException e) {
                logger.error("Could not instantiate zk-topic manager", e);
                throw new IOException(e);
            }
        }
        return tm;
    }

    protected Map<OperationType, Handler> initializeNettyHandlers(TopicManager tm, DeliveryManager dm,
            PersistenceManager pm, SubscriptionManager sm) {
        Map<OperationType, Handler> handlers = new HashMap<OperationType, Handler>();
        handlers.put(OperationType.PUBLISH, new PublishHandler(tm, pm, conf));
        handlers.put(OperationType.SUBSCRIBE, new SubscribeHandler(tm, dm, pm, sm, conf));
        handlers.put(OperationType.UNSUBSCRIBE, new UnsubscribeHandler(tm, conf, sm, dm));
        handlers.put(OperationType.CONSUME, new ConsumeHandler(tm, sm, conf));
        handlers = Collections.unmodifiableMap(handlers);
        return handlers;
    }

    protected void initializeNetty(SslServerContextFactory sslFactory, Map<OperationType, Handler> handlers) {
        boolean isSSLEnabled = (sslFactory != null) ? true : false;
        InternalLoggerFactory.setDefaultFactory(new Log4JLoggerFactory());
        ServerBootstrap bootstrap = new ServerBootstrap(serverChannelFactory);
        UmbrellaHandler umbrellaHandler = new UmbrellaHandler(allChannels, handlers, isSSLEnabled);
        PubSubServerPipelineFactory pipeline = new PubSubServerPipelineFactory(umbrellaHandler, sslFactory, conf
                .getMaximumMessageSize());

        bootstrap.setPipelineFactory(pipeline);
        bootstrap.setOption("child.tcpNoDelay", true);
        bootstrap.setOption("child.keepAlive", true);
        bootstrap.setOption("reuseAddress", true);

        // Bind and start to accept incoming connections.
        allChannels.add(bootstrap.bind(isSSLEnabled ? new InetSocketAddress(conf.getSSLServerPort())
                                       : new InetSocketAddress(conf.getServerPort())));
        logger.info("Going into receive loop");
    }

    public void shutdown() {
        // TODO: tell bk to close logs

        // Shutdown the ZooKeeper and BookKeeper clients only if we are
        // not in stand-alone mode.
        try {
            if (zk != null)
                zk.close();
            if (bk != null)
                bk.close();
        } catch (InterruptedException e) {
            logger.error("Error while closing ZooKeeper client!");
        } catch (BKException bke) {
            logger.error("Error while closing BookKeeper client");
        }

        // Stop the RegionManager.
        rm.stop();

        // Stop the DeliveryManager and ReadAheadCache threads (if
        // applicable).
        // TODO: It'd be cleaner and more general to modify the interfaces to
        // include a stop method. If the specific implementation starts threads,
        // then the stop method should take care of that clean up.
        if (pm instanceof ReadAheadCache) {
            ((ReadAheadCache) pm).stop();
        }
        if (dm instanceof FIFODeliveryManager) {
            ((FIFODeliveryManager) dm).stop();
        }

        // Stop the SubscriptionManager if needed.
        if (sm instanceof AbstractSubscriptionManager) {
            ((AbstractSubscriptionManager) sm).stop();
        }

        // Close and release the Netty channels and resources
        allChannels.close().awaitUninterruptibly();
        serverChannelFactory.releaseExternalResources();
        clientChannelFactory.releaseExternalResources();
        scheduler.shutdown();
    }

    /**
     * Starts the hedwig server on the given port
     *
     * @param port
     * @throws ConfigurationException
     *             if there is something wrong with the given configuration
     * @throws IOException
     * @throws InterruptedException
     * @throws ConfigurationException
     */
    public PubSubServer(final ServerConfiguration conf, final Thread.UncaughtExceptionHandler exceptionHandler)
            throws Exception {

        // First validate the conf
        this.conf = conf;
        conf.validate();

        // We need a custom thread group, so that we can override the uncaught
        // exception method
        ThreadGroup tg = new ThreadGroup("hedwig") {
            @Override
            public void uncaughtException(Thread t, Throwable e) {
                exceptionHandler.uncaughtException(t, e);
            }
        };
        // ZooKeeper threads register their own handler. But if some work that
        // we do in ZK threads throws an exception, we want our handler to be
        // called, not theirs.
        SafeAsyncCallback.setUncaughtExceptionHandler(exceptionHandler);

        final SynchronousQueue<Either<Object, Exception>> queue = new SynchronousQueue<Either<Object, Exception>>();

        new Thread(tg, new Runnable() {
            @Override
            public void run() {
                try {
                    // Since zk is needed by almost everyone,try to see if we
                    // need that first
                    scheduler = Executors.newSingleThreadScheduledExecutor();
                    serverChannelFactory = new NioServerSocketChannelFactory(Executors.newCachedThreadPool(), Executors
                            .newCachedThreadPool());
                    clientChannelFactory = new NioClientSocketChannelFactory(Executors.newCachedThreadPool(), Executors
                            .newCachedThreadPool());

                    instantiateZookeeperClient();
                    tm = instantiateTopicManager();
                    pm = instantiatePersistenceManager(tm);
                    dm = new FIFODeliveryManager(pm, conf);
                    sm = instantiateSubscriptionManager(tm, pm);
                    rm = instantiateRegionManager(pm, scheduler);
                    sm.addListener(rm);

                    allChannels = new DefaultChannelGroup("hedwig");
                    // Initialize the Netty Handlers (used by the
                    // UmbrellaHandler) once so they can be shared by
                    // both the SSL and non-SSL channels.
                    Map<OperationType, Handler> handlers = initializeNettyHandlers(tm, dm, pm, sm);
                    // Initialize Netty for the regular non-SSL channels
                    initializeNetty(null, handlers);
                    if (conf.isSSLEnabled()) {
                        initializeNetty(new SslServerContextFactory(conf), handlers);
                    }
                } catch (Exception e) {
                    ConcurrencyUtils.put(queue, Either.right(e));
                    return;
                }

                ConcurrencyUtils.put(queue, Either.of(new Object(), (Exception) null));
            }

        }).start();

        Either<Object, Exception> either = ConcurrencyUtils.take(queue);
        if (either.left() == null) {
            throw either.right();
        }
    }

    public PubSubServer(ServerConfiguration conf) throws Exception {
        this(conf, new TerminateJVMExceptionHandler());
    }

    /**
     *
     * @param msg
     * @param rc
     *            : code to exit with
     */
    public static void errorMsgAndExit(String msg, Throwable t, int rc) {
        logger.error(msg, t);
        System.err.println(msg);
        System.exit(rc);
    }

    public final static int RC_INVALID_CONF_FILE = 1;
    public final static int RC_MISCONFIGURED = 2;
    public final static int RC_OTHER = 3;

    /**
     * @param args
     */
    public static void main(String[] args) {

        logger.info("Attempting to start Hedwig");
        ServerConfiguration conf = new ServerConfiguration();
        if (args.length > 0) {
            String confFile = args[0];
            try {
                conf.loadConf(new File(confFile).toURI().toURL());
            } catch (MalformedURLException e) {
                String msg = "Could not open configuration file: " + confFile;
                errorMsgAndExit(msg, e, RC_INVALID_CONF_FILE);
            } catch (ConfigurationException e) {
                String msg = "Malformed configuration file: " + confFile;
                errorMsgAndExit(msg, e, RC_MISCONFIGURED);
            }
            logger.info("Using configuration file " + confFile);
        }
        try {
            new PubSubServer(conf);
        } catch (Throwable t) {
            errorMsgAndExit("Error during startup", t, RC_OTHER);
        }
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/netty/PubSubServerPipelineFactory.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.netty;

import org.jboss.netty.channel.ChannelPipeline;
import org.jboss.netty.channel.ChannelPipelineFactory;
import org.jboss.netty.channel.Channels;
import org.jboss.netty.handler.codec.frame.LengthFieldBasedFrameDecoder;
import org.jboss.netty.handler.codec.frame.LengthFieldPrepender;
import org.jboss.netty.handler.codec.protobuf.ProtobufDecoder;
import org.jboss.netty.handler.codec.protobuf.ProtobufEncoder;
import org.jboss.netty.handler.ssl.SslHandler;

import org.apache.hedwig.protocol.PubSubProtocol;
import org.apache.hedwig.server.ssl.SslServerContextFactory;

public class PubSubServerPipelineFactory implements ChannelPipelineFactory {

    // TODO: make these conf settings
    final static int MAX_WORKER_THREADS = 32;
    final static int MAX_CHANNEL_MEMORY_SIZE = 10 * 1024 * 1024;
    final static int MAX_TOTAL_MEMORY_SIZE = 100 * 1024 * 1024;

    private UmbrellaHandler uh;
    private SslServerContextFactory sslFactory;
    private int maxMessageSize;

    /**
     *
     * @param uh
     * @param sslFactory
     *            may be null if ssl is disabled
     * @param cfg
     */
    public PubSubServerPipelineFactory(UmbrellaHandler uh, SslServerContextFactory sslFactory, int maxMessageSize) {
        this.uh = uh;
        this.sslFactory = sslFactory;
        this.maxMessageSize = maxMessageSize;
    }

    public ChannelPipeline getPipeline() throws Exception {
        ChannelPipeline pipeline = Channels.pipeline();
        if (sslFactory != null) {
            pipeline.addLast("ssl", new SslHandler(sslFactory.getEngine()));
        }
        pipeline.addLast("lengthbaseddecoder",
                         new LengthFieldBasedFrameDecoder(maxMessageSize, 0, 4, 0, 4));
        pipeline.addLast("lengthprepender", new LengthFieldPrepender(4));

        pipeline.addLast("protobufdecoder", new ProtobufDecoder(PubSubProtocol.PubSubRequest.getDefaultInstance()));
        pipeline.addLast("protobufencoder", new ProtobufEncoder());

        // pipeline.addLast("executor", new ExecutionHandler(
        // new OrderedMemoryAwareThreadPoolExecutor(MAX_WORKER_THREADS,
        // MAX_CHANNEL_MEMORY_SIZE, MAX_TOTAL_MEMORY_SIZE)));
        //
        // Dependency injection.
        pipeline.addLast("umbrellahandler", uh);
        return pipeline;
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/netty/UmbrellaHandler.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.netty;

import java.io.IOException;
import java.util.Map;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;
import org.jboss.netty.channel.ChannelFuture;
import org.jboss.netty.channel.ChannelFutureListener;
import org.jboss.netty.channel.ChannelHandlerContext;
import org.jboss.netty.channel.ChannelPipelineCoverage;
import org.jboss.netty.channel.ChannelStateEvent;
import org.jboss.netty.channel.ExceptionEvent;
import org.jboss.netty.channel.MessageEvent;
import org.jboss.netty.channel.SimpleChannelHandler;
import org.jboss.netty.channel.group.ChannelGroup;
import org.jboss.netty.handler.codec.frame.CorruptedFrameException;
import org.jboss.netty.handler.codec.frame.TooLongFrameException;
import org.jboss.netty.handler.ssl.SslHandler;

import org.apache.hedwig.exceptions.PubSubException.MalformedRequestException;
import org.apache.hedwig.protocol.PubSubProtocol;
import org.apache.hedwig.protocol.PubSubProtocol.OperationType;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse;
import org.apache.hedwig.protoextensions.PubSubResponseUtils;
import org.apache.hedwig.server.handlers.ChannelDisconnectListener;
import org.apache.hedwig.server.handlers.Handler;

@ChannelPipelineCoverage("all")
public class UmbrellaHandler extends SimpleChannelHandler {
    static Logger logger = LoggerFactory.getLogger(UmbrellaHandler.class);

    private Map<OperationType, Handler> handlers;
    private ChannelGroup allChannels;
    private ChannelDisconnectListener subscribeHandler;
    private boolean isSSLEnabled = false;

    public UmbrellaHandler(ChannelGroup allChannels, Map<OperationType, Handler> handlers,
                           boolean isSSLEnabled) {
        this.allChannels = allChannels;
        this.isSSLEnabled = isSSLEnabled;
        this.handlers = handlers;
        subscribeHandler = (ChannelDisconnectListener) handlers.get(OperationType.SUBSCRIBE);
    }

    @Override
    public void exceptionCaught(ChannelHandlerContext ctx, ExceptionEvent e) throws Exception {
        Throwable throwable = e.getCause();

        // Add here if there are more exceptions we need to be able to tolerate.
        // 1. IOException may be thrown when a channel is forcefully closed by
        // the other end, or by the ProtobufDecoder when an invalid protobuf is
        // received
        // 2. TooLongFrameException is thrown by the LengthBasedDecoder if it
        // receives a packet that is too big
        // 3. CorruptedFramException is thrown by the LengthBasedDecoder when
        // the length is negative etc.
        if (throwable instanceof IOException || throwable instanceof TooLongFrameException
                || throwable instanceof CorruptedFrameException) {
            e.getChannel().close();
            if (logger.isDebugEnabled()) {
                logger.debug("Uncaught exception", throwable);
            }
        } else {
            // call our uncaught exception handler, which might decide to
            // shutdown the system
            Thread thread = Thread.currentThread();
            thread.getUncaughtExceptionHandler().uncaughtException(thread, throwable);
        }

    }

    @Override
    public void channelOpen(ChannelHandlerContext ctx, ChannelStateEvent e) throws Exception {
        // If SSL is NOT enabled, then we can add this channel to the
        // ChannelGroup. Otherwise, that is done when the channel is connected
        // and the SSL handshake has completed successfully.
        if (!isSSLEnabled) {
            allChannels.add(ctx.getChannel());
        }
    }

    @Override
    public void channelConnected(ChannelHandlerContext ctx, ChannelStateEvent e) throws Exception {
        if (isSSLEnabled) {
            ctx.getPipeline().get(SslHandler.class).handshake(e.getChannel()).addListener(new ChannelFutureListener() {
                public void operationComplete(ChannelFuture future) throws Exception {
                    if (future.isSuccess()) {
                        if (logger.isDebugEnabled()) {
                            logger.debug("SSL handshake has completed successfully!");
                        }
                        allChannels.add(future.getChannel());
                    } else {
                        future.getChannel().close();
                    }
                }
            });
        }
    }

    @Override
    public void channelDisconnected(ChannelHandlerContext ctx, ChannelStateEvent e) throws Exception {
        Channel channel = ctx.getChannel();
        // subscribe handler needs to know about channel disconnects
        subscribeHandler.channelDisconnected(channel);
        channel.close();
    }

    public static void sendErrorResponseToMalformedRequest(Channel channel, long txnId, String msg) {
        if (logger.isDebugEnabled()) {
            logger.debug("Malformed request from " + channel.getRemoteAddress() + " msg, = " + msg);
        }
        MalformedRequestException mre = new MalformedRequestException(msg);
        PubSubResponse response = PubSubResponseUtils.getResponseForException(mre, txnId);
        channel.write(response);
    }

    @Override
    public void messageReceived(ChannelHandlerContext ctx, MessageEvent e) throws Exception {

        if (!(e.getMessage() instanceof PubSubProtocol.PubSubRequest)) {
            ctx.sendUpstream(e);
            return;
        }

        PubSubProtocol.PubSubRequest request = (PubSubProtocol.PubSubRequest) e.getMessage();

        Handler handler = handlers.get(request.getType());
        Channel channel = ctx.getChannel();
        long txnId = request.getTxnId();

        if (handler == null) {
            sendErrorResponseToMalformedRequest(channel, txnId, "Request type " + request.getType().getNumber()
                                                + " unknown");
            return;
        }

        handler.handleRequest(request, channel);
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/persistence/BookkeeperPersistenceManager.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.persistence;

import java.io.IOException;
import java.util.Enumeration;
import java.util.Iterator;
import java.util.Map;
import java.util.TreeMap;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ScheduledExecutorService;

import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.client.BookKeeper;
import org.apache.bookkeeper.client.LedgerEntry;
import org.apache.bookkeeper.client.LedgerHandle;
import org.apache.bookkeeper.client.BookKeeper.DigestType;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.ZooDefs;
import org.apache.zookeeper.ZooKeeper;
import org.apache.zookeeper.KeeperException.Code;
import org.apache.zookeeper.data.Stat;

import com.google.protobuf.ByteString;
import com.google.protobuf.InvalidProtocolBufferException;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.ServerNotResponsibleForTopicException;
import org.apache.hedwig.protocol.PubSubProtocol.LedgerRange;
import org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protoextensions.MessageIdUtils;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.common.TopicOpQueuer;
import org.apache.hedwig.server.common.UnexpectedError;
import org.apache.hedwig.server.persistence.ScanCallback.ReasonForFinish;
import org.apache.hedwig.server.topics.TopicManager;
import org.apache.hedwig.server.topics.TopicOwnershipChangeListener;
import org.apache.hedwig.util.Callback;
import org.apache.hedwig.zookeeper.SafeAsynBKCallback;
import org.apache.hedwig.zookeeper.SafeAsyncZKCallback;
import org.apache.hedwig.zookeeper.ZkUtils;

/**
 * This persistence manager uses zookeeper and bookkeeper to store messages.
 *
 * Information about topics are stored in zookeeper with a znode named after the
 * topic that contains an ASCII encoded list with records of the following form:
 *
 * <pre>
 * startSeqId(included)\tledgerId\n
 * </pre>
 *
 */

public class BookkeeperPersistenceManager implements PersistenceManagerWithRangeScan, TopicOwnershipChangeListener {
    static Logger logger = LoggerFactory.getLogger(BookkeeperPersistenceManager.class);
    static byte[] passwd = "sillysecret".getBytes();
    private BookKeeper bk;
    private ZooKeeper zk;
    private ServerConfiguration cfg;

    static class InMemoryLedgerRange {
        LedgerRange range;
        long startSeqIdIncluded; // included, for the very first ledger, this
        // value is 1
        LedgerHandle handle;

        public InMemoryLedgerRange(LedgerRange range, long startSeqId, LedgerHandle handle) {
            this.range = range;
            this.startSeqIdIncluded = startSeqId;
            this.handle = handle;
        }

        public InMemoryLedgerRange(LedgerRange range, long startSeqId) {
            this(range, startSeqId, null);
        }

    }

    static class TopicInfo {
        /**
         * stores the last message-seq-id vector that has been pushed to BK for
         * persistence (but not necessarily acked yet by BK)
         *
         */
        MessageSeqId lastSeqIdPushed;

        /**
         * stores the last message-id that has been acked by BK. This number is
         * basically used for limiting scans to not read past what has been
         * persisted by BK
         */
        long lastEntryIdAckedInCurrentLedger = -1; // because BK ledgers starts
        // at 0

        /**
         * stores a sorted structure of the ledgers for a topic, mapping from
         * the endSeqIdIncluded to the ledger info. This structure does not
         * include the current ledger
         */
        TreeMap<Long, InMemoryLedgerRange> ledgerRanges = new TreeMap<Long, InMemoryLedgerRange>();

        /**
         * This is the handle of the current ledger that is being used to write
         * messages
         */
        InMemoryLedgerRange currentLedgerRange;

    }

    Map<ByteString, TopicInfo> topicInfos = new ConcurrentHashMap<ByteString, TopicInfo>();

    TopicOpQueuer queuer;

    /**
     * Instantiates a BookKeeperPersistence manager.
     *
     * @param bk
     *            a reference to bookkeeper to use.
     * @param zk
     *            a zookeeper handle to use.
     * @param zkPrefix
     *            the zookeeper subtree that stores the topic to ledger
     *            information. if this prefix does not exist, it will be
     *            created.
     */
    public BookkeeperPersistenceManager(BookKeeper bk, ZooKeeper zk, TopicManager tm, ServerConfiguration cfg,
                                        ScheduledExecutorService executor) {
        this.bk = bk;
        this.zk = zk;
        this.cfg = cfg;
        queuer = new TopicOpQueuer(executor);
        tm.addTopicOwnershipChangeListener(this);
    }

    class RangeScanOp extends TopicOpQueuer.SynchronousOp {
        RangeScanRequest request;
        int numMessagesRead = 0;
        long totalSizeRead = 0;
        TopicInfo topicInfo;

        public RangeScanOp(RangeScanRequest request) {
            queuer.super(request.topic);
            this.request = request;
        }

        @Override
        protected void runInternal() {
            topicInfo = topicInfos.get(topic);

            if (topicInfo == null) {
                request.callback.scanFailed(request.ctx, new PubSubException.ServerNotResponsibleForTopicException(""));
                return;
            }

            startReadingFrom(request.startSeqId);

        }

        protected void read(final InMemoryLedgerRange imlr, final long startSeqId, final long endSeqId) {

            if (imlr.handle == null) {

                bk.asyncOpenLedger(imlr.range.getLedgerId(), DigestType.CRC32, passwd,
                new SafeAsynBKCallback.OpenCallback() {
                    @Override
                    public void safeOpenComplete(int rc, LedgerHandle ledgerHandle, Object ctx) {
                        if (rc == BKException.Code.OK) {
                            imlr.handle = ledgerHandle;
                            read(imlr, startSeqId, endSeqId);
                            return;
                        }
                        BKException bke = BKException.create(rc);
                        logger.error("Could not open ledger: " + imlr.range.getLedgerId() + " for topic: "
                                     + topic);
                        request.callback.scanFailed(ctx, new PubSubException.ServiceDownException(bke));
                        return;
                    }
                }, request.ctx);
                return;
            }

            // ledger handle is not null, we can read from it
            long correctedEndSeqId = Math.min(startSeqId + request.messageLimit - numMessagesRead - 1, endSeqId);

            if (logger.isDebugEnabled()) {
                logger.debug("Issuing a bk read for ledger: " + imlr.handle.getId() + " from entry-id: "
                             + (startSeqId - imlr.startSeqIdIncluded) + " to entry-id: "
                             + (correctedEndSeqId - imlr.startSeqIdIncluded));
            }

            imlr.handle.asyncReadEntries(startSeqId - imlr.startSeqIdIncluded, correctedEndSeqId
            - imlr.startSeqIdIncluded, new SafeAsynBKCallback.ReadCallback() {

                long expectedEntryId = startSeqId - imlr.startSeqIdIncluded;

                @Override
                public void safeReadComplete(int rc, LedgerHandle lh, Enumeration<LedgerEntry> seq, Object ctx) {
                    if (rc != BKException.Code.OK || !seq.hasMoreElements()) {
                        BKException bke = BKException.create(rc);
                        logger.error("Error while reading from ledger: " + imlr.range.getLedgerId() + " for topic: "
                                     + topic.toStringUtf8(), bke);
                        request.callback.scanFailed(request.ctx, new PubSubException.ServiceDownException(bke));
                        return;
                    }

                    LedgerEntry entry = null;
                    while (seq.hasMoreElements()) {
                        entry = seq.nextElement();
                        Message message;
                        try {
                            message = Message.parseFrom(entry.getEntryInputStream());
                        } catch (IOException e) {
                            String msg = "Unreadable message found in ledger: " + imlr.range.getLedgerId()
                                         + " for topic: " + topic.toStringUtf8();
                            logger.error(msg, e);
                            request.callback.scanFailed(ctx, new PubSubException.UnexpectedConditionException(msg));
                            return;
                        }

                        if (logger.isDebugEnabled()) {
                            logger.debug("Read response from ledger: " + lh.getId() + " entry-id: "
                                         + entry.getEntryId());
                        }

                        assert expectedEntryId == entry.getEntryId() : "expectedEntryId (" + expectedEntryId
                        + ") != entry.getEntryId() (" + entry.getEntryId() + ")";
                        assert (message.getMsgId().getLocalComponent() - imlr.startSeqIdIncluded) == expectedEntryId;

                        expectedEntryId++;
                        request.callback.messageScanned(ctx, message);
                        numMessagesRead++;
                        totalSizeRead += message.getBody().size();

                        if (numMessagesRead >= request.messageLimit) {
                            request.callback.scanFinished(ctx, ReasonForFinish.NUM_MESSAGES_LIMIT_EXCEEDED);
                            return;
                        }

                        if (totalSizeRead >= request.sizeLimit) {
                            request.callback.scanFinished(ctx, ReasonForFinish.SIZE_LIMIT_EXCEEDED);
                            return;
                        }
                    }

                    startReadingFrom(imlr.startSeqIdIncluded + entry.getEntryId() + 1);

                }
            }, request.ctx);
        }

        protected void startReadingFrom(long startSeqId) {

            Map.Entry<Long, InMemoryLedgerRange> entry = topicInfo.ledgerRanges.ceilingEntry(startSeqId);

            if (entry == null) {
                // None of the old ledgers have this seq-id, we must use the
                // current ledger
                long endSeqId = topicInfo.currentLedgerRange.startSeqIdIncluded
                                + topicInfo.lastEntryIdAckedInCurrentLedger;

                if (endSeqId < startSeqId) {
                    request.callback.scanFinished(request.ctx, ReasonForFinish.NO_MORE_MESSAGES);
                    return;
                }

                read(topicInfo.currentLedgerRange, startSeqId, endSeqId);
            } else {
                read(entry.getValue(), startSeqId, entry.getValue().range.getEndSeqIdIncluded().getLocalComponent());
            }

        }

    }

    @Override
    public void scanMessages(RangeScanRequest request) {
        queuer.pushAndMaybeRun(request.topic, new RangeScanOp(request));
    }

    public void deliveredUntil(ByteString topic, Long seqId) {
        // Nothing to do here. this is just a hint that we cannot use.
    }

    public void consumedUntil(ByteString topic, Long seqId) {
        TopicInfo topicInfo = topicInfos.get(topic);
        if (topicInfo == null) {
            logger.error("Server is not responsible for topic!");
            return;
        }
        for (Long endSeqIdIncluded : topicInfo.ledgerRanges.keySet()) {
            if (endSeqIdIncluded <= seqId) {
                // This ledger's message entries have all been consumed already
                // so it is safe to delete it from BookKeeper.
                long ledgerId = topicInfo.ledgerRanges.get(endSeqIdIncluded).range.getLedgerId();
                try {
                    bk.deleteLedger(ledgerId);
                } catch (Exception e) {
                    // For now, just log an exception error message. In the
                    // future, we can have more complicated retry logic to
                    // delete a consumed ledger. The next time the ledger
                    // garbage collection job runs, we'll once again try to
                    // delete this ledger.
                    logger.error("Exception while deleting consumed ledgerId: " + ledgerId, e);
                }
            } else
                break;
        }
    }

    public MessageSeqId getCurrentSeqIdForTopic(ByteString topic) throws ServerNotResponsibleForTopicException {
        TopicInfo topicInfo = topicInfos.get(topic);

        if (topicInfo == null) {
            throw new PubSubException.ServerNotResponsibleForTopicException("");
        }

        return topicInfo.lastSeqIdPushed;
    }

    public long getSeqIdAfterSkipping(ByteString topic, long seqId, int skipAmount) {
        return seqId + skipAmount;
    }

    public class PersistOp extends TopicOpQueuer.SynchronousOp {
        PersistRequest request;

        public PersistOp(PersistRequest request) {
            queuer.super(request.topic);
            this.request = request;
        }

        @Override
        public void runInternal() {
            final TopicInfo topicInfo = topicInfos.get(topic);

            if (topicInfo == null) {
                request.callback.operationFailed(request.ctx,
                                                 new PubSubException.ServerNotResponsibleForTopicException(""));
                return;
            }

            final long localSeqId = topicInfo.lastSeqIdPushed.getLocalComponent() + 1;
            MessageSeqId.Builder builder = MessageSeqId.newBuilder();
            if (request.message.hasMsgId()) {
                MessageIdUtils.takeRegionMaximum(builder, topicInfo.lastSeqIdPushed, request.message.getMsgId());
            } else {
                builder.addAllRemoteComponents(topicInfo.lastSeqIdPushed.getRemoteComponentsList());
            }
            builder.setLocalComponent(localSeqId);

            topicInfo.lastSeqIdPushed = builder.build();
            Message msgToSerialize = Message.newBuilder(request.message).setMsgId(topicInfo.lastSeqIdPushed).build();

            topicInfo.currentLedgerRange.handle.asyncAddEntry(msgToSerialize.toByteArray(),
            new SafeAsynBKCallback.AddCallback() {
                @Override
                public void safeAddComplete(int rc, LedgerHandle lh, long entryId, Object ctx) {
                    if (rc != BKException.Code.OK) {
                        BKException bke = BKException.create(rc);
                        logger.error("Error while persisting entry to ledger: " + lh.getId() + " for topic: "
                                     + topic.toStringUtf8(), bke);

                        // To preserve ordering guarantees, we
                        // should give up the topic and not let
                        // other operations through
                        request.callback.operationFailed(ctx, new PubSubException.ServiceDownException(bke));
                        return;
                    }

                    if (entryId + topicInfo.currentLedgerRange.startSeqIdIncluded != localSeqId) {
                        String msg = "Expected BK to assign entry-id: "
                                     + (localSeqId - topicInfo.currentLedgerRange.startSeqIdIncluded)
                                     + " but it instead assigned entry-id: " + entryId + " topic: "
                                     + topic.toStringUtf8() + "ledger: " + lh.getId();
                        logger.error(msg);
                        throw new UnexpectedError(msg);
                    }

                    topicInfo.lastEntryIdAckedInCurrentLedger = entryId;
                    request.callback.operationFinished(ctx, localSeqId);
                }
            }, request.ctx);

        }
    }

    public void persistMessage(PersistRequest request) {
        queuer.pushAndMaybeRun(request.topic, new PersistOp(request));
    }

    public void scanSingleMessage(ScanRequest request) {
        throw new RuntimeException("Not implemented");
    }

    static SafeAsynBKCallback.CloseCallback noOpCloseCallback = new SafeAsynBKCallback.CloseCallback() {
        @Override
        public void safeCloseComplete(int rc, LedgerHandle ledgerHandle, Object ctx) {
        };
    };

    String ledgersPath(ByteString topic) {
        return cfg.getZkTopicPath(new StringBuilder(), topic).append("/ledgers").toString();
    }

    class AcquireOp extends TopicOpQueuer.AsynchronousOp<Void> {
        public AcquireOp(ByteString topic, Callback<Void> cb, Object ctx) {
            queuer.super(topic, cb, ctx);
        }

        @Override
        public void run() {
            if (topicInfos.containsKey(topic)) {
                // Already acquired, do nothing
                cb.operationFinished(ctx, null);
                return;
            }
            // read topic ledgers node data
            final String zNodePath = ledgersPath(topic);

            zk.getData(zNodePath, false, new SafeAsyncZKCallback.DataCallback() {
                @Override
                public void safeProcessResult(int rc, String path, Object ctx, byte[] data, Stat stat) {
                    if (rc == Code.OK.intValue()) {
                        processTopicLedgersNodeData(data, stat.getVersion());
                        return;
                    }

                    if (rc == Code.NONODE.intValue()) {
                        // create it
                        final byte[] initialData = LedgerRanges.getDefaultInstance().toByteArray();
                        ZkUtils.createFullPathOptimistic(zk, zNodePath, initialData, ZooDefs.Ids.OPEN_ACL_UNSAFE,
                        CreateMode.PERSISTENT, new SafeAsyncZKCallback.StringCallback() {
                            @Override
                            public void safeProcessResult(int rc, String path, Object ctx, String name) {
                                if (rc != Code.OK.intValue()) {
                                    KeeperException ke = ZkUtils.logErrorAndCreateZKException(
                                                             "Could not create ledgers node for topic: " + topic.toStringUtf8(),
                                                             path, rc);
                                    cb.operationFailed(ctx, new PubSubException.ServiceDownException(ke));
                                    return;
                                }
                                // initial version is version 1
                                // (guessing)
                                processTopicLedgersNodeData(initialData, 0);
                            }
                        }, ctx);
                        return;
                    }

                    // otherwise some other error
                    KeeperException ke = ZkUtils.logErrorAndCreateZKException("Could not read ledgers node for topic: "
                                         + topic.toStringUtf8(), path, rc);
                    cb.operationFailed(ctx, new PubSubException.ServiceDownException(ke));

                }
            }, ctx);
        }

        void processTopicLedgersNodeData(byte[] data, int version) {

            final LedgerRanges ranges;
            try {
                ranges = LedgerRanges.parseFrom(data);
            } catch (InvalidProtocolBufferException e) {
                String msg = "Ledger ranges for topic:" + topic.toStringUtf8() + " could not be deserialized";
                logger.error(msg, e);
                cb.operationFailed(ctx, new PubSubException.UnexpectedConditionException(msg));
                return;
            }

            Iterator<LedgerRange> lrIterator = ranges.getRangesList().iterator();
            TopicInfo topicInfo = new TopicInfo();

            long startOfLedger = 1;

            while (lrIterator.hasNext()) {
                LedgerRange range = lrIterator.next();

                if (range.hasEndSeqIdIncluded()) {
                    // this means it was a valid and completely closed ledger
                    long endOfLedger = range.getEndSeqIdIncluded().getLocalComponent();
                    topicInfo.ledgerRanges.put(endOfLedger, new InMemoryLedgerRange(range, startOfLedger));
                    startOfLedger = endOfLedger + 1;
                    continue;
                }

                // If it doesn't have a valid end, it must be the last ledger
                if (lrIterator.hasNext()) {
                    String msg = "Ledger-id: " + range.getLedgerId() + " for topic: " + topic.toStringUtf8()
                                 + " is not the last one but still does not have an end seq-id";
                    logger.error(msg);
                    cb.operationFailed(ctx, new PubSubException.UnexpectedConditionException(msg));
                    return;
                }

                // The last ledger does not have a valid seq-id, lets try to
                // find it out
                recoverLastTopicLedgerAndOpenNewOne(range.getLedgerId(), version, topicInfo);
                return;
            }

            // All ledgers were found properly closed, just start a new one
            openNewTopicLedger(version, topicInfo);
        }

        /**
         * Recovers the last ledger, opens a new one, and persists the new
         * information to ZK
         *
         * @param ledgerId
         *            Ledger to be recovered
         */
        private void recoverLastTopicLedgerAndOpenNewOne(final long ledgerId, final int expectedVersionOfLedgerNode,
                final TopicInfo topicInfo) {

            bk.asyncOpenLedger(ledgerId, DigestType.CRC32, passwd, new SafeAsynBKCallback.OpenCallback() {
                @Override
                public void safeOpenComplete(int rc, LedgerHandle ledgerHandle, Object ctx) {

                    if (rc != BKException.Code.OK) {
                        BKException bke = BKException.create(rc);
                        logger.error("While acquiring topic: " + topic.toStringUtf8()
                                     + ", could not open unrecovered ledger: " + ledgerId, bke);
                        cb.operationFailed(ctx, new PubSubException.ServiceDownException(bke));
                        return;
                    }

                    final long numEntriesInLastLedger = ledgerHandle.getLastAddConfirmed() + 1;

                    if (numEntriesInLastLedger <= 0) {
                        // this was an empty ledger that someone created but
                        // couldn't write to, so just ignore it
                        logger.info("Pruning empty ledger: " + ledgerId + " for topic: " + topic.toStringUtf8());
                        closeLedger(ledgerHandle);
                        openNewTopicLedger(expectedVersionOfLedgerNode, topicInfo);
                        return;
                    }

                    // we have to read the last entry of the ledger to find
                    // out the last seq-id

                    ledgerHandle.asyncReadEntries(numEntriesInLastLedger - 1, numEntriesInLastLedger - 1,
                    new SafeAsynBKCallback.ReadCallback() {
                        @Override
                        public void safeReadComplete(int rc, LedgerHandle lh, Enumeration<LedgerEntry> seq,
                        Object ctx) {
                            if (rc != BKException.Code.OK || !seq.hasMoreElements()) {
                                BKException bke = BKException.create(rc);
                                logger.error("While recovering ledger: " + ledgerId + " for topic: "
                                             + topic.toStringUtf8() + ", could not read last entry", bke);
                                cb.operationFailed(ctx, new PubSubException.ServiceDownException(bke));
                                return;
                            }

                            Message lastMessage;
                            try {
                                lastMessage = Message.parseFrom(seq.nextElement().getEntry());
                            } catch (InvalidProtocolBufferException e) {
                                String msg = "While recovering ledger: " + ledgerId + " for topic: "
                                             + topic.toStringUtf8() + ", could not deserialize last message";
                                logger.error(msg, e);
                                cb.operationFailed(ctx, new PubSubException.UnexpectedConditionException(msg));
                                return;
                            }

                            long prevLedgerEnd = topicInfo.ledgerRanges.isEmpty() ? 0 : topicInfo.ledgerRanges
                                                 .lastKey();
                            LedgerRange lr = LedgerRange.newBuilder().setLedgerId(ledgerId)
                                             .setEndSeqIdIncluded(lastMessage.getMsgId()).build();
                            topicInfo.ledgerRanges.put(lr.getEndSeqIdIncluded().getLocalComponent(),
                                                       new InMemoryLedgerRange(lr, prevLedgerEnd + 1, lh));

                            logger.info("Recovered unclosed ledger: " + ledgerId + " for topic: "
                                        + topic.toStringUtf8() + " with " + numEntriesInLastLedger + " entries");

                            openNewTopicLedger(expectedVersionOfLedgerNode, topicInfo);
                        }
                    }, ctx);

                }

            }, ctx);
        }

        /**
         *
         * @param requiredVersionOfLedgersNode
         *            The version of the ledgers node when we read it, should be
         *            the same when we try to write
         */
        private void openNewTopicLedger(final int expectedVersionOfLedgersNode, final TopicInfo topicInfo) {
            bk.asyncCreateLedger(cfg.getBkEnsembleSize(), cfg.getBkQuorumSize(), DigestType.CRC32, passwd,
            new SafeAsynBKCallback.CreateCallback() {
                boolean processed = false;

                @Override
                public void safeCreateComplete(int rc, LedgerHandle lh, Object ctx) {
                    if (processed) {
                        return;
                    } else {
                        processed = true;
                    }

                    if (rc != BKException.Code.OK) {
                        BKException bke = BKException.create(rc);
                        logger.error("Could not create new ledger while acquiring topic: "
                                     + topic.toStringUtf8(), bke);
                        cb.operationFailed(ctx, new PubSubException.ServiceDownException(bke));
                        return;
                    }

                    topicInfo.lastSeqIdPushed = topicInfo.ledgerRanges.isEmpty() ? MessageSeqId.newBuilder()
                                                .setLocalComponent(0).build() : topicInfo.ledgerRanges.lastEntry().getValue().range
                                                .getEndSeqIdIncluded();

                    LedgerRange lastRange = LedgerRange.newBuilder().setLedgerId(lh.getId()).build();
                    topicInfo.currentLedgerRange = new InMemoryLedgerRange(lastRange, topicInfo.lastSeqIdPushed
                            .getLocalComponent() + 1, lh);

                    // Persist the fact that we started this new
                    // ledger to ZK

                    LedgerRanges.Builder builder = LedgerRanges.newBuilder();
                    for (InMemoryLedgerRange imlr : topicInfo.ledgerRanges.values()) {
                        builder.addRanges(imlr.range);
                    }
                    builder.addRanges(lastRange);

                    writeTopicLedgersNode(topic, builder.build().toByteArray(), expectedVersionOfLedgersNode,
                                          topicInfo);
                    return;
                }
            }, ctx);
        }

        void writeTopicLedgersNode(final ByteString topic, byte[] data, int expectedVersion, final TopicInfo topicInfo) {
            final String zNodePath = ledgersPath(topic);

            zk.setData(zNodePath, data, expectedVersion, new SafeAsyncZKCallback.StatCallback() {
                @Override
                public void safeProcessResult(int rc, String path, Object ctx, Stat stat) {
                    if (rc != KeeperException.Code.OK.intValue()) {
                        KeeperException ke = ZkUtils.logErrorAndCreateZKException(
                                                 "Could not write ledgers node for topic: " + topic.toStringUtf8(), path, rc);
                        cb.operationFailed(ctx, new PubSubException.ServiceDownException(ke));
                        return;
                    }

                    // Finally, all done
                    topicInfos.put(topic, topicInfo);
                    cb.operationFinished(ctx, null);
                }
            }, ctx);

        }
    }

    /**
     * acquire ownership of a topic, doing whatever is needed to be able to
     * perform reads and writes on that topic from here on
     *
     * @param topic
     * @param callback
     * @param ctx
     */
    @Override
    public void acquiredTopic(ByteString topic, Callback<Void> callback, Object ctx) {
        queuer.pushAndMaybeRun(topic, new AcquireOp(topic, callback, ctx));
    }

    public void closeLedger(LedgerHandle lh) {
        // try {
        // lh.asyncClose(noOpCloseCallback, null);
        // } catch (InterruptedException e) {
        // logger.error(e);
        // Thread.currentThread().interrupt();
        // }
    }

    class ReleaseOp extends TopicOpQueuer.SynchronousOp {

        public ReleaseOp(ByteString topic) {
            queuer.super(topic);
        }

        @Override
        public void runInternal() {
            TopicInfo topicInfo = topicInfos.remove(topic);

            if (topicInfo == null) {
                return;
            }

            for (InMemoryLedgerRange imlr : topicInfo.ledgerRanges.values()) {
                if (imlr.handle != null) {
                    closeLedger(imlr.handle);
                }
            }

            if (topicInfo.currentLedgerRange != null && topicInfo.currentLedgerRange.handle != null) {
                closeLedger(topicInfo.currentLedgerRange.handle);
            }
        }
    }

    /**
     * Release any resources for the topic that might be currently held. There
     * wont be any subsequent reads or writes on that topic coming
     *
     * @param topic
     */
    @Override
    public void lostTopic(ByteString topic) {
        queuer.pushAndMaybeRun(topic, new ReleaseOp(topic));
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/persistence/CacheKey.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.persistence;

import com.google.protobuf.ByteString;
import org.apache.hedwig.server.common.ByteStringInterner;

public class CacheKey {

    ByteString topic;
    long seqId;

    public CacheKey(ByteString topic, long seqId) {
        this.topic = ByteStringInterner.intern(topic);
        this.seqId = seqId;
    }

    public ByteString getTopic() {
        return topic;
    }

    public long getSeqId() {
        return seqId;
    }

    @Override
    public int hashCode() {
        final int prime = 31;
        int result = 1;
        result = prime * result + (int) (seqId ^ (seqId >>> 32));
        result = prime * result + ((topic == null) ? 0 : topic.hashCode());
        return result;
    }

    @Override
    public boolean equals(Object obj) {
        if (this == obj)
            return true;
        if (obj == null)
            return false;
        if (getClass() != obj.getClass())
            return false;
        CacheKey other = (CacheKey) obj;
        if (seqId != other.seqId)
            return false;
        if (topic == null) {
            if (other.topic != null)
                return false;
        } else if (!topic.equals(other.topic))
            return false;
        return true;
    }

    @Override
    public String toString() {
        return "(" + topic.toStringUtf8() + "," + seqId + ")";
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/persistence/CacheValue.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.persistence;

import java.util.LinkedList;
import java.util.Queue;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.server.common.UnexpectedError;

/**
 * This class is NOT thread safe. It need not be thread-safe because our
 * read-ahead cache will operate with only 1 thread
 *
 */
public class CacheValue {

    static Logger logger = LoggerFactory.getLogger(ReadAheadCache.class);

    Queue<ScanCallbackWithContext> callbacks = new LinkedList<ScanCallbackWithContext>();
    Message message;
    long timeOfAddition = 0;

    public CacheValue() {
    }

    public boolean isStub() {
        return message == null;
    }

    public long getTimeOfAddition() {
        if (message == null) {
            throw new UnexpectedError("Time of add requested from a stub");
        }
        return timeOfAddition;
    }

    public void setMessageAndInvokeCallbacks(Message message, long currTime) {
        if (this.message != null) {
            // Duplicate read for the same message coming back
            return;
        }

        this.message = message;
        this.timeOfAddition = currTime;
        ScanCallbackWithContext callbackWithCtx;
        if (logger.isDebugEnabled()) {
            logger.debug("Invoking " + callbacks.size() + " callbacks for " + " message added to cache");
        }
        while ((callbackWithCtx = callbacks.poll()) != null) {
            callbackWithCtx.getScanCallback().messageScanned(callbackWithCtx.getCtx(), message);
        }
    }

    public void addCallback(ScanCallback callback, Object ctx) {
        if (!isStub()) {
            // call the callback right away
            callback.messageScanned(ctx, message);
            return;
        }

        callbacks.add(new ScanCallbackWithContext(callback, ctx));
    }

    public Message getMessage() {
        return message;
    }

    public void setErrorAndInvokeCallbacks(Exception exception) {
        ScanCallbackWithContext callbackWithCtx;
        while ((callbackWithCtx = callbacks.poll()) != null) {
            callbackWithCtx.getScanCallback().scanFailed(callbackWithCtx.getCtx(), exception);
        }
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/persistence/Factory.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.persistence;

public interface Factory<T> {
    public T newInstance();
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/persistence/LocalDBPersistenceManager.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.persistence;

import java.io.File;
import java.io.IOException;
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.sql.Statement;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ConcurrentMap;

import javax.sql.rowset.serial.SerialBlob;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.protobuf.ByteString;
import org.apache.hedwig.exceptions.PubSubException.ServiceDownException;
import org.apache.hedwig.exceptions.PubSubException.UnexpectedConditionException;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protoextensions.MessageIdUtils;
import org.apache.hedwig.server.persistence.ScanCallback.ReasonForFinish;
import org.apache.hedwig.util.Callback;
import org.apache.hedwig.util.FileUtils;

public class LocalDBPersistenceManager implements PersistenceManagerWithRangeScan {
    static Logger logger = LoggerFactory.getLogger(LocalDBPersistenceManager.class);

    static String connectionURL;

    static {
        try {
            File tempDir = FileUtils.createTempDirectory("derby", null);

            // Since derby needs to create it, I will have to delete it first
            if (!tempDir.delete()) {
                throw new IOException("Could not delete dir: " + tempDir.getAbsolutePath());
            }
            connectionURL = "jdbc:derby:" + tempDir.getAbsolutePath() + ";create=true";
        } catch (IOException e) {
            throw new RuntimeException(e);
        }

    }

    private static final ThreadLocal<Connection> threadLocalConnection = new ThreadLocal<Connection>() {
        @Override
        protected Connection initialValue() {
            try {
                return DriverManager.getConnection(connectionURL);
            } catch (SQLException e) {
                logger.error("Could not connect to derby", e);
                return null;
            }
        }
    };
    static final String ID_FIELD_NAME = "id";
    static final String MSG_FIELD_NAME = "msg";
    static final String driver = "org.apache.derby.jdbc.EmbeddedDriver";

    static final int SCAN_CHUNK = 1000;

    /**
     * Having trouble restarting the database multiple times from within the
     * same jvm. Hence to facilitate units tests, we are just going to have a
     * version number that we will append to every table name. This version
     * number will be incremented in lieu of shutting down the database and
     * restarting it, so that we get different table names, and it behaves like
     * a brand new database
     */
    private int version = 0;

    ConcurrentMap<ByteString, MessageSeqId> currTopicSeqIds = new ConcurrentHashMap<ByteString, MessageSeqId>();

    static LocalDBPersistenceManager instance = new LocalDBPersistenceManager();

    public static LocalDBPersistenceManager instance() {
        return instance;
    }

    private LocalDBPersistenceManager() {

        try {
            Class.forName(driver).newInstance();
            logger.info("Derby Driver loaded");
        } catch (java.lang.ClassNotFoundException e) {
            logger.error("Derby driver not found", e);
        } catch (InstantiationException e) {
            logger.error("Could not instantiate derby driver", e);
        } catch (IllegalAccessException e) {
            logger.error("Could not instantiate derby driver", e);
        }
    }

    /**
     * Ensures that at least the default seq-id exists in the map for the given
     * topic. Checks for race conditions (.e.g, another thread inserts the
     * default id before us), and returns the latest seq-id value in the map
     *
     * @param topic
     * @return
     */
    private MessageSeqId ensureSeqIdExistsForTopic(ByteString topic) {
        MessageSeqId presentSeqIdInMap = currTopicSeqIds.get(topic);

        if (presentSeqIdInMap != null) {
            return presentSeqIdInMap;
        }

        presentSeqIdInMap = MessageSeqId.newBuilder().setLocalComponent(0).build();
        MessageSeqId oldSeqIdInMap = currTopicSeqIds.putIfAbsent(topic, presentSeqIdInMap);

        if (oldSeqIdInMap != null) {
            return oldSeqIdInMap;
        }
        return presentSeqIdInMap;

    }

    /**
     * Adjust the current seq id of the topic based on the message we are about
     * to publish. The local component of the current seq-id is always
     * incremented by 1. For the other components, there are two cases:
     *
     * 1. If the message to be published doesn't have a seq-id (locally
     * published messages), the other components are left as is.
     *
     * 2. If the message to be published has a seq-id, we take the max of the
     * current one we have, and that in the message to be published.
     *
     * @param topic
     * @param messageToPublish
     * @return The value of the local seq-id obtained after incrementing the
     *         local component. This value should be used as an id while
     *         persisting to Derby
     * @throws UnexpectedConditionException
     */
    private long adjustTopicSeqIdForPublish(ByteString topic, Message messageToPublish)
            throws UnexpectedConditionException {
        long retValue = 0;
        MessageSeqId oldId;
        MessageSeqId.Builder newIdBuilder = MessageSeqId.newBuilder();

        do {
            oldId = ensureSeqIdExistsForTopic(topic);

            // Increment our own component by 1
            retValue = oldId.getLocalComponent() + 1;
            newIdBuilder.setLocalComponent(retValue);

            if (messageToPublish.hasMsgId()) {
                // take a region-wise max
                MessageIdUtils.takeRegionMaximum(newIdBuilder, messageToPublish.getMsgId(), oldId);

            } else {
                newIdBuilder.addAllRemoteComponents(oldId.getRemoteComponentsList());
            }
        } while (!currTopicSeqIds.replace(topic, oldId, newIdBuilder.build()));

        return retValue;

    }

    public long getSeqIdAfterSkipping(ByteString topic, long seqId, int skipAmount) {
        return seqId + skipAmount;
    }

    public void persistMessage(PersistRequest request) {

        Connection conn = threadLocalConnection.get();

        Callback<Long> callback = request.getCallback();
        Object ctx = request.getCtx();
        ByteString topic = request.getTopic();
        Message message = request.getMessage();

        if (conn == null) {
            callback.operationFailed(ctx, new ServiceDownException("Not connected to derby"));
            return;
        }

        long seqId;

        try {
            seqId = adjustTopicSeqIdForPublish(topic, message);
        } catch (UnexpectedConditionException e) {
            callback.operationFailed(ctx, e);
            return;
        }
        PreparedStatement stmt;

        boolean triedCreatingTable = false;
        while (true) {
            try {
                message.getBody();
                stmt = conn.prepareStatement("INSERT INTO " + getTableNameForTopic(topic) + " VALUES(?,?)");
                stmt.setLong(1, seqId);
                stmt.setBlob(2, new SerialBlob(message.toByteArray()));

                int rowCount = stmt.executeUpdate();
                stmt.close();
                if (rowCount != 1) {
                    logger.error("Unexpected number of affected rows from derby");
                    callback.operationFailed(ctx, new ServiceDownException("Unexpected response from derby"));
                    return;
                }
                break;
            } catch (SQLException sqle) {
                String theError = (sqle).getSQLState();
                if (theError.equals("42X05") && !triedCreatingTable) {
                    createTable(conn, topic);
                    triedCreatingTable = true;
                    continue;
                }

                logger.error("Error while executing derby insert", sqle);
                callback.operationFailed(ctx, new ServiceDownException(sqle));
                return;
            }
        }
        callback.operationFinished(ctx, seqId);
    }

    /*
     * This method does not throw an exception because another thread might
     * sneak in and create the table before us
     */
    private void createTable(Connection conn, ByteString topic) {

        try {
            Statement stmt = conn.createStatement();
            String tableName = getTableNameForTopic(topic);
            stmt.execute("CREATE TABLE " + tableName + " (" + ID_FIELD_NAME + " BIGINT NOT NULL CONSTRAINT ID_PK_"
                         + tableName + " PRIMARY KEY," + MSG_FIELD_NAME + " BLOB(2M) NOT NULL)");
        } catch (SQLException e) {
            logger.debug("Could not create table", e);
        }
    }

    public MessageSeqId getCurrentSeqIdForTopic(ByteString topic) {
        return ensureSeqIdExistsForTopic(topic);
    }

    public void scanSingleMessage(ScanRequest request) {
        scanMessagesInternal(request.getTopic(), request.getStartSeqId(), 1, Long.MAX_VALUE, request.getCallback(),
                             request.getCtx(), 1);
        return;
    }

    public void scanMessages(RangeScanRequest request) {
        scanMessagesInternal(request.getTopic(), request.getStartSeqId(), request.getMessageLimit(), request
                             .getSizeLimit(), request.getCallback(), request.getCtx(), SCAN_CHUNK);
        return;
    }

    private String getTableNameForTopic(ByteString topic) {
        return (topic.toStringUtf8() + "_" + version);
    }

    private void scanMessagesInternal(ByteString topic, long startSeqId, int messageLimit, long sizeLimit,
                                      ScanCallback callback, Object ctx, int scanChunk) {

        Connection conn = threadLocalConnection.get();

        if (conn == null) {
            callback.scanFailed(ctx, new ServiceDownException("Not connected to derby"));
            return;
        }

        long currentSeqId;
        currentSeqId = startSeqId;

        PreparedStatement stmt;
        try {
            try {
                stmt = conn.prepareStatement("SELECT * FROM " + getTableNameForTopic(topic) + " WHERE " + ID_FIELD_NAME
                                             + " >= ?  AND " + ID_FIELD_NAME + " <= ?");

            } catch (SQLException sqle) {
                String theError = (sqle).getSQLState();
                if (theError.equals("42X05")) {
                    // No table, scan is over
                    callback.scanFinished(ctx, ReasonForFinish.NO_MORE_MESSAGES);
                    return;
                } else {
                    throw sqle;
                }
            }

            int numMessages = 0;
            long totalSize = 0;

            while (true) {

                stmt.setLong(1, currentSeqId);
                stmt.setLong(2, currentSeqId + scanChunk);

                if (!stmt.execute()) {
                    String errorMsg = "Select query did not return a result set";
                    logger.error(errorMsg);
                    stmt.close();
                    callback.scanFailed(ctx, new ServiceDownException(errorMsg));
                    return;
                }

                ResultSet resultSet = stmt.getResultSet();

                if (!resultSet.next()) {
                    stmt.close();
                    callback.scanFinished(ctx, ReasonForFinish.NO_MORE_MESSAGES);
                    return;
                }

                do {

                    long localSeqId = resultSet.getLong(1);

                    Message.Builder messageBuilder = Message.newBuilder().mergeFrom(resultSet.getBinaryStream(2));

                    // Merge in the local seq-id since that is not stored with
                    // the message
                    Message message = MessageIdUtils.mergeLocalSeqId(messageBuilder, localSeqId);

                    callback.messageScanned(ctx, message);
                    numMessages++;
                    totalSize += message.getBody().size();

                    if (numMessages > messageLimit) {
                        stmt.close();
                        callback.scanFinished(ctx, ReasonForFinish.NUM_MESSAGES_LIMIT_EXCEEDED);
                        return;
                    } else if (totalSize > sizeLimit) {
                        stmt.close();
                        callback.scanFinished(ctx, ReasonForFinish.SIZE_LIMIT_EXCEEDED);
                        return;
                    }

                } while (resultSet.next());

                currentSeqId += SCAN_CHUNK;
            }
        } catch (SQLException e) {
            logger.error("SQL Exception", e);
            callback.scanFailed(ctx, new ServiceDownException(e));
            return;
        } catch (IOException e) {
            logger.error("Message stored in derby is not parseable", e);
            callback.scanFailed(ctx, new ServiceDownException(e));
            return;
        }

    }

    public void deliveredUntil(ByteString topic, Long seqId) {
        // noop
    }

    public void consumedUntil(ByteString topic, Long seqId) {
        Connection conn = threadLocalConnection.get();
        if (conn == null) {
            logger.error("Not connected to derby");
            return;
        }
        PreparedStatement stmt;
        try {
            stmt = conn.prepareStatement("DELETE FROM " + getTableNameForTopic(topic) + " WHERE " + ID_FIELD_NAME
                                         + " <= ?");
            stmt.setLong(1, seqId);
            int rowCount = stmt.executeUpdate();
            logger.debug("Deleted " + rowCount + " records for topic: " + topic.toStringUtf8() + ", seqId: " + seqId);
            stmt.close();
        } catch (SQLException sqle) {
            String theError = (sqle).getSQLState();
            if (theError.equals("42X05")) {
                logger.warn("Table for topic (" + topic + ") does not exist so no consumed messages to delete!");
            } else
                logger.error("Error while executing derby delete for consumed messages", sqle);
        }
    }

    @Override
    protected void finalize() throws Throwable {
        if (driver.equals("org.apache.derby.jdbc.EmbeddedDriver")) {
            boolean gotSQLExc = false;
            // This is weird: on normal shutdown, it throws an exception
            try {
                DriverManager.getConnection("jdbc:derby:;shutdown=true").close();
            } catch (SQLException se) {
                if (se.getSQLState().equals("XJ015")) {
                    gotSQLExc = true;
                }
            }
            if (!gotSQLExc) {
                logger.error("Database did not shut down normally");
            } else {
                logger.info("Database shut down normally");
            }
        }
        super.finalize();
    }

    public void reset() {
        // just move the namespace over to the next one
        version++;
        currTopicSeqIds.clear();
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/persistence/MapMethods.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.persistence;

import java.util.Collection;
import java.util.Map;

public class MapMethods {

    public static <K, V> V getAfterInsertingIfAbsent(Map<K, V> map, K key, Factory<V> valueFactory) {
        V value = map.get(key);

        if (value == null) {
            value = valueFactory.newInstance();
            map.put(key, value);
        }

        return value;
    }

    public static <K, V, Z extends Collection<V>> void addToMultiMap(Map<K, Z> map, K key, V value,
            Factory<Z> valueFactory) {
        Collection<V> collection = getAfterInsertingIfAbsent(map, key, valueFactory);

        collection.add(value);

    }

    public static <K, V, Z extends Collection<V>> boolean removeFromMultiMap(Map<K, Z> map, K key, V value) {
        Collection<V> collection = map.get(key);

        if (collection == null) {
            return false;
        }

        if (!collection.remove(value)) {
            return false;
        } else {
            if (collection.isEmpty()) {
                map.remove(key);
            }
            return true;
        }

    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/persistence/PersistenceManager.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.persistence;

import com.google.protobuf.ByteString;
import org.apache.hedwig.exceptions.PubSubException.ServerNotResponsibleForTopicException;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;

/**
 * An implementation of this interface will persist messages in order and assign
 * a seqId to each persisted message. SeqId need not be a single number in
 * general. SeqId is opaque to all layers above {@link PersistenceManager}. Only
 * the {@link PersistenceManager} needs to understand the format of the seqId
 * and maintain it in such a way that there is a total order on the seqIds of a
 * topic.
 *
 */
public interface PersistenceManager {

    /**
     * Executes the given persist request asynchronously. When done, the
     * callback specified in the request object is called with the result of the
     * operation set to the {@link LocalMessageSeqId} assigned to the persisted
     * message.
     */
    public void persistMessage(PersistRequest request);

    /**
     * Get the seqId of the last message that has been persisted to the given
     * topic. The returned seqId will be set as the consume position of any
     * brand new subscription on this topic.
     *
     * Note that the return value may quickly become invalid because a
     * {@link #persistMessage(String, PublishedMessage)} call from another
     * thread succeeds. For us, the typical use case is choosing the consume
     * position of a new subscriber. Since the subscriber need not receive all
     * messages that are published while the subscribe call is in progress, such
     * loose semantics from this method is acceptable.
     *
     * @param topic
     * @return the seqId of the last persisted message.
     * @throws ServerNotResponsibleForTopicException
     */
    public MessageSeqId getCurrentSeqIdForTopic(ByteString topic) throws ServerNotResponsibleForTopicException;

    /**
     * Executes the given scan request
     *
     */
    public void scanSingleMessage(ScanRequest request);

    /**
     * Gets the next seq-id. This method should never block.
     */
    public long getSeqIdAfterSkipping(ByteString topic, long seqId, int skipAmount);

    /**
     * Hint that the messages until the given seqId have been delivered and wont
     * be needed unless there is a failure of some kind
     */
    public void deliveredUntil(ByteString topic, Long seqId);

    /**
     * Hint that the messages until the given seqId have been consumed by all
     * subscribers to the topic and no longer need to be stored. The
     * implementation classes can decide how and if they want to garbage collect
     * and delete these older topic messages that are no longer needed.
     *
     * @param topic
     *            Topic
     * @param seqId
     *            Message local sequence ID
     */
    public void consumedUntil(ByteString topic, Long seqId);

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/persistence/PersistenceManagerWithRangeScan.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.persistence;

public interface PersistenceManagerWithRangeScan extends PersistenceManager {
    /**
     * Executes the given range scan request
     *
     * @param request
     */
    public void scanMessages(RangeScanRequest request);
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/persistence/PersistRequest.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.persistence;

import com.google.protobuf.ByteString;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.util.Callback;

/**
 * Encapsulates a request to persist a given message on a given topic. The
 * request is completed asynchronously, callback and context are provided
 *
 */
public class PersistRequest {
    ByteString topic;
    Message message;
    Callback<Long> callback;
    Object ctx;

    public PersistRequest(ByteString topic, Message message, Callback<Long> callback, Object ctx) {
        this.topic = topic;
        this.message = message;
        this.callback = callback;
        this.ctx = ctx;
    }

    public ByteString getTopic() {
        return topic;
    }

    public Message getMessage() {
        return message;
    }

    public Callback<Long> getCallback() {
        return callback;
    }

    public Object getCtx() {
        return ctx;
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/persistence/RangeScanRequest.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.persistence;

import com.google.protobuf.ByteString;

/**
 * Encapsulates a request to scan messages on the given topic starting from the
 * given seqId (included). A call-back {@link ScanCallback} is provided. As
 * messages are scanned, the relevant methods of the {@link ScanCallback} are
 * called. Two hints are provided as to when scanning should stop: in terms of
 * number of messages scanned, or in terms of the total size of messages
 * scanned. Scanning stops whenever one of these limits is exceeded. These
 * checks, especially the one about message size, are only approximate. The
 * {@link ScanCallback} used should be prepared to deal with more or less
 * messages scanned. If an error occurs during scanning, the
 * {@link ScanCallback} is notified of the error.
 *
 */
public class RangeScanRequest {
    ByteString topic;
    long startSeqId;
    int messageLimit;
    long sizeLimit;
    ScanCallback callback;
    Object ctx;

    public RangeScanRequest(ByteString topic, long startSeqId, int messageLimit, long sizeLimit, ScanCallback callback,
                            Object ctx) {
        this.topic = topic;
        this.startSeqId = startSeqId;
        this.messageLimit = messageLimit;
        this.sizeLimit = sizeLimit;
        this.callback = callback;
        this.ctx = ctx;
    }

    public ByteString getTopic() {
        return topic;
    }

    public long getStartSeqId() {
        return startSeqId;
    }

    public int getMessageLimit() {
        return messageLimit;
    }

    public long getSizeLimit() {
        return sizeLimit;
    }

    public ScanCallback getCallback() {
        return callback;
    }

    public Object getCtx() {
        return ctx;
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/persistence/ReadAheadCache.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.persistence;

import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.LinkedList;
import java.util.Map;
import java.util.Queue;
import java.util.Set;
import java.util.SortedMap;
import java.util.SortedSet;
import java.util.TreeMap;
import java.util.TreeSet;
import java.util.concurrent.BlockingQueue;
import java.util.concurrent.LinkedBlockingQueue;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.protobuf.ByteString;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.ServerNotResponsibleForTopicException;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protoextensions.MessageIdUtils;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.common.UnexpectedError;
import org.apache.hedwig.util.Callback;

public class ReadAheadCache implements PersistenceManager, Runnable {

    static Logger logger = LoggerFactory.getLogger(ReadAheadCache.class);

    protected interface CacheRequest {
        public void performRequest();
    }

    /**
     * The underlying persistence manager that will be used for persistence and
     * scanning below the cache
     */
    protected PersistenceManagerWithRangeScan realPersistenceManager;

    /**
     * The structure for the cache
     */
    protected Map<CacheKey, CacheValue> cache = new HashMap<CacheKey, CacheValue>();

    /**
     * To simplify synchronization, the cache will be maintained by a single
     * cache maintainer thread. This is the queue that will hold requests that
     * need to be served by this thread
     */
    protected BlockingQueue<CacheRequest> requestQueue = new LinkedBlockingQueue<CacheRequest>();

    /**
     * We want to keep track of when entries were added in the cache, so that we
     * can remove them in a FIFO fashion
     */
    protected SortedMap<Long, Set<CacheKey>> timeIndexOfAddition = new TreeMap<Long, Set<CacheKey>>();

    /**
     * We also want to track the entries in seq-id order so that we can clean up
     * entries after the last subscriber
     */
    protected Map<ByteString, SortedSet<Long>> orderedIndexOnSeqId = new HashMap<ByteString, SortedSet<Long>>();

    /**
     * We maintain an estimate of the current size of the cache, so that we know
     * when to evict entries.
     */
    protected long presentCacheSize = 0;

    /**
     * One instance of a callback that we will pass to the underlying
     * persistence manager when asking it to persist messages
     */
    protected PersistCallback persistCallbackInstance = new PersistCallback();

    /**
     * 2 kinds of exceptions that we will use to signal error from readahead
     */
    protected NoSuchSeqIdException noSuchSeqIdExceptionInstance = new NoSuchSeqIdException();
    protected ReadAheadException readAheadExceptionInstance = new ReadAheadException();

    protected ServerConfiguration cfg;
    protected Thread cacheThread;
    // Boolean indicating if this thread should continue running. This is used
    // when we want to stop the thread during a PubSubServer shutdown.
    protected boolean keepRunning = true;

    /**
     * Constructor. Starts the cache maintainer thread
     *
     * @param realPersistenceManager
     */
    public ReadAheadCache(PersistenceManagerWithRangeScan realPersistenceManager, ServerConfiguration cfg) {
        this.realPersistenceManager = realPersistenceManager;
        this.cfg = cfg;
        cacheThread = new Thread(this, "CacheThread");
    }

    public ReadAheadCache start() {
        cacheThread.start();
        return this;
    }

    /**
     * ========================================================================
     * Methods of {@link PersistenceManager} that we will pass straight down to
     * the real persistence manager.
     */

    public long getSeqIdAfterSkipping(ByteString topic, long seqId, int skipAmount) {
        return realPersistenceManager.getSeqIdAfterSkipping(topic, seqId, skipAmount);
    }

    public MessageSeqId getCurrentSeqIdForTopic(ByteString topic) throws ServerNotResponsibleForTopicException {
        return realPersistenceManager.getCurrentSeqIdForTopic(topic);
    }

    /**
     * ========================================================================
     * Other methods of {@link PersistenceManager} that the cache needs to take
     * some action on.
     *
     * 1. Persist: We pass it through to the real persistence manager but insert
     * our callback on the return path
     *
     */
    public void persistMessage(PersistRequest request) {
        // make a new PersistRequest object so that we can insert our own
        // callback in the middle. Assign the original request as the context
        // for the callback.

        PersistRequest newRequest = new PersistRequest(request.getTopic(), request.getMessage(),
                persistCallbackInstance, request);
        realPersistenceManager.persistMessage(newRequest);
    }

    /**
     * The callback that we insert on the persist request return path. The
     * callback simply forms a {@link PersistResponse} object and inserts it in
     * the request queue to be handled serially by the cache maintainer thread.
     *
     */
    public class PersistCallback implements Callback<Long> {

        /**
         * In case there is a failure in persisting, just pass it to the
         * original callback
         */
        public void operationFailed(Object ctx, PubSubException exception) {
            PersistRequest originalRequest = (PersistRequest) ctx;
            Callback<Long> originalCallback = originalRequest.getCallback();
            Object originalContext = originalRequest.getCtx();
            originalCallback.operationFailed(originalContext, exception);
        }

        /**
         * When the persist finishes, we first notify the original callback of
         * success, and then opportunistically treat the message as if it just
         * came in through a scan
         */
        public void operationFinished(Object ctx, Long resultOfOperation) {
            PersistRequest originalRequest = (PersistRequest) ctx;

            // Lets call the original callback first so that the publisher can
            // hear success
            originalRequest.getCallback().operationFinished(originalRequest.getCtx(), resultOfOperation);

            // Original message that was persisted didn't have the local seq-id.
            // Lets add that in
            Message messageWithLocalSeqId = MessageIdUtils.mergeLocalSeqId(originalRequest.getMessage(),
                                            resultOfOperation);

            // Now enqueue a request to add this newly persisted message to our
            // cache
            CacheKey cacheKey = new CacheKey(originalRequest.getTopic(), resultOfOperation);

            enqueueWithoutFailure(new ScanResponse(cacheKey, messageWithLocalSeqId));
        }

    }

    /**
     * Too complicated to deal with enqueue failures from the context of our
     * callbacks. Its just simpler to quit and restart afresh. Moreover, this
     * should not happen as the request queue for the cache maintainer is
     * unbounded.
     *
     * @param obj
     */
    protected void enqueueWithoutFailure(CacheRequest obj) {
        if (!requestQueue.offer(obj)) {
            throw new UnexpectedError("Could not enqueue object: " + obj.toString()
                                      + " to cache request queue. Exiting.");

        }
    }

    /**
     * Another method from {@link PersistenceManager}.
     *
     * 2. Scan - Since the scan needs to touch the cache, we will just enqueue
     * the scan request and let the cache maintainer thread handle it.
     */
    public void scanSingleMessage(ScanRequest request) {
        // Let the scan requests be serialized through the queue
        enqueueWithoutFailure(new ScanRequestWrapper(request));
    }

    /**
     * Another method from {@link PersistenceManager}.
     *
     * 3. Enqueue the request so that the cache maintainer thread can delete all
     * message-ids older than the one specified
     */
    public void deliveredUntil(ByteString topic, Long seqId) {
        enqueueWithoutFailure(new DeliveredUntil(topic, seqId));
    }

    /**
     * Another method from {@link PersistenceManager}.
     *
     * Since this is a cache layer on top of an underlying persistence manager,
     * we can just call the consumedUntil method there. The messages older than
     * the latest one passed here won't be accessed anymore so they should just
     * get aged out of the cache eventually. For now, there is no need to
     * proactively remove those entries from the cache.
     */
    public void consumedUntil(ByteString topic, Long seqId) {
        realPersistenceManager.consumedUntil(topic, seqId);
    }

    /**
     * ========================================================================
     * BEGINNING OF CODE FOR THE CACHE MAINTAINER THREAD
     *
     * 1. The run method. It simply dequeues from the request queue, checks the
     * type of object and acts accordingly
     */
    public void run() {
        while (keepRunning) {
            CacheRequest obj;
            try {
                obj = requestQueue.take();
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
                return;
            }
            obj.performRequest();
        }

    }

    /**
     * Stop method which will enqueue a ShutdownCacheRequest.
     */
    public void stop() {
        enqueueWithoutFailure(new ShutdownCacheRequest());
    }

    /**
     * The readahead policy is simple: We check if an entry already exists for
     * the message being requested. If an entry exists, it means that either
     * that message is already in the cache, or a read for that message is
     * outstanding. In that case, we look a little ahead (by readAheadCount/2)
     * and issue a range read of readAheadCount/2 messages. The idea is to
     * ensure that the next readAheadCount messages are always available.
     *
     * @return the range scan that should be issued for read ahead
     */
    protected RangeScanRequest doReadAhead(ScanRequest request) {
        ByteString topic = request.getTopic();
        Long seqId = request.getStartSeqId();

        int readAheadCount = cfg.getReadAheadCount();
        // To prevent us from getting screwed by bad configuration
        readAheadCount = Math.max(1, readAheadCount);

        RangeScanRequest readAheadRequest = doReadAheadStartingFrom(topic, seqId, readAheadCount);

        if (readAheadRequest != null) {
            return readAheadRequest;
        }

        // start key was already there in the cache so no readahead happened,
        // lets look a little beyond
        seqId = realPersistenceManager.getSeqIdAfterSkipping(topic, seqId, readAheadCount / 2);

        readAheadRequest = doReadAheadStartingFrom(topic, seqId, readAheadCount / 2);

        return readAheadRequest;
    }

    /**
     * This method just checks if the provided seq-id already exists in the
     * cache. If not, a range read of the specified amount is issued.
     *
     * @param topic
     * @param seqId
     * @param readAheadCount
     * @return The range read that should be issued
     */
    protected RangeScanRequest doReadAheadStartingFrom(ByteString topic, long seqId, int readAheadCount) {

        long startSeqId = seqId;
        Queue<CacheKey> installedStubs = new LinkedList<CacheKey>();

        int i = 0;

        for (; i < readAheadCount; i++) {
            CacheKey cacheKey = new CacheKey(topic, seqId);

            // Even if a stub exists, it means that a scan for that is
            // outstanding
            if (cache.containsKey(cacheKey)) {
                break;
            }
            CacheValue cacheValue = new CacheValue();
            cache.put(cacheKey, cacheValue);

            if (logger.isDebugEnabled()) {
                logger.debug("Adding stub for seq-id: " + seqId + " topic: " + topic.toStringUtf8());
            }
            installedStubs.add(cacheKey);

            seqId = realPersistenceManager.getSeqIdAfterSkipping(topic, seqId, 1);
        }

        // so how many did we decide to readahead
        if (i == 0) {
            // no readahead, hence return false
            return null;
        }

        long readAheadSizeLimit = cfg.getReadAheadSizeBytes();
        ReadAheadScanCallback callback = new ReadAheadScanCallback(installedStubs, topic);
        RangeScanRequest rangeScanRequest = new RangeScanRequest(topic, startSeqId, i, readAheadSizeLimit, callback,
                null);

        return rangeScanRequest;

    }

    /**
     * This is the callback that is used for the range scans.
     */
    protected class ReadAheadScanCallback implements ScanCallback {
        Queue<CacheKey> installedStubs;
        ByteString topic;

        /**
         * Constructor
         *
         * @param installedStubs
         *            The list of stubs that were installed for this range scan
         * @param topic
         */
        public ReadAheadScanCallback(Queue<CacheKey> installedStubs, ByteString topic) {
            this.installedStubs = installedStubs;
            this.topic = topic;
        }

        public void messageScanned(Object ctx, Message message) {

            // Any message we read is potentially useful for us, so lets first
            // enqueue it
            CacheKey cacheKey = new CacheKey(topic, message.getMsgId().getLocalComponent());
            enqueueWithoutFailure(new ScanResponse(cacheKey, message));

            // Now lets see if this message is the one we were expecting
            CacheKey expectedKey = installedStubs.peek();

            if (expectedKey == null) {
                // Was not expecting any more messages to come in, but they came
                // in so we will keep them
                return;
            }

            if (expectedKey.equals(cacheKey)) {
                // what we got is what we expected, dequeue it so we get the
                // next expected one
                installedStubs.poll();
                return;
            }

            // If reached here, what we scanned was not what we were expecting.
            // This means that we have wrong stubs installed in the cache. We
            // should remove them, so that whoever is waiting on them can retry.
            // This shouldn't be happening usually
            logger.warn("Unexpected message seq-id: " + message.getMsgId().getLocalComponent() + " on topic: "
                        + topic.toStringUtf8() + " from readahead scan, was expecting seq-id: " + expectedKey.seqId
                        + " topic: " + expectedKey.topic.toStringUtf8() + " installedStubs: " + installedStubs);
            enqueueDeleteOfRemainingStubs(noSuchSeqIdExceptionInstance);

        }

        public void scanFailed(Object ctx, Exception exception) {
            enqueueDeleteOfRemainingStubs(exception);
        }

        public void scanFinished(Object ctx, ReasonForFinish reason) {
            // If the scan finished because no more messages are present, its ok
            // to leave the stubs in place because they will get filled in as
            // new publishes happen. However, if the scan finished due to some
            // other reason, e.g., read ahead size limit was reached, we want to
            // delete the stubs, so that when the time comes, we can schedule
            // another readahead request.
            if (reason != ReasonForFinish.NO_MORE_MESSAGES) {
                enqueueDeleteOfRemainingStubs(readAheadExceptionInstance);
            }
        }

        private void enqueueDeleteOfRemainingStubs(Exception reason) {
            CacheKey installedStub;
            while ((installedStub = installedStubs.poll()) != null) {
                enqueueWithoutFailure(new ExceptionOnCacheKey(installedStub, reason));
            }
        }
    }

    protected static class HashSetCacheKeyFactory implements Factory<Set<CacheKey>> {
        protected static HashSetCacheKeyFactory instance = new HashSetCacheKeyFactory();

        public Set<CacheKey> newInstance() {
            return new HashSet<CacheKey>();
        }
    }

    protected static class TreeSetLongFactory implements Factory<SortedSet<Long>> {
        protected static TreeSetLongFactory instance = new TreeSetLongFactory();

        public SortedSet<Long> newInstance() {
            return new TreeSet<Long>();
        }
    }

    /**
     * For adding the message to the cache, we do some bookeeping such as the
     * total size of cache, order in which entries were added etc. If the size
     * of the cache has exceeded our budget, old entries are collected.
     *
     * @param cacheKey
     * @param message
     */
    protected void addMessageToCache(CacheKey cacheKey, Message message, long currTime) {
        if (logger.isDebugEnabled()) {
            logger.debug("Adding msg (topic: " + cacheKey.getTopic().toStringUtf8() + ", seq-id: "
                         + message.getMsgId().getLocalComponent() + ") to readahead cache");
        }

        CacheValue cacheValue;

        if ((cacheValue = cache.get(cacheKey)) == null) {
            cacheValue = new CacheValue();
            cache.put(cacheKey, cacheValue);
        }

        // update the cache size
        presentCacheSize += message.getBody().size();

        // maintain the time index of addition
        MapMethods.addToMultiMap(timeIndexOfAddition, currTime, cacheKey, HashSetCacheKeyFactory.instance);

        // maintain the index of seq-id
        MapMethods.addToMultiMap(orderedIndexOnSeqId, cacheKey.getTopic(), cacheKey.getSeqId(),
                                 TreeSetLongFactory.instance);

        // finally add the message to the cache
        cacheValue.setMessageAndInvokeCallbacks(message, currTime);

        // if overgrown, collect old entries
        collectOldCacheEntries();
    }

    protected void removeMessageFromCache(CacheKey cacheKey, Exception exception, boolean maintainTimeIndex,
                                          boolean maintainSeqIdIndex) {
        CacheValue cacheValue = cache.remove(cacheKey);

        if (cacheValue == null) {
            return;
        }

        if (cacheValue.isStub()) {
            cacheValue.setErrorAndInvokeCallbacks(exception);
            // Stubs are not present in the indexes, so dont need to maintain
            // indexes here
            return;
        }

        presentCacheSize -= cacheValue.getMessage().getBody().size();

        // maintain the 2 indexes
        // TODO: can we maintain these lazily?
        if (maintainSeqIdIndex) {
            MapMethods.removeFromMultiMap(orderedIndexOnSeqId, cacheKey.getTopic(), cacheKey.getSeqId());
        }
        if (maintainTimeIndex) {
            MapMethods.removeFromMultiMap(timeIndexOfAddition, cacheValue.getTimeOfAddition(), cacheKey);
        }
    }

    /**
     * Collection of old entries is simple. Just collect in insert-time order,
     * oldest to newest.
     */
    protected void collectOldCacheEntries() {
        long maxCacheSize = cfg.getMaximumCacheSize();

        while (presentCacheSize > maxCacheSize && !timeIndexOfAddition.isEmpty()) {
            Long earliestTime = timeIndexOfAddition.firstKey();
            Set<CacheKey> oldCacheEntries = timeIndexOfAddition.get(earliestTime);

            // Note: only concrete cache entries, and not stubs are in the time
            // index. Hence there can be no callbacks pending on these cache
            // entries. Hence safe to remove them directly.
            for (Iterator<CacheKey> iter = oldCacheEntries.iterator(); iter.hasNext();) {
                CacheKey cacheKey = iter.next();

                if (logger.isDebugEnabled()) {
                    logger.debug("Removing topic: " + cacheKey.getTopic() + "seq-id: " + cacheKey.getSeqId()
                                 + " from cache because its the oldest");
                }
                removeMessageFromCache(cacheKey, readAheadExceptionInstance, //
                                       // maintainTimeIndex=
                                       false,
                                       // maintainSeqIdIndex=
                                       true);
            }

            timeIndexOfAddition.remove(earliestTime);

        }
    }

    /**
     * ========================================================================
     * The rest is just simple wrapper classes.
     *
     */

    protected class ExceptionOnCacheKey implements CacheRequest {
        CacheKey cacheKey;
        Exception exception;

        public ExceptionOnCacheKey(CacheKey cacheKey, Exception exception) {
            this.cacheKey = cacheKey;
            this.exception = exception;
        }

        /**
         * If for some reason, an outstanding read on a cache stub fails,
         * exception for that key is enqueued by the
         * {@link ReadAheadScanCallback}. To handle this, we simply send error
         * on the callbacks registered for that stub, and delete the entry from
         * the cache
         */
        public void performRequest() {
            removeMessageFromCache(cacheKey, exception,
                                   // maintainTimeIndex=
                                   true,
                                   // maintainSeqIdIndex=
                                   true);
        }

    }

    @SuppressWarnings("serial")
    protected static class NoSuchSeqIdException extends Exception {

        public NoSuchSeqIdException() {
            super("No such seq-id");
        }
    }

    @SuppressWarnings("serial")
    protected static class ReadAheadException extends Exception {
        public ReadAheadException() {
            super("Readahead failed");
        }
    }

    protected class ScanResponse implements CacheRequest {
        CacheKey cacheKey;
        Message message;

        public ScanResponse(CacheKey cacheKey, Message message) {
            this.cacheKey = cacheKey;
            this.message = message;
        }

        public void performRequest() {
            addMessageToCache(cacheKey, message, System.currentTimeMillis());
        }

    }

    protected class DeliveredUntil implements CacheRequest {
        ByteString topic;
        Long seqId;

        public DeliveredUntil(ByteString topic, Long seqId) {
            this.topic = topic;
            this.seqId = seqId;
        }

        public void performRequest() {
            SortedSet<Long> orderedSeqIds = orderedIndexOnSeqId.get(topic);
            if (orderedSeqIds == null) {
                return;
            }

            // focus on the set of messages with seq-ids <= the one that
            // has been delivered until
            SortedSet<Long> headSet = orderedSeqIds.headSet(seqId + 1);

            for (Iterator<Long> iter = headSet.iterator(); iter.hasNext();) {
                Long seqId = iter.next();
                CacheKey cacheKey = new CacheKey(topic, seqId);

                if (logger.isDebugEnabled()) {
                    logger.debug("Removing seq-id: " + cacheKey.getSeqId() + " topic: "
                                 + cacheKey.getTopic().toStringUtf8()
                                 + " from cache because every subscriber has moved past");
                }

                removeMessageFromCache(cacheKey, readAheadExceptionInstance, //
                                       // maintainTimeIndex=
                                       true,
                                       // maintainSeqIdIndex=
                                       false);
                iter.remove();
            }

            if (orderedSeqIds.isEmpty()) {
                orderedIndexOnSeqId.remove(topic);
            }
        }
    }

    protected class ScanRequestWrapper implements CacheRequest {
        ScanRequest request;

        public ScanRequestWrapper(ScanRequest request) {
            this.request = request;
        }

        /**
         * To handle a scan request, we first try to do readahead (which might
         * cause a range read to be issued to the underlying persistence
         * manager). The readahead will put a stub in the cache, if the message
         * is not already present in the cache. The scan callback that is part
         * of the scan request is added to this stub, and will be called later
         * when the message arrives as a result of the range scan issued to the
         * underlying persistence manager.
         */

        public void performRequest() {

            RangeScanRequest readAheadRequest = doReadAhead(request);

            // Read ahead must have installed at least a stub for us, so this
            // can't be null
            CacheValue cacheValue = cache.get(new CacheKey(request.getTopic(), request.getStartSeqId()));

            // Add our callback to the stub. If the cache value was already a
            // concrete message, the callback will be called right away
            cacheValue.addCallback(request.getCallback(), request.getCtx());

            if (readAheadRequest != null) {
                realPersistenceManager.scanMessages(readAheadRequest);
            }
        }
    }

    protected class ShutdownCacheRequest implements CacheRequest {
        // This is a simple type of CacheRequest we will enqueue when
        // the PubSubServer is shut down and we want to stop the ReadAheadCache
        // thread.
        public void performRequest() {
            keepRunning = false;
        }
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/persistence/ScanCallback.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.persistence;

import org.apache.hedwig.protocol.PubSubProtocol.Message;

public interface ScanCallback {

    enum ReasonForFinish {
        NO_MORE_MESSAGES, SIZE_LIMIT_EXCEEDED, NUM_MESSAGES_LIMIT_EXCEEDED
    };

    /**
     * This method is called when a message is read from the persistence layer
     * as part of a scan. The message just read is handed to this listener which
     * can then take the desired action on it. The return value from the method
     * indicates whether the scan should continue or not.
     *
     * @param ctx
     *            The context for the callback
     * @param message
     *            The message just scanned from the log
     * @return true if the scan should continue, false otherwise
     */
    public void messageScanned(Object ctx, Message message);

    /**
     * This method is called when the scan finishes
     *
     *
     * @param ctx
     * @param reason
     */

    public abstract void scanFinished(Object ctx, ReasonForFinish reason);

    /**
     * This method is called when the operation failed due to some reason. The
     * reason for failure is passed in.
     *
     * @param ctx
     *            The context for the callback
     * @param exception
     *            The reason for the failure of the scan
     */
    public abstract void scanFailed(Object ctx, Exception exception);

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/persistence/ScanCallbackWithContext.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.persistence;

public class ScanCallbackWithContext {
    ScanCallback scanCallback;
    Object ctx;

    public ScanCallbackWithContext(ScanCallback callback, Object ctx) {
        this.scanCallback = callback;
        this.ctx = ctx;
    }

    public ScanCallback getScanCallback() {
        return scanCallback;
    }

    public Object getCtx() {
        return ctx;
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/persistence/ScanRequest.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.persistence;

import com.google.protobuf.ByteString;
import org.apache.hedwig.protocol.PubSubProtocol.Message;

/**
 * Encapsulates a request for reading a single message. The message on the given
 * topic <b>at</b> the given seqId is scanned. A call-back {@link ScanCallback}
 * is provided. When the message is scanned, the
 * {@link ScanCallback#messageScanned(Object, Message)} method is called. Since
 * there is only 1 record to be scanned the
 * {@link ScanCallback#operationFinished(Object)} method may not be called since
 * its redundant.
 * {@link ScanCallback#scanFailed(Object, org.apache.hedwig.exceptions.PubSubException)}
 * method is called in case of error.
 *
 */
public class ScanRequest {
    ByteString topic;
    long startSeqId;
    ScanCallback callback;
    Object ctx;

    public ScanRequest(ByteString topic, long startSeqId, ScanCallback callback, Object ctx) {
        this.topic = topic;
        this.startSeqId = startSeqId;
        this.callback = callback;
        this.ctx = ctx;
    }

    public ByteString getTopic() {
        return topic;
    }

    public long getStartSeqId() {
        return startSeqId;
    }

    public ScanCallback getCallback() {
        return callback;
    }

    public Object getCtx() {
        return ctx;
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/proxy/ChannelTracker.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.proxy;

import java.util.HashMap;
import java.util.LinkedList;
import java.util.List;

import org.jboss.netty.channel.Channel;

import com.google.protobuf.ByteString;
import org.apache.hedwig.client.api.Subscriber;
import org.apache.hedwig.client.data.TopicSubscriber;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.TopicBusyException;
import org.apache.hedwig.server.handlers.ChannelDisconnectListener;
import org.apache.hedwig.util.Callback;

public class ChannelTracker implements ChannelDisconnectListener {
    HashMap<TopicSubscriber, Channel> topicSub2Channel = new HashMap<TopicSubscriber, Channel>();
    HashMap<Channel, List<TopicSubscriber>> channel2TopicSubs = new HashMap<Channel, List<TopicSubscriber>>();
    Subscriber subscriber;

    public ChannelTracker(Subscriber subscriber) {
        this.subscriber = subscriber;
    }

    static Callback<Void> noOpCallback = new Callback<Void>() {
        public void operationFailed(Object ctx, PubSubException exception) {
        };

        public void operationFinished(Object ctx, Void resultOfOperation) {
        };
    };

    public synchronized void channelDisconnected(Channel channel) {
        List<TopicSubscriber> topicSubs = channel2TopicSubs.remove(channel);

        if (topicSubs == null) {
            return;
        }

        for (TopicSubscriber topicSub : topicSubs) {
            topicSub2Channel.remove(topicSub);
            subscriber.asyncCloseSubscription(topicSub.getTopic(), topicSub.getSubscriberId(), noOpCallback, null);
        }
    }

    public synchronized void subscribeSucceeded(TopicSubscriber topicSubscriber, Channel channel)
            throws TopicBusyException {

        if (!channel.isConnected()) {
            // channel got disconnected while we were processing the
            // subscribe request, nothing much we can do in this case
            return;
        }

        if (topicSub2Channel.containsKey(topicSubscriber)) {
            TopicBusyException pse = new PubSubException.TopicBusyException(
                "subscription for this topic, subscriberId is already being served on a different channel");
            throw pse;
        }

        topicSub2Channel.put(topicSubscriber, channel);

        List<TopicSubscriber> topicSubs = channel2TopicSubs.get(channel);

        if (topicSubs == null) {
            topicSubs = new LinkedList<TopicSubscriber>();
            channel2TopicSubs.put(channel, topicSubs);
        }
        topicSubs.add(topicSubscriber);

    }

    public synchronized void aboutToUnsubscribe(ByteString topic, ByteString subscriberId) {
        TopicSubscriber topicSub = new TopicSubscriber(topic, subscriberId);

        Channel channel = topicSub2Channel.remove(topicSub);

        if (channel != null) {
            List<TopicSubscriber> topicSubs = channel2TopicSubs.get(channel);
            if (topicSubs != null) {
                topicSubs.remove(topicSub);
            }
        }
    }

    public synchronized void checkChannelMatches(ByteString topic, ByteString subscriberId, Channel channel)
            throws PubSubException {
        Channel subscribedChannel = getChannel(topic, subscriberId);

        if (subscribedChannel == null) {
            throw new PubSubException.ClientNotSubscribedException(
                "Can't start delivery since client is not subscribed");
        }

        if (subscribedChannel != channel) {
            throw new PubSubException.TopicBusyException(
                "Can't start delivery since client is subscribed on a different channel");
        }

    }

    public synchronized Channel getChannel(ByteString topic, ByteString subscriberId) {
        return topicSub2Channel.get(new TopicSubscriber(topic, subscriberId));
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/proxy/HedwigProxy.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.proxy;

import java.io.File;
import java.lang.Thread.UncaughtExceptionHandler;
import java.net.InetSocketAddress;
import java.net.MalformedURLException;
import java.util.HashMap;
import java.util.Map;
import java.util.concurrent.Executors;
import java.util.concurrent.LinkedBlockingQueue;
import org.apache.commons.configuration.ConfigurationException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.bootstrap.ServerBootstrap;
import org.jboss.netty.channel.group.ChannelGroup;
import org.jboss.netty.channel.group.DefaultChannelGroup;
import org.jboss.netty.channel.socket.ServerSocketChannelFactory;
import org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory;
import org.jboss.netty.logging.InternalLoggerFactory;
import org.jboss.netty.logging.Log4JLoggerFactory;

import org.apache.hedwig.client.HedwigClient;
import org.apache.hedwig.protocol.PubSubProtocol.OperationType;
import org.apache.hedwig.server.common.TerminateJVMExceptionHandler;
import org.apache.hedwig.server.handlers.Handler;
import org.apache.hedwig.server.netty.PubSubServer;
import org.apache.hedwig.server.netty.PubSubServerPipelineFactory;
import org.apache.hedwig.server.netty.UmbrellaHandler;

public class HedwigProxy {
    static final Logger logger = LoggerFactory.getLogger(HedwigProxy.class);

    HedwigClient client;
    ServerSocketChannelFactory serverSocketChannelFactory;
    ChannelGroup allChannels;
    Map<OperationType, Handler> handlers;
    ProxyConfiguration cfg;

    public HedwigProxy(final ProxyConfiguration cfg, final UncaughtExceptionHandler exceptionHandler)
            throws InterruptedException {
        this.cfg = cfg;

        ThreadGroup tg = new ThreadGroup("hedwigproxy") {
            @Override
            public void uncaughtException(Thread t, Throwable e) {
                exceptionHandler.uncaughtException(t, e);
            }
        };

        final LinkedBlockingQueue<Boolean> queue = new LinkedBlockingQueue<Boolean>();

        new Thread(tg, new Runnable() {
            @Override
            public void run() {
                client = new HedwigClient(cfg);

                serverSocketChannelFactory = new NioServerSocketChannelFactory(Executors.newCachedThreadPool(),
                        Executors.newCachedThreadPool());
                initializeHandlers();
                initializeNetty();

                queue.offer(true);
            }
        }).start();

        queue.take();
    }

    public HedwigProxy(ProxyConfiguration conf) throws InterruptedException {
        this(conf, new TerminateJVMExceptionHandler());
    }

    protected void initializeHandlers() {
        handlers = new HashMap<OperationType, Handler>();
        ChannelTracker tracker = new ChannelTracker(client.getSubscriber());

        handlers.put(OperationType.PUBLISH, new ProxyPublishHander(client.getPublisher()));
        handlers.put(OperationType.SUBSCRIBE, new ProxySubscribeHandler(client.getSubscriber(), tracker));
        handlers.put(OperationType.UNSUBSCRIBE, new ProxyUnsubscribeHandler(client.getSubscriber(), tracker));
        handlers.put(OperationType.CONSUME, new ProxyConsumeHandler(client.getSubscriber()));
        handlers.put(OperationType.STOP_DELIVERY, new ProxyStopDeliveryHandler(client.getSubscriber(), tracker));
        handlers.put(OperationType.START_DELIVERY, new ProxyStartDeliveryHandler(client.getSubscriber(), tracker));

    }

    protected void initializeNetty() {
        InternalLoggerFactory.setDefaultFactory(new Log4JLoggerFactory());
        allChannels = new DefaultChannelGroup("hedwigproxy");
        ServerBootstrap bootstrap = new ServerBootstrap(serverSocketChannelFactory);
        UmbrellaHandler umbrellaHandler = new UmbrellaHandler(allChannels, handlers, false);
        PubSubServerPipelineFactory pipeline = new PubSubServerPipelineFactory(umbrellaHandler, null, cfg
                .getMaximumMessageSize());

        bootstrap.setPipelineFactory(pipeline);
        bootstrap.setOption("child.tcpNoDelay", true);
        bootstrap.setOption("child.keepAlive", true);
        bootstrap.setOption("reuseAddress", true);

        // Bind and start to accept incoming connections.
        allChannels.add(bootstrap.bind(new InetSocketAddress(cfg.getProxyPort())));
        logger.info("Going into receive loop");
    }

    public void shutdown() {
        allChannels.close().awaitUninterruptibly();
        client.close();
        serverSocketChannelFactory.releaseExternalResources();
    }

    // the following method only exists for unit-testing purposes, should go
    // away once we make start delivery totally server-side
    public Handler getStartDeliveryHandler() {
        return handlers.get(OperationType.START_DELIVERY);
    }

    /**
     * @param args
     */
    public static void main(String[] args) {

        logger.info("Attempting to start Hedwig Proxy");
        ProxyConfiguration conf = new ProxyConfiguration();
        if (args.length > 0) {
            String confFile = args[0];
            try {
                conf.loadConf(new File(confFile).toURI().toURL());
            } catch (MalformedURLException e) {
                String msg = "Could not open configuration file: " + confFile;
                PubSubServer.errorMsgAndExit(msg, e, PubSubServer.RC_INVALID_CONF_FILE);
            } catch (ConfigurationException e) {
                String msg = "Malformed configuration file: " + confFile;
                PubSubServer.errorMsgAndExit(msg, e, PubSubServer.RC_MISCONFIGURED);
            }
            logger.info("Using configuration file " + confFile);
        }
        try {
            new HedwigProxy(conf);
        } catch (Throwable t) {
            PubSubServer.errorMsgAndExit("Error during startup", t, PubSubServer.RC_OTHER);
        }
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/proxy/ProxyConfiguration.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.proxy;

import org.apache.hedwig.client.conf.ClientConfiguration;

public class ProxyConfiguration extends ClientConfiguration {

    protected static String PROXY_PORT = "proxy_port";
    protected static String MAX_MESSAGE_SIZE = "max_message_size";

    public int getProxyPort() {
        return conf.getInt(PROXY_PORT, 9099);
    }

    @Override
    public int getMaximumMessageSize() {
        return conf.getInt(MAX_MESSAGE_SIZE, 1258291); /* 1.2M */
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/proxy/ProxyConsumeHandler.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.proxy;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;

import org.apache.hedwig.client.api.Subscriber;
import org.apache.hedwig.exceptions.PubSubException.ClientNotSubscribedException;
import org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;
import org.apache.hedwig.server.handlers.Handler;
import org.apache.hedwig.server.netty.UmbrellaHandler;

public class ProxyConsumeHandler implements Handler {

    static final Logger logger = LoggerFactory.getLogger(ProxyConsumeHandler.class);

    Subscriber subscriber;

    public ProxyConsumeHandler(Subscriber subscriber) {
        this.subscriber = subscriber;
    }

    @Override
    public void handleRequest(PubSubRequest request, Channel channel) {
        if (!request.hasConsumeRequest()) {
            UmbrellaHandler.sendErrorResponseToMalformedRequest(channel, request.getTxnId(),
                    "Missing consume request data");
            return;
        }

        ConsumeRequest consumeRequest = request.getConsumeRequest();
        try {
            subscriber.consume(request.getTopic(), consumeRequest.getSubscriberId(), consumeRequest.getMsgId());
        } catch (ClientNotSubscribedException e) {
            // ignore
            logger.warn("Unexpected consume request", e);
        }

    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/proxy/ProxyPublishHander.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.proxy;

import org.jboss.netty.channel.Channel;

import org.apache.hedwig.client.api.Publisher;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;
import org.apache.hedwig.protocol.PubSubProtocol.PublishRequest;
import org.apache.hedwig.protoextensions.PubSubResponseUtils;
import org.apache.hedwig.server.handlers.Handler;
import org.apache.hedwig.server.netty.UmbrellaHandler;
import org.apache.hedwig.util.Callback;

public class ProxyPublishHander implements Handler {
    Publisher publisher;

    public ProxyPublishHander(Publisher publisher) {
        this.publisher = publisher;
    }

    @Override
    public void handleRequest(final PubSubRequest request, final Channel channel) {
        if (!request.hasPublishRequest()) {
            UmbrellaHandler.sendErrorResponseToMalformedRequest(channel, request.getTxnId(),
                    "Missing publish request data");
            return;
        }

        final PublishRequest publishRequest = request.getPublishRequest();

        publisher.asyncPublish(request.getTopic(), publishRequest.getMsg(), new Callback<Void>() {
            @Override
            public void operationFailed(Object ctx, PubSubException exception) {
                channel.write(PubSubResponseUtils.getResponseForException(exception, request.getTxnId()));
            }

            @Override
            public void operationFinished(Object ctx, Void resultOfOperation) {
                channel.write(PubSubResponseUtils.getSuccessResponse(request.getTxnId()));
            }
        }, null);

    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/proxy/ProxyStartDeliveryHandler.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.proxy;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;
import org.jboss.netty.channel.ChannelFuture;
import org.jboss.netty.channel.ChannelFutureListener;

import com.google.protobuf.ByteString;
import org.apache.hedwig.client.api.MessageHandler;
import org.apache.hedwig.client.api.Subscriber;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.ClientNotSubscribedException;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse;
import org.apache.hedwig.protocol.PubSubProtocol.StatusCode;
import org.apache.hedwig.protoextensions.PubSubResponseUtils;
import org.apache.hedwig.server.handlers.Handler;
import org.apache.hedwig.server.netty.UmbrellaHandler;
import org.apache.hedwig.util.Callback;

public class ProxyStartDeliveryHandler implements Handler {

    static final Logger logger = LoggerFactory.getLogger(ProxyStartDeliveryHandler.class);

    Subscriber subscriber;
    ChannelTracker tracker;

    public ProxyStartDeliveryHandler(Subscriber subscriber, ChannelTracker tracker) {
        this.subscriber = subscriber;
        this.tracker = tracker;
    }

    @Override
    public void handleRequest(PubSubRequest request, Channel channel) {

        if (!request.hasStartDeliveryRequest()) {
            UmbrellaHandler.sendErrorResponseToMalformedRequest(channel, request.getTxnId(),
                    "Missing start delivery request data");
            return;
        }

        final ByteString topic = request.getTopic();
        final ByteString subscriberId = request.getStartDeliveryRequest().getSubscriberId();

        synchronized (tracker) {
            // try {
            // tracker.checkChannelMatches(topic, subscriberId, channel);
            // } catch (PubSubException e) {
            // channel.write(PubSubResponseUtils.getResponseForException(e,
            // request.getTxnId()));
            // return;
            // }

            final Channel subscribedChannel = tracker.getChannel(topic, subscriberId);

            if (subscribedChannel == null) {
                channel.write(PubSubResponseUtils.getResponseForException(
                                  new PubSubException.ClientNotSubscribedException("no subscription to start delivery on"),
                                  request.getTxnId()));
                return;
            }

            MessageHandler handler = new MessageHandler() {
                @Override
                public void deliver(ByteString topic, ByteString subscriberId, Message msg,
                final Callback<Void> callback, final Object context) {

                    PubSubResponse response = PubSubResponse.newBuilder().setProtocolVersion(
                                                  ProtocolVersion.VERSION_ONE).setStatusCode(StatusCode.SUCCESS).setTxnId(0).setMessage(msg)
                                              .setTopic(topic).setSubscriberId(subscriberId).build();

                    ChannelFuture future = subscribedChannel.write(response);

                    future.addListener(new ChannelFutureListener() {
                        @Override
                        public void operationComplete(ChannelFuture future) throws Exception {
                            if (!future.isSuccess()) {
                                // ignoring this failure, because this will
                                // only happen due to channel disconnect.
                                // Channel disconnect will in turn stop
                                // delivery, and stop these errors
                                return;
                            }

                            // Tell the hedwig client, that it can send me
                            // more messages
                            callback.operationFinished(context, null);
                        }
                    });
                }
            };

            channel.write(PubSubResponseUtils.getSuccessResponse(request.getTxnId()));

            try {
                subscriber.startDelivery(topic, subscriberId, handler);
            } catch (ClientNotSubscribedException e) {
                // This should not happen, since we already checked the correct
                // channel and so on
                logger.error("Unexpected: No subscription when attempting to start delivery", e);
                throw new RuntimeException(e);
            }



        }

    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/proxy/ProxyStopDeliveryHandler.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.proxy;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;

import com.google.protobuf.ByteString;
import org.apache.hedwig.client.api.Subscriber;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.ClientNotSubscribedException;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;
import org.apache.hedwig.server.handlers.Handler;
import org.apache.hedwig.server.netty.UmbrellaHandler;

public class ProxyStopDeliveryHandler implements Handler {

    static final Logger logger = LoggerFactory.getLogger(ProxyStopDeliveryHandler.class);

    Subscriber subscriber;
    ChannelTracker tracker;

    public ProxyStopDeliveryHandler(Subscriber subscriber, ChannelTracker tracker) {
        this.subscriber = subscriber;
        this.tracker = tracker;
    }

    @Override
    public void handleRequest(PubSubRequest request, Channel channel) {
        if (!request.hasStopDeliveryRequest()) {
            UmbrellaHandler.sendErrorResponseToMalformedRequest(channel, request.getTxnId(),
                    "Missing stop delivery request data");
            return;
        }

        final ByteString topic = request.getTopic();
        final ByteString subscriberId = request.getStartDeliveryRequest().getSubscriberId();

        synchronized (tracker) {
            try {
                tracker.checkChannelMatches(topic, subscriberId, channel);
            } catch (PubSubException e) {
                // intentionally ignore this error, since stop delivery doesn't
                // send back a response
                return;
            }

            try {
                subscriber.stopDelivery(topic, subscriberId);
            } catch (ClientNotSubscribedException e) {
                // This should not happen, since we already checked the correct
                // channel and so on
                logger.warn("Unexpected: No subscription when attempting to stop delivery", e);
            }
        }

    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/proxy/ProxySubscribeHandler.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.proxy;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;
import org.apache.hedwig.client.api.Subscriber;
import org.apache.hedwig.client.data.TopicSubscriber;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.TopicBusyException;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;
import org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest;
import org.apache.hedwig.protoextensions.PubSubResponseUtils;
import org.apache.hedwig.server.handlers.ChannelDisconnectListener;
import org.apache.hedwig.server.handlers.Handler;
import org.apache.hedwig.server.netty.UmbrellaHandler;
import org.apache.hedwig.util.Callback;

public class ProxySubscribeHandler implements Handler, ChannelDisconnectListener {

    static final Logger logger = LoggerFactory.getLogger(ProxySubscribeHandler.class);

    Subscriber subscriber;
    ChannelTracker tracker;

    public ProxySubscribeHandler(Subscriber subscriber, ChannelTracker tracker) {
        this.subscriber = subscriber;
        this.tracker = tracker;
    }

    @Override
    public void channelDisconnected(Channel channel) {
        tracker.channelDisconnected(channel);
    }

    @Override
    public void handleRequest(final PubSubRequest request, final Channel channel) {
        if (!request.hasSubscribeRequest()) {
            UmbrellaHandler.sendErrorResponseToMalformedRequest(channel, request.getTxnId(),
                    "Missing subscribe request data");
            return;
        }

        SubscribeRequest subRequest = request.getSubscribeRequest();
        final TopicSubscriber topicSubscriber = new TopicSubscriber(request.getTopic(), subRequest.getSubscriberId());

        subscriber.asyncSubscribe(topicSubscriber.getTopic(), subRequest.getSubscriberId(), subRequest
        .getCreateOrAttach(), new Callback<Void>() {
            @Override
            public void operationFailed(Object ctx, PubSubException exception) {
                channel.write(PubSubResponseUtils.getResponseForException(exception, request.getTxnId()));
            }

            @Override
            public void operationFinished(Object ctx, Void resultOfOperation) {
                try {
                    tracker.subscribeSucceeded(topicSubscriber, channel);
                } catch (TopicBusyException e) {
                    channel.write(PubSubResponseUtils.getResponseForException(e, request.getTxnId()));
                    return;
                }
                channel.write(PubSubResponseUtils.getSuccessResponse(request.getTxnId()));
            }
        }, null);
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/proxy/ProxyUnsubscribeHandler.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.proxy;

import org.jboss.netty.channel.Channel;

import com.google.protobuf.ByteString;
import org.apache.hedwig.client.api.Subscriber;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;
import org.apache.hedwig.protoextensions.PubSubResponseUtils;
import org.apache.hedwig.server.handlers.Handler;
import org.apache.hedwig.server.netty.UmbrellaHandler;
import org.apache.hedwig.util.Callback;

public class ProxyUnsubscribeHandler implements Handler {

    Subscriber subscriber;
    ChannelTracker tracker;

    public ProxyUnsubscribeHandler(Subscriber subscriber, ChannelTracker tracker) {
        this.subscriber = subscriber;
        this.tracker = tracker;
    }

    @Override
    public void handleRequest(final PubSubRequest request, final Channel channel) {
        if (!request.hasUnsubscribeRequest()) {
            UmbrellaHandler.sendErrorResponseToMalformedRequest(channel, request.getTxnId(),
                    "Missing unsubscribe request data");
            return;
        }

        ByteString topic = request.getTopic();
        ByteString subscriberId = request.getUnsubscribeRequest().getSubscriberId();

        synchronized (tracker) {

            // Even if unsubscribe fails, the hedwig client closes the channel
            // on which the subscription is being served. Hence better to tell
            // the tracker beforehand that this subscription is no longer served
            tracker.aboutToUnsubscribe(topic, subscriberId);

            subscriber.asyncUnsubscribe(topic, subscriberId, new Callback<Void>() {
                @Override
                public void operationFailed(Object ctx, PubSubException exception) {
                    channel.write(PubSubResponseUtils.getResponseForException(exception, request.getTxnId()));
                }

                @Override
                public void operationFinished(Object ctx, Void resultOfOperation) {
                    channel.write(PubSubResponseUtils.getSuccessResponse(request.getTxnId()));
                }
            }, null);
        }

    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/regions/HedwigHubClient.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.regions;

import org.jboss.netty.channel.socket.ClientSocketChannelFactory;

import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.netty.HedwigClientImpl;

/**
 * This is a hub specific implementation of the HedwigClient. All this does
 * though is to override the HedwigSubscriber with the hub specific child class.
 * Creating this class so we can call the protected method in the parent to set
 * the subscriber since we don't want to expose that API to the public.
 */
public class HedwigHubClient extends HedwigClientImpl {

    // Constructor when we already have a ChannelFactory instantiated.
    public HedwigHubClient(ClientConfiguration cfg, ClientSocketChannelFactory channelFactory) {
        super(cfg, channelFactory);
        // Override the type of HedwigSubscriber with the hub specific one.
        setSubscriber(new HedwigHubSubscriber(this));
    }

    // Constructor when we don't have a ChannelFactory. The super constructor
    // will create one for us.
    public HedwigHubClient(ClientConfiguration cfg) {
        super(cfg);
        // Override the type of HedwigSubscriber with the hub specific one.
        setSubscriber(new HedwigHubSubscriber(this));
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/regions/HedwigHubClientFactory.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.regions;

import org.jboss.netty.channel.socket.ClientSocketChannelFactory;

import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.util.HedwigSocketAddress;

public class HedwigHubClientFactory {

    private final ServerConfiguration cfg;
    private final ClientSocketChannelFactory channelFactory;

    // Constructor that takes in a ServerConfiguration and a ChannelFactory
    // so we can reuse it for all Clients created here.
    public HedwigHubClientFactory(ServerConfiguration cfg, ClientSocketChannelFactory channelFactory) {
        this.cfg = cfg;
        this.channelFactory = channelFactory;
    }

    /**
     * Manufacture a hub client whose default server to connect to is the input
     * HedwigSocketAddress hub.
     *
     * @param hub
     *            The hub in another region to connect to.
     */
    HedwigHubClient create(final HedwigSocketAddress hub) {
        // Create a hub specific version of the client to use
        return new HedwigHubClient(new ClientConfiguration() {
            @Override
            protected HedwigSocketAddress getDefaultServerHedwigSocketAddress() {
                return hub;
            }

            @Override
            public boolean isSSLEnabled() {
                return cfg.isInterRegionSSLEnabled();
            }
        }, channelFactory);
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/regions/HedwigHubSubscriber.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.regions;

import com.google.protobuf.ByteString;
import org.apache.hedwig.client.exceptions.InvalidSubscriberIdException;
import org.apache.hedwig.client.netty.HedwigClientImpl;
import org.apache.hedwig.client.netty.HedwigSubscriber;
import org.apache.hedwig.exceptions.PubSubException.ClientAlreadySubscribedException;
import org.apache.hedwig.exceptions.PubSubException.ClientNotSubscribedException;
import org.apache.hedwig.exceptions.PubSubException.CouldNotConnectException;
import org.apache.hedwig.exceptions.PubSubException.ServiceDownException;
import org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach;
import org.apache.hedwig.util.Callback;

/**
 * This is a hub specific child class of the HedwigSubscriber. The main thing is
 * does is wrap the public subscribe/unsubscribe methods by calling the
 * overloaded protected ones passing in a true value for the input boolean
 * parameter isHub. That will just make sure we validate the subscriberId
 * passed, ensuring it is of the right format either for a local or hub
 * subscriber.
 */
public class HedwigHubSubscriber extends HedwigSubscriber {

    public HedwigHubSubscriber(HedwigClientImpl client) {
        super(client);
    }

    @Override
    public void subscribe(ByteString topic, ByteString subscriberId, CreateOrAttach mode)
            throws CouldNotConnectException, ClientAlreadySubscribedException, ServiceDownException,
        InvalidSubscriberIdException {
        subscribe(topic, subscriberId, mode, true);
    }

    @Override
    public void asyncSubscribe(ByteString topic, ByteString subscriberId, CreateOrAttach mode, Callback<Void> callback,
                               Object context) {
        asyncSubscribe(topic, subscriberId, mode, callback, context, true);
    }

    @Override
    public void unsubscribe(ByteString topic, ByteString subscriberId) throws CouldNotConnectException,
        ClientNotSubscribedException, ServiceDownException, InvalidSubscriberIdException {
        unsubscribe(topic, subscriberId, true);
    }

    @Override
    public void asyncUnsubscribe(final ByteString topic, final ByteString subscriberId, final Callback<Void> callback,
                                 final Object context) {
        asyncUnsubscribe(topic, subscriberId, callback, context, true);
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/regions/RegionManager.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.regions;

import java.util.ArrayList;
import java.util.concurrent.ScheduledExecutorService;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.apache.zookeeper.ZooKeeper;

import com.google.protobuf.ByteString;
import org.apache.hedwig.client.api.MessageHandler;
import org.apache.hedwig.client.netty.HedwigSubscriber;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach;
import org.apache.hedwig.protoextensions.SubscriptionStateUtils;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.common.TopicOpQueuer;
import org.apache.hedwig.server.persistence.PersistRequest;
import org.apache.hedwig.server.persistence.PersistenceManager;
import org.apache.hedwig.server.subscriptions.SubscriptionEventListener;
import org.apache.hedwig.util.Callback;
import org.apache.hedwig.util.CallbackUtils;
import org.apache.hedwig.util.HedwigSocketAddress;

public class RegionManager implements SubscriptionEventListener {

    protected static final Logger LOGGER = LoggerFactory.getLogger(RegionManager.class);

    private final ByteString mySubId;
    private final PersistenceManager pm;
    private final ArrayList<HedwigHubClient> clients = new ArrayList<HedwigHubClient>();
    private final TopicOpQueuer queue;

    public RegionManager(final PersistenceManager pm, final ServerConfiguration cfg, final ZooKeeper zk,
                         ScheduledExecutorService scheduler, HedwigHubClientFactory hubClientFactory) {
        this.pm = pm;
        mySubId = ByteString.copyFromUtf8(SubscriptionStateUtils.HUB_SUBSCRIBER_PREFIX + cfg.getMyRegion());
        queue = new TopicOpQueuer(scheduler);
        for (final String hub : cfg.getRegions()) {
            clients.add(hubClientFactory.create(new HedwigSocketAddress(hub)));
        }
    }

    @Override
    public void onFirstLocalSubscribe(final ByteString topic, final boolean synchronous, final Callback<Void> cb) {
        // Whenever we acquire a topic due to a (local) subscribe, subscribe on
        // it to all the other regions (currently using simple all-to-all
        // topology).
        queue.pushAndMaybeRun(topic, queue.new AsynchronousOp<Void>(topic, cb, null) {
            @Override
            public void run() {
                Callback<Void> postCb = synchronous ? cb : CallbackUtils.logger(LOGGER, 
                        "all cross-region subscriptions succeeded", 
                        "at least one cross-region subscription failed");
                final Callback<Void> mcb = CallbackUtils.multiCallback(clients.size(), postCb, ctx);
                for (final HedwigHubClient client : clients) {
                    final HedwigSubscriber sub = client.getSubscriber();
                    sub.asyncSubscribe(topic, mySubId, CreateOrAttach.CREATE_OR_ATTACH, new Callback<Void>() {
                        @Override
                        public void operationFinished(Object ctx, Void resultOfOperation) {
                            if (LOGGER.isDebugEnabled())
                                LOGGER.debug("cross-region subscription done for topic " + topic.toStringUtf8());
                            try {
                                sub.startDelivery(topic, mySubId, new MessageHandler() {
                                    @Override
                                    public void deliver(final ByteString topic, ByteString subscriberId, Message msg,
                                    final Callback<Void> callback, final Object context) {
                                        // When messages are first published
                                        // locally, the PublishHandler sets the
                                        // source region in the Message.
                                        if (msg.hasSrcRegion()) {
                                            Message.newBuilder(msg).setMsgId(
                                                MessageSeqId.newBuilder(msg.getMsgId()).addRemoteComponents(
                                                    RegionSpecificSeqId.newBuilder().setRegion(
                                                        msg.getSrcRegion()).setSeqId(
                                                        msg.getMsgId().getLocalComponent())));
                                        }
                                        pm.persistMessage(new PersistRequest(topic, msg, new Callback<Long>() {
                                            @Override
                                            public void operationFinished(Object ctx, Long resultOfOperation) {
                                                if (LOGGER.isDebugEnabled())
                                                    LOGGER.debug("cross-region recv-fwd succeeded for topic "
                                                                 + topic.toStringUtf8());
                                                callback.operationFinished(context, null);
                                            }

                                            @Override
                                            public void operationFailed(Object ctx, PubSubException exception) {
                                                if (LOGGER.isDebugEnabled())
                                                    LOGGER.error("cross-region recv-fwd failed for topic "
                                                                 + topic.toStringUtf8(), exception);
                                                callback.operationFailed(context, exception);
                                            }
                                        }, null));
                                    }
                                });
                                if (LOGGER.isDebugEnabled())
                                    LOGGER.debug("cross-region start-delivery succeeded for topic "
                                                 + topic.toStringUtf8());
                                mcb.operationFinished(ctx, null);
                            } catch (PubSubException ex) {
                                if (LOGGER.isDebugEnabled())
                                    LOGGER.error(
                                        "cross-region start-delivery failed for topic " + topic.toStringUtf8(), ex);
                                mcb.operationFailed(ctx, ex);
                            }
                        }

                        @Override
                        public void operationFailed(Object ctx, PubSubException exception) {
                            if (LOGGER.isDebugEnabled())
                                LOGGER.error("cross-region subscribe failed for topic " + topic.toStringUtf8(),
                                             exception);
                            mcb.operationFailed(ctx, exception);
                        }
                    }, null);
                }
                if (!synchronous)
                    cb.operationFinished(null, null);
            }
        });

    }

    @Override
    public void onLastLocalUnsubscribe(final ByteString topic) {
        // TODO may want to ease up on the eager unsubscribe; this is dropping
        // cross-region subscriptions ASAP
        queue.pushAndMaybeRun(topic, queue.new AsynchronousOp<Void>(topic, new Callback<Void>() {

            @Override
            public void operationFinished(Object ctx, Void result) {
                if (LOGGER.isDebugEnabled())
                    LOGGER.debug("cross-region unsubscribes succeeded for topic " + topic.toStringUtf8());
            }

            @Override
            public void operationFailed(Object ctx, PubSubException exception) {
                if (LOGGER.isDebugEnabled())
                    LOGGER.error("cross-region unsubscribes failed for topic " + topic.toStringUtf8(), exception);
            }

        }, null) {
            @Override
            public void run() {
                Callback<Void> mcb = CallbackUtils.multiCallback(clients.size(), cb, ctx);
                for (final HedwigHubClient client : clients) {
                    client.getSubscriber().asyncUnsubscribe(topic, mySubId, mcb, null);
                }
            }
        });
    }

    // Method to shutdown and stop all of the cross-region Hedwig clients.
    public void stop() {
        for (HedwigHubClient client : clients) {
            client.close();
        }
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/ssl/SslServerContextFactory.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.ssl;

import java.security.KeyStore;

import javax.net.ssl.KeyManagerFactory;
import javax.net.ssl.SSLContext;

import org.apache.hedwig.client.ssl.SslContextFactory;
import org.apache.hedwig.server.common.ServerConfiguration;

public class SslServerContextFactory extends SslContextFactory {

    public SslServerContextFactory(ServerConfiguration cfg) {
        try {
            // Load our Java key store.
            KeyStore ks = KeyStore.getInstance("pkcs12");
            ks.load(cfg.getCertStream(), cfg.getPassword().toCharArray());

            // Like ssh-agent.
            KeyManagerFactory kmf = KeyManagerFactory.getInstance("SunX509");
            kmf.init(ks, cfg.getPassword().toCharArray());

            // Create the SSL context.
            ctx = SSLContext.getInstance("TLS");
            ctx.init(kmf.getKeyManagers(), getTrustManagers(), null);
        } catch (Exception ex) {
            throw new RuntimeException(ex);
        }
    }

    @Override
    protected boolean isClient() {
        return false;
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/subscriptions/AbstractSubscriptionManager.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.subscriptions;

import java.util.ArrayList;
import java.util.Map;
import java.util.Timer;
import java.util.TimerTask;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.atomic.AtomicInteger;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.protobuf.ByteString;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState;
import org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach;
import org.apache.hedwig.protoextensions.MessageIdUtils;
import org.apache.hedwig.protoextensions.SubscriptionStateUtils;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.common.TopicOpQueuer;
import org.apache.hedwig.server.persistence.PersistenceManager;
import org.apache.hedwig.server.topics.TopicManager;
import org.apache.hedwig.server.topics.TopicOwnershipChangeListener;
import org.apache.hedwig.util.Callback;
import org.apache.hedwig.util.CallbackUtils;

public abstract class AbstractSubscriptionManager implements SubscriptionManager, TopicOwnershipChangeListener {

    ServerConfiguration cfg;
    ConcurrentHashMap<ByteString, Map<ByteString, InMemorySubscriptionState>> top2sub2seq = new ConcurrentHashMap<ByteString, Map<ByteString, InMemorySubscriptionState>>();
    static Logger logger = LoggerFactory.getLogger(AbstractSubscriptionManager.class);

    TopicOpQueuer queuer;
    private final ArrayList<SubscriptionEventListener> listeners = new ArrayList<SubscriptionEventListener>();
    private final ConcurrentHashMap<ByteString, AtomicInteger> topic2LocalCounts = new ConcurrentHashMap<ByteString, AtomicInteger>();

    // Handle to the PersistenceManager for the server so we can pass along the
    // message consume pointers for each topic.
    private final PersistenceManager pm;
    // Timer for running a recurring thread task to get the minimum message
    // sequence ID for each topic that all subscribers for it have consumed
    // already. With that information, we can call the PersistenceManager to
    // update it on the messages that are safe to be garbage collected.
    private final Timer timer = new Timer(true);
    // In memory mapping of topics to the minimum consumed message sequence ID
    // for all subscribers to the topic.
    private final ConcurrentHashMap<ByteString, Long> topic2MinConsumedMessagesMap = new ConcurrentHashMap<ByteString, Long>();

    public AbstractSubscriptionManager(ServerConfiguration cfg, TopicManager tm, PersistenceManager pm,
                                       ScheduledExecutorService scheduler) {
        this.cfg = cfg;
        queuer = new TopicOpQueuer(scheduler);
        tm.addTopicOwnershipChangeListener(this);
        this.pm = pm;
        // Schedule the recurring MessagesConsumedTask only if a
        // PersistenceManager is passed.
        if (pm != null) {
            timer.schedule(new MessagesConsumedTask(), 0, cfg.getMessagesConsumedThreadRunInterval());
        }
    }

    /**
     * This is the Timer Task for finding out for each topic, what the minimum
     * consumed message by the subscribers are. This information is used to pass
     * along to the server's PersistenceManager so it can garbage collect older
     * topic messages that are no longer needed by the subscribers.
     */
    class MessagesConsumedTask extends TimerTask {
        /**
         * Implement the TimerTask's abstract run method.
         */
        @Override
        public void run() {
            // We are looping through relatively small in memory data structures
            // so it should be safe to run this fairly often.
            for (ByteString topic : top2sub2seq.keySet()) {
                final Map<ByteString, InMemorySubscriptionState> topicSubscriptions = top2sub2seq.get(topic);
                long minConsumedMessage = Long.MAX_VALUE;
                // Loop through all subscribers to the current topic to find the
                // minimum consumed message id. The consume pointers are
                // persisted lazily so we'll use the stale in-memory value
                // instead. This keeps things consistent in case of a server
                // crash.
                for (InMemorySubscriptionState curSubscription : topicSubscriptions.values()) {
                    if (curSubscription.getSubscriptionState().getMsgId().getLocalComponent() < minConsumedMessage)
                        minConsumedMessage = curSubscription.getSubscriptionState().getMsgId().getLocalComponent();
                }
                boolean callPersistenceManager = true;
                // Don't call the PersistenceManager if nobody is subscribed to
                // the topic yet, or the consume pointer has not changed since
                // the last time, or if this is the initial subscription.
                if (topicSubscriptions.isEmpty()
                        || (topic2MinConsumedMessagesMap.containsKey(topic) && topic2MinConsumedMessagesMap.get(topic) == minConsumedMessage)
                        || minConsumedMessage == 0) {
                    callPersistenceManager = false;
                }
                // Pass the new consume pointers to the PersistenceManager.
                if (callPersistenceManager) {
                    topic2MinConsumedMessagesMap.put(topic, minConsumedMessage);
                    pm.consumedUntil(topic, minConsumedMessage);
                }
            }
        }
    }

    private class AcquireOp extends TopicOpQueuer.AsynchronousOp<Void> {
        public AcquireOp(ByteString topic, Callback<Void> callback, Object ctx) {
            queuer.super(topic, callback, ctx);
        }

        @Override
        public void run() {
            if (top2sub2seq.containsKey(topic)) {
                cb.operationFinished(ctx, null);
            }

            readSubscriptions(topic, new Callback<Map<ByteString, InMemorySubscriptionState>>() {
                @Override
                public void operationFailed(Object ctx, PubSubException exception) {
                    cb.operationFailed(ctx, exception);
                }

                @Override
                public void operationFinished(final Object ctx,
                final Map<ByteString, InMemorySubscriptionState> resultOfOperation) {
                    // We've just inherited a bunch of subscriber for this
                    // topic, some of which may be local. If they are, then we
                    // need to (1) notify listeners of this and (2) record the
                    // number for bookkeeping so that future
                    // subscribes/unsubscribes can efficiently notify listeners.

                    // Count the number of local subscribers we just inherited.
                    // This loop is OK since the number of subscribers per topic
                    // is expected to be small.
                    int localCount = 0;
                    for (ByteString subscriberId : resultOfOperation.keySet())
                        if (!SubscriptionStateUtils.isHubSubscriber(subscriberId))
                            localCount++;
                    topic2LocalCounts.put(topic, new AtomicInteger(localCount));

                    // The final "commit" (and "abort") operations.
                    final Callback<Void> cb2 = new Callback<Void>() {

                        @Override
                        public void operationFailed(Object ctx, PubSubException exception) {
                            logger.error("Subscription manager failed to acquired topic " + topic.toStringUtf8(),
                                         exception);
                            cb.operationFailed(ctx, null);
                        }

                        @Override
                        public void operationFinished(Object ctx, Void voidObj) {
                            top2sub2seq.put(topic, resultOfOperation);
                            logger.info("Subscription manager successfully acquired topic: " + topic.toStringUtf8());
                            cb.operationFinished(ctx, null);
                        }

                    };

                    // Notify listeners if necessary.
                    if (localCount > 0) {
                        notifySubscribe(topic, false, cb2, ctx);
                    } else {
                        cb2.operationFinished(ctx, null);
                    }
                }

            }, ctx);

        }

    }

    private void notifySubscribe(ByteString topic, boolean synchronous, final Callback<Void> cb, final Object ctx) {
        Callback<Void> mcb = CallbackUtils.multiCallback(listeners.size(), cb, ctx);
        for (SubscriptionEventListener listener : listeners) {
            listener.onFirstLocalSubscribe(topic, synchronous, mcb);
        }
    }

    /**
     * Figure out who is subscribed. Do nothing if already acquired. If there's
     * an error reading the subscribers' sequence IDs, then the topic is not
     * acquired.
     *
     * @param topic
     * @param callback
     * @param ctx
     */
    @Override
    public void acquiredTopic(final ByteString topic, final Callback<Void> callback, Object ctx) {
        queuer.pushAndMaybeRun(topic, new AcquireOp(topic, callback, ctx));
    }

    /**
     * Remove the local mapping.
     */
    @Override
    public void lostTopic(ByteString topic) {
        top2sub2seq.remove(topic);
        AtomicInteger count = topic2LocalCounts.remove(topic);
        // Notify listeners if necessary.
        if (null != count && count.get() > 0)
            notifyUnsubcribe(topic);
    }

    private void notifyUnsubcribe(ByteString topic) {
        for (SubscriptionEventListener listener : listeners)
            listener.onLastLocalUnsubscribe(topic);
    }

    protected abstract void readSubscriptions(final ByteString topic,
            final Callback<Map<ByteString, InMemorySubscriptionState>> cb, final Object ctx);

    private class SubscribeOp extends TopicOpQueuer.AsynchronousOp<MessageSeqId> {
        SubscribeRequest subRequest;
        MessageSeqId consumeSeqId;

        public SubscribeOp(ByteString topic, SubscribeRequest subRequest, MessageSeqId consumeSeqId,
                           Callback<MessageSeqId> callback, Object ctx) {
            queuer.super(topic, callback, ctx);
            this.subRequest = subRequest;
            this.consumeSeqId = consumeSeqId;
        }

        @Override
        public void run() {

            final Map<ByteString, InMemorySubscriptionState> topicSubscriptions = top2sub2seq.get(topic);
            if (topicSubscriptions == null) {
                cb.operationFailed(ctx, new PubSubException.ServerNotResponsibleForTopicException(""));
                return;
            }

            final ByteString subscriberId = subRequest.getSubscriberId();
            InMemorySubscriptionState subscriptionState = topicSubscriptions.get(subscriberId);
            CreateOrAttach createOrAttach = subRequest.getCreateOrAttach();

            if (subscriptionState != null) {

                if (createOrAttach.equals(CreateOrAttach.CREATE)) {
                    String msg = "Topic: " + topic.toStringUtf8() + " subscriberId: " + subscriberId.toStringUtf8()
                                 + " requested creating a subscription but it is already subscribed with state: "
                                 + SubscriptionStateUtils.toString(subscriptionState.getSubscriptionState());
                    logger.debug(msg);
                    cb.operationFailed(ctx, new PubSubException.ClientAlreadySubscribedException(msg));
                    return;
                }

                // otherwise just attach
                if (logger.isDebugEnabled()) {
                    logger.debug("Topic: " + topic.toStringUtf8() + " subscriberId: " + subscriberId.toStringUtf8()
                                 + " attaching to subscription with state: "
                                 + SubscriptionStateUtils.toString(subscriptionState.getSubscriptionState()));
                }

                cb.operationFinished(ctx, subscriptionState.getLastConsumeSeqId());
                return;
            }

            // we don't have a mapping for this subscriber
            if (createOrAttach.equals(CreateOrAttach.ATTACH)) {
                String msg = "Topic: " + topic.toStringUtf8() + " subscriberId: " + subscriberId.toStringUtf8()
                             + " requested attaching to an existing subscription but it is not subscribed";
                logger.debug(msg);
                cb.operationFailed(ctx, new PubSubException.ClientNotSubscribedException(msg));
                return;
            }

            // now the hard case, this is a brand new subscription, must record
            final SubscriptionState newState = SubscriptionState.newBuilder().setMsgId(consumeSeqId).build();
            createSubscriptionState(topic, subscriberId, newState, new Callback<Void>() {
                @Override
                public void operationFailed(Object ctx, PubSubException exception) {
                    cb.operationFailed(ctx, exception);
                }

                @Override
                public void operationFinished(Object ctx, Void resultOfOperation) {
                    Callback<Void> cb2 = new Callback<Void>() {

                        @Override
                        public void operationFailed(Object ctx, PubSubException exception) {
                            logger.error("subscription for subscriber " + subscriberId.toStringUtf8() + " to topic "
                                         + topic.toStringUtf8() + " failed due to failed listener callback", exception);
                            cb.operationFailed(ctx, exception);
                        }

                        @Override
                        public void operationFinished(Object ctx, Void resultOfOperation) {
                            topicSubscriptions.put(subscriberId, new InMemorySubscriptionState(newState));
                            cb.operationFinished(ctx, consumeSeqId);
                        }

                    };

                    if (!SubscriptionStateUtils.isHubSubscriber(subRequest.getSubscriberId())
                            && topic2LocalCounts.get(topic).incrementAndGet() == 1)
                        notifySubscribe(topic, subRequest.getSynchronous(), cb2, ctx);
                    else
                        cb2.operationFinished(ctx, resultOfOperation);
                }
            }, ctx);
        }
    }

    @Override
    public void serveSubscribeRequest(ByteString topic, SubscribeRequest subRequest, MessageSeqId consumeSeqId,
                                      Callback<MessageSeqId> callback, Object ctx) {
        queuer.pushAndMaybeRun(topic, new SubscribeOp(topic, subRequest, consumeSeqId, callback, ctx));
    }

    private class ConsumeOp extends TopicOpQueuer.AsynchronousOp<Void> {
        ByteString subscriberId;
        MessageSeqId consumeSeqId;

        public ConsumeOp(ByteString topic, ByteString subscriberId, MessageSeqId consumeSeqId, Callback<Void> callback,
                         Object ctx) {
            queuer.super(topic, callback, ctx);
            this.subscriberId = subscriberId;
            this.consumeSeqId = consumeSeqId;
        }

        @Override
        public void run() {
            Map<ByteString, InMemorySubscriptionState> topicSubs = top2sub2seq.get(topic);
            if (topicSubs == null) {
                cb.operationFinished(ctx, null);
                return;
            }

            InMemorySubscriptionState subState = topicSubs.get(subscriberId);
            if (subState == null) {
                cb.operationFinished(ctx, null);
                return;
            }

            if (subState.setLastConsumeSeqId(consumeSeqId, cfg.getConsumeInterval())) {
                updateSubscriptionState(topic, subscriberId, subState.getSubscriptionState(), cb, ctx);
            } else {
                if (logger.isDebugEnabled()) {
                    logger.debug("Only advanced consume pointer in memory, will persist later, topic: "
                                 + topic.toStringUtf8() + " subscriberId: " + subscriberId.toStringUtf8()
                                 + " persistentState: " + SubscriptionStateUtils.toString(subState.getSubscriptionState())
                                 + " in-memory consume-id: "
                                 + MessageIdUtils.msgIdToReadableString(subState.getLastConsumeSeqId()));
                }
                cb.operationFinished(ctx, null);
            }

        }
    }

    @Override
    public void setConsumeSeqIdForSubscriber(ByteString topic, ByteString subscriberId, MessageSeqId consumeSeqId,
            Callback<Void> callback, Object ctx) {
        queuer.pushAndMaybeRun(topic, new ConsumeOp(topic, subscriberId, consumeSeqId, callback, ctx));
    }

    private class UnsubscribeOp extends TopicOpQueuer.AsynchronousOp<Void> {
        ByteString subscriberId;

        public UnsubscribeOp(ByteString topic, ByteString subscriberId, Callback<Void> callback, Object ctx) {
            queuer.super(topic, callback, ctx);
            this.subscriberId = subscriberId;
        }

        @Override
        public void run() {
            final Map<ByteString, InMemorySubscriptionState> topicSubscriptions = top2sub2seq.get(topic);
            if (topicSubscriptions == null) {
                cb.operationFailed(ctx, new PubSubException.ServerNotResponsibleForTopicException(""));
                return;
            }

            if (!topicSubscriptions.containsKey(subscriberId)) {
                cb.operationFailed(ctx, new PubSubException.ClientNotSubscribedException(""));
                return;
            }

            deleteSubscriptionState(topic, subscriberId, new Callback<Void>() {
                @Override
                public void operationFailed(Object ctx, PubSubException exception) {
                    cb.operationFailed(ctx, exception);
                }

                @Override
                public void operationFinished(Object ctx, Void resultOfOperation) {
                    topicSubscriptions.remove(subscriberId);
                    // Notify listeners if necessary.
                    if (!SubscriptionStateUtils.isHubSubscriber(subscriberId)
                            && topic2LocalCounts.get(topic).decrementAndGet() == 0)
                        notifyUnsubcribe(topic);
                    cb.operationFinished(ctx, null);
                }
            }, ctx);

        }

    }

    @Override
    public void unsubscribe(ByteString topic, ByteString subscriberId, Callback<Void> callback, Object ctx) {
        queuer.pushAndMaybeRun(topic, new UnsubscribeOp(topic, subscriberId, callback, ctx));
    }

    /**
     * Not thread-safe.
     */
    @Override
    public void addListener(SubscriptionEventListener listener) {
        listeners.add(listener);
    }

    /**
     * Method to stop this class gracefully including releasing any resources
     * used and stopping all threads spawned.
     */
    public void stop() {
        timer.cancel();
    }

    protected abstract void createSubscriptionState(final ByteString topic, ByteString subscriberId,
            SubscriptionState state, Callback<Void> callback, Object ctx);

    protected abstract void updateSubscriptionState(ByteString topic, ByteString subscriberId, SubscriptionState state,
            Callback<Void> callback, Object ctx);

    protected abstract void deleteSubscriptionState(ByteString topic, ByteString subscriberId, Callback<Void> callback,
            Object ctx);

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/subscriptions/AllToAllTopologyFilter.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.subscriptions;

import com.google.protobuf.ByteString;
import org.apache.hedwig.protocol.PubSubProtocol.Message;

public class AllToAllTopologyFilter implements MessageFilter {

    ByteString subscriberRegion;

    public AllToAllTopologyFilter(ByteString subscriberRegion) {
        this.subscriberRegion = subscriberRegion;
    }

    public boolean testMessage(Message message) {
        if (message.getSrcRegion().equals(subscriberRegion)) {
            return false;
        } else {
            return true;
        }
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/subscriptions/InMemorySubscriptionManager.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.subscriptions;

import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ScheduledExecutorService;

import com.google.protobuf.ByteString;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.persistence.PersistenceManager;
import org.apache.hedwig.server.topics.TopicManager;
import org.apache.hedwig.util.Callback;

public class InMemorySubscriptionManager extends AbstractSubscriptionManager {

    public InMemorySubscriptionManager(TopicManager tm, PersistenceManager pm, ServerConfiguration conf, ScheduledExecutorService scheduler) {
        super(conf, tm, pm, scheduler);
    }

    @Override
    protected void createSubscriptionState(ByteString topic, ByteString subscriberId, SubscriptionState state,
                                           Callback<Void> callback, Object ctx) {
        // nothing to do, in-memory info is already recorded by base class
        callback.operationFinished(ctx, null);
    }

    @Override
    protected void deleteSubscriptionState(ByteString topic, ByteString subscriberId, Callback<Void> callback,
                                           Object ctx) {
        // nothing to do, in-memory info is already deleted by base class
        callback.operationFinished(ctx, null);
    }

    @Override
    protected void updateSubscriptionState(ByteString topic, ByteString subscriberId, SubscriptionState state,
                                           Callback<Void> callback, Object ctx) {
        // nothing to do, in-memory info is already updated by base class
        callback.operationFinished(ctx, null);
    }

    @Override
    public void lostTopic(ByteString topic) {
        // Intentionally do nothing, so that we dont lose in-memory information
    }

    @Override
    protected void readSubscriptions(ByteString topic,
                                     Callback<Map<ByteString, InMemorySubscriptionState>> cb, Object ctx) {
        // Since we don't lose in-memory information on lostTopic, we can just
        // return that back
        Map<ByteString, InMemorySubscriptionState> topicSubs = top2sub2seq.get(topic);

        if (topicSubs != null) {
            cb.operationFinished(ctx, topicSubs);
        } else {
            cb.operationFinished(ctx, new ConcurrentHashMap<ByteString, InMemorySubscriptionState>());
        }

    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/subscriptions/InMemorySubscriptionState.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.subscriptions;

import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState;

public class InMemorySubscriptionState {
    SubscriptionState subscriptionState;
    MessageSeqId lastConsumeSeqId;

    public InMemorySubscriptionState(SubscriptionState subscriptionState, MessageSeqId lastConsumeSeqId) {
        this.subscriptionState = subscriptionState;
        this.lastConsumeSeqId = lastConsumeSeqId;
    }

    public InMemorySubscriptionState(SubscriptionState subscriptionState) {
        this(subscriptionState, subscriptionState.getMsgId());
    }

    public SubscriptionState getSubscriptionState() {
        return subscriptionState;
    }

    public MessageSeqId getLastConsumeSeqId() {
        return lastConsumeSeqId;
    }

    /**
     *
     * @param lastConsumeSeqId
     * @param consumeInterval
     *            The amount of laziness we want in persisting the consume
     *            pointers
     * @return true if the resulting structure needs to be persisted, false
     *         otherwise
     */
    public boolean setLastConsumeSeqId(MessageSeqId lastConsumeSeqId, int consumeInterval) {
        this.lastConsumeSeqId = lastConsumeSeqId;

        if (lastConsumeSeqId.getLocalComponent() - subscriptionState.getMsgId().getLocalComponent() < consumeInterval) {
            return false;
        }

        subscriptionState = SubscriptionState.newBuilder(subscriptionState).setMsgId(lastConsumeSeqId).build();
        return true;
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/subscriptions/MessageFilter.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.subscriptions;

import org.apache.hedwig.protocol.PubSubProtocol.Message;

public interface MessageFilter {

    /**
     * Tests whether a particular message passes the filter or not
     *
     * @param message
     * @return
     */
    public boolean testMessage(Message message);
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/subscriptions/SubscriptionEventListener.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.subscriptions;

import com.google.protobuf.ByteString;
import org.apache.hedwig.util.Callback;

/**
 * For listening to events that are issued by a SubscriptionManager.
 *
 */
public interface SubscriptionEventListener {

    /**
     * Called by the subscription manager when it previously had zero local
     * subscribers for a topic and is currently accepting its first local
     * subscriber.
     *
     * @param topic
     *            The topic of interest.
     * @param synchronous
     *            Whether this request was actually initiated by a new local
     *            subscriber, or whether it was an existing subscription
     *            inherited by the hub (e.g. when recovering the state from ZK).
     * @param cb
     *            The subscription will not complete until success is called on
     *            this callback. An error on cb will result in a subscription
     *            error.
     */
    public void onFirstLocalSubscribe(ByteString topic, boolean synchronous, Callback<Void> cb);

    /**
     * Called by the SubscriptionManager when it previously had non-zero local
     * subscribers for a topic and is currently dropping its last local
     * subscriber. This is fully asynchronous so there is no callback.
     *
     * @param topic
     *            The topic of interest.
     */
    public void onLastLocalUnsubscribe(ByteString topic);

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/subscriptions/SubscriptionManager.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.subscriptions;

import com.google.protobuf.ByteString;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest;
import org.apache.hedwig.util.Callback;

/**
 * All methods are thread-safe.
 */
public interface SubscriptionManager {

    /**
     *
     * Register a new subscription for the given subscriber for the given topic.
     * This method should reliably persist the existence of the subscription in
     * a way that it can't be lost. If the subscription already exists,
     * depending on the create or attach flag in the subscribe request, an
     * exception may be returned.
     *
     * This is an asynchronous method.
     *
     * @param topic
     * @param subRequest
     * @param consumeSeqId
     *            The seqId to start serving the subscription from, if this is a
     *            brand new subscription
     * @param callback
     *            The seq id returned by the callback is where serving should
     *            start from
     * @param ctx
     */
    public void serveSubscribeRequest(ByteString topic, SubscribeRequest subRequest, MessageSeqId consumeSeqId,
                                      Callback<MessageSeqId> callback, Object ctx);

    /**
     * Set the consume position of a given subscriber on a given topic. Note
     * that this method need not persist the consume position immediately but
     * can be lazy and persist it later asynchronously, if that is more
     * efficient.
     *
     * @param topic
     * @param subscriberId
     * @param consumeSeqId
     */
    public void setConsumeSeqIdForSubscriber(ByteString topic, ByteString subscriberId, MessageSeqId consumeSeqId,
            Callback<Void> callback, Object ctx);

    /**
     * Delete a particular subscription
     *
     * @param topic
     * @param subscriberId
     */
    public void unsubscribe(ByteString topic, ByteString subscriberId, Callback<Void> callback, Object ctx);

    // Management API methods that we will fill in later
    // /**
    // * Get the ids of all subscribers for a given topic
    // *
    // * @param topic
    // * @return A list of subscriber ids that are currently subscribed to the
    // * given topic
    // */
    // public List<ByteString> getSubscriptionsForTopic(ByteString topic);
    //
    // /**
    // * Get the topics to which a given subscriber is subscribed to
    // *
    // * @param subscriberId
    // * @return A list of the topics to which the given subscriber is
    // subscribed
    // * to
    // * @throws ServiceDownException
    // * If there is an error in looking up the subscription
    // * information
    // */
    // public List<ByteString> getTopicsForSubscriber(ByteString subscriberId)
    // throws ServiceDownException;

    /**
     * Add a listener that is notified when topic-subscription pairs are added
     * or removed.
     */
    public void addListener(SubscriptionEventListener listener);

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/subscriptions/TrueFilter.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.subscriptions;

import org.apache.hedwig.protocol.PubSubProtocol.Message;

public class TrueFilter implements MessageFilter {
    protected static TrueFilter instance = new TrueFilter();

    public static TrueFilter instance() {
        return instance;
    }

    public boolean testMessage(Message message) {
        return true;
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/subscriptions/ZkSubscriptionManager.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.subscriptions;

import java.util.List;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicInteger;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.ZooKeeper;
import org.apache.zookeeper.KeeperException.Code;
import org.apache.zookeeper.ZooDefs.Ids;
import org.apache.zookeeper.data.Stat;
import com.google.protobuf.ByteString;
import com.google.protobuf.InvalidProtocolBufferException;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState;
import org.apache.hedwig.protoextensions.SubscriptionStateUtils;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.persistence.PersistenceManager;
import org.apache.hedwig.server.topics.TopicManager;
import org.apache.hedwig.util.Callback;
import org.apache.hedwig.zookeeper.SafeAsyncZKCallback;
import org.apache.hedwig.zookeeper.ZkUtils;

public class ZkSubscriptionManager extends AbstractSubscriptionManager {

    ZooKeeper zk;

    protected final static Logger logger = LoggerFactory.getLogger(ZkSubscriptionManager.class);

    public ZkSubscriptionManager(ZooKeeper zk, TopicManager topicMgr, PersistenceManager pm, ServerConfiguration cfg,
                                 ScheduledExecutorService scheduler) {
        super(cfg, topicMgr, pm, scheduler);
        this.zk = zk;
    }

    private StringBuilder topicSubscribersPath(StringBuilder sb, ByteString topic) {
        return cfg.getZkTopicPath(sb, topic).append("/subscribers");
    }

    private String topicSubscriberPath(ByteString topic, ByteString subscriber) {
        return topicSubscribersPath(new StringBuilder(), topic).append("/").append(subscriber.toStringUtf8())
               .toString();
    }

    @Override
    protected void readSubscriptions(final ByteString topic,
                                     final Callback<Map<ByteString, InMemorySubscriptionState>> cb, final Object ctx) {

        String topicSubscribersPath = topicSubscribersPath(new StringBuilder(), topic).toString();
        zk.getChildren(topicSubscribersPath, false, new SafeAsyncZKCallback.ChildrenCallback() {
            @Override
            public void safeProcessResult(int rc, String path, final Object ctx, final List<String> children) {

                if (rc != Code.OK.intValue() && rc != Code.NONODE.intValue()) {
                    KeeperException e = ZkUtils.logErrorAndCreateZKException("Could not read subscribers for topic "
                                        + topic.toStringUtf8(), path, rc);
                    cb.operationFailed(ctx, new PubSubException.ServiceDownException(e));
                    return;
                }

                final Map<ByteString, InMemorySubscriptionState> topicSubs = new ConcurrentHashMap<ByteString, InMemorySubscriptionState>();

                if (rc == Code.NONODE.intValue() || children.size() == 0) {
                    if (logger.isDebugEnabled()) {
                        logger.debug("No subscriptions found while acquiring topic: " + topic.toStringUtf8());
                    }
                    cb.operationFinished(ctx, topicSubs);
                    return;
                }

                final AtomicBoolean failed = new AtomicBoolean();
                final AtomicInteger count = new AtomicInteger();

                for (final String child : children) {

                    final ByteString subscriberId = ByteString.copyFromUtf8(child);
                    final String childPath = path + "/" + child;

                    zk.getData(childPath, false, new SafeAsyncZKCallback.DataCallback() {
                        @Override
                        public void safeProcessResult(int rc, String path, Object ctx, byte[] data, Stat stat) {

                            if (rc != Code.OK.intValue()) {
                                KeeperException e = ZkUtils.logErrorAndCreateZKException(
                                                        "Could not read subscription data for topic: " + topic.toStringUtf8()
                                                        + ", subscriberId: " + subscriberId.toStringUtf8(), path, rc);
                                reportFailure(new PubSubException.ServiceDownException(e));
                                return;
                            }

                            if (failed.get()) {
                                return;
                            }

                            SubscriptionState state;

                            try {
                                state = SubscriptionState.parseFrom(data);
                            } catch (InvalidProtocolBufferException ex) {
                                String msg = "Failed to deserialize state for topic: " + topic.toStringUtf8()
                                             + " subscriberId: " + subscriberId.toStringUtf8();
                                logger.error(msg, ex);
                                reportFailure(new PubSubException.UnexpectedConditionException(msg));
                                return;
                            }

                            if (logger.isDebugEnabled()) {
                                logger.debug("Found subscription while acquiring topic: " + topic.toStringUtf8()
                                             + " subscriberId: " + child + "state: "
                                             + SubscriptionStateUtils.toString(state));
                            }

                            topicSubs.put(subscriberId, new InMemorySubscriptionState(state));
                            if (count.incrementAndGet() == children.size()) {
                                assert topicSubs.size() == count.get();
                                cb.operationFinished(ctx, topicSubs);
                            }
                        }

                        private void reportFailure(PubSubException e) {
                            if (failed.compareAndSet(false, true))
                                cb.operationFailed(ctx, e);
                        }
                    }, ctx);
                }
            }
        }, ctx);
    }

    @Override
    protected void createSubscriptionState(final ByteString topic, final ByteString subscriberId,
                                           final SubscriptionState state, final Callback<Void> callback, final Object ctx) {
        ZkUtils.createFullPathOptimistic(zk, topicSubscriberPath(topic, subscriberId), state.toByteArray(),
        Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT, new SafeAsyncZKCallback.StringCallback() {

            @Override
            public void safeProcessResult(int rc, String path, Object ctx, String name) {
                if (rc == Code.OK.intValue()) {
                    if (logger.isDebugEnabled()) {
                        logger.debug("Successfully recorded subscription for topic: " + topic.toStringUtf8()
                                     + " subscriberId: " + subscriberId.toStringUtf8() + " state: "
                                     + SubscriptionStateUtils.toString(state));
                    }
                    callback.operationFinished(ctx, null);
                } else {
                    KeeperException ke = ZkUtils.logErrorAndCreateZKException(
                                             "Could not record new subscription for topic: " + topic.toStringUtf8()
                                             + " subscriberId: " + subscriberId.toStringUtf8(), path, rc);
                    callback.operationFailed(ctx, new PubSubException.ServiceDownException(ke));
                }
            }
        }, ctx);
    }

    @Override
    protected void updateSubscriptionState(final ByteString topic, final ByteString subscriberId,
                                           final SubscriptionState state, final Callback<Void> callback, final Object ctx) {
        zk.setData(topicSubscriberPath(topic, subscriberId), state.toByteArray(), -1,
        new SafeAsyncZKCallback.StatCallback() {
            @Override
            public void safeProcessResult(int rc, String path, Object ctx, Stat stat) {
                if (rc != Code.OK.intValue()) {
                    KeeperException e = ZkUtils.logErrorAndCreateZKException("Topic: " + topic.toStringUtf8()
                                        + " subscriberId: " + subscriberId.toStringUtf8()
                                        + " could not set subscription state: " + SubscriptionStateUtils.toString(state),
                                        path, rc);
                    callback.operationFailed(ctx, new PubSubException.ServiceDownException(e));
                } else {
                    if (logger.isDebugEnabled()) {
                        logger.debug("Successfully updated subscription for topic: " + topic.toStringUtf8()
                                     + " subscriberId: " + subscriberId.toStringUtf8() + " state: "
                                     + SubscriptionStateUtils.toString(state));
                    }

                    callback.operationFinished(ctx, null);
                }
            }
        }, ctx);
    }

    @Override
    protected void deleteSubscriptionState(final ByteString topic, final ByteString subscriberId,
                                           final Callback<Void> callback, final Object ctx) {
        zk.delete(topicSubscriberPath(topic, subscriberId), -1, new SafeAsyncZKCallback.VoidCallback() {
            @Override
            public void safeProcessResult(int rc, String path, Object ctx) {
                if (rc == Code.OK.intValue()) {
                    if (logger.isDebugEnabled()) {
                        logger.debug("Successfully deleted subscription for topic: " + topic.toStringUtf8()
                                     + " subscriberId: " + subscriberId.toStringUtf8());
                    }

                    callback.operationFinished(ctx, null);
                    return;
                }

                KeeperException e = ZkUtils.logErrorAndCreateZKException("Topic: " + topic.toStringUtf8()
                                    + " subscriberId: " + subscriberId.toStringUtf8() + " failed to delete subscription", path, rc);
                callback.operationFailed(ctx, new PubSubException.ServiceDownException(e));
            }
        }, ctx);
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/topics/AbstractTopicManager.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.topics;

import java.net.UnknownHostException;
import java.util.ArrayList;
import java.util.Collections;
import java.util.HashSet;
import java.util.Set;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.protobuf.ByteString;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.common.TopicOpQueuer;
import org.apache.hedwig.util.Callback;
import org.apache.hedwig.util.CallbackUtils;
import org.apache.hedwig.util.HedwigSocketAddress;

public abstract class AbstractTopicManager implements TopicManager {
    /**
     * My name.
     */
    protected HedwigSocketAddress addr;

    /**
     * Topic change listeners.
     */
    protected ArrayList<TopicOwnershipChangeListener> listeners = new ArrayList<TopicOwnershipChangeListener>();

    /**
     * List of topics I believe I am responsible for.
     */
    protected Set<ByteString> topics = Collections.synchronizedSet(new HashSet<ByteString>());

    protected TopicOpQueuer queuer;
    protected ServerConfiguration cfg;
    protected ScheduledExecutorService scheduler;

    private static final Logger logger = LoggerFactory.getLogger(AbstractTopicManager.class);

    private class GetOwnerOp extends TopicOpQueuer.AsynchronousOp<HedwigSocketAddress> {
        public boolean shouldClaim;

        public GetOwnerOp(final ByteString topic, boolean shouldClaim,
                          final Callback<HedwigSocketAddress> cb, Object ctx) {
            queuer.super(topic, cb, ctx);
            this.shouldClaim = shouldClaim;
        }

        @Override
        public void run() {
            realGetOwner(topic, shouldClaim, cb, ctx);
        }
    }

    private class ReleaseOp extends TopicOpQueuer.AsynchronousOp<Void> {
        public ReleaseOp(ByteString topic, Callback<Void> cb, Object ctx) {
            queuer.super(topic, cb, ctx);
        }

        @Override
        public void run() {
            if (!topics.contains(topic)) {
                cb.operationFinished(ctx, null);
                return;
            }
            realReleaseTopic(topic, cb, ctx);
        }
    }

    public AbstractTopicManager(ServerConfiguration cfg, ScheduledExecutorService scheduler)
            throws UnknownHostException {
        this.cfg = cfg;
        this.queuer = new TopicOpQueuer(scheduler);
        this.scheduler = scheduler;
        addr = cfg.getServerAddr();
    }

    @Override
    public synchronized void addTopicOwnershipChangeListener(TopicOwnershipChangeListener listener) {
        listeners.add(listener);
    }

    protected final synchronized void notifyListenersAndAddToOwnedTopics(final ByteString topic,
            final Callback<HedwigSocketAddress> originalCallback, final Object originalContext) {

        Callback<Void> postCb = new Callback<Void>() {

            @Override
            public void operationFinished(Object ctx, Void resultOfOperation) {
                topics.add(topic);
                if (cfg.getRetentionSecs() > 0) {
                    scheduler.schedule(new Runnable() {
                        @Override
                        public void run() {
                            // Enqueue a release operation. (Recall that release
                            // doesn't "fail" even if the topic is missing.)
                            releaseTopic(topic, new Callback<Void>() {

                                @Override
                                public void operationFailed(Object ctx, PubSubException exception) {
                                    logger.error("failure that should never happen when periodically releasing topic "
                                                 + topic, exception);
                                }

                                @Override
                                public void operationFinished(Object ctx, Void resultOfOperation) {
                                    logger.debug("successful periodic release of topic " + topic);
                                }

                            }, null);
                        }
                    }, cfg.getRetentionSecs(), TimeUnit.SECONDS);
                }
                originalCallback.operationFinished(originalContext, addr);
            }

            @Override
            public void operationFailed(final Object ctx, final PubSubException exception) {
                // TODO: optimization: we can release this as soon as we experience the first error.
                Callback<Void> cb = new Callback<Void>() {
                    public void operationFinished(Object _ctx, Void _resultOfOperation) {
                        originalCallback.operationFailed(ctx, exception);
                    }
                    public void operationFailed(Object _ctx, PubSubException _exception) {
                        logger.error("Exception releasing topic", _exception);
                        originalCallback.operationFailed(ctx, exception);
                    }
                };
                
                realReleaseTopic(topic, cb, originalContext);
            }
        };

        Callback<Void> mcb = CallbackUtils.multiCallback(listeners.size(), postCb, null);
        for (TopicOwnershipChangeListener listener : listeners) {
            listener.acquiredTopic(topic, mcb, null);
        }
    }

    private void realReleaseTopic(ByteString topic, Callback<Void> callback, Object ctx) {
        for (TopicOwnershipChangeListener listener : listeners)
            listener.lostTopic(topic);
        topics.remove(topic);
        postReleaseCleanup(topic, callback, ctx);
    }

    @Override
    public final void getOwner(ByteString topic, boolean shouldClaim,
                               Callback<HedwigSocketAddress> cb, Object ctx) {
        queuer.pushAndMaybeRun(topic, new GetOwnerOp(topic, shouldClaim, cb, ctx));
    }

    @Override
    public final void releaseTopic(ByteString topic, Callback<Void> cb, Object ctx) {
        queuer.pushAndMaybeRun(topic, new ReleaseOp(topic, cb, ctx));
    }

    /**
     * This method should "return" the owner of the topic if one has been chosen
     * already. If there is no pre-chosen owner, either this hub or some other
     * should be chosen based on the shouldClaim parameter. If its ends up
     * choosing this hub as the owner, the {@code
     * AbstractTopicManager#notifyListenersAndAddToOwnedTopics(ByteString,
     * OperationCallback, Object)} method must be called.
     *
     */
    protected abstract void realGetOwner(ByteString topic, boolean shouldClaim,
                                         Callback<HedwigSocketAddress> cb, Object ctx);

    /**
     * The method should do any cleanup necessary to indicate to other hubs that
     * this topic has been released
     */
    protected abstract void postReleaseCleanup(ByteString topic, Callback<Void> cb, Object ctx);

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/topics/TopicManager.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.topics;

import com.google.protobuf.ByteString;
import org.apache.hedwig.exceptions.PubSubException.ServiceDownException;
import org.apache.hedwig.server.persistence.PersistenceManager;
import org.apache.hedwig.util.Callback;
import org.apache.hedwig.util.HedwigSocketAddress;

/**
 * An implementor of this interface is basically responsible for ensuring that
 * there is at most a single host responsible for a given topic at a given time.
 * Also, it is desirable that on a host failure, some other hosts in the cluster
 * claim responsibilities for the topics that were at the failed host. On
 * claiming responsibility for a topic, a host should call its
 * {@link TopicOwnershipChangeListener}.
 *
 */

public interface TopicManager {
    /**
     * Get the name of the host responsible for the given topic.
     *
     * @param topic
     *            The topic whose owner to get.
     * @param cb
     *            Callback.
     * @return The name of host responsible for the given topic
     * @throws ServiceDownException
     *             If there is an error looking up the information
     */
    public void getOwner(ByteString topic, boolean shouldClaim,
                         Callback<HedwigSocketAddress> cb, Object ctx);

    /**
     * Whenever the topic manager finds out that the set of topics owned by this
     * node has changed, it can notify a set of
     * {@link TopicOwnershipChangeListener} objects. Any component of the system
     * (e.g., the {@link PersistenceManager}) can listen for such changes by
     * implementing the {@link TopicOwnershipChangeListener} interface and
     * registering themselves with the {@link TopicManager} using this method.
     * It is important that the {@link TopicOwnershipChangeListener} reacts
     * immediately to such notifications, and with no blocking (because multiple
     * listeners might need to be informed and they are all informed by the same
     * thread).
     *
     * @param listener
     */
    public void addTopicOwnershipChangeListener(TopicOwnershipChangeListener listener);

    /**
     * Give up ownership of a topic. If I don't own it, do nothing.
     *
     * @throws ServiceDownException
     *             If there is an error in claiming responsibility for the topic
     */
    public void releaseTopic(ByteString topic, Callback<Void> cb, Object ctx);

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/topics/TopicOwnershipChangeListener.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.topics;

import com.google.protobuf.ByteString;
import org.apache.hedwig.util.Callback;

public interface TopicOwnershipChangeListener {

    public void acquiredTopic(ByteString topic, Callback<Void> callback, Object ctx);

    public void lostTopic(ByteString topic);
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/topics/TrivialOwnAllTopicManager.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.topics;

import java.net.UnknownHostException;
import java.util.concurrent.ScheduledExecutorService;

import com.google.protobuf.ByteString;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.util.Callback;
import org.apache.hedwig.util.HedwigSocketAddress;

public class TrivialOwnAllTopicManager extends AbstractTopicManager {

    public TrivialOwnAllTopicManager(ServerConfiguration cfg, ScheduledExecutorService scheduler)
            throws UnknownHostException {
        super(cfg, scheduler);
    }

    @Override
    protected void realGetOwner(ByteString topic, boolean shouldClaim,
                                Callback<HedwigSocketAddress> cb, Object ctx) {

        if (topics.contains(topic)) {
            cb.operationFinished(ctx, addr);
            return;
        }

        notifyListenersAndAddToOwnedTopics(topic, cb, ctx);
    }

    @Override
    protected void postReleaseCleanup(ByteString topic, Callback<Void> cb, Object ctx) {
        // No cleanup to do
        cb.operationFinished(ctx, null);
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/topics/ZkTopicManager.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.topics;

import java.net.UnknownHostException;
import java.util.List;
import java.util.Random;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.SynchronousQueue;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.WatchedEvent;
import org.apache.zookeeper.Watcher;
import org.apache.zookeeper.ZooKeeper;
import org.apache.zookeeper.KeeperException.Code;
import org.apache.zookeeper.ZooDefs.Ids;
import org.apache.zookeeper.data.Stat;

import com.google.protobuf.ByteString;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.util.Callback;
import org.apache.hedwig.util.ConcurrencyUtils;
import org.apache.hedwig.util.Either;
import org.apache.hedwig.util.HedwigSocketAddress;
import org.apache.hedwig.zookeeper.SafeAsyncZKCallback;
import org.apache.hedwig.zookeeper.ZkUtils;
import org.apache.hedwig.zookeeper.SafeAsyncZKCallback.DataCallback;
import org.apache.hedwig.zookeeper.SafeAsyncZKCallback.StatCallback;

/**
 * Topics are operated on in parallel as they are independent.
 *
 */
public class ZkTopicManager extends AbstractTopicManager implements TopicManager {

    static Logger logger = LoggerFactory.getLogger(ZkTopicManager.class);
    Random rand = new Random();

    /**
     * Persistent storage for topic metadata.
     */
    private ZooKeeper zk;
    String ephemeralNodePath;

    StatCallback loadReportingStatCallback = new StatCallback() {
        @Override
        public void safeProcessResult(int rc, String path, Object ctx, Stat stat) {
            if (rc != KeeperException.Code.OK.intValue()) {
                logger.warn("Failed to update load information in zk");
            }
        }
    };

    // Boolean flag indicating if we should suspend activity. If this is true,
    // all of the Ops put into the queuer will fail automatically.
    protected volatile boolean isSuspended = false;

    /**
     * Create a new topic manager. Pass in an active ZooKeeper client object.
     *
     * @param zk
     */
    public ZkTopicManager(final ZooKeeper zk, final ServerConfiguration cfg, ScheduledExecutorService scheduler)
            throws UnknownHostException, PubSubException {

        super(cfg, scheduler);
        this.zk = zk;
        this.ephemeralNodePath = cfg.getZkHostsPrefix(new StringBuilder()).append("/").append(addr).toString();

        zk.register(new Watcher() {
            @Override
            public void process(WatchedEvent event) {
                if (event.getType().equals(Watcher.Event.EventType.None)) {
                    if (event.getState().equals(Watcher.Event.KeeperState.Disconnected)) {
                        logger.warn("ZK client has been disconnected to the ZK server!");
                        isSuspended = true;
                    } else if (event.getState().equals(Watcher.Event.KeeperState.SyncConnected)) {
                        if (isSuspended) {
                            logger.info("ZK client has been reconnected to the ZK server!");
                        }
                        isSuspended = false;
                    }
                }
                // Check for expired connection.
                if (event.getState().equals(Watcher.Event.KeeperState.Expired)) {
                    logger.error("ZK client connection to the ZK server has expired!");
                    System.exit(1);
                }
            }
        });
        final SynchronousQueue<Either<Void, PubSubException>> queue = new SynchronousQueue<Either<Void, PubSubException>>();

        registerWithZookeeper(new Callback<Void>() {
            @Override
            public void operationFailed(Object ctx, PubSubException exception) {
                logger.error("Failed to register hub with zookeeper", exception);
                ConcurrencyUtils.put(queue, Either.of((Void) null, exception));
            }

            @Override
            public void operationFinished(Object ctx, Void resultOfOperation) {
                logger.info("Successfully registered hub with zookeeper");
                ConcurrencyUtils.put(queue, Either.of(resultOfOperation, (PubSubException) null));
            }
        }, null);

        PubSubException pse = ConcurrencyUtils.take(queue).right();

        if (pse != null) {
            throw pse;
        }
    }

    void registerWithZookeeper(final Callback<Void> callback, Object ctx) {

        ZkUtils.createFullPathOptimistic(zk, ephemeralNodePath, getCurrentLoadData(), Ids.OPEN_ACL_UNSAFE,
        CreateMode.EPHEMERAL, new SafeAsyncZKCallback.StringCallback() {

            @Override
            public void safeProcessResult(int rc, String path, Object ctx, String name) {
                if (rc == Code.OK.intValue()) {
                    callback.operationFinished(ctx, null);
                    return;
                }
                if (rc != Code.NODEEXISTS.intValue()) {
                    KeeperException ke = ZkUtils.logErrorAndCreateZKException(
                                             "Could not create ephemeral node to register hub", ephemeralNodePath, rc);
                    callback.operationFailed(ctx, new PubSubException.ServiceDownException(ke));
                    return;
                }

                logger.info("Found stale ephemeral node while registering hub with ZK, deleting it");

                // Node exists, lets try to delete it and retry
                zk.delete(ephemeralNodePath, -1, new SafeAsyncZKCallback.VoidCallback() {
                    @Override
                    public void safeProcessResult(int rc, String path, Object ctx) {
                        if (rc == Code.OK.intValue() || rc == Code.NONODE.intValue()) {
                            registerWithZookeeper(callback, ctx);
                            return;
                        }
                        KeeperException ke = ZkUtils.logErrorAndCreateZKException(
                                                 "Could not delete stale ephemeral node to register hub", ephemeralNodePath, rc);
                        callback.operationFailed(ctx, new PubSubException.ServiceDownException(ke));
                        return;

                    }
                }, ctx);

            }
        }, null);
    }

    String hubPath(ByteString topic) {
        return cfg.getZkTopicPath(new StringBuilder(), topic).append("/hub").toString();
    }

    @Override
    protected void realGetOwner(final ByteString topic, final boolean shouldClaim,
                                final Callback<HedwigSocketAddress> cb, final Object ctx) {
        // If operations are suspended due to a ZK client disconnect, just error
        // out this call and return.
        if (isSuspended) {
            cb.operationFailed(ctx, new PubSubException.ServiceDownException(
                                   "ZKTopicManager service is temporarily suspended!"));
            return;
        }

        if (topics.contains(topic)) {
            cb.operationFinished(ctx, addr);
            return;
        }

        new ZkGetOwnerOp(topic, shouldClaim, cb, ctx).read();
    }

    // Recursively call each other.
    class ZkGetOwnerOp {
        ByteString topic;
        boolean shouldClaim;
        Callback<HedwigSocketAddress> cb;
        Object ctx;
        String hubPath;

        public ZkGetOwnerOp(ByteString topic, boolean shouldClaim, Callback<HedwigSocketAddress> cb, Object ctx) {
            this.topic = topic;
            this.shouldClaim = shouldClaim;
            this.cb = cb;
            this.ctx = ctx;
            hubPath = hubPath(topic);

        }

        public void choose() {
            // Get the list of existing hosts
            String registeredHostsPath = cfg.getZkHostsPrefix(new StringBuilder()).toString();
            zk.getChildren(registeredHostsPath, false, new SafeAsyncZKCallback.ChildrenCallback() {
                @Override
                public void safeProcessResult(int rc, String path, Object ctx, List<String> children) {
                    if (rc != Code.OK.intValue()) {
                        KeeperException e = ZkUtils.logErrorAndCreateZKException(
                                                "Could not get list of available hubs", path, rc);
                        cb.operationFailed(ctx, new PubSubException.ServiceDownException(e));
                        return;
                    }
                    chooseLeastLoadedNode(children);
                }
            }, null);
        }

        public void chooseLeastLoadedNode(final List<String> children) {
            DataCallback dataCallback = new DataCallback() {
                int numResponses = 0;
                int minLoad = Integer.MAX_VALUE;
                String leastLoaded = null;

                @Override
                public void safeProcessResult(int rc, String path, Object ctx, byte[] data, Stat stat) {
                    synchronized (this) {
                        if (rc == KeeperException.Code.OK.intValue()) {
                            try {
                                int load = Integer.parseInt(new String(data));
                                if (logger.isDebugEnabled()) {
                                    logger.debug("Found server: " + ctx + " with load: " + load);
                                }
                                if (load < minLoad  || (load == minLoad && rand.nextBoolean())) {
                                    minLoad = load;
                                    leastLoaded = (String) ctx;
                                }
                            } catch (NumberFormatException e) {
                                logger.warn("Corrupted load information from hub:" + ctx);
                                // some corrupted data, we'll just ignore this
                                // hub
                            }
                        }
                        numResponses++;

                        if (numResponses == children.size()) {
                            if (leastLoaded == null) {
                                cb.operationFailed(ZkGetOwnerOp.this.ctx, new PubSubException.ServiceDownException(
                                                       "No hub available"));
                                return;
                            }
                            HedwigSocketAddress owner = new HedwigSocketAddress(leastLoaded);
                            if (owner.equals(addr)) {
                                claim();
                            } else {
                                cb.operationFinished(ZkGetOwnerOp.this.ctx, owner);
                            }
                        }
                    }

                }
            };

            for (String child : children) {
                zk.getData(cfg.getZkHostsPrefix(new StringBuilder()).append("/").append(child).toString(), false,
                           dataCallback, child);
            }
        }

        public void claimOrChoose() {
            if (shouldClaim)
                claim();
            else
                choose();
        }

        public void read() {
            zk.getData(hubPath, false, new SafeAsyncZKCallback.DataCallback() {
                @Override
                public void safeProcessResult(int rc, String path, Object ctx, byte[] data, Stat stat) {

                    if (rc == Code.NONODE.intValue()) {
                        claimOrChoose();
                        return;
                    }

                    if (rc != Code.OK.intValue()) {
                        KeeperException e = ZkUtils.logErrorAndCreateZKException("Could not read ownership for topic: "
                                            + topic.toStringUtf8(), path, rc);
                        cb.operationFailed(ctx, new PubSubException.ServiceDownException(e));
                        return;
                    }

                    // successfully did a read
                    HedwigSocketAddress owner = new HedwigSocketAddress(new String(data));
                    if (!owner.equals(addr)) {
                        if (logger.isDebugEnabled()) {
                            logger.debug("topic: " + topic.toStringUtf8() + " belongs to someone else: " + owner);
                        }
                        cb.operationFinished(ctx, owner);
                        return;
                    }

                    logger.info("Discovered stale self-node for topic: " + topic.toStringUtf8() + ", will delete it");

                    // we must have previously failed and left a
                    // residual ephemeral node here, so we must
                    // delete it (clean it up) and then
                    // re-create/re-acquire the topic.
                    zk.delete(hubPath, stat.getVersion(), new VoidCallback() {
                        @Override
                        public void processResult(int rc, String path, Object ctx) {
                            if (Code.OK.intValue() == rc || Code.NONODE.intValue() == rc) {
                                claimOrChoose();
                            } else {
                                KeeperException e = ZkUtils.logErrorAndCreateZKException(
                                                        "Could not delete self node for topic: " + topic.toStringUtf8(), path, rc);
                                cb.operationFailed(ctx, new PubSubException.ServiceDownException(e));
                            }
                        }
                    }, ctx);
                }
            }, ctx);
        }

        public void claim() {
            if (logger.isDebugEnabled()) {
                logger.debug("claiming topic: " + topic.toStringUtf8());
            }

            ZkUtils.createFullPathOptimistic(zk, hubPath, addr.toString().getBytes(), Ids.OPEN_ACL_UNSAFE,
            CreateMode.EPHEMERAL, new SafeAsyncZKCallback.StringCallback() {

                @Override
                public void safeProcessResult(int rc, String path, Object ctx, String name) {
                    if (rc == Code.OK.intValue()) {
                        if (logger.isDebugEnabled()) {
                            logger.debug("claimed topic: " + topic.toStringUtf8());
                        }
                        notifyListenersAndAddToOwnedTopics(topic, cb, ctx);
                        updateLoadInformation();
                    } else if (rc == Code.NODEEXISTS.intValue()) {
                        read();
                    } else {
                        KeeperException e = ZkUtils.logErrorAndCreateZKException(
                                                "Failed to create ephemeral node to claim ownership of topic: "
                                                + topic.toStringUtf8(), path, rc);
                        cb.operationFailed(ctx, new PubSubException.ServiceDownException(e));
                    }
                }
            }, ctx);
        }

    }

    byte[] getCurrentLoadData() {
        // For now, using the number of topics as an indicator of load
        // information
        return (topics.size() + "").getBytes();
    }

    void updateLoadInformation() {
        byte[] currentLoad = getCurrentLoadData();
        if (logger.isDebugEnabled()) {
            logger.debug("Reporting load of " + new String(currentLoad));
        }
        zk.setData(ephemeralNodePath, currentLoad, -1, loadReportingStatCallback, null);
    }

    @Override
    protected void postReleaseCleanup(final ByteString topic, final Callback<Void> cb, Object ctx) {

        zk.getData(hubPath(topic), false, new SafeAsyncZKCallback.DataCallback() {
            @Override
            public void safeProcessResult(int rc, String path, Object ctx, byte[] data, Stat stat) {
                if (rc == Code.NONODE.intValue()) {
                    // Node has somehow disappeared from under us, live with it
                    // since its a transient node
                    logger.warn("While deleting self-node for topic: " + topic.toStringUtf8() + ", node not found");
                    cb.operationFinished(ctx, null);
                    return;
                }

                if (rc != Code.OK.intValue()) {
                    KeeperException e = ZkUtils.logErrorAndCreateZKException(
                                            "Failed to delete self-ownership node for topic: " + topic.toStringUtf8(), path, rc);
                    cb.operationFailed(ctx, new PubSubException.ServiceDownException(e));
                    return;
                }

                HedwigSocketAddress owner = new HedwigSocketAddress(new String(data));
                if (!owner.equals(addr)) {
                    logger.warn("Wanted to delete self-node for topic: " + topic.toStringUtf8() + " but node for "
                                + owner + " found, leaving untouched");
                    // Not our node, someone else's, leave it alone
                    cb.operationFinished(ctx, null);
                    return;
                }

                zk.delete(path, stat.getVersion(), new SafeAsyncZKCallback.VoidCallback() {
                    @Override
                    public void safeProcessResult(int rc, String path, Object ctx) {
                        if (rc != Code.OK.intValue() && rc != Code.NONODE.intValue()) {
                            KeeperException e = ZkUtils
                                                .logErrorAndCreateZKException("Failed to delete self-ownership node for topic: "
                                                        + topic.toStringUtf8(), path, rc);
                            cb.operationFailed(ctx, new PubSubException.ServiceDownException(e));
                            return;
                        }

                        cb.operationFinished(ctx, null);
                    }
                }, ctx);
            }
        }, ctx);
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/zookeeper/SafeAsynBKCallback.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.zookeeper;

import java.util.Enumeration;

import org.apache.bookkeeper.client.AsyncCallback;
import org.apache.bookkeeper.client.LedgerEntry;
import org.apache.bookkeeper.client.LedgerHandle;


public class SafeAsynBKCallback extends SafeAsyncCallback {

    public static abstract class OpenCallback implements AsyncCallback.OpenCallback {
        @Override
        public void openComplete(int rc, LedgerHandle ledgerHandle, Object ctx) {
            try {
                safeOpenComplete(rc, ledgerHandle, ctx);
            } catch(Throwable t) {
                invokeUncaughtExceptionHandler(t);
            }
        }

        public abstract void safeOpenComplete(int rc, LedgerHandle ledgerHandle, Object ctx);

    }

    public static abstract class CloseCallback implements AsyncCallback.CloseCallback {
        @Override
        public void closeComplete(int rc, LedgerHandle ledgerHandle, Object ctx) {
            try {
                safeCloseComplete(rc, ledgerHandle, ctx);
            } catch(Throwable t) {
                invokeUncaughtExceptionHandler(t);
            }
        }

        public abstract void safeCloseComplete(int rc, LedgerHandle ledgerHandle, Object ctx) ;
    }

    public static abstract class ReadCallback implements AsyncCallback.ReadCallback {

        @Override
        public void readComplete(int rc, LedgerHandle lh, Enumeration<LedgerEntry> seq, Object ctx) {
            try {
                safeReadComplete(rc, lh, seq, ctx);
            } catch(Throwable t) {
                invokeUncaughtExceptionHandler(t);
            }

        }

        public abstract void safeReadComplete(int rc, LedgerHandle lh, Enumeration<LedgerEntry> seq, Object ctx);
    }

    public static abstract class CreateCallback implements AsyncCallback.CreateCallback {

        @Override
        public void createComplete(int rc, LedgerHandle lh, Object ctx) {
            try {
                safeCreateComplete(rc, lh, ctx);
            } catch(Throwable t) {
                invokeUncaughtExceptionHandler(t);
            }

        }

        public abstract void safeCreateComplete(int rc, LedgerHandle lh, Object ctx);


    }

    public static abstract class AddCallback implements AsyncCallback.AddCallback {

        @Override
        public void addComplete(int rc, LedgerHandle lh, long entryId, Object ctx) {
            try {
                safeAddComplete(rc, lh, entryId, ctx);
            } catch(Throwable t) {
                invokeUncaughtExceptionHandler(t);
            }
        }

        public abstract void safeAddComplete(int rc, LedgerHandle lh, long entryId, Object ctx);

    }

}

"
hedwig-server/src/main/java/org/apache/hedwig/zookeeper/SafeAsyncCallback.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.zookeeper;

import java.lang.Thread.UncaughtExceptionHandler;

import org.apache.hedwig.server.common.TerminateJVMExceptionHandler;

public class SafeAsyncCallback {
    protected static UncaughtExceptionHandler uncaughtExceptionHandler = new TerminateJVMExceptionHandler();

    public static void setUncaughtExceptionHandler(UncaughtExceptionHandler uncaughtExceptionHandler) {
        SafeAsyncCallback.uncaughtExceptionHandler = uncaughtExceptionHandler;
    }

    static void invokeUncaughtExceptionHandler(Throwable t) {
        Thread thread = Thread.currentThread();
        uncaughtExceptionHandler.uncaughtException(thread, t);
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/zookeeper/SafeAsyncZKCallback.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.zookeeper;

import java.util.List;

import org.apache.zookeeper.AsyncCallback;
import org.apache.zookeeper.data.ACL;
import org.apache.zookeeper.data.Stat;

public class SafeAsyncZKCallback extends SafeAsyncCallback {
    public static abstract class StatCallback implements AsyncCallback.StatCallback {
        public void processResult(int rc, String path, Object ctx, Stat stat) {
            try {
                safeProcessResult(rc, path, ctx, stat);
            } catch (Throwable t) {
                invokeUncaughtExceptionHandler(t);
            }
        }

        public abstract void safeProcessResult(int rc, String path, Object ctx, Stat stat);
    }

    public static abstract class DataCallback implements AsyncCallback.DataCallback {
        public void processResult(int rc, String path, Object ctx, byte data[], Stat stat) {
            try {
                safeProcessResult(rc, path, ctx, data, stat);
            } catch (Throwable t) {
                invokeUncaughtExceptionHandler(t);
            }
        }

        public abstract void safeProcessResult(int rc, String path, Object ctx, byte data[], Stat stat);
    }

    public static abstract class ACLCallback implements AsyncCallback.ACLCallback {
        public void processResult(int rc, String path, Object ctx, List<ACL> acl, Stat stat) {
            try {
                safeProcessResult(rc, path, ctx, acl, stat);
            } catch (Throwable t) {
                invokeUncaughtExceptionHandler(t);
            }
        }

        public abstract void safeProcessResult(int rc, String path, Object ctx, List<ACL> acl, Stat stat);
    }

    public static abstract class ChildrenCallback implements AsyncCallback.ChildrenCallback {
        public void processResult(int rc, String path, Object ctx, List<String> children) {
            try {
                safeProcessResult(rc, path, ctx, children);
            } catch (Throwable t) {
                invokeUncaughtExceptionHandler(t);
            }
        }

        public abstract void safeProcessResult(int rc, String path, Object ctx, List<String> children);
    }

    public static abstract class StringCallback implements AsyncCallback.StringCallback {
        public void processResult(int rc, String path, Object ctx, String name) {
            try {
                safeProcessResult(rc, path, ctx, name);
            } catch (Throwable t) {
                invokeUncaughtExceptionHandler(t);
            }
        }

        public abstract void safeProcessResult(int rc, String path, Object ctx, String name);
    }

    public static abstract class VoidCallback implements AsyncCallback.VoidCallback {
        public void processResult(int rc, String path, Object ctx) {
            try {
                safeProcessResult(rc, path, ctx);
            } catch (Throwable t) {
                invokeUncaughtExceptionHandler(t);
            }
        }

        public abstract void safeProcessResult(int rc, String path, Object ctx);
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/zookeeper/ZkUtils.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.zookeeper;

import java.util.List;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.apache.zookeeper.AsyncCallback;
import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.ZooKeeper;
import org.apache.zookeeper.KeeperException.Code;
import org.apache.zookeeper.data.ACL;

import org.apache.hedwig.util.PathUtils;

public class ZkUtils {

    static Logger logger = LoggerFactory.getLogger(ZkUtils.class);

    public static void createFullPathOptimistic(final ZooKeeper zk, final String originalPath, final byte[] data,
            final List<ACL> acl, final CreateMode createMode, final AsyncCallback.StringCallback callback,
            final Object ctx) {

        zk.create(originalPath, data, acl, createMode, new SafeAsyncZKCallback.StringCallback() {
            @Override
            public void safeProcessResult(int rc, String path, Object ctx, String name) {

                if (rc != Code.NONODE.intValue()) {
                    callback.processResult(rc, path, ctx, name);
                    return;
                }

                // Since I got a nonode, it means that my parents don't exist
                // create mode is persistent since ephemeral nodes can't be
                // parents
                ZkUtils.createFullPathOptimistic(zk, PathUtils.parent(originalPath), new byte[0], acl,
                CreateMode.PERSISTENT, new SafeAsyncZKCallback.StringCallback() {

                    @Override
                    public void safeProcessResult(int rc, String path, Object ctx, String name) {
                        if (rc == Code.OK.intValue() || rc == Code.NODEEXISTS.intValue()) {
                            // succeeded in creating the parent, now
                            // create the original path
                            ZkUtils.createFullPathOptimistic(zk, originalPath, data, acl, createMode, callback,
                                                             ctx);
                        } else {
                            callback.processResult(rc, path, ctx, name);
                        }
                    }
                }, ctx);
            }
        }, ctx);

    }

    public static KeeperException logErrorAndCreateZKException(String msg, String path, int rc) {
        KeeperException ke = KeeperException.create(Code.get(rc), path);
        logger.error(msg + ",zkPath: " + path, ke);
        return ke;
    }

}
"
