File,Bug,SRC
bookkeeper-benchmark/src/main/java/org/apache/bookkeeper/benchmark/BenchBookie.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */
package org.apache.bookkeeper.benchmark;

import java.net.InetSocketAddress;
import java.util.concurrent.Executors;

import java.io.IOException;

import org.apache.zookeeper.KeeperException;

import org.apache.bookkeeper.proto.BookieClient;
import org.apache.bookkeeper.proto.BookieProtocol;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.WriteCallback;
import org.apache.bookkeeper.util.OrderedSafeExecutor;
import org.apache.bookkeeper.client.BookKeeper;
import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.client.LedgerHandle;
import org.apache.bookkeeper.conf.ClientConfiguration;
import org.jboss.netty.buffer.ChannelBuffer;
import org.jboss.netty.buffer.ChannelBuffers;
import org.jboss.netty.channel.socket.ClientSocketChannelFactory;
import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory;

import org.apache.commons.cli.HelpFormatter;
import org.apache.commons.cli.Option;
import org.apache.commons.cli.Options;
import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.CommandLineParser;
import org.apache.commons.cli.PosixParser;
import org.apache.commons.cli.ParseException;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class BenchBookie {
    static Logger LOG = LoggerFactory.getLogger(BenchBookie.class);

    static class LatencyCallback implements WriteCallback {
        boolean complete;
        @Override
        synchronized public void writeComplete(int rc, long ledgerId, long entryId,
                InetSocketAddress addr, Object ctx) {
            if (rc != 0) {
                LOG.error("Got error " + rc);
            }
            complete = true;
            notifyAll();
        }
        synchronized public void resetComplete() {
            complete = false;
        }
        synchronized public void waitForComplete() throws InterruptedException {
            while(!complete) {
                wait();
            }
        }
    }

    static class ThroughputCallback implements WriteCallback {
        int count;
        int waitingCount = Integer.MAX_VALUE;
        synchronized public void writeComplete(int rc, long ledgerId, long entryId,
                InetSocketAddress addr, Object ctx) {
            if (rc != 0) {
                LOG.error("Got error " + rc);
            }
            count++;
            if (count >= waitingCount) {
                notifyAll();
            }
        }
        synchronized public void waitFor(int count) throws InterruptedException {
            while(this.count < count) {
                waitingCount = count;
                wait(1000);
            }
            waitingCount = Integer.MAX_VALUE;
        }
    }

    private static long getValidLedgerId(String zkServers)
            throws IOException, BKException, KeeperException, InterruptedException {
        BookKeeper bkc = null;
        LedgerHandle lh = null;
        long id = 0;
        try {
            bkc =new BookKeeper(zkServers);
            lh = bkc.createLedger(1, 1, BookKeeper.DigestType.CRC32,
                                  new byte[20]);
            id = lh.getId();
            return id;
        } finally {
            if (lh != null) { lh.close(); }
            if (bkc != null) { bkc.close(); }
        }
    }
    /**
     * @param args
     * @throws InterruptedException
     */
    public static void main(String[] args)
            throws InterruptedException, ParseException, IOException,
            BKException, KeeperException {
        Options options = new Options();
        options.addOption("host", true, "Hostname or IP of bookie to benchmark");
        options.addOption("port", true, "Port of bookie to benchmark (default 3181)");
        options.addOption("zookeeper", true, "Zookeeper ensemble, (default \"localhost:2181\")");
        options.addOption("size", true, "Size of message to send, in bytes (default 1024)");
        options.addOption("help", false, "This message");

        CommandLineParser parser = new PosixParser();
        CommandLine cmd = parser.parse(options, args);

        if (cmd.hasOption("help") || !cmd.hasOption("host")) {
            HelpFormatter formatter = new HelpFormatter();
            formatter.printHelp("BenchBookie <options>", options);
            System.exit(-1);
        }

        String addr = cmd.getOptionValue("host");
        int port = Integer.valueOf(cmd.getOptionValue("port", "3181"));
        int size = Integer.valueOf(cmd.getOptionValue("size", "1024"));
        String servers = cmd.getOptionValue("zookeeper", "localhost:2181");



        ClientSocketChannelFactory channelFactory
            = new NioClientSocketChannelFactory(Executors.newCachedThreadPool(), Executors
                                                .newCachedThreadPool());
        OrderedSafeExecutor executor = new OrderedSafeExecutor(1);

        ClientConfiguration conf = new ClientConfiguration();
        BookieClient bc = new BookieClient(conf, channelFactory, executor);
        LatencyCallback lc = new LatencyCallback();

        ThroughputCallback tc = new ThroughputCallback();
        int warmUpCount = 999;

        long ledger = getValidLedgerId(servers);
        for(long entry = 0; entry < warmUpCount; entry++) {
            ChannelBuffer toSend = ChannelBuffers.buffer(size);
            toSend.resetReaderIndex();
            toSend.resetWriterIndex();
            toSend.writeLong(ledger);
            toSend.writeLong(entry);
            toSend.writerIndex(toSend.capacity());
            bc.addEntry(new InetSocketAddress(addr, port), ledger, new byte[20],
                        entry, toSend, tc, null, BookieProtocol.FLAG_NONE);
        }
        LOG.info("Waiting for warmup");
        tc.waitFor(warmUpCount);

        ledger = getValidLedgerId(servers);
        LOG.info("Benchmarking latency");
        int entryCount = 5000;
        long startTime = System.nanoTime();
        for(long entry = 0; entry < entryCount; entry++) {
            ChannelBuffer toSend = ChannelBuffers.buffer(size);
            toSend.resetReaderIndex();
            toSend.resetWriterIndex();
            toSend.writeLong(ledger);
            toSend.writeLong(entry);
            toSend.writerIndex(toSend.capacity());
            lc.resetComplete();
            bc.addEntry(new InetSocketAddress(addr, port), ledger, new byte[20],
                        entry, toSend, lc, null, BookieProtocol.FLAG_NONE);
            lc.waitForComplete();
        }
        long endTime = System.nanoTime();
        LOG.info("Latency: " + (((double)(endTime-startTime))/((double)entryCount))/1000000.0);

        entryCount = 50000;

        ledger = getValidLedgerId(servers);
        LOG.info("Benchmarking throughput");
        startTime = System.currentTimeMillis();
        tc = new ThroughputCallback();
        for(long entry = 0; entry < entryCount; entry++) {
            ChannelBuffer toSend = ChannelBuffers.buffer(size);
            toSend.resetReaderIndex();
            toSend.resetWriterIndex();
            toSend.writeLong(ledger);
            toSend.writeLong(entry);
            toSend.writerIndex(toSend.capacity());
            bc.addEntry(new InetSocketAddress(addr, port), ledger, new byte[20],
                        entry, toSend, tc, null, BookieProtocol.FLAG_NONE);
        }
        tc.waitFor(entryCount);
        endTime = System.currentTimeMillis();
        LOG.info("Throughput: " + ((long)entryCount)*1000/(endTime-startTime));

        bc.close();
        channelFactory.releaseExternalResources();
        executor.shutdown();
    }

}
"
bookkeeper-benchmark/src/main/java/org/apache/bookkeeper/benchmark/BenchReadThroughputLatency.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */
package org.apache.bookkeeper.benchmark;

import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.conf.ClientConfiguration;
import org.apache.bookkeeper.client.BookKeeper;
import org.apache.bookkeeper.client.LedgerHandle;
import org.apache.bookkeeper.client.LedgerEntry;
import org.apache.bookkeeper.client.AsyncCallback.AddCallback;

import org.apache.zookeeper.ZooKeeper;
import org.apache.zookeeper.Watcher;
import org.apache.zookeeper.WatchedEvent;
import org.apache.zookeeper.Watcher.Event;

import java.util.Enumeration;
import java.util.Collections;
import java.util.Comparator;
import java.util.List;
import java.util.ArrayList;
import java.util.regex.Pattern;
import java.util.regex.Matcher;

import java.util.concurrent.CountDownLatch;
import java.util.concurrent.atomic.AtomicInteger;

import org.apache.commons.cli.HelpFormatter;
import org.apache.commons.cli.Option;
import org.apache.commons.cli.Options;
import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.CommandLineParser;
import org.apache.commons.cli.PosixParser;
import org.apache.commons.cli.ParseException;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class BenchReadThroughputLatency {
    static Logger LOG = LoggerFactory.getLogger(BenchReadThroughputLatency.class);

    private static final Pattern LEDGER_PATTERN = Pattern.compile("L([0-9]+)$");

    private static final Comparator<String> ZK_LEDGER_COMPARE = new Comparator<String>() {
        public int compare(String o1, String o2) {
            try {
                Matcher m1 = LEDGER_PATTERN.matcher(o1);
                Matcher m2 = LEDGER_PATTERN.matcher(o2);
                if (m1.find() && m2.find()) {
                    return Integer.valueOf(m1.group(1))
                        - Integer.valueOf(m2.group(1));
                } else {
                    return o1.compareTo(o2);
                }
            } catch (Throwable t) {
                return o1.compareTo(o2);
            }
        }
    };

    private static void readLedger(ClientConfiguration conf, long ledgerId, byte[] passwd) {
        LOG.info("Reading ledger {}", ledgerId);
        BookKeeper bk = null;
        long time = 0;
        long entriesRead = 0;
        long lastRead = 0;
        int nochange = 0;

        long absoluteLimit = 5000000;
        LedgerHandle lh = null;
        try {
            bk = new BookKeeper(conf);
            while (true) {
                lh = bk.openLedgerNoRecovery(ledgerId, BookKeeper.DigestType.CRC32, 
                                             passwd);
                long lastConfirmed = Math.min(lh.getLastAddConfirmed(), absoluteLimit);
                if (lastConfirmed == lastRead) {
                    nochange++;
                    if (nochange == 10) {
                        break;
                    } else {
                        Thread.sleep(1000);
                        continue;
                    }
                } else {
                    nochange = 0;
                }
                long starttime = System.nanoTime();

                while (lastRead < lastConfirmed) {
                    long nextLimit = lastRead + 100000;
                    long readTo = Math.min(nextLimit, lastConfirmed);
                    Enumeration<LedgerEntry> entries = lh.readEntries(lastRead+1, readTo);
                    lastRead = readTo;
                    while (entries.hasMoreElements()) {
                        LedgerEntry e = entries.nextElement();
                        entriesRead++;
                        if ((entriesRead % 10000) == 0) {
                            LOG.info("{} entries read", entriesRead);
                        }
                    }
                }
                long endtime = System.nanoTime();
                time += endtime - starttime;

                lh.close();
                lh = null;
                Thread.sleep(1000);
            }
        } catch (InterruptedException ie) {
            // ignore
        } catch (Exception e ) {
            LOG.error("Exception in reader", e);
        } finally {
            LOG.info("Read {} in {}ms", entriesRead, time/1000/1000);

            try {
                if (lh != null) {
                    lh.close();
                }
                if (bk != null) {
                    bk.close();
                }
            } catch (Exception e) {
                LOG.error("Exception closing stuff", e);
            }
        }
    }

    private static void usage(Options options) {
        HelpFormatter formatter = new HelpFormatter();
        formatter.printHelp("BenchReadThroughputLatency <options>", options);
    }

    public static void main(String[] args) throws Exception {
        Options options = new Options();
        options.addOption("ledger", true, "Ledger to read. If empty, read all ledgers which come available. " 
                          + " Cannot be used with -listen");
        options.addOption("listen", true, "Listen for creation of <arg> ledgers, and read each one fully");
        options.addOption("password", true, "Password used to access ledgers (default 'benchPasswd')");
        options.addOption("zookeeper", true, "Zookeeper ensemble, default \"localhost:2181\"");
        options.addOption("sockettimeout", true, "Socket timeout for bookkeeper client. In seconds. Default 5");
        options.addOption("help", false, "This message");

        CommandLineParser parser = new PosixParser();
        CommandLine cmd = parser.parse(options, args);

        if (cmd.hasOption("help")) {
            usage(options);
            System.exit(-1);
        }

        final String servers = cmd.getOptionValue("zookeeper", "localhost:2181");
        final byte[] passwd = cmd.getOptionValue("password", "benchPasswd").getBytes();
        final int sockTimeout = Integer.valueOf(cmd.getOptionValue("sockettimeout", "5"));
        if (cmd.hasOption("ledger") && cmd.hasOption("listen")) {
            LOG.error("Cannot used -ledger and -listen together");
            usage(options);
            System.exit(-1);
        }

        final AtomicInteger ledger = new AtomicInteger(0);
        final AtomicInteger numLedgers = new AtomicInteger(0);
        if (cmd.hasOption("ledger")) {
            ledger.set(Integer.valueOf(cmd.getOptionValue("ledger")));
        } else if (cmd.hasOption("listen")) {
            numLedgers.set(Integer.valueOf(cmd.getOptionValue("listen")));
        } else {
            LOG.error("You must use -ledger or -listen");
            usage(options);
            System.exit(-1);
        }

        final CountDownLatch shutdownLatch = new CountDownLatch(1);
        final CountDownLatch connectedLatch = new CountDownLatch(1);
        final String nodepath = String.format("/ledgers/L%010d", ledger.get());

        final ClientConfiguration conf = new ClientConfiguration();
        conf.setReadTimeout(sockTimeout).setZkServers(servers);


        final ZooKeeper zk = new ZooKeeper(servers, 3000, new Watcher() {
                public void process(WatchedEvent event) {
                    if (event.getState() == Event.KeeperState.SyncConnected
                            && event.getType() == Event.EventType.None) {
                        connectedLatch.countDown();
                    }
                }
            });
        try {
            zk.register(new Watcher() {
                    public void process(WatchedEvent event) {
                        try {
                            if (event.getState() == Event.KeeperState.SyncConnected 
                                && event.getType() == Event.EventType.None) {
                                connectedLatch.countDown();
                            } else if (event.getType() == Event.EventType.NodeCreated
                                       && event.getPath().equals(nodepath)) {
                                readLedger(conf, ledger.get(), passwd);
                                shutdownLatch.countDown();
                            } else if (event.getType() == Event.EventType.NodeChildrenChanged) {
                                if (numLedgers.get() < 0) {
                                    return;
                                }
                                List<String> children = zk.getChildren("/ledgers", true);
                                List<String> ledgers = new ArrayList<String>();
                                for (String child : children) {
                                    if (LEDGER_PATTERN.matcher(child).find()) {
                                        ledgers.add(child);
                                    }
                                }
                                Collections.sort(ledgers, ZK_LEDGER_COMPARE);
                                String last = ledgers.get(ledgers.size() - 1);
                                final Matcher m = LEDGER_PATTERN.matcher(last);
                                if (m.find()) {
                                    int ledgersLeft = numLedgers.decrementAndGet();
                                    Thread t = new Thread() {
                                            public void run() {
                                                readLedger(conf, Long.valueOf(m.group(1)), passwd);
                                            }
                                        };
                                    t.start();
                                    if (ledgersLeft <= 0) {
                                        shutdownLatch.countDown();
                                    }
                                } else {
                                    LOG.error("Cant file ledger id in {}", last);
                                }
                            } else {
                                LOG.warn("Unknown event {}", event);
                            }
                        } catch (Exception e) {
                            LOG.error("Exception in watcher", e);
                        }
                    }
                });
            connectedLatch.await();
            if (ledger.get() != 0) {
                if (zk.exists(nodepath, true) != null) {
                    readLedger(conf, ledger.get(), passwd);
                    shutdownLatch.countDown();
                } else {
                    LOG.info("Watching for creation of" + nodepath);
                }
            } else {
                zk.getChildren("/ledgers", true);
            }
            shutdownLatch.await();
            LOG.info("Shutting down");
        } finally {
            zk.close();
        }
    }
}"
bookkeeper-benchmark/src/main/java/org/apache/bookkeeper/benchmark/BenchThroughputLatency.java,false,"/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */
package org.apache.bookkeeper.benchmark;

import java.io.BufferedOutputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.OutputStream;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collections;
import java.util.Random;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.Semaphore;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicLong;
import java.util.Timer;
import java.util.TimerTask;

import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.client.BookKeeper;
import org.apache.bookkeeper.client.LedgerHandle;
import org.apache.bookkeeper.client.AsyncCallback.AddCallback;
import org.apache.bookkeeper.conf.ClientConfiguration;
import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.CommandLineParser;
import org.apache.commons.cli.HelpFormatter;
import org.apache.commons.cli.Options;
import org.apache.commons.cli.ParseException;
import org.apache.commons.cli.PosixParser;
import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.WatchedEvent;
import org.apache.zookeeper.Watcher;
import org.apache.zookeeper.ZooDefs;
import org.apache.zookeeper.ZooKeeper;
import org.apache.zookeeper.Watcher.Event.EventType;
import org.apache.zookeeper.Watcher.Event.KeeperState;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class BenchThroughputLatency implements AddCallback, Runnable {
    static Logger LOG = LoggerFactory.getLogger(BenchThroughputLatency.class);

    BookKeeper bk;
    LedgerHandle lh[];
    AtomicLong counter;

    Semaphore sem;
    int numberOfLedgers = 1;
    final int sendLimit;
    final long latencies[];

    static class Context {
        long localStartTime;
        long id;

        Context(long id, long time){
            this.id = id;
            this.localStartTime = time;
        }
    }

    public BenchThroughputLatency(int ensemble, int writeQuorumSize, int ackQuorumSize, byte[] passwd,
            int numberOfLedgers, int sendLimit, ClientConfiguration conf)
            throws KeeperException, IOException, InterruptedException {
        this.sem = new Semaphore(conf.getThrottleValue());
        bk = new BookKeeper(conf);
        this.counter = new AtomicLong(0);
        this.numberOfLedgers = numberOfLedgers;
        this.sendLimit = sendLimit;
        this.latencies = new long[sendLimit];
        try{
            lh = new LedgerHandle[this.numberOfLedgers];

            for(int i = 0; i < this.numberOfLedgers; i++) {
                lh[i] = bk.createLedger(ensemble, writeQuorumSize,
                                        ackQuorumSize,
                                        BookKeeper.DigestType.CRC32,
                                        passwd);
                LOG.debug("Ledger Handle: " + lh[i].getId());
            }
        } catch (BKException e) {
            e.printStackTrace();
        }
    }

    Random rand = new Random();
    public void close() throws InterruptedException, BKException {
        for(int i = 0; i < numberOfLedgers; i++) {
            lh[i].close();
        }
        bk.close();
    }

    long previous = 0;
    byte bytes[];

    void setEntryData(byte data[]) {
        bytes = data;
    }

    int lastLedger = 0;
    private int getRandomLedger() {
         return rand.nextInt(numberOfLedgers);
    }

    int latencyIndex = -1;
    AtomicLong completedRequests = new AtomicLong(0);

    long duration = -1;
    synchronized public long getDuration() {
        return duration;
    }

    public void run() {
        LOG.info("Running...");
        long start = previous = System.currentTimeMillis();

        int sent = 0;

        Thread reporter = new Thread() {
                public void run() {
                    try {
                        while(true) {
                            Thread.sleep(1000);
                            LOG.info("ms: {} req: {}", System.currentTimeMillis(), completedRequests.getAndSet(0));
                        }
                    } catch (InterruptedException ie) {
                        LOG.info("Caught interrupted exception, going away");
                    }
                }
            };
        reporter.start();
        long beforeSend = System.nanoTime();

        while(!Thread.currentThread().isInterrupted() && sent < sendLimit) {
            try {
                sem.acquire();
                if (sent == 10000) {
                    long afterSend = System.nanoTime();
                    long time = afterSend - beforeSend;
                    LOG.info("Time to send first batch: {}s {}ns ",
                             time/1000/1000/1000, time);
                }
            } catch (InterruptedException e) {
                break;
            }

            final int index = getRandomLedger();
            LedgerHandle h = lh[index];
            if (h == null) {
                LOG.error("Handle " + index + " is null!");
            } else {
                long nanoTime = System.nanoTime();
                lh[index].asyncAddEntry(bytes, this, new Context(sent, nanoTime));
                counter.incrementAndGet();
            }
            sent++;
        }
        LOG.info("Sent: "  + sent);
        try {
            int i = 0;
            while(this.counter.get() > 0) {
                Thread.sleep(1000);
                i++;
                if (i > 30) {
                    break;
                }
            }
        } catch(InterruptedException e) {
            LOG.error("Interrupted while waiting", e);
        }
        synchronized(this) {
            duration = System.currentTimeMillis() - start;
        }
        throughput = sent*1000/getDuration();

        reporter.interrupt();
        try {
            reporter.join();
        } catch (InterruptedException ie) {
            // ignore
        }
        LOG.info("Finished processing in ms: " + getDuration() + " tp = " + throughput);
    }

    long throughput = -1;
    public long getThroughput() {
        return throughput;
    }

    long threshold = 20000;
    long runningAverageCounter = 0;
    long totalTime = 0;
    @Override
    public void addComplete(int rc, LedgerHandle lh, long entryId, Object ctx) {
        Context context = (Context) ctx;

        // we need to use the id passed in the context in the case of
        // multiple ledgers, and it works even with one ledger
        entryId = context.id;
        long newTime = System.nanoTime() - context.localStartTime;

        sem.release();
        counter.decrementAndGet();

        if (rc == 0) {
            latencies[(int)entryId] = newTime;
            completedRequests.incrementAndGet();
        }
    }

    public static void main(String[] args)
            throws KeeperException, IOException, InterruptedException, ParseException, BKException {
        Options options = new Options();
        options.addOption("time", true, "Running time (seconds), default 60");
        options.addOption("entrysize", true, "Entry size (bytes), default 1024");
        options.addOption("ensemble", true, "Ensemble size, default 3");
        options.addOption("quorum", true, "Quorum size, default 2");
        options.addOption("ackQuorum", true, "Ack quorum size, default is same as quorum");
        options.addOption("throttle", true, "Max outstanding requests, default 10000");
        options.addOption("ledgers", true, "Number of ledgers, default 1");
        options.addOption("zookeeper", true, "Zookeeper ensemble, default \"localhost:2181\"");
        options.addOption("password", true, "Password used to create ledgers (default 'benchPasswd')");
        options.addOption("coordnode", true, "Coordination znode for multi client benchmarks (optional)");
        options.addOption("timeout", true, "Number of seconds after which to give up");
        options.addOption("sockettimeout", true, "Socket timeout for bookkeeper client. In seconds. Default 5");
        options.addOption("skipwarmup", false, "Skip warm up, default false");
        options.addOption("sendlimit", true, "Max number of entries to send. Default 20000000");
        options.addOption("latencyFile", true, "File to dump latencies. Default is latencyDump.dat");
        options.addOption("help", false, "This message");

        CommandLineParser parser = new PosixParser();
        CommandLine cmd = parser.parse(options, args);

        if (cmd.hasOption("help")) {
            HelpFormatter formatter = new HelpFormatter();
            formatter.printHelp("BenchThroughputLatency <options>", options);
            System.exit(-1);
        }

        long runningTime = Long.valueOf(cmd.getOptionValue("time", "60"));
        String servers = cmd.getOptionValue("zookeeper", "localhost:2181");
        int entrysize = Integer.valueOf(cmd.getOptionValue("entrysize", "1024"));

        int ledgers = Integer.valueOf(cmd.getOptionValue("ledgers", "1"));
        int ensemble = Integer.valueOf(cmd.getOptionValue("ensemble", "3"));
        int quorum = Integer.valueOf(cmd.getOptionValue("quorum", "2"));
        int ackQuorum = quorum;
        if (cmd.hasOption("ackQuorum")) {
            ackQuorum = Integer.valueOf(cmd.getOptionValue("ackQuorum"));
        }
        int throttle = Integer.valueOf(cmd.getOptionValue("throttle", "10000"));
        int sendLimit = Integer.valueOf(cmd.getOptionValue("sendlimit", "20000000"));

        final int sockTimeout = Integer.valueOf(cmd.getOptionValue("sockettimeout", "5"));

        String coordinationZnode = cmd.getOptionValue("coordnode");
        final byte[] passwd = cmd.getOptionValue("password", "benchPasswd").getBytes();

        String latencyFile = cmd.getOptionValue("latencyFile", "latencyDump.dat");

        Timer timeouter = new Timer();
        if (cmd.hasOption("timeout")) {
            final long timeout = Long.valueOf(cmd.getOptionValue("timeout", "360")) * 1000;

            timeouter.schedule(new TimerTask() {
                    public void run() {
                        System.err.println("Timing out benchmark after " + timeout + "ms");
                        System.exit(-1);
                    }
                }, timeout);
        }

        LOG.warn("(Parameters received) running time: " + runningTime +
                ", entry size: " + entrysize + ", ensemble size: " + ensemble +
                ", quorum size: " + quorum +
                ", throttle: " + throttle +
                ", number of ledgers: " + ledgers +
                ", zk servers: " + servers +
                ", latency file: " + latencyFile);

        long totalTime = runningTime*1000;

        // Do a warmup run
        Thread thread;

        byte data[] = new byte[entrysize];
        Arrays.fill(data, (byte)'x');

        ClientConfiguration conf = new ClientConfiguration();
        conf.setThrottleValue(throttle).setReadTimeout(sockTimeout).setZkServers(servers);

        if (!cmd.hasOption("skipwarmup")) {
            long throughput;
            LOG.info("Starting warmup");

            throughput = warmUp(data, ledgers, ensemble, quorum, passwd, conf);
            LOG.info("Warmup tp: " + throughput);
            LOG.info("Warmup phase finished");
        }


        // Now do the benchmark
        BenchThroughputLatency bench = new BenchThroughputLatency(ensemble, quorum, ackQuorum,
                passwd, ledgers, sendLimit, conf);
        bench.setEntryData(data);
        thread = new Thread(bench);
        ZooKeeper zk = null;

        if (coordinationZnode != null) {
            final CountDownLatch connectLatch = new CountDownLatch(1);
            zk = new ZooKeeper(servers, 15000, new Watcher() {
                    @Override
                    public void process(WatchedEvent event) {
                        if (event.getState() == KeeperState.SyncConnected) {
                            connectLatch.countDown();
                        }
                    }});
            if (!connectLatch.await(10, TimeUnit.SECONDS)) {
                LOG.error("Couldn't connect to zookeeper at " + servers);
                zk.close();
                System.exit(-1);
            }

            final CountDownLatch latch = new CountDownLatch(1);
            LOG.info("Waiting for " + coordinationZnode);
            if (zk.exists(coordinationZnode, new Watcher() {
                @Override
                public void process(WatchedEvent event) {
                    if (event.getType() == EventType.NodeCreated) {
                        latch.countDown();
                    }
                }}) != null) {
                latch.countDown();
            }
            latch.await();
            LOG.info("Coordination znode created");
        }
        thread.start();
        Thread.sleep(totalTime);
        thread.interrupt();
        thread.join();

        LOG.info("Calculating percentiles");

        int numlat = 0;
        for(int i = 0; i < bench.latencies.length; i++) {
            if (bench.latencies[i] > 0) {
                numlat++;
            }
        }
        int numcompletions = numlat;
        numlat = Math.min(bench.sendLimit, numlat);
        long[] latency = new long[numlat];
        int j =0;
        for(int i = 0; i < bench.latencies.length && j < numlat; i++) {
            if (bench.latencies[i] > 0) {
                latency[j++] = bench.latencies[i];
            }
        }
        Arrays.sort(latency);

        long tp = (long)((double)(numcompletions*1000.0)/(double)bench.getDuration());

        LOG.info(numcompletions + " completions in " + bench.getDuration() + " seconds: " + tp + " ops/sec");

        if (zk != null) {
            zk.create(coordinationZnode + "/worker-",
                      ("tp " + tp + " duration " + bench.getDuration()).getBytes(),
                      ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT_SEQUENTIAL);
            zk.close();
        }

        // dump the latencies for later debugging (it will be sorted by entryid)
        OutputStream fos = new BufferedOutputStream(new FileOutputStream(latencyFile));

        for(Long l: latency) {
            fos.write((Long.toString(l)+"\t"+(l/1000000)+ "ms\n").getBytes());
        }
        fos.flush();
        fos.close();

        // now get the latencies
        LOG.info("99th percentile latency: {}", percentile(latency, 99));
        LOG.info("95th percentile latency: {}", percentile(latency, 95));

        bench.close();
        timeouter.cancel();
    }

    private static double percentile(long[] latency, int percentile) {
        int size = latency.length;
        int sampleSize = (size * percentile) / 100;
        long total = 0;
        int count = 0;
        for(int i = 0; i < sampleSize; i++) {
            total += latency[i];
            count++;
        }
        return ((double)total/(double)count)/1000000.0;
    }

    private static long warmUp(byte[] data, int ledgers, int ensemble, int qSize,
                               byte[] passwd, ClientConfiguration conf)
            throws KeeperException, IOException, InterruptedException, BKException {
        final CountDownLatch connectLatch = new CountDownLatch(1);
        final int bookies;
        String bookieRegistrationPath = conf.getZkAvailableBookiesPath();
        ZooKeeper zk = null;
        try {
            final String servers = conf.getZkServers();
            zk = new ZooKeeper(servers, 15000, new Watcher() {
                    @Override
                    public void process(WatchedEvent event) {
                        if (event.getState() == KeeperState.SyncConnected) {
                            connectLatch.countDown();
                        }
                    }});
            if (!connectLatch.await(10, TimeUnit.SECONDS)) {
                LOG.error("Couldn't connect to zookeeper at " + servers);
                throw new IOException("Couldn't connect to zookeeper " + servers);
            }
            bookies = zk.getChildren(bookieRegistrationPath, false).size();
        } finally {
            if (zk != null) {
                zk.close();
            }
        }

        BenchThroughputLatency warmup = new BenchThroughputLatency(bookies, bookies, bookies, passwd,
                                                                   ledgers, 10000, conf);
        warmup.setEntryData(data);
        Thread thread = new Thread(warmup);
        thread.start();
        thread.join();
        warmup.close();
        return warmup.getThroughput();
    }
}
"
bookkeeper-benchmark/src/main/java/org/apache/bookkeeper/benchmark/MySqlClient.java,false,"package org.apache.bookkeeper.benchmark;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.FileOutputStream;
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.SQLException;
import java.sql.Statement;
import java.util.HashMap;

import org.apache.bookkeeper.client.BookKeeper;
import org.apache.bookkeeper.client.LedgerHandle;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


import org.apache.zookeeper.KeeperException;

public class MySqlClient {
    static Logger LOG = LoggerFactory.getLogger(MySqlClient.class);

    BookKeeper x;
    LedgerHandle lh;
    Integer entryId;
    HashMap<Integer, Integer> map;

    FileOutputStream fStream;
    FileOutputStream fStreamLocal;
    long start, lastId;
    Connection con;
    Statement stmt;


    public MySqlClient(String hostport, String user, String pass)
            throws ClassNotFoundException {
        entryId = 0;
        map = new HashMap<Integer, Integer>();
        Class.forName("com.mysql.jdbc.Driver");
        // database is named "bookkeeper"
        String url = "jdbc:mysql://" + hostport + "/bookkeeper";
        try {
            con = DriverManager.getConnection(url, user, pass);
            stmt = con.createStatement();
            // drop table and recreate it
            stmt.execute("DROP TABLE IF EXISTS data;");
            stmt.execute("create table data(transaction_id bigint PRIMARY KEY AUTO_INCREMENT, content TEXT);");
            LOG.info("Database initialization terminated");
        } catch (SQLException e) {

            // TODO Auto-generated catch block
            e.printStackTrace();
        }
    }

    public void closeHandle() throws KeeperException, InterruptedException, SQLException {
        con.close();
    }
    /**
     * First parameter is an integer defining the length of the message
     * Second parameter is the number of writes
     * Third parameter is host:port
     * Fourth parameter is username
     * Fifth parameter is password
     * @param args
     * @throws ClassNotFoundException
     * @throws SQLException
     */
    public static void main(String[] args) throws ClassNotFoundException, SQLException {
        int lenght = Integer.parseInt(args[1]);
        StringBuilder sb = new StringBuilder();
        while(lenght-- > 0) {
            sb.append('a');
        }
        try {
            MySqlClient c = new MySqlClient(args[2], args[3], args[4]);
            c.writeSameEntryBatch(sb.toString().getBytes(), Integer.parseInt(args[0]));
            c.writeSameEntry(sb.toString().getBytes(), Integer.parseInt(args[0]));
            c.closeHandle();
        } catch (NumberFormatException e) {
            e.printStackTrace();
        } catch (InterruptedException e) {
            e.printStackTrace();
        } catch (KeeperException e) {
            e.printStackTrace();
        }

    }

    /**
     * 	Adds  data entry to the DB
     * 	@param data 	the entry to be written, given as a byte array
     * 	@param times	the number of times the entry should be written on the DB	*/
    void writeSameEntryBatch(byte[] data, int times) throws InterruptedException, SQLException {
        start = System.currentTimeMillis();
        int count = times;
        String content = new String(data);
        System.out.println("Data: " + content + ", " + data.length);
        while(count-- > 0) {
            stmt.addBatch("insert into data(content) values(\"" + content + "\");");
        }
        LOG.info("Finished writing batch SQL command in ms: " + (System.currentTimeMillis() - start));
        start = System.currentTimeMillis();
        stmt.executeBatch();
        System.out.println("Finished " + times + " writes in ms: " + (System.currentTimeMillis() - start));
        LOG.info("Ended computation");
    }

    void writeSameEntry(byte[] data, int times) throws InterruptedException, SQLException {
        start = System.currentTimeMillis();
        int count = times;
        String content = new String(data);
        System.out.println("Data: " + content + ", " + data.length);
        while(count-- > 0) {
            stmt.executeUpdate("insert into data(content) values(\"" + content + "\");");
        }
        System.out.println("Finished " + times + " writes in ms: " + (System.currentTimeMillis() - start));
        LOG.info("Ended computation");
    }

}
"
bookkeeper-benchmark/src/main/java/org/apache/bookkeeper/benchmark/TestClient.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */
package org.apache.bookkeeper.benchmark;

import java.io.FileOutputStream;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.Random;
import java.util.Timer;
import java.util.TimerTask;
import java.util.concurrent.Callable;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.Future;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicLong;

import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.client.BookKeeper;
import org.apache.bookkeeper.client.LedgerHandle;
import org.apache.bookkeeper.client.AsyncCallback.AddCallback;
import org.apache.bookkeeper.client.BookKeeper.DigestType;
import org.apache.bookkeeper.conf.ClientConfiguration;
import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.CommandLineParser;
import org.apache.commons.cli.HelpFormatter;
import org.apache.commons.cli.Options;
import org.apache.commons.cli.ParseException;
import org.apache.commons.cli.PosixParser;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.zookeeper.KeeperException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


/**
 * This is a simple test program to compare the performance of writing to
 * BookKeeper and to the local file system.
 *
 */

public class TestClient {
    private static final Logger LOG = LoggerFactory.getLogger(TestClient.class);

    /**
     * First says if entries should be written to BookKeeper (0) or to the local
     * disk (1). Second parameter is an integer defining the length of a ledger entry.
     * Third parameter is the number of writes.
     *
     * @param args
     */
    public static void main(String[] args) throws ParseException {
        Options options = new Options();
        options.addOption("length", true, "Length of packets being written. Default 1024");
        options.addOption("target", true, "Target medium to write to. Options are bk, fs & hdfs. Default fs");
        options.addOption("runfor", true, "Number of seconds to run for. Default 60");
        options.addOption("path", true, "Path to write to. fs & hdfs only. Default /foobar");
        options.addOption("zkservers", true, "ZooKeeper servers, comma separated. bk only. Default localhost:2181.");
        options.addOption("bkensemble", true, "BookKeeper ledger ensemble size. bk only. Default 3");
        options.addOption("bkquorum", true, "BookKeeper ledger quorum size. bk only. Default 2");
        options.addOption("bkthrottle", true, "BookKeeper throttle size. bk only. Default 10000");
        options.addOption("sync", false, "Use synchronous writes with BookKeeper. bk only.");
        options.addOption("numconcurrent", true, "Number of concurrently clients. Default 1");
        options.addOption("timeout", true, "Number of seconds after which to give up");
        options.addOption("help", false, "This message");

        CommandLineParser parser = new PosixParser();
        CommandLine cmd = parser.parse(options, args);

        if (cmd.hasOption("help")) {
            HelpFormatter formatter = new HelpFormatter();
            formatter.printHelp("TestClient <options>", options);
            System.exit(-1);
        }

        int length = Integer.valueOf(cmd.getOptionValue("length", "1024"));
        String target = cmd.getOptionValue("target", "fs");
        long runfor = Long.valueOf(cmd.getOptionValue("runfor", "60")) * 1000;

        StringBuilder sb = new StringBuilder();
        while(length-- > 0) {
            sb.append('a');
        }

        Timer timeouter = new Timer();
        if (cmd.hasOption("timeout")) {
            final long timeout = Long.valueOf(cmd.getOptionValue("timeout", "360")) * 1000;

            timeouter.schedule(new TimerTask() {
                    public void run() {
                        System.err.println("Timing out benchmark after " + timeout + "ms");
                        System.exit(-1);
                    }
                }, timeout);
        }

        BookKeeper bkc = null;
        try {
            int numFiles = Integer.valueOf(cmd.getOptionValue("numconcurrent", "1"));
            int numThreads = Math.min(numFiles, 1000);
            byte[] data = sb.toString().getBytes();
            long runid = System.currentTimeMillis();
            List<Callable<Long>> clients = new ArrayList<Callable<Long>>();

            if (target.equals("bk")) {
                String zkservers = cmd.getOptionValue("zkservers", "localhost:2181");
                int bkensemble = Integer.valueOf(cmd.getOptionValue("bkensemble", "3"));
                int bkquorum = Integer.valueOf(cmd.getOptionValue("bkquorum", "2"));
                int bkthrottle = Integer.valueOf(cmd.getOptionValue("bkthrottle", "10000"));

                ClientConfiguration conf = new ClientConfiguration();
                conf.setThrottleValue(bkthrottle);
                conf.setZkServers(zkservers);

                bkc = new BookKeeper(conf);
                List<LedgerHandle> handles = new ArrayList<LedgerHandle>();
                for (int i = 0; i < numFiles; i++) {
                    handles.add(bkc.createLedger(bkensemble, bkquorum, DigestType.CRC32, new byte[] {'a', 'b'}));
                }
                for (int i = 0; i < numFiles; i++) {
                    clients.add(new BKClient(handles, data, runfor, cmd.hasOption("sync")));
                }
            } else if (target.equals("hdfs")) {
                FileSystem fs = FileSystem.get(new Configuration());
                LOG.info("Default replication for HDFS: {}", fs.getDefaultReplication());

                List<FSDataOutputStream> streams = new ArrayList<FSDataOutputStream>();
                for (int i = 0; i < numFiles; i++) {
                    String path = cmd.getOptionValue("path", "/foobar");
                    streams.add(fs.create(new Path(path + runid + "_" + i)));
                }

                for (int i = 0; i < numThreads; i++) {
                    clients.add(new HDFSClient(streams, data, runfor));
                }
            } else if (target.equals("fs")) {
                List<FileOutputStream> streams = new ArrayList<FileOutputStream>();
                for (int i = 0; i < numFiles; i++) {
                    String path = cmd.getOptionValue("path", "/foobar " + i);
                    streams.add(new FileOutputStream(path + runid + "_" + i));
                }

                for (int i = 0; i < numThreads; i++) {
                    clients.add(new FileClient(streams, data, runfor));
                }
            } else {
                LOG.error("Unknown option: " + target);
                throw new IllegalArgumentException("Unknown target " + target);
            }

            ExecutorService executor = Executors.newFixedThreadPool(numThreads);
            long start = System.currentTimeMillis();

            List<Future<Long>> results = executor.invokeAll(clients,
                                                            10, TimeUnit.MINUTES);
            long end = System.currentTimeMillis();
            long count = 0;
            for (Future<Long> r : results) {
                if (!r.isDone()) {
                    LOG.warn("Job didn't complete");
                    System.exit(2);
                }
                long c = r.get();
                if (c == 0) {
                    LOG.warn("Task didn't complete");
                }
                count += c;
            }
            long time = end-start;
            LOG.info("Finished processing writes (ms): {} TPT: {} op/s",
                     time, count/((double)time/1000));
            executor.shutdown();
        } catch (ExecutionException ee) {
            LOG.error("Exception in worker", ee);
        }  catch (KeeperException ke) {
            LOG.error("Error accessing zookeeper", ke);
        } catch (BKException e) {
            LOG.error("Error accessing bookkeeper", e);
        } catch (IOException ioe) {
            LOG.error("I/O exception during benchmark", ioe);
        } catch (InterruptedException ie) {
            LOG.error("Benchmark interrupted", ie);
        } finally {
            if (bkc != null) {
                try {
                    bkc.close();
                } catch (BKException bke) {
                    LOG.error("Error closing bookkeeper client", bke);
                } catch (InterruptedException ie) {
                    LOG.warn("Interrupted closing bookkeeper client", ie);
                }
            }
        }
        timeouter.cancel();
    }

    static class HDFSClient implements Callable<Long> {
        final List<FSDataOutputStream> streams;
        final byte[] data;
        final long time;
        final Random r;

        HDFSClient(List<FSDataOutputStream> streams, byte[] data, long time) {
            this.streams = streams;
            this.data = data;
            this.time = time;
            this.r = new Random(System.identityHashCode(this));
        }

        public Long call() {
            try {
                long count = 0;
                long start = System.currentTimeMillis();
                long stopat = start + time;
                while(System.currentTimeMillis() < stopat) {
                    FSDataOutputStream stream = streams.get(r.nextInt(streams.size()));
                    synchronized(stream) {
                        stream.write(data);
                        stream.flush();
                        stream.hflush();
                    }
                    count++;
                }

                long time = (System.currentTimeMillis() - start);
                LOG.info("Worker finished processing writes (ms): {} TPT: {} op/s",
                         time, count/((double)time/1000));
                return count;
            } catch(IOException ioe) {
                LOG.error("Exception in worker thread", ioe);
                return 0L;
            }
        }
    }

    static class FileClient implements Callable<Long> {
        final List<FileOutputStream> streams;
        final byte[] data;
        final long time;
        final Random r;

        FileClient(List<FileOutputStream> streams, byte[] data, long time) {
            this.streams = streams;
            this.data = data;
            this.time = time;
            this.r = new Random(System.identityHashCode(this));
        }

        public Long call() {
            try {
                long count = 0;
                long start = System.currentTimeMillis();

                long stopat = start + time;
                while(System.currentTimeMillis() < stopat) {
                    FileOutputStream stream = streams.get(r.nextInt(streams.size()));
                    synchronized(stream) {
                        stream.write(data);
                        stream.flush();
                        stream.getChannel().force(false);
                    }
                    count++;
                }

                long time = (System.currentTimeMillis() - start);
                LOG.info("Worker finished processing writes (ms): {} TPT: {} op/s", time, count/((double)time/1000));
                return count;
            } catch(IOException ioe) {
                LOG.error("Exception in worker thread", ioe);
                return 0L;
            }
        }
    }

    static class BKClient implements Callable<Long>, AddCallback {
        final List<LedgerHandle> handles;
        final byte[] data;
        final long time;
        final Random r;
        final boolean sync;
        final AtomicLong success = new AtomicLong(0);
        final AtomicLong outstanding = new AtomicLong(0);

        BKClient(List<LedgerHandle> handles, byte[] data, long time, boolean sync) {
            this.handles = handles;
            this.data = data;
            this.time = time;
            this.r = new Random(System.identityHashCode(this));
            this.sync = sync;
        }

        public Long call() {
            try {
                long start = System.currentTimeMillis();

                long stopat = start + time;
                while(System.currentTimeMillis() < stopat) {
                    LedgerHandle lh = handles.get(r.nextInt(handles.size()));
                    if (sync) {
                        lh.addEntry(data);
                        success.incrementAndGet();
                    } else {
                        lh.asyncAddEntry(data, this, null);
                        outstanding.incrementAndGet();
                    }
                }

                int ticks = 10; // don't wait for more than 10 seconds
                while (outstanding.get() > 0 && ticks-- > 0) {
                    Thread.sleep(10);
                }

                long time = (System.currentTimeMillis() - start);
                LOG.info("Worker finished processing writes (ms): {} TPT: {} op/s",
                         time, success.get()/((double)time/1000));
                return success.get();
            } catch (BKException e) {
                LOG.error("Exception in worker thread", e);
                return 0L;
            } catch (InterruptedException ie) {
                LOG.error("Exception in worker thread", ie);
                return 0L;
            }
        }

        @Override
        public void addComplete(int rc, LedgerHandle lh, long entryId, Object ctx) {
            if (rc == BKException.Code.OK) {
                success.incrementAndGet();
            }
            outstanding.decrementAndGet();
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/Bookie.java,true,"/**
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

import java.io.File;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.io.FilenameFilter;
import java.net.InetAddress;
import java.net.InetSocketAddress;
import java.net.UnknownHostException;
import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Map;
import java.util.HashMap;
import java.util.List;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.Future;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;

import org.apache.bookkeeper.meta.LedgerManager;
import org.apache.bookkeeper.meta.LedgerManagerFactory;
import org.apache.bookkeeper.bookie.BookieException;
import org.apache.bookkeeper.bookie.GarbageCollectorThread.SafeEntryAdder;
import org.apache.bookkeeper.bookie.Journal.JournalScanner;
import org.apache.bookkeeper.bookie.LedgerDirsManager.LedgerDirsListener;
import org.apache.bookkeeper.bookie.LedgerDirsManager.NoWritableLedgerDirException;
import org.apache.bookkeeper.conf.ServerConfiguration;
import org.apache.bookkeeper.jmx.BKMBeanInfo;
import org.apache.bookkeeper.jmx.BKMBeanRegistry;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.WriteCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;
import org.apache.bookkeeper.util.BookKeeperConstants;
import org.apache.bookkeeper.util.IOUtils;
import org.apache.bookkeeper.util.MathUtils;
import org.apache.bookkeeper.util.ZkUtils;
import org.apache.bookkeeper.util.StringUtils;
import org.apache.bookkeeper.zookeeper.ZooKeeperWatcherBase;
import org.apache.commons.io.FileUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.KeeperException.NodeExistsException;
import org.apache.zookeeper.WatchedEvent;
import org.apache.zookeeper.Watcher;
import org.apache.zookeeper.ZooKeeper;
import org.apache.zookeeper.ZooDefs.Ids;
import org.apache.zookeeper.Watcher.Event.EventType;

import com.google.common.annotations.VisibleForTesting;

/**
 * Implements a bookie.
 *
 */

public class Bookie extends Thread {

    static Logger LOG = LoggerFactory.getLogger(Bookie.class);

    final File journalDirectory;
    final ServerConfiguration conf;

    final SyncThread syncThread;
    final LedgerManagerFactory ledgerManagerFactory;
    final LedgerManager ledgerManager;
    final LedgerStorage ledgerStorage;
    final Journal journal;

    final HandleFactory handles;

    static final long METAENTRY_ID_LEDGER_KEY = -0x1000;
    static final long METAENTRY_ID_FENCE_KEY  = -0x2000;

    // ZK registration path for this bookie
    private final String bookieRegistrationPath;

    private LedgerDirsManager ledgerDirsManager;

    // ZooKeeper client instance for the Bookie
    ZooKeeper zk;

    // Running flag
    private volatile boolean running = false;
    // Flag identify whether it is in shutting down progress
    private volatile boolean shuttingdown = false;

    private int exitCode = ExitCode.OK;

    // jmx related beans
    BookieBean jmxBookieBean;
    BKMBeanInfo jmxLedgerStorageBean;

    Map<Long, byte[]> masterKeyCache = Collections.synchronizedMap(new HashMap<Long, byte[]>());

    final private String zkBookieRegPath;

    final private AtomicBoolean readOnly = new AtomicBoolean(false);

    public static class NoLedgerException extends IOException {
        private static final long serialVersionUID = 1L;
        private long ledgerId;
        public NoLedgerException(long ledgerId) {
            super("Ledger " + ledgerId + " not found");
            this.ledgerId = ledgerId;
        }
        public long getLedgerId() {
            return ledgerId;
        }
    }
    public static class NoEntryException extends IOException {
        private static final long serialVersionUID = 1L;
        private long ledgerId;
        private long entryId;
        public NoEntryException(long ledgerId, long entryId) {
            this("Entry " + entryId + " not found in " + ledgerId, ledgerId, entryId);
        }

        public NoEntryException(String msg, long ledgerId, long entryId) {
            super(msg);
            this.ledgerId = ledgerId;
            this.entryId = entryId;
        }

        public long getLedger() {
            return ledgerId;
        }
        public long getEntry() {
            return entryId;
        }
    }

    // Write Callback do nothing
    static class NopWriteCallback implements WriteCallback {
        @Override
        public void writeComplete(int rc, long ledgerId, long entryId,
                                  InetSocketAddress addr, Object ctx) {
            if (LOG.isDebugEnabled()) {
                LOG.debug("Finished writing entry {} @ ledger {} for {} : {}",
                          new Object[] { entryId, ledgerId, addr, rc });
            }
        }
    }

    final static Future<Boolean> SUCCESS_FUTURE = new Future<Boolean>() {
        @Override
        public boolean cancel(boolean mayInterruptIfRunning) { return false; }
        @Override
        public Boolean get() { return true; }
        @Override
        public Boolean get(long timeout, TimeUnit unit) { return true; }
        @Override
        public boolean isCancelled() { return false; }
        @Override
        public boolean isDone() {
            return true;
        }
    };

    static class CountDownLatchFuture<T> implements Future<T> {

        T value = null;
        volatile boolean done = false;
        CountDownLatch latch = new CountDownLatch(1);

        @Override
        public boolean cancel(boolean mayInterruptIfRunning) { return false; }
        @Override
        public T get() throws InterruptedException {
            latch.await();
            return value;
        }
        @Override
        public T get(long timeout, TimeUnit unit) throws InterruptedException {
            latch.await(timeout, unit);
            return value;
        }

        @Override
        public boolean isCancelled() { return false; }

        @Override
        public boolean isDone() {
            return done;
        }

        void setDone(T value) {
            this.value = value;
            done = true;
            latch.countDown();
        }
    }

    static class FutureWriteCallback implements WriteCallback {

        CountDownLatchFuture<Boolean> result =
            new CountDownLatchFuture<Boolean>();

        @Override
        public void writeComplete(int rc, long ledgerId, long entryId,
                                  InetSocketAddress addr, Object ctx) {
            if (LOG.isDebugEnabled()) {
                LOG.debug("Finished writing entry {} @ ledger {} for {} : {}",
                          new Object[] { entryId, ledgerId, addr, rc });
            }
            result.setDone(0 == rc);
        }

        public Future<Boolean> getResult() {
            return result;
        }
    }

    /**
     * SyncThread is a background thread which flushes ledger index pages periodically.
     * Also it takes responsibility of garbage collecting journal files.
     *
     * <p>
     * Before flushing, SyncThread first records a log marker {journalId, journalPos} in memory,
     * which indicates entries before this log marker would be persisted to ledger files.
     * Then sync thread begins flushing ledger index pages to ledger index files, flush entry
     * logger to ensure all entries persisted to entry loggers for future reads.
     * </p>
     * <p>
     * After all data has been persisted to ledger index files and entry loggers, it is safe
     * to persist the log marker to disk. If bookie failed after persist log mark,
     * bookie is able to relay journal entries started from last log mark without losing
     * any entries.
     * </p>
     * <p>
     * Those journal files whose id are less than the log id in last log mark, could be
     * removed safely after persisting last log mark. We provide a setting to let user keeping
     * number of old journal files which may be used for manual recovery in critical disaster.
     * </p>
     */
    class SyncThread extends Thread {
        volatile boolean running = true;
        // flag to ensure sync thread will not be interrupted during flush
        final AtomicBoolean flushing = new AtomicBoolean(false);
        // make flush interval as a parameter
        final int flushInterval;
        public SyncThread(ServerConfiguration conf) {
            super("SyncThread");
            flushInterval = conf.getFlushInterval();
            LOG.debug("Flush Interval : {}", flushInterval);
        }

        private Object suspensionLock = new Object();
        private boolean suspended = false;

        /**
         * Suspend sync thread. (for testing)
         */
        @VisibleForTesting
        public void suspendSync() {
            synchronized(suspensionLock) {
                suspended = true;
            }
        }

        /**
         * Resume sync thread. (for testing)
         */
        @VisibleForTesting
        public void resumeSync() {
            synchronized(suspensionLock) {
                suspended = false;
                suspensionLock.notify();
            }
        }

        @Override
        public void run() {
            try {
                while (running) {
                    synchronized (this) {
                        try {
                            wait(flushInterval);
                            if (!ledgerStorage.isFlushRequired()) {
                                continue;
                            }
                        } catch (InterruptedException e) {
                            Thread.currentThread().interrupt();
                            continue;
                        }
                    }
                    synchronized (suspensionLock) {
                        while (suspended) {
                            suspensionLock.wait();
                        }
                    }
                    // try to mark flushing flag to make sure it would not be interrupted
                    // by shutdown during flushing. otherwise it will receive
                    // ClosedByInterruptException which may cause index file & entry logger
                    // closed and corrupted.
                    if (!flushing.compareAndSet(false, true)) {
                        // set flushing flag failed, means flushing is true now
                        // indicates another thread wants to interrupt sync thread to exit
                        break;
                    }

                    // journal mark log
                    journal.markLog();

                    boolean flushFailed = false;
                    try {
                        ledgerStorage.flush();
                    } catch (NoWritableLedgerDirException e) {
                        flushFailed = true;
                        flushing.set(false);
                        transitionToReadOnlyMode();
                    } catch (IOException e) {
                        LOG.error("Exception flushing Ledger", e);
                        flushFailed = true;
                    }

                    // if flush failed, we should not roll last mark, otherwise we would
                    // have some ledgers are not flushed and their journal entries were lost
                    if (!flushFailed) {
                        try {
                            journal.rollLog();
                            journal.gcJournals();
                        } catch (NoWritableLedgerDirException e) {
                            flushing.set(false);
                            transitionToReadOnlyMode();
                        }
                    }

                    // clear flushing flag
                    flushing.set(false);
                }
            } catch (Throwable t) {
                LOG.error("Exception in SyncThread", t);
                triggerBookieShutdown(ExitCode.BOOKIE_EXCEPTION);
            }
        }

        // shutdown sync thread
        void shutdown() throws InterruptedException {
            running = false;
            if (flushing.compareAndSet(false, true)) {
                // if setting flushing flag succeed, means syncThread is not flushing now
                // it is safe to interrupt itself now 
                this.interrupt();
            }
            this.join();
        }
    }

    public static void checkDirectoryStructure(File dir) throws IOException {
        if (!dir.exists()) {
            File parent = dir.getParentFile();
            File preV3versionFile = new File(dir.getParent(),
                    BookKeeperConstants.VERSION_FILENAME);

            final AtomicBoolean oldDataExists = new AtomicBoolean(false);
            parent.list(new FilenameFilter() {
                    public boolean accept(File dir, String name) {
                        if (name.endsWith(".txn") || name.endsWith(".idx") || name.endsWith(".log")) {
                            oldDataExists.set(true);
                        }
                        return true;
                    }
                });
            if (preV3versionFile.exists() || oldDataExists.get()) {
                String err = "Directory layout version is less than 3, upgrade needed";
                LOG.error(err);
                throw new IOException(err);
            }
            if (!dir.mkdirs()) {
                String err = "Unable to create directory " + dir;
                LOG.error(err);
                throw new IOException(err);
            }
        }
    }

    /**
     * Check that the environment for the bookie is correct.
     * This means that the configuration has stayed the same as the
     * first run and the filesystem structure is up to date.
     */
    private void checkEnvironment(ZooKeeper zk) throws BookieException, IOException {
        if (zk == null) { // exists only for testing, just make sure directories are correct
            checkDirectoryStructure(journalDirectory);
            for (File dir : ledgerDirsManager.getAllLedgerDirs()) {
                    checkDirectoryStructure(dir);
            }
            return;
        }
        try {
            String instanceId = getInstanceId(zk);
            boolean newEnv = false;
            Cookie masterCookie = Cookie.generateCookie(conf);
            if (null != instanceId) {
                masterCookie.setInstanceId(instanceId);
            }
            try {
                Cookie zkCookie = Cookie.readFromZooKeeper(zk, conf);
                masterCookie.verify(zkCookie);
            } catch (KeeperException.NoNodeException nne) {
                newEnv = true;
            }
            List<File> missedCookieDirs = new ArrayList<File>();
            checkDirectoryStructure(journalDirectory);

            // try to read cookie from journal directory
            try {
                Cookie journalCookie = Cookie.readFromDirectory(journalDirectory);
                journalCookie.verify(masterCookie);
            } catch (FileNotFoundException fnf) {
                missedCookieDirs.add(journalDirectory);
            }
            for (File dir : ledgerDirsManager.getAllLedgerDirs()) {
                checkDirectoryStructure(dir);
                try {
                    Cookie c = Cookie.readFromDirectory(dir);
                    c.verify(masterCookie);
                } catch (FileNotFoundException fnf) {
                    missedCookieDirs.add(dir);
                }
            }

            if (!newEnv && missedCookieDirs.size() > 0){
                LOG.error("Cookie exists in zookeeper, but not in all local directories. "
                        + " Directories missing cookie file are " + missedCookieDirs);
                throw new BookieException.InvalidCookieException();
            }
            if (newEnv) {
                if (missedCookieDirs.size() > 0) {
                    LOG.debug("Directories missing cookie file are {}", missedCookieDirs);
                    masterCookie.writeToDirectory(journalDirectory);
                    for (File dir : ledgerDirsManager.getAllLedgerDirs()) {
                        masterCookie.writeToDirectory(dir);
                    }
                }
                masterCookie.writeToZooKeeper(zk, conf);
            }
        } catch (KeeperException ke) {
            LOG.error("Couldn't access cookie in zookeeper", ke);
            throw new BookieException.InvalidCookieException(ke);
        } catch (UnknownHostException uhe) {
            LOG.error("Couldn't check cookies, networking is broken", uhe);
            throw new BookieException.InvalidCookieException(uhe);
        } catch (IOException ioe) {
            LOG.error("Error accessing cookie on disks", ioe);
            throw new BookieException.InvalidCookieException(ioe);
        } catch (InterruptedException ie) {
            LOG.error("Thread interrupted while checking cookies, exiting", ie);
            throw new BookieException.InvalidCookieException(ie);
        }
    }

    /**
     * Return the configured address of the bookie.
     */
    public static InetSocketAddress getBookieAddress(ServerConfiguration conf)
            throws UnknownHostException {
        return new InetSocketAddress(InetAddress.getLocalHost()
                .getHostAddress(), conf.getBookiePort());
    }

    private String getInstanceId(ZooKeeper zk) throws KeeperException,
            InterruptedException {
        String instanceId = null;
        try {
            byte[] data = zk.getData(conf.getZkLedgersRootPath() + "/"
                    + BookKeeperConstants.INSTANCEID, false, null);
            instanceId = new String(data);
        } catch (KeeperException.NoNodeException e) {
            LOG.warn("INSTANCEID not exists in zookeeper. Not considering it for data verification");
        }
        return instanceId;
    }

    public LedgerDirsManager getLedgerDirsManager() {
        return ledgerDirsManager;
    }

    public static File getCurrentDirectory(File dir) {
        return new File(dir, BookKeeperConstants.CURRENT_DIR);
    }

    public static File[] getCurrentDirectories(File[] dirs) {
        File[] currentDirs = new File[dirs.length];
        for (int i = 0; i < dirs.length; i++) {
            currentDirs[i] = getCurrentDirectory(dirs[i]);
        }
        return currentDirs;
    }


    public Bookie(ServerConfiguration conf)
            throws IOException, KeeperException, InterruptedException, BookieException {
        super("Bookie-" + conf.getBookiePort());
        this.bookieRegistrationPath = conf.getZkAvailableBookiesPath() + "/";
        this.conf = conf;
        this.journalDirectory = getCurrentDirectory(conf.getJournalDir());
        this.ledgerDirsManager = new LedgerDirsManager(conf);
        // instantiate zookeeper client to initialize ledger manager
        this.zk = instantiateZookeeperClient(conf);
        checkEnvironment(this.zk);
        ledgerManagerFactory = LedgerManagerFactory.newLedgerManagerFactory(conf, this.zk);
        LOG.info("instantiate ledger manager {}", ledgerManagerFactory.getClass().getName());
        ledgerManager = ledgerManagerFactory.newLedgerManager();
        syncThread = new SyncThread(conf);
        ledgerStorage = new InterleavedLedgerStorage(conf, ledgerManager,
                                                     ledgerDirsManager,
                                                     new BookieSafeEntryAdder());
        handles = new HandleFactoryImpl(ledgerStorage);
        // instantiate the journal
        journal = new Journal(conf, ledgerDirsManager);

        // ZK ephemeral node for this Bookie.
        zkBookieRegPath = this.bookieRegistrationPath + getMyId();
    }

    private String getMyId() throws UnknownHostException {
        return InetAddress.getLocalHost().getHostAddress() + ":"
                + conf.getBookiePort();
    }

    void readJournal() throws IOException, BookieException {
        journal.replay(new JournalScanner() {
            @Override
            public void process(int journalVersion, long offset, ByteBuffer recBuff) throws IOException {
                long ledgerId = recBuff.getLong();
                long entryId = recBuff.getLong();
                try {
                    LOG.debug("Replay journal - ledger id : {}, entry id : {}.", ledgerId, entryId);
                    if (entryId == METAENTRY_ID_LEDGER_KEY) {
                        if (journalVersion >= 3) {
                            int masterKeyLen = recBuff.getInt();
                            byte[] masterKey = new byte[masterKeyLen];

                            recBuff.get(masterKey);
                            masterKeyCache.put(ledgerId, masterKey);
                        } else {
                            throw new IOException("Invalid journal. Contains journalKey "
                                    + " but layout version (" + journalVersion
                                    + ") is too old to hold this");
                        }
                    } else if (entryId == METAENTRY_ID_FENCE_KEY) {
                        if (journalVersion >= 4) {
                            byte[] key = masterKeyCache.get(ledgerId);
                            if (key == null) {
                                key = ledgerStorage.readMasterKey(ledgerId);
                            }
                            LedgerDescriptor handle = handles.getHandle(ledgerId, key);
                            handle.setFenced();
                        } else {
                            throw new IOException("Invalid journal. Contains fenceKey "
                                    + " but layout version (" + journalVersion
                                    + ") is too old to hold this");
                        }
                    } else {
                        byte[] key = masterKeyCache.get(ledgerId);
                        if (key == null) {
                            key = ledgerStorage.readMasterKey(ledgerId);
                        }
                        LedgerDescriptor handle = handles.getHandle(ledgerId, key);

                        recBuff.rewind();
                        handle.addEntry(recBuff);
                    }
                } catch (NoLedgerException nsle) {
                    LOG.debug("Skip replaying entries of ledger {} since it was deleted.", ledgerId);
                } catch (BookieException be) {
                    throw new IOException(be);
                }
            }
        });
    }

    synchronized public void start() {
        setDaemon(true);
        LOG.debug("I'm starting a bookie with journal directory {}", journalDirectory.getName());
        // replay journals
        try {
            readJournal();
        } catch (IOException ioe) {
            LOG.error("Exception while replaying journals, shutting down", ioe);
            shutdown(ExitCode.BOOKIE_EXCEPTION);
            return;
        } catch (BookieException be) {
            LOG.error("Exception while replaying journals, shutting down", be);
            shutdown(ExitCode.BOOKIE_EXCEPTION);
            return;
        }
        // start bookie thread
        super.start();

        ledgerDirsManager.addLedgerDirsListener(getLedgerDirsListener());
        //Start DiskChecker thread
        ledgerDirsManager.start();

        ledgerStorage.start();

        syncThread.start();
        // set running here.
        // since bookie server use running as a flag to tell bookie server whether it is alive
        // if setting it in bookie thread, the watcher might run before bookie thread.
        running = true;
        try {
            registerBookie(conf);
        } catch (IOException e) {
            LOG.error("Couldn't register bookie with zookeeper, shutting down", e);
            shutdown(ExitCode.ZK_REG_FAIL);
        }
    }

    /*
     * Get the DiskFailure listener for the bookie
     */
    private LedgerDirsListener getLedgerDirsListener() {

        return new LedgerDirsListener() {

            @Override
            public void diskFull(File disk) {
                // Nothing needs to be handled here.
            }

            @Override
            public void diskFailed(File disk) {
                // Shutdown the bookie on disk failure.
                triggerBookieShutdown(ExitCode.BOOKIE_EXCEPTION);
            }

            @Override
            public void allDisksFull() {
                // Transition to readOnly mode on all disks full
                transitionToReadOnlyMode();
            }

            @Override
            public void fatalError() {
                LOG.error("Fatal error reported by ledgerDirsManager");
                triggerBookieShutdown(ExitCode.BOOKIE_EXCEPTION);
            }
        };
    }

    /**
     * Register jmx with parent
     *
     * @param parent parent bk mbean info
     */
    public void registerJMX(BKMBeanInfo parent) {
        try {
            jmxBookieBean = new BookieBean(this);
            BKMBeanRegistry.getInstance().register(jmxBookieBean, parent);

            try {
                jmxLedgerStorageBean = this.ledgerStorage.getJMXBean();
                BKMBeanRegistry.getInstance().register(jmxLedgerStorageBean, jmxBookieBean);
            } catch (Exception e) {
                LOG.warn("Failed to register with JMX for ledger cache", e);
                jmxLedgerStorageBean = null;
            }
        } catch (Exception e) {
            LOG.warn("Failed to register with JMX", e);
            jmxBookieBean = null;
        }
    }

    /**
     * Unregister jmx
     */
    public void unregisterJMX() {
        try {
            if (jmxLedgerStorageBean != null) {
                BKMBeanRegistry.getInstance().unregister(jmxLedgerStorageBean);
            }
        } catch (Exception e) {
            LOG.warn("Failed to unregister with JMX", e);
        }
        try {
            if (jmxBookieBean != null) {
                BKMBeanRegistry.getInstance().unregister(jmxBookieBean);
            }
        } catch (Exception e) {
            LOG.warn("Failed to unregister with JMX", e);
        }
        jmxBookieBean = null;
        jmxLedgerStorageBean = null;
    }


    /**
     * Instantiate the ZooKeeper client for the Bookie.
     */
    private ZooKeeper instantiateZookeeperClient(ServerConfiguration conf)
            throws IOException, InterruptedException, KeeperException {
        if (conf.getZkServers() == null) {
            LOG.warn("No ZK servers passed to Bookie constructor so BookKeeper clients won't know about this server!");
            return null;
        }
        // Create the ZooKeeper client instance
        return newZookeeper(conf.getZkServers(), conf.getZkTimeout());
    }

    /**
     * Register as an available bookie
     */
    protected void registerBookie(ServerConfiguration conf) throws IOException {
        if (null == zk) {
            // zookeeper instance is null, means not register itself to zk
            return;
        }

        // ZK ephemeral node for this Bookie.
        String zkBookieRegPath = this.bookieRegistrationPath
            + StringUtils.addrToString(getBookieAddress(conf));
        final CountDownLatch prevNodeLatch = new CountDownLatch(1);
        try{
            Watcher zkPrevRegNodewatcher = new Watcher() {
                @Override
                public void process(WatchedEvent event) {
                    // Check for prev znode deletion. Connection expiration is
                    // not handling, since bookie has logic to shutdown.
                    if (EventType.NodeDeleted == event.getType()) {
                        prevNodeLatch.countDown();
                    }
                }
            };
            if (null != zk.exists(zkBookieRegPath, zkPrevRegNodewatcher)) {
                LOG.info("Previous bookie registration znode: "
                        + zkBookieRegPath
                        + " exists, so waiting zk sessiontimeout: "
                        + conf.getZkTimeout() + "ms for znode deletion");
                // waiting for the previous bookie reg znode deletion
                if (!prevNodeLatch.await(conf.getZkTimeout(),
                        TimeUnit.MILLISECONDS)) {
                    throw new KeeperException.NodeExistsException(
                            zkBookieRegPath);
                }
            }

            // Create the ZK ephemeral node for this Bookie.
            zk.create(zkBookieRegPath, new byte[0], Ids.OPEN_ACL_UNSAFE,
                    CreateMode.EPHEMERAL);
        } catch (KeeperException ke) {
            LOG.error("ZK exception registering ephemeral Znode for Bookie!",
                    ke);
            // Throw an IOException back up. This will cause the Bookie
            // constructor to error out. Alternatively, we could do a System
            // exit here as this is a fatal error.
            throw new IOException(ke);
        } catch (InterruptedException ie) {
            LOG.error("ZK exception registering ephemeral Znode for Bookie!",
                    ie);
            // Throw an IOException back up. This will cause the Bookie
            // constructor to error out. Alternatively, we could do a System
            // exit here as this is a fatal error.
            throw new IOException(ie);
        }
    }

    /*
     * Transition the bookie to readOnly mode
     */
    @VisibleForTesting
    public void transitionToReadOnlyMode() {
        if (!readOnly.compareAndSet(false, true)) {
            return;
        }
        if (!conf.isReadOnlyModeEnabled()) {
            LOG.warn("ReadOnly mode is not enabled. "
                    + "Can be enabled by configuring "
                    + "'readOnlyModeEnabled=true' in configuration."
                    + "Shutting down bookie");
            triggerBookieShutdown(ExitCode.BOOKIE_EXCEPTION);
            return;
        }
        LOG.info("Transitioning Bookie to ReadOnly mode,"
                + " and will serve only read requests from clients!");
        try {
            if (null == zk.exists(this.bookieRegistrationPath
                    + BookKeeperConstants.READONLY, false)) {
                try {
                    zk.create(this.bookieRegistrationPath
                            + BookKeeperConstants.READONLY, new byte[0],
                            Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
                } catch (NodeExistsException e) {
                    // this node is just now created by someone.
                }
            }
            // Create the readonly node
            zk.create(this.bookieRegistrationPath
                    + BookKeeperConstants.READONLY + "/" + getMyId(),
                    new byte[0], Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL);
            // Clear the current registered node
            zk.delete(zkBookieRegPath, -1);
        } catch (IOException e) {
            LOG.error("Error in transition to ReadOnly Mode."
                    + " Shutting down", e);
            triggerBookieShutdown(ExitCode.BOOKIE_EXCEPTION);
            return;
        } catch (KeeperException e) {
            LOG.error("Error in transition to ReadOnly Mode."
                    + " Shutting down", e);
            triggerBookieShutdown(ExitCode.BOOKIE_EXCEPTION);
            return;
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            LOG.warn("Interrupted Exception while transitioning to ReadOnly Mode.");
            return;
        }
    }

    /*
     * Check whether Bookie is writable
     */
    public boolean isReadOnly() {
        return readOnly.get();
    }

    /**
     * Create a new zookeeper client to zk cluster.
     *
     * <p>
     * Bookie Server just used zk client when syncing ledgers for garbage collection.
     * So when zk client is expired, it means this bookie server is not available in
     * bookie server list. The bookie client will be notified for its expiration. No
     * more bookie request will be sent to this server. So it's better to exit when zk
     * expired.
     * </p>
     * <p>
     * Since there are lots of bk operations cached in queue, so we wait for all the operations
     * are processed and quit. It is done by calling <b>shutdown</b>.
     * </p>
     *
     * @param zkServers the quorum list of zk servers
     * @param sessionTimeout session timeout of zk connection
     *
     * @return zk client instance
     */
    private ZooKeeper newZookeeper(final String zkServers,
            final int sessionTimeout) throws IOException, InterruptedException,
            KeeperException {
        ZooKeeperWatcherBase w = new ZooKeeperWatcherBase(conf.getZkTimeout()) {
            @Override
            public void process(WatchedEvent event) {
                // Check for expired connection.
                if (event.getState().equals(Watcher.Event.KeeperState.Expired)) {
                    LOG.error("ZK client connection to the ZK server has expired!");
                    shutdown(ExitCode.ZK_EXPIRED);
                } else {
                    super.process(event);
                }
            }
        };
        return ZkUtils.createConnectedZookeeperClient(zkServers, w);
    }

    public boolean isRunning() {
        return running;
    }

    @Override
    public void run() {
        // bookie thread wait for journal thread
        try {
            // start journal
            journal.start();
            // wait until journal quits
            journal.join();
        } catch (InterruptedException ie) {
        }
        // if the journal thread quits due to shutting down, it is ok
        if (!shuttingdown) {
            // some error found in journal thread and it quits
            // following add operations to it would hang unit client timeout
            // so we should let bookie server exists
            LOG.error("Journal manager quits unexpectedly.");
            triggerBookieShutdown(ExitCode.BOOKIE_EXCEPTION);
        }
    }

    // Triggering the Bookie shutdown in its own thread,
    // because shutdown can be called from sync thread which would be
    // interrupted by shutdown call.
    void triggerBookieShutdown(final int exitCode) {
        Thread shutdownThread = new Thread() {
            public void run() {
                Bookie.this.shutdown(exitCode);
            }
        };
        shutdownThread.start();
        try {
            shutdownThread.join();
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            LOG.debug("InterruptedException while waiting for shutdown. Not a problem!!");
        }
    }

    // provided a public shutdown method for other caller
    // to shut down bookie gracefully
    public int shutdown() {
        return shutdown(ExitCode.OK);
    }

    // internal shutdown method to let shutdown bookie gracefully
    // when encountering exception
    synchronized int shutdown(int exitCode) {
        try {
            if (running) { // avoid shutdown twice
                // the exitCode only set when first shutdown usually due to exception found
                this.exitCode = exitCode;
                // mark bookie as in shutting down progress
                shuttingdown = true;

                // Shutdown the ZK client
                if(zk != null) zk.close();

                //Shutdown disk checker
                ledgerDirsManager.shutdown();

                // Shutdown journal
                journal.shutdown();
                this.join();
                syncThread.shutdown();

                // Shutdown the EntryLogger which has the GarbageCollector Thread running
                ledgerStorage.shutdown();

                // close Ledger Manager
                try {
                    ledgerManager.close();
                    ledgerManagerFactory.uninitialize();
                } catch (IOException ie) {
                    LOG.error("Failed to close active ledger manager : ", ie);
                }
                // setting running to false here, so watch thread in bookie server know it only after bookie shut down
                running = false;
            }
        } catch (InterruptedException ie) {
            LOG.error("Interrupted during shutting down bookie : ", ie);
        }
        return this.exitCode;
    }

    /** 
     * Retrieve the ledger descriptor for the ledger which entry should be added to.
     * The LedgerDescriptor returned from this method should be eventually freed with 
     * #putHandle().
     *
     * @throws BookieException if masterKey does not match the master key of the ledger
     */
    private LedgerDescriptor getLedgerForEntry(ByteBuffer entry, byte[] masterKey) 
            throws IOException, BookieException {
        long ledgerId = entry.getLong();
        LedgerDescriptor l = handles.getHandle(ledgerId, masterKey);
        if (!masterKeyCache.containsKey(ledgerId)) {
            // new handle, we should add the key to journal ensure we can rebuild
            ByteBuffer bb = ByteBuffer.allocate(8 + 8 + 4 + masterKey.length);
            bb.putLong(ledgerId);
            bb.putLong(METAENTRY_ID_LEDGER_KEY);
            bb.putInt(masterKey.length);
            bb.put(masterKey);
            bb.flip();

            journal.logAddEntry(bb, new NopWriteCallback(), null);
            masterKeyCache.put(ledgerId, masterKey);
        }
        return l;
    }

    protected void addEntryByLedgerId(long ledgerId, ByteBuffer entry)
        throws IOException, BookieException {
        byte[] key = ledgerStorage.readMasterKey(ledgerId);
        LedgerDescriptor handle = handles.getHandle(ledgerId, key);
        handle.addEntry(entry);
    }

    /**
     * Add an entry to a ledger as specified by handle. 
     */
    private void addEntryInternal(LedgerDescriptor handle, ByteBuffer entry, WriteCallback cb, Object ctx)
            throws IOException, BookieException {
        long ledgerId = handle.getLedgerId();
        entry.rewind();
        long entryId = handle.addEntry(entry);

        entry.rewind();
        LOG.trace("Adding {}@{}", entryId, ledgerId);
        journal.logAddEntry(entry, cb, ctx);
    }

    /**
     * Add entry to a ledger, even if the ledger has previous been fenced. This should only
     * happen in bookie recovery or ledger recovery cases, where entries are being replicates 
     * so that they exist on a quorum of bookies. The corresponding client side call for this
     * is not exposed to users.
     */
    public void recoveryAddEntry(ByteBuffer entry, WriteCallback cb, Object ctx, byte[] masterKey) 
            throws IOException, BookieException {
        try {
            LedgerDescriptor handle = getLedgerForEntry(entry, masterKey);
            synchronized (handle) {
                addEntryInternal(handle, entry, cb, ctx);
            }
        } catch (NoWritableLedgerDirException e) {
            transitionToReadOnlyMode();
            throw new IOException(e);
        }
    }
    
    /** 
     * Add entry to a ledger.
     * @throws BookieException.LedgerFencedException if the ledger is fenced
     */
    public void addEntry(ByteBuffer entry, WriteCallback cb, Object ctx, byte[] masterKey)
            throws IOException, BookieException {
        try {
            LedgerDescriptor handle = getLedgerForEntry(entry, masterKey);
            synchronized (handle) {
                if (handle.isFenced()) {
                    throw BookieException
                            .create(BookieException.Code.LedgerFencedException);
                }
                addEntryInternal(handle, entry, cb, ctx);
            }
        } catch (NoWritableLedgerDirException e) {
            transitionToReadOnlyMode();
            throw new IOException(e);
        }
    }

    /**
     * Fences a ledger. From this point on, clients will be unable to
     * write to this ledger. Only recoveryAddEntry will be
     * able to add entries to the ledger.
     * This method is idempotent. Once a ledger is fenced, it can
     * never be unfenced. Fencing a fenced ledger has no effect.
     */
    public Future<Boolean> fenceLedger(long ledgerId, byte[] masterKey) throws IOException, BookieException {
        LedgerDescriptor handle = handles.getHandle(ledgerId, masterKey);
        boolean success;
        synchronized (handle) {
            success = handle.setFenced();
        }
        if (success) {
            // fenced first time, we should add the key to journal ensure we can rebuild
            ByteBuffer bb = ByteBuffer.allocate(8 + 8);
            bb.putLong(ledgerId);
            bb.putLong(METAENTRY_ID_FENCE_KEY);
            bb.flip();

            FutureWriteCallback fwc = new FutureWriteCallback();
            LOG.debug("record fenced state for ledger {} in journal.", ledgerId);
            journal.logAddEntry(bb, fwc, null);
            return fwc.getResult();
        } else {
            // already fenced
            return SUCCESS_FUTURE;
        }
    }

    public ByteBuffer readEntry(long ledgerId, long entryId)
            throws IOException, NoLedgerException {
        LedgerDescriptor handle = handles.getReadOnlyHandle(ledgerId);
        LOG.trace("Reading {}@{}", entryId, ledgerId);
        return handle.readEntry(entryId);
    }

    // The rest of the code is test stuff
    static class CounterCallback implements WriteCallback {
        int count;

        synchronized public void writeComplete(int rc, long l, long e, InetSocketAddress addr, Object ctx) {
            count--;
            if (count == 0) {
                notifyAll();
            }
        }

        synchronized public void incCount() {
            count++;
        }

        synchronized public void waitZero() throws InterruptedException {
            while (count > 0) {
                wait();
            }
        }
    }

    /**
     * Format the bookie server data
     * 
     * @param conf
     *            ServerConfiguration
     * @param isInteractive
     *            Whether format should ask prompt for confirmation if old data
     *            exists or not.
     * @param force
     *            If non interactive and force is true, then old data will be
     *            removed without confirm prompt.
     * @return Returns true if the format is success else returns false
     */
    public static boolean format(ServerConfiguration conf,
            boolean isInteractive, boolean force) {
        File journalDir = conf.getJournalDir();
        if (journalDir.exists() && journalDir.isDirectory()
                && journalDir.list().length != 0) {
            try {
                boolean confirm = false;
                if (!isInteractive) {
                    // If non interactive and force is set, then delete old
                    // data.
                    if (force) {
                        confirm = true;
                    } else {
                        confirm = false;
                    }
                } else {
                    confirm = IOUtils
                            .confirmPrompt("Are you sure to format Bookie data..?");
                }

                if (!confirm) {
                    LOG.error("Bookie format aborted!!");
                    return false;
                }
            } catch (IOException e) {
                LOG.error("Error during bookie format", e);
                return false;
            }
        }
        if (!cleanDir(journalDir)) {
            LOG.error("Formatting journal directory failed");
            return false;
        }

        File[] ledgerDirs = conf.getLedgerDirs();
        for (File dir : ledgerDirs) {
            if (!cleanDir(dir)) {
                LOG.error("Formatting ledger directory " + dir + " failed");
                return false;
            }
        }
        LOG.info("Bookie format completed successfully");
        return true;
    }

    private static boolean cleanDir(File dir) {
        if (dir.exists()) {
            for (File child : dir.listFiles()) {
                boolean delete = FileUtils.deleteQuietly(child);
                if (!delete) {
                    LOG.error("Not able to delete " + child);
                    return false;
                }
            }
        } else if (!dir.mkdirs()) {
            LOG.error("Not able to create the directory " + dir);
            return false;
        }
        return true;
    }

    private class BookieSafeEntryAdder implements SafeEntryAdder {
        @Override
        public void safeAddEntry(final long ledgerId, final ByteBuffer buffer,
                                 final GenericCallback<Void> cb) {
            journal.logAddEntry(buffer, new WriteCallback() {
                    @Override
                    public void writeComplete(int rc, long ledgerId2, long entryId,
                                              InetSocketAddress addr, Object ctx) {
                        if (rc != BookieException.Code.OK) {
                            LOG.error("Error rewriting to journal (ledger {}, entry {})", ledgerId2, entryId);
                            cb.operationComplete(rc, null);
                            return;
                        }
                        try {
                            addEntryByLedgerId(ledgerId, buffer);
                            cb.operationComplete(rc, null);
                        } catch (IOException ioe) {
                            LOG.error("Error adding to ledger storage (ledger " + ledgerId2
                                      + ", entry " + entryId + ")", ioe);
                            // couldn't add to ledger storage
                            cb.operationComplete(BookieException.Code.IllegalOpException, null);
                        } catch (BookieException bke) {
                            LOG.error("Bookie error adding to ledger storage (ledger " + ledgerId2
                                      + ", entry " + entryId + ")", bke);
                            // couldn't add to ledger storage
                            cb.operationComplete(bke.getCode(), null);
                        }
                    }
                }, null);
        }
    }
    /**
     * @param args
     * @throws IOException
     * @throws InterruptedException
     */
    public static void main(String[] args) 
            throws IOException, InterruptedException, BookieException, KeeperException {
        Bookie b = new Bookie(new ServerConfiguration());
        b.start();
        CounterCallback cb = new CounterCallback();
        long start = MathUtils.now();
        for (int i = 0; i < 100000; i++) {
            ByteBuffer buff = ByteBuffer.allocate(1024);
            buff.putLong(1);
            buff.putLong(i);
            buff.limit(1024);
            buff.position(0);
            cb.incCount();
            b.addEntry(buff, cb, null, new byte[0]);
        }
        cb.waitZero();
        long end = MathUtils.now();
        System.out.println("Took " + (end-start) + "ms");
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/BookieBean.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.bookkeeper.bookie;

import java.io.File;

import org.apache.bookkeeper.jmx.BKMBeanInfo;

/**
 * Bookie Bean
 */
public class BookieBean implements BookieMXBean, BKMBeanInfo {

    protected Bookie bk;

    public BookieBean(Bookie bk) {
        this.bk = bk;
    }

    @Override
    public String getName() {
        return "Bookie";
    }

    @Override
    public boolean isHidden() {
        return false;
    }

    @Override
    public int getQueueLength() {
        return bk.journal.getJournalQueueLength();
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/BookieException.java,false,"package org.apache.bookkeeper.bookie;

/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */


import java.lang.Exception;

@SuppressWarnings("serial")
public abstract class BookieException extends Exception {

    private int code;
    public BookieException(int code) {
        this.code = code;
    }

    public BookieException(int code, Throwable t) {
        super(t);
    }

    public BookieException(int code, String reason) {
        super(reason);
    }

    public static BookieException create(int code) {
        switch(code) {
        case Code.UnauthorizedAccessException:
            return new BookieUnauthorizedAccessException();
        case Code.LedgerFencedException:
            return new LedgerFencedException();
        case Code.InvalidCookieException:
            return new InvalidCookieException();
        case Code.UpgradeException:
            return new UpgradeException();
        default:
            return new BookieIllegalOpException();
        }
    }

    public interface Code {
        int OK = 0;
        int UnauthorizedAccessException = -1;

        int IllegalOpException = -100;
        int LedgerFencedException = -101;

        int InvalidCookieException = -102;
        int UpgradeException = -103;
    }

    public void setCode(int code) {
        this.code = code;
    }

    public int getCode() {
        return this.code;
    }

    public String getMessage(int code) {
        String err = "Invalid operation";
        switch(code) {
        case Code.OK:
            err = "No problem";
            break;
        case Code.UnauthorizedAccessException:
            err = "Error while reading ledger";
            break;
        case Code.LedgerFencedException:
            err = "Ledger has been fenced; No more entries can be added";
            break;
        case Code.InvalidCookieException:
            err = "Invalid environment cookie found";
            break;
        case Code.UpgradeException:
            err = "Error performing an upgrade operation ";
            break;
        }
        String reason = super.getMessage();
        if (reason == null) {
            if (super.getCause() != null) {
                reason = super.getCause().getMessage();
            }
        }
        if (reason == null) {
            return err;
        } else {
            return String.format("%s [%s]", err, reason);
        }
    }

    public static class BookieUnauthorizedAccessException extends BookieException {
        public BookieUnauthorizedAccessException() {
            super(Code.UnauthorizedAccessException);
        }
    }

    public static class BookieIllegalOpException extends BookieException {
        public BookieIllegalOpException() {
            super(Code.UnauthorizedAccessException);
        }
    }

    public static class LedgerFencedException extends BookieException {
        public LedgerFencedException() {
            super(Code.LedgerFencedException);
        }
    }

    public static class InvalidCookieException extends BookieException {
        public InvalidCookieException() {
            this("");
        }

        public InvalidCookieException(String reason) {
            super(Code.InvalidCookieException, reason);
        }

        public InvalidCookieException(Throwable cause) {
            super(Code.InvalidCookieException, cause);
        }
    }

    public static class UpgradeException extends BookieException {
        public UpgradeException() {
            super(Code.UpgradeException);
        }

        public UpgradeException(Throwable cause) {
            super(Code.UpgradeException, cause);
        }

        public UpgradeException(String reason) {
            super(Code.UpgradeException, reason);
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/BookieMXBean.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.bookkeeper.bookie;

import java.io.File;

/**
 * Bookie MBean
 */
public interface BookieMXBean {
    /**
     * @return log entry queue length
     */
    public int getQueueLength();
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/BookieShell.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.bookkeeper.bookie;

import java.io.File;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.net.InetSocketAddress;
import java.nio.ByteBuffer;
import java.util.Formatter;
import java.util.HashMap;
import java.util.Map;

import org.apache.zookeeper.ZooKeeper;
import org.apache.bookkeeper.meta.LedgerManagerFactory;
import org.apache.bookkeeper.meta.LedgerUnderreplicationManager;
import org.apache.bookkeeper.zookeeper.ZooKeeperWatcherBase;

import org.apache.bookkeeper.bookie.EntryLogger.EntryLogScanner;
import org.apache.bookkeeper.bookie.Journal.JournalScanner;
import org.apache.bookkeeper.bookie.Journal.LastLogMark;
import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.client.BookKeeperAdmin;
import org.apache.bookkeeper.conf.ClientConfiguration;
import org.apache.bookkeeper.conf.ServerConfiguration;
import org.apache.bookkeeper.util.EntryFormatter;
import org.apache.bookkeeper.util.Tool;
import org.apache.bookkeeper.util.ZkUtils;

import org.apache.commons.configuration.Configuration;
import org.apache.commons.configuration.CompositeConfiguration;
import org.apache.commons.configuration.PropertiesConfiguration;
import org.apache.commons.cli.BasicParser;
import org.apache.commons.cli.MissingArgumentException;
import org.apache.commons.cli.Options;
import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.HelpFormatter;
import org.apache.commons.cli.ParseException;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Bookie Shell is to provide utilities for users to administer a bookkeeper cluster.
 */
public class BookieShell implements Tool {

    static final Logger LOG = LoggerFactory.getLogger(BookieShell.class);

    static final String ENTRY_FORMATTER_CLASS = "entryFormatterClass";

    static final String CMD_METAFORMAT = "metaformat";
    static final String CMD_BOOKIEFORMAT = "bookieformat";
    static final String CMD_RECOVER = "recover";
    static final String CMD_LEDGER = "ledger";
    static final String CMD_READLOG = "readlog";
    static final String CMD_READJOURNAL = "readjournal";
    static final String CMD_LASTMARK = "lastmark";
    static final String CMD_AUTORECOVERY = "autorecovery";
    static final String CMD_HELP = "help";

    final ServerConfiguration bkConf = new ServerConfiguration();
    File[] ledgerDirectories;
    File journalDirectory;

    EntryLogger entryLogger = null;
    Journal journal = null;
    EntryFormatter formatter;

    int pageSize;
    int entriesPerPage;

    interface Command {
        public int runCmd(String[] args) throws Exception;
        public void printUsage();
    }

    abstract class MyCommand implements Command {
        abstract Options getOptions();
        abstract String getDescription();
        abstract String getUsage();
        abstract int runCmd(CommandLine cmdLine) throws Exception;

        String cmdName;

        MyCommand(String cmdName) {
            this.cmdName = cmdName;
        }

        @Override
        public int runCmd(String[] args) throws Exception {
            try {
                BasicParser parser = new BasicParser();
                CommandLine cmdLine = parser.parse(getOptions(), args);
                return runCmd(cmdLine);
            } catch (ParseException e) {
                LOG.error("Error parsing command line arguments : ", e);
                printUsage();
                return -1;
            }
        }

        @Override
        public void printUsage() {
            HelpFormatter hf = new HelpFormatter();
            System.err.println(cmdName + ": " + getDescription());
            hf.printHelp(getUsage(), getOptions());
        }
    }

    /**
     * Format the bookkeeper metadata present in zookeeper
     */
    class MetaFormatCmd extends MyCommand {
        Options opts = new Options();

        MetaFormatCmd() {
            super(CMD_METAFORMAT);
            opts.addOption("n", "nonInteractive", false,
                    "Whether to confirm if old data exists..?");
            opts.addOption("f", "force", false,
                    "If [nonInteractive] is specified, then whether"
                            + " to force delete the old data without prompt.");
        }

        @Override
        Options getOptions() {
            return opts;
        }

        @Override
        String getDescription() {
            return "Format bookkeeper metadata in zookeeper";
        }

        @Override
        String getUsage() {
            return "metaformat [-nonInteractive] [-force]";
        }

        @Override
        int runCmd(CommandLine cmdLine) throws Exception {
            boolean interactive = (!cmdLine.hasOption("n"));
            boolean force = cmdLine.hasOption("f");

            ClientConfiguration adminConf = new ClientConfiguration(bkConf);
            boolean result = BookKeeperAdmin.format(adminConf, interactive,
                    force);
            return (result) ? 0 : 1;
        }
    }

    /**
     * Formats the local data present in current bookie server
     */
    class BookieFormatCmd extends MyCommand {
        Options opts = new Options();

        public BookieFormatCmd() {
            super(CMD_BOOKIEFORMAT);
            opts.addOption("n", "nonInteractive", false,
                    "Whether to confirm if old data exists..?");
            opts.addOption("f", "force", false,
                    "If [nonInteractive] is specified, then whether"
                            + " to force delete the old data without prompt..?");
        }

        @Override
        Options getOptions() {
            return opts;
        }

        @Override
        String getDescription() {
            return "Format the current server contents";
        }

        @Override
        String getUsage() {
            return "bookieformat [-nonInteractive] [-force]";
        }

        @Override
        int runCmd(CommandLine cmdLine) throws Exception {
            boolean interactive = (!cmdLine.hasOption("n"));
            boolean force = cmdLine.hasOption("f");

            ServerConfiguration conf = new ServerConfiguration(bkConf);
            boolean result = Bookie.format(conf, interactive, force);
            return (result) ? 0 : 1;
        }
    }

    /**
     * Recover command for ledger data recovery for failed bookie
     */
    class RecoverCmd extends MyCommand {
        Options opts = new Options();

        public RecoverCmd() {
            super(CMD_RECOVER);
        }

        @Override
        Options getOptions() {
            return opts;
        }

        @Override
        String getDescription() {
            return "Recover the ledger data for failed bookie";
        }

        @Override
        String getUsage() {
            return "recover <bookieSrc> [bookieDest]";
        }

        @Override
        int runCmd(CommandLine cmdLine) throws Exception {
            String[] args = cmdLine.getArgs();
            if (args.length < 1) {
                throw new MissingArgumentException(
                        "'bookieSrc' argument required");
            }

            ClientConfiguration adminConf = new ClientConfiguration(bkConf);
            BookKeeperAdmin admin = new BookKeeperAdmin(adminConf);
            try {
                return bkRecovery(admin, args);
            } finally {
                if (null != admin) {
                    admin.close();
                }
            }
        }

        private int bkRecovery(BookKeeperAdmin bkAdmin, String[] args)
                throws InterruptedException, BKException {
            final String bookieSrcString[] = args[0].split(":");
            if (bookieSrcString.length != 2) {
                System.err.println("BookieSrc inputted has invalid format"
                        + "(host:port expected): " + args[0]);
                return -1;
            }
            final InetSocketAddress bookieSrc = new InetSocketAddress(
                    bookieSrcString[0], Integer.parseInt(bookieSrcString[1]));
            InetSocketAddress bookieDest = null;
            if (args.length >= 2) {
                final String bookieDestString[] = args[1].split(":");
                if (bookieDestString.length < 2) {
                    System.err.println("BookieDest inputted has invalid format"
                            + "(host:port expected): " + args[1]);
                    return -1;
                }
                bookieDest = new InetSocketAddress(bookieDestString[0],
                        Integer.parseInt(bookieDestString[1]));
            }

            bkAdmin.recoverBookieData(bookieSrc, bookieDest);
            return 0;
        }
    }

    /**
     * Ledger Command Handles ledger related operations
     */
    class LedgerCmd extends MyCommand {
        Options lOpts = new Options();

        LedgerCmd() {
            super(CMD_LEDGER);
            lOpts.addOption("m", "meta", false, "Print meta information");
        }

        @Override
        public int runCmd(CommandLine cmdLine) throws Exception {
            String[] leftArgs = cmdLine.getArgs();
            if (leftArgs.length <= 0) {
                System.err.println("ERROR: missing ledger id");
                printUsage();
                return -1;
            }

            boolean printMeta = false;
            if (cmdLine.hasOption("m")) {
                printMeta = true;
            }
            long ledgerId;
            try {
                ledgerId = Long.parseLong(leftArgs[0]);
            } catch (NumberFormatException nfe) {
                System.err.println("ERROR: invalid ledger id " + leftArgs[0]);
                printUsage();
                return -1;
            }
            if (printMeta) {
                // print meta
                readLedgerMeta(ledgerId);
            }
            // dump ledger info
            readLedgerIndexEntries(ledgerId);
            return 0;
        }

        @Override
        String getDescription() {
            return "Dump ledger index entries into readable format.";
        }

        @Override
        String getUsage() {
            return "ledger [-m] <ledger_id>";
        }

        @Override
        Options getOptions() {
            return lOpts;
        }
    }

    /**
     * Command to read entry log files.
     */
    class ReadLogCmd extends MyCommand {
        Options rlOpts = new Options();

        ReadLogCmd() {
            super(CMD_READLOG);
            rlOpts.addOption("m", "msg", false, "Print message body");
        }

        @Override
        public int runCmd(CommandLine cmdLine) throws Exception {
            String[] leftArgs = cmdLine.getArgs();
            if (leftArgs.length <= 0) {
                System.err.println("ERROR: missing entry log id or entry log file name");
                printUsage();
                return -1;
            }

            boolean printMsg = false;
            if (cmdLine.hasOption("m")) {
                printMsg = true;
            }
            long logId;
            try {
                logId = Long.parseLong(leftArgs[0]);
            } catch (NumberFormatException nfe) {
                // not a entry log id
                File f = new File(leftArgs[0]);
                String name = f.getName();
                if (!name.endsWith(".log")) {
                    // not a log file
                    System.err.println("ERROR: invalid entry log file name " + leftArgs[0]);
                    printUsage();
                    return -1;
                }
                String idString = name.split("\\.")[0];
                logId = Long.parseLong(idString, 16);
            }
            // scan entry log
            scanEntryLog(logId, printMsg);
            return 0;
        }

        @Override
        String getDescription() {
            return "Scan an entry file and format the entries into readable format.";
        }

        @Override
        String getUsage() {
            return "readlog [-m] <entry_log_id | entry_log_file_name>";
        }

        @Override
        Options getOptions() {
            return rlOpts;
        }
    }

    /**
     * Command to read journal files
     */
    class ReadJournalCmd extends MyCommand {
        Options rjOpts = new Options();

        ReadJournalCmd() {
            super(CMD_READJOURNAL);
            rjOpts.addOption("m", "msg", false, "Print message body");
        }

        @Override
        public int runCmd(CommandLine cmdLine) throws Exception {
            String[] leftArgs = cmdLine.getArgs();
            if (leftArgs.length <= 0) {
                System.err.println("ERROR: missing journal id or journal file name");
                printUsage();
                return -1;
            }

            boolean printMsg = false;
            if (cmdLine.hasOption("m")) {
                printMsg = true;
            }
            long journalId;
            try {
                journalId = Long.parseLong(leftArgs[0]);
            } catch (NumberFormatException nfe) {
                // not a journal id
                File f = new File(leftArgs[0]);
                String name = f.getName();
                if (!name.endsWith(".txn")) {
                    // not a journal file
                    System.err.println("ERROR: invalid journal file name " + leftArgs[0]);
                    printUsage();
                    return -1;
                }
                String idString = name.split("\\.")[0];
                journalId = Long.parseLong(idString, 16);
            }
            // scan journal
            scanJournal(journalId, printMsg);
            return 0;
        }

        @Override
        String getDescription() {
            return "Scan a journal file and format the entries into readable format.";
        }

        @Override
        String getUsage() {
            return "readjournal [-m] <journal_id | journal_file_name>";
        }

        @Override
        Options getOptions() {
            return rjOpts;
        }
    }

    /**
     * Command to print last log mark
     */
    class LastMarkCmd implements Command {
        @Override
        public int runCmd(String[] args) throws Exception {
            printLastLogMark();
            return 0;
        }

        @Override
        public void printUsage() {
            System.err.println("lastmark: Print last log marker.");
        }
    }

    /**
     * Command to print help message
     */
    class HelpCmd implements Command {
        @Override
        public int runCmd(String[] args) throws Exception {
            if (args.length == 0) {
                printShellUsage();
                return 0;
            }
            String cmdName = args[0];
            Command cmd = commands.get(cmdName);
            if (null == cmd) {
                System.err.println("Unknown command " + cmdName);
                printShellUsage();
                return -1;
            }
            cmd.printUsage();
            return 0;
        }

        @Override
        public void printUsage() {
            System.err.println("help: Describe the usage of this program or its subcommands.");
            System.err.println("usage: help [COMMAND]");
        }
    }

    /**
     * Command for administration of autorecovery
     */
    class AutoRecoveryCmd extends MyCommand {
        Options opts = new Options();

        public AutoRecoveryCmd() {
            super(CMD_AUTORECOVERY);
            opts.addOption("e", "enable", false,
                           "Enable auto recovery of underreplicated ledgers");
            opts.addOption("d", "disable", false,
                           "Disable auto recovery of underreplicated ledgers");
        }

        @Override
        Options getOptions() {
            return opts;
        }

        @Override
        String getDescription() {
            return "Enable or disable autorecovery in the cluster.";
        }

        @Override
        String getUsage() {
            return "autorecovery [-enable|-disable]";
        }

        @Override
        int runCmd(CommandLine cmdLine) throws Exception {
            boolean disable = cmdLine.hasOption("d");
            boolean enable = cmdLine.hasOption("e");

            if ((!disable && !enable)
                || (enable && disable)) {
                LOG.error("One and only one of -enable and -disable must be specified");
                printUsage();
                return 1;
            }
            ZooKeeper zk = null;
            try {
                ZooKeeperWatcherBase w = new ZooKeeperWatcherBase(bkConf.getZkTimeout());
                zk = ZkUtils.createConnectedZookeeperClient(bkConf.getZkServers(), w);
                LedgerManagerFactory mFactory = LedgerManagerFactory.newLedgerManagerFactory(bkConf, zk);
                LedgerUnderreplicationManager underreplicationManager = mFactory.newLedgerUnderreplicationManager();
                if (enable) {
                    if (underreplicationManager.isLedgerReplicationEnabled()) {
                        LOG.warn("Autorecovery already enabled. Doing nothing");
                    } else {
                        LOG.info("Enabling autorecovery");
                        underreplicationManager.enableLedgerReplication();
                    }
                } else {
                    if (!underreplicationManager.isLedgerReplicationEnabled()) {
                        LOG.warn("Autorecovery already disabled. Doing nothing");
                    } else {
                        LOG.info("Disabling autorecovery");
                        underreplicationManager.disableLedgerReplication();
                    }
                }
            } finally {
                if (zk != null) {
                    zk.close();
                }
            }

            return 0;
        }
    }

    final Map<String, Command> commands;
    {
        commands = new HashMap<String, Command>();
        commands.put(CMD_METAFORMAT, new MetaFormatCmd());
        commands.put(CMD_BOOKIEFORMAT, new BookieFormatCmd());
        commands.put(CMD_RECOVER, new RecoverCmd());
        commands.put(CMD_LEDGER, new LedgerCmd());
        commands.put(CMD_READLOG, new ReadLogCmd());
        commands.put(CMD_READJOURNAL, new ReadJournalCmd());
        commands.put(CMD_LASTMARK, new LastMarkCmd());
        commands.put(CMD_AUTORECOVERY, new AutoRecoveryCmd());
        commands.put(CMD_HELP, new HelpCmd());
    }

    @Override
    public void setConf(Configuration conf) throws Exception {
        bkConf.loadConf(conf);
        journalDirectory = Bookie.getCurrentDirectory(bkConf.getJournalDir());
        ledgerDirectories = Bookie.getCurrentDirectories(bkConf.getLedgerDirs());
        formatter = EntryFormatter.newEntryFormatter(bkConf, ENTRY_FORMATTER_CLASS);
        LOG.info("Using entry formatter " + formatter.getClass().getName());
        pageSize = bkConf.getPageSize();
        entriesPerPage = pageSize / 8;
    }

    private static void printShellUsage() {
        System.err.println("Usage: BookieShell [-conf configuration] <command>");
        System.err.println();
        System.err.println("       metaformat   [-nonInteractive] [-force]");
        System.err.println("       bookieformat [-nonInteractive] [-force]");
        System.err.println("       recover      <bookieSrc> [bookieDest]");
        System.err.println("       ledger       [-meta] <ledger_id>");
        System.err.println("       readlog      [-msg] <entry_log_id|entry_log_file_name>");
        System.err.println("       readjournal  [-msg] <journal_id|journal_file_name>");
        System.err.println("       autorecovery [-enable|-disable]");
        System.err.println("       lastmark");
        System.err.println("       help");
    }

    @Override
    public int run(String[] args) throws Exception {
        if (args.length <= 0) {
            printShellUsage();
            return -1;
        }
        String cmdName = args[0];
        Command cmd = commands.get(cmdName);
        if (null == cmd) {
            System.err.println("ERROR: Unknown command " + cmdName);
            printShellUsage();
            return -1;
        }
        // prepare new args
        String[] newArgs = new String[args.length - 1];
        System.arraycopy(args, 1, newArgs, 0, newArgs.length);
        return cmd.runCmd(newArgs);
    }

    public static void main(String argv[]) throws Exception {
        if (argv.length <= 0) {
            printShellUsage();
            System.exit(-1);
        }

        CompositeConfiguration conf = new CompositeConfiguration();
        // load configuration
        if ("-conf".equals(argv[0])) {
            if (argv.length <= 1) {
                printShellUsage();
                System.exit(-1);
            }
            conf.addConfiguration(new PropertiesConfiguration(
                                  new File(argv[1]).toURI().toURL()));

            String[] newArgv = new String[argv.length - 2];
            System.arraycopy(argv, 2, newArgv, 0, newArgv.length);
            argv = newArgv;
        }

        BookieShell shell = new BookieShell();
        shell.setConf(conf);
        int res = shell.run(argv);
        System.exit(res);
    }

    ///
    /// Bookie File Operations
    ///

    /**
     * Get the ledger file of a specified ledger.
     *
     * @param ledgerId
     *          Ledger Id
     *
     * @return file object.
     */
    private File getLedgerFile(long ledgerId) {
        String ledgerName = LedgerCacheImpl.getLedgerName(ledgerId);
        File lf = null;
        for (File d : ledgerDirectories) {
            lf = new File(d, ledgerName);
            if (lf.exists()) {
                break;
            }
            lf = null;
        }
        return lf;
    }

    /**
     * Get FileInfo for a specified ledger.
     *
     * @param ledgerId
     *          Ledger Id
     * @return read only file info instance
     */
    ReadOnlyFileInfo getFileInfo(long ledgerId) throws IOException {
        File ledgerFile = getLedgerFile(ledgerId);
        if (null == ledgerFile) {
            throw new FileNotFoundException("No index file found for ledger " + ledgerId + ". It may be not flushed yet.");
        }
        ReadOnlyFileInfo fi = new ReadOnlyFileInfo(ledgerFile, null);
        fi.readHeader();
        return fi;
    }

    private synchronized void initEntryLogger() throws IOException {
        if (null == entryLogger) {
            // provide read only entry logger
            entryLogger = new ReadOnlyEntryLogger(bkConf);
        }
    }

    /**
     * scan over entry log
     *
     * @param logId
     *          Entry Log Id
     * @param scanner
     *          Entry Log Scanner
     */
    protected void scanEntryLog(long logId, EntryLogScanner scanner) throws IOException {
        initEntryLogger();
        entryLogger.scanEntryLog(logId, scanner);
    }

    private synchronized Journal getJournal() throws IOException {
        if (null == journal) {
            journal = new Journal(bkConf, new LedgerDirsManager(bkConf));
        }
        return journal;
    }

    /**
     * Scan journal file
     *
     * @param journalId
     *          Journal File Id
     * @param scanner
     *          Journal File Scanner
     */
    protected void scanJournal(long journalId, JournalScanner scanner) throws IOException {
        getJournal().scanJournal(journalId, 0L, scanner);
    }

    ///
    /// Bookie Shell Commands
    ///

    /**
     * Read ledger meta
     *
     * @param ledgerId
     *          Ledger Id
     */
    protected void readLedgerMeta(long ledgerId) throws Exception {
        System.out.println("===== LEDGER: " + ledgerId + " =====");
        FileInfo fi = getFileInfo(ledgerId);
        byte[] masterKey = fi.getMasterKey();
        if (null == masterKey) {
            System.out.println("master key  : NULL");
        } else {
            System.out.println("master key  : " + bytes2Hex(fi.getMasterKey()));
        }
        long size = fi.size();
        if (size % 8 == 0) {
            System.out.println("size        : " + size);
        } else {
            System.out.println("size : " + size + " (not aligned with 8, may be corrupted or under flushing now)");
        }
        System.out.println("entries     : " + (size / 8));
    }

    /**
     * Read ledger index entires
     *
     * @param ledgerId
     *          Ledger Id
     * @throws IOException
     */
    protected void readLedgerIndexEntries(long ledgerId) throws IOException {
        System.out.println("===== LEDGER: " + ledgerId + " =====");
        FileInfo fi = getFileInfo(ledgerId);
        long size = fi.size();
        System.out.println("size        : " + size);
        long curSize = 0;
        long curEntry = 0;
        LedgerEntryPage lep = new LedgerEntryPage(pageSize, entriesPerPage);
        lep.usePage();
        try {
            while (curSize < size) {
                lep.setLedger(ledgerId);
                lep.setFirstEntry(curEntry);
                lep.readPage(fi);

                // process a page
                for (int i=0; i<entriesPerPage; i++) {
                    long offset = lep.getOffset(i * 8);
                    if (0 == offset) {
                        System.out.println("entry " + curEntry + "\t:\tN/A");
                    } else {
                        long entryLogId = offset >> 32L;
                        long pos = offset & 0xffffffffL;
                        System.out.println("entry " + curEntry + "\t:\t(log:" + entryLogId + ", pos: " + pos + ")");
                    }
                    ++curEntry;
                }

                curSize += pageSize;
            }
        } catch (IOException ie) {
            LOG.error("Failed to read index page : ", ie);
            if (curSize + pageSize < size) {
                System.out.println("Failed to read index page @ " + curSize + ", the index file may be corrupted : " + ie.getMessage());
            } else {
                System.out.println("Failed to read last index page @ " + curSize
                                 + ", the index file may be corrupted or last index page is not fully flushed yet : " + ie.getMessage());
            }
        }
    }

    /**
     * Scan over an entry log file.
     *
     * @param logId
     *          Entry Log File id.
     * @param printMsg
     *          Whether printing the entry data.
     */
    protected void scanEntryLog(long logId, final boolean printMsg) throws Exception {
        System.out.println("Scan entry log " + logId + " (" + Long.toHexString(logId) + ".log)");
        scanEntryLog(logId, new EntryLogScanner() {
            @Override
            public boolean accept(long ledgerId) {
                return true;
            }
            @Override
            public void process(long ledgerId, long startPos, ByteBuffer entry) {
                formatEntry(startPos, entry, printMsg);
            }
        });
    }

    /**
     * Scan a journal file
     *
     * @param journalId
     *          Journal File Id
     * @param printMsg
     *          Whether printing the entry data.
     */
    protected void scanJournal(long journalId, final boolean printMsg) throws Exception {
        System.out.println("Scan journal " + journalId + " (" + Long.toHexString(journalId) + ".txn)");
        scanJournal(journalId, new JournalScanner() {
            boolean printJournalVersion = false;
            @Override
            public void process(int journalVersion, long offset, ByteBuffer entry) throws IOException {
                if (!printJournalVersion) {
                    System.out.println("Journal Version : " + journalVersion);
                    printJournalVersion = true;
                }
                formatEntry(offset, entry, printMsg);
            }
        });
    }

    /**
     * Print last log mark
     */
    protected void printLastLogMark() throws IOException {
        LastLogMark lastLogMark = getJournal().getLastLogMark();
        System.out.println("LastLogMark: Journal Id - " + lastLogMark.getTxnLogId() + "("
                + Long.toHexString(lastLogMark.getTxnLogId()) + ".txn), Pos - "
                + lastLogMark.getTxnLogPosition());
    }

    /**
     * Format the message into a readable format.
     *
     * @param pos
     *          File offset of the message stored in entry log file
     * @param recBuff
     *          Entry Data
     * @param printMsg
     *          Whether printing the message body
     */
    private void formatEntry(long pos, ByteBuffer recBuff, boolean printMsg) {
        long ledgerId = recBuff.getLong();
        long entryId = recBuff.getLong();
        int entrySize = recBuff.limit();

        System.out.println("--------- Lid=" + ledgerId + ", Eid=" + entryId
                         + ", ByteOffset=" + pos + ", EntrySize=" + entrySize + " ---------");
        if (entryId == Bookie.METAENTRY_ID_LEDGER_KEY) {
            int masterKeyLen = recBuff.getInt();
            byte[] masterKey = new byte[masterKeyLen];
            recBuff.get(masterKey);
            System.out.println("Type:           META");
            System.out.println("MasterKey:      " + bytes2Hex(masterKey));
            System.out.println();
            return;
        }
        // process a data entry
        long lastAddConfirmed = recBuff.getLong();
        System.out.println("Type:           DATA");
        System.out.println("LastConfirmed:  " + lastAddConfirmed);
        if (!printMsg) {
            System.out.println();
            return;
        }
        // skip digest checking
        recBuff.position(32 + 8);
        System.out.println("Data:");
        System.out.println();
        try {
            byte[] ret = new byte[recBuff.remaining()];
            recBuff.get(ret);
            formatter.formatEntry(ret);
        } catch (Exception e) {
            System.out.println("N/A. Corrupted.");
        }
        System.out.println();
    }

    static String bytes2Hex(byte[] data) {
        StringBuilder sb = new StringBuilder(data.length * 2);
        Formatter formatter = new Formatter(sb);
        for (byte b : data) {
            formatter.format("%02x", b);
        }
        return sb.toString();
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/BufferedChannel.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

import java.io.IOException;
import java.nio.ByteBuffer;
import java.nio.channels.FileChannel;

/**
 * Provides a buffering layer in front of a FileChannel.
 */
public class BufferedChannel
{
    ByteBuffer writeBuffer;
    ByteBuffer readBuffer;
    private FileChannel bc;
    long position;
    int capacity;
    long readBufferStartPosition;
    long writeBufferStartPosition;
    // make constructor to be public for unit test
    public BufferedChannel(FileChannel bc, int capacity) throws IOException {
        this.bc = bc;
        this.capacity = capacity;
        position = bc.position();
        writeBufferStartPosition = position;
    }

    /**
     * @return file channel
     */
    FileChannel getFileChannel() {
        return this.bc;
    }

    /*    public void close() throws IOException {
            bc.close();
        }
    */
//    public boolean isOpen() {
//        return bc.isOpen();
//    }

    synchronized public int write(ByteBuffer src) throws IOException {
        int copied = 0;
        if (writeBuffer == null) {
            writeBuffer = ByteBuffer.allocateDirect(capacity);
        }
        while(src.remaining() > 0) {
            int truncated = 0;
            if (writeBuffer.remaining() < src.remaining()) {
                truncated = src.remaining() - writeBuffer.remaining();
                src.limit(src.limit()-truncated);
            }
            copied += src.remaining();
            writeBuffer.put(src);
            src.limit(src.limit()+truncated);
            if (writeBuffer.remaining() == 0) {
                writeBuffer.flip();
                bc.write(writeBuffer);
                writeBuffer.clear();
                writeBufferStartPosition = bc.position();
            }
        }
        position += copied;
        return copied;
    }

    public long position() {
        return position;
    }

    /**
     * Retrieve the current size of the underlying FileChannel
     *
     * @return FileChannel size measured in bytes
     *
     * @throws IOException if some I/O error occurs reading the FileChannel
     */
    public long size() throws IOException {
        return bc.size();
    }

    public void flush(boolean sync) throws IOException {
        synchronized(this) {
            if (writeBuffer == null) {
                return;
            }
            writeBuffer.flip();
            bc.write(writeBuffer);
            writeBuffer.clear();
            writeBufferStartPosition = bc.position();
        }
        if (sync) {
            bc.force(false);
        }
    }

    /*public Channel getInternalChannel() {
        return bc;
    }*/
    synchronized public int read(ByteBuffer buff, long pos) throws IOException {
        if (readBuffer == null) {
            readBuffer = ByteBuffer.allocateDirect(capacity);
            readBufferStartPosition = Long.MIN_VALUE;
        }
        long prevPos = pos;
        while(buff.remaining() > 0) {
            // check if it is in the write buffer
            if (writeBuffer != null && writeBufferStartPosition <= pos) {
                long positionInBuffer = pos - writeBufferStartPosition;
                long bytesToCopy = writeBuffer.position()-positionInBuffer;
                if (bytesToCopy > buff.remaining()) {
                    bytesToCopy = buff.remaining();
                }
                if (bytesToCopy == 0) {
                    throw new IOException("Read past EOF");
                }
                ByteBuffer src = writeBuffer.duplicate();
                src.position((int) positionInBuffer);
                src.limit((int) (positionInBuffer+bytesToCopy));
                buff.put(src);
                pos+= bytesToCopy;
            } else if (writeBuffer == null && writeBufferStartPosition <= pos) {
                // here we reach the end
                break;
                // first check if there is anything we can grab from the readBuffer
            } else if (readBufferStartPosition <= pos && pos < readBufferStartPosition+readBuffer.capacity()) {
                long positionInBuffer = pos - readBufferStartPosition;
                long bytesToCopy = readBuffer.capacity()-positionInBuffer;
                if (bytesToCopy > buff.remaining()) {
                    bytesToCopy = buff.remaining();
                }
                ByteBuffer src = readBuffer.duplicate();
                src.position((int) positionInBuffer);
                src.limit((int) (positionInBuffer+bytesToCopy));
                buff.put(src);
                pos += bytesToCopy;
                // let's read it
            } else {
                readBufferStartPosition = pos;
                readBuffer.clear();
                // make sure that we don't overlap with the write buffer
                if (readBufferStartPosition + readBuffer.capacity() >= writeBufferStartPosition) {
                    readBufferStartPosition = writeBufferStartPosition - readBuffer.capacity();
                    if (readBufferStartPosition < 0) {
                        readBuffer.put(LedgerEntryPage.zeroPage, 0, (int)-readBufferStartPosition);
                    }
                }
                while(readBuffer.remaining() > 0) {
                    if (bc.read(readBuffer, readBufferStartPosition+readBuffer.position()) <= 0) {
                        throw new IOException("Short read");
                    }
                }
                readBuffer.put(LedgerEntryPage.zeroPage, 0, readBuffer.remaining());
                readBuffer.clear();
            }
        }
        return (int)(pos - prevPos);
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/Cookie.java,false,"/**
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */
package org.apache.bookkeeper.bookie;

import java.io.BufferedReader;
import java.io.EOFException;
import java.io.File;
import java.io.FileOutputStream;
import java.io.FileReader;
import java.io.OutputStreamWriter;
import java.io.BufferedWriter;
import java.io.IOException;
import java.io.StringReader;

import java.net.UnknownHostException;

import org.apache.zookeeper.ZooKeeper;
import org.apache.zookeeper.data.Stat;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.ZooDefs.Ids;

import org.apache.bookkeeper.util.BookKeeperConstants;
import org.apache.bookkeeper.util.StringUtils;
import org.apache.bookkeeper.conf.ServerConfiguration;
import org.apache.bookkeeper.proto.DataFormats.CookieFormat;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.protobuf.TextFormat;

/**
 * When a bookie starts for the first time it generates  a cookie, and stores
 * the cookie in zookeeper as well as in the each of the local filesystem
 * directories it uses. This cookie is used to ensure that for the life of the
 * bookie, its configuration stays the same. If any of the bookie directories
 * becomes unavailable, the bookie becomes unavailable. If the bookie changes
 * port, it must also reset all of its data.
 *
 * This is done to ensure data integrity. Without the cookie a bookie could
 * start with one of its ledger directories missing, so data would be missing,
 * but the bookie would be up, so the client would think that everything is ok
 * with the cluster. It's better to fail early and obviously.
 */
class Cookie {
    static Logger LOG = LoggerFactory.getLogger(Cookie.class);

    static final int CURRENT_COOKIE_LAYOUT_VERSION = 4;
    private int layoutVersion = 0;
    private String bookieHost = null;
    private String journalDir = null;
    private String ledgerDirs = null;
    private int znodeVersion = -1;
    private String instanceId = null;

    private Cookie() {
    }

    public void verify(Cookie c) throws BookieException.InvalidCookieException {
        String errMsg;
        if (c.layoutVersion < 3 && c.layoutVersion != layoutVersion) {
            errMsg = "Cookie is of too old version " + c.layoutVersion;
            LOG.error(errMsg);
            throw new BookieException.InvalidCookieException(errMsg);
        } else if (!(c.layoutVersion >= 3 && c.bookieHost.equals(bookieHost)
                && c.journalDir.equals(journalDir) && c.ledgerDirs
                    .equals(ledgerDirs))) {
            errMsg = "Cookie [" + this + "] is not matching with [" + c + "]";
            throw new BookieException.InvalidCookieException(errMsg);
        } else if ((instanceId == null && c.instanceId != null)
                || (instanceId != null && !instanceId.equals(c.instanceId))) {
            // instanceId should be same in both cookies
            errMsg = "instanceId " + instanceId
                    + " is not matching with " + c.instanceId;
            throw new BookieException.InvalidCookieException(errMsg);
        }
    }

    public String toString() {
        if (layoutVersion <= 3) {
            return toStringVersion3();
        }
        CookieFormat.Builder builder = CookieFormat.newBuilder();
        builder.setBookieHost(bookieHost);
        builder.setJournalDir(journalDir);
        builder.setLedgerDirs(ledgerDirs);
        if (null != instanceId) {
            builder.setInstanceId(instanceId);
        }
        StringBuilder b = new StringBuilder();
        b.append(CURRENT_COOKIE_LAYOUT_VERSION).append("\n");
        b.append(TextFormat.printToString(builder.build()));
        return b.toString();
    }

    private String toStringVersion3() {
        StringBuilder b = new StringBuilder();
        b.append(CURRENT_COOKIE_LAYOUT_VERSION).append("\n")
            .append(bookieHost).append("\n")
            .append(journalDir).append("\n")
            .append(ledgerDirs).append("\n");
        return b.toString();
    }

    private static Cookie parse(BufferedReader reader) throws IOException {
        Cookie c = new Cookie();
        String line = reader.readLine();
        if (null == line) {
            throw new EOFException("Exception in parsing cookie");
        }
        try {
            c.layoutVersion = Integer.parseInt(line.trim());
        } catch (NumberFormatException e) {
            throw new IOException("Invalid string '" + line.trim()
                    + "', cannot parse cookie.");
        }
        if (c.layoutVersion == 3) {
            c.bookieHost = reader.readLine();
            c.journalDir = reader.readLine();
            c.ledgerDirs = reader.readLine();
        } else if (c.layoutVersion >= 4) {
            CookieFormat.Builder builder = CookieFormat.newBuilder();
            TextFormat.merge(reader, builder);
            CookieFormat data = builder.build();
            c.bookieHost = data.getBookieHost();
            c.journalDir = data.getJournalDir();
            c.ledgerDirs = data.getLedgerDirs();
            // Since InstanceId is optional
            if (null != data.getInstanceId() && !data.getInstanceId().isEmpty()) {
                c.instanceId = data.getInstanceId();
            }
        }
        return c;
    }

    void writeToDirectory(File directory) throws IOException {
        File versionFile = new File(directory,
                BookKeeperConstants.VERSION_FILENAME);

        FileOutputStream fos = new FileOutputStream(versionFile);
        BufferedWriter bw = null;
        try {
            bw = new BufferedWriter(new OutputStreamWriter(fos));
            bw.write(toString());
        } finally {
            if (bw != null) {
                bw.close();
            }
            fos.close();
        }
    }

    void writeToZooKeeper(ZooKeeper zk, ServerConfiguration conf)
            throws KeeperException, InterruptedException, UnknownHostException {
        String bookieCookiePath = conf.getZkLedgersRootPath() + "/"
                + BookKeeperConstants.COOKIE_NODE;
        String zkPath = getZkPath(conf);
        byte[] data = toString().getBytes();
        if (znodeVersion != -1) {
            zk.setData(zkPath, data, znodeVersion);
        } else {
            if (zk.exists(bookieCookiePath, false) == null) {
                try {
                    zk.create(bookieCookiePath, new byte[0],
                              Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
                } catch (KeeperException.NodeExistsException nne) {
                    LOG.info("More than one bookie tried to create {} at once. Safe to ignore",
                             bookieCookiePath);
                }
            }
            zk.create(zkPath, data,
                      Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
            Stat stat = zk.exists(zkPath, false);
            this.znodeVersion = stat.getVersion();
        }
    }

    void deleteFromZooKeeper(ZooKeeper zk, ServerConfiguration conf)
            throws KeeperException, InterruptedException, UnknownHostException {
        String zkPath = getZkPath(conf);
        if (znodeVersion != -1) {
            zk.delete(zkPath, znodeVersion);
        }
        znodeVersion = -1;
    }

    static Cookie generateCookie(ServerConfiguration conf)
            throws UnknownHostException {
        Cookie c = new Cookie();
        c.layoutVersion = CURRENT_COOKIE_LAYOUT_VERSION;
        c.bookieHost = StringUtils.addrToString(Bookie.getBookieAddress(conf));
        c.journalDir = conf.getJournalDirName();
        StringBuilder b = new StringBuilder();
        String[] dirs = conf.getLedgerDirNames();
        b.append(dirs.length);
        for (String d : dirs) {
            b.append("\t").append(d);
        }
        c.ledgerDirs = b.toString();
        return c;
    }

    static Cookie readFromZooKeeper(ZooKeeper zk, ServerConfiguration conf)
            throws KeeperException, InterruptedException, IOException, UnknownHostException {
        String zkPath = getZkPath(conf);

        Stat stat = zk.exists(zkPath, false);
        byte[] data = zk.getData(zkPath, false, stat);
        BufferedReader reader = new BufferedReader(new StringReader(new String(
                data)));
        try {
            Cookie c = parse(reader);
            c.znodeVersion = stat.getVersion();
            return c;
        } finally {
            reader.close();
        }
    }

    static Cookie readFromDirectory(File directory) throws IOException {
        File versionFile = new File(directory,
                BookKeeperConstants.VERSION_FILENAME);
        BufferedReader reader = new BufferedReader(new FileReader(versionFile));
        try {
            return parse(reader);
        } finally {
            reader.close();
        }
    }

    public void setInstanceId(String instanceId) {
        this.instanceId = instanceId;
    }

    private static String getZkPath(ServerConfiguration conf)
            throws UnknownHostException {
        String bookieCookiePath = conf.getZkLedgersRootPath() + "/"
                + BookKeeperConstants.COOKIE_NODE;
        return bookieCookiePath + "/" + StringUtils.addrToString(Bookie.getBookieAddress(conf));
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/EntryLogger.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileFilter;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.OutputStreamWriter;
import java.io.RandomAccessFile;
import java.nio.ByteBuffer;
import java.nio.channels.FileChannel;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import java.util.Map.Entry;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.atomic.AtomicBoolean;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.bookkeeper.bookie.LedgerDirsManager.LedgerDirsListener;
import org.apache.bookkeeper.conf.ServerConfiguration;
import org.apache.bookkeeper.util.IOUtils;

/**
 * This class manages the writing of the bookkeeper entries. All the new
 * entries are written to a common log. The LedgerCache will have pointers
 * into files created by this class with offsets into the files to find
 * the actual ledger entry. The entry log files created by this class are
 * identified by a long.
 */
public class EntryLogger {
    private static final Logger LOG = LoggerFactory.getLogger(EntryLogger.class);

    volatile File currentDir;
    private LedgerDirsManager ledgerDirsManager;
    private AtomicBoolean shouldCreateNewEntryLog = new AtomicBoolean(false);

    private long logId;

    /**
     * The maximum size of a entry logger file.
     */
    final long logSizeLimit;
    private volatile BufferedChannel logChannel;
    /**
     * The 1K block at the head of the entry logger file
     * that contains the fingerprint and (future) meta-data
     */
    final static int LOGFILE_HEADER_SIZE = 1024;
    final ByteBuffer LOGFILE_HEADER = ByteBuffer.allocate(LOGFILE_HEADER_SIZE);

    final static int MIN_SANE_ENTRY_SIZE = 8 + 8;
    final static long MB = 1024 * 1024;

    /**
     * Scan entries in a entry log file.
     */
    static interface EntryLogScanner {
        /**
         * Tests whether or not the entries belongs to the specified ledger
         * should be processed.
         *
         * @param ledgerId
         *          Ledger ID.
         * @return true if and only the entries of the ledger should be scanned.
         */
        public boolean accept(long ledgerId);

        /**
         * Process an entry.
         *
         * @param ledgerId
         *          Ledger ID.
         * @param offset
         *          File offset of this entry.
         * @param entry
         *          Entry ByteBuffer
         * @throws IOException
         */
        public void process(long ledgerId, long offset, ByteBuffer entry) throws IOException;
    }

    /**
     * Create an EntryLogger that stores it's log files in the given
     * directories
     */
    public EntryLogger(ServerConfiguration conf,
            LedgerDirsManager ledgerDirsManager) throws IOException {
        this.ledgerDirsManager = ledgerDirsManager;
        // log size limit
        this.logSizeLimit = conf.getEntryLogSizeLimit();

        // Initialize the entry log header buffer. This cannot be a static object
        // since in our unit tests, we run multiple Bookies and thus EntryLoggers
        // within the same JVM. All of these Bookie instances access this header
        // so there can be race conditions when entry logs are rolled over and
        // this header buffer is cleared before writing it into the new logChannel.
        LOGFILE_HEADER.put("BKLO".getBytes());

        // Find the largest logId
        logId = -1;
        for (File dir : ledgerDirsManager.getAllLedgerDirs()) {
            if (!dir.exists()) {
                throw new FileNotFoundException(
                        "Entry log directory does not exist");
            }
            long lastLogId = getLastLogId(dir);
            if (lastLogId > logId) {
                logId = lastLogId;
            }
        }

        initialize();
    }

    /**
     * Maps entry log files to open channels.
     */
    private ConcurrentHashMap<Long, BufferedChannel> channels = new ConcurrentHashMap<Long, BufferedChannel>();

    synchronized long getCurrentLogId() {
        return logId;
    }

    protected void initialize() throws IOException {
        // Register listener for disk full notifications.
        ledgerDirsManager.addLedgerDirsListener(getLedgerDirsListener());
        // create a new log to write
        createNewLog();
    }

    private LedgerDirsListener getLedgerDirsListener() {
        return new LedgerDirsListener() {
            @Override
            public void diskFull(File disk) {
                // If the current entry log disk is full, then create new entry
                // log.
                if (currentDir != null && currentDir.equals(disk)) {
                    shouldCreateNewEntryLog.set(true);
                }
            }

            @Override
            public void diskFailed(File disk) {
                // Nothing to handle here. Will be handled in Bookie
            }

            @Override
            public void allDisksFull() {
                // Nothing to handle here. Will be handled in Bookie
            }

            @Override
            public void fatalError() {
                // Nothing to handle here. Will be handled in Bookie
            }
        };
    }

    /**
     * Creates a new log file
     */
    void createNewLog() throws IOException {
        if (logChannel != null) {
            logChannel.flush(true);
        }

        // It would better not to overwrite existing entry log files
        String logFileName = null;
        do {
            logFileName = Long.toHexString(++logId) + ".log";
            for (File dir : ledgerDirsManager.getAllLedgerDirs()) {
                File newLogFile = new File(dir, logFileName);
                if (newLogFile.exists()) {
                    LOG.warn("Found existed entry log " + newLogFile
                           + " when trying to create it as a new log.");
                    logFileName = null;
                    break;
                }
            }
        } while (logFileName == null);

        // Update last log id first
        currentDir = ledgerDirsManager.pickRandomWritableDir();
        setLastLogId(currentDir, logId);

        File newLogFile = new File(currentDir, logFileName);
        logChannel = new BufferedChannel(new RandomAccessFile(newLogFile, "rw").getChannel(), 64*1024);
        logChannel.write((ByteBuffer) LOGFILE_HEADER.clear());
        channels.put(logId, logChannel);
    }

    /**
     * Remove entry log.
     *
     * @param entryLogId
     *          Entry Log File Id
     */
    protected boolean removeEntryLog(long entryLogId) {
        BufferedChannel bc = channels.remove(entryLogId);
        if (null != bc) {
            // close its underlying file channel, so it could be deleted really
            try {
                bc.getFileChannel().close();
            } catch (IOException ie) {
                LOG.warn("Exception while closing garbage collected entryLog file : ", ie);
            }
        }
        File entryLogFile;
        try {
            entryLogFile = findFile(entryLogId);
        } catch (FileNotFoundException e) {
            LOG.error("Trying to delete an entryLog file that could not be found: "
                    + entryLogId + ".log");
            return false;
        }
        if (!entryLogFile.delete()) {
            LOG.warn("Could not delete entry log file {}", entryLogFile);
        }
        return true;
    }

    /**
     * writes the given id to the "lastId" file in the given directory.
     */
    private void setLastLogId(File dir, long logId) throws IOException {
        FileOutputStream fos;
        fos = new FileOutputStream(new File(dir, "lastId"));
        BufferedWriter bw = new BufferedWriter(new OutputStreamWriter(fos));
        try {
            bw.write(Long.toHexString(logId) + "\n");
            bw.flush();
        } finally {
            try {
                bw.close();
            } catch (IOException e) {
            }
        }
    }

    private long getLastLogId(File dir) {
        long id = readLastLogId(dir);
        // read success
        if (id > 0) {
            return id;
        }
        // read failed, scan the ledger directories to find biggest log id
        File[] logFiles = dir.listFiles(new FileFilter() {
            @Override
            public boolean accept(File file) {
                return file.getName().endsWith(".log");
            }
        });
        List<Long> logs = new ArrayList<Long>();
        for (File lf : logFiles) {
            String idString = lf.getName().split("\\.")[0];
            try {
                long lid = Long.parseLong(idString, 16);
                logs.add(lid);
            } catch (NumberFormatException nfe) {
            }
        }
        // no log file found in this directory
        if (0 == logs.size()) {
            return -1;
        }
        // order the collections
        Collections.sort(logs);
        return logs.get(logs.size() - 1);
    }

    /**
     * reads id from the "lastId" file in the given directory.
     */
    private long readLastLogId(File f) {
        FileInputStream fis;
        try {
            fis = new FileInputStream(new File(f, "lastId"));
        } catch (FileNotFoundException e) {
            return -1;
        }
        BufferedReader br = new BufferedReader(new InputStreamReader(fis));
        try {
            String lastIdString = br.readLine();
            return Long.parseLong(lastIdString, 16);
        } catch (IOException e) {
            return -1;
        } catch(NumberFormatException e) {
            return -1;
        } finally {
            try {
                br.close();
            } catch (IOException e) {
            }
        }
    }

    synchronized void flush() throws IOException {
        if (logChannel != null) {
            logChannel.flush(true);
        }
    }
    synchronized long addEntry(long ledger, ByteBuffer entry) throws IOException {
        // Create new log if logSizeLimit reached or current disk is full
        boolean createNewLog = shouldCreateNewEntryLog.get();
        if (createNewLog
                || (logChannel.position() + entry.remaining() + 4 > logSizeLimit)) {
            createNewLog();

            // Reset the flag
            if (createNewLog) {
                shouldCreateNewEntryLog.set(false);
            }
        }
        ByteBuffer buff = ByteBuffer.allocate(4);
        buff.putInt(entry.remaining());
        buff.flip();
        logChannel.write(buff);
        long pos = logChannel.position();
        logChannel.write(entry);
        //logChannel.flush(false);

        return (logId << 32L) | pos;
    }

    byte[] readEntry(long ledgerId, long entryId, long location) throws IOException, Bookie.NoEntryException {
        long entryLogId = location >> 32L;
        long pos = location & 0xffffffffL;
        ByteBuffer sizeBuff = ByteBuffer.allocate(4);
        pos -= 4; // we want to get the ledgerId and length to check
        BufferedChannel fc;
        try {
            fc = getChannelForLogId(entryLogId);
        } catch (FileNotFoundException e) {
            FileNotFoundException newe = new FileNotFoundException(e.getMessage() + " for " + ledgerId + " with location " + location);
            newe.setStackTrace(e.getStackTrace());
            throw newe;
        }
        if (fc.read(sizeBuff, pos) != sizeBuff.capacity()) {
            throw new Bookie.NoEntryException("Short read from entrylog " + entryLogId,
                                              ledgerId, entryId);
        }
        pos += 4;
        sizeBuff.flip();
        int entrySize = sizeBuff.getInt();
        // entrySize does not include the ledgerId
        if (entrySize > MB) {
            LOG.error("Sanity check failed for entry size of " + entrySize + " at location " + pos + " in " + entryLogId);

        }
        if (entrySize < MIN_SANE_ENTRY_SIZE) {
            LOG.error("Read invalid entry length {}", entrySize);
            throw new IOException("Invalid entry length " + entrySize);
        }
        byte data[] = new byte[entrySize];
        ByteBuffer buff = ByteBuffer.wrap(data);
        int rc = fc.read(buff, pos);
        if ( rc != data.length) {
            // Note that throwing NoEntryException here instead of IOException is not
            // without risk. If all bookies in a quorum throw this same exception
            // the client will assume that it has reached the end of the ledger.
            // However, this may not be the case, as a very specific error condition
            // could have occurred, where the length of the entry was corrupted on all
            // replicas. However, the chance of this happening is very very low, so
            // returning NoEntryException is mostly safe.
            throw new Bookie.NoEntryException("Short read for " + ledgerId + "@"
                                              + entryId + " in " + entryLogId + "@"
                                              + pos + "("+rc+"!="+data.length+")", ledgerId, entryId);
        }
        buff.flip();
        long thisLedgerId = buff.getLong();
        if (thisLedgerId != ledgerId) {
            throw new IOException("problem found in " + entryLogId + "@" + entryId + " at position + " + pos + " entry belongs to " + thisLedgerId + " not " + ledgerId);
        }
        long thisEntryId = buff.getLong();
        if (thisEntryId != entryId) {
            throw new IOException("problem found in " + entryLogId + "@" + entryId + " at position + " + pos + " entry is " + thisEntryId + " not " + entryId);
        }

        return data;
    }

    private BufferedChannel getChannelForLogId(long entryLogId) throws IOException {
        BufferedChannel fc = channels.get(entryLogId);
        if (fc != null) {
            return fc;
        }
        File file = findFile(entryLogId);
        // get channel is used to open an existing entry log file
        // it would be better to open using read mode
        FileChannel newFc = new RandomAccessFile(file, "r").getChannel();
        // If the file already exists before creating a BufferedChannel layer above it,
        // set the FileChannel's position to the end so the write buffer knows where to start.
        newFc.position(newFc.size());
        fc = new BufferedChannel(newFc, 8192);

        BufferedChannel oldfc = channels.putIfAbsent(entryLogId, fc);
        if (oldfc != null) {
            newFc.close();
            return oldfc;
        } else {
            return fc;
        }
    }

    /**
     * Whether the log file exists or not.
     */
    boolean logExists(long logId) {
        for (File d : ledgerDirsManager.getAllLedgerDirs()) {
            File f = new File(d, Long.toHexString(logId) + ".log");
            if (f.exists()) {
                return true;
            }
        }
        return false;
    }

    private File findFile(long logId) throws FileNotFoundException {
        for (File d : ledgerDirsManager.getAllLedgerDirs()) {
            File f = new File(d, Long.toHexString(logId)+".log");
            if (f.exists()) {
                return f;
            }
        }
        throw new FileNotFoundException("No file for log " + Long.toHexString(logId));
    }

    /**
     * Scan entry log
     *
     * @param entryLogId
     *          Entry Log Id
     * @param scanner
     *          Entry Log Scanner
     * @throws IOException
     */
    protected void scanEntryLog(long entryLogId, EntryLogScanner scanner) throws IOException {
        ByteBuffer sizeBuff = ByteBuffer.allocate(4);
        ByteBuffer lidBuff = ByteBuffer.allocate(8);
        BufferedChannel bc;
        // Get the BufferedChannel for the current entry log file
        try {
            bc = getChannelForLogId(entryLogId);
        } catch (IOException e) {
            LOG.warn("Failed to get channel to scan entry log: " + entryLogId + ".log");
            throw e;
        }
        // Start the read position in the current entry log file to be after
        // the header where all of the ledger entries are.
        long pos = LOGFILE_HEADER_SIZE;
        // Read through the entry log file and extract the ledger ID's.
        while (true) {
            // Check if we've finished reading the entry log file.
            if (pos >= bc.size()) {
                break;
            }
            if (bc.read(sizeBuff, pos) != sizeBuff.capacity()) {
                throw new IOException("Short read for entry size from entrylog " + entryLogId);
            }
            long offset = pos;
            pos += 4;
            sizeBuff.flip();
            int entrySize = sizeBuff.getInt();
            if (entrySize > MB) {
                LOG.warn("Found large size entry of " + entrySize + " at location " + pos + " in "
                        + entryLogId);
            }
            sizeBuff.clear();
            // try to read ledger id first
            if (bc.read(lidBuff, pos) != lidBuff.capacity()) {
                throw new IOException("Short read for ledger id from entrylog " + entryLogId);
            }
            lidBuff.flip();
            long lid = lidBuff.getLong();
            lidBuff.clear();
            if (!scanner.accept(lid)) {
                // skip this entry
                pos += entrySize;
                continue;
            }
            // read the entry
            byte data[] = new byte[entrySize];
            ByteBuffer buff = ByteBuffer.wrap(data);
            int rc = bc.read(buff, pos);
            if (rc != data.length) {
                throw new IOException("Short read for ledger entry from entryLog " + entryLogId
                                    + "@" + pos + "(" + rc + "!=" + data.length + ")");
            }
            buff.flip();
            // process the entry
            scanner.process(lid, offset, buff);
            // Advance position to the next entry
            pos += entrySize;
        }
    }

    /**
     * Shutdown method to gracefully stop entry logger.
     */
    public void shutdown() {
        // since logChannel is buffered channel, do flush when shutting down
        try {
            flush();
            for (Entry<Long, BufferedChannel> channelEntry : channels
                    .entrySet()) {
                channelEntry.getValue().getFileChannel().close();
            }
        } catch (IOException ie) {
            // we have no idea how to avoid io exception during shutting down, so just ignore it
            LOG.error("Error flush entry log during shutting down, which may cause entry log corrupted.", ie);
        } finally {
            for (Entry<Long, BufferedChannel> channelEntry : channels
                    .entrySet()) {
                FileChannel fileChannel = channelEntry.getValue()
                        .getFileChannel();
                if (fileChannel.isOpen()) {
                    IOUtils.close(LOG, fileChannel);
                }
            }
        }
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/ExitCode.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

/**
 * Exit code used to exit bookie server
 */
public class ExitCode {
    // normal quit
    public final static int OK                  = 0;
    // invalid configuration
    public final static int INVALID_CONF        = 1;
    // exception running bookie server
    public final static int SERVER_EXCEPTION    = 2;
    // zookeeper is expired
    public final static int ZK_EXPIRED          = 3;
    // register bookie on zookeeper failed
    public final static int ZK_REG_FAIL         = 4;
    // exception running bookie
    public final static int BOOKIE_EXCEPTION    = 5;
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/FileInfo.java,true,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

import java.io.File;
import java.io.IOException;
import java.io.RandomAccessFile;
import java.nio.ByteBuffer;
import java.nio.BufferUnderflowException;
import java.nio.channels.FileChannel;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * This is the file handle for a ledger's index file that maps entry ids to location.
 * It is used by LedgerCache.
 *
 * <p>
 * Ledger index file is made of a header and several fixed-length index pages, which records the offsets of data stored in entry loggers
 * <pre>&lt;header&gt;&lt;index pages&gt;</pre>
 * <b>Header</b> is formated as below:
 * <pre>&lt;magic bytes&gt;&lt;len of master key&gt;&lt;master key&gt;</pre>
 * <ul>
 * <li>magic bytes: 4 bytes, 'BKLE', version: 4 bytes
 * <li>len of master key: indicates length of master key. -1 means no master key stored in header.
 * <li>master key: master key
 * <li>state: bit map to indicate the state, 32 bits.
 * </ul>
 * <b>Index page</b> is a fixed-length page, which contains serveral entries which point to the offsets of data stored in entry loggers.
 * </p>
 */
class FileInfo {
    static Logger LOG = LoggerFactory.getLogger(FileInfo.class);

    static final int NO_MASTER_KEY = -1;
    static final int STATE_FENCED_BIT = 0x1;

    private FileChannel fc;
    private File lf;
    byte[] masterKey;

    /**
     * The fingerprint of a ledger index file
     */
    static final public int signature = ByteBuffer.wrap("BKLE".getBytes()).getInt();
    static final public int headerVersion = 0;

    static final long START_OF_DATA = 1024;
    private long size;
    private int useCount;
    private boolean isClosed;
    private long sizeSinceLastwrite;

    // bit map for states of the ledger.
    private int stateBits;
    private boolean needFlushHeader = false;

    // file access mode
    protected String mode;

    public FileInfo(File lf, byte[] masterKey) throws IOException {
        this.lf = lf;

        this.masterKey = masterKey;
        mode = "rw";
    }

    public File getLf() {
        return lf;
    }

    public long getSizeSinceLastwrite() {
        return sizeSinceLastwrite;
    }

    synchronized public void readHeader() throws IOException {
        if (lf.exists()) {
            if (fc != null) {
                return;
            }

            fc = new RandomAccessFile(lf, mode).getChannel();
            size = fc.size();
            sizeSinceLastwrite = size;

            // avoid hang on reading partial index
            ByteBuffer bb = ByteBuffer.allocate((int)(Math.min(size, START_OF_DATA)));
            while(bb.hasRemaining()) {
                fc.read(bb);
            }
            bb.flip();
            if (bb.getInt() != signature) {
                throw new IOException("Missing ledger signature");
            }
            int version = bb.getInt();
            if (version != headerVersion) {
                throw new IOException("Incompatible ledger version " + version);
            }
            int length = bb.getInt();
            if (length < 0) {
                throw new IOException("Length " + length + " is invalid");
            } else if (length > bb.remaining()) {
                throw new BufferUnderflowException();
            }
            masterKey = new byte[length];
            bb.get(masterKey);
            stateBits = bb.getInt();
            needFlushHeader = false;
        } else {
            throw new IOException("Ledger index file does not exist");
        }
    }

    synchronized private void checkOpen(boolean create) throws IOException {
        if (fc != null) {
            return;
        }
        boolean exists = lf.exists();
        if (masterKey == null && !exists) {
            throw new IOException(lf + " not found");
        }

        if (!exists) { 
            if (create) {
                // delayed the creation of parents directories
                checkParents(lf);
                fc = new RandomAccessFile(lf, mode).getChannel();
                size = fc.size();
                if (size == 0) {
                    writeHeader();
                }
            }
        } else {
            try {
                readHeader();
            } catch (BufferUnderflowException buf) {
                LOG.warn("Exception when reading header of {} : {}", lf, buf);
                if (null != masterKey) {
                    LOG.warn("Attempting to write header of {} again.", lf);
                    writeHeader();
                } else {
                    throw new IOException("Error reading header " + lf);
                }
            }
        }
    }

    private void writeHeader() throws IOException {
        ByteBuffer bb = ByteBuffer.allocate((int)START_OF_DATA);
        bb.putInt(signature);
        bb.putInt(headerVersion);
        bb.putInt(masterKey.length);
        bb.put(masterKey);
        bb.putInt(stateBits);
        bb.rewind();
        fc.position(0);
        fc.write(bb);
    }

    synchronized public boolean isFenced() throws IOException {
        checkOpen(false);
        return (stateBits & STATE_FENCED_BIT) == STATE_FENCED_BIT;
    }

    /**
     * @return true if set fence succeed, otherwise false when
     * it already fenced or failed to set fenced.
     */
    synchronized public boolean setFenced() throws IOException {
        checkOpen(false);
        LOG.debug("Try to set fenced state in file info {} : state bits {}.", lf, stateBits);
        if ((stateBits & STATE_FENCED_BIT) != STATE_FENCED_BIT) {
            // not fenced yet
            stateBits |= STATE_FENCED_BIT;
            needFlushHeader = true;
            return true;
        } else {
            return false;
        }
    }

    // flush the header when header is changed
    synchronized public void flushHeader() throws IOException {
        if (needFlushHeader) {
            checkOpen(true);
            writeHeader();
            needFlushHeader = false;
        }
    }

    synchronized public long size() throws IOException {
        checkOpen(false);
        long rc = size-START_OF_DATA;
        if (rc < 0) {
            rc = 0;
        }
        return rc;
    }

    synchronized public int read(ByteBuffer bb, long position) throws IOException {
        return readAbsolute(bb, position + START_OF_DATA);
    }

    private int readAbsolute(ByteBuffer bb, long start) throws IOException {
        checkOpen(false);
        int total = 0;
        while(bb.remaining() > 0) {
            int rc = fc.read(bb, start);
            if (rc <= 0) {
                throw new IOException("Short read");
            }
            total += rc;
            // should move read position
            start += rc;
        }
        return total;
    }

    /**
     * Close a file info
     *
     * @param force
     *          if set to true, the index is forced to create before closed,
     *          if set to false, the index is not forced to create.
     */
    synchronized public void close(boolean force) throws IOException {
        isClosed = true;
        checkOpen(force);
        if (useCount == 0 && fc != null) {
            fc.close();
        }
    }

    synchronized public long write(ByteBuffer[] buffs, long position) throws IOException {
        checkOpen(true);
        long total = 0;
        try {
            fc.position(position+START_OF_DATA);
            while(buffs[buffs.length-1].remaining() > 0) {
                long rc = fc.write(buffs);
                if (rc <= 0) {
                    throw new IOException("Short write");
                }
                total += rc;
            }
        } finally {
            fc.force(true);
            long newsize = position+START_OF_DATA+total;
            if (newsize > size) {
                size = newsize;
            }
        }
        sizeSinceLastwrite = fc.size();
        return total;
    }

    /**
     * Copies current file contents upto specified size to the target file and
     * deletes the current file. If size not known then pass size as
     * Long.MAX_VALUE to copy complete file.
     */
    public synchronized void moveToNewLocation(File newFile, long size) throws IOException {
        checkOpen(false);
        if (size > fc.size()) {
            size = fc.size();
        }
        File rlocFile = new File(newFile.getParentFile(), newFile.getName() + LedgerCacheImpl.RLOC);
        if (!rlocFile.exists()) {
            checkParents(rlocFile);
            if (!rlocFile.createNewFile()) {
                throw new IOException("Creating new cache index file " + rlocFile + " failed ");
            }
        }
        // copy contents from old.idx to new.idx.rloc
        FileChannel newFc = new RandomAccessFile(rlocFile, "rw").getChannel();
        try {
            long written = 0;
            while (written < size) {
                long count = fc.transferTo(written, size, newFc);
                if (count <= 0) {
                    throw new IOException("Copying to new location " + rlocFile + " failed");
                }
                written += count;
            }
            if (written <= 0 && size > 0) {
                throw new IOException("Copying to new location " + rlocFile + " failed");
            }
        } finally {
            newFc.force(true);
            newFc.close();
        }
        // delete old.idx
        fc.close();
        if (!delete()) {
            LOG.error("Failed to delete the previous index file " + lf);
            throw new IOException("Failed to delete the previous index file " + lf);
        }

        // rename new.idx.rloc to new.idx
        if (!rlocFile.renameTo(newFile)) {
            LOG.error("Failed to rename " + rlocFile + " to " + newFile);
            throw new IOException("Failed to rename " + rlocFile + " to " + newFile);
        }
        fc = new RandomAccessFile(newFile, mode).getChannel();
        lf = newFile;
    }

    synchronized public byte[] getMasterKey() throws IOException {
        checkOpen(false);
        return masterKey;
    }

    synchronized public void use() {
        useCount++;
    }

    synchronized public void release() {
        useCount--;
        if (isClosed && useCount == 0 && fc != null) {
            try {
                fc.close();
            } catch (IOException e) {
                LOG.error("Error closing file channel", e);
            }
        }
    }

    public boolean delete() {
        return lf.delete();
    }

    static final private void checkParents(File f) throws IOException {
        File parent = f.getParentFile();
        if (parent.exists()) {
            return;
        }
        if (parent.mkdirs() == false) {
            throw new IOException("Counldn't mkdirs for " + parent);
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/FileSystemUpgrade.java,false,"/**
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

import org.apache.bookkeeper.util.BookKeeperConstants;
import org.apache.bookkeeper.util.HardLink;

import org.apache.commons.io.FileUtils;
import org.apache.commons.cli.BasicParser;
import org.apache.commons.cli.Options;
import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.HelpFormatter;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.File;
import java.io.FilenameFilter;
import java.io.IOException;

import java.util.concurrent.CountDownLatch;
import java.util.concurrent.TimeUnit;

import java.util.Map;
import java.util.HashMap;
import java.util.List;
import java.util.ArrayList;
import java.util.Scanner;
import java.util.NoSuchElementException;

import java.net.MalformedURLException;
import org.apache.bookkeeper.conf.ServerConfiguration;
import org.apache.commons.configuration.ConfigurationException;

import org.apache.zookeeper.ZooKeeper;
import org.apache.zookeeper.Watcher;
import org.apache.zookeeper.WatchedEvent;
import org.apache.zookeeper.KeeperException;

/**
 * Application for upgrading the bookkeeper filesystem
 * between versions
 */
public class FileSystemUpgrade {
    static Logger LOG = LoggerFactory.getLogger(FileSystemUpgrade.class);

    static FilenameFilter BOOKIE_FILES_FILTER = new FilenameFilter() {
            private boolean containsIndexFiles(File dir, String name) {
                if (name.endsWith(".idx")) {
                    return true;
                }

                try {
                    Long.parseLong(name, 16);
                    File d = new File(dir, name);
                    if (d.isDirectory()) {
                        String[] files = d.list();
                        for (String f : files) {
                            if (containsIndexFiles(d, f)) {
                                return true;
                            }
                        }
                    }
                } catch (NumberFormatException nfe) {
                    return false;
                }
                return false;
            }

            public boolean accept(File dir, String name) {
                if (name.endsWith(".txn") || name.endsWith(".log")
                    || name.equals("lastId") || name.equals("lastMark")) {
                    return true;
                }
                if (containsIndexFiles(dir, name)) {
                    return true;
                }
                return false;
            }
        };

    private static List<File> getAllDirectories(ServerConfiguration conf) {
        List<File> dirs = new ArrayList<File>();
        dirs.add(conf.getJournalDir());
        for (File d: conf.getLedgerDirs()) {
            dirs.add(d);
        }
        return dirs;
    }

    private static int detectPreviousVersion(File directory) throws IOException {
        String[] files = directory.list(BOOKIE_FILES_FILTER);
        File v2versionFile = new File(directory,
                BookKeeperConstants.VERSION_FILENAME);
        if (files.length == 0 && !v2versionFile.exists()) { // no old data, so we're ok
            return Cookie.CURRENT_COOKIE_LAYOUT_VERSION;
        }

        if (!v2versionFile.exists()) {
            return 1;
        }
        Scanner s = new Scanner(v2versionFile);
        try {
            return s.nextInt();
        } catch (NoSuchElementException nse) {
            LOG.error("Couldn't parse version file " + v2versionFile , nse);
            throw new IOException("Couldn't parse version file", nse);
        } catch (IllegalStateException ise) {
            LOG.error("Error reading file " + v2versionFile, ise);
            throw new IOException("Error reading version file", ise);
        } finally {
            s.close();
        }
    }

    private static ZooKeeper newZookeeper(final ServerConfiguration conf)
            throws BookieException.UpgradeException {
        try {
            final CountDownLatch latch = new CountDownLatch(1);
            ZooKeeper zk = new ZooKeeper(conf.getZkServers(), conf.getZkTimeout(),
                    new Watcher() {
                        @Override
                        public void process(WatchedEvent event) {
                            // handle session disconnects and expires
                            if (event.getState().equals(Watcher.Event.KeeperState.SyncConnected)) {
                                latch.countDown();
                            }
                        }
                    });
            if (!latch.await(conf.getZkTimeout()*2, TimeUnit.MILLISECONDS)) {
                zk.close();
                throw new BookieException.UpgradeException("Couldn't connect to zookeeper");
            }
            return zk;
        } catch (InterruptedException ie) {
            throw new BookieException.UpgradeException(ie);
        } catch (IOException ioe) {
            throw new BookieException.UpgradeException(ioe);
        }
    }

    private static void linkIndexDirectories(File srcPath, File targetPath) throws IOException {
        String[] files = srcPath.list();

        for (String f : files) {
            if (f.endsWith(".idx")) { // this is an index dir, create the links
                if (!targetPath.mkdirs()) {
                    throw new IOException("Could not create target path ["+targetPath+"]");
                }
                HardLink.createHardLinkMult(srcPath, files, targetPath);
                return;
            }
            File newSrcPath = new File(srcPath, f);
            if (newSrcPath.isDirectory()) {
                try {
                    Long.parseLong(f, 16);
                    linkIndexDirectories(newSrcPath, new File(targetPath, f));
                } catch (NumberFormatException nfe) {
                    // filename does not parse to a hex Long, so
                    // it will not contain idx files. Ignoring
                }
            }
        }
    }

    public static void upgrade(ServerConfiguration conf)
            throws BookieException.UpgradeException, InterruptedException {
        LOG.info("Upgrading...");

        ZooKeeper zk = newZookeeper(conf);
        try {
            Map<File,File> deferredMoves = new HashMap<File, File>();
            Cookie c = Cookie.generateCookie(conf);
            for (File d : getAllDirectories(conf)) {
                LOG.info("Upgrading {}", d);
                int version = detectPreviousVersion(d);
                if (version == Cookie.CURRENT_COOKIE_LAYOUT_VERSION) {
                    LOG.info("Directory is current, no need to upgrade");
                    continue;
                }
                try {
                    File curDir = new File(d, BookKeeperConstants.CURRENT_DIR);
                    File tmpDir = new File(d, "upgradeTmp." + System.nanoTime());
                    deferredMoves.put(curDir, tmpDir);
                    if (!tmpDir.mkdirs()) {
                        throw new BookieException.UpgradeException("Could not create temporary directory " + tmpDir);
                    }
                    c.writeToDirectory(tmpDir);

                    String[] files = d.list(new FilenameFilter() {
                            public boolean accept(File dir, String name) {
                                return BOOKIE_FILES_FILTER.accept(dir, name)
                                    && !(new File(dir, name).isDirectory());
                            }
                        });
                    HardLink.createHardLinkMult(d, files, tmpDir);

                    linkIndexDirectories(d, tmpDir);
                } catch (IOException ioe) {
                    LOG.error("Error upgrading {}", d);
                    throw new BookieException.UpgradeException(ioe);
                }
            }

            for (Map.Entry<File,File> e : deferredMoves.entrySet()) {
                try {
                    FileUtils.moveDirectory(e.getValue(), e.getKey());
                } catch (IOException ioe) {
                    String err = String.format("Error moving upgraded directories into place %s -> %s ",
                                               e.getValue(), e.getKey());
                    LOG.error(err, ioe);
                    throw new BookieException.UpgradeException(ioe);
                }
            }

            if (deferredMoves.isEmpty()) {
                return;
            }

            try {
                c.writeToZooKeeper(zk, conf);
            } catch (KeeperException ke) {
                LOG.error("Error writing cookie to zookeeper");
                throw new BookieException.UpgradeException(ke);
            }
        } catch (IOException ioe) {
            throw new BookieException.UpgradeException(ioe);
        } finally {
            zk.close();
        }
        LOG.info("Done");
    }

    public static void finalizeUpgrade(ServerConfiguration conf)
            throws BookieException.UpgradeException, InterruptedException {
        LOG.info("Finalizing upgrade...");
        // verify that upgrade is correct
        for (File d : getAllDirectories(conf)) {
            LOG.info("Finalizing {}", d);
            try {
                int version = detectPreviousVersion(d);
                if (version < 3) {
                    if (version == 2) {
                        File v2versionFile = new File(d,
                                BookKeeperConstants.VERSION_FILENAME);
                        if (!v2versionFile.delete()) {
                            LOG.warn("Could not delete old version file {}", v2versionFile);
                        }
                    }
                    File[] files = d.listFiles(BOOKIE_FILES_FILTER);
                    for (File f : files) {
                        if (f.isDirectory()) {
                            FileUtils.deleteDirectory(f);
                        } else{
                            if (!f.delete()) {
                                LOG.warn("Could not delete {}", f);
                            }
                        }
                    }
                }
            } catch (IOException ioe) {
                LOG.error("Error finalizing {}", d);
                throw new BookieException.UpgradeException(ioe);
            }
        }
        // noop at the moment
        LOG.info("Done");
    }

    public static void rollback(ServerConfiguration conf)
            throws BookieException.UpgradeException, InterruptedException {
        LOG.info("Rolling back upgrade...");
        ZooKeeper zk = newZookeeper(conf);
        try {
            for (File d : getAllDirectories(conf)) {
                LOG.info("Rolling back {}", d);
                try {
                    // ensure there is a previous version before rollback
                    int version = detectPreviousVersion(d);

                    if (version <= Cookie.CURRENT_COOKIE_LAYOUT_VERSION) {
                        File curDir = new File(d,
                                BookKeeperConstants.CURRENT_DIR);
                        FileUtils.deleteDirectory(curDir);
                    } else {
                        throw new BookieException.UpgradeException(
                                "Cannot rollback as previous data does not exist");
                    }
                } catch (IOException ioe) {
                    LOG.error("Error rolling back {}", d);
                    throw new BookieException.UpgradeException(ioe);
                }
            }
            try {
                Cookie c = Cookie.readFromZooKeeper(zk, conf);
                c.deleteFromZooKeeper(zk, conf);
            } catch (KeeperException ke) {
                LOG.error("Error deleting cookie from ZooKeeper");
                throw new BookieException.UpgradeException(ke);
            } catch (IOException ioe) {
                LOG.error("I/O Error deleting cookie from ZooKeeper");
                throw new BookieException.UpgradeException(ioe);
            }
        } finally {
            zk.close();
        }
        LOG.info("Done");
    }

    private static void printHelp(Options opts) {
        HelpFormatter hf = new HelpFormatter();
        hf.printHelp("FileSystemUpgrade [options]", opts);
    }

    public static void main(String[] args) throws Exception {
        org.apache.log4j.Logger root = org.apache.log4j.Logger.getRootLogger();
        root.addAppender(new org.apache.log4j.ConsoleAppender(
                                 new org.apache.log4j.PatternLayout("%-5p [%t]: %m%n")));
        root.setLevel(org.apache.log4j.Level.ERROR);
        org.apache.log4j.Logger.getLogger(FileSystemUpgrade.class).setLevel(
                org.apache.log4j.Level.INFO);

        final Options opts = new Options();
        opts.addOption("c", "conf", true, "Configuration for Bookie");
        opts.addOption("u", "upgrade", false, "Upgrade bookie directories");
        opts.addOption("f", "finalize", false, "Finalize upgrade");
        opts.addOption("r", "rollback", false, "Rollback upgrade");
        opts.addOption("h", "help", false, "Print help message");

        BasicParser parser = new BasicParser();
        CommandLine cmdLine = parser.parse(opts, args);
        if (cmdLine.hasOption("h")) {
            printHelp(opts);
            return;
        }

        if (!cmdLine.hasOption("c")) {
            String err = "Cannot upgrade without configuration";
            LOG.error(err);
            printHelp(opts);
            throw new IllegalArgumentException(err);
        }

        String confFile = cmdLine.getOptionValue("c");
        ServerConfiguration conf = new ServerConfiguration();
        try {
            conf.loadConf(new File(confFile).toURI().toURL());
        } catch (MalformedURLException mue) {
            LOG.error("Could not open configuration file " + confFile, mue);
            throw new IllegalArgumentException();
        } catch (ConfigurationException ce) {
            LOG.error("Invalid configuration file " + confFile, ce);
            throw new IllegalArgumentException();
        }

        if (cmdLine.hasOption("u")) {
            upgrade(conf);
        } else if (cmdLine.hasOption("r")) {
            rollback(conf);
        } else if (cmdLine.hasOption("f")) {
            finalizeUpgrade(conf);
        } else {
            String err = "Must specify -upgrade, -finalize or -rollback";
            LOG.error(err);
            printHelp(opts);
            throw new IllegalArgumentException(err);
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/GarbageCollector.java,false,"/**
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

/**
 * This is the garbage collector interface, garbage collector implementers
 * need to extends this class to remove the deleted ledgers.
 */
public interface GarbageCollector {
    /**
     * Do the garbage collector work
     *
     * @param garbageCleaner
     *          cleaner used to clean selected garbages
     */
    public abstract void gc(GarbageCleaner garbageCleaner);

    /**
     * A interface used to define customised garbage cleaner
     */
    public interface GarbageCleaner {

        /**
         * Clean a specific ledger
         *
         * @param ledgerId
         *          Ledger ID to be cleaned
         */
        public void clean(final long ledgerId) ;
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/GarbageCollectorThread.java,true,"/**
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

import java.io.IOException;
import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Comparator;
import java.util.List;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicInteger;

import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;
import org.apache.bookkeeper.bookie.EntryLogger.EntryLogScanner;
import org.apache.bookkeeper.bookie.GarbageCollector.GarbageCleaner;
import org.apache.bookkeeper.conf.ServerConfiguration;
import org.apache.bookkeeper.meta.LedgerManager;
import org.apache.bookkeeper.util.MathUtils;
import org.apache.bookkeeper.util.SnapshotMap;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * This is the garbage collector thread that runs in the background to
 * remove any entry log files that no longer contains any active ledger.
 */
public class GarbageCollectorThread extends Thread {
    private static final Logger LOG = LoggerFactory.getLogger(GarbageCollectorThread.class);
    private static final int COMPACTION_MAX_OUTSTANDING_REQUESTS = 1000;
    private static final int SECOND = 1000;

    // Maps entry log files to the set of ledgers that comprise the file and the size usage per ledger
    private Map<Long, EntryLogMetadata> entryLogMetaMap = new ConcurrentHashMap<Long, EntryLogMetadata>();

    // This is how often we want to run the Garbage Collector Thread (in milliseconds).
    final long gcWaitTime;

    // Compaction parameters
    boolean enableMinorCompaction = false;
    final double minorCompactionThreshold;
    final long minorCompactionInterval;

    boolean enableMajorCompaction = false;
    final double majorCompactionThreshold;
    final long majorCompactionInterval;

    long lastMinorCompactionTime;
    long lastMajorCompactionTime;

    // Entry Logger Handle
    final EntryLogger entryLogger;
    final SafeEntryAdder safeEntryAdder;

    // Ledger Cache Handle
    final LedgerCache ledgerCache;
    final SnapshotMap<Long, Boolean> activeLedgers;

    // flag to ensure gc thread will not be interrupted during compaction
    // to reduce the risk getting entry log corrupted
    final AtomicBoolean compacting = new AtomicBoolean(false);

    volatile boolean running = true;

    // track the last scanned successfully log id
    long scannedLogId = 0;

    final GarbageCollector garbageCollector;
    final GarbageCleaner garbageCleaner;


    /**
     * Interface for adding entries. When the write callback is triggered, the
     * entry must be guaranteed to be presisted.
     */
    interface SafeEntryAdder {
        public void safeAddEntry(long ledgerId, ByteBuffer buffer, GenericCallback<Void> cb);
    }

    /**
     * A scanner wrapper to check whether a ledger is alive in an entry log file
     */
    class CompactionScanner implements EntryLogScanner {
        EntryLogMetadata meta;
        Object completionLock = new Object();
        AtomicInteger outstandingRequests = new AtomicInteger(0);
        AtomicBoolean allSuccessful = new AtomicBoolean(true);

        public CompactionScanner(EntryLogMetadata meta) {
            this.meta = meta;
        }

        @Override
        public boolean accept(long ledgerId) {
            return meta.containsLedger(ledgerId);
        }

        @Override
        public void process(final long ledgerId, long offset, ByteBuffer entry)
            throws IOException {
            if (!allSuccessful.get()) {
                return;
            }

            outstandingRequests.incrementAndGet();
            synchronized (completionLock) {
                while (outstandingRequests.get() >= COMPACTION_MAX_OUTSTANDING_REQUESTS) {
                    try {
                        completionLock.wait();
                    } catch (InterruptedException ie) {
                        LOG.error("Interrupted while waiting to re-add entry", ie);
                        Thread.currentThread().interrupt();
                        throw new IOException("Interrupted while waiting to re-add entry", ie);
                    }
                }
            }
            safeEntryAdder.safeAddEntry(ledgerId, entry, new GenericCallback<Void>() {
                    @Override
                    public void operationComplete(int rc, Void result) {
                        if (rc != BookieException.Code.OK) {
                            LOG.error("Error {} re-adding entry for ledger {})",
                                      rc, ledgerId);
                            allSuccessful.set(false);
                        }
                        synchronized(completionLock) {
                            outstandingRequests.decrementAndGet();
                            completionLock.notifyAll();
                        }
                    }
                });
        }

        void awaitComplete() throws InterruptedException, IOException {
            synchronized(completionLock) {
                while (outstandingRequests.get() > 0) {
                    completionLock.wait();
                }
                if (allSuccessful.get() == false) {
                    throw new IOException("Couldn't re-add all entries");
                }
            }
        }
    }


    /**
     * Create a garbage collector thread.
     *
     * @param conf
     *          Server Configuration Object.
     * @throws IOException
     */
    public GarbageCollectorThread(ServerConfiguration conf,
                                  final LedgerCache ledgerCache,
                                  EntryLogger entryLogger,
                                  SnapshotMap<Long, Boolean> activeLedgers,
                                  SafeEntryAdder safeEntryAdder,
                                  LedgerManager ledgerManager)
        throws IOException {
        super("GarbageCollectorThread");

        this.ledgerCache = ledgerCache;
        this.entryLogger = entryLogger;
        this.activeLedgers = activeLedgers;
        this.safeEntryAdder = safeEntryAdder;

        this.gcWaitTime = conf.getGcWaitTime();

        this.garbageCleaner = new GarbageCollector.GarbageCleaner() {
            @Override
            public void clean(long ledgerId) {
                try {
                    if (LOG.isDebugEnabled()) {
                        LOG.debug("delete ledger : " + ledgerId);
                    }
                    ledgerCache.deleteLedger(ledgerId);
                } catch (IOException e) {
                    LOG.error("Exception when deleting the ledger index file on the Bookie: ", e);
                }
            }
        };

        this.garbageCollector = new ScanAndCompareGarbageCollector(ledgerManager, activeLedgers);

        // compaction parameters
        minorCompactionThreshold = conf.getMinorCompactionThreshold();
        minorCompactionInterval = conf.getMinorCompactionInterval() * SECOND;
        majorCompactionThreshold = conf.getMajorCompactionThreshold();
        majorCompactionInterval = conf.getMajorCompactionInterval() * SECOND;

        if (minorCompactionInterval > 0 && minorCompactionThreshold > 0) {
            if (minorCompactionThreshold > 1.0f) {
                throw new IOException("Invalid minor compaction threshold "
                                    + minorCompactionThreshold);
            }
            if (minorCompactionInterval <= gcWaitTime) {
                throw new IOException("Too short minor compaction interval : "
                                    + minorCompactionInterval);
            }
            enableMinorCompaction = true;
        }

        if (majorCompactionInterval > 0 && majorCompactionThreshold > 0) {
            if (majorCompactionThreshold > 1.0f) {
                throw new IOException("Invalid major compaction threshold "
                                    + majorCompactionThreshold);
            }
            if (majorCompactionInterval <= gcWaitTime) {
                throw new IOException("Too short major compaction interval : "
                                    + majorCompactionInterval);
            }
            enableMajorCompaction = true;
        }

        if (enableMinorCompaction && enableMajorCompaction) {
            if (minorCompactionInterval >= majorCompactionInterval ||
                minorCompactionThreshold >= majorCompactionThreshold) {
                throw new IOException("Invalid minor/major compaction settings : minor ("
                                    + minorCompactionThreshold + ", " + minorCompactionInterval
                                    + "), major (" + majorCompactionThreshold + ", "
                                    + majorCompactionInterval + ")");
            }
        }

        LOG.info("Minor Compaction : enabled=" + enableMinorCompaction + ", threshold="
               + minorCompactionThreshold + ", interval=" + minorCompactionInterval);
        LOG.info("Major Compaction : enabled=" + enableMajorCompaction + ", threshold="
               + majorCompactionThreshold + ", interval=" + majorCompactionInterval);

        lastMinorCompactionTime = lastMajorCompactionTime = MathUtils.now();
    }

    @Override
    public void run() {
        while (running) {
            synchronized (this) {
                try {
                    wait(gcWaitTime);
                } catch (InterruptedException e) {
                    Thread.currentThread().interrupt();
                    continue;
                }
            }

            // Extract all of the ledger ID's that comprise all of the entry logs
            // (except for the current new one which is still being written to).
            entryLogMetaMap = extractMetaFromEntryLogs(entryLogMetaMap);

            // gc inactive/deleted ledgers
            doGcLedgers();

            // gc entry logs
            doGcEntryLogs();

            long curTime = MathUtils.now();
            if (enableMajorCompaction &&
                curTime - lastMajorCompactionTime > majorCompactionInterval) {
                // enter major compaction
                LOG.info("Enter major compaction");
                doCompactEntryLogs(majorCompactionThreshold);
                lastMajorCompactionTime = MathUtils.now();
                // also move minor compaction time
                lastMinorCompactionTime = lastMajorCompactionTime;
                continue;
            }

            if (enableMinorCompaction &&
                curTime - lastMinorCompactionTime > minorCompactionInterval) {
                // enter minor compaction
                LOG.info("Enter minor compaction");
                doCompactEntryLogs(minorCompactionThreshold);
                lastMinorCompactionTime = MathUtils.now();
            }
        }
    }

    /**
     * Do garbage collection ledger index files
     */
    private void doGcLedgers() {
        garbageCollector.gc(garbageCleaner);
    }

    /**
     * Garbage collect those entry loggers which are not associated with any active ledgers
     */
    private void doGcEntryLogs() {
        // Loop through all of the entry logs and remove the non-active ledgers.
        for (Long entryLogId : entryLogMetaMap.keySet()) {
            EntryLogMetadata meta = entryLogMetaMap.get(entryLogId);
            for (Long entryLogLedger : meta.ledgersMap.keySet()) {
                // Remove the entry log ledger from the set if it isn't active.
                if (!activeLedgers.containsKey(entryLogLedger)) {
                    meta.removeLedger(entryLogLedger);
                }
            }
            if (meta.isEmpty()) {
                // This means the entry log is not associated with any active ledgers anymore.
                // We can remove this entry log file now.
                LOG.info("Deleting entryLogId " + entryLogId + " as it has no active ledgers!");
                removeEntryLog(entryLogId);
            }
        }
    }

    /**
     * Compact entry logs if necessary.
     *
     * <p>
     * Compaction will be executed from low unused space to high unused space.
     * Those entry log files whose remaining size percentage is higher than threshold
     * would not be compacted.
     * </p>
     */
    private void doCompactEntryLogs(double threshold) {
        LOG.info("Do compaction to compact those files lower than " + threshold);
        // sort the ledger meta by occupied unused space
        Comparator<EntryLogMetadata> sizeComparator = new Comparator<EntryLogMetadata>() {
            @Override
            public int compare(EntryLogMetadata m1, EntryLogMetadata m2) {
                long unusedSize1 = m1.totalSize - m1.remainingSize;
                long unusedSize2 = m2.totalSize - m2.remainingSize;
                if (unusedSize1 > unusedSize2) {
                    return -1;
                } else if (unusedSize1 < unusedSize2) {
                    return 1;
                } else {
                    return 0;
                }
            }
        };
        List<EntryLogMetadata> logsToCompact = new ArrayList<EntryLogMetadata>();
        logsToCompact.addAll(entryLogMetaMap.values());
        Collections.sort(logsToCompact, sizeComparator);
        for (EntryLogMetadata meta : logsToCompact) {
            if (meta.getUsage() >= threshold) {
                break;
            }
            LOG.debug("Compacting entry log {} below threshold {}.", meta.entryLogId, threshold);
            compactEntryLog(meta.entryLogId);
            if (!running) { // if gc thread is not running, stop compaction
                return;
            }
        }
    }

    /**
     * Shutdown the garbage collector thread.
     *
     * @throws InterruptedException if there is an exception stopping gc thread.
     */
    public void shutdown() throws InterruptedException {
        this.running = false;
        if (compacting.compareAndSet(false, true)) {
            // if setting compacting flag succeed, means gcThread is not compacting now
            // it is safe to interrupt itself now
            this.interrupt();
        }
        this.join();
    }

    /**
     * Remove entry log.
     *
     * @param entryLogId
     *          Entry Log File Id
     */
    private void removeEntryLog(long entryLogId) {
        // remove entry log file successfully
        if (entryLogger.removeEntryLog(entryLogId)) {
            entryLogMetaMap.remove(entryLogId);
        }
    }

    /**
     * Compact an entry log.
     *
     * @param entryLogId
     *          Entry Log File Id
     */
    protected void compactEntryLog(long entryLogId) {
        EntryLogMetadata entryLogMeta = entryLogMetaMap.get(entryLogId);
        if (null == entryLogMeta) {
            LOG.warn("Can't get entry log meta when compacting entry log " + entryLogId + ".");
            return;
        }

        // Similar with Sync Thread
        // try to mark compacting flag to make sure it would not be interrupted
        // by shutdown during compaction. otherwise it will receive
        // ClosedByInterruptException which may cause index file & entry logger
        // closed and corrupted.
        if (!compacting.compareAndSet(false, true)) {
            // set compacting flag failed, means compacting is true now
            // indicates another thread wants to interrupt gc thread to exit
            return;
        }

        LOG.info("Compacting entry log : " + entryLogId);

        try {
            CompactionScanner scanner = new CompactionScanner(entryLogMeta);
            entryLogger.scanEntryLog(entryLogId, scanner);
            scanner.awaitComplete();
            // after moving entries to new entry log, remove this old one
            removeEntryLog(entryLogId);
        } catch (IOException e) {
            LOG.info("Premature exception when compacting " + entryLogId, e);
        } catch (InterruptedException ie) {
            Thread.currentThread().interrupt();
            LOG.warn("Interrupted while compacting", ie);
        } finally {
            // clear compacting flag
            compacting.set(false);
        }
    }

    /**
     * Records the total size, remaining size and the set of ledgers that comprise a entry log.
     */
    static class EntryLogMetadata {
        long entryLogId;
        long totalSize;
        long remainingSize;
        ConcurrentHashMap<Long, Long> ledgersMap;

        public EntryLogMetadata(long logId) {
            this.entryLogId = logId;

            totalSize = remainingSize = 0;
            ledgersMap = new ConcurrentHashMap<Long, Long>();
        }

        public void addLedgerSize(long ledgerId, long size) {
            totalSize += size;
            remainingSize += size;
            Long ledgerSize = ledgersMap.get(ledgerId);
            if (null == ledgerSize) {
                ledgerSize = 0L;
            }
            ledgerSize += size;
            ledgersMap.put(ledgerId, ledgerSize);
        }

        public void removeLedger(long ledgerId) {
            Long size = ledgersMap.remove(ledgerId);
            if (null == size) {
                return;
            }
            remainingSize -= size;
        }

        public boolean containsLedger(long ledgerId) {
            return ledgersMap.containsKey(ledgerId);
        }

        public double getUsage() {
            if (totalSize == 0L) {
                return 0.0f;
            }
            return (double)remainingSize / totalSize;
        }

        public boolean isEmpty() {
            return ledgersMap.isEmpty();
        }

        @Override
        public String toString() {
            StringBuilder sb = new StringBuilder();
            sb.append("{ totalSize = ").append(totalSize).append(", remainingSize = ")
              .append(remainingSize).append(", ledgersMap = ").append(ledgersMap).append(" }");
            return sb.toString();
        }
    }

    /**
     * A scanner used to extract entry log meta from entry log files.
     */
    static class ExtractionScanner implements EntryLogScanner {
        EntryLogMetadata meta;

        public ExtractionScanner(EntryLogMetadata meta) {
            this.meta = meta;
        }

        @Override
        public boolean accept(long ledgerId) {
            return true;
        }
        @Override
        public void process(long ledgerId, long offset, ByteBuffer entry) {
            // add new entry size of a ledger to entry log meta
            meta.addLedgerSize(ledgerId, entry.limit() + 4);
        }
    }

    /**
     * Method to read in all of the entry logs (those that we haven't done so yet),
     * and find the set of ledger ID's that make up each entry log file.
     *
     * @param entryLogMetaMap
     *          Existing EntryLogs to Meta
     * @throws IOException
     */
    protected Map<Long, EntryLogMetadata> extractMetaFromEntryLogs(Map<Long, EntryLogMetadata> entryLogMetaMap) {
        // Extract it for every entry log except for the current one.
        // Entry Log ID's are just a long value that starts at 0 and increments
        // by 1 when the log fills up and we roll to a new one.
        long curLogId = entryLogger.getCurrentLogId();
        boolean hasExceptionWhenScan = false;
        for (long entryLogId = scannedLogId; entryLogId < curLogId; entryLogId++) {
            // Comb the current entry log file if it has not already been extracted.
            if (entryLogMetaMap.containsKey(entryLogId)) {
                continue;
            }

            // check whether log file exists or not
            // if it doesn't exist, this log file might have been garbage collected.
            if (!entryLogger.logExists(entryLogId)) {
                continue;
            }

            LOG.info("Extracting entry log meta from entryLogId: " + entryLogId);

            try {
                // Read through the entry log file and extract the entry log meta
                EntryLogMetadata entryLogMeta = extractMetaFromEntryLog(entryLogger, entryLogId);
                entryLogMetaMap.put(entryLogId, entryLogMeta);
            } catch (IOException e) {
                hasExceptionWhenScan = true;
                LOG.warn("Premature exception when processing " + entryLogId +
                         " recovery will take care of the problem", e);
            }

            // if scan failed on some entry log, we don't move 'scannedLogId' to next id
            // if scan succeed, we don't need to scan it again during next gc run,
            // we move 'scannedLogId' to next id
            if (!hasExceptionWhenScan) {
                ++scannedLogId;
            }
        }
        return entryLogMetaMap;
    }

    static EntryLogMetadata extractMetaFromEntryLog(EntryLogger entryLogger, long entryLogId)
            throws IOException {
        EntryLogMetadata entryLogMeta = new EntryLogMetadata(entryLogId);
        ExtractionScanner scanner = new ExtractionScanner(entryLogMeta);
        // Read through the entry log file and extract the entry log meta
        entryLogger.scanEntryLog(entryLogId, scanner);
        LOG.info("Retrieved entry log meta data entryLogId: "
                 + entryLogId + ", meta: " + entryLogMeta);
        return entryLogMeta;
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/HandleFactory.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

import java.io.IOException;

interface HandleFactory {
    LedgerDescriptor getHandle(long ledgerId, byte[] masterKey)
            throws IOException, BookieException;

    LedgerDescriptor getReadOnlyHandle(long ledgerId)
            throws IOException, Bookie.NoLedgerException;
}"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/HandleFactoryImpl.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

import java.io.IOException;
import java.util.HashMap;

class HandleFactoryImpl implements HandleFactory {
    HashMap<Long, LedgerDescriptor> ledgers = new HashMap<Long, LedgerDescriptor>();
    HashMap<Long, LedgerDescriptor> readOnlyLedgers
        = new HashMap<Long, LedgerDescriptor>();

    final LedgerStorage ledgerStorage;

    HandleFactoryImpl(LedgerStorage ledgerStorage) {
        this.ledgerStorage = ledgerStorage;
    }

    @Override
    public LedgerDescriptor getHandle(long ledgerId, byte[] masterKey)
            throws IOException, BookieException {
        LedgerDescriptor handle = null;
        synchronized (ledgers) {
            handle = ledgers.get(ledgerId);
            if (handle == null) {
                handle = LedgerDescriptor.create(masterKey, ledgerId, ledgerStorage);
                ledgers.put(ledgerId, handle);
            }
            handle.checkAccess(masterKey);
        }
        return handle;
    }

    @Override
    public LedgerDescriptor getReadOnlyHandle(long ledgerId)
            throws IOException, Bookie.NoLedgerException {
        LedgerDescriptor handle = null;
        synchronized (ledgers) {
            handle = readOnlyLedgers.get(ledgerId);
            if (handle == null) {
                handle = LedgerDescriptor.createReadOnly(ledgerId, ledgerStorage);
                readOnlyLedgers.put(ledgerId, handle);
            }
        }
        return handle;
    }
}"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/InterleavedLedgerStorage.java,true,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

import java.nio.ByteBuffer;
import java.io.IOException;

import org.apache.bookkeeper.jmx.BKMBeanInfo;
import org.apache.bookkeeper.conf.ServerConfiguration;
import org.apache.bookkeeper.meta.LedgerManager;
import org.apache.bookkeeper.proto.BookieProtocol;
import org.apache.bookkeeper.util.SnapshotMap;
import org.apache.zookeeper.ZooKeeper;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Interleave ledger storage
 * This ledger storage implementation stores all entries in a single
 * file and maintains an index file for each ledger.
 */
class InterleavedLedgerStorage implements LedgerStorage {
    final static Logger LOG = LoggerFactory.getLogger(InterleavedLedgerStorage.class);

    EntryLogger entryLogger;
    LedgerCache ledgerCache;

    // A sorted map to stored all active ledger ids
    protected final SnapshotMap<Long, Boolean> activeLedgers;

    // This is the thread that garbage collects the entry logs that do not
    // contain any active ledgers in them; and compacts the entry logs that
    // has lower remaining percentage to reclaim disk space.
    final GarbageCollectorThread gcThread;

    // this indicates that a write has happened since the last flush
    private volatile boolean somethingWritten = false;

    InterleavedLedgerStorage(ServerConfiguration conf,
                             LedgerManager ledgerManager, LedgerDirsManager ledgerDirsManager,
                             GarbageCollectorThread.SafeEntryAdder safeEntryAdder)
			throws IOException {
        activeLedgers = new SnapshotMap<Long, Boolean>();
        entryLogger = new EntryLogger(conf, ledgerDirsManager);
        ledgerCache = new LedgerCacheImpl(conf, activeLedgers, ledgerDirsManager);
        gcThread = new GarbageCollectorThread(conf, ledgerCache, entryLogger,
                activeLedgers, safeEntryAdder, ledgerManager);
    }

    @Override
    public void start() {
        gcThread.start();
    }

    @Override
    public void shutdown() throws InterruptedException {
        // shut down gc thread, which depends on zookeeper client
        // also compaction will write entries again to entry log file
        gcThread.shutdown();
        entryLogger.shutdown();
        try {
            ledgerCache.close();
        } catch (IOException e) {
            LOG.error("Error while closing the ledger cache", e);
        }
    }

    @Override
    public boolean setFenced(long ledgerId) throws IOException {
        return ledgerCache.setFenced(ledgerId);
    }

    @Override
    public boolean isFenced(long ledgerId) throws IOException {
        return ledgerCache.isFenced(ledgerId);
    }

    @Override
    public void setMasterKey(long ledgerId, byte[] masterKey) throws IOException {
        ledgerCache.setMasterKey(ledgerId, masterKey);
    }

    @Override
    public byte[] readMasterKey(long ledgerId) throws IOException, BookieException {
        return ledgerCache.readMasterKey(ledgerId);
    }

    @Override
    public boolean ledgerExists(long ledgerId) throws IOException {
        return ledgerCache.ledgerExists(ledgerId);
    }

    @Override
    synchronized public long addEntry(ByteBuffer entry) throws IOException {
        long ledgerId = entry.getLong();
        long entryId = entry.getLong();
        entry.rewind();
        
        /*
         * Log the entry
         */
        long pos = entryLogger.addEntry(ledgerId, entry);
        
        
        /*
         * Set offset of entry id to be the current ledger position
         */
        ledgerCache.putEntryOffset(ledgerId, entryId, pos);

        somethingWritten = true;

        return entryId;
    }

    @Override
    public ByteBuffer getEntry(long ledgerId, long entryId) throws IOException {
        long offset;
        /*
         * If entryId is BookieProtocol.LAST_ADD_CONFIRMED, then return the last written.
         */
        if (entryId == BookieProtocol.LAST_ADD_CONFIRMED) {
            entryId = ledgerCache.getLastEntry(ledgerId);
        }

        offset = ledgerCache.getEntryOffset(ledgerId, entryId);
        if (offset == 0) {
            throw new Bookie.NoEntryException(ledgerId, entryId);
        }
        return ByteBuffer.wrap(entryLogger.readEntry(ledgerId, entryId, offset));
    }

    @Override
    public boolean isFlushRequired() {
        return somethingWritten;
    };

    @Override
    public synchronized void flush() throws IOException {

        if (!somethingWritten) {
            return;
        }
        somethingWritten = false;
        boolean flushFailed = false;

        try {
            ledgerCache.flushLedger(true);
        } catch (IOException ioe) {
            LOG.error("Exception flushing Ledger cache", ioe);
            flushFailed = true;
        }

        try {
            entryLogger.flush();
        } catch (IOException ioe) {
            LOG.error("Exception flushing Ledger", ioe);
            flushFailed = true;
        }
        if (flushFailed) {
            throw new IOException("Flushing to storage failed, check logs");
        }
    }

    @Override
    public BKMBeanInfo getJMXBean() {
        return ledgerCache.getJMXBean();
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/Journal.java,true,"/**
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.Collections;
import java.util.LinkedList;
import java.util.List;
import java.util.concurrent.LinkedBlockingQueue;

import org.apache.bookkeeper.bookie.LedgerDirsManager.NoWritableLedgerDirException;
import org.apache.bookkeeper.conf.ServerConfiguration;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.WriteCallback;
import org.apache.bookkeeper.util.IOUtils;
import org.apache.bookkeeper.util.MathUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Provide journal related management.
 */
class Journal extends Thread {

    static Logger LOG = LoggerFactory.getLogger(Journal.class);

    /**
     * Filter to pickup journals
     */
    private static interface JournalIdFilter {
        public boolean accept(long journalId);
    }

    /**
     * List all journal ids by a specified journal id filer
     *
     * @param journalDir journal dir
     * @param filter journal id filter
     * @return list of filtered ids
     */
    private static List<Long> listJournalIds(File journalDir, JournalIdFilter filter) {
        File logFiles[] = journalDir.listFiles();
        List<Long> logs = new ArrayList<Long>();
        for(File f: logFiles) {
            String name = f.getName();
            if (!name.endsWith(".txn")) {
                continue;
            }
            String idString = name.split("\\.")[0];
            long id = Long.parseLong(idString, 16);
            if (filter != null) {
                if (filter.accept(id)) {
                    logs.add(id);
                }
            } else {
                logs.add(id);
            }
        }
        Collections.sort(logs);
        return logs;
    }

    /**
     * Last Log Mark
     */
    class LastLogMark {
        private long txnLogId;
        private long txnLogPosition;
        private LastLogMark lastMark;
        LastLogMark(long logId, long logPosition) {
            this.txnLogId = logId;
            this.txnLogPosition = logPosition;
        }
        synchronized void setLastLogMark(long logId, long logPosition) {
            txnLogId = logId;
            txnLogPosition = logPosition;
        }
        synchronized void markLog() {
            lastMark = new LastLogMark(txnLogId, txnLogPosition);
        }

        synchronized LastLogMark getLastMark() {
            return lastMark;
        }
        synchronized long getTxnLogId() {
            return txnLogId;
        }
        synchronized long getTxnLogPosition() {
            return txnLogPosition;
        }

        synchronized void rollLog() throws NoWritableLedgerDirException {
            byte buff[] = new byte[16];
            ByteBuffer bb = ByteBuffer.wrap(buff);
            // we should record <logId, logPosition> marked in markLog
            // which is safe since records before lastMark have been
            // persisted to disk (both index & entry logger)
            bb.putLong(lastMark.getTxnLogId());
            bb.putLong(lastMark.getTxnLogPosition());
            LOG.debug("RollLog to persist last marked log : {}", lastMark);
            List<File> writableLedgerDirs = ledgerDirsManager
                    .getWritableLedgerDirs();
            for (File dir : writableLedgerDirs) {
                File file = new File(dir, "lastMark");
                FileOutputStream fos = null;
                try {
                    fos = new FileOutputStream(file);
                    fos.write(buff);
                    fos.getChannel().force(true);
                    fos.close();
                    fos = null;
                } catch (IOException e) {
                    LOG.error("Problems writing to " + file, e);
                } finally {
                    // if stream already closed in try block successfully,
                    // stream might have nullified, in such case below
                    // call will simply returns
                    IOUtils.close(LOG, fos);
                }
            }
        }

        /**
         * Read last mark from lastMark file.
         * The last mark should first be max journal log id,
         * and then max log position in max journal log.
         */
        synchronized void readLog() {
            byte buff[] = new byte[16];
            ByteBuffer bb = ByteBuffer.wrap(buff);
            for(File dir: ledgerDirsManager.getAllLedgerDirs()) {
                File file = new File(dir, "lastMark");
                try {
                    FileInputStream fis = new FileInputStream(file);
                    try {
                        int bytesRead = fis.read(buff);
                        if (bytesRead != 16) {
                            throw new IOException("Couldn't read enough bytes from lastMark."
                                                  + " Wanted " + 16 + ", got " + bytesRead);
                        }
                    } finally {
                        fis.close();
                    }
                    bb.clear();
                    long i = bb.getLong();
                    long p = bb.getLong();
                    if (i > txnLogId) {
                        txnLogId = i;
                        if(p > txnLogPosition) {
                          txnLogPosition = p;
                        }
                    }
                } catch (IOException e) {
                    LOG.error("Problems reading from " + file + " (this is okay if it is the first time starting this bookie");
                }
            }
        }

        @Override
        public String toString() {
            StringBuilder sb = new StringBuilder();

            sb.append("LastMark: logId - ").append(txnLogId)
              .append(" , position - ").append(txnLogPosition);

            return sb.toString();
        }
    }

    /**
     * Filter to return list of journals for rolling
     */
    private class JournalRollingFilter implements JournalIdFilter {
        @Override
        public boolean accept(long journalId) {
            if (journalId < lastLogMark.getLastMark().getTxnLogId()) {
                return true;
            } else {
                return false;
            }
        }
    }

    /**
     * Scanner used to scan a journal
     */
    public static interface JournalScanner {
        /**
         * Process a journal entry.
         *
         * @param journalVersion
         *          Journal Version
         * @param offset
         *          File offset of the journal entry
         * @param entry
         *          Journal Entry
         * @throws IOException
         */
        public void process(int journalVersion, long offset, ByteBuffer entry) throws IOException;
    }

    /**
     * Journal Entry to Record
     */
    private static class QueueEntry {
        QueueEntry(ByteBuffer entry, long ledgerId, long entryId,
                   WriteCallback cb, Object ctx) {
            this.entry = entry.duplicate();
            this.cb = cb;
            this.ctx = ctx;
            this.ledgerId = ledgerId;
            this.entryId = entryId;
        }

        ByteBuffer entry;

        long ledgerId;

        long entryId;

        WriteCallback cb;

        Object ctx;
    }

    final static long MB = 1024 * 1024L;
    // max journal file size
    final long maxJournalSize;
    // number journal files kept before marked journal
    final int maxBackupJournals;

    final File journalDirectory;
    final ServerConfiguration conf;

    private LastLogMark lastLogMark = new LastLogMark(0, 0);

    // journal entry queue to commit
    LinkedBlockingQueue<QueueEntry> queue = new LinkedBlockingQueue<QueueEntry>();

    volatile boolean running = true;
    private LedgerDirsManager ledgerDirsManager;

    public Journal(ServerConfiguration conf, LedgerDirsManager ledgerDirsManager) {
        super("BookieJournal-" + conf.getBookiePort());
        this.ledgerDirsManager = ledgerDirsManager;
        this.conf = conf;
        this.journalDirectory = Bookie.getCurrentDirectory(conf.getJournalDir());
        this.maxJournalSize = conf.getMaxJournalSize() * MB;
        this.maxBackupJournals = conf.getMaxBackupJournals();

        // read last log mark
        lastLogMark.readLog();
        LOG.debug("Last Log Mark : {}", lastLogMark);
    }

    LastLogMark getLastLogMark() {
        return lastLogMark;
    }

    /**
     * Records a <i>LastLogMark</i> in memory.
     *
     * <p>
     * The <i>LastLogMark</i> contains two parts: first one is <i>txnLogId</i>
     * (file id of a journal) and the second one is <i>txnLogPos</i> (offset in
     *  a journal). The <i>LastLogMark</i> indicates that those entries before
     * it have been persisted to both index and entry log files.
     * </p>
     *
     * <p>
     * This method is called before flushing entry log files and ledger cache.
     * </p>
     */
    public void markLog() {
        lastLogMark.markLog();
    }

    /**
     * Persists the <i>LastLogMark</i> marked by #markLog() to disk.
     *
     * <p>
     * This action means entries added before <i>LastLogMark</i> whose entry data
     * and index pages were already persisted to disk. It is the time to safely
     * remove journal files created earlier than <i>LastLogMark.txnLogId</i>.
     * </p>
     * <p>
     * If the bookie has crashed before persisting <i>LastLogMark</i> to disk,
     * it still has journal files contains entries for which index pages may not
     * have been persisted. Consequently, when the bookie restarts, it inspects
     * journal files to restore those entries; data isn't lost.
     * </p>
     * <p>
     * This method is called after flushing entry log files and ledger cache successfully, which is to ensure <i>LastLogMark</i> is pesisted.
     * </p>
     * @see #markLog()
     */
    public void rollLog() throws NoWritableLedgerDirException {
        lastLogMark.rollLog();
    }

    /**
     * Garbage collect older journals
     */
    public void gcJournals() {
        // list the journals that have been marked
        List<Long> logs = listJournalIds(journalDirectory, new JournalRollingFilter());
        // keep MAX_BACKUP_JOURNALS journal files before marked journal
        if (logs.size() >= maxBackupJournals) {
            int maxIdx = logs.size() - maxBackupJournals;
            for (int i=0; i<maxIdx; i++) {
                long id = logs.get(i);
                // make sure the journal id is smaller than marked journal id
                if (id < lastLogMark.getLastMark().getTxnLogId()) {
                    File journalFile = new File(journalDirectory, Long.toHexString(id) + ".txn");
                    if (!journalFile.delete()) {
                        LOG.warn("Could not delete old journal file {}", journalFile);
                    }
                    LOG.info("garbage collected journal " + journalFile.getName());
                }
            }
        }
    }

    /**
     * Scan the journal
     *
     * @param journalId
     *          Journal Log Id
     * @param journalPos
     *          Offset to start scanning
     * @param scanner
     *          Scanner to handle entries
     * @throws IOException
     */
    public void scanJournal(long journalId, long journalPos, JournalScanner scanner)
        throws IOException {
        JournalChannel recLog;
        if (journalPos <= 0) {
            recLog = new JournalChannel(journalDirectory, journalId);
        } else {
            recLog = new JournalChannel(journalDirectory, journalId, journalPos);
        }
        int journalVersion = recLog.getFormatVersion();
        try {
            ByteBuffer lenBuff = ByteBuffer.allocate(4);
            ByteBuffer recBuff = ByteBuffer.allocate(64*1024);
            while(true) {
                // entry start offset
                long offset = recLog.fc.position();
                // start reading entry
                lenBuff.clear();
                fullRead(recLog, lenBuff);
                if (lenBuff.remaining() != 0) {
                    break;
                }
                lenBuff.flip();
                int len = lenBuff.getInt();
                if (len == 0) {
                    break;
                }
                recBuff.clear();
                if (recBuff.remaining() < len) {
                    recBuff = ByteBuffer.allocate(len);
                }
                recBuff.limit(len);
                if (fullRead(recLog, recBuff) != len) {
                    // This seems scary, but it just means that this is where we
                    // left off writing
                    break;
                }
                recBuff.flip();
                scanner.process(journalVersion, offset, recBuff);
            }
        } finally {
            recLog.close();
        }
    }

    /**
     * Replay journal files
     *
     * @param scanner
     *          Scanner to process replayed entries.
     * @throws IOException
     */
    public void replay(JournalScanner scanner) throws IOException {
        final long markedLogId = lastLogMark.getTxnLogId();
        List<Long> logs = listJournalIds(journalDirectory, new JournalIdFilter() {
            @Override
            public boolean accept(long journalId) {
                if (journalId < markedLogId) {
                    return false;
                }
                return true;
            }
        });
        // last log mark may be missed due to no sync up before
        // validate filtered log ids only when we have markedLogId
        if (markedLogId > 0) {
            if (logs.size() == 0 || logs.get(0) != markedLogId) {
                throw new IOException("Recovery log " + markedLogId + " is missing");
            }
        }
        LOG.debug("Try to relay journal logs : {}", logs);
        // TODO: When reading in the journal logs that need to be synced, we
        // should use BufferedChannels instead to minimize the amount of
        // system calls done.
        for(Long id: logs) {
            long logPosition = 0L;
            if(id == markedLogId) {
                logPosition = lastLogMark.getTxnLogPosition();
            }
            scanJournal(id, logPosition, scanner);
        }
    }

    /**
     * record an add entry operation in journal
     */
    public void logAddEntry(ByteBuffer entry, WriteCallback cb, Object ctx) {
        long ledgerId = entry.getLong();
        long entryId = entry.getLong();
        entry.rewind();
        queue.add(new QueueEntry(entry, ledgerId, entryId, cb, ctx));
    }

    /**
     * Get the length of journal entries queue.
     *
     * @return length of journal entry queue.
     */
    public int getJournalQueueLength() {
        return queue.size();
    }

    /**
     * A thread used for persisting journal entries to journal files.
     *
     * <p>
     * Besides persisting journal entries, it also takes responsibility of
     * rolling journal files when a journal file reaches journal file size
     * limitation.
     * </p>
     * <p>
     * During journal rolling, it first closes the writing journal, generates
     * new journal file using current timestamp, and continue persistence logic.
     * Those journals will be garbage collected in SyncThread.
     * </p>
     * @see Bookie#SyncThread
     */
    @Override
    public void run() {
        LinkedList<QueueEntry> toFlush = new LinkedList<QueueEntry>();
        ByteBuffer lenBuff = ByteBuffer.allocate(4);
        JournalChannel logFile = null;
        try {
            long logId = 0;
            BufferedChannel bc = null;
            long lastFlushPosition = 0;

            QueueEntry qe = null;
            while (true) {
                // new journal file to write
                if (null == logFile) {
                    logId = MathUtils.now();
                    logFile = new JournalChannel(journalDirectory, logId);
                    bc = logFile.getBufferedChannel();

                    lastFlushPosition = 0;
                }

                if (qe == null) {
                    if (toFlush.isEmpty()) {
                        qe = queue.take();
                    } else {
                        qe = queue.poll();
                        if (qe == null || bc.position() > lastFlushPosition + 512*1024) {
                            //logFile.force(false);
                            bc.flush(true);
                            lastFlushPosition = bc.position();
                            lastLogMark.setLastLogMark(logId, lastFlushPosition);
                            for (QueueEntry e : toFlush) {
                                e.cb.writeComplete(BookieException.Code.OK,
                                                   e.ledgerId, e.entryId, null, e.ctx);
                            }
                            toFlush.clear();

                            // check whether journal file is over file limit
                            if (bc.position() > maxJournalSize) {
                                logFile.close();
                                logFile = null;
                                continue;
                            }
                        }
                    }
                }

                if (!running) {
                    LOG.info("Journal Manager is asked to shut down, quit.");
                    break;
                }

                if (qe == null) { // no more queue entry
                    continue;
                }
                lenBuff.clear();
                lenBuff.putInt(qe.entry.remaining());
                lenBuff.flip();
                //
                // we should be doing the following, but then we run out of
                // direct byte buffers
                // logFile.write(new ByteBuffer[] { lenBuff, qe.entry });
                bc.write(lenBuff);
                bc.write(qe.entry);

                logFile.preAllocIfNeeded();

                toFlush.add(qe);
                qe = null;
            }
            logFile.close();
            logFile = null;
        } catch (IOException ioe) {
            LOG.error("I/O exception in Journal thread!", ioe);
        } catch (InterruptedException ie) {
            LOG.warn("Journal exits when shutting down", ie);
        } finally {
            IOUtils.close(LOG, logFile);
        }
    }

    /**
     * Shuts down the journal.
     */
    public synchronized void shutdown() {
        try {
            if (!running) {
                return;
            }
            running = false;
            this.interrupt();
            this.join();
        } catch (InterruptedException ie) {
            LOG.warn("Interrupted during shutting down journal : ", ie);
        }
    }

    private static int fullRead(JournalChannel fc, ByteBuffer bb) throws IOException {
        int total = 0;
        while(bb.remaining() > 0) {
            int rc = fc.read(bb);
            if (rc <= 0) {
                return total;
            }
            total += rc;
        }
        return total;
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/JournalChannel.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

import java.util.Arrays;

import java.io.Closeable;
import java.io.File;
import java.io.RandomAccessFile;
import java.io.IOException;
import java.nio.channels.FileChannel;
import java.nio.ByteBuffer;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Simple wrapper around FileChannel to add versioning
 * information to the file.
 */
class JournalChannel implements Closeable {
    static Logger LOG = LoggerFactory.getLogger(JournalChannel.class);

    final FileChannel fc;
    final BufferedChannel bc;
    final int formatVersion;
    long nextPrealloc = 0;

    final byte[] MAGIC_WORD = "BKLG".getBytes();

    private final static int START_OF_FILE = -12345;

    int HEADER_SIZE = 8; // 4byte magic word, 4 byte version
    int MIN_COMPAT_JOURNAL_FORMAT_VERSION = 1;
    int CURRENT_JOURNAL_FORMAT_VERSION = 4;

    public final static long preAllocSize = 4*1024*1024;
    public final static ByteBuffer zeros = ByteBuffer.allocate(512);

    JournalChannel(File journalDirectory, long logId) throws IOException {
        this(journalDirectory, logId, START_OF_FILE);
    }

    JournalChannel(File journalDirectory, long logId, long position) throws IOException {
        File fn = new File(journalDirectory, Long.toHexString(logId) + ".txn");

        LOG.info("Opening journal {}", fn);
        if (!fn.exists()) { // new file, write version
            fc = new RandomAccessFile(fn, "rw").getChannel();
            formatVersion = CURRENT_JOURNAL_FORMAT_VERSION;

            ByteBuffer bb = ByteBuffer.allocate(HEADER_SIZE);
            bb.put(MAGIC_WORD);
            bb.putInt(formatVersion);
            bb.flip();
            fc.write(bb);
            fc.force(true);

            bc = new BufferedChannel(fc, 65536);

            nextPrealloc = preAllocSize;
            fc.write(zeros, nextPrealloc);
        } else {  // open an existing file
            fc = new RandomAccessFile(fn, "r").getChannel();
            bc = null; // readonly

            ByteBuffer bb = ByteBuffer.allocate(HEADER_SIZE);
            int c = fc.read(bb);
            bb.flip();

            if (c == HEADER_SIZE) {
                byte[] first4 = new byte[4];
                bb.get(first4);

                if (Arrays.equals(first4, MAGIC_WORD)) {
                    formatVersion = bb.getInt();
                } else {
                    // pre magic word journal, reset to 0;
                    formatVersion = 1;
                }
            } else {
                // no header, must be old version
                formatVersion = 1;
            }

            if (formatVersion < MIN_COMPAT_JOURNAL_FORMAT_VERSION
                || formatVersion > CURRENT_JOURNAL_FORMAT_VERSION) {
                String err = String.format("Invalid journal version, unable to read."
                        + " Expected between (%d) and (%d), got (%d)",
                        MIN_COMPAT_JOURNAL_FORMAT_VERSION, CURRENT_JOURNAL_FORMAT_VERSION,
                        formatVersion);
                LOG.error(err);
                throw new IOException(err);
            }

            try {
                if (position == START_OF_FILE) {
                    if (formatVersion >= 2) {
                        fc.position(HEADER_SIZE);
                    } else {
                        fc.position(0);
                    }
                } else {
                    fc.position(position);
                }
            } catch (IOException e) {
                LOG.error("Bookie journal file can seek to position :", e);
            }
        }
    }

    int getFormatVersion() {
        return formatVersion;
    }

    BufferedChannel getBufferedChannel() throws IOException {
        if (bc == null) {
            throw new IOException("Read only journal channel");
        }
        return bc;
    }

    void preAllocIfNeeded() throws IOException {
        if (bc.position() > nextPrealloc) {
            nextPrealloc = ((fc.size() + HEADER_SIZE) / preAllocSize + 1) * preAllocSize;
            zeros.clear();
            fc.write(zeros, nextPrealloc);
        }
    }

    int read(ByteBuffer dst)
            throws IOException {
        return fc.read(dst);
    }

    public void close() throws IOException {
        fc.close();
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/LedgerCache.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

import java.io.Closeable;
import java.io.IOException;

/**
 * This class maps a ledger entry number into a location (entrylogid, offset) in
 * an entry log file. It does user level caching to more efficiently manage disk
 * head scheduling.
 */
interface LedgerCache extends Closeable {

    boolean setFenced(long ledgerId) throws IOException;
    boolean isFenced(long ledgerId) throws IOException;

    void setMasterKey(long ledgerId, byte[] masterKey) throws IOException;
    byte[] readMasterKey(long ledgerId) throws IOException, BookieException;
    boolean ledgerExists(long ledgerId) throws IOException;

    void putEntryOffset(long ledger, long entry, long offset) throws IOException;
    long getEntryOffset(long ledger, long entry) throws IOException;

    void flushLedger(boolean doAll) throws IOException;
    long getLastEntry(long ledgerId) throws IOException;

    void deleteLedger(long ledgerId) throws IOException;

    LedgerCacheBean getJMXBean();
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/LedgerCacheBean.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.bookkeeper.bookie;

import org.apache.bookkeeper.jmx.BKMBeanInfo;

/**
 * Ledger Cache Bean
 */
public interface LedgerCacheBean extends LedgerCacheMXBean, BKMBeanInfo {
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/LedgerCacheImpl.java,true,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

import java.io.File;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Comparator;
import java.util.HashMap;
import java.util.Iterator;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.concurrent.atomic.AtomicBoolean;

import com.google.common.annotations.VisibleForTesting;

import org.apache.bookkeeper.util.SnapshotMap;
import org.apache.bookkeeper.bookie.LedgerDirsManager.LedgerDirsListener;
import org.apache.bookkeeper.bookie.LedgerDirsManager.NoWritableLedgerDirException;
import org.apache.bookkeeper.conf.ServerConfiguration;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Implementation of LedgerCache interface.
 * This class serves two purposes.
 */
public class LedgerCacheImpl implements LedgerCache {
    private final static Logger LOG = LoggerFactory.getLogger(LedgerDescriptor.class);
    private static final String IDX = ".idx";
    static final String RLOC = ".rloc";

    private LedgerDirsManager ledgerDirsManager;
    final private AtomicBoolean shouldRelocateIndexFile = new AtomicBoolean(false);

    public LedgerCacheImpl(ServerConfiguration conf, SnapshotMap<Long, Boolean> activeLedgers,
            LedgerDirsManager ledgerDirsManager)
            throws IOException {
        this.ledgerDirsManager = ledgerDirsManager;
        this.openFileLimit = conf.getOpenFileLimit();
        this.pageSize = conf.getPageSize();
        this.entriesPerPage = pageSize / 8;

        if (conf.getPageLimit() <= 0) {
            // allocate half of the memory to the page cache
            this.pageLimit = (int)((Runtime.getRuntime().maxMemory() / 3) / this.pageSize);
        } else {
            this.pageLimit = conf.getPageLimit();
        }
        LOG.info("maxMemory = " + Runtime.getRuntime().maxMemory());
        LOG.info("openFileLimit is " + openFileLimit + ", pageSize is " + pageSize + ", pageLimit is " + pageLimit);
        this.activeLedgers = activeLedgers;
        // Retrieve all of the active ledgers.
        getActiveLedgers();
        ledgerDirsManager.addLedgerDirsListener(getLedgerDirsListener());
    }
    /**
     * the list of potentially clean ledgers
     */
    LinkedList<Long> cleanLedgers = new LinkedList<Long>();

    /**
     * the list of potentially dirty ledgers
     */
    LinkedList<Long> dirtyLedgers = new LinkedList<Long>();

    HashMap<Long, FileInfo> fileInfoCache = new HashMap<Long, FileInfo>();

    LinkedList<Long> openLedgers = new LinkedList<Long>();

    // Manage all active ledgers in LedgerManager
    // so LedgerManager has knowledge to garbage collect inactive/deleted ledgers
    final SnapshotMap<Long, Boolean> activeLedgers;

    final int openFileLimit;
    final int pageSize;
    final int pageLimit;
    final int entriesPerPage;

    /**
     * @return page size used in ledger cache
     */
    public int getPageSize() {
        return pageSize;
    }

    /**
     * @return entries per page used in ledger cache
     */
    public int getEntriesPerPage() {
        return entriesPerPage;
    }

    /**
     * @return page limitation in ledger cache
     */
    public int getPageLimit() {
        return pageLimit;
    }

    // The number of pages that have actually been used
    private int pageCount = 0;
    HashMap<Long, HashMap<Long,LedgerEntryPage>> pages = new HashMap<Long, HashMap<Long,LedgerEntryPage>>();

    /**
     * @return number of page used in ledger cache
     */
    public int getNumUsedPages() {
        return pageCount;
    }

    private void putIntoTable(HashMap<Long, HashMap<Long,LedgerEntryPage>> table, LedgerEntryPage lep) {
        HashMap<Long, LedgerEntryPage> map = table.get(lep.getLedger());
        if (map == null) {
            map = new HashMap<Long, LedgerEntryPage>();
            table.put(lep.getLedger(), map);
        }
        map.put(lep.getFirstEntry(), lep);
    }

    private static LedgerEntryPage getFromTable(HashMap<Long, HashMap<Long,LedgerEntryPage>> table,
                                                Long ledger, Long firstEntry) {
        HashMap<Long, LedgerEntryPage> map = table.get(ledger);
        if (map != null) {
            return map.get(firstEntry);
        }
        return null;
    }

    synchronized protected LedgerEntryPage getLedgerEntryPage(Long ledger, Long firstEntry, boolean onlyDirty) {
        LedgerEntryPage lep = getFromTable(pages, ledger, firstEntry);
        if (lep == null) {
            return null;
        }

        lep.usePage();

        if (onlyDirty && lep.isClean()) {
            return null;
        } else {
            return lep;
        }
    }

    /** 
     * Grab ledger entry page whose first entry is <code>pageEntry</code>.
     *
     * If the page doesn't existed before, we allocate a memory page.
     * Otherwise, we grab a clean page and read it from disk.
     *
     * @param ledger
     *          Ledger Id
     * @param pageEntry
     *          Start entry of this entry page.
     */
    private LedgerEntryPage grabLedgerEntryPage(long ledger, long pageEntry) throws IOException {
        LedgerEntryPage lep = grabCleanPage(ledger, pageEntry);
        try {
            // should update page before we put it into table
            // otherwise we would put an empty page in it
            updatePage(lep);
            synchronized(this) {
                putIntoTable(pages, lep);
            }   
        } catch (IOException ie) {
            // if we grab a clean page, but failed to update the page
            // we are exhausting the count of ledger entry pages.
            // since this page will be never used, so we need to decrement
            // page count of ledger cache.
            lep.releasePage();
            synchronized (this) {
                --pageCount;
            }
            throw ie; 
        }   
        return lep;
    }

    @Override
    public void putEntryOffset(long ledger, long entry, long offset) throws IOException {
        int offsetInPage = (int) (entry % entriesPerPage);
        // find the id of the first entry of the page that has the entry
        // we are looking for
        long pageEntry = entry-offsetInPage;
        LedgerEntryPage lep = getLedgerEntryPage(ledger, pageEntry, false);
        if (lep == null) {
            lep = grabLedgerEntryPage(ledger, pageEntry); 
        }
        if (lep != null) {
            lep.setOffset(offset, offsetInPage*8);
            lep.releasePage();
            return;
        }
    }

    @Override
    public long getEntryOffset(long ledger, long entry) throws IOException {
        int offsetInPage = (int) (entry%entriesPerPage);
        // find the id of the first entry of the page that has the entry
        // we are looking for
        long pageEntry = entry-offsetInPage;
        LedgerEntryPage lep = getLedgerEntryPage(ledger, pageEntry, false);
        try {
            if (lep == null) {
                lep = grabLedgerEntryPage(ledger, pageEntry);
            }
            return lep.getOffset(offsetInPage*8);
        } finally {
            if (lep != null) {
                lep.releasePage();
            }
        }
    }

    @VisibleForTesting
    public static final String getLedgerName(long ledgerId) {
        int parent = (int) (ledgerId & 0xff);
        int grandParent = (int) ((ledgerId & 0xff00) >> 8);
        StringBuilder sb = new StringBuilder();
        sb.append(Integer.toHexString(grandParent));
        sb.append('/');
        sb.append(Integer.toHexString(parent));
        sb.append('/');
        sb.append(Long.toHexString(ledgerId));
        sb.append(IDX);
        return sb.toString();
    }

    FileInfo getFileInfo(Long ledger, byte masterKey[]) throws IOException {
        synchronized(fileInfoCache) {
            FileInfo fi = fileInfoCache.get(ledger);
            if (fi == null) {
                File lf = findIndexFile(ledger);
                if (lf == null) {
                    if (masterKey == null) {
                        throw new Bookie.NoLedgerException(ledger);
                    }
                    lf = getNewLedgerIndexFile(ledger, null);
                    // A new ledger index file has been created for this Bookie.
                    // Add this new ledger to the set of active ledgers.
                    LOG.debug("New ledger index file created for ledgerId: {}", ledger);
                    activeLedgers.put(ledger, true);
                }
                evictFileInfoIfNecessary();
                fi = new FileInfo(lf, masterKey);
                if (ledgerDirsManager.isDirFull(getLedgerDirForLedger(fi))) {
                    moveLedgerIndexFile(ledger, fi);
                }
                fileInfoCache.put(ledger, fi);
                openLedgers.add(ledger);
            }
            if (fi != null) {
                fi.use();
            }
            return fi;
        }
    }

    /**
     * Get a new index file for ledger excluding directory <code>excludedDir</code>.
     *
     * @param ledger
     *          Ledger id.
     * @param excludedDir
     *          The ledger directory to exclude.
     * @return new index file object.
     * @throws NoWritableLedgerDirException if there is no writable dir available.
     */
    private File getNewLedgerIndexFile(Long ledger, File excludedDir)
    throws NoWritableLedgerDirException {
        File dir = ledgerDirsManager.pickRandomWritableDir(excludedDir);
        String ledgerName = getLedgerName(ledger);
        return new File(dir, ledgerName);
    }

    private void updatePage(LedgerEntryPage lep) throws IOException {
        if (!lep.isClean()) {
            throw new IOException("Trying to update a dirty page");
        }
        FileInfo fi = null;
        try {
            fi = getFileInfo(lep.getLedger(), null);
            long pos = lep.getFirstEntry()*8;
            if (pos >= fi.size()) {
                lep.zeroPage();
            } else {
                lep.readPage(fi);
            }
        } finally {
            if (fi != null) {
                fi.release();
            }
        }
    }

    private LedgerDirsListener getLedgerDirsListener() {
        return new LedgerDirsListener() {
            @Override
            public void diskFull(File disk) {
                // If the current entry log disk is full, then create new entry
                // log.
                shouldRelocateIndexFile.set(true);
            }

            @Override
            public void diskFailed(File disk) {
                // Nothing to handle here. Will be handled in Bookie
            }

            @Override
            public void allDisksFull() {
                // Nothing to handle here. Will be handled in Bookie
            }

            @Override
            public void fatalError() {
                // Nothing to handle here. Will be handled in Bookie
            }
        };
    }

    @Override
    public void flushLedger(boolean doAll) throws IOException {
        synchronized(dirtyLedgers) {
            if (dirtyLedgers.isEmpty()) {
                synchronized(this) {
                    for(Long l: pages.keySet()) {
                        if (LOG.isTraceEnabled()) {
                            LOG.trace("Adding {} to dirty pages", Long.toHexString(l));
                        }
                        dirtyLedgers.add(l);
                    }
                }
            }
            if (dirtyLedgers.isEmpty()) {
                return;
            }

            if (shouldRelocateIndexFile.get()) {
                // if some new dir detected as full, then move all corresponding
                // open index files to new location
                for (Long l : dirtyLedgers) {
                    FileInfo fi = getFileInfo(l, null);
                    File currentDir = getLedgerDirForLedger(fi);
                    if (ledgerDirsManager.isDirFull(currentDir)) {
                        moveLedgerIndexFile(l, fi);
                    }
                }
                shouldRelocateIndexFile.set(false);
            }

            while(!dirtyLedgers.isEmpty()) {
                Long l = dirtyLedgers.removeFirst();

                flushLedger(l);

                if (!doAll) {
                    break;
                }
                // Yield. if we are doing all the ledgers we don't want to block other flushes that
                // need to happen
                try {
                    dirtyLedgers.wait(1);
                } catch (InterruptedException e) {
                    // just pass it on
                    Thread.currentThread().interrupt();
                }
            }
        }
    }

    /**
     * Get the ledger directory that the ledger index belongs to.
     *
     * @param fi File info of a ledger
     * @return ledger directory that the ledger belongs to.
     */
    private File getLedgerDirForLedger(FileInfo fi) {
        return fi.getLf().getParentFile().getParentFile().getParentFile();
    }

    private void moveLedgerIndexFile(Long l, FileInfo fi) throws NoWritableLedgerDirException, IOException {
        File newLedgerIndexFile = getNewLedgerIndexFile(l, getLedgerDirForLedger(fi));
        fi.moveToNewLocation(newLedgerIndexFile, fi.getSizeSinceLastwrite());
    }

    /**
     * Flush a specified ledger
     *
     * @param l
     *          Ledger Id
     * @throws IOException
     */
    private void flushLedger(long l) throws IOException {
        LinkedList<Long> firstEntryList;
        synchronized(this) {
            HashMap<Long, LedgerEntryPage> pageMap = pages.get(l);
            if (pageMap == null || pageMap.isEmpty()) {
                FileInfo fi = null;
                try {
                    fi = getFileInfo(l, null);
                    fi.flushHeader();
                } finally {
                    if (null != fi) {
                        fi.release();
                    }
                }
                return;
            }
            firstEntryList = new LinkedList<Long>();
            for(Map.Entry<Long, LedgerEntryPage> entry: pageMap.entrySet()) {
                LedgerEntryPage lep = entry.getValue();
                if (lep.isClean()) {
                    LOG.trace("Page is clean {}", lep);
                    continue;
                }
                firstEntryList.add(lep.getFirstEntry());
            }
        }

        if (firstEntryList.size() == 0) {
            LOG.debug("Nothing to flush for ledger {}.", l);
            // nothing to do
            return;
        }

        // Now flush all the pages of a ledger
        List<LedgerEntryPage> entries = new ArrayList<LedgerEntryPage>(firstEntryList.size());
        FileInfo fi = null;
        try {
            for(Long firstEntry: firstEntryList) {
                LedgerEntryPage lep = getLedgerEntryPage(l, firstEntry, true);
                if (lep != null) {
                    entries.add(lep);
                }
            }
            Collections.sort(entries, new Comparator<LedgerEntryPage>() {
                    @Override
                    public int compare(LedgerEntryPage o1, LedgerEntryPage o2) {
                    return (int)(o1.getFirstEntry()-o2.getFirstEntry());
                    }
                    });
            ArrayList<Integer> versions = new ArrayList<Integer>(entries.size());
            fi = getFileInfo(l, null);
            // flush the header if necessary
            fi.flushHeader();
            int start = 0;
            long lastOffset = -1;
            for(int i = 0; i < entries.size(); i++) {
                versions.add(i, entries.get(i).getVersion());
                if (lastOffset != -1 && (entries.get(i).getFirstEntry() - lastOffset) != entriesPerPage) {
                    // send up a sequential list
                    int count = i - start;
                    if (count == 0) {
                        LOG.warn("Count cannot possibly be zero!");
                    }
                    writeBuffers(l, entries, fi, start, count);
                    start = i;
                }
                lastOffset = entries.get(i).getFirstEntry();
            }
            if (entries.size()-start == 0 && entries.size() != 0) {
                LOG.warn("Nothing to write, but there were entries!");
            }
            writeBuffers(l, entries, fi, start, entries.size()-start);
            synchronized(this) {
                for(int i = 0; i < entries.size(); i++) {
                    LedgerEntryPage lep = entries.get(i);
                    lep.setClean(versions.get(i));
                }
            }
        } finally {
            for(LedgerEntryPage lep: entries) {
                lep.releasePage();
            }
            if (fi != null) {
                fi.release();
            }
        }
    }

    private void writeBuffers(Long ledger,
                              List<LedgerEntryPage> entries, FileInfo fi,
                              int start, int count) throws IOException {
        if (LOG.isTraceEnabled()) {
            LOG.trace("Writing {} buffers of {}", count, Long.toHexString(ledger));
        }
        if (count == 0) {
            return;
        }
        ByteBuffer buffs[] = new ByteBuffer[count];
        for(int j = 0; j < count; j++) {
            buffs[j] = entries.get(start+j).getPageToWrite();
            if (entries.get(start+j).getLedger() != ledger) {
                throw new IOException("Writing to " + ledger + " but page belongs to "
                                      + entries.get(start+j).getLedger());
            }
        }
        long totalWritten = 0;
        while(buffs[buffs.length-1].remaining() > 0) {
            long rc = fi.write(buffs, entries.get(start+0).getFirstEntry()*8);
            if (rc <= 0) {
                throw new IOException("Short write to ledger " + ledger + " rc = " + rc);
            }
            totalWritten += rc;
        }
        if (totalWritten != (long)count * (long)pageSize) {
            throw new IOException("Short write to ledger " + ledger + " wrote " + totalWritten
                                  + " expected " + count * pageSize);
        }
    }
    private LedgerEntryPage grabCleanPage(long ledger, long entry) throws IOException {
        if (entry % entriesPerPage != 0) {
            throw new IllegalArgumentException(entry + " is not a multiple of " + entriesPerPage);
        }
        outerLoop:
        while(true) {
            synchronized(this) {
                if (pageCount  < pageLimit) {
                    // let's see if we can allocate something
                    LedgerEntryPage lep = new LedgerEntryPage(pageSize, entriesPerPage);
                    lep.setLedger(ledger);
                    lep.setFirstEntry(entry);

                    // note, this will not block since it is a new page
                    lep.usePage();
                    pageCount++;
                    return lep;
                }
            }

            synchronized(cleanLedgers) {
                if (cleanLedgers.isEmpty()) {
                    flushLedger(false);
                    synchronized(this) {
                        for(Long l: pages.keySet()) {
                            cleanLedgers.add(l);
                        }
                    }
                }
                synchronized(this) {
                    // if ledgers deleted between checking pageCount and putting
                    // ledgers into cleanLedgers list, the cleanLedgers list would be empty.
                    // so give it a chance to go back to check pageCount again because
                    // deleteLedger would decrement pageCount to return the number of pages
                    // occupied by deleted ledgers.
                    if (cleanLedgers.isEmpty()) {
                        continue outerLoop;
                    }
                    Long cleanLedger = cleanLedgers.getFirst();
                    Map<Long, LedgerEntryPage> map = pages.get(cleanLedger);
                    while (map == null || map.isEmpty()) {
                        cleanLedgers.removeFirst();
                        if (cleanLedgers.isEmpty()) {
                            continue outerLoop; 
                        }
                        cleanLedger = cleanLedgers.getFirst();
                        map = pages.get(cleanLedger);
                    }
                    Iterator<Map.Entry<Long, LedgerEntryPage>> it = map.entrySet().iterator();
                    LedgerEntryPage lep = it.next().getValue();
                    while((lep.inUse() || !lep.isClean())) {
                        if (!it.hasNext()) {
                            // no clean page found in this ledger
                            cleanLedgers.removeFirst();
                            continue outerLoop;
                        }
                        lep = it.next().getValue();
                    }
                    it.remove();
                    if (map.isEmpty()) {
                        pages.remove(lep.getLedger());
                    }
                    lep.usePage();
                    lep.zeroPage();
                    lep.setLedger(ledger);
                    lep.setFirstEntry(entry);
                    return lep;
                }
            }
        }
    }

    @Override
    public long getLastEntry(long ledgerId) throws IOException {
        long lastEntry = 0;
        // Find the last entry in the cache
        synchronized(this) {
            Map<Long, LedgerEntryPage> map = pages.get(ledgerId);
            if (map != null) {
                for(LedgerEntryPage lep: map.values()) {
                    if (lep.getFirstEntry() + entriesPerPage < lastEntry) {
                        continue;
                    }
                    lep.usePage();
                    long highest = lep.getLastEntry();
                    if (highest > lastEntry) {
                        lastEntry = highest;
                    }
                    lep.releasePage();
                }
            }
        }

        FileInfo fi = null;
        try {
            fi = getFileInfo(ledgerId, null);
            long size = fi.size();
            // make sure the file size is aligned with index entry size
            // otherwise we may read incorret data
            if (0 != size % 8) {
                LOG.warn("Index file of ledger {} is not aligned with index entry size.", ledgerId);
                size = size - size % 8;
            }
            // we may not have the last entry in the cache
            if (size > lastEntry*8) {
                ByteBuffer bb = ByteBuffer.allocate(getPageSize());
                long position = size - getPageSize();
                if (position < 0) {
                    position = 0;
                }
                fi.read(bb, position);
                bb.flip();
                long startingEntryId = position/8;
                for(int i = getEntriesPerPage()-1; i >= 0; i--) {
                    if (bb.getLong(i*8) != 0) {
                        if (lastEntry < startingEntryId+i) {
                            lastEntry = startingEntryId+i;
                        }
                        break;
                    }
                }
            }
        } finally {
            if (fi != null) {
                fi.release();
            }
        }

        return lastEntry;
    }

    /**
     * This method will look within the ledger directories for the ledger index
     * files. That will comprise the set of active ledgers this particular
     * BookieServer knows about that have not yet been deleted by the BookKeeper
     * Client. This is called only once during initialization.
     */
    private void getActiveLedgers() throws IOException {
        // Ledger index files are stored in a file hierarchy with a parent and
        // grandParent directory. We'll have to go two levels deep into these
        // directories to find the index files.
        for (File ledgerDirectory : ledgerDirsManager.getAllLedgerDirs()) {
            for (File grandParent : ledgerDirectory.listFiles()) {
                if (grandParent.isDirectory()) {
                    for (File parent : grandParent.listFiles()) {
                        if (parent.isDirectory()) {
                            for (File index : parent.listFiles()) {
                                if (!index.isFile()
                                        || (!index.getName().endsWith(IDX) && !index.getName().endsWith(RLOC))) {
                                    continue;
                                }

                                // We've found a ledger index file. The file
                                // name is the HexString representation of the
                                // ledgerId.
                                String ledgerIdInHex = index.getName().replace(RLOC, "").replace(IDX, "");
                                if (index.getName().endsWith(RLOC)) {
                                    if (findIndexFile(Long.parseLong(ledgerIdInHex)) != null) {
                                        if (!index.delete()) {
                                            LOG.warn("Deleting the rloc file " + index + " failed");
                                        }
                                        continue;
                                    } else {
                                        File dest = new File(index.getParentFile(), ledgerIdInHex + IDX);
                                        if (!index.renameTo(dest)) {
                                            throw new IOException("Renaming rloc file " + index
                                                    + " to index file has failed");
                                        }
                                    }
                                }
                                activeLedgers.put(Long.parseLong(ledgerIdInHex, 16), true);
                            }
                        }
                    }
                }
            }
        }
    }

    /**
     * This method is called whenever a ledger is deleted by the BookKeeper Client
     * and we want to remove all relevant data for it stored in the LedgerCache.
     */
    @Override
    public void deleteLedger(long ledgerId) throws IOException {
        LOG.debug("Deleting ledgerId: {}", ledgerId);

        // remove pages first to avoid page flushed when deleting file info
        synchronized(this) {
            Map<Long, LedgerEntryPage> lpages = pages.remove(ledgerId);
            if (null != lpages) {
                pageCount -= lpages.size();
                if (pageCount < 0) {
                    LOG.error("Page count of ledger cache has been decremented to be less than zero.");
                }
            }
        }
        // Delete the ledger's index file and close the FileInfo
        FileInfo fi = null;
        try {
            fi = getFileInfo(ledgerId, null);
            fi.close(false);
            fi.delete();
        } finally {
            // should release use count
            // otherwise the file channel would not be closed.
            if (null != fi) {
                fi.release();
            }
        }

        // Remove it from the active ledger manager
        activeLedgers.remove(ledgerId);

        // Now remove it from all the other lists and maps.
        // These data structures need to be synchronized first before removing entries.
        synchronized(fileInfoCache) {
            fileInfoCache.remove(ledgerId);
        }
        synchronized(cleanLedgers) {
            cleanLedgers.remove(ledgerId);
        }
        synchronized(dirtyLedgers) {
            dirtyLedgers.remove(ledgerId);
        }
        synchronized(openLedgers) {
            openLedgers.remove(ledgerId);
        }
    }

    private File findIndexFile(long ledgerId) throws IOException {
        String ledgerName = getLedgerName(ledgerId);
        for (File d : ledgerDirsManager.getAllLedgerDirs()) {
            File lf = new File(d, ledgerName);
            if (lf.exists()) {
                return lf;
            }
        }
        return null;
    }

    @Override
    public byte[] readMasterKey(long ledgerId) throws IOException, BookieException {
        synchronized(fileInfoCache) {
            FileInfo fi = fileInfoCache.get(ledgerId);
            if (fi == null) {
                File lf = findIndexFile(ledgerId);
                if (lf == null) {
                    throw new Bookie.NoLedgerException(ledgerId);
                }
                evictFileInfoIfNecessary();        
                fi = new FileInfo(lf, null);
                byte[] key = fi.getMasterKey();
                fileInfoCache.put(ledgerId, fi);
                openLedgers.add(ledgerId);
                return key;
            }
            return fi.getMasterKey();
        }
    }

    // evict file info if necessary
    private void evictFileInfoIfNecessary() throws IOException {
        synchronized (fileInfoCache) {
            if (openLedgers.size() > openFileLimit) {
                long ledgerToRemove = openLedgers.removeFirst();
                LOG.info("Ledger {} is evicted from file info cache.",
                         ledgerToRemove);
                fileInfoCache.remove(ledgerToRemove).close(true);
            }
        }
    }

    @Override
    public boolean setFenced(long ledgerId) throws IOException {
        FileInfo fi = null;
        try {
            fi = getFileInfo(ledgerId, null);
            if (null != fi) {
                return fi.setFenced();
            }
            return false;
        } finally {
            if (null != fi) {
                fi.release();
            }
        }
    }

    @Override
    public boolean isFenced(long ledgerId) throws IOException {
        FileInfo fi = null;
        try {
            fi = getFileInfo(ledgerId, null);
            if (null != fi) {
                return fi.isFenced();
            }
            return false;
        } finally {
            if (null != fi) {
                fi.release();
            }
        }
    }

    @Override
    public void setMasterKey(long ledgerId, byte[] masterKey) throws IOException {
        FileInfo fi = null;
        try {
            fi = getFileInfo(ledgerId, masterKey);
        } finally {
            if (null != fi) {
                fi.release();
            }
        }
    }

    @Override
    public boolean ledgerExists(long ledgerId) throws IOException {
        synchronized(fileInfoCache) {
            FileInfo fi = fileInfoCache.get(ledgerId);
            if (fi == null) {
                File lf = findIndexFile(ledgerId);
                if (lf == null) {
                    return false;
                }
            }
        }
        return true;
    }

    @Override
    public LedgerCacheBean getJMXBean() {
        return new LedgerCacheBean() {
            @Override
            public String getName() {
                return "LedgerCache";
            }

            @Override
            public boolean isHidden() {
                return false;
            }

            @Override
            public int getPageCount() {
                return LedgerCacheImpl.this.getNumUsedPages();
            }

            @Override
            public int getPageSize() {
                return LedgerCacheImpl.this.getPageSize();
            }

            @Override
            public int getOpenFileLimit() {
                return openFileLimit;
            }

            @Override
            public int getPageLimit() {
                return LedgerCacheImpl.this.getPageLimit();
            }

            @Override
            public int getNumCleanLedgers() {
                return cleanLedgers.size();
            }

            @Override
            public int getNumDirtyLedgers() {
                return dirtyLedgers.size();
            }

            @Override
            public int getNumOpenLedgers() {
                return openLedgers.size();
            }
        };
    }

    @Override
    public void close() throws IOException {
        synchronized (fileInfoCache) {
            for (Entry<Long, FileInfo> fileInfo : fileInfoCache.entrySet()) {
                FileInfo value = fileInfo.getValue();
                if (value != null) {
                    value.close(true);
                }
            }
            fileInfoCache.clear();
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/LedgerCacheMXBean.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.bookkeeper.bookie;

/**
 * Ledger Cache MBean
 */
public interface LedgerCacheMXBean {

    /**
     * @return number of page used in cache
     */
    public int getPageCount();

    /**
     * @return page size
     */
    public int getPageSize();

    /**
     * @return the limit of open files
     */
    public int getOpenFileLimit();

    /**
     * @return the limit number of pages
     */
    public int getPageLimit();

    /**
     * @return number of clean ledgers
     */
    public int getNumCleanLedgers();

    /**
     * @return number of dirty ledgers
     */
    public int getNumDirtyLedgers();

    /**
     * @return number of open ledgers
     */
    public int getNumOpenLedgers();
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/LedgerDescriptor.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

import java.io.IOException;
import java.nio.ByteBuffer;
import java.util.Arrays;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Implements a ledger inside a bookie. In particular, it implements operations
 * to write entries to a ledger and read entries from a ledger.
 */
public abstract class LedgerDescriptor {
    static LedgerDescriptor create(byte[] masterKey,
                                   long ledgerId,
                                   LedgerStorage ledgerStorage) throws IOException {
        LedgerDescriptor ledger = new LedgerDescriptorImpl(masterKey, ledgerId, ledgerStorage);
        ledgerStorage.setMasterKey(ledgerId, masterKey);
        return ledger;
    }

    static LedgerDescriptor createReadOnly(long ledgerId,
                                           LedgerStorage ledgerStorage)
            throws IOException, Bookie.NoLedgerException {
        if (!ledgerStorage.ledgerExists(ledgerId)) {
            throw new Bookie.NoLedgerException(ledgerId);
        }
        return new LedgerDescriptorReadOnlyImpl(ledgerId, ledgerStorage);
    }

    abstract void checkAccess(byte masterKey[]) throws BookieException, IOException;

    abstract long getLedgerId();

    abstract boolean setFenced() throws IOException;
    abstract boolean isFenced() throws IOException;

    abstract long addEntry(ByteBuffer entry) throws IOException;
    abstract ByteBuffer readEntry(long entryId) throws IOException;
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/LedgerDescriptorImpl.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

import java.io.IOException;
import java.nio.ByteBuffer;
import java.util.Arrays;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Implements a ledger inside a bookie. In particular, it implements operations
 * to write entries to a ledger and read entries from a ledger.
 *
 */
public class LedgerDescriptorImpl extends LedgerDescriptor {
    final static Logger LOG = LoggerFactory.getLogger(LedgerDescriptor.class);
    final LedgerStorage ledgerStorage;
    private long ledgerId;

    final byte[] masterKey;

    LedgerDescriptorImpl(byte[] masterKey, long ledgerId, LedgerStorage ledgerStorage) {
        this.masterKey = masterKey;
        this.ledgerId = ledgerId;
        this.ledgerStorage = ledgerStorage;
    }

    @Override
    void checkAccess(byte masterKey[]) throws BookieException, IOException {
        if (!Arrays.equals(this.masterKey, masterKey)) {
            throw BookieException.create(BookieException.Code.UnauthorizedAccessException);
        }
    }

    @Override
    public long getLedgerId() {
        return ledgerId;
    }

    @Override
    boolean setFenced() throws IOException {
        return ledgerStorage.setFenced(ledgerId);
    }

    @Override
    boolean isFenced() throws IOException {
        return ledgerStorage.isFenced(ledgerId);
    }

    @Override
    long addEntry(ByteBuffer entry) throws IOException {
        long ledgerId = entry.getLong();

        if (ledgerId != this.ledgerId) {
            throw new IOException("Entry for ledger " + ledgerId + " was sent to " + this.ledgerId);
        }
        entry.rewind();

        return ledgerStorage.addEntry(entry);
    }

    @Override
    ByteBuffer readEntry(long entryId) throws IOException {
        return ledgerStorage.getEntry(ledgerId, entryId);
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/LedgerDescriptorReadOnlyImpl.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

import java.io.IOException;
import java.nio.ByteBuffer;
/**
 * Implements a ledger inside a bookie. In particular, it implements operations
 * to write entries to a ledger and read entries from a ledger.
 */
public class LedgerDescriptorReadOnlyImpl extends LedgerDescriptorImpl {
    LedgerDescriptorReadOnlyImpl(long ledgerId, LedgerStorage storage) {
        super(null, ledgerId, storage);
    }

    @Override
    boolean setFenced() throws IOException {
        assert false;
        throw new IOException("Invalid action on read only descriptor");
    }

    @Override
    long addEntry(ByteBuffer entry) throws IOException {
        assert false;
        throw new IOException("Invalid action on read only descriptor");
    }

    @Override
    void checkAccess(byte masterKey[]) throws BookieException, IOException {
        assert false;
        throw new IOException("Invalid action on read only descriptor");
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/LedgerDirsManager.java,false,"/**
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */
package org.apache.bookkeeper.bookie;

import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.Random;

import com.google.common.annotations.VisibleForTesting;
import org.apache.bookkeeper.conf.ServerConfiguration;
import org.apache.bookkeeper.util.DiskChecker;
import org.apache.bookkeeper.util.DiskChecker.DiskErrorException;
import org.apache.bookkeeper.util.DiskChecker.DiskOutOfSpaceException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * This class manages ledger directories used by the bookie.
 */
public class LedgerDirsManager {
    private static Logger LOG = LoggerFactory
            .getLogger(LedgerDirsManager.class);

    private volatile List<File> filledDirs;
    private final List<File> ledgerDirectories;
    private volatile List<File> writableLedgerDirectories;
    private DiskChecker diskChecker;
    private List<LedgerDirsListener> listeners;
    private LedgerDirsMonitor monitor;
    private final Random rand = new Random();

    public LedgerDirsManager(ServerConfiguration conf) {
        this.ledgerDirectories = Arrays.asList(Bookie
                .getCurrentDirectories(conf.getLedgerDirs()));
        this.writableLedgerDirectories = new ArrayList<File>(ledgerDirectories);
        this.filledDirs = new ArrayList<File>();
        listeners = new ArrayList<LedgerDirsManager.LedgerDirsListener>();
        diskChecker = new DiskChecker(conf.getDiskUsageThreshold());
        monitor = new LedgerDirsMonitor(conf.getDiskCheckInterval());
    }

    /**
     * Get all ledger dirs configured
     */
    public List<File> getAllLedgerDirs() {
        return ledgerDirectories;
    }

    /**
     * Get only writable ledger dirs.
     */
    public List<File> getWritableLedgerDirs()
            throws NoWritableLedgerDirException {
        if (writableLedgerDirectories.isEmpty()) {
            String errMsg = "All ledger directories are non writable";
            NoWritableLedgerDirException e = new NoWritableLedgerDirException(
                    errMsg);
            LOG.error(errMsg, e);
            throw e;
        }
        return writableLedgerDirectories;
    }

    /**
     * Get dirs, which are full more than threshold
     */
    public boolean isDirFull(File dir) {
        return filledDirs.contains(dir);
    }

    /**
     * Add the dir to filled dirs list
     */
    @VisibleForTesting
    public void addToFilledDirs(File dir) {
        if (!filledDirs.contains(dir)) {
            LOG.warn(dir + " is out of space."
                    + " Adding it to filled dirs list");
            // Update filled dirs list
            List<File> updatedFilledDirs = new ArrayList<File>(filledDirs);
            updatedFilledDirs.add(dir);
            filledDirs = updatedFilledDirs;
            // Update the writable ledgers list
            List<File> newDirs = new ArrayList<File>(writableLedgerDirectories);
            newDirs.removeAll(filledDirs);
            writableLedgerDirectories = newDirs;
            // Notify listeners about disk full
            for (LedgerDirsListener listener : listeners) {
                listener.diskFull(dir);
            }
        }
    }

    /**
     * Returns one of the ledger dir from writable dirs list randomly.
     */
    File pickRandomWritableDir() throws NoWritableLedgerDirException {
        return pickRandomWritableDir(null);
    }

    /**
     * Pick up a writable dir from available dirs list randomly. The <code>excludedDir</code>
     * will not be pickedup.
     *
     * @param excludedDir
     *          The directory to exclude during pickup.
     * @throws NoWritableLedgerDirException if there is no writable dir available.
     */
    File pickRandomWritableDir(File excludedDir) throws NoWritableLedgerDirException {
        List<File> writableDirs = getWritableLedgerDirs();

        final int start = rand.nextInt(writableDirs.size());
        int idx = start;
        File candidate = writableDirs.get(idx);
        while (null != excludedDir && excludedDir.equals(candidate)) {
            idx = (idx + 1) % writableDirs.size();
            if (idx == start) {
                // after searching all available dirs,
                // no writable dir is found
                throw new NoWritableLedgerDirException("No writable directories found from "
                        + " available writable dirs (" + writableDirs + ") : exclude dir "
                        + excludedDir);
            }
            candidate = writableDirs.get(idx);
        }
        return candidate;
    }

    public void addLedgerDirsListener(LedgerDirsListener listener) {
        if (listener != null) {
            listeners.add(listener);
        }
    }

    // start the daemon for disk monitoring
    public void start() {
        monitor.setDaemon(true);
        monitor.start();
    }

    // shutdown disk monitoring daemon
    public void shutdown() {
        monitor.interrupt();
        try {
            monitor.join();
        } catch (InterruptedException e) {
            // Ignore
        }
    }

    /**
     * Thread to monitor the disk space periodically.
     */
    private class LedgerDirsMonitor extends Thread {
        int interval;

        public LedgerDirsMonitor(int interval) {
            this.interval = interval;
        }

        @Override
        public void run() {
            try {
                while (true) {
                    List<File> writableDirs;
                    try {
                        writableDirs = getWritableLedgerDirs();
                    } catch (NoWritableLedgerDirException e) {
                        for (LedgerDirsListener listener : listeners) {
                            listener.allDisksFull();
                        }
                        break;
                    }
                    // Check all writable dirs disk space usage.
                    for (File dir : writableDirs) {
                        try {
                            diskChecker.checkDir(dir);
                        } catch (DiskErrorException e) {
                            // Notify disk failure to all listeners
                            for (LedgerDirsListener listener : listeners) {
                                listener.diskFailed(dir);
                            }
                        } catch (DiskOutOfSpaceException e) {
                            // Notify disk full to all listeners
                            addToFilledDirs(dir);
                        }
                    }
                    try {
                        Thread.sleep(interval);
                    } catch (InterruptedException e) {
                        LOG.info("LedgerDirsMonitor thread is interrupted");
                        break;
                    }
                }
            } catch (Exception e) {
                LOG.error("Error Occured while checking disks", e);
                // Notify disk failure to all listeners
                for (LedgerDirsListener listener : listeners) {
                    listener.fatalError();
                }
            }
        }
    }

    /**
     * Indicates All configured ledger directories are full.
     */
    public static class NoWritableLedgerDirException extends IOException {
        private static final long serialVersionUID = -8696901285061448421L;

        public NoWritableLedgerDirException(String errMsg) {
            super(errMsg);
        }
    }

    /**
     * Listener for the disk check events will be notified from the
     * {@link LedgerDirsManager} whenever disk full/failure detected.
     */
    public static interface LedgerDirsListener {
        /**
         * This will be notified on disk failure/disk error
         * 
         * @param disk
         *            Failed disk
         */
        void diskFailed(File disk);

        /**
         * This will be notified on disk detected as full
         * 
         * @param disk
         *            Filled disk
         */
        void diskFull(File disk);

        /**
         * This will be notified whenever all disks are detected as full.
         */
        void allDisksFull();

        /**
         * This will notify the fatal errors.
         */
        void fatalError();
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/LedgerEntryPage.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

import java.io.IOException;
import java.nio.ByteBuffer;

import org.apache.bookkeeper.proto.BookieProtocol;

/**
 * This is a page in the LedgerCache. It holds the locations
 * (entrylogfile, offset) for entry ids.
 */
public class LedgerEntryPage {
    private final int pageSize;
    private final int entriesPerPage;
    private long ledger = -1;
    private long firstEntry = BookieProtocol.INVALID_ENTRY_ID;
    private final ByteBuffer page;
    private boolean clean = true;
    private boolean pinned = false;
    private int useCount;
    private int version;

    public LedgerEntryPage(int pageSize, int entriesPerPage) {
        this.pageSize = pageSize;
        this.entriesPerPage = entriesPerPage;
        page = ByteBuffer.allocateDirect(pageSize);
    }

    @Override
    public String toString() {
        StringBuilder sb = new StringBuilder();
        sb.append(getLedger());
        sb.append('@');
        sb.append(getFirstEntry());
        sb.append(clean ? " clean " : " dirty ");
        sb.append(useCount);
        return sb.toString();
    }
    synchronized public void usePage() {
        useCount++;
    }
    synchronized public void pin() {
        pinned = true;
    }
    synchronized public void unpin() {
        pinned = false;
    }
    synchronized public boolean isPinned() {
        return pinned;
    }
    synchronized public void releasePage() {
        useCount--;
        if (useCount < 0) {
            throw new IllegalStateException("Use count has gone below 0");
        }
    }
    synchronized private void checkPage() {
        if (useCount <= 0) {
            throw new IllegalStateException("Page not marked in use");
        }
    }
    @Override
    public boolean equals(Object other) {
        if (other instanceof LedgerEntryPage) {
            LedgerEntryPage otherLEP = (LedgerEntryPage) other;
            return otherLEP.getLedger() == getLedger() && otherLEP.getFirstEntry() == getFirstEntry();
        } else {
            return false;
        }
    }
    @Override
    public int hashCode() {
        return (int)getLedger() ^ (int)(getFirstEntry());
    }
    void setClean(int versionOfCleaning) {
        this.clean = (versionOfCleaning == version);
    }
    boolean isClean() {
        return clean;
    }
    public void setOffset(long offset, int position) {
        checkPage();
        version++;
        this.clean = false;
        page.putLong(position, offset);
    }
    public long getOffset(int position) {
        checkPage();
        return page.getLong(position);
    }
    static final byte zeroPage[] = new byte[64*1024];
    public void zeroPage() {
        checkPage();
        page.clear();
        page.put(zeroPage, 0, page.remaining());
        clean = true;
    }
    public void readPage(FileInfo fi) throws IOException {
        checkPage();
        page.clear();
        while(page.remaining() != 0) {
            if (fi.read(page, getFirstEntry()*8) <= 0) {
                throw new IOException("Short page read of ledger " + getLedger() + " tried to get " + page.capacity() + " from position " + getFirstEntry()*8 + " still need " + page.remaining());
            }
        }
        clean = true;
    }
    public ByteBuffer getPageToWrite() {
        checkPage();
        page.clear();
        return page;
    }
    void setLedger(long ledger) {
        this.ledger = ledger;
    }
    long getLedger() {
        return ledger;
    }
    int getVersion() {
        return version;
    }
    void setFirstEntry(long firstEntry) {
        if (firstEntry % entriesPerPage != 0) {
            throw new IllegalArgumentException(firstEntry + " is not a multiple of " + entriesPerPage);
        }
        this.firstEntry = firstEntry;
    }
    long getFirstEntry() {
        return firstEntry;
    }
    public boolean inUse() {
        return useCount > 0;
    }
    public long getLastEntry() {
        for(int i = entriesPerPage - 1; i >= 0; i--) {
            if (getOffset(i*8) > 0) {
                return i + firstEntry;
            }
        }
        return 0;
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/LedgerStorage.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

import java.io.IOException;
import java.nio.ByteBuffer;

import org.apache.bookkeeper.jmx.BKMBeanInfo;

/**
 * Interface for storing ledger data
 * on persistant storage.
 */
interface LedgerStorage {
    /**
     * Start any background threads
     * belonging to the storage system. For example,
     * garbage collection.
     */
    void start();

    /**
     * Cleanup and free any resources
     * being used by the storage system.
     */
    void shutdown() throws InterruptedException;

    /**
     * Whether a ledger exists
     */
    boolean ledgerExists(long ledgerId) throws IOException;

    /**
     * Fenced the ledger id in ledger storage.
     *
     * @param ledgerId
     *          Ledger Id.
     * @throws IOException when failed to fence the ledger.
     */
    boolean setFenced(long ledgerId) throws IOException;

    /**
     * Check whether the ledger is fenced in ledger storage or not.
     *
     * @param ledgerId
     *          Ledger ID.
     * @throws IOException
     */
    boolean isFenced(long ledgerId) throws IOException;

    /**
     * Set the master key for a ledger
     */
    void setMasterKey(long ledgerId, byte[] masterKey) throws IOException;

    /**
     * Get the master key for a ledger
     * @throws IOException if there is an error reading the from the ledger
     * @throws BookieException if no such ledger exists
     */
    byte[] readMasterKey(long ledgerId) throws IOException, BookieException;

    /**
     * Add an entry to the storage.
     * @return the entry id of the entry added
     */
    long addEntry(ByteBuffer entry) throws IOException;

    /**
     * Read an entry from storage
     */
    ByteBuffer getEntry(long ledgerId, long entryId) throws IOException;

    /**
     * Whether there is data in the storage which needs to be flushed
     */
    boolean isFlushRequired();

    /**
     * Flushes all data in the storage. Once this is called,
     * add data written to the LedgerStorage up until this point
     * has been persisted to perminant storage
     */
    void flush() throws IOException;

    /**
     * Get the JMX management bean for this LedgerStorage
     */
    BKMBeanInfo getJMXBean();
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/MarkerFileChannel.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

import java.io.IOException;
import java.nio.ByteBuffer;
import java.nio.MappedByteBuffer;
import java.nio.channels.FileChannel;
import java.nio.channels.FileLock;
import java.nio.channels.ReadableByteChannel;
import java.nio.channels.WritableByteChannel;

/**
 * This class is just a stub that can be used in collections with
 * FileChannels
 */
public class MarkerFileChannel extends FileChannel {

    @Override
    public void force(boolean metaData) throws IOException {
        // TODO Auto-generated method stub

    }

    @Override
    public FileLock lock(long position, long size, boolean shared)
            throws IOException {
        // TODO Auto-generated method stub
        return null;
    }

    @Override
    public MappedByteBuffer map(MapMode mode, long position, long size)
            throws IOException {
        // TODO Auto-generated method stub
        return null;
    }

    @Override
    public long position() throws IOException {
        // TODO Auto-generated method stub
        return 0;
    }

    @Override
    public FileChannel position(long newPosition) throws IOException {
        // TODO Auto-generated method stub
        return null;
    }

    @Override
    public int read(ByteBuffer dst) throws IOException {
        // TODO Auto-generated method stub
        return 0;
    }

    @Override
    public int read(ByteBuffer dst, long position) throws IOException {
        // TODO Auto-generated method stub
        return 0;
    }

    @Override
    public long read(ByteBuffer[] dsts, int offset, int length)
            throws IOException {
        // TODO Auto-generated method stub
        return 0;
    }

    @Override
    public long size() throws IOException {
        // TODO Auto-generated method stub
        return 0;
    }

    @Override
    public long transferFrom(ReadableByteChannel src, long position, long count)
            throws IOException {
        // TODO Auto-generated method stub
        return 0;
    }

    @Override
    public long transferTo(long position, long count, WritableByteChannel target)
            throws IOException {
        // TODO Auto-generated method stub
        return 0;
    }

    @Override
    public FileChannel truncate(long size) throws IOException {
        // TODO Auto-generated method stub
        return null;
    }

    @Override
    public FileLock tryLock(long position, long size, boolean shared)
            throws IOException {
        // TODO Auto-generated method stub
        return null;
    }

    @Override
    public int write(ByteBuffer src) throws IOException {
        // TODO Auto-generated method stub
        return 0;
    }

    @Override
    public int write(ByteBuffer src, long position) throws IOException {
        // TODO Auto-generated method stub
        return 0;
    }

    @Override
    public long write(ByteBuffer[] srcs, int offset, int length)
            throws IOException {
        // TODO Auto-generated method stub
        return 0;
    }

    @Override
    protected void implCloseChannel() throws IOException {
        // TODO Auto-generated method stub

    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/ReadOnlyEntryLogger.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

import java.io.IOException;
import java.nio.ByteBuffer;

import org.apache.bookkeeper.conf.ServerConfiguration;

/**
 * Read Only Entry Logger
 */
public class ReadOnlyEntryLogger extends EntryLogger {

    public ReadOnlyEntryLogger(ServerConfiguration conf) throws IOException {
        super(conf, new LedgerDirsManager(conf));
    }

    @Override
    protected void initialize() throws IOException {
        // do nothing for read only entry logger
    }

    @Override
    void createNewLog() throws IOException {
        throw new IOException("Can't create new entry log using a readonly entry logger.");
    }

    @Override
    protected boolean removeEntryLog(long entryLogId) {
        // can't remove entry log in readonly mode
        return false;
    }

    @Override
    synchronized long addEntry(long ledger, ByteBuffer entry) throws IOException {
        throw new IOException("Can't add entry to a readonly entry logger.");
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/ReadOnlyFileInfo.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

import java.io.File;
import java.io.IOException;
import java.io.RandomAccessFile;
import java.nio.ByteBuffer;
import java.nio.BufferUnderflowException;
import java.nio.channels.FileChannel;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Provide a readonly file info.
 */
class ReadOnlyFileInfo extends FileInfo {

    public ReadOnlyFileInfo(File lf, byte[] masterKey) throws IOException {
        super(lf, masterKey);
        mode = "r";
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/bookie/ScanAndCompareGarbageCollector.java,true,"/**
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.bookie;

import java.util.Map;
import java.util.NavigableMap;
import java.util.Set;

import org.apache.bookkeeper.meta.LedgerManager;
import org.apache.bookkeeper.meta.LedgerManager.LedgerRange;
import org.apache.bookkeeper.meta.LedgerManager.LedgerRangeIterator;
import org.apache.bookkeeper.util.SnapshotMap;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Garbage collector implementation using scan and compare.
 *
 * <p>
 * Garbage collection is processed as below:
 * <ul>
 * <li> fetch all existing ledgers from zookeeper or metastore according to
 * the LedgerManager, called <b>globalActiveLedgers</b>
 * <li> fetch all active ledgers from bookie server, said <b>bkActiveLedgers</b>
 * <li> loop over <b>bkActiveLedgers</b> to find those ledgers that are not in
 * <b>globalActiveLedgers</b>, do garbage collection on them.
 * </ul>
 * </p>
 */
public class ScanAndCompareGarbageCollector implements GarbageCollector{

    static final Logger LOG = LoggerFactory.getLogger(ScanAndCompareGarbageCollector.class);
    private SnapshotMap<Long, Boolean> activeLedgers;
    private LedgerManager ledgerManager;

    public ScanAndCompareGarbageCollector(LedgerManager ledgerManager, SnapshotMap<Long, Boolean> activeLedgers) {
        this.ledgerManager = ledgerManager;
        this.activeLedgers = activeLedgers;
    }

    @Override
    public void gc(GarbageCleaner garbageCleaner) {
        // create a snapshot first
        NavigableMap<Long, Boolean> bkActiveLedgersSnapshot =
                this.activeLedgers.snapshot();
        LedgerRangeIterator ledgerRangeIterator = ledgerManager.getLedgerRanges();
        try {
            // Empty global active ledgers, need to remove all local active ledgers.
            if (!ledgerRangeIterator.hasNext()) {
                for (Long bkLid : bkActiveLedgersSnapshot.keySet()) {
                    // remove it from current active ledger
                    bkActiveLedgersSnapshot.remove(bkLid);
                    garbageCleaner.clean(bkLid);
                }
            }
            while(ledgerRangeIterator.hasNext()) {
                LedgerRange lRange = ledgerRangeIterator.next();
                Map<Long, Boolean> subBkActiveLedgers = null;
                Long start = lRange.start();
                Long end = lRange.end();
                if (end != LedgerRange.NOLIMIT) {
                    subBkActiveLedgers = bkActiveLedgersSnapshot.subMap(start,
                            true, end, true);
                } else {
                    if (start != LedgerRange.NOLIMIT) {
                        subBkActiveLedgers = bkActiveLedgersSnapshot.tailMap(start);
                    } else {
                        subBkActiveLedgers = bkActiveLedgersSnapshot;
                    }
                }
                Set<Long> globalActiveLedgers = lRange.getLedgers();
                LOG.debug("All active ledgers for hash node {}, Current active ledgers from Bookie for hash node {}",
                        globalActiveLedgers, subBkActiveLedgers.keySet());
                for (Long bkLid : subBkActiveLedgers.keySet()) {
                    if (!globalActiveLedgers.contains(bkLid)) {
                        // remove it from current active ledger
                        subBkActiveLedgers.remove(bkLid);
                        garbageCleaner.clean(bkLid);
                    }
                }
            }
        } catch (Exception e) {
            // ignore exception, collecting garbage next time
            LOG.warn("Exception when iterating over the metadata {}", e);
        }
    }
}


"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/AsyncCallback.java,false,"package org.apache.bookkeeper.client;

import java.util.Enumeration;

/**
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements. See the NOTICE file distributed with this
 * work for additional information regarding copyright ownership. The ASF
 * licenses this file to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 * License for the specific language governing permissions and limitations under
 * the License.
 */

public interface AsyncCallback {
    public interface AddCallback {
        /**
         * Callback declaration
         *
         * @param rc
         *          return code
         * @param lh
         *          ledger handle
         * @param entryId
         *          entry identifier
         * @param ctx
         *          context object
         */
        void addComplete(int rc, LedgerHandle lh, long entryId, Object ctx);
    }

    public interface CloseCallback {
        /**
         * Callback definition
         *
         * @param rc
         *          return code
         * @param lh
         *          ledger handle
         * @param ctx
         *          context object
         */
        void closeComplete(int rc, LedgerHandle lh, Object ctx);
    }

    public interface CreateCallback {
        /**
         * Declaration of callback method
         *
         * @param rc
         *          return status
         * @param lh
         *          ledger handle
         * @param ctx
         *          context object
         */

        void createComplete(int rc, LedgerHandle lh, Object ctx);
    }

    public interface OpenCallback {
        /**
         * Callback for asynchronous call to open ledger
         *
         * @param rc
         *          Return code
         * @param lh
         *          ledger handle
         * @param ctx
         *          context object
         */

        public void openComplete(int rc, LedgerHandle lh, Object ctx);

    }

    public interface ReadCallback {
        /**
         * Callback declaration
         *
         * @param rc
         *          return code
         * @param lh
         *          ledger handle
         * @param seq
         *          sequence of entries
         * @param ctx
         *          context object
         */

        void readComplete(int rc, LedgerHandle lh, Enumeration<LedgerEntry> seq,
                          Object ctx);
    }

    public interface DeleteCallback {
        /**
         * Callback definition for delete operations
         *
         * @param rc
         *          return code
         * @param ctx
         *          context object
         */
        void deleteComplete(int rc, Object ctx);
    }

    public interface ReadLastConfirmedCallback {
        /**
         * Callback definition for bookie recover operations
         *
         * @param rc Return code
         * @param lastConfirmed The entry id of the last confirmed write or
         *                      {@link LedgerHandle#INVALID_ENTRY_ID INVALID_ENTRY_ID}
         *                      if no entry has been confirmed
         * @param ctx
         *          context object
         */
        void readLastConfirmedComplete(int rc, long lastConfirmed, Object ctx);
    }

    public interface RecoverCallback {
        /**
         * Callback definition for bookie recover operations
         *
         * @param rc
         *          return code
         * @param ctx
         *          context object
         */
        void recoverComplete(int rc, Object ctx);
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/BKException.java,true,"package org.apache.bookkeeper.client;

/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

import java.lang.Exception;

/**
 * Class the enumerates all the possible error conditions
 *
 */

@SuppressWarnings("serial")
public abstract class BKException extends Exception {

    private int code;

    BKException(int code) {
        this.code = code;
    }

    /**
     * Create an exception from an error code
     * @param code return error code
     * @return correponding exception
     */
    public static BKException create(int code) {
        switch (code) {
        case Code.ReadException:
            return new BKReadException();
        case Code.QuorumException:
            return new BKQuorumException();
        case Code.NoBookieAvailableException:
            return new BKBookieException();
        case Code.DigestNotInitializedException:
            return new BKDigestNotInitializedException();
        case Code.DigestMatchException:
            return new BKDigestMatchException();
        case Code.NotEnoughBookiesException:
            return new BKNotEnoughBookiesException();
        case Code.NoSuchLedgerExistsException:
            return new BKNoSuchLedgerExistsException();
        case Code.BookieHandleNotAvailableException:
            return new BKBookieHandleNotAvailableException();
        case Code.ZKException:
            return new ZKException();
        case Code.MetaStoreException:
            return new MetaStoreException();
        case Code.LedgerRecoveryException:
            return new BKLedgerRecoveryException();
        case Code.LedgerClosedException:
            return new BKLedgerClosedException();
        case Code.WriteException:
            return new BKWriteException();
        case Code.NoSuchEntryException:
            return new BKNoSuchEntryException();
        case Code.IncorrectParameterException:
            return new BKIncorrectParameterException();
        case Code.InterruptedException:
            return new BKInterruptedException();
        case Code.ProtocolVersionException:
            return new BKProtocolVersionException();
        case Code.MetadataVersionException:
            return new BKMetadataVersionException();
        case Code.LedgerFencedException:
            return new BKLedgerFencedException();
        case Code.UnauthorizedAccessException:
            return new BKUnauthorizedAccessException();
        case Code.UnclosedFragmentException:
            return new BKUnclosedFragmentException();
        case Code.WriteOnReadOnlyBookieException:
            return new BKWriteOnReadOnlyBookieException();
        default:
            return new BKIllegalOpException();
        }
    }

    /**
     * List of return codes
     *
     */
    public interface Code {
        int OK = 0;
        int ReadException = -1;
        int QuorumException = -2;
        int NoBookieAvailableException = -3;
        int DigestNotInitializedException = -4;
        int DigestMatchException = -5;
        int NotEnoughBookiesException = -6;
        int NoSuchLedgerExistsException = -7;
        int BookieHandleNotAvailableException = -8;
        int ZKException = -9;
        int LedgerRecoveryException = -10;
        int LedgerClosedException = -11;
        int WriteException = -12;
        int NoSuchEntryException = -13;
        int IncorrectParameterException = -14;
        int InterruptedException = -15;
        int ProtocolVersionException = -16;
        int MetadataVersionException = -17;
        int MetaStoreException = -18;

        int IllegalOpException = -100;
        int LedgerFencedException = -101;
        int UnauthorizedAccessException = -102;
        int UnclosedFragmentException = -103;
        int WriteOnReadOnlyBookieException = -104;
    }

    public void setCode(int code) {
        this.code = code;
    }

    public int getCode() {
        return this.code;
    }

    public static String getMessage(int code) {
        switch (code) {
        case Code.OK:
            return "No problem";
        case Code.ReadException:
            return "Error while reading ledger";
        case Code.QuorumException:
            return "Invalid quorum size on ensemble size";
        case Code.NoBookieAvailableException:
            return "Invalid quorum size on ensemble size";
        case Code.DigestNotInitializedException:
            return "Digest engine not initialized";
        case Code.DigestMatchException:
            return "Entry digest does not match";
        case Code.NotEnoughBookiesException:
            return "Not enough non-faulty bookies available";
        case Code.NoSuchLedgerExistsException:
            return "No such ledger exists";
        case Code.BookieHandleNotAvailableException:
            return "Bookie handle is not available";
        case Code.ZKException:
            return "Error while using ZooKeeper";
        case Code.MetaStoreException:
            return "Error while using MetaStore";
        case Code.LedgerRecoveryException:
            return "Error while recovering ledger";
        case Code.LedgerClosedException:
            return "Attempt to write to a closed ledger";
        case Code.WriteException:
            return "Write failed on bookie";
        case Code.NoSuchEntryException:
            return "No such entry";
        case Code.IncorrectParameterException:
            return "Incorrect parameter input";
        case Code.InterruptedException:
            return "Interrupted while waiting for permit";
        case Code.ProtocolVersionException:
            return "Bookie protocol version on server is incompatible with client";
        case Code.MetadataVersionException:
            return "Bad ledger metadata version";
        case Code.LedgerFencedException:
            return "Ledger has been fenced off. Some other client must have opened it to read";
        case Code.UnauthorizedAccessException:
            return "Attempted to access ledger using the wrong password";
        case Code.UnclosedFragmentException:
            return "Attempting to use an unclosed fragment; This is not safe";
        case Code.WriteOnReadOnlyBookieException:
            return "Attempting to write on ReadOnly bookie";
        default:
            return "Invalid operation";
        }
    }

    public static class BKReadException extends BKException {
        public BKReadException() {
            super(Code.ReadException);
        }
    }

    public static class BKNoSuchEntryException extends BKException {
        public BKNoSuchEntryException() {
            super(Code.NoSuchEntryException);
        }
    }

    public static class BKQuorumException extends BKException {
        public BKQuorumException() {
            super(Code.QuorumException);
        }
    }

    public static class BKBookieException extends BKException {
        public BKBookieException() {
            super(Code.NoBookieAvailableException);
        }
    }

    public static class BKDigestNotInitializedException extends BKException {
        public BKDigestNotInitializedException() {
            super(Code.DigestNotInitializedException);
        }
    }

    public static class BKDigestMatchException extends BKException {
        public BKDigestMatchException() {
            super(Code.DigestMatchException);
        }
    }

    public static class BKIllegalOpException extends BKException {
        public BKIllegalOpException() {
            super(Code.IllegalOpException);
        }
    }

    public static class BKNotEnoughBookiesException extends BKException {
        public BKNotEnoughBookiesException() {
            super(Code.NotEnoughBookiesException);
        }
    }

    public static class BKWriteException extends BKException {
        public BKWriteException() {
            super(Code.WriteException);
        }
    }

    public static class BKProtocolVersionException extends BKException {
        public BKProtocolVersionException() {
            super(Code.ProtocolVersionException);
        }
    }

    public static class BKMetadataVersionException extends BKException {
        public BKMetadataVersionException() {
            super(Code.MetadataVersionException);
        }
    }

    public static class BKNoSuchLedgerExistsException extends BKException {
        public BKNoSuchLedgerExistsException() {
            super(Code.NoSuchLedgerExistsException);
        }
    }

    public static class BKBookieHandleNotAvailableException extends BKException {
        public BKBookieHandleNotAvailableException() {
            super(Code.BookieHandleNotAvailableException);
        }
    }

    public static class ZKException extends BKException {
        public ZKException() {
            super(Code.ZKException);
        }
    }

    public static class MetaStoreException extends BKException {
        public MetaStoreException() {
            super(Code.MetaStoreException);
        }
    }

    public static class BKLedgerRecoveryException extends BKException {
        public BKLedgerRecoveryException() {
            super(Code.LedgerRecoveryException);
        }
    }

    public static class BKLedgerClosedException extends BKException {
        public BKLedgerClosedException() {
            super(Code.LedgerClosedException);
        }
    }

    public static class BKIncorrectParameterException extends BKException {
        public BKIncorrectParameterException() {
            super(Code.IncorrectParameterException);
        }
    }

    public static class BKInterruptedException extends BKException {
        public BKInterruptedException() {
            super(Code.InterruptedException);
        }
    }

    public static class BKLedgerFencedException extends BKException {
        public BKLedgerFencedException() {
            super(Code.LedgerFencedException);
        }
    }

    public static class BKUnauthorizedAccessException extends BKException {
        public BKUnauthorizedAccessException() {
            super(Code.UnauthorizedAccessException);
        }
    }

    public static class BKUnclosedFragmentException extends BKException {
        public BKUnclosedFragmentException() {
            super(Code.UnclosedFragmentException);
        }
    }

    public static class BKWriteOnReadOnlyBookieException extends BKException {
        public BKWriteOnReadOnlyBookieException() {
            super(Code.WriteOnReadOnlyBookieException);
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/BookieWatcher.java,false,"package org.apache.bookkeeper.client;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.IOException;
import java.net.InetSocketAddress;
import java.util.ArrayList;
import java.util.Collections;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;

import org.apache.bookkeeper.client.BKException.BKNotEnoughBookiesException;
import org.apache.bookkeeper.conf.ClientConfiguration;
import org.apache.bookkeeper.util.BookKeeperConstants;
import org.apache.bookkeeper.util.SafeRunnable;
import org.apache.bookkeeper.util.StringUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.WatchedEvent;
import org.apache.zookeeper.Watcher;
import org.apache.zookeeper.AsyncCallback.ChildrenCallback;
import org.apache.zookeeper.KeeperException.Code;
import org.apache.zookeeper.KeeperException.NodeExistsException;
import org.apache.zookeeper.ZooDefs.Ids;

/**
 * This class is responsible for maintaining a consistent view of what bookies
 * are available by reading Zookeeper (and setting watches on the bookie nodes).
 * When a bookie fails, the other parts of the code turn to this class to find a
 * replacement
 *
 */
class BookieWatcher implements Watcher, ChildrenCallback {
    static final Logger logger = LoggerFactory.getLogger(BookieWatcher.class);

    // Bookie registration path in ZK
    private final String bookieRegistrationPath;
    static final Set<InetSocketAddress> EMPTY_SET = new HashSet<InetSocketAddress>();
    public static int ZK_CONNECT_BACKOFF_SEC = 1;

    final BookKeeper bk;

    HashSet<InetSocketAddress> knownBookies = new HashSet<InetSocketAddress>();
    final ScheduledExecutorService scheduler;

    SafeRunnable reReadTask = new SafeRunnable() {
        @Override
        public void safeRun() {
            readBookies();
        }
    };
    private ReadOnlyBookieWatcher readOnlyBookieWatcher;

    public BookieWatcher(ClientConfiguration conf,
                         ScheduledExecutorService scheduler,
                         BookKeeper bk) throws KeeperException, InterruptedException  {
        this.bk = bk;
        // ZK bookie registration path
        this.bookieRegistrationPath = conf.getZkAvailableBookiesPath();
        this.scheduler = scheduler;
        readOnlyBookieWatcher = new ReadOnlyBookieWatcher(conf, bk);
    }

    public void readBookies() {
        readBookies(this);
    }

    public void readBookies(ChildrenCallback callback) {
        bk.getZkHandle().getChildren(this.bookieRegistrationPath, this, callback, null);
    }

    @Override
    public void process(WatchedEvent event) {
        readBookies();
    }

    @Override
    public void processResult(int rc, String path, Object ctx, List<String> children) {

        if (rc != KeeperException.Code.OK.intValue()) {
            //logger.error("Error while reading bookies", KeeperException.create(Code.get(rc), path));
            // try the read after a second again
            scheduler.schedule(reReadTask, ZK_CONNECT_BACKOFF_SEC, TimeUnit.SECONDS);
            return;
        }

        // Just exclude the 'readonly' znode to exclude r-o bookies from
        // available nodes list.
        children.remove(BookKeeperConstants.READONLY);

        HashSet<InetSocketAddress> newBookieAddrs = convertToBookieAddresses(children);

        final HashSet<InetSocketAddress> deadBookies;
        synchronized (this) {
            deadBookies = (HashSet<InetSocketAddress>)knownBookies.clone();
            deadBookies.removeAll(newBookieAddrs);
            // No need to close readonly bookie clients.
            deadBookies.removeAll(readOnlyBookieWatcher.getReadOnlyBookies());
            knownBookies = newBookieAddrs;
        }

        if (bk.getBookieClient() != null) {
            bk.getBookieClient().closeClients(deadBookies);
        }
    }

    private static HashSet<InetSocketAddress> convertToBookieAddresses(List<String> children) {
        // Read the bookie addresses into a set for efficient lookup
        HashSet<InetSocketAddress> newBookieAddrs = new HashSet<InetSocketAddress>();
        for (String bookieAddrString : children) {
            InetSocketAddress bookieAddr;
            try {
                bookieAddr = StringUtils.parseAddr(bookieAddrString);
            } catch (IOException e) {
                logger.error("Could not parse bookie address: " + bookieAddrString + ", ignoring this bookie");
                continue;
            }
            newBookieAddrs.add(bookieAddr);
        }
        return newBookieAddrs;
    }

    /**
     * Blocks until bookies are read from zookeeper, used in the {@link BookKeeper} constructor.
     * @throws InterruptedException
     * @throws KeeperException
     */
    public void readBookiesBlocking() throws InterruptedException, KeeperException {
        // Read readonly bookies first
        readOnlyBookieWatcher.readROBookiesBlocking();

        final LinkedBlockingQueue<Integer> queue = new LinkedBlockingQueue<Integer>();
        readBookies(new ChildrenCallback() {
            public void processResult(int rc, String path, Object ctx, List<String> children) {
                try {
                    BookieWatcher.this.processResult(rc, path, ctx, children);
                    queue.put(rc);
                } catch (InterruptedException e) {
                    logger.error("Interruped when trying to read bookies in a blocking fashion");
                    throw new RuntimeException(e);
                }
            }
        });
        int rc = queue.take();

        if (rc != KeeperException.Code.OK.intValue()) {
            throw KeeperException.create(Code.get(rc));
        }
    }

    /**
     * Wrapper over the {@link #getAdditionalBookies(Set, int)} method when there is no exclusion list (or exisiting bookies)
     * @param numBookiesNeeded
     * @return
     * @throws BKNotEnoughBookiesException
     */
    public ArrayList<InetSocketAddress> getNewBookies(int numBookiesNeeded) throws BKNotEnoughBookiesException {
        return getAdditionalBookies(EMPTY_SET, numBookiesNeeded);
    }

    /**
     * Wrapper over the {@link #getAdditionalBookies(Set, int)} method when you just need 1 extra bookie
     * @param existingBookies
     * @return
     * @throws BKNotEnoughBookiesException
     */
    public InetSocketAddress getAdditionalBookie(List<InetSocketAddress> existingBookies)
            throws BKNotEnoughBookiesException {
        return getAdditionalBookies(new HashSet<InetSocketAddress>(existingBookies), 1).get(0);
    }

    /**
     * Returns additional bookies given an exclusion list and how many are needed
     * @param existingBookies
     * @param numAdditionalBookiesNeeded
     * @return
     * @throws BKNotEnoughBookiesException
     */
    public ArrayList<InetSocketAddress> getAdditionalBookies(Set<InetSocketAddress> existingBookies,
            int numAdditionalBookiesNeeded) throws BKNotEnoughBookiesException {

        ArrayList<InetSocketAddress> newBookies = new ArrayList<InetSocketAddress>();

        if (numAdditionalBookiesNeeded <= 0) {
            return newBookies;
        }

        List<InetSocketAddress> allBookies;

        synchronized (this) {
            allBookies = new ArrayList<InetSocketAddress>(knownBookies);
        }

        Collections.shuffle(allBookies);

        for (InetSocketAddress bookie : allBookies) {
            if (existingBookies.contains(bookie)) {
                continue;
            }

            newBookies.add(bookie);
            numAdditionalBookiesNeeded--;

            if (numAdditionalBookiesNeeded == 0) {
                return newBookies;
            }
        }

        throw new BKNotEnoughBookiesException();
    }

    /**
     * Watcher implementation to watch the readonly bookies under
     * &lt;available&gt;/readonly
     */
    private static class ReadOnlyBookieWatcher implements Watcher, ChildrenCallback {

        private final static Logger LOG = LoggerFactory.getLogger(ReadOnlyBookieWatcher.class);
        private HashSet<InetSocketAddress> readOnlyBookies = new HashSet<InetSocketAddress>();
        private BookKeeper bk;
        private String readOnlyBookieRegPath;

        public ReadOnlyBookieWatcher(ClientConfiguration conf, BookKeeper bk) throws KeeperException,
                InterruptedException {
            this.bk = bk;
            readOnlyBookieRegPath = conf.getZkAvailableBookiesPath() + "/"
                    + BookKeeperConstants.READONLY;
            if (null == bk.getZkHandle().exists(readOnlyBookieRegPath, false)) {
                try {
                    bk.getZkHandle().create(readOnlyBookieRegPath, new byte[0], Ids.OPEN_ACL_UNSAFE,
                            CreateMode.PERSISTENT);
                } catch (NodeExistsException e) {
                    // this node is just now created by someone.
                }
            }
        }

        @Override
        public void process(WatchedEvent event) {
            readROBookies();
        }

        // read the readonly bookies in blocking fashion. Used only for first
        // time.
        void readROBookiesBlocking() throws InterruptedException, KeeperException {

            final LinkedBlockingQueue<Integer> queue = new LinkedBlockingQueue<Integer>();
            readROBookies(new ChildrenCallback() {
                public void processResult(int rc, String path, Object ctx, List<String> children) {
                    try {
                        ReadOnlyBookieWatcher.this.processResult(rc, path, ctx, children);
                        queue.put(rc);
                    } catch (InterruptedException e) {
                        logger.error("Interruped when trying to read readonly bookies in a blocking fashion");
                        throw new RuntimeException(e);
                    }
                }
            });
            int rc = queue.take();

            if (rc != KeeperException.Code.OK.intValue()) {
                throw KeeperException.create(Code.get(rc));
            }
        }

        // Read children and register watcher for readonly bookies path
        void readROBookies(ChildrenCallback callback) {
            bk.getZkHandle().getChildren(this.readOnlyBookieRegPath, this, callback, null);
        }

        void readROBookies() {
            readROBookies(this);
        }

        @Override
        public void processResult(int rc, String path, Object ctx, List<String> children) {
            if (rc != Code.OK.intValue()) {
                LOG.error("Not able to read readonly bookies : ", KeeperException.create(Code.get(rc)));
                return;
            }

            HashSet<InetSocketAddress> newReadOnlyBookies = convertToBookieAddresses(children);
            readOnlyBookies = newReadOnlyBookies;
        }

        // returns the readonly bookies
        public HashSet<InetSocketAddress> getReadOnlyBookies() {
            return readOnlyBookies;
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/BookKeeper.java,true,"/**
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */
package org.apache.bookkeeper.client;

import java.io.IOException;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;

import org.apache.bookkeeper.client.AsyncCallback.CreateCallback;
import org.apache.bookkeeper.client.AsyncCallback.DeleteCallback;
import org.apache.bookkeeper.client.AsyncCallback.OpenCallback;
import org.apache.bookkeeper.client.BKException.Code;
import org.apache.bookkeeper.conf.ClientConfiguration;
import org.apache.bookkeeper.meta.LedgerManager;
import org.apache.bookkeeper.meta.LedgerManagerFactory;
import org.apache.bookkeeper.proto.BookieClient;
import org.apache.bookkeeper.util.OrderedSafeExecutor;
import org.apache.bookkeeper.util.ZkUtils;
import org.apache.bookkeeper.zookeeper.ZooKeeperWatcherBase;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.ZooKeeper;
import org.jboss.netty.channel.socket.ClientSocketChannelFactory;
import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * BookKeeper client. We assume there is one single writer to a ledger at any
 * time.
 *
 * There are four possible operations: start a new ledger, write to a ledger,
 * read from a ledger and delete a ledger.
 *
 * The exceptions resulting from synchronous calls and error code resulting from
 * asynchronous calls can be found in the class {@link BKException}.
 *
 *
 */

public class BookKeeper {

    static final Logger LOG = LoggerFactory.getLogger(BookKeeper.class);

    final ZooKeeper zk;
    final CountDownLatch connectLatch = new CountDownLatch(1);
    final static int zkConnectTimeoutMs = 5000;
    final ClientSocketChannelFactory channelFactory;

    // whether the socket factory is one we created, or is owned by whoever
    // instantiated us
    boolean ownChannelFactory = false;
    // whether the zk handle is one we created, or is owned by whoever
    // instantiated us
    boolean ownZKHandle = false;

    final BookieClient bookieClient;
    final BookieWatcher bookieWatcher;

    final OrderedSafeExecutor mainWorkerPool;
    final ScheduledExecutorService scheduler;

    // Ledger manager responsible for how to store ledger meta data
    final LedgerManagerFactory ledgerManagerFactory;
    final LedgerManager ledgerManager;

    final ClientConfiguration conf;

    interface ZKConnectCallback {
        public void connected();
        public void connectionFailed(int code);
    }

    /**
     * Create a bookkeeper client. A zookeeper client and a client socket factory
     * will be instantiated as part of this constructor.
     *
     * @param servers
     *          A list of one of more servers on which zookeeper is running. The
     *          client assumes that the running bookies have been registered with
     *          zookeeper under the path
     *          {@link BookieWatcher#bookieRegistrationPath}
     * @throws IOException
     * @throws InterruptedException
     * @throws KeeperException
     */
    public BookKeeper(String servers) throws IOException, InterruptedException,
        KeeperException {
        this(new ClientConfiguration().setZkServers(servers));
    }

    /**
     * Create a bookkeeper client using a configuration object.
     * A zookeeper client and a client socket factory will be 
     * instantiated as part of this constructor.
     *
     * @param conf
     *          Client Configuration object
     * @throws IOException
     * @throws InterruptedException
     * @throws KeeperException
     */
    public BookKeeper(final ClientConfiguration conf)
            throws IOException, InterruptedException, KeeperException {
        this.conf = conf;
        ZooKeeperWatcherBase w = new ZooKeeperWatcherBase(conf.getZkTimeout());
        this.zk = ZkUtils
                .createConnectedZookeeperClient(conf.getZkServers(), w);

        this.channelFactory = new NioClientSocketChannelFactory(Executors.newCachedThreadPool(),
                                                                Executors.newCachedThreadPool());
        this.scheduler = Executors.newSingleThreadScheduledExecutor();

        mainWorkerPool = new OrderedSafeExecutor(conf.getNumWorkerThreads());
        bookieClient = new BookieClient(conf, channelFactory, mainWorkerPool);
        bookieWatcher = new BookieWatcher(conf, scheduler, this);
        bookieWatcher.readBookiesBlocking();

        ledgerManagerFactory = LedgerManagerFactory.newLedgerManagerFactory(conf, zk);
        ledgerManager = ledgerManagerFactory.newLedgerManager();

        ownChannelFactory = true;
        ownZKHandle = true;
     }

    /**
     * Create a bookkeeper client but use the passed in zookeeper client instead
     * of instantiating one.
     *
     * @param conf
     *          Client Configuration object
     *          {@link ClientConfiguration}
     * @param zk
     *          Zookeeper client instance connected to the zookeeper with which
     *          the bookies have registered
     * @throws IOException
     * @throws InterruptedException
     * @throws KeeperException
     */
    public BookKeeper(ClientConfiguration conf, ZooKeeper zk)
        throws IOException, InterruptedException, KeeperException {
        this(conf, zk, new NioClientSocketChannelFactory(Executors.newCachedThreadPool(),
                Executors.newCachedThreadPool()));
        ownChannelFactory = true;
    }

    /**
     * Create a bookkeeper client but use the passed in zookeeper client and
     * client socket channel factory instead of instantiating those.
     *
     * @param conf
     *          Client Configuration Object
     *          {@link ClientConfiguration}
     * @param zk
     *          Zookeeper client instance connected to the zookeeper with which
     *          the bookies have registered. The ZooKeeper client must be connected
     *          before it is passed to BookKeeper. Otherwise a KeeperException is thrown.
     * @param channelFactory
     *          A factory that will be used to create connections to the bookies
     * @throws IOException
     * @throws InterruptedException
     * @throws KeeperException if the passed zk handle is not connected
     */
    public BookKeeper(ClientConfiguration conf, ZooKeeper zk, ClientSocketChannelFactory channelFactory)
            throws IOException, InterruptedException, KeeperException {
        if (zk == null || channelFactory == null) {
            throw new NullPointerException();
        }
        if (!zk.getState().isConnected()) {
            LOG.error("Unconnected zookeeper handle passed to bookkeeper");
            throw KeeperException.create(KeeperException.Code.CONNECTIONLOSS);
        }
        this.conf = conf;
        this.zk = zk;
        this.channelFactory = channelFactory;
        this.scheduler = Executors.newSingleThreadScheduledExecutor();

        mainWorkerPool = new OrderedSafeExecutor(conf.getNumWorkerThreads());
        bookieClient = new BookieClient(conf, channelFactory, mainWorkerPool);
        bookieWatcher = new BookieWatcher(conf, scheduler, this);
        bookieWatcher.readBookiesBlocking();

        ledgerManagerFactory = LedgerManagerFactory.newLedgerManagerFactory(conf, zk);
        ledgerManager = ledgerManagerFactory.newLedgerManager();
    }

    LedgerManager getLedgerManager() {
        return ledgerManager;
    }

    /**
     * There are 2 digest types that can be used for verification. The CRC32 is
     * cheap to compute but does not protect against byzantine bookies (i.e., a
     * bookie might report fake bytes and a matching CRC32). The MAC code is more
     * expensive to compute, but is protected by a password, i.e., a bookie can't
     * report fake bytes with a mathching MAC unless it knows the password
     */
    public enum DigestType {
        MAC, CRC32
    };

    ZooKeeper getZkHandle() {
        return zk;
    }

    protected ClientConfiguration getConf() {
        return conf;
    }

    /**
     * Get the BookieClient, currently used for doing bookie recovery.
     *
     * @return BookieClient for the BookKeeper instance.
     */
    BookieClient getBookieClient() {
        return bookieClient;
    }

    /**
     * Creates a new ledger asynchronously. To create a ledger, we need to specify
     * the ensemble size, the quorum size, the digest type, a password, a callback
     * implementation, and an optional control object. The ensemble size is how
     * many bookies the entries should be striped among and the quorum size is the
     * degree of replication of each entry. The digest type is either a MAC or a
     * CRC. Note that the CRC option is not able to protect a client against a
     * bookie that replaces an entry. The password is used not only to
     * authenticate access to a ledger, but also to verify entries in ledgers.
     *
     * @param ensSize
     *          number of bookies over which to stripe entries
     * @param writeQuorumSize
     *          number of bookies each entry will be written to. each of these bookies
     *          must acknowledge the entry before the call is completed.
     * @param digestType
     *          digest type, either MAC or CRC32
     * @param passwd
     *          password
     * @param cb
     *          createCallback implementation
     * @param ctx
     *          optional control object
     */
    public void asyncCreateLedger(final int ensSize,
                                  final int writeQuorumSize,
                                  final DigestType digestType,
                                  final byte[] passwd, final CreateCallback cb, final Object ctx)
    {
        asyncCreateLedger(ensSize, writeQuorumSize, writeQuorumSize, digestType, passwd, cb, ctx);
    }

    /**
     * Creates a new ledger asynchronously. Ledgers created with this call have
     * a separate write quorum and ack quorum size. The write quorum must be larger than
     * the ack quorum.
     *
     * Separating the write and the ack quorum allows the BookKeeper client to continue
     * writing when a bookie has failed but the failure has not yet been detected. Detecting
     * a bookie has failed can take a number of seconds, as configured by the read timeout
     * {@link ClientConfiguration#getReadTimeout()}. Once the bookie failure is detected,
     * that bookie will be removed from the ensemble.
     *
     * The other parameters match those of {@link #asyncCreateLedger(int, int, DigestType, byte[],
     *                                      AsyncCallback.CreateCallback, Object)}
     *
     * @param ensSize
     *          number of bookies over which to stripe entries
     * @param writeQuorumSize
     *          number of bookies each entry will be written to
     * @param ackQuorumSize
     *          number of bookies which must acknowledge an entry before the call is completed
     * @param digestType
     *          digest type, either MAC or CRC32
     * @param passwd
     *          password
     * @param cb
     *          createCallback implementation
     * @param ctx
     *          optional control object
     */

    public void asyncCreateLedger(final int ensSize,
                                  final int writeQuorumSize,
                                  final int ackQuorumSize,
                                  final DigestType digestType,
                                  final byte[] passwd, final CreateCallback cb, final Object ctx) {
        if (writeQuorumSize < ackQuorumSize) {
            throw new IllegalArgumentException("Write quorum must be larger than ack quorum");
        }
        new LedgerCreateOp(BookKeeper.this, ensSize, writeQuorumSize,
                           ackQuorumSize, digestType, passwd, cb, ctx)
            .initiate();
    }


    /**
     * Creates a new ledger. Default of 3 servers, and quorum of 2 servers.
     *
     * @param digestType
     *          digest type, either MAC or CRC32
     * @param passwd
     *          password
     * @return a handle to the newly created ledger
     * @throws InterruptedException
     * @throws BKException
     */
    public LedgerHandle createLedger(DigestType digestType, byte passwd[])
            throws BKException, InterruptedException {
        return createLedger(3, 2, digestType, passwd);
    }

    /**
     * Synchronous call to create ledger. Parameters match those of
     * {@link #asyncCreateLedger(int, int, DigestType, byte[], 
     *                           AsyncCallback.CreateCallback, Object)}
     *
     * @param ensSize
     * @param qSize
     * @param digestType
     * @param passwd
     * @return a handle to the newly created ledger
     * @throws InterruptedException
     * @throws BKException
     */
    public LedgerHandle createLedger(int ensSize, int qSize,
                                     DigestType digestType, byte passwd[])
            throws InterruptedException, BKException {
        return createLedger(ensSize, qSize, qSize, digestType, passwd);
    }

    /**
     * Synchronous call to create ledger. Parameters match those of
     * {@link #asyncCreateLedger(int, int, int, DigestType, byte[],
     *                           AsyncCallback.CreateCallback, Object)}
     *
     * @param ensSize
     * @param writeQuorumSize
     * @param ackQuorumSize
     * @param digestType
     * @param passwd
     * @return a handle to the newly created ledger
     * @throws InterruptedException
     * @throws BKException
     */
    public LedgerHandle createLedger(int ensSize, int writeQuorumSize, int ackQuorumSize,
                                     DigestType digestType, byte passwd[])
            throws InterruptedException, BKException {
        SyncCounter counter = new SyncCounter();
        counter.inc();
        /*
         * Calls asynchronous version
         */
        asyncCreateLedger(ensSize, writeQuorumSize, ackQuorumSize, digestType, passwd,
                          new SyncCreateCallback(), counter);

        /*
         * Wait
         */
        counter.block(0);
        if (counter.getLh() == null) {
            LOG.error("ZooKeeper error: " + counter.getrc());
            throw BKException.create(Code.ZKException);
        }

        return counter.getLh();
    }

    /**
     * Open existing ledger asynchronously for reading.
     * 
     * Opening a ledger with this method invokes fencing and recovery on the ledger 
     * if the ledger has not been closed. Fencing will block all other clients from 
     * writing to the ledger. Recovery will make sure that the ledger is closed 
     * before reading from it. 
     *
     * Recovery also makes sure that any entries which reached one bookie, but not a 
     * quorum, will be replicated to a quorum of bookies. This occurs in cases were
     * the writer of a ledger crashes after sending a write request to one bookie but
     * before being able to send it to the rest of the bookies in the quorum. 
     *
     * If the ledger is already closed, neither fencing nor recovery will be applied.
     * 
     * @see LedgerHandle#asyncClose
     *
     * @param lId
     *          ledger identifier
     * @param digestType
     *          digest type, either MAC or CRC32
     * @param passwd
     *          password
     * @param ctx
     *          optional control object
     */
    public void asyncOpenLedger(final long lId, final DigestType digestType, final byte passwd[],
                                final OpenCallback cb, final Object ctx) {
        new LedgerOpenOp(BookKeeper.this, lId, digestType, passwd, cb, ctx).initiate();
    }

    /**
     * Open existing ledger asynchronously for reading, but it does not try to
     * recover the ledger if it is not yet closed. The application needs to use
     * it carefully, since the writer might have crashed and ledger will remain
     * unsealed forever if there is no external mechanism to detect the failure
     * of the writer and the ledger is not open in a safe manner, invoking the
     * recovery procedure.
     * 
     * Opening a ledger without recovery does not fence the ledger. As such, other
     * clients can continue to write to the ledger. 
     *
     * This method returns a read only ledger handle. It will not be possible 
     * to add entries to the ledger. Any attempt to add entries will throw an 
     * exception.
     * 
     * Reads from the returned ledger will only be able to read entries up until
     * the lastConfirmedEntry at the point in time at which the ledger was opened.
     *
     * @param lId
     *          ledger identifier
     * @param digestType
     *          digest type, either MAC or CRC32
     * @param passwd
     *          password
     * @param ctx
     *          optional control object
     */
    public void asyncOpenLedgerNoRecovery(final long lId, final DigestType digestType, final byte passwd[],
                                          final OpenCallback cb, final Object ctx) {
        new LedgerOpenOp(BookKeeper.this, lId, digestType, passwd, cb, ctx).initiateWithoutRecovery();
    }


    /**
     * Synchronous open ledger call
     *
     * @see #asyncOpenLedger
     * @param lId
     *          ledger identifier
     * @param digestType
     *          digest type, either MAC or CRC32
     * @param passwd
     *          password
     * @return a handle to the open ledger
     * @throws InterruptedException
     * @throws BKException
     */

    public LedgerHandle openLedger(long lId, DigestType digestType, byte passwd[])
            throws BKException, InterruptedException {
        SyncCounter counter = new SyncCounter();
        counter.inc();

        /*
         * Calls async open ledger
         */
        asyncOpenLedger(lId, digestType, passwd, new SyncOpenCallback(), counter);

        /*
         * Wait
         */
        counter.block(0);
        if (counter.getrc() != BKException.Code.OK)
            throw BKException.create(counter.getrc());

        return counter.getLh();
    }

    /**
     * Synchronous, unsafe open ledger call
     *
     * @see #asyncOpenLedgerNoRecovery
     * @param lId
     *          ledger identifier
     * @param digestType
     *          digest type, either MAC or CRC32
     * @param passwd
     *          password
     * @return a handle to the open ledger
     * @throws InterruptedException
     * @throws BKException
     */

    public LedgerHandle openLedgerNoRecovery(long lId, DigestType digestType, byte passwd[])
            throws BKException, InterruptedException {
        SyncCounter counter = new SyncCounter();
        counter.inc();

        /*
         * Calls async open ledger
         */
        asyncOpenLedgerNoRecovery(lId, digestType, passwd,
                                  new SyncOpenCallback(), counter);

        /*
         * Wait
         */
        counter.block(0);
        if (counter.getrc() != BKException.Code.OK)
            throw BKException.create(counter.getrc());

        return counter.getLh();
    }

    /**
     * Deletes a ledger asynchronously.
     *
     * @param lId
     *            ledger Id
     * @param cb
     *            deleteCallback implementation
     * @param ctx
     *            optional control object
     */
    public void asyncDeleteLedger(final long lId, final DeleteCallback cb, final Object ctx) {
        new LedgerDeleteOp(BookKeeper.this, lId, cb, ctx).initiate();
    }


    /**
     * Synchronous call to delete a ledger. Parameters match those of
     * {@link #asyncDeleteLedger(long, AsyncCallback.DeleteCallback, Object)}
     *
     * @param lId
     *            ledgerId
     * @throws InterruptedException
     * @throws BKException.BKNoSuchLedgerExistsException if the ledger doesn't exist
     * @throws BKException
     */
    public void deleteLedger(long lId) throws InterruptedException, BKException {
        SyncCounter counter = new SyncCounter();
        counter.inc();
        // Call asynchronous version
        asyncDeleteLedger(lId, new SyncDeleteCallback(), counter);
        // Wait
        counter.block(0);
        if (counter.getrc() != BKException.Code.OK) {
            LOG.error("Error deleting ledger " + lId + " : " + counter.getrc());
            throw BKException.create(counter.getrc());
        }
    }

    /**
     * Shuts down client.
     *
     */
    public void close() throws InterruptedException, BKException {
        scheduler.shutdown();
        if (!scheduler.awaitTermination(10, TimeUnit.SECONDS)) {
            LOG.warn("The scheduler did not shutdown cleanly");
        }
        mainWorkerPool.shutdown();
        if (!mainWorkerPool.awaitTermination(10, TimeUnit.SECONDS)) {
            LOG.warn("The mainWorkerPool did not shutdown cleanly");
        }

        bookieClient.close();
        try {
            ledgerManager.close();
            ledgerManagerFactory.uninitialize();
        } catch (IOException ie) {
            LOG.error("Failed to close ledger manager : ", ie);
        }

        if (ownChannelFactory) {
            channelFactory.releaseExternalResources();
        }
        if (ownZKHandle) {
            zk.close();
        }
    }

    private static class SyncCreateCallback implements CreateCallback {
        /**
         * Create callback implementation for synchronous create call.
         *
         * @param rc
         *          return code
         * @param lh
         *          ledger handle object
         * @param ctx
         *          optional control object
         */
        public void createComplete(int rc, LedgerHandle lh, Object ctx) {
            SyncCounter counter = (SyncCounter) ctx;
            counter.setLh(lh);
            counter.setrc(rc);
            counter.dec();
        }
    }

    static class SyncOpenCallback implements OpenCallback {
        /**
         * Callback method for synchronous open operation
         *
         * @param rc
         *          return code
         * @param lh
         *          ledger handle
         * @param ctx
         *          optional control object
         */
        public void openComplete(int rc, LedgerHandle lh, Object ctx) {
            SyncCounter counter = (SyncCounter) ctx;
            counter.setLh(lh);
            
            LOG.debug("Open complete: {}", rc);
            
            counter.setrc(rc);
            counter.dec();
        }
    }

    private static class SyncDeleteCallback implements DeleteCallback {
        /**
         * Delete callback implementation for synchronous delete call.
         *
         * @param rc
         *            return code
         * @param ctx
         *            optional control object
         */
        public void deleteComplete(int rc, Object ctx) {
            SyncCounter counter = (SyncCounter) ctx;
            counter.setrc(rc);
            counter.dec();
        }
    }



}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/BookKeeperAdmin.java,true,"/**
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */
package org.apache.bookkeeper.client;

import java.io.IOException;
import java.net.InetSocketAddress;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.Random;
import java.util.UUID;

import org.apache.bookkeeper.client.AsyncCallback.OpenCallback;
import org.apache.bookkeeper.client.AsyncCallback.RecoverCallback;
import org.apache.bookkeeper.client.BookKeeper.SyncOpenCallback;
import org.apache.bookkeeper.client.LedgerFragmentReplicator.SingleFragmentCallback;
import org.apache.bookkeeper.conf.ClientConfiguration;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.MultiCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.Processor;
import org.apache.bookkeeper.util.BookKeeperConstants;
import org.apache.bookkeeper.util.IOUtils;
import org.apache.bookkeeper.util.ZkUtils;
import org.apache.bookkeeper.zookeeper.ZooKeeperWatcherBase;
import org.apache.zookeeper.AsyncCallback;
import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.ZKUtil;
import org.apache.zookeeper.ZooKeeper;
import org.apache.zookeeper.KeeperException.Code;
import org.apache.zookeeper.ZooDefs.Ids;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Admin client for BookKeeper clusters
 */
public class BookKeeperAdmin {
    private static Logger LOG = LoggerFactory.getLogger(BookKeeperAdmin.class);
    // ZK client instance
    private ZooKeeper zk;
    // ZK ledgers related String constants
    private final String bookiesPath;

    // BookKeeper client instance
    private BookKeeper bkc;
    
    // LedgerFragmentReplicator instance
    private LedgerFragmentReplicator lfr;

    /*
     * Random number generator used to choose an available bookie server to
     * replicate data from a dead bookie.
     */
    private Random rand = new Random();

    /**
     * Constructor that takes in a ZooKeeper servers connect string so we know
     * how to connect to ZooKeeper to retrieve information about the BookKeeper
     * cluster. We need this before we can do any type of admin operations on
     * the BookKeeper cluster.
     *
     * @param zkServers
     *            Comma separated list of hostname:port pairs for the ZooKeeper
     *            servers cluster.
     * @throws IOException
     *             throws this exception if there is an error instantiating the
     *             ZooKeeper client.
     * @throws InterruptedException
     *             Throws this exception if there is an error instantiating the
     *             BookKeeper client.
     * @throws KeeperException
     *             Throws this exception if there is an error instantiating the
     *             BookKeeper client.
     */
    public BookKeeperAdmin(String zkServers) throws IOException, InterruptedException, KeeperException {
        this(new ClientConfiguration().setZkServers(zkServers));
    }

    /**
     * Constructor that takes in a configuration object so we know
     * how to connect to ZooKeeper to retrieve information about the BookKeeper
     * cluster. We need this before we can do any type of admin operations on
     * the BookKeeper cluster.
     *
     * @param conf
     *           Client Configuration Object
     * @throws IOException
     *             throws this exception if there is an error instantiating the
     *             ZooKeeper client.
     * @throws InterruptedException
     *             Throws this exception if there is an error instantiating the
     *             BookKeeper client.
     * @throws KeeperException
     *             Throws this exception if there is an error instantiating the
     *             BookKeeper client.
     */
    public BookKeeperAdmin(ClientConfiguration conf) throws IOException, InterruptedException, KeeperException {
        // Create the ZooKeeper client instance
        ZooKeeperWatcherBase w = new ZooKeeperWatcherBase(conf.getZkTimeout());
        zk = ZkUtils.createConnectedZookeeperClient(conf.getZkServers(), w);
        // Create the bookie path
        bookiesPath = conf.getZkAvailableBookiesPath();
        // Create the BookKeeper client instance
        bkc = new BookKeeper(conf, zk);
        this.lfr = new LedgerFragmentReplicator(bkc);
    }

    /**
     * Constructor that takes in a BookKeeper instance . This will be useful,
     * when users already has bk instance ready.
     * 
     * @param bkc
     *            - bookkeeper instance
     */
    public BookKeeperAdmin(final BookKeeper bkc) {
        this.bkc = bkc;
        this.zk = bkc.zk;
        this.bookiesPath = bkc.getConf().getZkAvailableBookiesPath();
        this.lfr = new LedgerFragmentReplicator(bkc);
    }

    /**
     * Gracefully release resources that this client uses.
     *
     * @throws InterruptedException
     *             if there is an error shutting down the clients that this
     *             class uses.
     */
    public void close() throws InterruptedException, BKException {
        bkc.close();
        zk.close();
    }

    /**
     * Open a ledger as an administrator. This means that no digest password
     * checks are done. Otherwise, the call is identical to BookKeeper#asyncOpenLedger
     *
     * @param lId
     *          ledger identifier
     * @param cb
     *          Callback which will receive a LedgerHandle object
     * @param ctx
     *          optional context object, to be passwd to the callback (can be null)
     *
     * @see BookKeeper#asyncOpenLedger
     */
    public void asyncOpenLedger(final long lId, final OpenCallback cb, final Object ctx) {
        new LedgerOpenOp(bkc, lId, cb, ctx).initiate();
    }
    
    /**
     * Open a ledger as an administrator. This means that no digest password
     * checks are done. Otherwise, the call is identical to
     * BookKeeper#openLedger
     * 
     * @param lId
     *            - ledger identifier
     * @see BookKeeper#openLedger
     */
    public LedgerHandle openLedger(final long lId) throws InterruptedException,
            BKException {
        SyncCounter counter = new SyncCounter();
        counter.inc();
        new LedgerOpenOp(bkc, lId, new SyncOpenCallback(), counter).initiate();
        /*
         * Wait
         */
        counter.block(0);
        if (counter.getrc() != BKException.Code.OK) {
            throw BKException.create(counter.getrc());
        }

        return counter.getLh();
    }

    /**
     * Open a ledger as an administrator without recovering the ledger. This means
     * that no digest password  checks are done. Otherwise, the call is identical
     * to BookKeeper#asyncOpenLedgerNoRecovery
     *
     * @param lId
     *          ledger identifier
     * @param cb
     *          Callback which will receive a LedgerHandle object
     * @param ctx
     *          optional context object, to be passwd to the callback (can be null)
     *
     * @see BookKeeper#asyncOpenLedgerNoRecovery
     */
    public void asyncOpenLedgerNoRecovery(final long lId, final OpenCallback cb, final Object ctx) {
        new LedgerOpenOp(bkc, lId, cb, ctx).initiateWithoutRecovery();
    }
    
    /**
     * Open a ledger as an administrator without recovering the ledger. This
     * means that no digest password checks are done. Otherwise, the call is
     * identical to BookKeeper#openLedgerNoRecovery
     * 
     * @param lId
     *            ledger identifier
     * @see BookKeeper#openLedgerNoRecovery
     */
    public LedgerHandle openLedgerNoRecovery(final long lId)
            throws InterruptedException, BKException {
        SyncCounter counter = new SyncCounter();
        counter.inc();
        new LedgerOpenOp(bkc, lId, new SyncOpenCallback(), counter)
                .initiateWithoutRecovery();
        /*
         * Wait
         */
        counter.block(0);
        if (counter.getrc() != BKException.Code.OK) {
            throw BKException.create(counter.getrc());
        }

        return counter.getLh();
    }

    // Object used for calling async methods and waiting for them to complete.
    static class SyncObject {
        boolean value;
        int rc;

        public SyncObject() {
            value = false;
            rc = BKException.Code.OK;
        }
    }

    /**
     * Synchronous method to rebuild and recover the ledger fragments data that
     * was stored on the source bookie. That bookie could have failed completely
     * and now the ledger data that was stored on it is under replicated. An
     * optional destination bookie server could be given if we want to copy all
     * of the ledger fragments data on the failed source bookie to it.
     * Otherwise, we will just randomly distribute the ledger fragments to the
     * active set of bookies, perhaps based on load. All ZooKeeper ledger
     * metadata will be updated to point to the new bookie(s) that contain the
     * replicated ledger fragments.
     *
     * @param bookieSrc
     *            Source bookie that had a failure. We want to replicate the
     *            ledger fragments that were stored there.
     * @param bookieDest
     *            Optional destination bookie that if passed, we will copy all
     *            of the ledger fragments from the source bookie over to it.
     */
    public void recoverBookieData(final InetSocketAddress bookieSrc, final InetSocketAddress bookieDest)
            throws InterruptedException, BKException {
        SyncObject sync = new SyncObject();
        // Call the async method to recover bookie data.
        asyncRecoverBookieData(bookieSrc, bookieDest, new RecoverCallback() {
            @Override
            public void recoverComplete(int rc, Object ctx) {
                LOG.info("Recover bookie operation completed with rc: " + rc);
                SyncObject syncObj = (SyncObject) ctx;
                synchronized (syncObj) {
                    syncObj.rc = rc;
                    syncObj.value = true;
                    syncObj.notify();
                }
            }
        }, sync);

        // Wait for the async method to complete.
        synchronized (sync) {
            while (sync.value == false) {
                sync.wait();
            }
        }
        if (sync.rc != BKException.Code.OK) {
            throw BKException.create(sync.rc);
        }
    }

    /**
     * Async method to rebuild and recover the ledger fragments data that was
     * stored on the source bookie. That bookie could have failed completely and
     * now the ledger data that was stored on it is under replicated. An
     * optional destination bookie server could be given if we want to copy all
     * of the ledger fragments data on the failed source bookie to it.
     * Otherwise, we will just randomly distribute the ledger fragments to the
     * active set of bookies, perhaps based on load. All ZooKeeper ledger
     * metadata will be updated to point to the new bookie(s) that contain the
     * replicated ledger fragments.
     *
     * @param bookieSrc
     *            Source bookie that had a failure. We want to replicate the
     *            ledger fragments that were stored there.
     * @param bookieDest
     *            Optional destination bookie that if passed, we will copy all
     *            of the ledger fragments from the source bookie over to it.
     * @param cb
     *            RecoverCallback to invoke once all of the data on the dead
     *            bookie has been recovered and replicated.
     * @param context
     *            Context for the RecoverCallback to call.
     */
    public void asyncRecoverBookieData(final InetSocketAddress bookieSrc, final InetSocketAddress bookieDest,
                                       final RecoverCallback cb, final Object context) {
        // Sync ZK to make sure we're reading the latest bookie data.
        zk.sync(bookiesPath, new AsyncCallback.VoidCallback() {
            @Override
            public void processResult(int rc, String path, Object ctx) {
                if (rc != Code.OK.intValue()) {
                    LOG.error("ZK error syncing: ", KeeperException.create(KeeperException.Code.get(rc), path));
                    cb.recoverComplete(BKException.Code.ZKException, context);
                    return;
                }
                getAvailableBookies(bookieSrc, bookieDest, cb, context);
            };
        }, null);
    }

    /**
     * This method asynchronously gets the set of available Bookies that the
     * dead input bookie's data will be copied over into. If the user passed in
     * a specific destination bookie, then just use that one. Otherwise, we'll
     * randomly pick one of the other available bookies to use for each ledger
     * fragment we are replicating.
     *
     * @param bookieSrc
     *            Source bookie that had a failure. We want to replicate the
     *            ledger fragments that were stored there.
     * @param bookieDest
     *            Optional destination bookie that if passed, we will copy all
     *            of the ledger fragments from the source bookie over to it.
     * @param cb
     *            RecoverCallback to invoke once all of the data on the dead
     *            bookie has been recovered and replicated.
     * @param context
     *            Context for the RecoverCallback to call.
     */
    private void getAvailableBookies(final InetSocketAddress bookieSrc, final InetSocketAddress bookieDest,
                                     final RecoverCallback cb, final Object context) {
        final List<InetSocketAddress> availableBookies = new LinkedList<InetSocketAddress>();
        if (bookieDest != null) {
            availableBookies.add(bookieDest);
            // Now poll ZK to get the active ledgers
            getActiveLedgers(bookieSrc, bookieDest, cb, context, availableBookies);
        } else {
            zk.getChildren(bookiesPath, null, new AsyncCallback.ChildrenCallback() {
                @Override
                public void processResult(int rc, String path, Object ctx, List<String> children) {
                    if (rc != Code.OK.intValue()) {
                        LOG.error("ZK error getting bookie nodes: ", KeeperException.create(KeeperException.Code
                                  .get(rc), path));
                        cb.recoverComplete(BKException.Code.ZKException, context);
                        return;
                    }
                    for (String bookieNode : children) {
                        if (BookKeeperConstants.READONLY
                                        .equals(bookieNode)) {
                            // exclude the readonly node from available bookies.
                            continue;
                        }
                        String parts[] = bookieNode.split(BookKeeperConstants.COLON);
                        if (parts.length < 2) {
                            LOG.error("Bookie Node retrieved from ZK has invalid name format: " + bookieNode);
                            cb.recoverComplete(BKException.Code.ZKException, context);
                            return;
                        }
                        availableBookies.add(new InetSocketAddress(parts[0], Integer.parseInt(parts[1])));
                    }
                    // Now poll ZK to get the active ledgers
                    getActiveLedgers(bookieSrc, null, cb, context, availableBookies);
                }
            }, null);
        }
    }

    /**
     * This method asynchronously polls ZK to get the current set of active
     * ledgers. From this, we can open each ledger and look at the metadata to
     * determine if any of the ledger fragments for it were stored at the dead
     * input bookie.
     *
     * @param bookieSrc
     *            Source bookie that had a failure. We want to replicate the
     *            ledger fragments that were stored there.
     * @param bookieDest
     *            Optional destination bookie that if passed, we will copy all
     *            of the ledger fragments from the source bookie over to it.
     * @param cb
     *            RecoverCallback to invoke once all of the data on the dead
     *            bookie has been recovered and replicated.
     * @param context
     *            Context for the RecoverCallback to call.
     * @param availableBookies
     *            List of Bookie Servers that are available to use for
     *            replicating data on the failed bookie. This could contain a
     *            single bookie server if the user explicitly chose a bookie
     *            server to replicate data to.
     */
    private void getActiveLedgers(final InetSocketAddress bookieSrc, final InetSocketAddress bookieDest,
                                  final RecoverCallback cb, final Object context, final List<InetSocketAddress> availableBookies) {
        // Wrapper class around the RecoverCallback so it can be used
        // as the final VoidCallback to process ledgers
        class RecoverCallbackWrapper implements AsyncCallback.VoidCallback {
            final RecoverCallback cb;

            RecoverCallbackWrapper(RecoverCallback cb) {
                this.cb = cb;
            }

            @Override
            public void processResult(int rc, String path, Object ctx) {
                cb.recoverComplete(rc, ctx);
            }
        }

        Processor<Long> ledgerProcessor = new Processor<Long>() {
            @Override
            public void process(Long ledgerId, AsyncCallback.VoidCallback iterCallback) {
                recoverLedger(bookieSrc, ledgerId, iterCallback, availableBookies);
            }
        };
        bkc.getLedgerManager().asyncProcessLedgers(
            ledgerProcessor, new RecoverCallbackWrapper(cb),
            context, BKException.Code.OK, BKException.Code.LedgerRecoveryException);
    }

    /**
     * Get a new random bookie, but ensure that it isn't one that is already
     * in the ensemble for the ledger.
     */
    private InetSocketAddress getNewBookie(final List<InetSocketAddress> bookiesAlreadyInEnsemble, 
                                           final List<InetSocketAddress> availableBookies) 
            throws BKException.BKNotEnoughBookiesException {
        ArrayList<InetSocketAddress> candidates = new ArrayList<InetSocketAddress>();
        candidates.addAll(availableBookies);
        candidates.removeAll(bookiesAlreadyInEnsemble);
        if (candidates.size() == 0) {
            throw new BKException.BKNotEnoughBookiesException();
        }
        return candidates.get(rand.nextInt(candidates.size()));
    }

    /**
     * This method asynchronously recovers a given ledger if any of the ledger
     * entries were stored on the failed bookie.
     *
     * @param bookieSrc
     *            Source bookie that had a failure. We want to replicate the
     *            ledger fragments that were stored there.
     * @param lId
     *            Ledger id we want to recover.
     * @param ledgerIterCb
     *            IterationCallback to invoke once we've recovered the current
     *            ledger.
     * @param availableBookies
     *            List of Bookie Servers that are available to use for
     *            replicating data on the failed bookie. This could contain a
     *            single bookie server if the user explicitly chose a bookie
     *            server to replicate data to.
     */
    private void recoverLedger(final InetSocketAddress bookieSrc, final long lId,
                               final AsyncCallback.VoidCallback ledgerIterCb, final List<InetSocketAddress> availableBookies) {
        LOG.debug("Recovering ledger : {}", lId);

        asyncOpenLedgerNoRecovery(lId, new OpenCallback() {
            @Override
            public void openComplete(int rc, final LedgerHandle lh, Object ctx) {
                if (rc != Code.OK.intValue()) {
                    LOG.error("BK error opening ledger: " + lId, BKException.create(rc));
                    ledgerIterCb.processResult(rc, null, null);
                    return;
                }

                LedgerMetadata lm = lh.getLedgerMetadata();
                if (!lm.isClosed() &&
                    lm.getEnsembles().size() > 0) {
                    Long lastKey = lm.getEnsembles().lastKey();
                    ArrayList<InetSocketAddress> lastEnsemble = lm.getEnsembles().get(lastKey);
                    // the original write has not removed faulty bookie from
                    // current ledger ensemble. to avoid data loss issue in
                    // the case of concurrent updates to the ensemble composition,
                    // the recovery tool should first close the ledger
                    if (lastEnsemble.contains(bookieSrc)) {
                        // close opened non recovery ledger handle
                        try {
                            lh.close();
                        } catch (Exception ie) {
                            LOG.warn("Error closing non recovery ledger handle for ledger " + lId, ie);
                        }
                        asyncOpenLedger(lId, new OpenCallback() {
                            @Override
                            public void openComplete(int newrc, final LedgerHandle newlh, Object newctx) {
                                if (newrc != Code.OK.intValue()) {
                                    LOG.error("BK error close ledger: " + lId, BKException.create(newrc));
                                    ledgerIterCb.processResult(newrc, null, null);
                                    return;
                                }
                                // do recovery
                                recoverLedger(bookieSrc, lId, ledgerIterCb, availableBookies);
                            }
                        }, null);
                        return;
                    }
                }

                /*
                 * This List stores the ledger fragments to recover indexed by
                 * the start entry ID for the range. The ensembles TreeMap is
                 * keyed off this.
                 */
                final List<Long> ledgerFragmentsToRecover = new LinkedList<Long>();
                /*
                 * This Map will store the start and end entry ID values for
                 * each of the ledger fragment ranges. The only exception is the
                 * current active fragment since it has no end yet. In the event
                 * of a bookie failure, a new ensemble is created so the current
                 * ensemble should not contain the dead bookie we are trying to
                 * recover.
                 */
                Map<Long, Long> ledgerFragmentsRange = new HashMap<Long, Long>();
                Long curEntryId = null;
                for (Map.Entry<Long, ArrayList<InetSocketAddress>> entry : lh.getLedgerMetadata().getEnsembles()
                         .entrySet()) {
                    if (curEntryId != null)
                        ledgerFragmentsRange.put(curEntryId, entry.getKey() - 1);
                    curEntryId = entry.getKey();
                    if (entry.getValue().contains(bookieSrc)) {
                        /*
                         * Current ledger fragment has entries stored on the
                         * dead bookie so we'll need to recover them.
                         */
                        ledgerFragmentsToRecover.add(entry.getKey());
                    }
                }
                // add last ensemble otherwise if the failed bookie existed in
                // the last ensemble of a closed ledger. the entries belonged to
                // last ensemble would not be replicated.
                if (curEntryId != null) {
                    ledgerFragmentsRange.put(curEntryId, lh.getLastAddConfirmed());
                }
                /*
                 * See if this current ledger contains any ledger fragment that
                 * needs to be re-replicated. If not, then just invoke the
                 * multiCallback and return.
                 */
                if (ledgerFragmentsToRecover.size() == 0) {
                    ledgerIterCb.processResult(BKException.Code.OK, null, null);
                    return;
                }

                /*
                 * Multicallback for ledger. Once all fragments for the ledger have been recovered
                 * trigger the ledgerIterCb
                 */
                MultiCallback ledgerFragmentsMcb
                    = new MultiCallback(ledgerFragmentsToRecover.size(), ledgerIterCb, null,
                                        BKException.Code.OK, BKException.Code.LedgerRecoveryException);
                /*
                 * Now recover all of the necessary ledger fragments
                 * asynchronously using a MultiCallback for every fragment.
                 */
                for (final Long startEntryId : ledgerFragmentsToRecover) {
                    Long endEntryId = ledgerFragmentsRange.get(startEntryId);
                    InetSocketAddress newBookie = null;
                    try {
                        newBookie = getNewBookie(lh.getLedgerMetadata().getEnsembles().get(startEntryId),
                                                 availableBookies);
                    } catch (BKException.BKNotEnoughBookiesException bke) {
                        ledgerFragmentsMcb.processResult(BKException.Code.NotEnoughBookiesException, 
                                                         null, null);
                        continue;
                    }
                    
                    if (LOG.isDebugEnabled()) {
                        LOG.debug("Replicating fragment from [" + startEntryId 
                                  + "," + endEntryId + "] of ledger " + lh.getId()
                                  + " to " + newBookie);
                    }
                    try {
                        LedgerFragmentReplicator.SingleFragmentCallback cb = new LedgerFragmentReplicator.SingleFragmentCallback(
                                                                               ledgerFragmentsMcb, lh, startEntryId, bookieSrc, newBookie);
                        ArrayList<InetSocketAddress> currentEnsemble =  lh.getLedgerMetadata().getEnsemble(startEntryId);
                        int bookieIndex = -1;
                        if (null != currentEnsemble) {
                            for (int i = 0; i < currentEnsemble.size(); i++) {
                                if (currentEnsemble.get(i).equals(bookieSrc)) {
                                    bookieIndex = i;
                                    break;
                                }
                            }
                        }
                        LedgerFragment ledgerFragment = new LedgerFragment(lh,
                                startEntryId, endEntryId, bookieIndex);
                        asyncRecoverLedgerFragment(lh, ledgerFragment, cb, newBookie);
                    } catch(InterruptedException e) {
                        Thread.currentThread().interrupt();
                        return;
                    }
                }
            }
            }, null);
    }

    /**
     * This method asynchronously recovers a ledger fragment which is a
     * contiguous portion of a ledger that was stored in an ensemble that
     * included the failed bookie.
     * 
     * @param lh
     *            - LedgerHandle for the ledger
     * @param lf
     *            - LedgerFragment to replicate
     * @param ledgerFragmentMcb
     *            - MultiCallback to invoke once we've recovered the current
     *            ledger fragment.
     * @param newBookie
     *            - New bookie we want to use to recover and replicate the
     *            ledger entries that were stored on the failed bookie.
     */
    private void asyncRecoverLedgerFragment(final LedgerHandle lh,
            final LedgerFragment ledgerFragment,
            final AsyncCallback.VoidCallback ledgerFragmentMcb,
            final InetSocketAddress newBookie) throws InterruptedException {
        lfr.replicate(lh, ledgerFragment, ledgerFragmentMcb, newBookie);
    }

    /**
     * Replicate the Ledger fragment to target Bookie passed.
     * 
     * @param lh
     *            - ledgerHandle
     * @param ledgerFragment
     *            - LedgerFragment to replicate
     * @param targetBookieAddress
     *            - target Bookie, to where entries should be replicated.
     */
    public void replicateLedgerFragment(LedgerHandle lh,
            final LedgerFragment ledgerFragment,
            final InetSocketAddress targetBookieAddress)
            throws InterruptedException, BKException {
        SyncCounter syncCounter = new SyncCounter();
        ResultCallBack resultCallBack = new ResultCallBack(syncCounter);
        SingleFragmentCallback cb = new SingleFragmentCallback(resultCallBack,
                lh, ledgerFragment.getFirstEntryId(), ledgerFragment
                        .getAddress(), targetBookieAddress);
        syncCounter.inc();
        asyncRecoverLedgerFragment(lh, ledgerFragment, cb, targetBookieAddress);
        syncCounter.block(0);
        if (syncCounter.getrc() != BKException.Code.OK) {
            throw BKException.create(syncCounter.getrc());
        }
    }

    /** This is the class for getting the replication result */
    static class ResultCallBack implements AsyncCallback.VoidCallback {
        private SyncCounter sync;

        public ResultCallBack(SyncCounter sync) {
            this.sync = sync;
        }

        @Override
        public void processResult(int rc, String s, Object obj) {
            sync.setrc(rc);
            sync.dec();
        }
    }

    /**
     * Format the BookKeeper metadata in zookeeper
     * 
     * @param isInteractive
     *            Whether format should ask prompt for confirmation if old data
     *            exists or not.
     * @param force
     *            If non interactive and force is true, then old data will be
     *            removed without prompt.
     * @return Returns true if format succeeds else false.
     */
    public static boolean format(ClientConfiguration conf,
            boolean isInteractive, boolean force) throws Exception {
        ZooKeeperWatcherBase w = new ZooKeeperWatcherBase(conf.getZkTimeout());
        ZooKeeper zkc = ZkUtils.createConnectedZookeeperClient(
                conf.getZkServers(), w);
        BookKeeper bkc = null;
        try {
            boolean ledgerRootExists = null != zkc.exists(
                    conf.getZkLedgersRootPath(), false);
            boolean availableNodeExists = null != zkc.exists(
                    conf.getZkAvailableBookiesPath(), false);

            // Create ledgers root node if not exists
            if (!ledgerRootExists) {
                zkc.create(conf.getZkLedgersRootPath(), "".getBytes(),
                        Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
            }
            // create available bookies node if not exists
            if (!availableNodeExists) {
                zkc.create(conf.getZkAvailableBookiesPath(), "".getBytes(),
                        Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
            }

            // If old data was there then confirm with admin.
            if (ledgerRootExists) {
                boolean confirm = false;
                if (!isInteractive) {
                    // If non interactive and force is set, then delete old
                    // data.
                    if (force) {
                        confirm = true;
                    } else {
                        confirm = false;
                    }
                } else {
                    // Confirm with the admin.
                    confirm = IOUtils
                            .confirmPrompt("Are you sure to format bookkeeper metadata ?");
                }
                if (!confirm) {
                    LOG.error("BookKeeper metadata Format aborted!!");
                    return false;
                }
            }
            bkc = new BookKeeper(conf, zkc);
            // Format all ledger metadata layout
            bkc.ledgerManagerFactory.format(conf, zkc);

            // Clear the cookies
            try {
                ZKUtil.deleteRecursive(zkc, conf.getZkLedgersRootPath()
                        + "/cookies");
            } catch (KeeperException.NoNodeException e) {
                LOG.debug("cookies node not exists in zookeeper to delete");
            }

            // Clear the INSTANCEID
            try {
                zkc.delete(conf.getZkLedgersRootPath() + "/"
                        + BookKeeperConstants.INSTANCEID, -1);
            } catch (KeeperException.NoNodeException e) {
                LOG.debug("INSTANCEID not exists in zookeeper to delete");
            }

            // create INSTANCEID
            String instanceId = UUID.randomUUID().toString();
            zkc.create(conf.getZkLedgersRootPath() + "/"
                    + BookKeeperConstants.INSTANCEID, instanceId.getBytes(),
                    Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);

            LOG.info("Successfully formatted BookKeeper metadata");
        } finally {
            if (null != bkc) {
                bkc.close();
            }
            if (null != zkc) {
                zkc.close();
            }
        }
        return true;
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/CRC32DigestManager.java,false,"package org.apache.bookkeeper.client;

/*
* Licensed to the Apache Software Foundation (ASF) under one
* or more contributor license agreements.  See the NOTICE file
* distributed with this work for additional information
* regarding copyright ownership.  The ASF licenses this file
* to you under the Apache License, Version 2.0 (the
* "License"); you may not use this file except in compliance
* with the License.  You may obtain a copy of the License at
*
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*/


import java.nio.ByteBuffer;
import java.util.zip.CRC32;

class CRC32DigestManager extends DigestManager {
    private final ThreadLocal<CRC32> crc = new ThreadLocal<CRC32>() {
        @Override
        protected CRC32 initialValue() {
            return new CRC32();
        }
    };

    public CRC32DigestManager(long ledgerId) {
        super(ledgerId);
    }

    @Override
    int getMacCodeLength() {
        return 8;
    }

    @Override
    byte[] getValueAndReset() {
        byte[] value = new byte[8];
        ByteBuffer buf = ByteBuffer.wrap(value);
        buf.putLong(crc.get().getValue());
        crc.get().reset();
        return value;
    }

    @Override
    void update(byte[] data, int offset, int length) {
        crc.get().update(data, offset, length);
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/DigestManager.java,false,"package org.apache.bookkeeper.client;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.nio.ByteBuffer;
import java.security.GeneralSecurityException;

import org.apache.bookkeeper.client.BKException.BKDigestMatchException;
import org.apache.bookkeeper.client.BookKeeper.DigestType;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.buffer.ChannelBuffer;
import org.jboss.netty.buffer.ChannelBufferInputStream;
import org.jboss.netty.buffer.ChannelBuffers;

/**
 * This class takes an entry, attaches a digest to it and packages it with relevant
 * data so that it can be shipped to the bookie. On the return side, it also
 * gets a packet, checks that the digest matches, and extracts the original entry
 * for the packet. Currently 2 types of digests are supported: MAC (based on SHA-1) and CRC32
 */

abstract class DigestManager {
    static final Logger logger = LoggerFactory.getLogger(DigestManager.class);

    static final int METADATA_LENGTH = 32;

    long ledgerId;

    abstract int getMacCodeLength();

    void update(byte[] data) {
        update(data, 0, data.length);
    }

    abstract void update(byte[] data, int offset, int length);
    abstract byte[] getValueAndReset();

    final int macCodeLength;

    public DigestManager(long ledgerId) {
        this.ledgerId = ledgerId;
        macCodeLength = getMacCodeLength();
    }

    static DigestManager instantiate(long ledgerId, byte[] passwd, DigestType digestType) throws GeneralSecurityException {
        switch(digestType) {
        case MAC:
            return new MacDigestManager(ledgerId, passwd);
        case CRC32:
            return new CRC32DigestManager(ledgerId);
        default:
            throw new GeneralSecurityException("Unknown checksum type: " + digestType);
        }
    }

    /**
     * Computes the digest for an entry and put bytes together for sending.
     *
     * @param entryId
     * @param lastAddConfirmed
     * @param length
     * @param data
     * @return
     */

    public ChannelBuffer computeDigestAndPackageForSending(long entryId, long lastAddConfirmed, long length, byte[] data, int doffset, int dlength) {

        byte[] bufferArray = new byte[METADATA_LENGTH + macCodeLength];
        ByteBuffer buffer = ByteBuffer.wrap(bufferArray);
        buffer.putLong(ledgerId);
        buffer.putLong(entryId);
        buffer.putLong(lastAddConfirmed);
        buffer.putLong(length);
        buffer.flip();

        update(buffer.array(), 0, METADATA_LENGTH);
        update(data, doffset, dlength);
        byte[] digest = getValueAndReset();

        buffer.limit(buffer.capacity());
        buffer.position(METADATA_LENGTH);
        buffer.put(digest);
        buffer.flip();

        return ChannelBuffers.wrappedBuffer(ChannelBuffers.wrappedBuffer(buffer), ChannelBuffers.wrappedBuffer(data, doffset, dlength));
    }

    private void verifyDigest(ChannelBuffer dataReceived) throws BKDigestMatchException {
        verifyDigest(LedgerHandle.INVALID_ENTRY_ID, dataReceived, true);
    }

    private void verifyDigest(long entryId, ChannelBuffer dataReceived) throws BKDigestMatchException {
        verifyDigest(entryId, dataReceived, false);
    }

    private void verifyDigest(long entryId, ChannelBuffer dataReceived, boolean skipEntryIdCheck)
            throws BKDigestMatchException {

        ByteBuffer dataReceivedBuffer = dataReceived.toByteBuffer();
        byte[] digest;

        if ((METADATA_LENGTH + macCodeLength) > dataReceived.readableBytes()) {
            logger.error("Data received is smaller than the minimum for this digest type. "
                    + " Either the packet it corrupt, or the wrong digest is configured. "
                    + " Digest type: {}, Packet Length: {}",
                    this.getClass().getName(), dataReceived.readableBytes());
            throw new BKDigestMatchException();
        }
        update(dataReceivedBuffer.array(), dataReceivedBuffer.position(), METADATA_LENGTH);

        int offset = METADATA_LENGTH + macCodeLength;
        update(dataReceivedBuffer.array(), dataReceivedBuffer.position() + offset, dataReceived.readableBytes() - offset);
        digest = getValueAndReset();

        for (int i = 0; i < digest.length; i++) {
            if (digest[i] != dataReceived.getByte(METADATA_LENGTH + i)) {
                logger.error("Mac mismatch for ledger-id: " + ledgerId + ", entry-id: " + entryId);
                throw new BKDigestMatchException();
            }
        }

        long actualLedgerId = dataReceived.readLong();
        long actualEntryId = dataReceived.readLong();

        if (actualLedgerId != ledgerId) {
            logger.error("Ledger-id mismatch in authenticated message, expected: " + ledgerId + " , actual: "
                         + actualLedgerId);
            throw new BKDigestMatchException();
        }

        if (!skipEntryIdCheck && actualEntryId != entryId) {
            logger.error("Entry-id mismatch in authenticated message, expected: " + entryId + " , actual: "
                         + actualEntryId);
            throw new BKDigestMatchException();
        }

    }

    /**
     * Verify that the digest matches and returns the data in the entry.
     *
     * @param entryId
     * @param dataReceived
     * @return
     * @throws BKDigestMatchException
     */
    ChannelBufferInputStream verifyDigestAndReturnData(long entryId, ChannelBuffer dataReceived)
            throws BKDigestMatchException {
        verifyDigest(entryId, dataReceived);
        dataReceived.readerIndex(METADATA_LENGTH + macCodeLength);
        return new ChannelBufferInputStream(dataReceived);
    }

    static class RecoveryData {
        long lastAddConfirmed;
        long length;

        public RecoveryData(long lastAddConfirmed, long length) {
            this.lastAddConfirmed = lastAddConfirmed;
            this.length = length;
        }

    }

    RecoveryData verifyDigestAndReturnLastConfirmed(ChannelBuffer dataReceived) throws BKDigestMatchException {
        verifyDigest(dataReceived);
        dataReceived.readerIndex(8);

        dataReceived.readLong(); // skip unused entryId
        long lastAddConfirmed = dataReceived.readLong();
        long length = dataReceived.readLong();
        return new RecoveryData(lastAddConfirmed, length);
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/DistributionSchedule.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.client;

import java.util.List;
/**
 * This interface determins how entries are distributed among bookies.
 *
 * Every entry gets replicated to some number of replicas. The first replica for
 * an entry is given a replicaIndex of 0, and so on. To distribute write load,
 * not all entries go to all bookies. Given an entry-id and replica index, an
 * {@link DistributionSchedule} determines which bookie that replica should go
 * to.
 */

interface DistributionSchedule {

    /**
     * return the set of bookie indices to send the message to
     */
    public List<Integer> getWriteSet(long entryId);

    /**
     * An ack set represents the set of bookies from which
     * a response must be received so that an entry can be
     * considered to be replicated on a quorum.
     */
    public interface AckSet {
        /**
         * Add a bookie response and check if quorum has been met
         * @return true if quorum has been met, false otherwise
         */
        public boolean addBookieAndCheck(int bookieIndexHeardFrom);

        /**
         * Invalidate a previous bookie response.
         * Used for reissuing write requests.
         */
        public void removeBookie(int bookie);
    }

    /**
     * Returns an ackset object, responses should be checked against this
     */
    public AckSet getAckSet();


    /**
     * Interface to keep track of which bookies in an ensemble, an action
     * has been performed for.
     */
    public interface QuorumCoverageSet {
        /**
         * Add a bookie to the set, and check if all quorum in the set
         * have had the action performed for it.
         * @param bookieIndexHeardFrom Bookie we've just heard from
         * @return whether all quorums have been covered
         */
        public boolean addBookieAndCheckCovered(int bookieIndexHeardFrom);
    }

    public QuorumCoverageSet getCoverageSet();
    
    /**
     * Whether entry presents on given bookie index
     * 
     * @param entryId
     *            - entryId to check the presence on given bookie index
     * @param bookieIndex
     *            - bookie index on which it need to check the possible presence
     *            of the entry
     * @return true if it has entry otherwise false.
     */
    public boolean hasEntry(long entryId, int bookieIndex);
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/LedgerChecker.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */
package org.apache.bookkeeper.client;

import java.util.ArrayList;
import java.util.HashSet;
import java.util.Set;
import java.util.Map;

import org.apache.bookkeeper.proto.BookieClient;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.ReadEntryCallback;

import org.jboss.netty.buffer.ChannelBuffer;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.net.InetSocketAddress;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicLong;
import java.util.concurrent.atomic.AtomicInteger;

/**
 *Checks the complete ledger and finds the UnderReplicated fragments if any
 */
public class LedgerChecker {
    private static Logger LOG = LoggerFactory.getLogger(LedgerChecker.class);

    public final BookieClient bookieClient;

    static class InvalidFragmentException extends Exception {
        private static final long serialVersionUID = 1467201276417062353L;
    }

    /**
     * This will collect all the entry read call backs and finally it will give
     * call back to previous call back API which is waiting for it once it meets
     * the expected call backs from down
     */
    private static class ReadManyEntriesCallback implements ReadEntryCallback {
        AtomicBoolean completed = new AtomicBoolean(false);
        final AtomicLong numEntries;
        final LedgerFragment fragment;
        final GenericCallback<LedgerFragment> cb;

        ReadManyEntriesCallback(long numEntries, LedgerFragment fragment,
                GenericCallback<LedgerFragment> cb) {
            this.numEntries = new AtomicLong(numEntries);
            this.fragment = fragment;
            this.cb = cb;
        }

        public void readEntryComplete(int rc, long ledgerId, long entryId,
                ChannelBuffer buffer, Object ctx) {
            if (rc == BKException.Code.OK) {
                if (numEntries.decrementAndGet() == 0
                        && !completed.getAndSet(true)) {
                    cb.operationComplete(rc, fragment);
                }
            } else if (!completed.getAndSet(true)) {
                cb.operationComplete(rc, fragment);
            }
        }
    }

    public LedgerChecker(BookKeeper bkc) {
        bookieClient = bkc.getBookieClient();
    }

    private void verifyLedgerFragment(LedgerFragment fragment,
            GenericCallback<LedgerFragment> cb) throws InvalidFragmentException {
        long firstStored = fragment.getFirstStoredEntryId();
        long lastStored = fragment.getLastStoredEntryId();

        if (firstStored == LedgerHandle.INVALID_ENTRY_ID) {
            if (lastStored != LedgerHandle.INVALID_ENTRY_ID) {
                throw new InvalidFragmentException();
            }
            cb.operationComplete(BKException.Code.OK, fragment);
            return;
        }
        if (firstStored == lastStored) {
            ReadManyEntriesCallback manycb = new ReadManyEntriesCallback(1,
                    fragment, cb);
            bookieClient.readEntry(fragment.getAddress(), fragment
                    .getLedgerId(), firstStored, manycb, null);
        } else {
            ReadManyEntriesCallback manycb = new ReadManyEntriesCallback(2,
                    fragment, cb);
            bookieClient.readEntry(fragment.getAddress(), fragment
                    .getLedgerId(), firstStored, manycb, null);
            bookieClient.readEntry(fragment.getAddress(), fragment
                    .getLedgerId(), lastStored, manycb, null);
        }
    }

    /**
     * Callback for checking whether an entry exists or not.
     * It is used to differentiate the cases where it has been written
     * but now cannot be read, and where it never has been written.
     */
    private static class EntryExistsCallback implements ReadEntryCallback {
        AtomicBoolean entryMayExist = new AtomicBoolean(false);
        final AtomicInteger numReads;
        final GenericCallback<Boolean> cb;

        EntryExistsCallback(int numReads,
                            GenericCallback<Boolean> cb) {
            this.numReads = new AtomicInteger(numReads);
            this.cb = cb;
        }

        public void readEntryComplete(int rc, long ledgerId, long entryId,
                                      ChannelBuffer buffer, Object ctx) {
            if (rc != BKException.Code.NoSuchEntryException) {
                entryMayExist.set(true);
            }

            if (numReads.decrementAndGet() == 0) {
                cb.operationComplete(rc, entryMayExist.get());
            }
        }
    }

    /**
     * This will collect all the fragment read call backs and finally it will
     * give call back to above call back API which is waiting for it once it
     * meets the expected call backs from down
     */
    private static class FullLedgerCallback implements
            GenericCallback<LedgerFragment> {
        final Set<LedgerFragment> badFragments;
        final AtomicLong numFragments;
        final GenericCallback<Set<LedgerFragment>> cb;

        FullLedgerCallback(long numFragments,
                GenericCallback<Set<LedgerFragment>> cb) {
            badFragments = new HashSet<LedgerFragment>();
            this.numFragments = new AtomicLong(numFragments);
            this.cb = cb;
        }

        public void operationComplete(int rc, LedgerFragment result) {
            if (rc != BKException.Code.OK) {
                badFragments.add(result);
            }
            if (numFragments.decrementAndGet() == 0) {
                cb.operationComplete(BKException.Code.OK, badFragments);
            }
        }
    }

    /**
     * Check that all the fragments in the passed in ledger, and report those
     * which are missing.
     */
    public void checkLedger(LedgerHandle lh,
                            final GenericCallback<Set<LedgerFragment>> cb) {
        // build a set of all fragment replicas
        final Set<LedgerFragment> fragments = new HashSet<LedgerFragment>();

        Long curEntryId = null;
        ArrayList<InetSocketAddress> curEnsemble = null;
        for (Map.Entry<Long, ArrayList<InetSocketAddress>> e : lh
                .getLedgerMetadata().getEnsembles().entrySet()) {
            if (curEntryId != null) {
                for (int i = 0; i < curEnsemble.size(); i++) {
                    fragments.add(new LedgerFragment(lh, curEntryId,
                            e.getKey() - 1, i));
                }
            }
            curEntryId = e.getKey();
            curEnsemble = e.getValue();
        }

        /* Checking the last segment of the ledger can be complicated in some cases.
         * In the case that the ledger is closed, we can just check the fragments of
         * the segment as normal, except in the case that no entry was ever written,
         * to the ledger, in which case we check no fragments.
         * In the case that the ledger is open, but enough entries have been written,
         * for lastAddConfirmed to be set above the start entry of the segment, we
         * can also check as normal.
         * However, if lastAddConfirmed cannot be trusted, such as when it's lower than
         * the first entry id, or not set at all, we cannot be sure if there has been
         * data written to the segment. For this reason, we have to send a read request
         * to the bookies which should have the first entry. If they respond with
         * NoSuchEntry we can assume it was never written. If they respond with anything
         * else, we must assume the entry has been written, so we run the check.
         */
        if (curEntryId != null
            && !(lh.getLastAddConfirmed() == LedgerHandle.INVALID_ENTRY_ID
                 && lh.getLedgerMetadata().isClosed())) {
            long lastEntry = lh.getLastAddConfirmed();

            if (lastEntry < curEntryId) {
                lastEntry = curEntryId;
            }

            final Set<LedgerFragment> finalSegmentFragments = new HashSet<LedgerFragment>();
            for (int i = 0; i < curEnsemble.size(); i++) {
                finalSegmentFragments.add(new LedgerFragment(lh, curEntryId,
                        lastEntry, i));
            }

            // Check for the case that no last confirmed entry has
            // been set.
            if (curEntryId == lastEntry) {
                final long entryToRead = curEntryId;

                EntryExistsCallback eecb
                    = new EntryExistsCallback(lh.getLedgerMetadata().getWriteQuorumSize(),
                                              new GenericCallback<Boolean>() {
                                                  public void operationComplete(int rc, Boolean result) {
                                                      if (result) {
                                                          fragments.addAll(finalSegmentFragments);
                                                      }
                                                      checkFragments(fragments, cb);
                                                  }
                                              });

                for (int bi : lh.getDistributionSchedule().getWriteSet(entryToRead)) {
                    InetSocketAddress addr = curEnsemble.get(bi);
                    bookieClient.readEntry(addr, lh.getId(),
                                           entryToRead, eecb, null);
                }
                return;
            } else {
                fragments.addAll(finalSegmentFragments);
            }
        }

        checkFragments(fragments, cb);
    }

    private void checkFragments(Set<LedgerFragment> fragments,
                                GenericCallback<Set<LedgerFragment>> cb) {
        if (fragments.size() == 0) { // no fragments to verify
            cb.operationComplete(BKException.Code.OK, fragments);
            return;
        }

        // verify all the collected fragment replicas
        FullLedgerCallback allFragmentsCb = new FullLedgerCallback(fragments
                .size(), cb);
        for (LedgerFragment r : fragments) {
            LOG.debug("Checking fragment {}", r);
            try {
                verifyLedgerFragment(r, allFragmentsCb);
            } catch (InvalidFragmentException ife) {
                LOG.error("Invalid fragment found : {}", r);
                allFragmentsCb.operationComplete(
                        BKException.Code.IncorrectParameterException, r);
            }
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/LedgerCreateOp.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.client;

import java.net.InetSocketAddress;
import java.security.GeneralSecurityException;
import java.util.ArrayList;

import org.apache.bookkeeper.client.AsyncCallback.CreateCallback;
import org.apache.bookkeeper.client.BKException.BKNotEnoughBookiesException;
import org.apache.bookkeeper.client.BookKeeper.DigestType;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Encapsulates asynchronous ledger create operation
 *
 */
class LedgerCreateOp implements GenericCallback<Long> {

    static final Logger LOG = LoggerFactory.getLogger(LedgerCreateOp.class);

    CreateCallback cb;
    LedgerMetadata metadata;
    LedgerHandle lh;
    Object ctx;
    byte[] passwd;
    BookKeeper bk;
    DigestType digestType;

    /**
     * Constructor
     *
     * @param bk
     *       BookKeeper object
     * @param ensembleSize
     *       ensemble size
     * @param quorumSize
     *       quorum size
     * @param digestType
     *       digest type, either MAC or CRC32
     * @param passwd
     *       passowrd
     * @param cb
     *       callback implementation
     * @param ctx
     *       optional control object
     */

    LedgerCreateOp(BookKeeper bk, int ensembleSize,
                   int writeQuorumSize, int ackQuorumSize,
                   DigestType digestType,
                   byte[] passwd, CreateCallback cb, Object ctx) {
        this.bk = bk;
        this.metadata = new LedgerMetadata(ensembleSize, writeQuorumSize, ackQuorumSize, digestType, passwd);
        this.digestType = digestType;
        this.passwd = passwd;
        this.cb = cb;
        this.ctx = ctx;
    }

    /**
     * Initiates the operation
     */
    public void initiate() {
        // allocate ensemble first

        /*
         * Adding bookies to ledger handle
         */

        ArrayList<InetSocketAddress> ensemble;
        try {
            ensemble = bk.bookieWatcher.getNewBookies(metadata.getEnsembleSize());
        } catch (BKNotEnoughBookiesException e) {
            LOG.error("Not enough bookies to create ledger");
            cb.createComplete(e.getCode(), null, this.ctx);
            return;
        }

        /*
         * Add ensemble to the configuration
         */
        metadata.addEnsemble(0L, ensemble);

        // create a ledger with metadata
        bk.getLedgerManager().createLedger(metadata, this);
    }

    /**
     * Callback when created ledger.
     */
    @Override
    public void operationComplete(int rc, Long ledgerId) {
        if (BKException.Code.OK != rc) {
            cb.createComplete(rc, null, this.ctx);
            return;
        }

        try {
            lh = new LedgerHandle(bk, ledgerId, metadata, digestType, passwd);
        } catch (GeneralSecurityException e) {
            LOG.error("Security exception while creating ledger: " + ledgerId, e);
            cb.createComplete(BKException.Code.DigestNotInitializedException, null, this.ctx);
            return;
        } catch (NumberFormatException e) {
            LOG.error("Incorrectly entered parameter throttle: " + bk.getConf().getThrottleValue(), e);
            cb.createComplete(BKException.Code.IncorrectParameterException, null, this.ctx);
            return;
        }

        // return the ledger handle back
        cb.createComplete(BKException.Code.OK, lh, this.ctx);
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/LedgerDeleteOp.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.client;

import org.apache.bookkeeper.client.AsyncCallback.DeleteCallback;
import org.apache.bookkeeper.util.OrderedSafeExecutor.OrderedSafeGenericCallback;
import org.apache.bookkeeper.versioning.Version;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Encapsulates asynchronous ledger delete operation
 *
 */
class LedgerDeleteOp extends OrderedSafeGenericCallback<Void> {

    static final Logger LOG = LoggerFactory.getLogger(LedgerDeleteOp.class);

    BookKeeper bk;
    long ledgerId;
    DeleteCallback cb;
    Object ctx;

    /**
     * Constructor
     *
     * @param bk
     *            BookKeeper object
     * @param ledgerId
     *            ledger Id
     * @param cb
     *            callback implementation
     * @param ctx
     *            optional control object
     */
    LedgerDeleteOp(BookKeeper bk, long ledgerId, DeleteCallback cb, Object ctx) {
        super(bk.mainWorkerPool, ledgerId);
        this.bk = bk;
        this.ledgerId = ledgerId;
        this.cb = cb;
        this.ctx = ctx;
    }

    /**
     * Initiates the operation
     */
    public void initiate() {
        // Asynchronously delete the ledger from meta manager
        // When this completes, it will invoke the callback method below.
        bk.getLedgerManager().removeLedgerMetadata(ledgerId, Version.ANY, this);
    }

    /**
     * Implements Delete Callback.
     */
    @Override
    public void safeOperationComplete(int rc, Void result) {
        cb.deleteComplete(rc, this.ctx);
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/LedgerEntry.java,false,"package org.apache.bookkeeper.client;

/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

import java.io.IOException;
import java.io.InputStream;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.buffer.ChannelBufferInputStream;

/**
 * Ledger entry. Its a simple tuple containing the ledger id, the entry-id, and
 * the entry content.
 *
 */

public class LedgerEntry {
    Logger LOG = LoggerFactory.getLogger(LedgerEntry.class);

    long ledgerId;
    long entryId;
    long length;
    ChannelBufferInputStream entryDataStream;

    LedgerEntry(long lId, long eId) {
        this.ledgerId = lId;
        this.entryId = eId;
    }

    public long getLedgerId() {
        return ledgerId;
    }

    public long getEntryId() {
        return entryId;
    }

    public long getLength() {
        return length;
    }

    public byte[] getEntry() {
        try {
            // In general, you can't rely on the available() method of an input
            // stream, but ChannelBufferInputStream is backed by a byte[] so it
            // accurately knows the # bytes available
            byte[] ret = new byte[entryDataStream.available()];
            entryDataStream.readFully(ret);
            return ret;
        } catch (IOException e) {
            // The channelbufferinput stream doesnt really throw the
            // ioexceptions, it just has to be in the signature because
            // InputStream says so. Hence this code, should never be reached.
            LOG.error("Unexpected IOException while reading from channel buffer", e);
            return new byte[0];
        }
    }

    public InputStream getEntryInputStream() {
        return entryDataStream;
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/LedgerFragment.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */
package org.apache.bookkeeper.client;

import java.net.InetSocketAddress;
import java.util.ArrayList;
import java.util.List;
import java.util.SortedMap;

/**
 * Represents the entries of a segment of a ledger which are stored on a single
 * bookie in the segments bookie ensemble.
 * 
 * Used for checking and recovery
 */
public class LedgerFragment {
    private final int bookieIndex;
    private final List<InetSocketAddress> ensemble;
    private final long firstEntryId;
    private final long lastKnownEntryId;
    private final long ledgerId;
    private final DistributionSchedule schedule;
    private final boolean isLedgerClosed;

    LedgerFragment(LedgerHandle lh, long firstEntryId,
            long lastKnownEntryId, int bookieIndex) {
        this.ledgerId = lh.getId();
        this.firstEntryId = firstEntryId;
        this.lastKnownEntryId = lastKnownEntryId;
        this.bookieIndex = bookieIndex;
        this.ensemble = lh.getLedgerMetadata().getEnsemble(firstEntryId);
        this.schedule = lh.getDistributionSchedule();
        SortedMap<Long, ArrayList<InetSocketAddress>> ensembles = lh
                .getLedgerMetadata().getEnsembles();
        this.isLedgerClosed = lh.getLedgerMetadata().isClosed()
                || !ensemble.equals(ensembles.get(ensembles.lastKey()));
    }

    /**
     * Returns true, if and only if the ledger fragment will never be modified
     * by any of the clients in future, otherwise false. i.e,
     * <ol>
     * <li>If ledger is in closed state, then no other clients can modify this
     * fragment.</li>
     * <li>If ledger is not in closed state and the current fragment is not a
     * last fragment, then no one will modify this fragment.</li>
     * </ol>
     */
    public boolean isClosed() {
        return isLedgerClosed;
    }

    long getLedgerId() {
        return ledgerId;
    }

    long getFirstEntryId() {
        return firstEntryId;
    }

    long getLastKnownEntryId() {
        return lastKnownEntryId;
    }

    /**
     * Gets the failedBookie address
     */
    public InetSocketAddress getAddress() {
        return ensemble.get(bookieIndex);
    }
    
    /**
     * Gets the failedBookie index
     */
    public int getBookiesIndex() {
        return bookieIndex;
    }

    /**
     * Gets the first stored entry id of the fragment in failed bookie.
     * 
     * @return entryId
     */
    public long getFirstStoredEntryId() {
        long firstEntry = firstEntryId;

        for (int i = 0; i < ensemble.size() && firstEntry <= lastKnownEntryId; i++) {
            if (schedule.hasEntry(firstEntry, bookieIndex)) {
                return firstEntry;
            } else {
                firstEntry++;
            }
        }
        return LedgerHandle.INVALID_ENTRY_ID;
    }

    /**
     * Gets the last stored entry id of the fragment in failed bookie.
     * 
     * @return entryId
     */
    public long getLastStoredEntryId() {
        long lastEntry = lastKnownEntryId;
        for (int i = 0; i < ensemble.size() && lastEntry >= firstEntryId; i++) {
            if (schedule.hasEntry(lastEntry, bookieIndex)) {
                return lastEntry;
            } else {
                lastEntry--;
            }
        }
        return LedgerHandle.INVALID_ENTRY_ID;
    }

    /**
     * Gets the ensemble of fragment
     * 
     * @return the ensemble for the segment which this fragment is a part of
     */
    public List<InetSocketAddress> getEnsemble() {
        return this.ensemble;
    }

    @Override
    public String toString() {
        return String.format("Fragment(LedgerID: %d, FirstEntryID: %d[%d], "
                + "LastKnownEntryID: %d[%d], Host: %s, Closed: %s)", ledgerId, firstEntryId,
                getFirstStoredEntryId(), lastKnownEntryId, getLastStoredEntryId(),
                getAddress(), isLedgerClosed);
    }
}"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/LedgerFragmentReplicator.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */
package org.apache.bookkeeper.client;

import java.net.InetSocketAddress;
import java.util.ArrayList;
import java.util.Enumeration;
import java.util.HashSet;
import java.util.Iterator;
import java.util.LinkedList;
import java.util.List;
import java.util.Set;

import org.apache.bookkeeper.client.AsyncCallback.ReadCallback;
import org.apache.bookkeeper.proto.BookieProtocol;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.MultiCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.WriteCallback;
import org.apache.bookkeeper.util.OrderedSafeExecutor.OrderedSafeGenericCallback;

import org.apache.zookeeper.AsyncCallback;
import org.apache.zookeeper.KeeperException.Code;
import org.jboss.netty.buffer.ChannelBuffer;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * This is the helper class for replicating the fragments from one bookie to
 * another
 */
public class LedgerFragmentReplicator {

    // BookKeeper instance
    private BookKeeper bkc;

    public LedgerFragmentReplicator(BookKeeper bkc) {
        this.bkc = bkc;
    }

    private static Logger LOG = LoggerFactory
            .getLogger(LedgerFragmentReplicator.class);

    private void replicateFragmentInternal(final LedgerHandle lh,
            final LedgerFragment lf,
            final AsyncCallback.VoidCallback ledgerFragmentMcb,
            final InetSocketAddress newBookie) throws InterruptedException {
        if (!lf.isClosed()) {
            LOG.error("Trying to replicate an unclosed fragment;"
                      + " This is not safe {}", lf);
            ledgerFragmentMcb.processResult(BKException.Code.UnclosedFragmentException,
                                            null, null);
            return;
        }
        Long startEntryId = lf.getFirstStoredEntryId();
        Long endEntryId = lf.getLastStoredEntryId();
        if (endEntryId == null) {
            /*
             * Ideally this should never happen if bookie failure is taken care
             * of properly. Nothing we can do though in this case.
             */
            LOG.warn("Dead bookie (" + lf.getAddress()
                    + ") is still part of the current"
                    + " active ensemble for ledgerId: " + lh.getId());
            ledgerFragmentMcb.processResult(BKException.Code.OK, null, null);
            return;
        }
        if (startEntryId > endEntryId) {
            // for open ledger which there is no entry, the start entry id is 0,
            // the end entry id is -1.
            // we can return immediately to trigger forward read
            ledgerFragmentMcb.processResult(BKException.Code.OK, null, null);
            return;
        }

        /*
         * Add all the entries to entriesToReplicate list from
         * firstStoredEntryId to lastStoredEntryID.
         */
        List<Long> entriesToReplicate = new LinkedList<Long>();
        long lastStoredEntryId = lf.getLastStoredEntryId();
        for (long i = lf.getFirstStoredEntryId(); i <= lastStoredEntryId; i++) {
            entriesToReplicate.add(i);
        }
        /*
         * Now asynchronously replicate all of the entries for the ledger
         * fragment that were on the dead bookie.
         */
        MultiCallback ledgerFragmentEntryMcb = new MultiCallback(
                entriesToReplicate.size(), ledgerFragmentMcb, null, BKException.Code.OK,
                BKException.Code.LedgerRecoveryException);
        for (final Long entryId : entriesToReplicate) {
            recoverLedgerFragmentEntry(entryId, lh, ledgerFragmentEntryMcb,
                    newBookie);
        }
    }

    /**
     * This method replicate a ledger fragment which is a contiguous portion of
     * a ledger that was stored in an ensemble that included the failed bookie.
     * It will Splits the fragment into multiple sub fragments by keeping the
     * max entries up to the configured value of rereplicationEntryBatchSize and
     * then it re-replicates that batched entry fragments one by one. After
     * re-replication of all batched entry fragments, it will update the
     * ensemble info with new Bookie once
     * 
     * @param lh
     *            LedgerHandle for the ledger
     * @param lf
     *            LedgerFragment to replicate
     * @param ledgerFragmentMcb
     *            MultiCallback to invoke once we've recovered the current
     *            ledger fragment.
     * @param targetBookieAddress
     *            New bookie we want to use to recover and replicate the ledger
     *            entries that were stored on the failed bookie.
     */
    void replicate(final LedgerHandle lh, final LedgerFragment lf,
            final AsyncCallback.VoidCallback ledgerFragmentMcb,
            final InetSocketAddress targetBookieAddress)
            throws InterruptedException {
        Set<LedgerFragment> partionedFragments = splitIntoSubFragments(lh, lf,
                bkc.getConf().getRereplicationEntryBatchSize());
        LOG.info("Fragment :" + lf + " is split into sub fragments :"
                + partionedFragments);
        replicateNextBatch(lh, partionedFragments.iterator(),
                ledgerFragmentMcb, targetBookieAddress);
    }

    /** Replicate the batched entry fragments one after other */
    private void replicateNextBatch(final LedgerHandle lh,
            final Iterator<LedgerFragment> fragments,
            final AsyncCallback.VoidCallback ledgerFragmentMcb,
            final InetSocketAddress targetBookieAddress) {
        if (fragments.hasNext()) {
            try {
                replicateFragmentInternal(lh, fragments.next(),
                        new AsyncCallback.VoidCallback() {
                            @Override
                            public void processResult(int rc, String v, Object ctx) {
                                if (rc != BKException.Code.OK) {
                                    ledgerFragmentMcb.processResult(rc, null,
                                            null);
                                } else {
                                    replicateNextBatch(lh, fragments,
                                            ledgerFragmentMcb,
                                            targetBookieAddress);
                                }
                            }

                        }, targetBookieAddress);
            } catch (InterruptedException e) {
                ledgerFragmentMcb.processResult(
                        BKException.Code.InterruptedException, null, null);
                Thread.currentThread().interrupt();
            }
        } else {
            ledgerFragmentMcb.processResult(BKException.Code.OK, null, null);
        }
    }

    /**
     * Split the full fragment into batched entry fragments by keeping
     * rereplicationEntryBatchSize of entries in each one and can treat them as
     * sub fragments
     */
    static Set<LedgerFragment> splitIntoSubFragments(LedgerHandle lh,
            LedgerFragment ledgerFragment, long rereplicationEntryBatchSize) {
        Set<LedgerFragment> fragments = new HashSet<LedgerFragment>();
        if (rereplicationEntryBatchSize <= 0) {
            // rereplicationEntryBatchSize can not be 0 or less than 0,
            // returning with the current fragment
            fragments.add(ledgerFragment);
            return fragments;
        }

        long firstEntryId = ledgerFragment.getFirstStoredEntryId();
        long lastEntryId = ledgerFragment.getLastStoredEntryId();
        long numberOfEntriesToReplicate = (lastEntryId - firstEntryId) + 1;
        long splitsWithFullEntries = numberOfEntriesToReplicate
                / rereplicationEntryBatchSize;

        if (splitsWithFullEntries == 0) {// only one fragment
            fragments.add(ledgerFragment);
            return fragments;
        }

        long fragmentSplitLastEntry = 0;
        for (int i = 0; i < splitsWithFullEntries; i++) {
            fragmentSplitLastEntry = (firstEntryId + rereplicationEntryBatchSize) - 1;
            fragments.add(new LedgerFragment(lh, firstEntryId,
                    fragmentSplitLastEntry, ledgerFragment.getBookiesIndex()));
            firstEntryId = fragmentSplitLastEntry + 1;
        }

        long lastSplitWithPartialEntries = numberOfEntriesToReplicate
                % rereplicationEntryBatchSize;
        if (lastSplitWithPartialEntries > 0) {
            fragments.add(new LedgerFragment(lh, firstEntryId, firstEntryId
                    + lastSplitWithPartialEntries - 1, ledgerFragment
                    .getBookiesIndex()));
        }
        return fragments;
    }

    /**
     * This method asynchronously recovers a specific ledger entry by reading
     * the values via the BookKeeper Client (which would read it from the other
     * replicas) and then writing it to the chosen new bookie.
     * 
     * @param entryId
     *            Ledger Entry ID to recover.
     * @param lh
     *            LedgerHandle for the ledger
     * @param ledgerFragmentEntryMcb
     *            MultiCallback to invoke once we've recovered the current
     *            ledger entry.
     * @param newBookie
     *            New bookie we want to use to recover and replicate the ledger
     *            entries that were stored on the failed bookie.
     */
    private void recoverLedgerFragmentEntry(final Long entryId,
            final LedgerHandle lh,
            final AsyncCallback.VoidCallback ledgerFragmentEntryMcb,
            final InetSocketAddress newBookie) throws InterruptedException {
        /*
         * Read the ledger entry using the LedgerHandle. This will allow us to
         * read the entry from one of the other replicated bookies other than
         * the dead one.
         */
        lh.asyncReadEntries(entryId, entryId, new ReadCallback() {
            @Override
            public void readComplete(int rc, LedgerHandle lh,
                    Enumeration<LedgerEntry> seq, Object ctx) {
                if (rc != Code.OK.intValue()) {
                    LOG.error("BK error reading ledger entry: " + entryId,
                            BKException.create(rc));
                    ledgerFragmentEntryMcb.processResult(rc, null, null);
                    return;
                }
                /*
                 * Now that we've read the ledger entry, write it to the new
                 * bookie we've selected.
                 */
                LedgerEntry entry = seq.nextElement();
                byte[] data = entry.getEntry();
                ChannelBuffer toSend = lh.getDigestManager()
                        .computeDigestAndPackageForSending(entryId,
                                lh.getLastAddConfirmed(), entry.getLength(),
                                data, 0, data.length);
                bkc.getBookieClient().addEntry(newBookie, lh.getId(),
                        lh.getLedgerKey(), entryId, toSend,
                        new WriteCallback() {
                            @Override
                            public void writeComplete(int rc, long ledgerId,
                                    long entryId, InetSocketAddress addr,
                                    Object ctx) {
                                if (rc != Code.OK.intValue()) {
                                    LOG.error(
                                            "BK error writing entry for ledgerId: "
                                                    + ledgerId + ", entryId: "
                                                    + entryId + ", bookie: "
                                                    + addr, BKException
                                                    .create(rc));
                                } else {
                                    if (LOG.isDebugEnabled()) {
                                        LOG.debug("Success writing ledger id "
                                                + ledgerId + ", entry id "
                                                + entryId + " to a new bookie "
                                                + addr + "!");
                                    }
                                }
                                /*
                                 * Pass the return code result up the chain with
                                 * the parent callback.
                                 */
                                ledgerFragmentEntryMcb.processResult(rc, null,
                                        null);
                            }
                        }, null, BookieProtocol.FLAG_RECOVERY_ADD);
            }
        }, null);
    }
    
    /**
     * Callback for recovery of a single ledger fragment. Once the fragment has
     * had all entries replicated, update the ensemble in zookeeper. Once
     * finished propogate callback up to ledgerFragmentsMcb which should be a
     * multicallback responsible for all fragments in a single ledger
     */
    static class SingleFragmentCallback implements AsyncCallback.VoidCallback {
        final AsyncCallback.VoidCallback ledgerFragmentsMcb;
        final LedgerHandle lh;
        final long fragmentStartId;
        final InetSocketAddress oldBookie;
        final InetSocketAddress newBookie;

        SingleFragmentCallback(AsyncCallback.VoidCallback ledgerFragmentsMcb,
                LedgerHandle lh, long fragmentStartId,
                InetSocketAddress oldBookie, InetSocketAddress newBookie) {
            this.ledgerFragmentsMcb = ledgerFragmentsMcb;
            this.lh = lh;
            this.fragmentStartId = fragmentStartId;
            this.newBookie = newBookie;
            this.oldBookie = oldBookie;
        }

        @Override
        public void processResult(int rc, String path, Object ctx) {
            if (rc != Code.OK.intValue()) {
                LOG.error("BK error replicating ledger fragments for ledger: "
                        + lh.getId(), BKException.create(rc));
                ledgerFragmentsMcb.processResult(rc, null, null);
                return;
            }
            updateEnsembleInfo(ledgerFragmentsMcb, fragmentStartId, lh,
                                        oldBookie, newBookie);
        }
    }

    /** Updates the ensemble with newBookie and notify the ensembleUpdatedCb */
    private static void updateEnsembleInfo(
            AsyncCallback.VoidCallback ensembleUpdatedCb, long fragmentStartId,
            LedgerHandle lh, InetSocketAddress oldBookie,
            InetSocketAddress newBookie) {
        /*
         * Update the ledger metadata's ensemble info to point to the new
         * bookie.
         */
        ArrayList<InetSocketAddress> ensemble = lh.getLedgerMetadata()
                .getEnsembles().get(fragmentStartId);
        int deadBookieIndex = ensemble.indexOf(oldBookie);
        ensemble.remove(deadBookieIndex);
        ensemble.add(deadBookieIndex, newBookie);
        lh.writeLedgerConfig(new UpdateEnsembleCb(ensembleUpdatedCb,
                fragmentStartId, lh, oldBookie, newBookie));
    }

    /**
     * Update the ensemble data with newBookie. re-reads the metadata on
     * MetadataVersionException and update ensemble again. On successfull
     * updation, it will also notify to super call back
     */
    private static class UpdateEnsembleCb implements GenericCallback<Void> {
        final AsyncCallback.VoidCallback ensembleUpdatedCb;
        final LedgerHandle lh;
        final long fragmentStartId;
        final InetSocketAddress oldBookie;
        final InetSocketAddress newBookie;

        public UpdateEnsembleCb(AsyncCallback.VoidCallback ledgerFragmentsMcb,
                long fragmentStartId, LedgerHandle lh,
                InetSocketAddress oldBookie, InetSocketAddress newBookie) {
            this.ensembleUpdatedCb = ledgerFragmentsMcb;
            this.lh = lh;
            this.fragmentStartId = fragmentStartId;
            this.newBookie = newBookie;
            this.oldBookie = oldBookie;
        }

        @Override
        public void operationComplete(int rc, Void result) {
            if (rc == BKException.Code.MetadataVersionException) {
                LOG.warn("Two fragments attempted update at once; ledger id: "
                        + lh.getId() + " startid: " + fragmentStartId);
                // try again, the previous success (with which this has
                // conflicted) will have updated the stat other operations
                // such as (addEnsemble) would update it too.
                lh
                        .rereadMetadata(new OrderedSafeGenericCallback<LedgerMetadata>(
                                lh.bk.mainWorkerPool, lh.getId()) {
                            @Override
                            public void safeOperationComplete(int rc,
                                    LedgerMetadata newMeta) {
                                if (rc != BKException.Code.OK) {
                                    LOG
                                            .error("Error reading updated ledger metadata for ledger "
                                                    + lh.getId());
                                    ensembleUpdatedCb.processResult(rc, null,
                                            null);
                                } else {
                                    lh.metadata = newMeta;
                                    updateEnsembleInfo(ensembleUpdatedCb,
                                            fragmentStartId, lh, oldBookie,
                                            newBookie);
                                }
                            }
                        });
                return;
            } else if (rc != BKException.Code.OK) {
                LOG.error("Error updating ledger config metadata for ledgerId "
                        + lh.getId() + " : " + BKException.getMessage(rc));
            } else {
                LOG.info("Updated ZK for ledgerId: (" + lh.getId() + " : "
                        + fragmentStartId
                        + ") to point ledger fragments from old dead bookie: ("
                        + oldBookie + ") to new bookie: (" + newBookie + ")");
            }
            /*
             * Pass the return code result up the chain with the parent
             * callback.
             */
            ensembleUpdatedCb.processResult(rc, null, null);
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/LedgerHandle.java,true,"package org.apache.bookkeeper.client;

/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

import java.net.InetSocketAddress;
import java.security.GeneralSecurityException;
import java.util.concurrent.ConcurrentLinkedQueue;
import java.util.Arrays;
import java.util.ArrayList;
import java.util.Enumeration;
import java.util.Queue;
import java.util.concurrent.atomic.AtomicInteger;

import org.apache.bookkeeper.client.AsyncCallback.ReadLastConfirmedCallback;
import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.client.AsyncCallback.AddCallback;
import org.apache.bookkeeper.client.AsyncCallback.CloseCallback;
import org.apache.bookkeeper.client.AsyncCallback.ReadCallback;
import org.apache.bookkeeper.client.BKException.BKNotEnoughBookiesException;
import org.apache.bookkeeper.client.BookKeeper.DigestType;
import org.apache.bookkeeper.client.LedgerMetadata;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;
import org.apache.bookkeeper.util.OrderedSafeExecutor.OrderedSafeGenericCallback;

import org.apache.bookkeeper.proto.BookieProtocol;
import org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.State;
import org.apache.bookkeeper.util.SafeRunnable;

import com.google.common.util.concurrent.RateLimiter;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.jboss.netty.buffer.ChannelBuffer;

/**
 * Ledger handle contains ledger metadata and is used to access the read and
 * write operations to a ledger.
 */
public class LedgerHandle {
    final static Logger LOG = LoggerFactory.getLogger(LedgerHandle.class);

    final byte[] ledgerKey;
    LedgerMetadata metadata;
    final BookKeeper bk;
    final long ledgerId;
    long lastAddPushed;
    long lastAddConfirmed;
    long length;
    final DigestManager macManager;
    final DistributionSchedule distributionSchedule;

    final RateLimiter throttler;

    /**
     * Invalid entry id. This value is returned from methods which
     * should return an entry id but there is no valid entry available.
     */
    final static public long INVALID_ENTRY_ID = BookieProtocol.INVALID_ENTRY_ID;

    final AtomicInteger blockAddCompletions = new AtomicInteger(0);
    final Queue<PendingAddOp> pendingAddOps = new ConcurrentLinkedQueue<PendingAddOp>();

    LedgerHandle(BookKeeper bk, long ledgerId, LedgerMetadata metadata,
                 DigestType digestType, byte[] password)
            throws GeneralSecurityException, NumberFormatException {
        this.bk = bk;
        this.metadata = metadata;

        if (metadata.isClosed()) {
            lastAddConfirmed = lastAddPushed = metadata.getLastEntryId();
            length = metadata.getLength();
        } else {
            lastAddConfirmed = lastAddPushed = INVALID_ENTRY_ID;
            length = 0;
        }

        this.ledgerId = ledgerId;

        this.throttler = RateLimiter.create(bk.getConf().getThrottleValue());

        macManager = DigestManager.instantiate(ledgerId, password, digestType);
        this.ledgerKey = MacDigestManager.genDigest("ledger", password);
        distributionSchedule = new RoundRobinDistributionSchedule(
                metadata.getWriteQuorumSize(), metadata.getAckQuorumSize(), metadata.getEnsembleSize());
    }

    /**
     * Get the id of the current ledger
     *
     * @return the id of the ledger
     */
    public long getId() {
        return ledgerId;
    }

    /**
     * Get the last confirmed entry id on this ledger. It reads
     * the local state of the ledger handle, which is different
     * from the readLastConfirmed call. In the case the ledger
     * is not closed and the client is a reader, it is necessary
     * to call readLastConfirmed to obtain an estimate of the
     * last add operation that has been confirmed.
     *
     * @see #readLastConfirmed()
     *
     * @return the last confirmed entry id or {@link #INVALID_ENTRY_ID INVALID_ENTRY_ID} if no entry has been confirmed
     */
    public long getLastAddConfirmed() {
        return lastAddConfirmed;
    }

    /**
     * Get the entry id of the last entry that has been enqueued for addition (but
     * may not have possibly been persited to the ledger)
     *
     * @return the id of the last entry pushed or {@link #INVALID_ENTRY_ID INVALID_ENTRY_ID} if no entry has been pushed
     */
    synchronized public long getLastAddPushed() {
        return lastAddPushed;
    }

    /**
     * Get the Ledger's key/password.
     *
     * @return byte array for the ledger's key/password.
     */
    public byte[] getLedgerKey() {
        return Arrays.copyOf(ledgerKey, ledgerKey.length);
    }

    /**
     * Get the LedgerMetadata
     *
     * @return LedgerMetadata for the LedgerHandle
     */
    LedgerMetadata getLedgerMetadata() {
        return metadata;
    }

    /**
     * Get the DigestManager
     *
     * @return DigestManager for the LedgerHandle
     */
    DigestManager getDigestManager() {
        return macManager;
    }

    /**
     *  Add to the length of the ledger in bytes.
     *
     * @param delta
     * @return
     */
    long addToLength(long delta) {
        this.length += delta;
        return this.length;
    }

    /**
     * Returns the length of the ledger in bytes.
     *
     * @return the length of the ledger in bytes
     */
    synchronized public long getLength() {
        return this.length;
    }

    /**
     * Get the Distribution Schedule
     *
     * @return DistributionSchedule for the LedgerHandle
     */
    DistributionSchedule getDistributionSchedule() {
        return distributionSchedule;
    }

    void writeLedgerConfig(GenericCallback<Void> writeCb) {
        LOG.debug("Writing metadata to ledger manager: {}, {}", this.ledgerId, metadata.getVersion());

        bk.getLedgerManager().writeLedgerMetadata(ledgerId, metadata, writeCb);
    }

    /**
     * Close this ledger synchronously.
     * @see #asyncClose
     */
    public void close() 
            throws InterruptedException, BKException {
        SyncCounter counter = new SyncCounter();
        counter.inc();

        asyncClose(new SyncCloseCallback(), counter);

        counter.block(0);
        if (counter.getrc() != BKException.Code.OK) {
            throw BKException.create(counter.getrc());
        }
    }

    /**
     * Asynchronous close, any adds in flight will return errors.
     * 
     * Closing a ledger will ensure that all clients agree on what the last entry 
     * of the ledger is. This ensures that, once the ledger has been closed, all 
     * reads from the ledger will return the same set of entries. 
     * 
     * @param cb
     *          callback implementation
     * @param ctx
     *          control object
     * @throws InterruptedException
     */
    public void asyncClose(CloseCallback cb, Object ctx) {
        asyncCloseInternal(cb, ctx, BKException.Code.LedgerClosedException);
    }

    /**
     * Same as public version of asyncClose except that this one takes an
     * additional parameter which is the return code to hand to all the pending
     * add ops
     *
     * @param cb
     * @param ctx
     * @param rc
     */
    void asyncCloseInternal(final CloseCallback cb, final Object ctx, final int rc) {
        bk.mainWorkerPool.submitOrdered(ledgerId, new SafeRunnable() {
            @Override
            public void safeRun() {
                final long prevLastEntryId;
                final long prevLength;
                final State prevState;

                synchronized(LedgerHandle.this) {
                    prevState = metadata.getState();
                    prevLastEntryId = metadata.getLastEntryId();
                    prevLength = metadata.getLength();

                    // synchronized on LedgerHandle.this to ensure that 
                    // lastAddPushed can not be updated after the metadata 
                    // is closed. 
                    metadata.setLength(length);

                    // Close operation is idempotent, so no need to check if we are
                    // already closed
                    metadata.close(lastAddConfirmed);
                    errorOutPendingAdds(rc);
                    lastAddPushed = lastAddConfirmed;
                }

                if (LOG.isDebugEnabled()) {
                    LOG.debug("Closing ledger: " + ledgerId + " at entryId: "
                              + metadata.getLastEntryId() + " with this many bytes: " + metadata.getLength());
                }

                final class CloseCb extends OrderedSafeGenericCallback<Void> {
                    CloseCb() {
                        super(bk.mainWorkerPool, ledgerId);
                    }

                    @Override
                    public void safeOperationComplete(final int rc, Void result) {
                        if (rc == BKException.Code.MetadataVersionException) {
                            rereadMetadata(new OrderedSafeGenericCallback<LedgerMetadata>(bk.mainWorkerPool,
                                                                                          ledgerId) {
                                @Override
                                public void safeOperationComplete(int newrc, LedgerMetadata newMeta) {
                                    if (newrc != BKException.Code.OK) {
                                        LOG.error("Error reading new metadata from ledger " + ledgerId
                                                  + " when closing, code=" + newrc);
                                        cb.closeComplete(rc, LedgerHandle.this, ctx);
                                    } else {
                                        metadata.setState(prevState);
                                        if (prevState.equals(State.CLOSED)) {
                                            metadata.close(prevLastEntryId);
                                        }

                                        metadata.setLength(prevLength);
                                        if (metadata.resolveConflict(newMeta)) {
                                            metadata.setLength(length);
                                            metadata.close(lastAddConfirmed);
                                            writeLedgerConfig(new CloseCb());
                                            return;
                                        } else {
                                            metadata.setLength(length);
                                            metadata.close(lastAddConfirmed);
                                            LOG.warn("Conditional update ledger metadata for ledger " + ledgerId + " failed.");
                                            cb.closeComplete(rc, LedgerHandle.this, ctx);
                                        }
                                    }
                                }
                            });
                        } else if (rc != BKException.Code.OK) {
                            LOG.error("Error update ledger metadata for ledger " + ledgerId + " : " + rc);
                            cb.closeComplete(rc, LedgerHandle.this, ctx);
                        } else {
                            cb.closeComplete(BKException.Code.OK, LedgerHandle.this, ctx);
                        }
                    }
                };

                writeLedgerConfig(new CloseCb());

            }
        });
    }

    /**
     * Read a sequence of entries synchronously.
     *
     * @param firstEntry
     *          id of first entry of sequence (included)
     * @param lastEntry
     *          id of last entry of sequence (included)
     *
     */
    public Enumeration<LedgerEntry> readEntries(long firstEntry, long lastEntry)
            throws InterruptedException, BKException {
        SyncCounter counter = new SyncCounter();
        counter.inc();

        asyncReadEntries(firstEntry, lastEntry, new SyncReadCallback(), counter);

        counter.block(0);
        if (counter.getrc() != BKException.Code.OK) {
            throw BKException.create(counter.getrc());
        }

        return counter.getSequence();
    }

    /**
     * Read a sequence of entries asynchronously.
     *
     * @param firstEntry
     *          id of first entry of sequence
     * @param lastEntry
     *          id of last entry of sequence
     * @param cb
     *          object implementing read callback interface
     * @param ctx
     *          control object
     */
    public void asyncReadEntries(long firstEntry, long lastEntry,
                                 ReadCallback cb, Object ctx) {
        // Little sanity check
        if (firstEntry < 0 || lastEntry > lastAddConfirmed
                || firstEntry > lastEntry) {
            cb.readComplete(BKException.Code.ReadException, this, null, ctx);
            return;
        }

        try {
            new PendingReadOp(this, bk.scheduler,
                              firstEntry, lastEntry, cb, ctx).initiate();
        } catch (InterruptedException e) {
            cb.readComplete(BKException.Code.InterruptedException, this, null, ctx);
        }
    }

    /**
     * Add entry synchronously to an open ledger.
     *
     * @param data
     *         array of bytes to be written to the ledger
     * @return the entryId of the new inserted entry
     */
    public long addEntry(byte[] data) throws InterruptedException, BKException {
        return addEntry(data, 0, data.length);
    }

    /**
     * Add entry synchronously to an open ledger.
     *
     * @param data
     *         array of bytes to be written to the ledger
     * @param offset
     *          offset from which to take bytes from data
     * @param length
     *          number of bytes to take from data
     * @return the entryId of the new inserted entry
     */
    public long addEntry(byte[] data, int offset, int length)
            throws InterruptedException, BKException {
        LOG.debug("Adding entry {}", data);

        SyncCounter counter = new SyncCounter();
        counter.inc();

        SyncAddCallback callback = new SyncAddCallback();
        asyncAddEntry(data, offset, length, callback, counter);
        counter.block(0);
        
        if (counter.getrc() != BKException.Code.OK) {
            throw BKException.create(counter.getrc());
        }

        return callback.entryId;
    }

    /**
     * Add entry asynchronously to an open ledger.
     *
     * @param data
     *          array of bytes to be written
     * @param cb
     *          object implementing callbackinterface
     * @param ctx
     *          some control object
     */
    public void asyncAddEntry(final byte[] data, final AddCallback cb,
                              final Object ctx) {
        asyncAddEntry(data, 0, data.length, cb, ctx);
    }

    /**
     * Add entry asynchronously to an open ledger, using an offset and range.
     *
     * @param data
     *          array of bytes to be written
     * @param offset
     *          offset from which to take bytes from data
     * @param length
     *          number of bytes to take from data
     * @param cb
     *          object implementing callbackinterface
     * @param ctx
     *          some control object
     * @throws ArrayIndexOutOfBoundsException if offset or length is negative or
     *          offset and length sum to a value higher than the length of data.
     */
    public void asyncAddEntry(final byte[] data, final int offset, final int length,
                              final AddCallback cb, final Object ctx) {
        PendingAddOp op = new PendingAddOp(LedgerHandle.this, cb, ctx);
        doAsyncAddEntry(op, data, offset, length, cb, ctx);
    }

    /**
     * Make a recovery add entry request. Recovery adds can add to a ledger even if
     * it has been fenced.
     *
     * This is only valid for bookie and ledger recovery, which may need to replicate
     * entries to a quorum of bookies to ensure data safety.
     *
     * Normal client should never call this method.
     */
    void asyncRecoveryAddEntry(final byte[] data, final int offset, final int length,
                               final AddCallback cb, final Object ctx) {
        PendingAddOp op = new PendingAddOp(LedgerHandle.this, cb, ctx).enableRecoveryAdd();
        doAsyncAddEntry(op, data, offset, length, cb, ctx);
    }

    private void doAsyncAddEntry(final PendingAddOp op, final byte[] data, final int offset, final int length,
                                 final AddCallback cb, final Object ctx) {
        if (offset < 0 || length < 0
                || (offset + length) > data.length) {
            throw new ArrayIndexOutOfBoundsException(
                "Invalid values for offset("+offset
                +") or length("+length+")");
        }
        throttler.acquire();

        final long entryId;
        final long currentLength;
        synchronized(this) {
            // synchronized on this to ensure that
            // the ledger isn't closed between checking and 
            // updating lastAddPushed
            if (metadata.isClosed()) {
                LOG.warn("Attempt to add to closed ledger: " + ledgerId);
                cb.addComplete(BKException.Code.LedgerClosedException,
                               LedgerHandle.this, INVALID_ENTRY_ID, ctx);
                return;
            }

            entryId = ++lastAddPushed;
            currentLength = addToLength(length);
            op.setEntryId(entryId);
            pendingAddOps.add(op);
        }

        try {
            bk.mainWorkerPool.submit(new SafeRunnable() {
                @Override
                public void safeRun() {
                    ChannelBuffer toSend = macManager.computeDigestAndPackageForSending(
                                               entryId, lastAddConfirmed, currentLength, data, offset, length);
                    op.initiate(toSend);
                }
            });
        } catch (RuntimeException e) {
            cb.addComplete(BKException.Code.InterruptedException,
                    LedgerHandle.this, INVALID_ENTRY_ID, ctx);
        }
    }

    /**
     * Obtains asynchronously the last confirmed write from a quorum of bookies. This 
     * call obtains the the last add confirmed each bookie has received for this ledger
     * and returns the maximum. If the ledger has been closed, the value returned by this
     * call may not correspond to the id of the last entry of the ledger, since it reads
     * the hint of bookies. Consequently, in the case the ledger has been closed, it may 
     * return a different value than getLastAddConfirmed, which returns the local value 
     * of the ledger handle.
     * 
     * @see #getLastAddConfirmed()
     *
     * @param cb
     * @param ctx
     */

    public void asyncReadLastConfirmed(final ReadLastConfirmedCallback cb, final Object ctx) {
        ReadLastConfirmedOp.LastConfirmedDataCallback innercb = new ReadLastConfirmedOp.LastConfirmedDataCallback() {
                public void readLastConfirmedDataComplete(int rc, DigestManager.RecoveryData data) {
                    if (rc == BKException.Code.OK) {
                        lastAddConfirmed = Math.max(lastAddConfirmed, data.lastAddConfirmed);
                        lastAddPushed = Math.max(lastAddPushed, data.lastAddConfirmed);
                        length = Math.max(length, data.length);
                        cb.readLastConfirmedComplete(rc, data.lastAddConfirmed, ctx);
                    } else {
                        cb.readLastConfirmedComplete(rc, INVALID_ENTRY_ID, ctx);
                    }
                }
            };
        new ReadLastConfirmedOp(this, innercb).initiate();
    }


    /**
     * Context objects for synchronous call to read last confirmed.
     */
    static class LastConfirmedCtx {
        final static long ENTRY_ID_PENDING = -10;
        long response;
        int rc;

        LastConfirmedCtx() {
            this.response = ENTRY_ID_PENDING;
        }

        void setLastConfirmed(long lastConfirmed) {
            this.response = lastConfirmed;
        }

        long getlastConfirmed() {
            return this.response;
        }

        void setRC(int rc) {
            this.rc = rc;
        }

        int getRC() {
            return this.rc;
        }

        boolean ready() {
            return (this.response != ENTRY_ID_PENDING);
        }
    }

    /**
     * Obtains synchronously the last confirmed write from a quorum of bookies. This call
     * obtains the the last add confirmed each bookie has received for this ledger
     * and returns the maximum. If the ledger has been closed, the value returned by this
     * call may not correspond to the id of the last entry of the ledger, since it reads
     * the hint of bookies. Consequently, in the case the ledger has been closed, it may 
     * return a different value than getLastAddConfirmed, which returns the local value 
     * of the ledger handle.
     * 
     * @see #getLastAddConfirmed()
     * 
     * @return The entry id of the last confirmed write or {@link #INVALID_ENTRY_ID INVALID_ENTRY_ID}
     *         if no entry has been confirmed
     * @throws InterruptedException
     * @throws BKException
     */
    
    public long readLastConfirmed()
            throws InterruptedException, BKException {
        LastConfirmedCtx ctx = new LastConfirmedCtx();
        asyncReadLastConfirmed(new SyncReadLastConfirmedCallback(), ctx);
        synchronized(ctx) {
            while(!ctx.ready()) {
                ctx.wait();
            }
        }

        if(ctx.getRC() != BKException.Code.OK) throw BKException.create(ctx.getRC());
        return ctx.getlastConfirmed();
    }

    // close the ledger and send fails to all the adds in the pipeline
    void handleUnrecoverableErrorDuringAdd(int rc) {
        if (metadata.isInRecovery()) {
            // we should not close ledger if ledger is recovery mode
            // otherwise we may lose entry.
            errorOutPendingAdds(rc);
            return;
        }
        asyncCloseInternal(NoopCloseCallback.instance, null, rc);
    }

    void errorOutPendingAdds(int rc) {
        PendingAddOp pendingAddOp;
        while ((pendingAddOp = pendingAddOps.poll()) != null) {
            pendingAddOp.submitCallback(rc);
        }
    }

    void sendAddSuccessCallbacks() {
        // Start from the head of the queue and proceed while there are
        // entries that have had all their responses come back
        PendingAddOp pendingAddOp;
        while ((pendingAddOp = pendingAddOps.peek()) != null
               && blockAddCompletions.get() == 0) {
            if (!pendingAddOp.completed) {
                return;
            }
            pendingAddOps.remove();
            lastAddConfirmed = pendingAddOp.entryId;
            pendingAddOp.submitCallback(BKException.Code.OK);
        }

    }

    ArrayList<InetSocketAddress> replaceBookieInMetadata(final InetSocketAddress addr, final int bookieIndex)
            throws BKException.BKNotEnoughBookiesException {
        InetSocketAddress newBookie;
        LOG.info("Handling failure of bookie: {} index: {}", addr, bookieIndex);
        final ArrayList<InetSocketAddress> newEnsemble = new ArrayList<InetSocketAddress>();
        final long newEnsembleStartEntry = lastAddConfirmed + 1;

        // avoid parallel ensemble changes to same ensemble.
        synchronized (metadata) {
            newBookie = bk.bookieWatcher.getAdditionalBookie(metadata.currentEnsemble);

            newEnsemble.addAll(metadata.currentEnsemble);
            newEnsemble.set(bookieIndex, newBookie);

            if (LOG.isDebugEnabled()) {
                LOG.debug("Changing ensemble from: " + metadata.currentEnsemble
                        + " to: " + newEnsemble + " for ledger: " + ledgerId
                        + " starting at entry: " + (lastAddConfirmed + 1));
            }

            metadata.addEnsemble(newEnsembleStartEntry, newEnsemble);
        }
        return newEnsemble;
    }

    void handleBookieFailure(final InetSocketAddress addr, final int bookieIndex) {
        blockAddCompletions.incrementAndGet();

        synchronized (metadata) {
            if (!metadata.currentEnsemble.get(bookieIndex).equals(addr)) {
                // ensemble has already changed, failure of this addr is immaterial
                LOG.warn("Write did not succeed to {}, bookieIndex {}, but we have already fixed it.",
                         addr, bookieIndex);
                blockAddCompletions.decrementAndGet();
                return;
            }

            try {
                ArrayList<InetSocketAddress> newEnsemble = replaceBookieInMetadata(addr, bookieIndex);

                EnsembleInfo ensembleInfo = new EnsembleInfo(newEnsemble, bookieIndex,
                                                             addr);
                writeLedgerConfig(new ChangeEnsembleCb(ensembleInfo));
            } catch (BKException.BKNotEnoughBookiesException e) {
                LOG.error("Could not get additional bookie to "
                          + "remake ensemble, closing ledger: " + ledgerId);
                handleUnrecoverableErrorDuringAdd(e.getCode());
                return;
            }
        }
    }

    // Contains newly reformed ensemble, bookieIndex, failedBookieAddress
    private static final class EnsembleInfo {
        private final ArrayList<InetSocketAddress> newEnsemble;
        private final int bookieIndex;
        private final InetSocketAddress addr;

        public EnsembleInfo(ArrayList<InetSocketAddress> newEnsemble,
                int bookieIndex, InetSocketAddress addr) {
            this.newEnsemble = newEnsemble;
            this.bookieIndex = bookieIndex;
            this.addr = addr;
        }
    }

    /**
     * Callback which is updating the ledgerMetadata in zk with the newly
     * reformed ensemble. On MetadataVersionException, will reread latest
     * ledgerMetadata and act upon.
     */
    private final class ChangeEnsembleCb extends OrderedSafeGenericCallback<Void> {
        private final EnsembleInfo ensembleInfo;

        ChangeEnsembleCb(EnsembleInfo ensembleInfo) {
            super(bk.mainWorkerPool, ledgerId);
            this.ensembleInfo = ensembleInfo;
        }

        @Override
        public void safeOperationComplete(final int rc, Void result) {
            if (rc == BKException.Code.MetadataVersionException) {
                rereadMetadata(new ReReadLedgerMetadataCb(rc,
                                       ensembleInfo));
                return;
            } else if (rc != BKException.Code.OK) {
                LOG.error("Could not persist ledger metadata while "
                          + "changing ensemble to: "
                          + ensembleInfo.newEnsemble
                          + " , closing ledger");
                handleUnrecoverableErrorDuringAdd(rc);
                return;
            }
            blockAddCompletions.decrementAndGet();

            // the failed bookie has been replaced
            unsetSuccessAndSendWriteRequest(ensembleInfo.bookieIndex);
        }
    };

    /**
     * Callback which is reading the ledgerMetadata present in zk. This will try
     * to resolve the version conflicts.
     */
    private final class ReReadLedgerMetadataCb extends OrderedSafeGenericCallback<LedgerMetadata> {
        private final int rc;
        private final EnsembleInfo ensembleInfo;

        ReReadLedgerMetadataCb(int rc, EnsembleInfo ensembleInfo) {
            super(bk.mainWorkerPool, ledgerId);
            this.rc = rc;
            this.ensembleInfo = ensembleInfo;
        }

        @Override
        public void safeOperationComplete(int newrc, LedgerMetadata newMeta) {
            if (newrc != BKException.Code.OK) {
                LOG.error("Error reading new metadata from ledger "
                        + "after changing ensemble, code=" + newrc);
                handleUnrecoverableErrorDuringAdd(rc);
            } else {
                if (!resolveConflict(newMeta)) {
                    LOG.error("Could not resolve ledger metadata conflict "
                            + "while changing ensemble to: "
                            + ensembleInfo.newEnsemble
                            + ", old meta data is \n"
                            + new String(metadata.serialize())
                            + "\n, new meta data is \n"
                            + new String(newMeta.serialize())
                            + "\n ,closing ledger");
                    handleUnrecoverableErrorDuringAdd(rc);
                }
            }
        }

        /**
         * Resolving the version conflicts between local ledgerMetadata and zk
         * ledgerMetadata. This will do the following:
         * <ul>
         * <li>
         * check whether ledgerMetadata state matches of local and zk</li>
         * <li>
         * if the zk ledgerMetadata still contains the failed bookie, then
         * update zookeeper with the newBookie otherwise send write request</li>
         * </ul>
         */
        private boolean resolveConflict(LedgerMetadata newMeta) {
            // close have changed, another client has opened
            // the ledger, can't resolve this conflict.
            if (metadata.getState() != newMeta.getState()) {
                return false;
            }
            // update znode version
            metadata.setVersion(newMeta.getVersion());
            // Resolve the conflicts if zk metadata still contains failed
            // bookie.
            if (newMeta.currentEnsemble.get(ensembleInfo.bookieIndex).equals(
                    ensembleInfo.addr)) {
                // Update ledger metadata in zk, if in-memory metadata doesn't
                // contains the failed bookie.
                if (!metadata.currentEnsemble.get(ensembleInfo.bookieIndex)
                        .equals(ensembleInfo.addr)) {
                    LOG.info("Resolve ledger metadata conflict "
                            + "while changing ensemble to: "
                            + ensembleInfo.newEnsemble
                            + ", old meta data is \n"
                            + new String(metadata.serialize())
                            + "\n, new meta data is \n"
                            + new String(newMeta.serialize()));
                    writeLedgerConfig(new ChangeEnsembleCb(ensembleInfo));
                }
            } else {
                // the failed bookie has been replaced
                blockAddCompletions.decrementAndGet();
                unsetSuccessAndSendWriteRequest(ensembleInfo.bookieIndex);
            }
            return true;
        }

    };

    void unsetSuccessAndSendWriteRequest(final int bookieIndex) {
        for (PendingAddOp pendingAddOp : pendingAddOps) {
            pendingAddOp.unsetSuccessAndSendWriteRequest(bookieIndex);
        }
    }

    void rereadMetadata(final GenericCallback<LedgerMetadata> cb) {
        bk.getLedgerManager().readLedgerMetadata(ledgerId, cb);
    }

    synchronized void recover(final GenericCallback<Void> cb) {
        if (metadata.isClosed()) {
            lastAddConfirmed = lastAddPushed = metadata.getLastEntryId();
            length = metadata.getLength();

            // We are already closed, nothing to do
            cb.operationComplete(BKException.Code.OK, null);
            return;
        }

        // if metadata is already in recover, dont try to write again,
        // just do the recovery from the starting point
        if (metadata.isInRecovery()) {
            new LedgerRecoveryOp(LedgerHandle.this, cb).initiate();
            return;
        }

        metadata.markLedgerInRecovery();

        writeLedgerConfig(new OrderedSafeGenericCallback<Void>(bk.mainWorkerPool, ledgerId) {
            @Override
            public void safeOperationComplete(final int rc, Void result) {
                if (rc == BKException.Code.MetadataVersionException) {
                    rereadMetadata(new OrderedSafeGenericCallback<LedgerMetadata>(bk.mainWorkerPool,
                                                                                  ledgerId) {
                        @Override
                        public void safeOperationComplete(int rc, LedgerMetadata newMeta) {
                            if (rc != BKException.Code.OK) {
                                cb.operationComplete(rc, null);
                            } else {
                                metadata = newMeta;
                                recover(cb);
                            }
                        }
                    });
                } else if (rc == BKException.Code.OK) {
                    new LedgerRecoveryOp(LedgerHandle.this, cb).initiate();
                } else {
                    LOG.error("Error writing ledger config " + rc + " of ledger " + ledgerId);
                    cb.operationComplete(rc, null);
                }
            }
        });
    }

    static class NoopCloseCallback implements CloseCallback {
        static NoopCloseCallback instance = new NoopCloseCallback();

        @Override
        public void closeComplete(int rc, LedgerHandle lh, Object ctx) {
            if (rc != BKException.Code.OK) {
                LOG.warn("Close failed: " + BKException.getMessage(rc));
            }
            // noop
        }
    }
    
    private static class SyncReadCallback implements ReadCallback {
        /**
         * Implementation of callback interface for synchronous read method.
         *
         * @param rc
         *          return code
         * @param leder
         *          ledger identifier
         * @param seq
         *          sequence of entries
         * @param ctx
         *          control object
         */
        public void readComplete(int rc, LedgerHandle lh,
                                 Enumeration<LedgerEntry> seq, Object ctx) {
            
            SyncCounter counter = (SyncCounter) ctx;
            synchronized (counter) {
                counter.setSequence(seq);
                counter.setrc(rc);
                counter.dec();
                counter.notify();
            }
        }
    }

    private static class SyncAddCallback implements AddCallback {
        long entryId = -1;

        /**
         * Implementation of callback interface for synchronous read method.
         *
         * @param rc
         *          return code
         * @param leder
         *          ledger identifier
         * @param entry
         *          entry identifier
         * @param ctx
         *          control object
         */
        public void addComplete(int rc, LedgerHandle lh, long entry, Object ctx) {
            SyncCounter counter = (SyncCounter) ctx;

            this.entryId = entry;
            counter.setrc(rc);
            counter.dec();
        }
    }

    private static class SyncReadLastConfirmedCallback implements ReadLastConfirmedCallback {
        /**
         * Implementation of  callback interface for synchronous read last confirmed method.
         */
        public void readLastConfirmedComplete(int rc, long lastConfirmed, Object ctx) {
            LastConfirmedCtx lcCtx = (LastConfirmedCtx) ctx;
            
            synchronized(lcCtx) {
                lcCtx.setRC(rc);
                lcCtx.setLastConfirmed(lastConfirmed);
                lcCtx.notify();
            }
        }
    }

    private static class SyncCloseCallback implements CloseCallback {
        /**
         * Close callback method
         *
         * @param rc
         * @param lh
         * @param ctx
         */
        public void closeComplete(int rc, LedgerHandle lh, Object ctx) {
            SyncCounter counter = (SyncCounter) ctx;
            counter.setrc(rc);
            synchronized (counter) {
                counter.dec();
                counter.notify();
            }
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/LedgerMetadata.java,true,"package org.apache.bookkeeper.client;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.BufferedReader;
import java.io.StringReader;
import java.io.IOException;
import java.net.InetSocketAddress;
import java.util.ArrayList;
import java.util.Iterator;
import java.util.Map;
import java.util.SortedMap;
import java.util.TreeMap;
import java.util.Arrays;

import org.apache.bookkeeper.versioning.Version;
import com.google.protobuf.TextFormat;
import com.google.protobuf.ByteString;
import org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat;
import org.apache.bookkeeper.util.StringUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * This class encapsulates all the ledger metadata that is persistently stored
 * in zookeeper. It provides parsing and serialization methods of such metadata.
 *
 */
public class LedgerMetadata {
    static final Logger LOG = LoggerFactory.getLogger(LedgerMetadata.class);

    private static final String closed = "CLOSED";
    private static final String lSplitter = "\n";
    private static final String tSplitter = "\t";

    // can't use -1 for NOTCLOSED because that is reserved for a closed, empty
    // ledger
    private static final int NOTCLOSED = -101;
    private static final int IN_RECOVERY = -102;

    public static final int LOWEST_COMPAT_METADATA_FORMAT_VERSION = 0;
    public static final int CURRENT_METADATA_FORMAT_VERSION = 2;
    public static final String VERSION_KEY = "BookieMetadataFormatVersion";

    private int metadataFormatVersion = 0;

    private int ensembleSize;
    private int writeQuorumSize;
    private int ackQuorumSize;
    private long length;
    private long lastEntryId;

    private LedgerMetadataFormat.State state;
    private SortedMap<Long, ArrayList<InetSocketAddress>> ensembles = new TreeMap<Long, ArrayList<InetSocketAddress>>();
    ArrayList<InetSocketAddress> currentEnsemble;
    volatile Version version = Version.NEW;

    private boolean hasPassword = false;
    private LedgerMetadataFormat.DigestType digestType;
    private byte[] password;

    public LedgerMetadata(int ensembleSize, int writeQuorumSize, int ackQuorumSize,
                          BookKeeper.DigestType digestType, byte[] password) {
        this.ensembleSize = ensembleSize;
        this.writeQuorumSize = writeQuorumSize;
        this.ackQuorumSize = ackQuorumSize;

        /*
         * It is set in PendingReadOp.readEntryComplete, and
         * we read it in LedgerRecoveryOp.readComplete.
         */
        this.length = 0;
        this.state = LedgerMetadataFormat.State.OPEN;
        this.lastEntryId = LedgerHandle.INVALID_ENTRY_ID;
        this.metadataFormatVersion = CURRENT_METADATA_FORMAT_VERSION;

        this.digestType = digestType.equals(BookKeeper.DigestType.MAC) ?
            LedgerMetadataFormat.DigestType.HMAC : LedgerMetadataFormat.DigestType.CRC32;
        this.password = Arrays.copyOf(password, password.length);
        this.hasPassword = true;
    }

    private LedgerMetadata() {
        this(0, 0, 0, BookKeeper.DigestType.MAC, new byte[] {});
        this.hasPassword = false;
    }

    /**
     * Get the Map of bookie ensembles for the various ledger fragments
     * that make up the ledger.
     *
     * @return SortedMap of Ledger Fragments and the corresponding
     * bookie ensembles that store the entries.
     */
    public SortedMap<Long, ArrayList<InetSocketAddress>> getEnsembles() {
        return ensembles;
    }

    public int getEnsembleSize() {
        return ensembleSize;
    }

    public int getWriteQuorumSize() {
        return writeQuorumSize;
    }

    public int getAckQuorumSize() {
        return ackQuorumSize;
    }

    /**
     * In versions 4.1.0 and below, the digest type and password were not
     * stored in the metadata.
     *
     * @return whether the password has been stored in the metadata
     */
    boolean hasPassword() {
        return hasPassword;
    }

    byte[] getPassword() {
        return Arrays.copyOf(password, password.length);
    }

    BookKeeper.DigestType getDigestType() {
        if (digestType.equals(LedgerMetadataFormat.DigestType.HMAC)) {
            return BookKeeper.DigestType.MAC;
        } else {
            return BookKeeper.DigestType.CRC32;
        }
    }

    public long getLastEntryId() {
        return lastEntryId;
    }

    public long getLength() {
        return length;
    }

    void setLength(long length) {
        this.length = length;
    }

    public boolean isClosed() {
        return state == LedgerMetadataFormat.State.CLOSED;
    }

    public boolean isInRecovery() {
        return state == LedgerMetadataFormat.State.IN_RECOVERY;
    }

    LedgerMetadataFormat.State getState() {
        return state;
    }

    void setState(LedgerMetadataFormat.State state) {
        this.state = state;
    }

    void markLedgerInRecovery() {
        state = LedgerMetadataFormat.State.IN_RECOVERY;
    }

    void close(long entryId) {
        lastEntryId = entryId;
        state = LedgerMetadataFormat.State.CLOSED;
    }

    void addEnsemble(long startEntryId, ArrayList<InetSocketAddress> ensemble) {
        assert ensembles.isEmpty() || startEntryId >= ensembles.lastKey();

        ensembles.put(startEntryId, ensemble);
        currentEnsemble = ensemble;
    }

    ArrayList<InetSocketAddress> getEnsemble(long entryId) {
        // the head map cannot be empty, since we insert an ensemble for
        // entry-id 0, right when we start
        return ensembles.get(ensembles.headMap(entryId + 1).lastKey());
    }

    /**
     * the entry id > the given entry-id at which the next ensemble change takes
     * place ( -1 if no further ensemble changes)
     *
     * @param entryId
     * @return
     */
    long getNextEnsembleChange(long entryId) {
        SortedMap<Long, ArrayList<InetSocketAddress>> tailMap = ensembles.tailMap(entryId + 1);

        if (tailMap.isEmpty()) {
            return -1;
        } else {
            return tailMap.firstKey();
        }
    }

    /**
     * Generates a byte array of this object
     *
     * @return the metadata serialized into a byte array
     */
    public byte[] serialize() {
        if (metadataFormatVersion == 1) {
            return serializeVersion1();
        }
        LedgerMetadataFormat.Builder builder = LedgerMetadataFormat.newBuilder();
        builder.setQuorumSize(writeQuorumSize).setAckQuorumSize(ackQuorumSize)
            .setEnsembleSize(ensembleSize).setLength(length)
            .setState(state).setLastEntryId(lastEntryId);

        if (hasPassword) {
            builder.setDigestType(digestType).setPassword(ByteString.copyFrom(password));
        }

        for (Map.Entry<Long, ArrayList<InetSocketAddress>> entry : ensembles.entrySet()) {
            LedgerMetadataFormat.Segment.Builder segmentBuilder = LedgerMetadataFormat.Segment.newBuilder();
            segmentBuilder.setFirstEntryId(entry.getKey());
            for (InetSocketAddress addr : entry.getValue()) {
                segmentBuilder.addEnsembleMember(addr.getAddress().getHostAddress() + ":" + addr.getPort());
            }
            builder.addSegment(segmentBuilder.build());
        }

        StringBuilder s = new StringBuilder();
        s.append(VERSION_KEY).append(tSplitter).append(CURRENT_METADATA_FORMAT_VERSION).append(lSplitter);
        s.append(TextFormat.printToString(builder.build()));
        LOG.debug("Serialized config: {}", s);
        return s.toString().getBytes();
    }

    private byte[] serializeVersion1() {
        StringBuilder s = new StringBuilder();
        s.append(VERSION_KEY).append(tSplitter).append(metadataFormatVersion).append(lSplitter);
        s.append(writeQuorumSize).append(lSplitter).append(ensembleSize).append(lSplitter).append(length);

        for (Map.Entry<Long, ArrayList<InetSocketAddress>> entry : ensembles.entrySet()) {
            s.append(lSplitter).append(entry.getKey());
            for (InetSocketAddress addr : entry.getValue()) {
                s.append(tSplitter);
                s.append(StringUtils.addrToString(addr));
            }
        }

        if (state == LedgerMetadataFormat.State.IN_RECOVERY) {
            s.append(lSplitter).append(IN_RECOVERY).append(tSplitter).append(closed);
        } else if (state == LedgerMetadataFormat.State.CLOSED) {
            s.append(lSplitter).append(getLastEntryId()).append(tSplitter).append(closed);
        }

        LOG.debug("Serialized config: {}", s);

        return s.toString().getBytes();
    }

    /**
     * Parses a given byte array and transforms into a LedgerConfig object
     *
     * @param bytes
     *            byte array to parse
     * @param version
     *            version of the ledger metadata
     * @return LedgerConfig
     * @throws IOException
     *             if the given byte[] cannot be parsed
     */
    public static LedgerMetadata parseConfig(byte[] bytes, Version version) throws IOException {
        LedgerMetadata lc = new LedgerMetadata();
        lc.version = version;

        String config = new String(bytes);

        LOG.debug("Parsing Config: {}", config);
        BufferedReader reader = new BufferedReader(new StringReader(config));
        String versionLine = reader.readLine();
        if (versionLine == null) {
            throw new IOException("Invalid metadata. Content missing");
        }
        int i = 0;
        if (versionLine.startsWith(VERSION_KEY)) {
            String parts[] = versionLine.split(tSplitter);
            lc.metadataFormatVersion = new Integer(parts[1]);
        } else {
            // if no version is set, take it to be version 1
            // as the parsing is the same as what we had before
            // we introduce versions
            lc.metadataFormatVersion = 1;
            // reset the reader
            reader.close();
            reader = new BufferedReader(new StringReader(config));
        }

        if (lc.metadataFormatVersion < LOWEST_COMPAT_METADATA_FORMAT_VERSION
            || lc.metadataFormatVersion > CURRENT_METADATA_FORMAT_VERSION) {
            throw new IOException("Metadata version not compatible. Expected between "
                    + LOWEST_COMPAT_METADATA_FORMAT_VERSION + " and " + CURRENT_METADATA_FORMAT_VERSION
                                  + ", but got " + lc.metadataFormatVersion);
        }

        if (lc.metadataFormatVersion == 1) {
            return parseVersion1Config(lc, reader);
        }

        LedgerMetadataFormat.Builder builder = LedgerMetadataFormat.newBuilder();
        TextFormat.merge(reader, builder);
        LedgerMetadataFormat data = builder.build();
        lc.writeQuorumSize = data.getQuorumSize();
        if (data.hasAckQuorumSize()) {
            lc.ackQuorumSize = data.getAckQuorumSize();
        } else {
            lc.ackQuorumSize = lc.writeQuorumSize;
        }

        lc.ensembleSize = data.getEnsembleSize();
        lc.length = data.getLength();
        lc.state = data.getState();
        lc.lastEntryId = data.getLastEntryId();

        if (data.hasPassword()) {
            lc.digestType = data.getDigestType();
            lc.password = data.getPassword().toByteArray();
            lc.hasPassword = true;
        }

        for (LedgerMetadataFormat.Segment s : data.getSegmentList()) {
            ArrayList<InetSocketAddress> addrs = new ArrayList<InetSocketAddress>();
            for (String member : s.getEnsembleMemberList()) {
                addrs.add(StringUtils.parseAddr(member));
            }
            lc.addEnsemble(s.getFirstEntryId(), addrs);
        }
        return lc;
    }

    static LedgerMetadata parseVersion1Config(LedgerMetadata lc,
                                              BufferedReader reader) throws IOException {
        try {
            lc.writeQuorumSize = lc.ackQuorumSize = new Integer(reader.readLine());
            lc.ensembleSize = new Integer(reader.readLine());
            lc.length = new Long(reader.readLine());

            String line = reader.readLine();
            while (line != null) {
                String parts[] = line.split(tSplitter);

                if (parts[1].equals(closed)) {
                    Long l = new Long(parts[0]);
                    if (l == IN_RECOVERY) {
                        lc.state = LedgerMetadataFormat.State.IN_RECOVERY;
                    } else {
                        lc.state = LedgerMetadataFormat.State.CLOSED;
                        lc.lastEntryId = l;
                    }
                    break;
                } else {
                    lc.state = LedgerMetadataFormat.State.OPEN;
                }

                ArrayList<InetSocketAddress> addrs = new ArrayList<InetSocketAddress>();
                for (int j = 1; j < parts.length; j++) {
                    addrs.add(StringUtils.parseAddr(parts[j]));
                }
                lc.addEnsemble(new Long(parts[0]), addrs);
                line = reader.readLine();
            }
        } catch (NumberFormatException e) {
            throw new IOException(e);
        }
        return lc;
    }

    /**
     * Updates the version of this metadata.
     * 
     * @param v Version
     */
    public void setVersion(Version v) {
        this.version = v;
    }

    /**
     * Returns the last version.
     * 
     * @return version
     */
    public Version getVersion() {
        return this.version;
    }

    /**
     * Resolve conflict with new updated metadata.
     *
     * @param newMeta
     *          Re-read metadata
     * @return true if the conflict has been resolved, otherwise false.
     */
    boolean resolveConflict(LedgerMetadata newMeta) {
        /*
         *  if length & close have changed, then another client has
         *  opened the ledger, can't resolve this conflict.
         */

        if (metadataFormatVersion != newMeta.metadataFormatVersion ||
            ensembleSize != newMeta.ensembleSize ||
            writeQuorumSize != newMeta.writeQuorumSize ||
            ackQuorumSize != newMeta.ackQuorumSize ||
            length != newMeta.length ||
            state != newMeta.state ||
            !digestType.equals(newMeta.digestType) ||
            !Arrays.equals(password, newMeta.password)) {
            return false;
        }
        if (state == LedgerMetadataFormat.State.CLOSED
            && lastEntryId != newMeta.lastEntryId) {
            return false;
        }
        // new meta znode version should be larger than old one
        if (null != version &&
            Version.Occurred.AFTER == version.compare(newMeta.version)) {
            return false;
        }
        // if ledger is closed, we can just take the new ensembles
        if (newMeta.state != LedgerMetadataFormat.State.CLOSED) {
            // ensemble size should be same
            if (ensembles.size() != newMeta.ensembles.size()) {
                return false;
            }
            // ensemble distribution should be same
            // we don't check the detail ensemble, since new bookie will be set
            // using recovery tool.
            Iterator<Long> keyIter = ensembles.keySet().iterator();
            Iterator<Long> newMetaKeyIter = newMeta.ensembles.keySet().iterator();
            for (int i=0; i<ensembles.size(); i++) {
                Long curKey = keyIter.next();
                Long newMetaKey = newMetaKeyIter.next();
                if (!curKey.equals(newMetaKey)) {
                    return false;
                }
            }
        }
        /*
         *  if the conflict has been resolved, then update
         *  ensemble and znode version
         */
        ensembles = newMeta.ensembles;
        version = newMeta.version;
        return true;
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/LedgerOpenOp.java,true,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.client;

import java.util.Arrays;
import java.security.GeneralSecurityException;

import org.apache.bookkeeper.client.AsyncCallback.OpenCallback;
import org.apache.bookkeeper.client.AsyncCallback.ReadLastConfirmedCallback;
import org.apache.bookkeeper.client.BookKeeper.DigestType;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;
import org.apache.bookkeeper.util.OrderedSafeExecutor.OrderedSafeGenericCallback;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Encapsulates the ledger open operation
 *
 */
class LedgerOpenOp implements GenericCallback<LedgerMetadata> {
    static final Logger LOG = LoggerFactory.getLogger(LedgerOpenOp.class);

    final BookKeeper bk;
    final long ledgerId;
    final OpenCallback cb;
    final Object ctx;
    LedgerHandle lh;
    final byte[] passwd;
    final DigestType digestType;
    boolean doRecovery = true;
    boolean administrativeOpen = false;

    /**
     * Constructor.
     *
     * @param bk
     * @param ledgerId
     * @param digestType
     * @param passwd
     * @param cb
     * @param ctx
     */
    public LedgerOpenOp(BookKeeper bk, long ledgerId, DigestType digestType, byte[] passwd,
                        OpenCallback cb, Object ctx) {
        this.bk = bk;
        this.ledgerId = ledgerId;
        this.passwd = passwd;
        this.cb = cb;
        this.ctx = ctx;
        this.digestType = digestType;
    }

    public LedgerOpenOp(BookKeeper bk, long ledgerId, OpenCallback cb, Object ctx) {
        this.bk = bk;
        this.ledgerId = ledgerId;
        this.cb = cb;
        this.ctx = ctx;

        this.passwd = bk.getConf().getBookieRecoveryPasswd();
        this.digestType = bk.getConf().getBookieRecoveryDigestType();
        this.administrativeOpen = true;
    }

    /**
     * Inititates the ledger open operation
     */
    public void initiate() {
        /**
         * Asynchronously read the ledger metadata node.
         */
        bk.getLedgerManager().readLedgerMetadata(ledgerId, this);
    }

    /**
     * Inititates the ledger open operation without recovery
     */
    public void initiateWithoutRecovery() {
        this.doRecovery = false;
        initiate();
    }

    /**
     * Implements Open Ledger Callback.
     */
    public void operationComplete(int rc, LedgerMetadata metadata) {
        if (BKException.Code.OK != rc) {
            // open ledger failed.
            cb.openComplete(rc, null, this.ctx);
            return;
        }

        final byte[] passwd;
        final DigestType digestType;

        /* For an administrative open, the default passwords
         * are read from the configuration, but if the metadata
         * already contains passwords, use these instead. */
        if (administrativeOpen && metadata.hasPassword()) {
            passwd = metadata.getPassword();
            digestType = metadata.getDigestType();
        } else {
            passwd = this.passwd;
            digestType = this.digestType;

            if (metadata.hasPassword()) {
                if (!Arrays.equals(passwd, metadata.getPassword())) {
                    LOG.error("Provided passwd does not match that in metadata");
                    cb.openComplete(BKException.Code.UnauthorizedAccessException, null, this.ctx);
                    return;
                }
                if (digestType != metadata.getDigestType()) {
                    LOG.error("Provided digest does not match that in metadata");
                    cb.openComplete(BKException.Code.DigestMatchException, null, this.ctx);
                    return;
                }
            }
        }

        // get the ledger metadata back
        try {
            lh = new ReadOnlyLedgerHandle(bk, ledgerId, metadata, digestType, passwd);
        } catch (GeneralSecurityException e) {
            LOG.error("Security exception while opening ledger: " + ledgerId, e);
            cb.openComplete(BKException.Code.DigestNotInitializedException, null, this.ctx);
            return;
        } catch (NumberFormatException e) {
            LOG.error("Incorrectly entered parameter throttle: " + bk.getConf().getThrottleValue(), e);
            cb.openComplete(BKException.Code.IncorrectParameterException, null, this.ctx);
            return;
        }

        if (metadata.isClosed()) {
            // Ledger was closed properly
            cb.openComplete(BKException.Code.OK, lh, this.ctx);
            return;
        }

        if (doRecovery) {
            lh.recover(new OrderedSafeGenericCallback<Void>(bk.mainWorkerPool, ledgerId) {
                    @Override
                    public void safeOperationComplete(int rc, Void result) {
                        if (rc == BKException.Code.OK) {
                            cb.openComplete(BKException.Code.OK, lh, LedgerOpenOp.this.ctx);
                        } else if (rc == BKException.Code.UnauthorizedAccessException) {
                            cb.openComplete(BKException.Code.UnauthorizedAccessException, null, LedgerOpenOp.this.ctx);
                        } else {
                            cb.openComplete(BKException.Code.LedgerRecoveryException, null, LedgerOpenOp.this.ctx);
                        }
                    }
                });
        } else {
            lh.asyncReadLastConfirmed(new ReadLastConfirmedCallback() {

                @Override
                public void readLastConfirmedComplete(int rc,
                        long lastConfirmed, Object ctx) {
                    if (rc != BKException.Code.OK) {
                        cb.openComplete(BKException.Code.ReadException, null, LedgerOpenOp.this.ctx);
                    } else {
                        lh.lastAddConfirmed = lh.lastAddPushed = lastConfirmed;
                        cb.openComplete(BKException.Code.OK, lh, LedgerOpenOp.this.ctx);
                    }
                }
                
            }, null);
            
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/LedgerRecoveryOp.java,true,"package org.apache.bookkeeper.client;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.util.Enumeration;

import org.apache.bookkeeper.client.AsyncCallback.AddCallback;
import org.apache.bookkeeper.client.AsyncCallback.CloseCallback;
import org.apache.bookkeeper.client.AsyncCallback.ReadCallback;
import org.apache.bookkeeper.client.AsyncCallback.ReadLastConfirmedCallback;
import org.apache.bookkeeper.client.BKException.BKDigestMatchException;
import org.apache.bookkeeper.client.LedgerHandle.NoopCloseCallback;
import org.apache.bookkeeper.client.DigestManager.RecoveryData;
import org.apache.bookkeeper.proto.BookieProtocol;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;

import org.apache.zookeeper.KeeperException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.buffer.ChannelBuffer;

/**
 * This class encapsulated the ledger recovery operation. It first does a read
 * with entry-id of -1 (BookieProtocol.LAST_ADD_CONFIRMED) to all bookies. Then
 * starting from the last confirmed entry (from hints in the ledger entries),
 * it reads forward until it is not able to find a particular entry. It closes
 * the ledger at that entry.
 *
 */
class LedgerRecoveryOp implements ReadCallback, AddCallback {
    static final Logger LOG = LoggerFactory.getLogger(LedgerRecoveryOp.class);
    LedgerHandle lh;
    int numResponsesPending;
    boolean proceedingWithRecovery = false;
    long maxAddPushed = LedgerHandle.INVALID_ENTRY_ID;
    long maxAddConfirmed = LedgerHandle.INVALID_ENTRY_ID;
    long maxLength = 0;

    GenericCallback<Void> cb;

    public LedgerRecoveryOp(LedgerHandle lh, GenericCallback<Void> cb) {
        this.cb = cb;
        this.lh = lh;
        numResponsesPending = lh.metadata.getEnsembleSize();
    }

    public void initiate() {
        ReadLastConfirmedOp rlcop = new ReadLastConfirmedOp(lh,
                new ReadLastConfirmedOp.LastConfirmedDataCallback() {
                public void readLastConfirmedDataComplete(int rc, RecoveryData data) {
                    if (rc == BKException.Code.OK) {
                        lh.lastAddPushed = lh.lastAddConfirmed = data.lastAddConfirmed;
                        lh.length = data.length;
                        doRecoveryRead();
                    } else if (rc == BKException.Code.UnauthorizedAccessException) {
                        cb.operationComplete(rc, null);
                    } else {
                        cb.operationComplete(BKException.Code.ReadException, null);
                    }
                }
                });

        /**
         * Enable fencing on this op. When the read request reaches the bookies
         * server it will fence off the ledger, stopping any subsequent operation
         * from writing to it.
         */
        rlcop.initiateWithFencing();
    }

    /**
     * Try to read past the last confirmed.
     */
    private void doRecoveryRead() {
        lh.lastAddConfirmed++;
        lh.asyncReadEntries(lh.lastAddConfirmed, lh.lastAddConfirmed, this, null);
    }

    @Override
    public void readComplete(int rc, LedgerHandle lh, Enumeration<LedgerEntry> seq, Object ctx) {
        // get back to prev value
        lh.lastAddConfirmed--;
        if (rc == BKException.Code.OK) {
            LedgerEntry entry = seq.nextElement();
            byte[] data = entry.getEntry();

            /*
             * We will add this entry again to make sure it is written to enough
             * replicas. We subtract the length of the data itself, since it will
             * be added again when processing the call to add it.
             */
            synchronized (lh) {
                lh.length = entry.getLength() - (long) data.length;
            }
            lh.asyncRecoveryAddEntry(data, 0, data.length, this, null);
            return;
        }

        if (rc == BKException.Code.NoSuchEntryException || rc == BKException.Code.NoSuchLedgerExistsException) {
            lh.asyncCloseInternal(new CloseCallback() {
                @Override
                public void closeComplete(int rc, LedgerHandle lh, Object ctx) {
                    if (rc != KeeperException.Code.OK.intValue()) {
                        LOG.warn("Close failed: " + BKException.getMessage(rc));
                        cb.operationComplete(BKException.Code.ZKException, null);
                    } else {
                        cb.operationComplete(BKException.Code.OK, null);
                        LOG.debug("After closing length is: {}", lh.getLength());
                    }
                }
                }, null, BKException.Code.LedgerClosedException);
            return;
        }

        // otherwise, some other error, we can't handle
        LOG.error("Failure " + BKException.getMessage(rc) + " while reading entry: " + (lh.lastAddConfirmed + 1)
                  + " ledger: " + lh.ledgerId + " while recovering ledger");
        cb.operationComplete(rc, null);
        return;
    }

    @Override
    public void addComplete(int rc, LedgerHandle lh, long entryId, Object ctx) {
        if (rc != BKException.Code.OK) {
            // Give up, we can't recover from this error

            LOG.error("Failure " + BKException.getMessage(rc) + " while writing entry: " + (lh.lastAddConfirmed + 1)
                      + " ledger: " + lh.ledgerId + " while recovering ledger");
            cb.operationComplete(rc, null);
            return;
        }

        doRecoveryRead();

    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/MacDigestManager.java,false,"package org.apache.bookkeeper.client;

/*
* Licensed to the Apache Software Foundation (ASF) under one
* or more contributor license agreements.  See the NOTICE file
* distributed with this work for additional information
* regarding copyright ownership.  The ASF licenses this file
* to you under the Apache License, Version 2.0 (the
* "License"); you may not use this file except in compliance
* with the License.  You may obtain a copy of the License at
*
*     http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*/

import java.security.GeneralSecurityException;
import java.security.MessageDigest;
import java.security.NoSuchAlgorithmException;

import javax.crypto.Mac;
import javax.crypto.spec.SecretKeySpec;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

class MacDigestManager extends DigestManager {
    final static Logger LOG = LoggerFactory.getLogger(MacDigestManager.class);

    public static String DIGEST_ALGORITHM = "SHA-1";
    public static String KEY_ALGORITHM = "HmacSHA1";

    final byte[] passwd;

    private final ThreadLocal<Mac> mac = new ThreadLocal<Mac>() {
        @Override
        protected Mac initialValue() {
            try {
                byte[] macKey = genDigest("mac", passwd);
                SecretKeySpec keySpec = new SecretKeySpec(macKey, KEY_ALGORITHM);
                Mac mac = Mac.getInstance(KEY_ALGORITHM);
                mac.init(keySpec);
                return mac;
            } catch (GeneralSecurityException gse) {
                LOG.error("Couldn't not get mac instance", gse);
                return null;
            }
        }
    };

    public MacDigestManager(long ledgerId, byte[] passwd) throws GeneralSecurityException {
        super(ledgerId);
        this.passwd = passwd;
    }

    static byte[] genDigest(String pad, byte[] passwd) throws NoSuchAlgorithmException {
        MessageDigest digest = MessageDigest.getInstance(DIGEST_ALGORITHM);
        digest.update(pad.getBytes());
        digest.update(passwd);
        return digest.digest();
    }

    @Override
    int getMacCodeLength() {
        return 20;
    }


    @Override
    byte[] getValueAndReset() {
        return mac.get().doFinal();
    }

    @Override
    void update(byte[] data, int offset, int length) {
        mac.get().update(data, offset, length);
    }


}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/PendingAddOp.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.client;

import java.util.HashSet;
import java.util.Set;
import java.net.InetSocketAddress;
import org.apache.bookkeeper.client.AsyncCallback.AddCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.WriteCallback;
import org.apache.bookkeeper.proto.BookieProtocol;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.buffer.ChannelBuffer;

/**
 * This represents a pending add operation. When it has got success from all
 * bookies, it sees if its at the head of the pending adds queue, and if yes,
 * sends ack back to the application. If a bookie fails, a replacement is made
 * and placed at the same position in the ensemble. The pending adds are then
 * rereplicated.
 *
 *
 */
class PendingAddOp implements WriteCallback {
    final static Logger LOG = LoggerFactory.getLogger(PendingAddOp.class);

    ChannelBuffer toSend;
    AddCallback cb;
    Object ctx;
    long entryId;
    Set<Integer> writeSet;

    DistributionSchedule.AckSet ackSet;
    boolean completed = false;

    LedgerHandle lh;
    boolean isRecoveryAdd = false;

    PendingAddOp(LedgerHandle lh, AddCallback cb, Object ctx) {
        this.lh = lh;
        this.cb = cb;
        this.ctx = ctx;
        this.entryId = LedgerHandle.INVALID_ENTRY_ID;
        
        ackSet = lh.distributionSchedule.getAckSet();
    }

    /**
     * Enable the recovery add flag for this operation.
     * @see LedgerHandle#asyncRecoveryAddEntry
     */
    PendingAddOp enableRecoveryAdd() {
        isRecoveryAdd = true;
        return this;
    }

    void setEntryId(long entryId) {
        this.entryId = entryId;
        writeSet = new HashSet<Integer>(lh.distributionSchedule.getWriteSet(entryId));
    }

    void sendWriteRequest(int bookieIndex) {
        int flags = isRecoveryAdd ? BookieProtocol.FLAG_RECOVERY_ADD : BookieProtocol.FLAG_NONE;

        lh.bk.bookieClient.addEntry(lh.metadata.currentEnsemble.get(bookieIndex), lh.ledgerId, lh.ledgerKey, entryId, toSend,
                this, bookieIndex, flags);
    }

    void unsetSuccessAndSendWriteRequest(int bookieIndex) {
        if (toSend == null) {
            // this addOp hasn't yet had its mac computed. When the mac is
            // computed, its write requests will be sent, so no need to send it
            // now
            return;
        }
        // Suppose that unset doesn't happen on the write set of an entry. In this
        // case we don't need to resend the write request upon an ensemble change.
        // We do need to invoke #sendAddSuccessCallbacks() for such entries because
        // they may have already completed, but they are just waiting for the ensemble
        // to change.
        // E.g.
        // ensemble (A, B, C, D), entry k is written to (A, B, D). An ensemble change
        // happens to replace C with E. Entry k does not complete until C is
        // replaced with E successfully. When the ensemble change completes, it tries
        // to unset entry k. C however is not in k's write set, so no entry is written
        // again, and no one triggers #sendAddSuccessCallbacks. Consequently, k never
        // completes.
        //
        // We call sendAddSuccessCallback when unsetting t cover this case.
        if (!writeSet.contains(bookieIndex)) {
            lh.sendAddSuccessCallbacks();
            return;
        }

        if (LOG.isDebugEnabled()) {
            LOG.debug("Unsetting success for ledger: " + lh.ledgerId + " entry: " + entryId + " bookie index: "
                      + bookieIndex);
        }

        // if we had already heard a success from this array index, need to
        // increment our number of responses that are pending, since we are
        // going to unset this success
        ackSet.removeBookie(bookieIndex);
        completed = false;

        sendWriteRequest(bookieIndex);
    }

    void initiate(ChannelBuffer toSend) {
        this.toSend = toSend;
        for (int bookieIndex : writeSet) {
            sendWriteRequest(bookieIndex);
        }
    }

    @Override
    public void writeComplete(int rc, long ledgerId, long entryId, InetSocketAddress addr, Object ctx) {
        int bookieIndex = (Integer) ctx;


        switch (rc) {
        case BKException.Code.OK:
            // continue
            break;
        case BKException.Code.LedgerFencedException:
            LOG.warn("Fencing exception on write: " + ledgerId + ", " + entryId);
            lh.handleUnrecoverableErrorDuringAdd(rc);
            return;
        case BKException.Code.UnauthorizedAccessException:
            LOG.warn("Unauthorized access exception on write: " + ledgerId + ", " + entryId);
            lh.handleUnrecoverableErrorDuringAdd(rc);
            return;
        default:
            LOG.warn("Write did not succeed: " + ledgerId + ", " + entryId);
            lh.handleBookieFailure(addr, bookieIndex);
            return;
        }

        if (!writeSet.contains(bookieIndex)) {
            LOG.warn("Received a response for (lid:{}, eid:{}) from {}@{}, but it doesn't belong to {}.",
                     new Object[] { ledgerId, entryId, addr, bookieIndex, writeSet });
            return;
        }

        if (ackSet.addBookieAndCheck(bookieIndex) && !completed) {
            completed = true;

            LOG.debug("Complete (lid:{}, eid:{}).", ledgerId, entryId);
            // when completed an entry, try to send success add callbacks in order
            lh.sendAddSuccessCallbacks();
        }
    }

    void submitCallback(final int rc) {
        cb.addComplete(rc, lh, entryId, ctx);
    }

    @Override
    public String toString() {
        StringBuilder sb = new StringBuilder();
        sb.append("PendingAddOp(lid:").append(lh.ledgerId)
          .append(", eid:").append(entryId).append(", completed:")
          .append(completed).append(")");
        return sb.toString();
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/PendingReadOp.java,true,"package org.apache.bookkeeper.client;

/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */
import java.net.InetSocketAddress;
import java.util.concurrent.ArrayBlockingQueue;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.ScheduledFuture;
import java.util.ArrayList;
import java.util.Enumeration;
import java.util.NoSuchElementException;
import java.util.Queue;
import java.util.BitSet;
import java.util.Set;
import java.util.HashSet;
import java.util.List;

import java.util.concurrent.atomic.AtomicBoolean;
import org.apache.bookkeeper.client.AsyncCallback.ReadCallback;
import org.apache.bookkeeper.client.BKException.BKDigestMatchException;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.ReadEntryCallback;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.buffer.ChannelBuffer;
import org.jboss.netty.buffer.ChannelBufferInputStream;

/**
 * Sequence of entries of a ledger that represents a pending read operation.
 * When all the data read has come back, the application callback is called.
 * This class could be improved because we could start pushing data to the
 * application as soon as it arrives rather than waiting for the whole thing.
 *
 */
class PendingReadOp implements Enumeration<LedgerEntry>, ReadEntryCallback {
    Logger LOG = LoggerFactory.getLogger(PendingReadOp.class);

    final int speculativeReadTimeout;
    final private ScheduledExecutorService scheduler;
    private ScheduledFuture<?> speculativeTask = null;
    Queue<LedgerEntryRequest> seq;
    Set<InetSocketAddress> heardFromHosts;
    ReadCallback cb;
    Object ctx;
    LedgerHandle lh;
    long numPendingEntries;
    long startEntryId;
    long endEntryId;
    final int maxMissedReadsAllowed;

    class LedgerEntryRequest extends LedgerEntry {
        final static int NOT_FOUND = -1;
        int nextReplicaIndexToReadFrom = 0;
        AtomicBoolean complete = new AtomicBoolean(false);

        int firstError = BKException.Code.OK;
        int numMissedEntryReads = 0;

        final ArrayList<InetSocketAddress> ensemble;
        final List<Integer> writeSet;
        final BitSet sentReplicas;
        final BitSet erroredReplicas;

        LedgerEntryRequest(ArrayList<InetSocketAddress> ensemble, long lId, long eId) {
            super(lId, eId);

            this.ensemble = ensemble;
            this.writeSet = lh.distributionSchedule.getWriteSet(entryId);
            this.sentReplicas = new BitSet(lh.getLedgerMetadata().getWriteQuorumSize());
            this.erroredReplicas = new BitSet(lh.getLedgerMetadata().getWriteQuorumSize());
        }

        private int getReplicaIndex(InetSocketAddress host) {
            int bookieIndex = ensemble.indexOf(host);
            if (bookieIndex == -1) {
                return NOT_FOUND;
            }
            return writeSet.indexOf(bookieIndex);
        }

        private BitSet getSentToBitSet() {
            BitSet b = new BitSet(ensemble.size());

            for (int i = 0; i < sentReplicas.length(); i++) {
                if (sentReplicas.get(i)) {
                    b.set(writeSet.get(i));
                }
            }
            return b;
        }

        private BitSet getHeardFromBitSet(Set<InetSocketAddress> heardFromHosts) {
            BitSet b = new BitSet(ensemble.size());
            for (InetSocketAddress i : heardFromHosts) {
                int index = ensemble.indexOf(i);
                if (index != -1) {
                    b.set(index);
                }
            }
            return b;
        }

        private boolean readsOutstanding() {
            return (sentReplicas.cardinality() - erroredReplicas.cardinality()) > 0;
        }

        /**
         * Send to next replica speculatively, if required and possible.
         * This returns the host we may have sent to for unit testing.
         * @return host we sent to if we sent. null otherwise.
         */
        synchronized InetSocketAddress maybeSendSpeculativeRead(Set<InetSocketAddress> heardFromHosts) {
            if (nextReplicaIndexToReadFrom >= lh.getLedgerMetadata().getWriteQuorumSize()) {
                return null;
            }

            BitSet sentTo = getSentToBitSet();
            BitSet heardFrom = getHeardFromBitSet(heardFromHosts);
            sentTo.and(heardFrom);

            // only send another read, if we have had no response at all (even for other entries)
            // from any of the other bookies we have sent the request to
            if (sentTo.cardinality() == 0) {
                return sendNextRead();
            } else {
                return null;
            }
        }

        synchronized InetSocketAddress sendNextRead() {
            if (nextReplicaIndexToReadFrom >= lh.metadata.getWriteQuorumSize()) {
                // we are done, the read has failed from all replicas, just fail the
                // read

                // Do it a bit pessimistically, only when finished trying all replicas
                // to check whether we received more missed reads than maxMissedReadsAllowed
                if (BKException.Code.BookieHandleNotAvailableException == firstError &&
                    numMissedEntryReads > maxMissedReadsAllowed) {
                    firstError = BKException.Code.NoSuchEntryException;
                }

                submitCallback(firstError);
                return null;
            }

            int replica = nextReplicaIndexToReadFrom;
            int bookieIndex = lh.distributionSchedule.getWriteSet(entryId).get(nextReplicaIndexToReadFrom);
            nextReplicaIndexToReadFrom++;

            try {
                InetSocketAddress to = ensemble.get(bookieIndex);
                sendReadTo(to, this);
                sentReplicas.set(replica);
                return to;
            } catch (InterruptedException ie) {
                LOG.error("Interrupted reading entry " + this, ie);
                Thread.currentThread().interrupt();
                submitCallback(BKException.Code.ReadException);
                return null;
            }
        }

        synchronized void logErrorAndReattemptRead(InetSocketAddress host, String errMsg, int rc) {
            if (BKException.Code.OK == firstError ||
                BKException.Code.NoSuchEntryException == firstError) {
                firstError = rc;
            } else if (BKException.Code.BookieHandleNotAvailableException == firstError &&
                       BKException.Code.NoSuchEntryException != rc) {
                // if other exception rather than NoSuchEntryException is returned
                // we need to update firstError to indicate that it might be a valid read but just failed.
                firstError = rc;
            }
            if (BKException.Code.NoSuchEntryException == rc) {
                ++numMissedEntryReads;
            }

            LOG.error(errMsg + " while reading entry: " + entryId + " ledgerId: " + lh.ledgerId + " from bookie: "
                      + host);

            int replica = getReplicaIndex(host);
            if (replica == NOT_FOUND) {
                LOG.error("Received error from a host which is not in the ensemble {} {}.", host, ensemble);
                return;
            }
            erroredReplicas.set(replica);

            if (!readsOutstanding()) {
                sendNextRead();
            }
        }

        // return true if we managed to complete the entry
        // return false if the read entry is not complete or it is already completed before
        boolean complete(InetSocketAddress host, final ChannelBuffer buffer) {
            ChannelBufferInputStream is;
            try {
                is = lh.macManager.verifyDigestAndReturnData(entryId, buffer);
            } catch (BKDigestMatchException e) {
                logErrorAndReattemptRead(host, "Mac mismatch", BKException.Code.DigestMatchException);
                return false;
            }

            if (!complete.getAndSet(true)) {
                entryDataStream = is;

                /*
                 * The length is a long and it is the last field of the metadata of an entry.
                 * Consequently, we have to subtract 8 from METADATA_LENGTH to get the length.
                 */
                length = buffer.getLong(DigestManager.METADATA_LENGTH - 8);
                return true;
            } else {
                return false;
            }
        }

        boolean isComplete() {
            return complete.get();
        }

        public String toString() {
            return String.format("L%d-E%d", ledgerId, entryId);
        }
    }

    PendingReadOp(LedgerHandle lh, ScheduledExecutorService scheduler,
                  long startEntryId, long endEntryId, ReadCallback cb, Object ctx) {
        seq = new ArrayBlockingQueue<LedgerEntryRequest>((int) ((endEntryId + 1) - startEntryId));
        this.cb = cb;
        this.ctx = ctx;
        this.lh = lh;
        this.startEntryId = startEntryId;
        this.endEntryId = endEntryId;
        this.scheduler = scheduler;
        numPendingEntries = endEntryId - startEntryId + 1;
        maxMissedReadsAllowed = lh.metadata.getWriteQuorumSize() - lh.metadata.getAckQuorumSize();
        speculativeReadTimeout = lh.bk.getConf().getSpeculativeReadTimeout();
        heardFromHosts = new HashSet<InetSocketAddress>();
    }

    public void initiate() throws InterruptedException {
        long nextEnsembleChange = startEntryId, i = startEntryId;

        ArrayList<InetSocketAddress> ensemble = null;

        if (speculativeReadTimeout > 0) {
            speculativeTask = scheduler.scheduleWithFixedDelay(new Runnable() {
                    public void run() {
                        int x = 0;
                        for (LedgerEntryRequest r : seq) {
                            if (!r.isComplete()) {
                                if (null != r.maybeSendSpeculativeRead(heardFromHosts)) {
                                    LOG.debug("Send speculative read for {}. Hosts heard are {}.",
                                              r, heardFromHosts);
                                    ++x;
                                }
                            }
                        }
                        if (x > 0) {
                            LOG.info("Send {} speculative reads for ledger {} ({}, {}). Hosts heard are {}.",
                                     new Object[] { x, lh.getId(), startEntryId, endEntryId, heardFromHosts });
                        }
                    }
                }, speculativeReadTimeout, speculativeReadTimeout, TimeUnit.MILLISECONDS);
        }

        do {
            LOG.debug("Acquiring lock: {}", i);

            if (i == nextEnsembleChange) {
                ensemble = lh.metadata.getEnsemble(i);
                nextEnsembleChange = lh.metadata.getNextEnsembleChange(i);
            }
            LedgerEntryRequest entry = new LedgerEntryRequest(ensemble, lh.ledgerId, i);
            seq.add(entry);
            i++;

            entry.sendNextRead();
        } while (i <= endEntryId);
    }

    private static class ReadContext {
        final InetSocketAddress to;
        final LedgerEntryRequest entry;

        ReadContext(InetSocketAddress to, LedgerEntryRequest entry) {
            this.to = to;
            this.entry = entry;
        }
    }

    void sendReadTo(InetSocketAddress to, LedgerEntryRequest entry) throws InterruptedException {
        lh.throttler.acquire();

        lh.bk.bookieClient.readEntry(to, lh.ledgerId, entry.entryId, 
                                     this, new ReadContext(to, entry));
    }

    @Override
    public void readEntryComplete(int rc, long ledgerId, final long entryId, final ChannelBuffer buffer, Object ctx) {
        final ReadContext rctx = (ReadContext)ctx;
        final LedgerEntryRequest entry = rctx.entry;

        if (rc != BKException.Code.OK) {
            entry.logErrorAndReattemptRead(rctx.to, "Error: " + BKException.getMessage(rc), rc);
            return;
        }

        heardFromHosts.add(rctx.to);

        if (entry.complete(rctx.to, buffer)) {
            numPendingEntries--;
            if (numPendingEntries == 0) {
                submitCallback(BKException.Code.OK);
            }
        }

        if(numPendingEntries < 0)
            LOG.error("Read too many values");
    }

    private void submitCallback(int code) {
        if (speculativeTask != null) {
            speculativeTask.cancel(true);
            speculativeTask = null;
        }
        cb.readComplete(code, lh, PendingReadOp.this, PendingReadOp.this.ctx);
    }
    public boolean hasMoreElements() {
        return !seq.isEmpty();
    }

    public LedgerEntry nextElement() throws NoSuchElementException {
        return seq.remove();
    }

    public int size() {
        return seq.size();
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/ReadLastConfirmedOp.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.client;

import org.apache.bookkeeper.client.BKException.BKDigestMatchException;
import org.apache.bookkeeper.client.DigestManager.RecoveryData;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.ReadEntryCallback;
import org.apache.bookkeeper.proto.BookieProtocol;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.buffer.ChannelBuffer;

/**
 * This class encapsulated the read last confirmed operation.
 *
 */
class ReadLastConfirmedOp implements ReadEntryCallback {
    static final Logger LOG = LoggerFactory.getLogger(ReadLastConfirmedOp.class);
    LedgerHandle lh;
    int numResponsesPending;
    RecoveryData maxRecoveredData;
    volatile boolean completed = false;

    LastConfirmedDataCallback cb;
    final DistributionSchedule.QuorumCoverageSet coverageSet;

    /**
     * Wrapper to get all recovered data from the request
     */
    interface LastConfirmedDataCallback {
        public void readLastConfirmedDataComplete(int rc, RecoveryData data);
    }

    public ReadLastConfirmedOp(LedgerHandle lh, LastConfirmedDataCallback cb) {
        this.cb = cb;
        this.maxRecoveredData = new RecoveryData(LedgerHandle.INVALID_ENTRY_ID, 0);
        this.lh = lh;
        this.numResponsesPending = lh.metadata.getEnsembleSize();
        this.coverageSet = lh.distributionSchedule.getCoverageSet();
    }

    public void initiate() {
        for (int i = 0; i < lh.metadata.currentEnsemble.size(); i++) {
            lh.bk.bookieClient.readEntry(lh.metadata.currentEnsemble.get(i),
                                         lh.ledgerId,
                                         BookieProtocol.LAST_ADD_CONFIRMED,
                                         this, i);
        }
    }

    public void initiateWithFencing() {
        for (int i = 0; i < lh.metadata.currentEnsemble.size(); i++) {
            lh.bk.bookieClient.readEntryAndFenceLedger(lh.metadata.currentEnsemble.get(i),
                                                       lh.ledgerId,
                                                       lh.ledgerKey,
                                                       BookieProtocol.LAST_ADD_CONFIRMED,
                                                       this, i);
        }
    }

    public synchronized void readEntryComplete(final int rc, final long ledgerId, final long entryId,
            final ChannelBuffer buffer, final Object ctx) {
        int bookieIndex = (Integer) ctx;

        numResponsesPending--;
        boolean heardValidResponse = false;
        if (rc == BKException.Code.OK) {
            try {
                RecoveryData recoveryData = lh.macManager.verifyDigestAndReturnLastConfirmed(buffer);
                if (recoveryData.lastAddConfirmed > maxRecoveredData.lastAddConfirmed) {
                    maxRecoveredData = recoveryData;
                }
                heardValidResponse = true;
            } catch (BKDigestMatchException e) {
                // Too bad, this bookie didn't give us a valid answer, we
                // still might be able to recover though so continue
                LOG.error("Mac mismatch for ledger: " + ledgerId + ", entry: " + entryId
                          + " while reading last entry from bookie: "
                          + lh.metadata.currentEnsemble.get(bookieIndex));
            }
        }

        if (rc == BKException.Code.NoSuchLedgerExistsException || rc == BKException.Code.NoSuchEntryException) {
            // this still counts as a valid response, e.g., if the client crashed without writing any entry
            heardValidResponse = true;
        }

        if (rc == BKException.Code.UnauthorizedAccessException  && !completed) {
            cb.readLastConfirmedDataComplete(rc, maxRecoveredData);
            completed = true;
        }
        // other return codes dont count as valid responses
        if (heardValidResponse
            && coverageSet.addBookieAndCheckCovered(bookieIndex)
            && !completed) {
            completed = true;
            LOG.debug("Read Complete with enough validResponses for ledger: {}, entry: {}",
                ledgerId, entryId);

            cb.readLastConfirmedDataComplete(BKException.Code.OK, maxRecoveredData);
            return;
        }

        if (numResponsesPending == 0 && !completed) {
            // Have got all responses back but was still not enough, just fail the operation
            LOG.error("While readLastConfirmed ledger: " + ledgerId + " did not hear success responses from all quorums");
            cb.readLastConfirmedDataComplete(BKException.Code.LedgerRecoveryException, maxRecoveredData);
        }

    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/ReadOnlyLedgerHandle.java,true,"package org.apache.bookkeeper.client;

/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

import org.apache.bookkeeper.client.AsyncCallback.AddCallback;
import org.apache.bookkeeper.client.AsyncCallback.CloseCallback;
import org.apache.bookkeeper.client.BookKeeper.DigestType;
import java.security.GeneralSecurityException;
import java.net.InetSocketAddress;

/**
 * Read only ledger handle. This ledger handle allows you to 
 * read from a ledger but not to write to it. It overrides all 
 * the public write operations from LedgerHandle.
 * It should be returned for BookKeeper#openLedger operations.
 */
class ReadOnlyLedgerHandle extends LedgerHandle {
    ReadOnlyLedgerHandle(BookKeeper bk, long ledgerId, LedgerMetadata metadata,
                         DigestType digestType, byte[] password)
            throws GeneralSecurityException, NumberFormatException {
        super(bk, ledgerId, metadata, digestType, password);
    }

    @Override
    public void close() 
            throws InterruptedException, BKException {
        // noop
    }

    @Override
    public void asyncClose(CloseCallback cb, Object ctx) {
        cb.closeComplete(BKException.Code.OK, this, ctx);
    }
    
    @Override
    public long addEntry(byte[] data) throws InterruptedException, BKException {
        return addEntry(data, 0, data.length);
    }
    
    @Override
    public long addEntry(byte[] data, int offset, int length)
            throws InterruptedException, BKException {
        LOG.error("Tried to add entry on a Read-Only ledger handle, ledgerid=" + ledgerId);        
        throw BKException.create(BKException.Code.IllegalOpException);
    }

    @Override
    public void asyncAddEntry(final byte[] data, final AddCallback cb,
                              final Object ctx) {
        asyncAddEntry(data, 0, data.length, cb, ctx);
    }

    @Override
    public void asyncAddEntry(final byte[] data, final int offset, final int length,
                              final AddCallback cb, final Object ctx) {
        LOG.error("Tried to add entry on a Read-Only ledger handle, ledgerid=" + ledgerId);
        cb.addComplete(BKException.Code.IllegalOpException, this,
                       LedgerHandle.INVALID_ENTRY_ID, ctx);
    }

    @Override
    void handleBookieFailure(final InetSocketAddress addr, final int bookieIndex) {
        blockAddCompletions.incrementAndGet();
        synchronized (metadata) {
            try {
                if (!metadata.currentEnsemble.get(bookieIndex).equals(addr)) {
                    // ensemble has already changed, failure of this addr is immaterial
                    LOG.warn("Write did not succeed to {}, bookieIndex {}, but we have already fixed it.",
                             addr, bookieIndex);
                    blockAddCompletions.decrementAndGet();
                    return;
                }

                replaceBookieInMetadata(addr, bookieIndex);

                blockAddCompletions.decrementAndGet();
                // the failed bookie has been replaced
                unsetSuccessAndSendWriteRequest(bookieIndex);
            } catch (BKException.BKNotEnoughBookiesException e) {
                LOG.error("Could not get additional bookie to "
                          + "remake ensemble, closing ledger: " + ledgerId);
                handleUnrecoverableErrorDuringAdd(e.getCode());
                return;
            }
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/RoundRobinDistributionSchedule.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.client;

import org.apache.bookkeeper.util.MathUtils;

import java.util.List;
import java.util.ArrayList;
import java.util.HashSet;

/**
 * A specific {@link DistributionSchedule} that places entries in round-robin
 * fashion. For ensemble size 3, and quorum size 2, Entry 0 goes to bookie 0 and
 * 1, entry 1 goes to bookie 1 and 2, and entry 2 goes to bookie 2 and 0, and so
 * on.
 *
 */
class RoundRobinDistributionSchedule implements DistributionSchedule {
    private int writeQuorumSize;
    private int ackQuorumSize;
    private int ensembleSize;


    public RoundRobinDistributionSchedule(int writeQuorumSize, int ackQuorumSize, int ensembleSize) {
        this.writeQuorumSize = writeQuorumSize;
        this.ackQuorumSize = ackQuorumSize;
        this.ensembleSize = ensembleSize;
    }

    @Override
    public List<Integer> getWriteSet(long entryId) {
        List<Integer> set = new ArrayList<Integer>();
        for (int i = 0; i < this.writeQuorumSize; i++) {
            set.add((int)((entryId + i) % ensembleSize));
        }
        return set;
    }

    @Override
    public AckSet getAckSet() {
        final HashSet<Integer> ackSet = new HashSet<Integer>();
        return new AckSet() {
            public boolean addBookieAndCheck(int bookieIndexHeardFrom) {
                ackSet.add(bookieIndexHeardFrom);
                return ackSet.size() >= ackQuorumSize;
            }

            public void removeBookie(int bookie) {
                ackSet.remove(bookie);
            }
        };
    }

    private class RRQuorumCoverageSet implements QuorumCoverageSet {
        // covered[i] is true if the quorum starting at bookie index i has been
        // covered by a recovery reply
        private boolean[] covered = null;
        private int numQuorumsUncovered;

        private RRQuorumCoverageSet() {
            covered = new boolean[ensembleSize];
            numQuorumsUncovered = ensembleSize;
        }

        public synchronized boolean addBookieAndCheckCovered(int bookieIndexHeardFrom) {
            if (numQuorumsUncovered == 0) {
                return true;
            }

            for (int i = 0; i < ackQuorumSize; i++) {
                int quorumStartIndex = MathUtils.signSafeMod(bookieIndexHeardFrom - i, ensembleSize);
                if (!covered[quorumStartIndex]) {
                    covered[quorumStartIndex] = true;
                    numQuorumsUncovered--;

                    if (numQuorumsUncovered == 0) {
                        return true;
                    }
                }
            }
            return false;
        }
    }

    @Override
    public QuorumCoverageSet getCoverageSet() {
        return new RRQuorumCoverageSet();
    }
    
    @Override
    public boolean hasEntry(long entryId, int bookieIndex) {
        return getWriteSet(entryId).contains(bookieIndex);
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/client/SyncCounter.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.client;

import java.util.Enumeration;

/**
 * Implements objects to help with the synchronization of asynchronous calls
 *
 */

class SyncCounter {
    int i;
    int rc;
    int total;
    Enumeration<LedgerEntry> seq = null;
    LedgerHandle lh = null;

    synchronized void inc() {
        i++;
        total++;
    }

    synchronized void dec() {
        i--;
        notifyAll();
    }

    synchronized void block(int limit) throws InterruptedException {
        while (i > limit) {
            int prev = i;
            wait();
            if (i == prev) {
                break;
            }
        }
    }

    synchronized int total() {
        return total;
    }

    void setrc(int rc) {
        this.rc = rc;
    }

    int getrc() {
        return rc;
    }

    void setSequence(Enumeration<LedgerEntry> seq) {
        this.seq = seq;
    }

    Enumeration<LedgerEntry> getSequence() {
        return seq;
    }

    void setLh(LedgerHandle lh) {
        this.lh = lh;
    }

    LedgerHandle getLh() {
        return lh;
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/conf/AbstractConfiguration.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.conf;

import java.net.URL;

import org.apache.commons.configuration.CompositeConfiguration;
import org.apache.commons.configuration.Configuration;
import org.apache.commons.configuration.ConfigurationException;
import org.apache.commons.configuration.PropertiesConfiguration;
import org.apache.commons.configuration.SystemConfiguration;

import org.apache.bookkeeper.meta.LedgerManagerFactory;
import org.apache.bookkeeper.util.ReflectionUtils;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Abstract configuration
 */
public abstract class AbstractConfiguration extends CompositeConfiguration {

    static final Logger LOG = LoggerFactory.getLogger(AbstractConfiguration.class);

    private static ClassLoader defaultLoader;
    static {
        defaultLoader = Thread.currentThread().getContextClassLoader();
        if (null == defaultLoader) {
            defaultLoader = AbstractConfiguration.class.getClassLoader();
        }
    }

    // Ledger Manager
    protected final static String LEDGER_MANAGER_TYPE = "ledgerManagerType";
    protected final static String LEDGER_MANAGER_FACTORY_CLASS = "ledgerManagerFactoryClass";
    protected final static String ZK_LEDGERS_ROOT_PATH = "zkLedgersRootPath";
    protected final static String AVAILABLE_NODE = "available";
    protected final static String REREPLICATION_ENTRY_BATCH_SIZE = "rereplicationEntryBatchSize";

    // Metastore settings, only being used when LEDGER_MANAGER_FACTORY_CLASS is MSLedgerManagerFactory
    protected final static String METASTORE_IMPL_CLASS = "metastoreImplClass";
    protected final static String METASTORE_MAX_ENTRIES_PER_SCAN = "metastoreMaxEntriesPerScan";

    protected AbstractConfiguration() {
        super();
        // add configuration for system properties
        addConfiguration(new SystemConfiguration());
    }

    /**
     * You can load configurations in precedence order. The first one takes
     * precedence over any loaded later.
     *
     * @param confURL
     *          Configuration URL
     */
    public void loadConf(URL confURL) throws ConfigurationException {
        Configuration loadedConf = new PropertiesConfiguration(confURL);
        addConfiguration(loadedConf);
    }

    /**
     * You can load configuration from other configuration
     *
     * @param baseConf
     *          Other Configuration
     */
    public void loadConf(AbstractConfiguration baseConf) {
        addConfiguration(baseConf); 
    }

    /**
     * Load configuration from other configuration object
     *
     * @param otherConf
     *          Other configuration object
     */
    public void loadConf(Configuration otherConf) {
        addConfiguration(otherConf);
    }

    /**
     * Set Ledger Manager Type.
     *
     * @param lmType
     *          Ledger Manager Type
     * @deprecated replaced by {@link #setLedgerManagerFactoryClass}
     */
    @Deprecated
    public void setLedgerManagerType(String lmType) {
        setProperty(LEDGER_MANAGER_TYPE, lmType); 
    }

    /**
     * Get Ledger Manager Type.
     *
     * @return ledger manager type
     * @throws ConfigurationException
     * @deprecated replaced by {@link #getLedgerManagerFactoryClass()}
     */
    @Deprecated
    public String getLedgerManagerType() {
        return getString(LEDGER_MANAGER_TYPE);
    }

    /**
     * Set Ledger Manager Factory Class Name.
     *
     * @param factoryClassName
     *          Ledger Manager Factory Class Name
     */
    public void setLedgerManagerFactoryClassName(String factoryClassName) {
        setProperty(LEDGER_MANAGER_FACTORY_CLASS, factoryClassName);
    }

    /**
     * Set Ledger Manager Factory Class.
     *
     * @param factoryClass
     *          Ledger Manager Factory Class
     */
    public void setLedgerManagerFactoryClass(Class<? extends LedgerManagerFactory> factoryClass) {
        setProperty(LEDGER_MANAGER_FACTORY_CLASS, factoryClass.getName());
    }

    /**
     * Get ledger manager factory class.
     *
     * @return ledger manager factory class
     */
    public Class<? extends LedgerManagerFactory> getLedgerManagerFactoryClass()
        throws ConfigurationException {
        return ReflectionUtils.getClass(this, LEDGER_MANAGER_FACTORY_CLASS,
                                        null, LedgerManagerFactory.class,
                                        defaultLoader);
    }

    /**
     * Set Zk Ledgers Root Path.
     *
     * @param zkLedgersPath zk ledgers root path
     */
    public void setZkLedgersRootPath(String zkLedgersPath) {
        setProperty(ZK_LEDGERS_ROOT_PATH, zkLedgersPath);
    }

    /**
     * Get Zk Ledgers Root Path.
     *
     * @return zk ledgers root path
     */
    public String getZkLedgersRootPath() {
        return getString(ZK_LEDGERS_ROOT_PATH, "/ledgers");
    }

    /**
     * Get the node under which available bookies are stored
     *
     * @return Node under which available bookies are stored.
     */
    public String getZkAvailableBookiesPath() {
        return getZkLedgersRootPath() + "/" + AVAILABLE_NODE;
    }
    
    /**
     * Set the max entries to keep in fragment for re-replication. If fragment
     * has more entries than this count, then the original fragment will be
     * split into multiple small logical fragments by keeping max entries count
     * to rereplicationEntryBatchSize. So, re-replication will happen in batches
     * wise.
     */
    public void setRereplicationEntryBatchSize(long rereplicationEntryBatchSize) {
        setProperty(REREPLICATION_ENTRY_BATCH_SIZE, rereplicationEntryBatchSize);
    }

    /**
     * Get the re-replication entry batch size
     */
    public long getRereplicationEntryBatchSize() {
        return getLong(REREPLICATION_ENTRY_BATCH_SIZE, 10);
    }

    /**
     * Get metastore implementation class.
     *
     * @return metastore implementation class name.
     */
    public String getMetastoreImplClass() {
        return getString(METASTORE_IMPL_CLASS);
    }

    /**
     * Set metastore implementation class.
     *
     * @param metastoreImplClass
     *          Metastore implementation Class name.
     */
    public void setMetastoreImplClass(String metastoreImplClass) {
        setProperty(METASTORE_IMPL_CLASS, metastoreImplClass);
    }

    /**
     * Get max entries per scan in metastore.
     *
     * @return max entries per scan in metastore.
     */
    public int getMetastoreMaxEntriesPerScan() {
        return getInt(METASTORE_MAX_ENTRIES_PER_SCAN, 50);
    }

    /**
     * Set max entries per scan in metastore.
     *
     * @param maxEntries
     *          Max entries per scan in metastore.
     */
    public void setMetastoreMaxEntriesPerScan(int maxEntries) {
        setProperty(METASTORE_MAX_ENTRIES_PER_SCAN, maxEntries);
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/conf/ClientConfiguration.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.conf;

import java.util.List;

import org.apache.bookkeeper.client.BookKeeper.DigestType;

import org.apache.commons.lang.StringUtils;

/**
 * Configuration settings for client side
 */
public class ClientConfiguration extends AbstractConfiguration {

    // Zookeeper Parameters
    protected final static String ZK_TIMEOUT = "zkTimeout";
    protected final static String ZK_SERVERS = "zkServers";

    // Throttle value
    protected final static String THROTTLE = "throttle";

    // Digest Type
    protected final static String DIGEST_TYPE = "digestType";
    // Passwd
    protected final static String PASSWD = "passwd";

    // NIO Parameters
    protected final static String CLIENT_TCP_NODELAY = "clientTcpNoDelay";
    protected final static String READ_TIMEOUT = "readTimeout";
    protected final static String SPECULATIVE_READ_TIMEOUT = "speculativeReadTimeout";

    // Number Woker Threads
    protected final static String NUM_WORKER_THREADS = "numWorkerThreads";

    /**
     * Construct a default client-side configuration
     */
    public ClientConfiguration() {
        super();
    }

    /**
     * Construct a client-side configuration using a base configuration
     *
     * @param conf
     *          Base configuration
     */
    public ClientConfiguration(AbstractConfiguration conf) {
        super();
        loadConf(conf);
    }

    /**
     * Get throttle value
     *
     * @return throttle value
     * @see #setThrottleValue
     */
    public int getThrottleValue() {
        return this.getInt(THROTTLE, 5000);
    }

    /**
     * Set throttle value.
     *
     * Since BookKeeper process requests in asynchrous way, it will holds 
     * those pending request in queue. You may easily run it out of memory
     * if producing too many requests than the capability of bookie servers can handle.
     * To prevent that from happeding, you can set a throttle value here.
     *
     * @param throttle
     *          Throttle Value
     * @return client configuration
     */
    public ClientConfiguration setThrottleValue(int throttle) {
        this.setProperty(THROTTLE, Integer.toString(throttle));
        return this;
    }

    /**
     * Get digest type used in bookkeeper admin
     *
     * @return digest type
     * @see #setBookieRecoveryDigestType
     */
    public DigestType getBookieRecoveryDigestType() {
        return DigestType.valueOf(this.getString(DIGEST_TYPE, DigestType.CRC32.toString()));
    }

    /**
     * Set digest type used in bookkeeper admin.
     *
     * Digest Type and Passwd used to open ledgers for admin tool
     * For now, assume that all ledgers were created with the same DigestType
     * and password. In the future, this admin tool will need to know for each
     * ledger, what was the DigestType and password used to create it before it
     * can open it. These values will come from System properties, though fixed
     * defaults are defined here.
     *
     * @param digestType
     *          Digest Type
     * @return client configuration
     */
    public ClientConfiguration setBookieRecoveryDigestType(DigestType digestType) {
        this.setProperty(DIGEST_TYPE, digestType.toString());
        return this;
    }

    /**
     * Get passwd used in bookkeeper admin
     *
     * @return password
     * @see #setBookieRecoveryPasswd
     */
    public byte[] getBookieRecoveryPasswd() {
        return this.getString(PASSWD, "").getBytes();
    }

    /**
     * Set passwd used in bookkeeper admin.
     *
     * Digest Type and Passwd used to open ledgers for admin tool
     * For now, assume that all ledgers were created with the same DigestType
     * and password. In the future, this admin tool will need to know for each
     * ledger, what was the DigestType and password used to create it before it
     * can open it. These values will come from System properties, though fixed
     * defaults are defined here.
     *
     * @param passwd
     *          Password
     * @return client configuration
     */
    public ClientConfiguration setBookieRecoveryPasswd(byte[] passwd) {
        setProperty(PASSWD, new String(passwd));
        return this;
    }

    /**
     * Is tcp connection no delay.
     *
     * @return tcp socket nodelay setting
     * @see #setClientTcpNoDelay
     */
    public boolean getClientTcpNoDelay() {
        return getBoolean(CLIENT_TCP_NODELAY, true);
    }

    /**
     * Set socket nodelay setting.
     *
     * This settings is used to enabled/disabled Nagle's algorithm, which is a means of
     * improving the efficiency of TCP/IP networks by reducing the number of packets
     * that need to be sent over the network. If you are sending many small messages, 
     * such that more than one can fit in a single IP packet, setting client.tcpnodelay
     * to false to enable Nagle algorithm can provide better performance.
     * <br>
     * Default value is true.
     *
     * @param noDelay
     *          NoDelay setting
     * @return client configuration
     */
    public ClientConfiguration setClientTcpNoDelay(boolean noDelay) {
        setProperty(CLIENT_TCP_NODELAY, Boolean.toString(noDelay));
        return this;
    }

    /**
     * Get zookeeper servers to connect
     *
     * @return zookeeper servers
     */
    public String getZkServers() {
        List<Object> servers = getList(ZK_SERVERS, null);
        if (null == servers || 0 == servers.size()) {
            return "localhost";
        }
        return StringUtils.join(servers, ",");
    }

    /**
     * Set zookeeper servers to connect
     *
     * @param zkServers
     *          ZooKeeper servers to connect
     */
    public ClientConfiguration setZkServers(String zkServers) {
        setProperty(ZK_SERVERS, zkServers);
        return this;
    }

    /**
     * Get zookeeper timeout
     *
     * @return zookeeper client timeout
     */
    public int getZkTimeout() {
        return getInt(ZK_TIMEOUT, 10000);
    }

    /**
     * Set zookeeper timeout
     *
     * @param zkTimeout
     *          ZooKeeper client timeout
     * @return client configuration
     */
    public ClientConfiguration setZkTimeout(int zkTimeout) {
        setProperty(ZK_TIMEOUT, Integer.toString(zkTimeout));
        return this;
    }

    /**
     * Get the socket read timeout. This is the number of
     * seconds we wait without hearing a response from a bookie
     * before we consider it failed.
     *
     * The default is 5 seconds.
     *
     * @return the current read timeout in seconds
     */
    public int getReadTimeout() {
        return getInt(READ_TIMEOUT, 5);
    }

    /**
     * Set the socket read timeout.
     * @see #getReadTimeout()
     * @param timeout The new read timeout in seconds
     * @return client configuration
     */
    public ClientConfiguration setReadTimeout(int timeout) {
        setProperty(READ_TIMEOUT, Integer.toString(timeout));
        return this;
    }

    /**
     * Get the number of worker threads. This is the number of
     * worker threads used by bookkeeper client to submit operations.
     *
     * @return the number of worker threads
     */
    public int getNumWorkerThreads() {
        return getInt(NUM_WORKER_THREADS, Runtime.getRuntime().availableProcessors());
    }

    /**
     * Set the number of worker threads.
     *
     * <p>
     * NOTE: setting the number of worker threads after BookKeeper object is constructed
     * will not take any effect on the number of threads in the pool.
     * </p>
     *
     * @see #getNumWorkerThreads()
     * @param numThreads number of worker threads used for bookkeeper
     * @return client configuration
     */
    public ClientConfiguration setNumWorkerThreads(int numThreads) {
        setProperty(NUM_WORKER_THREADS, numThreads);
        return this;
    }

    /**
     * Get the period of time after which a speculative entry read should be triggered.
     * A speculative entry read is sent to the next replica bookie before
     * an error or response has been received for the previous entry read request.
     *
     * A speculative entry read is only sent if we have not heard from the current
     * replica bookie during the entire read operation which may comprise of many entries.
     *
     * Speculative reads allow the client to avoid having to wait for the connect timeout
     * in the case that a bookie has failed. It induces higher load on the network and on
     * bookies. This should be taken into account before changing this configuration value.
     *
     * @see org.apache.bookkeeper.client.LedgerHandle#asyncReadEntries
     * @return the speculative read timeout in milliseconds. Default 2000.
     */
    public int getSpeculativeReadTimeout() {
        return getInt(SPECULATIVE_READ_TIMEOUT, 2000);
    }

    /**
     * Set the speculative read timeout. A lower timeout will reduce read latency in the
     * case of a failed bookie, while increasing the load on bookies and the network.
     *
     * The default is 2000 milliseconds. A value of 0 will disable speculative reads
     * completely.
     *
     * @see #getSpeculativeReadTimeout()
     * @param timeout the timeout value, in milliseconds
     * @return client configuration
     */
    public ClientConfiguration setSpeculativeReadTimeout(int timeout) {
        setProperty(SPECULATIVE_READ_TIMEOUT, timeout);
        return this;
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/conf/ServerConfiguration.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.conf;

import java.io.File;
import java.util.List;

import org.apache.commons.lang.StringUtils;

/**
 * Configuration manages server-side settings
 */
public class ServerConfiguration extends AbstractConfiguration {
    // Entry Log Parameters
    protected final static String ENTRY_LOG_SIZE_LIMIT = "logSizeLimit";
    protected final static String MINOR_COMPACTION_INTERVAL = "minorCompactionInterval";
    protected final static String MINOR_COMPACTION_THRESHOLD = "minorCompactionThreshold";
    protected final static String MAJOR_COMPACTION_INTERVAL = "majorCompactionInterval";
    protected final static String MAJOR_COMPACTION_THRESHOLD = "majorCompactionThreshold";

    // Gc Parameters
    protected final static String GC_WAIT_TIME = "gcWaitTime";
    // Sync Parameters
    protected final static String FLUSH_INTERVAL = "flushInterval";
    // Bookie death watch interval
    protected final static String DEATH_WATCH_INTERVAL = "bookieDeathWatchInterval";
    // Ledger Cache Parameters
    protected final static String OPEN_FILE_LIMIT = "openFileLimit";
    protected final static String PAGE_LIMIT = "pageLimit";
    protected final static String PAGE_SIZE = "pageSize";
    // Journal Parameters
    protected final static String MAX_JOURNAL_SIZE = "journalMaxSizeMB";
    protected final static String MAX_BACKUP_JOURNALS = "journalMaxBackups";
    // Bookie Parameters
    protected final static String BOOKIE_PORT = "bookiePort";
    protected final static String JOURNAL_DIR = "journalDirectory";
    protected final static String LEDGER_DIRS = "ledgerDirectories";
    // NIO Parameters
    protected final static String SERVER_TCP_NODELAY = "serverTcpNoDelay";
    // Zookeeper Parameters
    protected final static String ZK_TIMEOUT = "zkTimeout";
    protected final static String ZK_SERVERS = "zkServers";
    // Statistics Parameters
    protected final static String ENABLE_STATISTICS = "enableStatistics";
    protected final static String OPEN_LEDGER_REREPLICATION_GRACE_PERIOD = "openLedgerRereplicationGracePeriod";
    //ReadOnly mode support on all disk full
    protected final static String READ_ONLY_MODE_ENABLED = "readOnlyModeEnabled";
    //Disk utilization
    protected final static String DISK_USAGE_THRESHOLD = "diskUsageThreshold";
    protected final static String DISK_CHECK_INTERVAL = "diskCheckInterval";
    protected final static String AUDITOR_PERIODIC_CHECK_INTERVAL = "auditorPeriodicCheckInterval";
    protected final static String AUTO_RECOVERY_DAEMON_ENABLED = "autoRecoveryDaemonEnabled";

    /**
     * Construct a default configuration object
     */
    public ServerConfiguration() {
        super();
    }

    /**
     * Construct a configuration based on other configuration
     *
     * @param conf
     *          Other configuration
     */
    public ServerConfiguration(AbstractConfiguration conf) {
        super();
        loadConf(conf);
    }

    /**
     * Get entry logger size limitation
     *
     * @return entry logger size limitation
     */
    public long getEntryLogSizeLimit() {
        return this.getLong(ENTRY_LOG_SIZE_LIMIT, 2 * 1024 * 1024 * 1024L);
    }

    /**
     * Set entry logger size limitation
     *
     * @param logSizeLimit
     *          new log size limitation
     */
    public ServerConfiguration setEntryLogSizeLimit(long logSizeLimit) {
        this.setProperty(ENTRY_LOG_SIZE_LIMIT, Long.toString(logSizeLimit));
        return this;
    }

    /**
     * Get Garbage collection wait time
     *
     * @return gc wait time
     */
    public long getGcWaitTime() {
        return this.getLong(GC_WAIT_TIME, 1000);
    }

    /**
     * Set garbage collection wait time
     *
     * @param gcWaitTime
     *          gc wait time
     * @return server configuration
     */
    public ServerConfiguration setGcWaitTime(long gcWaitTime) {
        this.setProperty(GC_WAIT_TIME, Long.toString(gcWaitTime));
        return this;
    }

    /**
     * Get flush interval
     *
     * @return flush interval
     */
    public int getFlushInterval() {
        return this.getInt(FLUSH_INTERVAL, 100);
    }

    /**
     * Set flush interval
     *
     * @param flushInterval
     *          Flush Interval
     * @return server configuration
     */
    public ServerConfiguration setFlushInterval(int flushInterval) {
        this.setProperty(FLUSH_INTERVAL, Integer.toString(flushInterval));
        return this;
    }

    /**
     * Get bookie death watch interval
     *
     * @return watch interval
     */
    public int getDeathWatchInterval() {
        return this.getInt(DEATH_WATCH_INTERVAL, 1000);
    }

    /**
     * Get open file limit
     *
     * @return max number of files to open
     */
    public int getOpenFileLimit() {
        return this.getInt(OPEN_FILE_LIMIT, 900);
    }

    /**
     * Set limitation of number of open files.
     *
     * @param fileLimit
     *          Limitation of number of open files.
     * @return server configuration
     */
    public ServerConfiguration setOpenFileLimit(int fileLimit) {
        setProperty(OPEN_FILE_LIMIT, fileLimit);
        return this;
    }

    /**
     * Get limitation number of index pages in ledger cache
     *
     * @return max number of index pages in ledger cache
     */
    public int getPageLimit() {
        return this.getInt(PAGE_LIMIT, -1);
    }

    /**
     * Set limitation number of index pages in ledger cache.
     *
     * @param pageLimit
     *          Limitation of number of index pages in ledger cache.
     * @return server configuration
     */
    public ServerConfiguration setPageLimit(int pageLimit) {
        this.setProperty(PAGE_LIMIT, pageLimit);
        return this;
    }

    /**
     * Get page size
     *
     * @return page size in ledger cache
     */
    public int getPageSize() {
        return this.getInt(PAGE_SIZE, 8192);
    }

    /**
     * Set page size
     *
     * @see #getPageSize()
     *
     * @param pageSize
     *          Page Size
     * @return Server Configuration
     */
    public ServerConfiguration setPageSize(int pageSize) {
        this.setProperty(PAGE_SIZE, pageSize);
        return this;
    }

    /**
     * Max journal file size
     *
     * @return max journal file size
     */
    public long getMaxJournalSize() {
        return this.getLong(MAX_JOURNAL_SIZE, 2 * 1024);
    }

    /**
     * Set new max journal file size
     *
     * @param maxJournalSize
     *          new max journal file size
     * @return server configuration
     */
    public ServerConfiguration setMaxJournalSize(long maxJournalSize) {
        this.setProperty(MAX_JOURNAL_SIZE, Long.toString(maxJournalSize));
        return this;
    }

    /**
     * Max number of older journal files kept
     *
     * @return max number of older journal files to kept
     */
    public int getMaxBackupJournals() {
        return this.getInt(MAX_BACKUP_JOURNALS, 5);
    }

    /**
     * Set max number of older journal files to kept
     *
     * @param maxBackupJournals
     *          Max number of older journal files
     * @return server configuration
     */
    public ServerConfiguration setMaxBackupJournals(int maxBackupJournals) {
        this.setProperty(MAX_BACKUP_JOURNALS, Integer.toString(maxBackupJournals));
        return this;
    }

    /**
     * Get bookie port that bookie server listen on
     *
     * @return bookie port
     */
    public int getBookiePort() {
        return this.getInt(BOOKIE_PORT, 3181);
    }

    /**
     * Set new bookie port that bookie server listen on
     *
     * @param port
     *          Port to listen on
     * @return server configuration
     */
    public ServerConfiguration setBookiePort(int port) {
        this.setProperty(BOOKIE_PORT, Integer.toString(port));
        return this;
    }

    /**
     * Get dir name to store journal files
     *
     * @return journal dir name
     */
    public String getJournalDirName() {
        return this.getString(JOURNAL_DIR, "/tmp/bk-txn");
    }

    /**
     * Set dir name to store journal files
     *
     * @param journalDir
     *          Dir to store journal files
     * @return server configuration
     */
    public ServerConfiguration setJournalDirName(String journalDir) {
        this.setProperty(JOURNAL_DIR, journalDir);
        return this;
    }

    /**
     * Get dir to store journal files
     *
     * @return journal dir, if no journal dir provided return null
     */
    public File getJournalDir() {
        String journalDirName = getJournalDirName();
        if (null == journalDirName) {
            return null;
        }
        return new File(journalDirName);
    }

    /**
     * Get dir names to store ledger data
     *
     * @return ledger dir names, if not provided return null
     */
    public String[] getLedgerDirNames() {
        String[] ledgerDirs = this.getStringArray(LEDGER_DIRS);
        if (null == ledgerDirs) {
            return new String[] { "/tmp/bk-data" };
        }
        return ledgerDirs;
    }

    /**
     * Set dir names to store ledger data
     *
     * @param ledgerDirs
     *          Dir names to store ledger data
     * @return server configuration
     */
    public ServerConfiguration setLedgerDirNames(String[] ledgerDirs) {
        if (null == ledgerDirs) {
            return this;
        }
        this.setProperty(LEDGER_DIRS, ledgerDirs);
        return this;
    }

    /**
     * Get dirs that stores ledger data
     *
     * @return ledger dirs
     */
    public File[] getLedgerDirs() {
        String[] ledgerDirNames = getLedgerDirNames();
        if (null == ledgerDirNames) {
            return null;
        }
        File[] ledgerDirs = new File[ledgerDirNames.length];
        for (int i = 0; i < ledgerDirNames.length; i++) {
            ledgerDirs[i] = new File(ledgerDirNames[i]);
        }
        return ledgerDirs;
    }

    /**
     * Is tcp connection no delay.
     *
     * @return tcp socket nodelay setting
     */
    public boolean getServerTcpNoDelay() {
        return getBoolean(SERVER_TCP_NODELAY, true);
    }

    /**
     * Set socket nodelay setting
     *
     * @param noDelay
     *          NoDelay setting
     * @return server configuration
     */
    public ServerConfiguration setServerTcpNoDelay(boolean noDelay) {
        setProperty(SERVER_TCP_NODELAY, Boolean.toString(noDelay));
        return this;
    }

    /**
     * Get zookeeper servers to connect
     *
     * @return zookeeper servers
     */
    public String getZkServers() {
        List<Object> servers = getList(ZK_SERVERS, null);
        if (null == servers || 0 == servers.size()) {
            return null;
        }
        return StringUtils.join(servers, ",");
    }

    /**
     * Set zookeeper servers to connect
     *
     * @param zkServers
     *          ZooKeeper servers to connect
     */
    public ServerConfiguration setZkServers(String zkServers) {
        setProperty(ZK_SERVERS, zkServers);
        return this;
    }

    /**
     * Get zookeeper timeout
     *
     * @return zookeeper server timeout
     */
    public int getZkTimeout() {
        return getInt(ZK_TIMEOUT, 10000);
    }

    /**
     * Set zookeeper timeout
     *
     * @param zkTimeout
     *          ZooKeeper server timeout
     * @return server configuration
     */
    public ServerConfiguration setZkTimeout(int zkTimeout) {
        setProperty(ZK_TIMEOUT, Integer.toString(zkTimeout));
        return this;
    }

    /**
     * Is statistics enabled
     *
     * @return is statistics enabled
     */
    public boolean isStatisticsEnabled() {
        return getBoolean(ENABLE_STATISTICS, true);
    }

    /**
     * Turn on/off statistics
     *
     * @param enabled
     *          Whether statistics enabled or not.
     * @return server configuration
     */
    public ServerConfiguration setStatisticsEnabled(boolean enabled) {
        setProperty(ENABLE_STATISTICS, Boolean.toString(enabled));
        return this;
    }

    /**
     * Get threshold of minor compaction.
     *
     * For those entry log files whose remaining size percentage reaches below
     * this threshold  will be compacted in a minor compaction.
     *
     * If it is set to less than zero, the minor compaction is disabled.
     *
     * @return threshold of minor compaction
     */
    public double getMinorCompactionThreshold() {
        return getDouble(MINOR_COMPACTION_THRESHOLD, 0.2f);
    }

    /**
     * Set threshold of minor compaction
     *
     * @see #getMinorCompactionThreshold()
     *
     * @param threshold
     *          Threshold for minor compaction
     * @return server configuration
     */
    public ServerConfiguration setMinorCompactionThreshold(double threshold) {
        setProperty(MINOR_COMPACTION_THRESHOLD, threshold);
        return this;
    }

    /**
     * Get threshold of major compaction.
     *
     * For those entry log files whose remaining size percentage reaches below
     * this threshold  will be compacted in a major compaction.
     *
     * If it is set to less than zero, the major compaction is disabled.
     *
     * @return threshold of major compaction
     */
    public double getMajorCompactionThreshold() {
        return getDouble(MAJOR_COMPACTION_THRESHOLD, 0.8f);
    }

    /**
     * Set threshold of major compaction.
     *
     * @see #getMajorCompactionThreshold()
     *
     * @param threshold
     *          Threshold of major compaction
     * @return server configuration
     */
    public ServerConfiguration setMajorCompactionThreshold(double threshold) {
        setProperty(MAJOR_COMPACTION_THRESHOLD, threshold);
        return this;
    }

    /**
     * Get interval to run minor compaction, in seconds.
     *
     * If it is set to less than zero, the minor compaction is disabled.
     *
     * @return threshold of minor compaction
     */
    public long getMinorCompactionInterval() {
        return getLong(MINOR_COMPACTION_INTERVAL, 3600);
    }

    /**
     * Set interval to run minor compaction
     *
     * @see #getMinorCompactionInterval()
     *
     * @param interval
     *          Interval to run minor compaction
     * @return server configuration
     */
    public ServerConfiguration setMinorCompactionInterval(long interval) {
        setProperty(MINOR_COMPACTION_INTERVAL, interval);
        return this;
    }

    /**
     * Get interval to run major compaction, in seconds.
     *
     * If it is set to less than zero, the major compaction is disabled.
     *
     * @return high water mark
     */
    public long getMajorCompactionInterval() {
        return getLong(MAJOR_COMPACTION_INTERVAL, 86400);
    }

    /**
     * Set interval to run major compaction.
     *
     * @see #getMajorCompactionInterval()
     *
     * @param interval
     *          Interval to run major compaction
     * @return server configuration
     */
    public ServerConfiguration setMajorCompactionInterval(long interval) {
        setProperty(MAJOR_COMPACTION_INTERVAL, interval);
        return this;
    }
    
    /**
     * Set the grace period which the rereplication worker will wait before
     * fencing and rereplicating a ledger fragment which is still being written
     * to, on bookie failure.
     * 
     * The grace period allows the writer to detect the bookie failure, and
     * start replicating the ledger fragment. If the writer writes nothing
     * during the grace period, the rereplication worker assumes that it has
     * crashed and fences the ledger, preventing any further writes to that 
     * ledger.
     * 
     * @see org.apache.bookkeeper.client.BookKeeper#openLedger
     * 
     * @param waitTime time to wait before replicating ledger fragment
     */
    public void setOpenLedgerRereplicationGracePeriod(String waitTime) {
        setProperty(OPEN_LEDGER_REREPLICATION_GRACE_PERIOD, waitTime);
    }

    /**
     * Get the grace period which the rereplication worker to wait before
     * fencing and rereplicating a ledger fragment which is still being written
     * to, on bookie failure.
     * 
     * @return long
     */
    public long getOpenLedgerRereplicationGracePeriod() {
        return getLong(OPEN_LEDGER_REREPLICATION_GRACE_PERIOD, 30000);
    }

    /**
     * Set the ReadOnlyModeEnabled status
     * 
     * @param enabled enables read-only mode.
     * 
     * @return ServerConfiguration 
     */
    public ServerConfiguration setReadOnlyModeEnabled(boolean enabled) {
        setProperty(READ_ONLY_MODE_ENABLED, enabled);
        return this;
    }

    /**
     * Get ReadOnlyModeEnabled status
     * 
     * @return boolean
     */
    public boolean isReadOnlyModeEnabled() {
        return getBoolean(READ_ONLY_MODE_ENABLED, false);
    }

    /**
     * Set the Disk free space threshold as a fraction of the total
     * after which disk will be considered as full during disk check.
     * 
     * @param threshold threshold to declare a disk full
     * 
     * @return ServerConfiguration
     */
    public ServerConfiguration setDiskUsageThreshold(float threshold) {
        setProperty(DISK_USAGE_THRESHOLD, threshold);
        return this;
    }

    /**
     * Returns disk free space threshold. By default it is 0.95.
     * 
     * @return float
     */
    public float getDiskUsageThreshold() {
        return getFloat(DISK_USAGE_THRESHOLD, 0.95f);
    }

    /**
     * Set the disk checker interval to monitor ledger disk space
     * 
     * @param interval interval between disk checks for space.
     * 
     * @return ServerConfiguration
     */
    public ServerConfiguration setDiskCheckInterval(int interval) {
        setProperty(DISK_CHECK_INTERVAL, interval);
        return this;
    }

    /**
     * Get the disk checker interval
     * 
     * @return int
     */
    public int getDiskCheckInterval() {
        return getInt(DISK_CHECK_INTERVAL, 10 * 1000);
    }

    /**
     * Set the regularity at which the auditor will run a check
     * of all ledgers. This should not be run very often, and at most,
     * once a day.
     *
     * @param interval The interval in seconds. e.g. 86400 = 1 day, 604800 = 1 week
     */
    public void setAuditorPeriodicCheckInterval(long interval) {
        setProperty(AUDITOR_PERIODIC_CHECK_INTERVAL, interval);
    }

    /**
     * Get the regularity at which the auditor checks all ledgers.
     * @return The interval in seconds
     */
    public long getAuditorPeriodicCheckInterval() {
        return getLong(AUDITOR_PERIODIC_CHECK_INTERVAL, 86400);
    }

    /**
     * Sets that whether the auto-recovery service can start along with Bookie
     * server itself or not
     *
     * @param enabled
     *            - true if need to start auto-recovery service. Otherwise
     *            false.
     * @return ServerConfiguration
     */
    public ServerConfiguration setAutoRecoveryDaemonEnabled(boolean enabled) {
        setProperty(AUTO_RECOVERY_DAEMON_ENABLED, enabled);
        return this;
    }

    /**
     * Get whether the Bookie itself can start auto-recovery service also or not
     *
     * @return true - if Bookie should start auto-recovery service along with
     *         it. false otherwise.
     */
    public boolean isAutoRecoveryDaemonEnabled() {
        return getBoolean(AUTO_RECOVERY_DAEMON_ENABLED, false);
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/jmx/BKMBeanInfo.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.bookkeeper.jmx;

import org.apache.zookeeper.jmx.ZKMBeanInfo;

/**
 * BookKeeper MBean info interface.
 */
public interface BKMBeanInfo extends ZKMBeanInfo {
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/jmx/BKMBeanRegistry.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.bookkeeper.jmx;

import javax.management.MalformedObjectNameException;
import javax.management.ObjectName;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.apache.zookeeper.jmx.MBeanRegistry;
import org.apache.zookeeper.jmx.ZKMBeanInfo;

/**
 * This class provides a unified interface for registering/unregistering of
 * bookkeeper MBeans with the platform MBean server. It builds a hierarchy of MBeans
 * where each MBean represented by a filesystem-like path. Eventually, this hierarchy
 * will be stored in the zookeeper data tree instance as a virtual data tree.
 */
public class BKMBeanRegistry extends MBeanRegistry {
    static final Logger LOG = LoggerFactory.getLogger(BKMBeanRegistry.class);
    
    static final String DOMAIN = "org.apache.BookKeeperService";

    static BKMBeanRegistry instance=new BKMBeanRegistry(); 

    public static BKMBeanRegistry getInstance(){
        return instance;
    }

    protected String getDomainName() {
        return DOMAIN;
    }
    
    /**
     * This takes a path, such as /a/b/c, and converts it to 
     * name0=a,name1=b,name2=c
     * 
     * Copy from zookeeper MBeanRegistry since tokenize is private
     */
    protected int tokenize(StringBuilder sb, String path, int index) {
        String[] tokens = path.split("/");
        for (String s: tokens) {
            if (s.length()==0)
                continue;
            sb.append("name").append(index++).append("=").append(s).append(",");
        }
        return index;
    }

    /**
     * Builds an MBean path and creates an ObjectName instance using the path. 
     * @param path MBean path
     * @param bean the MBean instance
     * @return ObjectName to be registered with the platform MBean server
     */
    protected ObjectName makeObjectName(String path, ZKMBeanInfo bean)
        throws MalformedObjectNameException {
        if(path==null)
            return null;
        StringBuilder beanName = new StringBuilder(getDomainName() + ":");
        int counter=0;
        counter=tokenize(beanName,path,counter);
        tokenize(beanName,bean.getName(),counter);
        beanName.deleteCharAt(beanName.length()-1);
        try {
            return new ObjectName(beanName.toString());
        } catch (MalformedObjectNameException e) {
            LOG.warn("Invalid name \"" + beanName.toString() + "\" for class "
                    + bean.getClass().toString());
            throw e;
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/meta/AbstractZkLedgerManager.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.meta;

import java.io.IOException;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

import org.apache.bookkeeper.conf.AbstractConfiguration;
import org.apache.bookkeeper.client.LedgerMetadata;
import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.meta.LedgerManager;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.MultiCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.Processor;
import org.apache.bookkeeper.util.BookKeeperConstants;
import org.apache.bookkeeper.versioning.Version;
import org.apache.bookkeeper.util.ZkUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.zookeeper.AsyncCallback;
import org.apache.zookeeper.AsyncCallback.DataCallback;
import org.apache.zookeeper.AsyncCallback.VoidCallback;
import org.apache.zookeeper.AsyncCallback.StatCallback;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.KeeperException.Code;
import org.apache.zookeeper.ZooKeeper;
import org.apache.zookeeper.data.Stat;

/**
 * Abstract ledger manager based on zookeeper, which provides common methods such as query zk nodes.
 */
abstract class AbstractZkLedgerManager implements LedgerManager {

    static Logger LOG = LoggerFactory.getLogger(AbstractZkLedgerManager.class);

    protected final AbstractConfiguration conf;
    protected final ZooKeeper zk;
    protected final String ledgerRootPath;

    /**
     * ZooKeeper-based Ledger Manager Constructor
     *
     * @param conf
     *          Configuration object
     * @param zk
     *          ZooKeeper Client Handle
     */
    protected AbstractZkLedgerManager(AbstractConfiguration conf, ZooKeeper zk) {
        this.conf = conf;
        this.zk = zk;
        this.ledgerRootPath = conf.getZkLedgersRootPath();
    }

    /**
     * Get the znode path that is used to store ledger metadata
     *
     * @param ledgerId
     *          Ledger ID
     * @return ledger node path
     */
    protected abstract String getLedgerPath(long ledgerId);

    /**
     * Get ledger id from its znode ledger path
     *
     * @param ledgerPath
     *          Ledger path to store metadata
     * @return ledger id
     * @throws IOException when the ledger path is invalid
     */
    protected abstract long getLedgerId(String ledgerPath) throws IOException;

    /**
     * Removes ledger metadata from ZooKeeper if version matches.
     *
     * @param   ledgerId    ledger identifier
     * @param   version     local version of metadata znode
     * @param   cb          callback object
     */
    @Override
    public void removeLedgerMetadata(final long ledgerId, final Version version,
            final GenericCallback<Void> cb) {
        int znodeVersion = -1;
        if (Version.NEW == version) {
            LOG.error("Request to delete ledger {} metadata with version set to the initial one", ledgerId);
            cb.operationComplete(BKException.Code.MetadataVersionException, (Void)null);
            return;
        } else if (Version.ANY != version) {
            if (!(version instanceof ZkVersion)) {
                LOG.info("Not an instance of ZKVersion: {}", ledgerId);
                cb.operationComplete(BKException.Code.MetadataVersionException, (Void)null);
                return;
            } else {
                znodeVersion = ((ZkVersion)version).getZnodeVersion();
            }
        }

        zk.delete(getLedgerPath(ledgerId), znodeVersion, new VoidCallback() {
            @Override
            public void processResult(int rc, String path, Object ctx) {
                int bkRc;
                if (rc == KeeperException.Code.NONODE.intValue()) {
                    LOG.warn("Ledger node does not exist in ZooKeeper: ledgerId={}", ledgerId);
                    bkRc = BKException.Code.NoSuchLedgerExistsException;
                } else if (rc == KeeperException.Code.OK.intValue()) {
                    bkRc = BKException.Code.OK;
                } else {
                    bkRc = BKException.Code.ZKException;
                }
                cb.operationComplete(bkRc, (Void)null);
            }
        }, null);
    }

    @Override
    public void readLedgerMetadata(final long ledgerId, final GenericCallback<LedgerMetadata> readCb) {
        zk.getData(getLedgerPath(ledgerId), false, new DataCallback() {
            @Override
            public void processResult(int rc, String path, Object ctx, byte[] data, Stat stat) {
                if (rc == KeeperException.Code.NONODE.intValue()) {
                    if (LOG.isDebugEnabled()) {
                        LOG.debug("No such ledger: " + ledgerId,
                                  KeeperException.create(KeeperException.Code.get(rc), path));
                    }
                    readCb.operationComplete(BKException.Code.NoSuchLedgerExistsException, null);
                    return;
                }
                if (rc != KeeperException.Code.OK.intValue()) {
                    LOG.error("Could not read metadata for ledger: " + ledgerId,
                              KeeperException.create(KeeperException.Code.get(rc), path));
                    readCb.operationComplete(BKException.Code.ZKException, null);
                    return;
                }

                LedgerMetadata metadata;
                try {
                    metadata = LedgerMetadata.parseConfig(data, new ZkVersion(stat.getVersion()));
                } catch (IOException e) {
                    LOG.error("Could not parse ledger metadata for ledger: " + ledgerId, e);
                    readCb.operationComplete(BKException.Code.ZKException, null);
                    return;
                }
                readCb.operationComplete(BKException.Code.OK, metadata);
            }
        }, null);
    }

    @Override
    public void writeLedgerMetadata(final long ledgerId, final LedgerMetadata metadata,
                                    final GenericCallback<Void> cb) {
        Version v = metadata.getVersion();
        if (Version.NEW == v || !(v instanceof ZkVersion)) {
            cb.operationComplete(BKException.Code.MetadataVersionException, null);
            return;
        }
        final ZkVersion zv = (ZkVersion) v;
        zk.setData(getLedgerPath(ledgerId),
                   metadata.serialize(), zv.getZnodeVersion(),
                   new StatCallback() {
            @Override
            public void processResult(int rc, String path, Object ctx, Stat stat) {
                if (KeeperException.Code.BadVersion == rc) {
                    cb.operationComplete(BKException.Code.MetadataVersionException, null);
                } else if (KeeperException.Code.OK.intValue() == rc) {
                    // update metadata version
                    metadata.setVersion(zv.setZnodeVersion(stat.getVersion()));
                    cb.operationComplete(BKException.Code.OK, null);
                } else {
                    LOG.warn("Conditional update ledger metadata failed: ", KeeperException.Code.get(rc));
                    cb.operationComplete(BKException.Code.ZKException, null);
                }
            }
        }, null);
    }

    /**
     * Process ledgers in a single zk node.
     *
     * <p>
     * for each ledger found in this zk node, processor#process(ledgerId) will be triggerred
     * to process a specific ledger. after all ledgers has been processed, the finalCb will
     * be called with provided context object. The RC passed to finalCb is decided by :
     * <ul>
     * <li> All ledgers are processed successfully, successRc will be passed.
     * <li> Either ledger is processed failed, failureRc will be passed.
     * </ul>
     * </p>
     *
     * @param path
     *          Zk node path to store ledgers
     * @param processor
     *          Processor provided to process ledger
     * @param finalCb
     *          Callback object when all ledgers are processed
     * @param ctx
     *          Context object passed to finalCb
     * @param successRc
     *          RC passed to finalCb when all ledgers are processed successfully
     * @param failureRc
     *          RC passed to finalCb when either ledger is processed failed
     */
    protected void asyncProcessLedgersInSingleNode(
            final String path, final Processor<Long> processor,
            final AsyncCallback.VoidCallback finalCb, final Object ctx,
            final int successRc, final int failureRc) {
        ZkUtils.getChildrenInSingleNode(zk, path, new GenericCallback<List<String>>() {
            @Override
            public void operationComplete(int rc, List<String> ledgerNodes) {
                if (Code.OK.intValue() != rc) {
                    finalCb.processResult(failureRc, null, ctx);
                    return;
                }

                Set<Long> zkActiveLedgers = ledgerListToSet(ledgerNodes, path);
                LOG.debug("Processing ledgers: {}", zkActiveLedgers);

                // no ledgers found, return directly
                if (zkActiveLedgers.size() == 0) {
                    finalCb.processResult(successRc, null, ctx);
                    return;
                }

                MultiCallback mcb = new MultiCallback(zkActiveLedgers.size(), finalCb, ctx,
                                                      successRc, failureRc);
                // start loop over all ledgers
                for (Long ledger : zkActiveLedgers) {
                    processor.process(ledger, mcb);
                }
            }
        });
    }

    /**
     * Whether the znode a special znode
     *
     * @param znode
     *          Znode Name
     * @return true  if the znode is a special znode otherwise false
     */
    protected boolean isSpecialZnode(String znode) {
        if (BookKeeperConstants.AVAILABLE_NODE.equals(znode)
                || BookKeeperConstants.COOKIE_NODE.equals(znode)
                || BookKeeperConstants.LAYOUT_ZNODE.equals(znode)
                || BookKeeperConstants.INSTANCEID.equals(znode)
                || BookKeeperConstants.UNDER_REPLICATION_NODE.equals(znode)) {
            return true;
        }
        return false;
    }

    /**
     * Convert the ZK retrieved ledger nodes to a HashSet for easier comparisons.
     *
     * @param ledgerNodes
     *          zk ledger nodes
     * @param path
     *          the prefix path of the ledger nodes
     * @return ledger id hash set
     */
    protected Set<Long> ledgerListToSet(List<String> ledgerNodes, String path) {
        Set<Long> zkActiveLedgers = new HashSet<Long>(ledgerNodes.size(), 1.0f);
        for (String ledgerNode : ledgerNodes) {
            if (isSpecialZnode(ledgerNode)) {
                continue;
            }
            try {
                // convert the node path to ledger id according to different ledger manager implementation
                zkActiveLedgers.add(getLedgerId(path + "/" + ledgerNode));
            } catch (IOException e) {
                LOG.warn("Error extracting ledgerId from ZK ledger node: " + ledgerNode);
                // This is a pretty bad error as it indicates a ledger node in ZK
                // has an incorrect format. For now just continue and consider
                // this as a non-existent ledger.
                continue;
            }
        }
        return zkActiveLedgers;
    }

    @Override
    public void close() {
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/meta/FlatLedgerManager.java,true,"package org.apache.bookkeeper.meta;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.IOException;
import java.util.NoSuchElementException;
import java.util.Set;

import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.client.LedgerMetadata;
import org.apache.bookkeeper.conf.AbstractConfiguration;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.Processor;
import org.apache.bookkeeper.util.StringUtils;
import org.apache.bookkeeper.util.ZkUtils;
import org.apache.zookeeper.AsyncCallback;
import org.apache.zookeeper.AsyncCallback.StringCallback;
import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.KeeperException.Code;
import org.apache.zookeeper.ZooDefs.Ids;
import org.apache.zookeeper.ZooKeeper;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Manage all ledgers in a single zk node.
 *
 * <p>
 * All ledgers' metadata are put in a single zk node, created using zk sequential node.
 * Each ledger node is prefixed with 'L'.
 * </p>
 */
class FlatLedgerManager extends AbstractZkLedgerManager {

    static final Logger LOG = LoggerFactory.getLogger(FlatLedgerManager.class);
    // path prefix to store ledger znodes
    private final String ledgerPrefix;

    /**
     * Constructor
     *
     * @param conf
     *          Configuration object
     * @param zk
     *          ZooKeeper Client Handle
     * @param ledgerRootPath
     *          ZooKeeper Path to store ledger metadata
     * @throws IOException when version is not compatible
     */
    public FlatLedgerManager(AbstractConfiguration conf, ZooKeeper zk) {
        super(conf, zk);

        ledgerPrefix = ledgerRootPath + "/" + StringUtils.LEDGER_NODE_PREFIX;
    }

    @Override
    public void createLedger(final LedgerMetadata metadata, final GenericCallback<Long> cb) {
        StringCallback scb = new StringCallback() {
            @Override
            public void processResult(int rc, String path, Object ctx,
                    String name) {
                if (Code.OK.intValue() != rc) {
                    LOG.error("Could not create node for ledger",
                              KeeperException.create(KeeperException.Code.get(rc), path));
                    cb.operationComplete(rc, null);
                } else {
                    // update znode status
                    metadata.setVersion(new ZkVersion(0));
                    try {
                        long ledgerId = getLedgerId(name);
                        cb.operationComplete(rc, ledgerId);
                    } catch (IOException ie) {
                        LOG.error("Could not extract ledger-id from path:" + name, ie);
                        cb.operationComplete(BKException.Code.ZKException, null);
                    }
                }
            }
        };
        ZkUtils.createFullPathOptimistic(zk, ledgerPrefix, metadata.serialize(),
            Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT_SEQUENTIAL, scb, null);
    }

    @Override
    public String getLedgerPath(long ledgerId) {
        StringBuilder sb = new StringBuilder();
        sb.append(ledgerPrefix)
          .append(StringUtils.getZKStringId(ledgerId));
        return sb.toString();
    }

    @Override
    public long getLedgerId(String nodeName) throws IOException {
        long ledgerId;
        try {
            String parts[] = nodeName.split(ledgerPrefix);
            ledgerId = Long.parseLong(parts[parts.length - 1]);
        } catch (NumberFormatException e) {
            throw new IOException(e);
        }
        return ledgerId;
    }

    @Override
    public void asyncProcessLedgers(final Processor<Long> processor,
                                    final AsyncCallback.VoidCallback finalCb, final Object ctx,
                                    final int successRc, final int failureRc) {
        asyncProcessLedgersInSingleNode(ledgerRootPath, processor, finalCb, ctx, successRc, failureRc);
    }

    @Override
    public LedgerRangeIterator getLedgerRanges() {
        return new LedgerRangeIterator() {
            // single iterator, can visit only one time
            boolean hasMoreElement = true;
            @Override
            public boolean hasNext() {
                return hasMoreElement;
            }
            @Override
            public LedgerRange next() throws IOException {
                if (!hasMoreElement) {
                    throw new NoSuchElementException();
                }
                hasMoreElement = false;
                Set<Long> zkActiveLedgers;
                try {
                    zkActiveLedgers = ledgerListToSet(
                            ZkUtils.getChildrenInSingleNode(zk, ledgerRootPath), ledgerRootPath);
                } catch (InterruptedException e) {
                    throw new IOException("Error when get child nodes from zk", e);
                }
                return new LedgerRange(zkActiveLedgers);
            }
        };
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/meta/FlatLedgerManagerFactory.java,false,"package org.apache.bookkeeper.meta;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.IOException;
import java.util.List;

import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.ZKUtil;
import org.apache.bookkeeper.replication.ReplicationException;
import org.apache.bookkeeper.conf.AbstractConfiguration;
import org.apache.zookeeper.ZooKeeper;

/**
 * Flat Ledger Manager Factory
 */
class FlatLedgerManagerFactory extends LedgerManagerFactory {

    public static final String NAME = "flat";
    public static final int CUR_VERSION = 1;

    AbstractConfiguration conf;
    ZooKeeper zk;

    @Override
    public int getCurrentVersion() {
        return CUR_VERSION;
    }

    @Override
    public LedgerManagerFactory initialize(final AbstractConfiguration conf,
                                           final ZooKeeper zk,
                                           final int factoryVersion)
    throws IOException {
        if (CUR_VERSION != factoryVersion) {
            throw new IOException("Incompatible layout version found : "
                                + factoryVersion);
        }
        this.conf = conf;
        this.zk = zk;
        return this;
    }

    @Override
    public void uninitialize() throws IOException {
        // since zookeeper instance is passed from outside
        // we don't need to close it here
    }

    @Override
    public LedgerManager newLedgerManager() {
        return new FlatLedgerManager(conf, zk);
    }

    @Override
    public LedgerUnderreplicationManager newLedgerUnderreplicationManager()
            throws KeeperException, InterruptedException, ReplicationException.CompatibilityException {
        return new ZkLedgerUnderreplicationManager(conf, zk);
    }

    @Override
    public void format(AbstractConfiguration conf, ZooKeeper zk)
            throws InterruptedException, KeeperException, IOException {
        FlatLedgerManager ledgerManager = (FlatLedgerManager) newLedgerManager();
        String ledgersRootPath = conf.getZkLedgersRootPath();
        List<String> children = zk.getChildren(ledgersRootPath, false);
        for (String child : children) {
            if (ledgerManager.isSpecialZnode(child)) {
                continue;
            }
            ZKUtil.deleteRecursive(zk, ledgersRootPath + "/" + child);
        }
        // Delete and recreate the LAYOUT information.
        super.format(conf, zk);
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/meta/HierarchicalLedgerManager.java,true,"package org.apache.bookkeeper.meta;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.IOException;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.Iterator;
import java.util.List;
import java.util.NoSuchElementException;
import java.util.Set;

import org.apache.bookkeeper.client.LedgerMetadata;
import org.apache.bookkeeper.conf.AbstractConfiguration;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.Processor;
import org.apache.bookkeeper.util.StringUtils;
import org.apache.bookkeeper.util.ZkUtils;
import org.apache.zookeeper.AsyncCallback;
import org.apache.zookeeper.AsyncCallback.StringCallback;
import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.KeeperException.Code;
import org.apache.zookeeper.ZooDefs.Ids;
import org.apache.zookeeper.ZooKeeper;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Hierarchical Ledger Manager which manages ledger meta in zookeeper using 2-level hierarchical znodes.
 *
 * <p>
 * Hierarchical Ledger Manager first obtain a global unique id from zookeeper using a EPHEMERAL_SEQUENTIAL
 * znode <i>(ledgersRootPath)/ledgers/idgen/ID-</i>.
 * Since zookeeper sequential counter has a format of %10d -- that is 10 digits with 0 (zero) padding, i.e.
 * "&lt;path&gt;0000000001", HierarchicalLedgerManager splits the generated id into 3 parts (2-4-4):
 * <pre>&lt;level1 (2 digits)&gt;&lt;level2 (4 digits)&gt;&lt;level3 (4 digits)&gt;</pre>
 * These 3 parts are used to form the actual ledger node path used to store ledger metadata:
 * <pre>(ledgersRootPath)/level1/level2/L(level3)</pre>
 * E.g Ledger 0000000001 is split into 3 parts <i>00</i>, <i>0000</i>, <i>0001</i>, which is stored in
 * <i>(ledgersRootPath)/00/0000/L0001</i>. So each znode could have at most 10000 ledgers, which avoids
 * errors during garbage collection due to lists of children that are too long.
 */
class HierarchicalLedgerManager extends AbstractZkLedgerManager {

    static final Logger LOG = LoggerFactory.getLogger(HierarchicalLedgerManager.class);

    static final String IDGEN_ZNODE = "idgen";
    static final String IDGENERATION_PREFIX = "/" + IDGEN_ZNODE + "/ID-";
    private static final String MAX_ID_SUFFIX = "9999";
    private static final String MIN_ID_SUFFIX = "0000";

    // Path to generate global id
    private final String idGenPath;

    // we use this to prevent long stack chains from building up in callbacks
    ScheduledExecutorService scheduler;

    /**
     * Constructor
     *
     * @param conf
     *          Configuration object
     * @param zk
     *          ZooKeeper Client Handle
     */
    public HierarchicalLedgerManager(AbstractConfiguration conf, ZooKeeper zk) {
        super(conf, zk);

        this.idGenPath = ledgerRootPath + IDGENERATION_PREFIX;
        this.scheduler = Executors.newSingleThreadScheduledExecutor();
        LOG.debug("Using HierarchicalLedgerManager with root path : {}", ledgerRootPath);
    }

    @Override
    public void close() {
        try {
            scheduler.shutdown();
        } catch (Exception e) {
            LOG.warn("Error when closing HierarchicalLedgerManager : ", e);
        }
        super.close();
    }

    @Override
    public void createLedger(final LedgerMetadata metadata, final GenericCallback<Long> ledgerCb) {
        ZkUtils.createFullPathOptimistic(zk, idGenPath, new byte[0], Ids.OPEN_ACL_UNSAFE,
            CreateMode.EPHEMERAL_SEQUENTIAL, new StringCallback() {
            @Override
            public void processResult(int rc, String path, Object ctx, final String idPathName) {
                if (rc != KeeperException.Code.OK.intValue()) {
                    LOG.error("Could not generate new ledger id",
                              KeeperException.create(KeeperException.Code.get(rc), path));
                    ledgerCb.operationComplete(rc, null);
                    return;
                }
                /*
                 * Extract ledger id from gen path
                 */
                long ledgerId;
                try {
                    ledgerId = getLedgerIdFromGenPath(idPathName);
                } catch (IOException e) {
                    LOG.error("Could not extract ledger-id from id gen path:" + path, e);
                    ledgerCb.operationComplete(KeeperException.Code.SYSTEMERROR.intValue(), null);
                    return;
                }
                String ledgerPath = getLedgerPath(ledgerId);
                final long lid = ledgerId;
                StringCallback scb = new StringCallback() {
                    @Override
                    public void processResult(int rc, String path,
                            Object ctx, String name) {
                        if (rc != KeeperException.Code.OK.intValue()) {
                            LOG.error("Could not create node for ledger",
                                      KeeperException.create(KeeperException.Code.get(rc), path));
                            ledgerCb.operationComplete(rc, null);
                        } else {
                            // update version
                            metadata.setVersion(new ZkVersion(0));
                            ledgerCb.operationComplete(rc, lid);
                        }
                    }
                };
                ZkUtils.createFullPathOptimistic(zk, ledgerPath, metadata.serialize(),
                    Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT, scb, null);
                // delete the znode for id generation
                scheduler.submit(new Runnable() {
                    @Override
                    public void run() {
                        zk.delete(idPathName, -1, new AsyncCallback.VoidCallback() {
                            @Override
                            public void processResult(int rc, String path, Object ctx) {
                                if (rc != KeeperException.Code.OK.intValue()) {
                                    LOG.warn("Exception during deleting znode for id generation : ",
                                             KeeperException.create(KeeperException.Code.get(rc), path));
                                } else {
                                    LOG.debug("Deleting znode for id generation : {}", idPathName);
                                }
                            }
                        }, null);
                    }
                });
            }
        }, null);
    }

    // get ledger id from generation path
    private long getLedgerIdFromGenPath(String nodeName) throws IOException {
        long ledgerId;
        try {
            String parts[] = nodeName.split(IDGENERATION_PREFIX);
            ledgerId = Long.parseLong(parts[parts.length - 1]);
        } catch (NumberFormatException e) {
            throw new IOException(e);
        }
        return ledgerId;
    }

    @Override
    public String getLedgerPath(long ledgerId) {
        return ledgerRootPath + StringUtils.getHierarchicalLedgerPath(ledgerId);
    }

    @Override
    public long getLedgerId(String pathName) throws IOException {
        if (!pathName.startsWith(ledgerRootPath)) {
            throw new IOException("it is not a valid hashed path name : " + pathName);
        }
        String hierarchicalPath = pathName.substring(ledgerRootPath.length() + 1);
        return StringUtils.stringToHierarchicalLedgerId(hierarchicalPath);
    }

    // get ledger from all level nodes
    private long getLedgerId(String...levelNodes) throws IOException {
        return StringUtils.stringToHierarchicalLedgerId(levelNodes);
    }

    //
    // Active Ledger Manager
    //

    /**
     * Get the smallest cache id in a specified node /level1/level2
     *
     * @param level1
     *          1st level node name
     * @param level2
     *          2nd level node name
     * @return the smallest ledger id
     */
    private long getStartLedgerIdByLevel(String level1, String level2) throws IOException {
        return getLedgerId(level1, level2, MIN_ID_SUFFIX);
    }

    /**
     * Get the largest cache id in a specified node /level1/level2
     *
     * @param level1
     *          1st level node name
     * @param level2
     *          2nd level node name
     * @return the largest ledger id
     */
    private long getEndLedgerIdByLevel(String level1, String level2) throws IOException {
        return getLedgerId(level1, level2, MAX_ID_SUFFIX);
    }

    @Override
    public void asyncProcessLedgers(final Processor<Long> processor,
                                    final AsyncCallback.VoidCallback finalCb, final Object context,
                                    final int successRc, final int failureRc) {
        // process 1st level nodes
        asyncProcessLevelNodes(ledgerRootPath, new Processor<String>() {
            @Override
            public void process(final String l1Node, final AsyncCallback.VoidCallback cb1) {
                if (isSpecialZnode(l1Node)) {
                    cb1.processResult(successRc, null, context);
                    return;
                }
                final String l1NodePath = ledgerRootPath + "/" + l1Node;
                // process level1 path, after all children of level1 process
                // it callback to continue processing next level1 node
                asyncProcessLevelNodes(l1NodePath, new Processor<String>() {
                    @Override
                    public void process(String l2Node, AsyncCallback.VoidCallback cb2) {
                        // process level1/level2 path
                        String l2NodePath = ledgerRootPath + "/" + l1Node + "/" + l2Node;
                        // process each ledger
                        // after all ledger are processed, cb2 will be call to continue processing next level2 node
                        asyncProcessLedgersInSingleNode(l2NodePath, processor, cb2,
                                                        context, successRc, failureRc);
                    }
                }, cb1, context, successRc, failureRc);
            }
        }, finalCb, context, successRc, failureRc);
    }

    /**
     * Process hash nodes in a given path
     */
    private void asyncProcessLevelNodes(
        final String path, final Processor<String> processor,
        final AsyncCallback.VoidCallback finalCb, final Object context,
        final int successRc, final int failureRc) {
        zk.sync(path, new AsyncCallback.VoidCallback() {
            @Override
            public void processResult(int rc, String path, Object ctx) {
                if (rc != Code.OK.intValue()) {
                    LOG.error("Error syncing path " + path + " when getting its chidren: ",
                              KeeperException.create(KeeperException.Code.get(rc), path));
                    finalCb.processResult(failureRc, null, context);
                    return;
                }

                zk.getChildren(path, false, new AsyncCallback.ChildrenCallback() {
                    @Override
                    public void processResult(int rc, String path, Object ctx,
                                              List<String> levelNodes) {
                        if (rc != Code.OK.intValue()) {
                            LOG.error("Error polling hash nodes of " + path,
                                      KeeperException.create(KeeperException.Code.get(rc), path));
                            finalCb.processResult(failureRc, null, context);
                            return;
                        }
                        AsyncListProcessor<String> listProcessor =
                                new AsyncListProcessor<String>(scheduler);
                        // process its children
                        listProcessor.process(levelNodes, processor, finalCb,
                                              context, successRc, failureRc);
                    }
                }, null);
            }
        }, null);
    }

    /**
     * Process list one by one in asynchronize way. Process will be stopped immediately
     * when error occurred.
     */
    private static class AsyncListProcessor<T> {
        // use this to prevent long stack chains from building up in callbacks
        ScheduledExecutorService scheduler;

        /**
         * Constructor
         *
         * @param scheduler
         *          Executor used to prevent long stack chains
         */
        public AsyncListProcessor(ScheduledExecutorService scheduler) {
            this.scheduler = scheduler;
        }

        /**
         * Process list of items
         *
         * @param data
         *          List of data to process
         * @param processor
         *          Callback to process element of list when success
         * @param finalCb
         *          Final callback to be called after all elements in the list are processed
         * @param contxt
         *          Context of final callback
         * @param successRc
         *          RC passed to final callback on success
         * @param failureRc
         *          RC passed to final callback on failure
         */
        public void process(final List<T> data, final Processor<T> processor,
                            final AsyncCallback.VoidCallback finalCb, final Object context,
                            final int successRc, final int failureRc) {
            if (data == null || data.size() == 0) {
                finalCb.processResult(successRc, null, context);
                return;
            }
            final int size = data.size();
            final AtomicInteger current = new AtomicInteger(0);
            AsyncCallback.VoidCallback stubCallback = new AsyncCallback.VoidCallback() {
                @Override
                public void processResult(int rc, String path, Object ctx) {
                    if (rc != successRc) {
                        // terminal immediately
                        finalCb.processResult(failureRc, null, context);
                        return;
                    }
                    // process next element
                    int next = current.incrementAndGet();
                    if (next >= size) { // reach the end of list
                        finalCb.processResult(successRc, null, context);
                        return;
                    }
                    final T dataToProcess = data.get(next);
                    final AsyncCallback.VoidCallback stub = this;
                    scheduler.submit(new Runnable() {
                        @Override
                        public final void run() {
                            processor.process(dataToProcess, stub);
                        }
                    });
                }
            };
            T firstElement = data.get(0);
            processor.process(firstElement, stubCallback);
        }
    }

    @Override
    protected boolean isSpecialZnode(String znode) {
        return IDGEN_ZNODE.equals(znode) || super.isSpecialZnode(znode);
    }

    @Override
    public LedgerRangeIterator getLedgerRanges() {
        return new HierarchicalLedgerRangeIterator();
    }

    /**
     * Iterator through each metadata bucket with hierarchical mode
     */
    private class HierarchicalLedgerRangeIterator implements LedgerRangeIterator {
        private Iterator<String> l1NodesIter = null;
        private Iterator<String> l2NodesIter = null;
        private String curL1Nodes = "";
        private boolean hasMoreElement = true;

        /**
         * iterate next level1 znode
         *
         * @return false if have visited all level1 nodes
         * @throws InterruptedException/KeeperException if error occurs reading zookeeper children
         */
        private boolean nextL1Node() throws KeeperException, InterruptedException {
            l2NodesIter = null;
            while (l2NodesIter == null) {
                if (l1NodesIter.hasNext()) {
                    curL1Nodes = l1NodesIter.next();
                } else {
                    return false;
                }
                if (isSpecialZnode(curL1Nodes)) {
                    continue;
                }
                List<String> l2Nodes = zk.getChildren(ledgerRootPath + "/" + curL1Nodes, null);
                l2NodesIter = l2Nodes.iterator();
                if (!l2NodesIter.hasNext()) {
                    l2NodesIter = null;
                    continue;
                }
            }
            return true;
        }

        @Override
        public boolean hasNext() throws IOException {
            try {
                if (l1NodesIter == null) {
                    l1NodesIter = zk.getChildren(ledgerRootPath, null).iterator();
                    hasMoreElement = nextL1Node();
                } else if (!l2NodesIter.hasNext()) {
                    hasMoreElement = nextL1Node();
                }
            } catch (Exception e) {
                throw new IOException("Error when check more elements", e);
            }
            return hasMoreElement;
        }

        @Override
        public LedgerRange next() throws IOException {
            if (!hasMoreElement) {
                throw new NoSuchElementException();
            }
            return getLedgerRangeByLevel(curL1Nodes, l2NodesIter.next());
        }

        /**
         * Get a single node level1/level2
         *
         * @param level1
         *          1st level node name
         * @param level2
         *          2nd level node name
         * @throws IOException
         */
        LedgerRange getLedgerRangeByLevel(final String level1, final String level2)
                throws IOException {
            StringBuilder nodeBuilder = new StringBuilder();
            nodeBuilder.append(ledgerRootPath).append("/")
                       .append(level1).append("/").append(level2);
            String nodePath = nodeBuilder.toString();
            List<String> ledgerNodes = null;
            try {
                ledgerNodes = ZkUtils.getChildrenInSingleNode(zk, nodePath);
            } catch (InterruptedException e) {
                throw new IOException("Error when get child nodes from zk", e);
            }
            Set<Long> zkActiveLedgers = ledgerListToSet(ledgerNodes, nodePath);
            if (LOG.isDebugEnabled()) {
                LOG.debug("All active ledgers from ZK for hash node "
                          + level1 + "/" + level2 + " : " + zkActiveLedgers);
            }
            return new LedgerRange(zkActiveLedgers,
                    getStartLedgerIdByLevel(level1, level2), getEndLedgerIdByLevel(level1, level2));
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/meta/HierarchicalLedgerManagerFactory.java,false,"package org.apache.bookkeeper.meta;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.IOException;
import java.util.List;

import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.ZKUtil;
import org.apache.bookkeeper.replication.ReplicationException;
import org.apache.bookkeeper.conf.AbstractConfiguration;
import org.apache.zookeeper.ZooKeeper;

/**
 * Hierarchical Ledger Manager Factory
 */
class HierarchicalLedgerManagerFactory extends LedgerManagerFactory {

    public static final String NAME = "hierarchical";
    public static final int CUR_VERSION = 1;

    AbstractConfiguration conf;
    ZooKeeper zk;

    @Override
    public int getCurrentVersion() {
        return CUR_VERSION;
    }

    @Override
    public LedgerManagerFactory initialize(final AbstractConfiguration conf,
                                           final ZooKeeper zk,
                                           final int factoryVersion)
    throws IOException {
        if (CUR_VERSION != factoryVersion) {
            throw new IOException("Incompatible layout version found : "
                                + factoryVersion);
        }
        this.conf = conf;
        this.zk = zk;
        return this;
    }

    @Override
    public void uninitialize() throws IOException {
        // since zookeeper instance is passed from outside
        // we don't need to close it here
    }

    @Override
    public LedgerManager newLedgerManager() {
        return new HierarchicalLedgerManager(conf, zk);
    }

    @Override
    public LedgerUnderreplicationManager newLedgerUnderreplicationManager()
            throws KeeperException, InterruptedException, ReplicationException.CompatibilityException{
        return new ZkLedgerUnderreplicationManager(conf, zk);
    }

    @Override
    public void format(AbstractConfiguration conf, ZooKeeper zk)
            throws InterruptedException, KeeperException, IOException {
        HierarchicalLedgerManager ledgerManager = (HierarchicalLedgerManager) newLedgerManager();
        String ledgersRootPath = conf.getZkLedgersRootPath();
        List<String> children = zk.getChildren(ledgersRootPath, false);
        for (String child : children) {
            if (!HierarchicalLedgerManager.IDGEN_ZNODE.equals(child)
                    && ledgerManager.isSpecialZnode(child)) {
                continue;
            }
            ZKUtil.deleteRecursive(zk, ledgersRootPath + "/" + child);
        }
        // Delete and recreate the LAYOUT information.
        super.format(conf, zk);
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/meta/LedgerLayout.java,false,"package org.apache.bookkeeper.meta;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.IOException;

import org.apache.bookkeeper.util.BookKeeperConstants;
import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.ZooDefs.Ids;
import org.apache.zookeeper.ZooKeeper;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * This class encapsulates ledger layout information that is persistently stored
 * in zookeeper. It provides parsing and serialization methods of such information.
 *
 */
class LedgerLayout {
    static final Logger LOG = LoggerFactory.getLogger(LedgerLayout.class);

   
    // version of compability layout version
    public static final int LAYOUT_MIN_COMPAT_VERSION = 1;
    // version of ledger layout metadata
    public static final int LAYOUT_FORMAT_VERSION = 2;

    /**
     * Read ledger layout from zookeeper
     *
     * @param zk            ZooKeeper Client
     * @param ledgersRoot   Root of the ledger namespace to check
     * @return ledger layout, or null if none set in zookeeper
     */
    public static LedgerLayout readLayout(final ZooKeeper zk, final String ledgersRoot)
            throws IOException, KeeperException {
        String ledgersLayout = ledgersRoot + "/" + BookKeeperConstants.LAYOUT_ZNODE;

        try {
            LedgerLayout layout;

            try {
                byte[] layoutData = zk.getData(ledgersLayout, false, null);
                layout = parseLayout(layoutData);
            } catch (KeeperException.NoNodeException nne) {
                return null;
            }
            
            return layout;
        } catch (InterruptedException ie) {
            throw new IOException(ie);
        }
    }

    static final String splitter = ":";
    static final String lSplitter = "\n";

    // ledger manager factory class
    private String managerFactoryCls;
    // ledger manager version
    private int managerVersion;

    // layout version of how to store layout information
    private int layoutFormatVersion = LAYOUT_FORMAT_VERSION;

    /**
     * Ledger Layout Constructor
     *
     * @param managerFactoryCls
     *          Ledger Manager Factory Class
     * @param managerVersion
     *          Ledger Manager Version
     * @param layoutFormatVersion
     *          Ledger Layout Format Version
     */
    public LedgerLayout(String managerFactoryCls, int managerVersion) {
        this(managerFactoryCls, managerVersion, LAYOUT_FORMAT_VERSION);
    }

    LedgerLayout(String managerFactoryCls, int managerVersion,
                 int layoutVersion) {
        this.managerFactoryCls = managerFactoryCls;
        this.managerVersion = managerVersion;
        this.layoutFormatVersion = layoutVersion;
    }

    /**
     * Get Ledger Manager Type
     *
     * @return ledger manager type
     * @deprecated replaced by {@link #getManagerFactoryClass()}
     */
    @Deprecated
    public String getManagerType() {
        // pre V2 layout store as manager type
        return this.managerFactoryCls;
    }

    /**
     * Get ledger manager factory class
     *
     * @return ledger manager factory class
     */
    public String getManagerFactoryClass() {
        return this.managerFactoryCls;
    }

    public int getManagerVersion() {
        return this.managerVersion;
    }

    /**
     * Return layout format version
     *
     * @return layout format version
     */
    public int getLayoutFormatVersion() {
        return this.layoutFormatVersion;
    }

    /**
     * Store the ledger layout into zookeeper
     */
    public void store(final ZooKeeper zk, String ledgersRoot) 
            throws IOException, KeeperException, InterruptedException {
        String ledgersLayout = ledgersRoot + "/"
                + BookKeeperConstants.LAYOUT_ZNODE;
        zk.create(ledgersLayout, serialize(), Ids.OPEN_ACL_UNSAFE,
                CreateMode.PERSISTENT);
    }

    /**
     * Delete the LAYOUT from zookeeper
     */
    public void delete(final ZooKeeper zk, String ledgersRoot)
            throws KeeperException, InterruptedException {
        String ledgersLayout = ledgersRoot + "/"
                + BookKeeperConstants.LAYOUT_ZNODE;
        zk.delete(ledgersLayout, -1);
    }

    /**
     * Generates a byte array based on the LedgerLayout object.
     *
     * @return byte[]
     */
    private byte[] serialize() throws IOException {
        String s =
          new StringBuilder().append(layoutFormatVersion).append(lSplitter)
              .append(managerFactoryCls).append(splitter).append(managerVersion).toString();

        LOG.debug("Serialized layout info: {}", s);
        return s.getBytes("UTF-8");
    }

    /**
     * Parses a given byte array and transforms into a LedgerLayout object
     *
     * @param bytes
     *          byte array to parse
     * @param znodeVersion
     *          version of znode
     * @return LedgerLayout
     * @throws IOException
     *             if the given byte[] cannot be parsed
     */
    private static LedgerLayout parseLayout(byte[] bytes) throws IOException {
        String layout = new String(bytes, "UTF-8");

        LOG.debug("Parsing Layout: {}", layout);

        String lines[] = layout.split(lSplitter);

        try {
            int layoutFormatVersion = new Integer(lines[0]);
            if (LAYOUT_FORMAT_VERSION < layoutFormatVersion ||
                LAYOUT_MIN_COMPAT_VERSION > layoutFormatVersion) {
                throw new IOException("Metadata version not compatible. Expected " 
                        + LAYOUT_FORMAT_VERSION + ", but got " + layoutFormatVersion);
            }

            if (lines.length < 2) {
                throw new IOException("Ledger manager and its version absent from layout: " + layout);
            }

            String[] parts = lines[1].split(splitter);
            if (parts.length != 2) {
                throw new IOException("Invalid Ledger Manager defined in layout : " + layout);
            }
            // ledger manager factory class
            String managerFactoryCls = parts[0];
            // ledger manager version
            int managerVersion = new Integer(parts[1]);
            return new LedgerLayout(managerFactoryCls, managerVersion, layoutFormatVersion);
        } catch (NumberFormatException e) {
            throw new IOException("Could not parse layout '" + layout + "'", e);
        }
    }

    @Override
    public boolean equals(Object obj) {
        if (null == obj) {
            return false;
        }
        if (!(obj instanceof LedgerLayout)) {
            return false;
        }
        LedgerLayout other = (LedgerLayout)obj;
        return managerFactoryCls.equals(other.managerFactoryCls)
            && managerVersion == other.managerVersion;
    }

    @Override
    public int hashCode() {
        return (managerFactoryCls + managerVersion).hashCode();
    }

    @Override
    public String toString() {
        StringBuilder sb = new StringBuilder();
        sb.append("LV").append(layoutFormatVersion).append(":")
            .append(",Type:").append(managerFactoryCls).append(":")
            .append(managerVersion);
        return sb.toString();
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/meta/LedgerManager.java,true,"package org.apache.bookkeeper.meta;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.Closeable;
import java.io.IOException;
import java.util.Set;

import org.apache.zookeeper.AsyncCallback;
import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.client.LedgerMetadata;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.Processor;
import org.apache.bookkeeper.versioning.Version;

/**
 * LedgerManager takes responsibility of ledger management in client side.
 *
 * <ul>
 * <li>How to store ledger meta (e.g. in ZooKeeper or other key/value store)
 * </ul>
 */
public interface LedgerManager extends Closeable {

    /**
     * Create a new ledger with provided metadata
     *
     * @param metadata
     *        Metadata provided when creating a new ledger
     * @param cb
     *        Callback when creating a new ledger.
     *        {@link BKException.Code.ZKException} return code when can't generate
     *        or extract new ledger id
     */
    public void createLedger(LedgerMetadata metadata, GenericCallback<Long> cb);

    /**
     * Remove a specified ledger metadata by ledgerId and version.
     *
     * @param ledgerId
     *          Ledger Id
     * @param version
     *          Ledger metadata version
     * @param cb
     *          Callback when removed ledger metadata.
     *          {@link BKException.Code.MetadataVersionException} return code when version doesn't match,
     *          {@link BKException.Code.NoSuchLedgerExistsException} return code when ledger doesn't exist,
     *          {@link BKException.Code.ZKException} return code when other issues happen.
     */
    public void removeLedgerMetadata(long ledgerId, Version version, GenericCallback<Void> vb);

    /**
     * Read ledger metadata of a specified ledger.
     *
     * @param ledgerId
     *          Ledger Id
     * @param readCb
     *          Callback when read ledger metadata.
     *          {@link BKException.Code.NoSuchLedgerExistsException} return code when ledger doesn't exist,
     *          {@link BKException.Code.ZKException} return code when other issues happen.
     */
    public void readLedgerMetadata(long ledgerId, GenericCallback<LedgerMetadata> readCb);

    /**
     * Write ledger metadata.
     *
     * @param ledgerId
     *          Ledger Id
     * @param metadata
     *          Ledger Metadata to write
     * @param cb
     *          Callback when finished writing ledger metadata.
     *          {@link BKException.Code.MetadataVersionException} return code when version doesn't match,
     *          {@link BKException.Code.ZKException} return code when other issues happen.
     */
    public void writeLedgerMetadata(long ledgerId, LedgerMetadata metadata, GenericCallback<Void> cb);

    /**
     * Loop to process all ledgers.
     * <p>
     * <ul>
     * After all ledgers were processed, finalCb will be triggerred:
     * <li> if all ledgers are processed done with OK, success rc will be passed to finalCb.
     * <li> if some ledgers are prcoessed failed, failure rc will be passed to finalCb.
     * </ul>
     * </p>
     *
     * @param processor
     *          Ledger Processor to process a specific ledger
     * @param finalCb
     *          Callback triggered after all ledgers are processed
     * @param context
     *          Context of final callback
     * @param successRc
     *          Success RC code passed to finalCb when callback
     * @param failureRc
     *          Failure RC code passed to finalCb when exceptions occured.
     */
    public void asyncProcessLedgers(Processor<Long> processor, AsyncCallback.VoidCallback finalCb,
                                    Object context, int successRc, int failureRc);

    /**
     * Loop to scan a range of metadata from metadata storage
     *
     * @return will return a iterator of the Ranges
     */
    public LedgerRangeIterator getLedgerRanges();

    /*
     * Used to represent the Ledgers range returned from the
     * current scan.
     */
    public static class LedgerRange {
        // ledger start and end ranges
        private final long start;
        private final long end;
        public final static long NOLIMIT = -1;

        // returned ledgers
        private Set<Long> ledgers;

        public LedgerRange(Set<Long> ledgers) {
            this(ledgers, NOLIMIT, NOLIMIT);
        }

        public LedgerRange(Set<Long> ledgers, long start) {
            this(ledgers, start, NOLIMIT);
        }

        public LedgerRange(Set<Long> ledgers, long start, long end) {
            this.ledgers = ledgers;
            this.start = start;
            this.end = end;
        }

        public Long start() {
            return this.start;
        }

        public Long end() {
            return this.end;
        }

        public Set<Long> getLedgers() {
            return this.ledgers;
        }
    }

    /**
     * Interface of the ledger meta range iterator from
     * storage (e.g. in ZooKeeper or other key/value store)
     */
    interface LedgerRangeIterator {

        /**
         * @return true if there are records in the ledger metadata store. false
         * only when there are indeed no records in ledger metadata store.
         * @throws IOException thrown when there is any problem accessing the ledger
         * metadata store. It is critical that it doesn't return false in the case
         * in the case it fails to access the ledger metadata store. Otherwise it
         * will end up deleting all ledgers by accident.
         */
        public boolean hasNext() throws IOException;

        /**
         * Get the next element.
         *
         * @return the next element.
         * @throws IOException thrown when there is a problem accessing the ledger
         * metadata store. It is critical that it doesn't return false in the case
         * in the case it fails to access the ledger metadata store. Otherwise it
         * will end up deleting all ledgers by accident.
         */
        public LedgerRange next() throws IOException;
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/meta/LedgerManagerFactory.java,false,"package org.apache.bookkeeper.meta;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.IOException;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.bookkeeper.replication.ReplicationException;
import org.apache.bookkeeper.conf.AbstractConfiguration;
import org.apache.bookkeeper.util.ReflectionUtils;
import org.apache.commons.configuration.ConfigurationException;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.ZooKeeper;

public abstract class LedgerManagerFactory {

    static final Logger LOG = LoggerFactory.getLogger(LedgerManagerFactory.class);
    // v1 layout
    static final int V1 = 1;

    /**
     * Return current factory version.
     *
     * @return current version used by factory.
     */
    public abstract int getCurrentVersion();

    /**
     * Initialize a factory.
     *
     * @param conf
     *          Configuration object used to initialize factory
     * @param zk
     *          Available zookeeper handle for ledger manager to use.
     * @param factoryVersion
     *          What version used to initialize factory.
     * @return ledger manager factory instance
     * @throws IOException when fail to initialize the factory.
     */
    public abstract LedgerManagerFactory initialize(final AbstractConfiguration conf,
                                                    final ZooKeeper zk,
                                                    final int factoryVersion)
    throws IOException;

    /**
     * Uninitialize the factory.
     *
     * @throws IOException when fail to uninitialize the factory.
     */
    public abstract void uninitialize() throws IOException;

    /**
     * return ledger manager for client-side to manage ledger metadata.
     *
     * @return ledger manager
     * @see LedgerManager
     */
    public abstract LedgerManager newLedgerManager();

    /**
     * Return a ledger underreplication manager, which is used to
     * mark ledgers as unreplicated, and to retrieve a ledger which
     * is underreplicated so that it can be rereplicated.
     *
     * @return ledger underreplication manager
     * @see LedgerUnderreplicationManager
     */
    public abstract LedgerUnderreplicationManager newLedgerUnderreplicationManager()
            throws KeeperException, InterruptedException, ReplicationException.CompatibilityException;

    /**
     * Create new Ledger Manager Factory.
     *
     * @param conf
     *          Configuration Object.
     * @param zk
     *          ZooKeeper Client Handle, talk to zk to know which ledger manager is used.
     * @return new ledger manager factory
     * @throws IOException
     */
    public static LedgerManagerFactory newLedgerManagerFactory(
        final AbstractConfiguration conf, final ZooKeeper zk)
            throws IOException, KeeperException, InterruptedException {
        Class<? extends LedgerManagerFactory> factoryClass;
        try {
            factoryClass = conf.getLedgerManagerFactoryClass();
        } catch (Exception e) {
            throw new IOException("Failed to get ledger manager factory class from configuration : ", e);
        }
        String ledgerRootPath = conf.getZkLedgersRootPath();

        if (null == ledgerRootPath || ledgerRootPath.length() == 0) {
            throw new IOException("Empty Ledger Root Path.");
        }

        // if zk is null, return the default ledger manager
        if (zk == null) {
            return new FlatLedgerManagerFactory()
                   .initialize(conf, null, FlatLedgerManagerFactory.CUR_VERSION);
        }

        LedgerManagerFactory lmFactory;

        // check that the configured ledger manager is
        // compatible with the existing layout
        LedgerLayout layout = LedgerLayout.readLayout(zk, ledgerRootPath);
        if (layout == null) { // no existing layout
            lmFactory = createNewLMFactory(conf, zk, factoryClass);
            return lmFactory
                    .initialize(conf, zk, lmFactory.getCurrentVersion());
        }
        LOG.debug("read ledger layout {}", layout);

        // there is existing layout, we need to look into the layout.
        // handle pre V2 layout
        if (layout.getLayoutFormatVersion() <= V1) {
            // pre V2 layout we use type of ledger manager
            String lmType = conf.getLedgerManagerType();
            if (lmType != null && !layout.getManagerType().equals(lmType)) {
                throw new IOException("Configured layout " + lmType
                                    + " does not match existing layout "  + layout.getManagerType());
            }

            // create the ledger manager
            if (FlatLedgerManagerFactory.NAME.equals(layout.getManagerType())) {
                lmFactory = new FlatLedgerManagerFactory();
            } else if (HierarchicalLedgerManagerFactory.NAME.equals(layout.getManagerType())) {
                lmFactory = new HierarchicalLedgerManagerFactory();
            } else {
                throw new IOException("Unknown ledger manager type: " + lmType);
            }
            return lmFactory.initialize(conf, zk, layout.getManagerVersion());
        }

        // handle V2 layout case
        if (factoryClass != null &&
            !layout.getManagerFactoryClass().equals(factoryClass.getName())) {

            throw new IOException("Configured layout " + factoryClass.getName()
                                + " does not match existing layout "  + layout.getManagerFactoryClass());
        }
        if (factoryClass == null) {
            // no factory specified in configuration
            try {
                Class<?> theCls = Class.forName(layout.getManagerFactoryClass());
                if (!LedgerManagerFactory.class.isAssignableFrom(theCls)) {
                    throw new IOException("Wrong ledger manager factory " + layout.getManagerFactoryClass());
                }
                factoryClass = theCls.asSubclass(LedgerManagerFactory.class);
            } catch (ClassNotFoundException cnfe) {
                throw new IOException("Failed to instantiate ledger manager factory " + layout.getManagerFactoryClass());
            }
        }
        // instantiate a factory
        lmFactory = ReflectionUtils.newInstance(factoryClass);
        return lmFactory.initialize(conf, zk, layout.getManagerVersion());
    }

    /**
     * Creates the new layout and stores in zookeeper and returns the
     * LedgerManagerFactory instance.
     */
    private static LedgerManagerFactory createNewLMFactory(
            final AbstractConfiguration conf, final ZooKeeper zk,
            Class<? extends LedgerManagerFactory> factoryClass)
            throws IOException, KeeperException, InterruptedException {

        String ledgerRootPath = conf.getZkLedgersRootPath();
        LedgerManagerFactory lmFactory;
        LedgerLayout layout;
        // use default ledger manager factory if no one provided
        if (factoryClass == null) {
            // for backward compatibility, check manager type
            String lmType = conf.getLedgerManagerType();
            if (lmType == null) {
                factoryClass = FlatLedgerManagerFactory.class;
            } else {
                if (FlatLedgerManagerFactory.NAME.equals(lmType)) {
                    factoryClass = FlatLedgerManagerFactory.class;
                } else if (HierarchicalLedgerManagerFactory.NAME.equals(lmType)) {
                    factoryClass = HierarchicalLedgerManagerFactory.class;
                } else {
                    throw new IOException("Unknown ledger manager type: "
                            + lmType);
                }
            }
        }

        lmFactory = ReflectionUtils.newInstance(factoryClass);

        layout = new LedgerLayout(factoryClass.getName(),
                lmFactory.getCurrentVersion());
        try {
            layout.store(zk, ledgerRootPath);
        } catch (KeeperException.NodeExistsException nee) {
            LedgerLayout layout2 = LedgerLayout.readLayout(zk, ledgerRootPath);
            if (!layout2.equals(layout)) {
                throw new IOException(
                        "Contention writing to layout to zookeeper, "
                                + " other layout " + layout2
                                + " is incompatible with our " + "layout "
                                + layout);
            }
        }
        return lmFactory;
    }

    /**
     * Format the ledger metadata for LedgerManager
     * 
     * @param conf
     *            Configuration instance
     * @param zk
     *            Zookeeper instance
     */
    public void format(final AbstractConfiguration conf, final ZooKeeper zk)
            throws InterruptedException, KeeperException, IOException {
        
        Class<? extends LedgerManagerFactory> factoryClass;
        try {
            factoryClass = conf.getLedgerManagerFactoryClass();
        } catch (ConfigurationException e) {
            throw new IOException("Failed to get ledger manager factory class from configuration : ", e);
        }
       
        LedgerLayout layout = LedgerLayout.readLayout(zk,
                conf.getZkLedgersRootPath());
        layout.delete(zk, conf.getZkLedgersRootPath());
        // Create new layout information again.        
        createNewLMFactory(conf, zk, factoryClass);
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/meta/LedgerUnderreplicationManager.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.meta;

import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;
import org.apache.bookkeeper.replication.ReplicationException;

/**
 * Interface for marking ledgers which need to be rereplicated
 */
public interface LedgerUnderreplicationManager {
    /**
     * Mark a ledger as underreplicated. The replication should
     * then check which fragments are underreplicated and rereplicate them
     */
    void markLedgerUnderreplicated(long ledgerId, String missingReplica)
            throws ReplicationException.UnavailableException;

    /**
     * Mark a ledger as fully replicated. If the ledger is not
     * already marked as underreplicated, this is a noop.
     */
    void markLedgerReplicated(long ledgerId)
            throws ReplicationException.UnavailableException;

    /**
     * Acquire a underreplicated ledger for rereplication. The ledger
     * should be locked, so that no other agent will receive the ledger
     * from this call.
     * The ledger should remain locked until either #markLedgerComplete
     * or #releaseLedger are called.
     * This call is blocking, so will not return until a ledger is
     * available for rereplication.
     */
    long getLedgerToRereplicate()
            throws ReplicationException.UnavailableException;

    /**
     * Poll for a underreplicated ledger to rereplicate.
     * @see #getLedgerToRereplicate
     * @return the ledgerId, or -1 if none are available
     */
    long pollLedgerToRereplicate()
            throws ReplicationException.UnavailableException;


    /**
     * Release a previously acquired ledger. This allows others to acquire
     * the ledger
     */
    void releaseUnderreplicatedLedger(long ledgerId)
            throws ReplicationException.UnavailableException;

    /**
     * Release all resources held by the ledger underreplication manager
     */
    void close()
            throws ReplicationException.UnavailableException;

    /**
     * Stop ledger replication. Currently running ledger rereplication tasks
     * will be continued and will be stopped from next task. This will block
     * ledger replication {@link #Auditor} and {@link #getLedgerToRereplicate()}
     * tasks
     */
    void disableLedgerReplication()
            throws ReplicationException.UnavailableException;

    /**
     * Resuming ledger replication. This will allow ledger replication
     * {@link #Auditor} and {@link #getLedgerToRereplicate()} tasks to continue
     */
    void enableLedgerReplication()
            throws ReplicationException.UnavailableException;

    /**
     * Check whether the ledger replication is enabled or not. This will return
     * true if the ledger replication is enabled, otherwise return false
     * 
     * @return - return true if it is enabled otherwise return false
     */
    boolean isLedgerReplicationEnabled()
            throws ReplicationException.UnavailableException;

    /**
     * Receive notification asynchronously when the ledger replication process
     * is enabled
     * 
     * @param cb
     *            - callback implementation to receive the notification
     */
    void notifyLedgerReplicationEnabled(GenericCallback<Void> cb)
            throws ReplicationException.UnavailableException;
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/meta/MSLedgerManagerFactory.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.meta;

import static org.apache.bookkeeper.metastore.MetastoreTable.ALL_FIELDS;
import static org.apache.bookkeeper.metastore.MetastoreTable.NON_FIELDS;

import java.io.IOException;
import java.util.Iterator;
import java.util.Set;
import java.util.SortedSet;
import java.util.TreeSet;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.CountDownLatch;

import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.client.LedgerMetadata;
import org.apache.bookkeeper.conf.AbstractConfiguration;
import org.apache.bookkeeper.metastore.MSException;
import org.apache.bookkeeper.metastore.MetaStore;
import org.apache.bookkeeper.metastore.MetastoreCallback;
import org.apache.bookkeeper.metastore.MetastoreCursor;
import org.apache.bookkeeper.metastore.MetastoreCursor.ReadEntriesCallback;
import org.apache.bookkeeper.metastore.MetastoreException;
import org.apache.bookkeeper.metastore.MetastoreFactory;
import org.apache.bookkeeper.metastore.MetastoreScannableTable;
import org.apache.bookkeeper.metastore.MetastoreTableItem;
import org.apache.bookkeeper.metastore.Value;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.Processor;
import org.apache.bookkeeper.replication.ReplicationException;
import org.apache.bookkeeper.util.StringUtils;
import org.apache.bookkeeper.util.ZkUtils;
import org.apache.bookkeeper.versioning.Version;
import org.apache.bookkeeper.versioning.Versioned;
import org.apache.zookeeper.AsyncCallback;
import org.apache.zookeeper.AsyncCallback.StringCallback;
import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.ZooDefs.Ids;
import org.apache.zookeeper.ZooKeeper;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * MetaStore Based Ledger Manager Factory
 */
public class MSLedgerManagerFactory extends LedgerManagerFactory {

    static Logger LOG = LoggerFactory.getLogger(MSLedgerManagerFactory.class);

    public static final int CUR_VERSION = 1;

    public static final String TABLE_NAME = "LEDGER";
    public static final String META_FIELD = ".META";

    AbstractConfiguration conf;
    ZooKeeper zk;
    MetaStore metastore;

    @Override
    public int getCurrentVersion() {
        return CUR_VERSION;
    }

    @Override
    public LedgerManagerFactory initialize(final AbstractConfiguration conf, final ZooKeeper zk,
            final int factoryVersion) throws IOException {
        if (CUR_VERSION != factoryVersion) {
            throw new IOException("Incompatible layout version found : " + factoryVersion);
        }
        this.conf = conf;
        this.zk = zk;

        // load metadata store
        String msName = conf.getMetastoreImplClass();
        try {
            metastore = MetastoreFactory.createMetaStore(msName);

            // TODO: should record version in somewhere. e.g. ZooKeeper
            int msVersion = metastore.getVersion();
            metastore.init(conf, msVersion);
        } catch (Throwable t) {
            throw new IOException("Failed to initialize metastore " + msName + " : ", t);
        }

        return this;
    }

    @Override
    public void uninitialize() throws IOException {
        metastore.close();
    }

    static Long key2LedgerId(String key) {
        return null == key ? null : Long.parseLong(key, 10);
    }

    static String ledgerId2Key(Long lid) {
        return null == lid ? null : StringUtils.getZKStringId(lid);
    }

    static String rangeToString(Long firstLedger, boolean firstInclusive, Long lastLedger, boolean lastInclusive) {
        StringBuilder sb = new StringBuilder();
        sb.append(firstInclusive ? "[ " : "( ").append(firstLedger).append(" ~ ").append(lastLedger)
                .append(lastInclusive ? " ]" : " )");
        return sb.toString();
    }

    static SortedSet<Long> entries2Ledgers(Iterator<MetastoreTableItem> entries) {
        SortedSet<Long> ledgers = new TreeSet<Long>();
        while (entries.hasNext()) {
            MetastoreTableItem item = entries.next();
            try {
                ledgers.add(key2LedgerId(item.getKey()));
            } catch (NumberFormatException nfe) {
                LOG.warn("Found invalid ledger key {}", item.getKey());
            }
        }
        return ledgers;
    }

    static class SyncResult<T> {
        T value;
        int rc;
        boolean finished = false;

        public synchronized void complete(int rc, T value) {
            this.rc = rc;
            this.value = value;
            finished = true;

            notify();
        }

        public synchronized void block() {
            try {
                while (!finished) {
                    wait();
                }
            } catch (InterruptedException ie) {
            }
        }

        public synchronized int getRetCode() {
            return rc;
        }

        public synchronized T getResult() {
            return value;
        }
    }

    static class MsLedgerManager implements LedgerManager {
        final ZooKeeper zk;
        final AbstractConfiguration conf;

        final MetaStore metastore;
        final MetastoreScannableTable ledgerTable;
        final int maxEntriesPerScan;

        static final String IDGEN_ZNODE = "ms-idgen";
        static final String IDGENERATION_PREFIX = "/" + IDGEN_ZNODE + "/ID-";

        // Path to generate global id
        private final String idGenPath;

        // we use this to prevent long stack chains from building up in
        // callbacks
        ScheduledExecutorService scheduler;

        MsLedgerManager(final AbstractConfiguration conf, final ZooKeeper zk, final MetaStore metastore) {
            this.conf = conf;
            this.zk = zk;
            this.metastore = metastore;

            try {
                ledgerTable = metastore.createScannableTable(TABLE_NAME);
            } catch (MetastoreException mse) {
                LOG.error("Failed to instantiate table " + TABLE_NAME + " in metastore " + metastore.getName());
                throw new RuntimeException("Failed to instantiate table " + TABLE_NAME + " in metastore "
                        + metastore.getName());
            }
            // configuration settings
            maxEntriesPerScan = conf.getMetastoreMaxEntriesPerScan();

            this.idGenPath = conf.getZkLedgersRootPath() + IDGENERATION_PREFIX;
            this.scheduler = Executors.newSingleThreadScheduledExecutor();
        }

        @Override
        public void close() {
            try {
                scheduler.shutdown();
            } catch (Exception e) {
                LOG.warn("Error when closing MsLedgerManager : ", e);
            }
            ledgerTable.close();
        }

        @Override
        public void createLedger(final LedgerMetadata metadata, final GenericCallback<Long> ledgerCb) {
            ZkUtils.createFullPathOptimistic(zk, idGenPath, new byte[0], Ids.OPEN_ACL_UNSAFE,
                    CreateMode.EPHEMERAL_SEQUENTIAL, new StringCallback() {
                        @Override
                        public void processResult(int rc, String path, Object ctx, final String idPathName) {
                            if (rc != KeeperException.Code.OK.intValue()) {
                                LOG.error("Could not generate new ledger id",
                                        KeeperException.create(KeeperException.Code.get(rc), path));
                                ledgerCb.operationComplete(BKException.Code.ZKException, null);
                                return;
                            }
                            /*
                             * Extract ledger id from gen path
                             */
                            long ledgerId;
                            try {
                                ledgerId = getLedgerIdFromGenPath(idPathName);
                            } catch (IOException e) {
                                LOG.error("Could not extract ledger-id from id gen path:" + path, e);
                                ledgerCb.operationComplete(BKException.Code.ZKException, null);
                                return;
                            }

                            final long lid = ledgerId;
                            MetastoreCallback<Version> msCallback = new MetastoreCallback<Version>() {
                                @Override
                                public void complete(int rc, Version version, Object ctx) {
                                    if (MSException.Code.BadVersion.getCode() == rc) {
                                        ledgerCb.operationComplete(BKException.Code.MetadataVersionException, null);
                                        return;
                                    }
                                    if (MSException.Code.OK.getCode() != rc) {
                                        ledgerCb.operationComplete(BKException.Code.MetaStoreException, null);
                                        return;
                                    }
                                    LOG.debug("Create ledger {} with version {} successfuly.", new Object[] { lid,
                                            version });
                                    // update version
                                    metadata.setVersion(version);
                                    ledgerCb.operationComplete(BKException.Code.OK, lid);
                                }
                            };

                            ledgerTable.put(ledgerId2Key(lid), new Value().setField(META_FIELD, metadata.serialize()),
                                    Version.NEW, msCallback, null);
                            zk.delete(idPathName, -1, new AsyncCallback.VoidCallback() {
                                @Override
                                public void processResult(int rc, String path, Object ctx) {
                                    if (rc != KeeperException.Code.OK.intValue()) {
                                        LOG.warn("Exception during deleting znode for id generation : ",
                                                KeeperException.create(KeeperException.Code.get(rc), path));
                                    } else {
                                        LOG.debug("Deleting znode for id generation : {}", idPathName);
                                    }
                                }
                            }, null);
                        }
                    }, null);
        }

        // get ledger id from generation path
        private long getLedgerIdFromGenPath(String nodeName) throws IOException {
            long ledgerId;
            try {
                String parts[] = nodeName.split(IDGENERATION_PREFIX);
                ledgerId = Long.parseLong(parts[parts.length - 1]);
            } catch (NumberFormatException e) {
                throw new IOException(e);
            }
            return ledgerId;
        }

        @Override
        public void removeLedgerMetadata(final long ledgerId, final Version version,
                                         final GenericCallback<Void> cb) {
            MetastoreCallback<Void> msCallback = new MetastoreCallback<Void>() {
                @Override
                public void complete(int rc, Void value, Object ctx) {
                    int bkRc;
                    if (MSException.Code.NoKey.getCode() == rc) {
                        LOG.warn("Ledger entry does not exist in meta table: ledgerId={}", ledgerId);
                        bkRc = BKException.Code.NoSuchLedgerExistsException;
                    } else if (MSException.Code.OK.getCode() == rc) {
                        bkRc = BKException.Code.OK;
                    } else {
                        bkRc = BKException.Code.MetaStoreException;
                    }
                    cb.operationComplete(bkRc, (Void) null);
                }
            };
            ledgerTable.remove(ledgerId2Key(ledgerId), version, msCallback, null);
        }

        @Override
        public void readLedgerMetadata(final long ledgerId, final GenericCallback<LedgerMetadata> readCb) {
            final String key = ledgerId2Key(ledgerId);
            MetastoreCallback<Versioned<Value>> msCallback = new MetastoreCallback<Versioned<Value>>() {
                @Override
                public void complete(int rc, Versioned<Value> value, Object ctx) {
                    if (MSException.Code.NoKey.getCode() == rc) {
                        LOG.error("No ledger metadata found for ledger " + ledgerId + " : ",
                                MSException.create(MSException.Code.get(rc), "No key " + key + " found."));
                        readCb.operationComplete(BKException.Code.NoSuchLedgerExistsException, null);
                        return;
                    }
                    if (MSException.Code.OK.getCode() != rc) {
                        LOG.error("Could not read metadata for ledger " + ledgerId + " : ",
                                MSException.create(MSException.Code.get(rc), "Failed to get key " + key));
                        readCb.operationComplete(BKException.Code.MetaStoreException, null);
                        return;
                    }
                    LedgerMetadata metadata;
                    try {
                        metadata = LedgerMetadata
                                .parseConfig(value.getValue().getField(META_FIELD), value.getVersion());
                    } catch (IOException e) {
                        LOG.error("Could not parse ledger metadata for ledger " + ledgerId + " : ", e);
                        readCb.operationComplete(BKException.Code.MetaStoreException, null);
                        return;
                    }
                    readCb.operationComplete(BKException.Code.OK, metadata);
                }
            };
            ledgerTable.get(key, msCallback, ALL_FIELDS);
        }

        @Override
        public void writeLedgerMetadata(final long ledgerId, final LedgerMetadata metadata,
                final GenericCallback<Void> cb) {
            Value data = new Value().setField(META_FIELD, metadata.serialize());

            LOG.debug("Writing ledger {} metadata, version {}", new Object[] { ledgerId, metadata.getVersion() });

            final String key = ledgerId2Key(ledgerId);
            MetastoreCallback<Version> msCallback = new MetastoreCallback<Version>() {
                @Override
                public void complete(int rc, Version version, Object ctx) {
                    int bkRc;
                    if (MSException.Code.BadVersion.getCode() == rc) {
                        LOG.info("Bad version provided to updat metadata for ledger {}", ledgerId);
                        bkRc = BKException.Code.MetadataVersionException;
                    } else if (MSException.Code.NoKey.getCode() == rc) {
                        LOG.warn("Ledger {} doesn't exist when writing its ledger metadata.", ledgerId);
                        bkRc = BKException.Code.NoSuchLedgerExistsException;
                    } else if (MSException.Code.OK.getCode() == rc) {
                        metadata.setVersion(version);
                        bkRc = BKException.Code.OK;
                    } else {
                        LOG.warn("Conditional update ledger metadata failed: ",
                                MSException.create(MSException.Code.get(rc), "Failed to put key " + key));
                        bkRc = BKException.Code.MetaStoreException;
                    }

                    cb.operationComplete(bkRc, null);
                }
            };
            ledgerTable.put(key, data, metadata.getVersion(), msCallback, null);
        }

        @Override
        public void asyncProcessLedgers(final Processor<Long> processor, final AsyncCallback.VoidCallback finalCb,
                final Object context, final int successRc, final int failureRc) {
            MetastoreCallback<MetastoreCursor> openCursorCb = new MetastoreCallback<MetastoreCursor>() {
                @Override
                public void complete(int rc, MetastoreCursor cursor, Object ctx) {
                    if (MSException.Code.OK.getCode() != rc) {
                        finalCb.processResult(failureRc, null, context);
                        return;
                    }
                    if (!cursor.hasMoreEntries()) {
                        finalCb.processResult(successRc, null, context);
                        return;
                    }
                    asyncProcessLedgers(cursor, processor, finalCb, context, successRc, failureRc);
                }
            };
            ledgerTable.openCursor(NON_FIELDS, openCursorCb, null);
        }

        void asyncProcessLedgers(final MetastoreCursor cursor, final Processor<Long> processor,
                                 final AsyncCallback.VoidCallback finalCb, final Object context,
                                 final int successRc, final int failureRc) {
            scheduler.submit(new Runnable() {
                @Override
                public void run() {
                    doAsyncProcessLedgers(cursor, processor, finalCb, context, successRc, failureRc);
                }
            });
        }

        void doAsyncProcessLedgers(final MetastoreCursor cursor, final Processor<Long> processor,
                                   final AsyncCallback.VoidCallback finalCb, final Object context,
                                   final int successRc, final int failureRc) {
            // no entries now
            if (!cursor.hasMoreEntries()) {
                finalCb.processResult(successRc, null, context);
                return;
            }
            ReadEntriesCallback msCallback = new ReadEntriesCallback() {
                @Override
                public void complete(int rc, Iterator<MetastoreTableItem> entries, Object ctx) {
                    if (MSException.Code.OK.getCode() != rc) {
                        finalCb.processResult(failureRc, null, context);
                        return;
                    }

                    SortedSet<Long> ledgers = new TreeSet<Long>();
                    while (entries.hasNext()) {
                        MetastoreTableItem item = entries.next();
                        try {
                            ledgers.add(key2LedgerId(item.getKey()));
                        } catch (NumberFormatException nfe) {
                            LOG.warn("Found invalid ledger key {}", item.getKey());
                        }
                    }

                    if (0 == ledgers.size()) {
                        // process next batch of ledgers
                        asyncProcessLedgers(cursor, processor, finalCb, context, successRc, failureRc);
                        return;
                    }

                    final long startLedger = ledgers.first();
                    final long endLedger = ledgers.last();

                    AsyncSetProcessor<Long> setProcessor = new AsyncSetProcessor<Long>(scheduler);
                    // process set
                    setProcessor.process(ledgers, processor, new AsyncCallback.VoidCallback() {
                        @Override
                        public void processResult(int rc, String path, Object ctx) {
                            if (successRc != rc) {
                                LOG.error("Failed when processing range "
                                        + rangeToString(startLedger, true, endLedger, true));
                                finalCb.processResult(failureRc, null, context);
                                return;
                            }
                            // process next batch of ledgers
                            asyncProcessLedgers(cursor, processor, finalCb, context, successRc, failureRc);
                        }
                    }, context, successRc, failureRc);
                }
            };
            cursor.asyncReadEntries(maxEntriesPerScan, msCallback, null);
        }

        class MSLedgerRangeIterator implements LedgerRangeIterator {
            final CountDownLatch openCursorLatch = new CountDownLatch(1);
            MetastoreCursor cursor = null;

            MSLedgerRangeIterator() {
                MetastoreCallback<MetastoreCursor> openCursorCb = new MetastoreCallback<MetastoreCursor>() {
                    @Override
                    public void complete(int rc, MetastoreCursor newCursor, Object ctx) {
                        if (MSException.Code.OK.getCode() != rc) {
                            LOG.error("Error opening cursor for ledger range iterator {}", rc);
                        } else {
                            cursor = newCursor;
                        }
                        openCursorLatch.countDown();
                    }
                };
                ledgerTable.openCursor(NON_FIELDS, openCursorCb, null);
            }

            @Override
            public boolean hasNext() {
                try {
                    openCursorLatch.await();
                } catch (InterruptedException ie) {
                    LOG.error("Interrupted waiting for cursor to open", ie);
                    Thread.currentThread().interrupt();
                    return false;
                }
                if (cursor == null) {
                    return false;
                }
                return cursor.hasMoreEntries();
            }

            @Override
            public LedgerRange next() throws IOException {
                try {
                    Set<Long> ledgerIds = new TreeSet<Long>();
                    Iterator<MetastoreTableItem> iter = cursor.readEntries(maxEntriesPerScan);
                    while (iter.hasNext()) {
                        ledgerIds.add(key2LedgerId(iter.next().getKey()));
                    }
                    return new LedgerRange(ledgerIds);
                } catch (MSException mse) {
                    LOG.error("Exception occurred reading from metastore", mse);
                    throw new IOException("Couldn't read from metastore", mse);
                }
            }
        }

        @Override
        public LedgerRangeIterator getLedgerRanges() {
            return new MSLedgerRangeIterator();
        }
    }

    @Override
    public LedgerManager newLedgerManager() {
        return new MsLedgerManager(conf, zk, metastore);
    }

    @Override
    public LedgerUnderreplicationManager newLedgerUnderreplicationManager() throws KeeperException,
            InterruptedException, ReplicationException.CompatibilityException {
        // TODO: currently just use zk ledger underreplication manager
        return new ZkLedgerUnderreplicationManager(conf, zk);
    }

    /**
     * Process set one by one in asynchronize way. Process will be stopped
     * immediately when error occurred.
     */
    private static class AsyncSetProcessor<T> {
        // use this to prevent long stack chains from building up in callbacks
        ScheduledExecutorService scheduler;

        /**
         * Constructor
         *
         * @param scheduler
         *            Executor used to prevent long stack chains
         */
        public AsyncSetProcessor(ScheduledExecutorService scheduler) {
            this.scheduler = scheduler;
        }

        /**
         * Process set of items
         *
         * @param data
         *            Set of data to process
         * @param processor
         *            Callback to process element of list when success
         * @param finalCb
         *            Final callback to be called after all elements in the list
         *            are processed
         * @param contxt
         *            Context of final callback
         * @param successRc
         *            RC passed to final callback on success
         * @param failureRc
         *            RC passed to final callback on failure
         */
        public void process(final Set<T> data, final Processor<T> processor, final AsyncCallback.VoidCallback finalCb,
                final Object context, final int successRc, final int failureRc) {
            if (data == null || data.size() == 0) {
                finalCb.processResult(successRc, null, context);
                return;
            }
            final Iterator<T> iter = data.iterator();
            AsyncCallback.VoidCallback stubCallback = new AsyncCallback.VoidCallback() {
                @Override
                public void processResult(int rc, String path, Object ctx) {
                    if (rc != successRc) {
                        // terminal immediately
                        finalCb.processResult(failureRc, null, context);
                        return;
                    }
                    if (!iter.hasNext()) { // reach the end of list
                        finalCb.processResult(successRc, null, context);
                        return;
                    }
                    // process next element
                    final T dataToProcess = iter.next();
                    final AsyncCallback.VoidCallback stub = this;
                    scheduler.submit(new Runnable() {
                        @Override
                        public final void run() {
                            processor.process(dataToProcess, stub);
                        }
                    });
                }
            };
            T firstElement = iter.next();
            processor.process(firstElement, stubCallback);
        }
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/meta/ZkLedgerUnderreplicationManager.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.bookkeeper.meta;

import org.apache.bookkeeper.replication.ReplicationEnableCb;
import org.apache.bookkeeper.replication.ReplicationException;
import org.apache.bookkeeper.replication.ReplicationException.UnavailableException;
import org.apache.bookkeeper.util.BookKeeperConstants;
import org.apache.bookkeeper.util.ZkUtils;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;
import org.apache.bookkeeper.proto.DataFormats.LedgerRereplicationLayoutFormat;
import org.apache.bookkeeper.proto.DataFormats.UnderreplicatedLedgerFormat;
import org.apache.bookkeeper.proto.DataFormats.LockDataFormat;
import org.apache.bookkeeper.conf.AbstractConfiguration;
import org.apache.zookeeper.ZooKeeper;
import org.apache.zookeeper.WatchedEvent;
import org.apache.zookeeper.Watcher;
import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.data.Stat;
import org.apache.zookeeper.ZooDefs.Ids;

import com.google.protobuf.TextFormat;
import com.google.common.base.Joiner;
import static com.google.common.base.Charsets.UTF_8;

import java.net.InetAddress;
import java.net.UnknownHostException;

import java.util.concurrent.CountDownLatch;
import java.util.concurrent.ConcurrentHashMap;
import java.util.Map;
import java.util.List;
import java.util.Collections;
import java.util.Arrays;


import java.util.regex.Pattern;
import java.util.regex.Matcher;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * ZooKeeper implementation of underreplication manager.
 * This is implemented in a heirarchical fashion, so it'll work with
 * FlatLedgerManagerFactory and HierarchicalLedgerManagerFactory.
 *
 * Layout is:
 * /root/underreplication/ LAYOUT
 *                         ledgers/(hierarchicalpath)/urL(ledgerId)
 *                         locks/(ledgerId)
 *
 * The hierarchical path is created by splitting the ledger into 4 2byte
 * segments which are represented in hexidecimal.
 * e.g. For ledger id 0xcafebeef0000feed, the path is
 *  cafe/beef/0000/feed/
 */
public class ZkLedgerUnderreplicationManager implements LedgerUnderreplicationManager {
    static final Logger LOG = LoggerFactory.getLogger(ZkLedgerUnderreplicationManager.class);
    static final String LAYOUT="BASIC";
    static final int LAYOUT_VERSION=1;

    private static class Lock {
        private final String lockZNode;
        private final int ledgerZNodeVersion;

        Lock(String lockZNode, int ledgerZNodeVersion) {
            this.lockZNode = lockZNode;
            this.ledgerZNodeVersion = ledgerZNodeVersion;
        }

        String getLockZNode() { return lockZNode; }
        int getLedgerZNodeVersion() { return ledgerZNodeVersion; }
    };
    private final Map<Long, Lock> heldLocks = new ConcurrentHashMap<Long, Lock>();
    private final Pattern idExtractionPattern;

    private final String basePath;
    private final String urLedgerPath;
    private final String urLockPath;
    private final String layoutZNode;
    private final LockDataFormat lockData;

    private final ZooKeeper zkc;

    public ZkLedgerUnderreplicationManager(AbstractConfiguration conf, ZooKeeper zkc)
            throws KeeperException, InterruptedException, ReplicationException.CompatibilityException {
        basePath = conf.getZkLedgersRootPath() + '/'
                + BookKeeperConstants.UNDER_REPLICATION_NODE;
        layoutZNode = basePath + '/' + BookKeeperConstants.LAYOUT_ZNODE;
        urLedgerPath = basePath
                + BookKeeperConstants.DEFAULT_ZK_LEDGERS_ROOT_PATH;
        urLockPath = basePath + "/locks";

        idExtractionPattern = Pattern.compile("urL(\\d+)$");
        this.zkc = zkc;

        LockDataFormat.Builder lockDataBuilder = LockDataFormat.newBuilder();
        try {
            lockDataBuilder.setBookieId(InetAddress.getLocalHost().getHostAddress().toString());
        } catch (UnknownHostException uhe) {
            // if we cant get the address, ignore. it's optional
            // in the data structure in any case
        }
        lockData = lockDataBuilder.build();

        checkLayout();
    }

    private void checkLayout()
            throws KeeperException, InterruptedException, ReplicationException.CompatibilityException {
        if (zkc.exists(basePath, false) == null) {
            try {
                zkc.create(basePath, new byte[0], Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
            } catch (KeeperException.NodeExistsException nee) {
                // do nothing, someone each could have created it
            }
        }
        while (true) {
            if (zkc.exists(layoutZNode, false) == null) {
                LedgerRereplicationLayoutFormat.Builder builder
                    = LedgerRereplicationLayoutFormat.newBuilder();
                builder.setType(LAYOUT).setVersion(LAYOUT_VERSION);
                try {
                    zkc.create(layoutZNode, TextFormat.printToString(builder.build()).getBytes(UTF_8),
                               Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
                } catch (KeeperException.NodeExistsException nne) {
                    // someone else managed to create it
                    continue;
                }
            } else {
                byte[] layoutData = zkc.getData(layoutZNode, false, null);

                LedgerRereplicationLayoutFormat.Builder builder
                    = LedgerRereplicationLayoutFormat.newBuilder();

                try {
                    TextFormat.merge(new String(layoutData, UTF_8), builder);
                    LedgerRereplicationLayoutFormat layout = builder.build();
                    if (!layout.getType().equals(LAYOUT)
                            || layout.getVersion() != LAYOUT_VERSION) {
                        throw new ReplicationException.CompatibilityException(
                                "Incompatible layout found (" + LAYOUT + ":" + LAYOUT_VERSION + ")");
                    }
                } catch (TextFormat.ParseException pe) {
                    throw new ReplicationException.CompatibilityException(
                            "Invalid data found", pe);
                }
                break;
            }
        }
        if (zkc.exists(urLedgerPath, false) == null) {
            try {
                zkc.create(urLedgerPath, new byte[0], Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
            } catch (KeeperException.NodeExistsException nee) {
                // do nothing, someone each could have created it
            }
        }
        if (zkc.exists(urLockPath, false) == null) {
            try {
                zkc.create(urLockPath, new byte[0], Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
            } catch (KeeperException.NodeExistsException nee) {
                // do nothing, someone each could have created it
            }
        }
    }

    private long getLedgerId(String path) throws NumberFormatException {
        Matcher m = idExtractionPattern.matcher(path);
        if (m.find()) {
            return Long.valueOf(m.group(1));
        } else {
            throw new NumberFormatException("Couldn't find ledgerid in path");
        }
    }

    public static String getParentZnodePath(String base, long ledgerId) {
        String subdir1 = String.format("%04x", ledgerId >> 48 & 0xffff);
        String subdir2 = String.format("%04x", ledgerId >> 32 & 0xffff);
        String subdir3 = String.format("%04x", ledgerId >> 16 & 0xffff);
        String subdir4 = String.format("%04x", ledgerId & 0xffff);
        
        return String.format("%s/%s/%s/%s/%s",
                             base, subdir1, subdir2, subdir3, subdir4);
    }

    public static String getUrLedgerZnode(String base, long ledgerId) {
        return String.format("%s/urL%010d", getParentZnodePath(base, ledgerId), ledgerId);
    }

    private String getUrLedgerZnode(long ledgerId) {
        return getUrLedgerZnode(urLedgerPath, ledgerId);
    }


    @Override
    public void markLedgerUnderreplicated(long ledgerId, String missingReplica)
            throws ReplicationException.UnavailableException {
        LOG.debug("markLedgerUnderreplicated(ledgerId={}, missingReplica={})", ledgerId, missingReplica);
        try {
            String znode = getUrLedgerZnode(ledgerId);
            while (true) {
                UnderreplicatedLedgerFormat.Builder builder = UnderreplicatedLedgerFormat.newBuilder();
                try {
                    builder.addReplica(missingReplica);
                    ZkUtils.createFullPathOptimistic(zkc, znode, TextFormat
                            .printToString(builder.build()).getBytes(UTF_8),
                            Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
                } catch (KeeperException.NodeExistsException nee) {
                    Stat s = zkc.exists(znode, false);
                    if (s == null) {
                        continue;
                    }
                    try {
                        byte[] bytes = zkc.getData(znode, false, s);
                        builder.clear();
                        TextFormat.merge(new String(bytes, UTF_8), builder);
                        UnderreplicatedLedgerFormat data = builder.build();
                        if (data.getReplicaList().contains(missingReplica)) {
                            return; // nothing to add
                        }
                        builder.addReplica(missingReplica);
                        zkc.setData(znode,
                                    TextFormat.printToString(builder.build()).getBytes(UTF_8),
                                    s.getVersion());
                    } catch (KeeperException.NoNodeException nne) {
                        continue;
                    } catch (KeeperException.BadVersionException bve) {
                        continue;
                    } catch (TextFormat.ParseException pe) {
                        throw new ReplicationException.UnavailableException(
                                "Invalid data found", pe);
                    }
                }
                break;
            }
        } catch (KeeperException ke) {
            throw new ReplicationException.UnavailableException("Error contacting zookeeper", ke);
        } catch (InterruptedException ie) {
            Thread.currentThread().interrupt();
            throw new ReplicationException.UnavailableException("Interrupted while contacting zookeeper", ie);
        }
    }

    @Override
    public void markLedgerReplicated(long ledgerId) throws ReplicationException.UnavailableException {
        LOG.debug("markLedgerReplicated(ledgerId={})", ledgerId);
        try {
            Lock l = heldLocks.get(ledgerId);
            if (l != null) {
                zkc.delete(getUrLedgerZnode(ledgerId), l.getLedgerZNodeVersion());

                try {
                    // clean up the hierarchy
                    String parts[] = getUrLedgerZnode(ledgerId).split("/");
                    for (int i = 1; i <= 4; i++) {
                        String p[] = Arrays.copyOf(parts, parts.length - i);
                        String path = Joiner.on("/").join(p);
                        Stat s = zkc.exists(path, null);
                        if (s != null) {
                            zkc.delete(path, s.getVersion());
                        }
                    }
                } catch (KeeperException.NotEmptyException nee) {
                    // This can happen when cleaning up the hierarchy.
                    // It's safe to ignore, it simply means another
                    // ledger in the same hierarchy has been marked as
                    // underreplicated.
                }
            }
        } catch (KeeperException.NoNodeException nne) {
            // this is ok
        } catch (KeeperException.BadVersionException bve) {
            // if this is the case, some has marked the ledger
            // for rereplication again. Leave the underreplicated
            // znode in place, so the ledger is checked.
        } catch (KeeperException ke) {
            LOG.error("Error deleting underreplicated ledger znode", ke);
            throw new ReplicationException.UnavailableException("Error contacting zookeeper", ke);
        } catch (InterruptedException ie) {
            Thread.currentThread().interrupt();
            throw new ReplicationException.UnavailableException("Interrupted while contacting zookeeper", ie);
        } finally {
            releaseUnderreplicatedLedger(ledgerId);
        }
    }

    private long getLedgerToRereplicateFromHierarchy(String parent, long depth, Watcher w)
            throws KeeperException, InterruptedException {
        if (depth == 4) {
            List<String> children;
            try {
                children = zkc.getChildren(parent, w);
            } catch (KeeperException.NoNodeException nne) {
                // can occur if another underreplicated ledger's
                // hierarchy is being cleaned up
                return -1;
            }

            Collections.shuffle(children);

            while (children.size() > 0) {
                String tryChild = children.get(0);
                try {
                    String lockPath = urLockPath + "/" + tryChild;
                    if (zkc.exists(lockPath, w) != null) {
                        children.remove(tryChild);
                        continue;
                    }

                    Stat stat = zkc.exists(parent + "/" + tryChild, false);
                    if (stat == null) {
                        LOG.debug("{}/{} doesn't exist", parent, tryChild);
                        children.remove(tryChild);
                        continue;
                    }

                    long ledgerId = getLedgerId(tryChild);
                    zkc.create(lockPath, TextFormat.printToString(lockData).getBytes(UTF_8),
                               Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL);
                    heldLocks.put(ledgerId, new Lock(lockPath, stat.getVersion()));
                    return ledgerId;
                } catch (KeeperException.NodeExistsException nee) {
                    children.remove(tryChild);
                } catch (NumberFormatException nfe) {
                    children.remove(tryChild);
                }
            }
            return -1;
        }

        List<String> children;
        try {
            children = zkc.getChildren(parent, w);
        } catch (KeeperException.NoNodeException nne) {
            // can occur if another underreplicated ledger's
            // hierarchy is being cleaned up
            return -1;
        }

        Collections.shuffle(children);

        while (children.size() > 0) {
            String tryChild = children.get(0);
            String tryPath = parent + "/" + tryChild;
            long ledger = getLedgerToRereplicateFromHierarchy(tryPath, depth + 1, w);
            if (ledger != -1) {
                return ledger;
            }
            children.remove(tryChild);
        }
        return -1;
    }


    @Override
    public long pollLedgerToRereplicate() throws ReplicationException.UnavailableException {
        LOG.debug("pollLedgerToRereplicate()");
        try {
            Watcher w = new Watcher() {
                    public void process(WatchedEvent e) { // do nothing
                    }
                };
            return getLedgerToRereplicateFromHierarchy(urLedgerPath, 0, w);
        } catch (KeeperException ke) {
            throw new ReplicationException.UnavailableException("Error contacting zookeeper", ke);
        } catch (InterruptedException ie) {
            Thread.currentThread().interrupt();
            throw new ReplicationException.UnavailableException("Interrupted while connecting zookeeper", ie);
        }
    }

    @Override
    public long getLedgerToRereplicate() throws ReplicationException.UnavailableException {
        LOG.debug("getLedgerToRereplicate()");
        try {
            while (true) {
                waitIfLedgerReplicationDisabled();
                final CountDownLatch changedLatch = new CountDownLatch(1);
                Watcher w = new Watcher() {
                        public void process(WatchedEvent e) {
                            if (e.getType() == Watcher.Event.EventType.NodeChildrenChanged
                                || e.getType() == Watcher.Event.EventType.NodeDeleted
                                || e.getType() == Watcher.Event.EventType.NodeCreated
                                || e.getState() == Watcher.Event.KeeperState.Expired
                                || e.getState() == Watcher.Event.KeeperState.Disconnected) {
                                changedLatch.countDown();
                            }
                        }
                    };
                long ledger = getLedgerToRereplicateFromHierarchy(urLedgerPath, 0, w);
                if (ledger != -1) {
                    return ledger;
                }
                // nothing found, wait for a watcher to trigger
                changedLatch.await();
            }
        } catch (KeeperException ke) {
            throw new ReplicationException.UnavailableException("Error contacting zookeeper", ke);
        } catch (InterruptedException ie) {
            Thread.currentThread().interrupt();
            throw new ReplicationException.UnavailableException("Interrupted while connecting zookeeper", ie);
        }
    }

    private void waitIfLedgerReplicationDisabled() throws UnavailableException,
            InterruptedException {
        ReplicationEnableCb cb = new ReplicationEnableCb();
        if (!this.isLedgerReplicationEnabled()) {
            this.notifyLedgerReplicationEnabled(cb);
            cb.await();
        }
    }
    
    @Override
    public void releaseUnderreplicatedLedger(long ledgerId) throws ReplicationException.UnavailableException {
        LOG.debug("releaseLedger(ledgerId={})", ledgerId);
        try {
            Lock l = heldLocks.remove(ledgerId);
            if (l != null) {
                zkc.delete(l.getLockZNode(), -1);
            }
        } catch (KeeperException.NoNodeException nne) {
            // this is ok
        } catch (KeeperException ke) {
            LOG.error("Error deleting underreplicated ledger lock", ke);
            throw new ReplicationException.UnavailableException("Error contacting zookeeper", ke);
        } catch (InterruptedException ie) {
            Thread.currentThread().interrupt();
            throw new ReplicationException.UnavailableException("Interrupted while connecting zookeeper", ie);
        }
    }

    @Override
    public void close() throws ReplicationException.UnavailableException {
        LOG.debug("close()");
        try {
            for (Map.Entry<Long, Lock> e : heldLocks.entrySet()) {
                zkc.delete(e.getValue().getLockZNode(), -1);
            }
        } catch (KeeperException.NoNodeException nne) {
            // this is ok
        } catch (KeeperException ke) {
            LOG.error("Error deleting underreplicated ledger lock", ke);
            throw new ReplicationException.UnavailableException("Error contacting zookeeper", ke);
        } catch (InterruptedException ie) {
            Thread.currentThread().interrupt();
            throw new ReplicationException.UnavailableException("Interrupted while connecting zookeeper", ie);
        }
    }

    @Override
    public void disableLedgerReplication()
            throws ReplicationException.UnavailableException {
        LOG.debug("disableLedegerReplication()");
        try {
            ZkUtils.createFullPathOptimistic(zkc, basePath + '/'
                    + BookKeeperConstants.DISABLE_NODE, "".getBytes(),
                    Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
            LOG.info("Auto ledger re-replication is disabled!");
        } catch (KeeperException.NodeExistsException ke) {
            LOG.warn("AutoRecovery is already disabled!", ke);
            throw new ReplicationException.UnavailableException(
                    "AutoRecovery is already disabled!", ke);
        } catch (KeeperException ke) {
            LOG.error("Exception while stopping auto ledger re-replication", ke);
            throw new ReplicationException.UnavailableException(
                    "Exception while stopping auto ledger re-replication", ke);
        } catch (InterruptedException ie) {
            Thread.currentThread().interrupt();
            throw new ReplicationException.UnavailableException(
                    "Interrupted while stopping auto ledger re-replication", ie);
        }
    }

    @Override
    public void enableLedgerReplication()
            throws ReplicationException.UnavailableException {
        LOG.debug("enableLedegerReplication()");
        try {
            zkc.delete(basePath + '/' + BookKeeperConstants.DISABLE_NODE, -1);
            LOG.info("Resuming automatic ledger re-replication");
        } catch (KeeperException.NoNodeException ke) {
            LOG.warn("AutoRecovery is already enabled!", ke);
            throw new ReplicationException.UnavailableException(
                    "AutoRecovery is already enabled!", ke);
        } catch (KeeperException ke) {
            LOG.error("Exception while resuming ledger replication", ke);
            throw new ReplicationException.UnavailableException(
                    "Exception while resuming auto ledger re-replication", ke);
        } catch (InterruptedException ie) {
            Thread.currentThread().interrupt();
            throw new ReplicationException.UnavailableException(
                    "Interrupted while resuming auto ledger re-replication", ie);
        }
    }

    @Override
    public boolean isLedgerReplicationEnabled()
            throws ReplicationException.UnavailableException {
        LOG.debug("isLedgerReplicationEnabled()");
        try {
            if (null != zkc.exists(basePath + '/'
                    + BookKeeperConstants.DISABLE_NODE, false)) {
                return false;
            }
            return true;
        } catch (KeeperException ke) {
            LOG.error("Error while checking the state of "
                    + "ledger re-replication", ke);
            throw new ReplicationException.UnavailableException(
                    "Error contacting zookeeper", ke);
        } catch (InterruptedException ie) {
            Thread.currentThread().interrupt();
            throw new ReplicationException.UnavailableException(
                    "Interrupted while contacting zookeeper", ie);
        }
    }

    @Override
    public void notifyLedgerReplicationEnabled(final GenericCallback<Void> cb)
            throws ReplicationException.UnavailableException {
        LOG.debug("notifyLedgerReplicationEnabled()");
        Watcher w = new Watcher() {
            public void process(WatchedEvent e) {
                if (e.getType() == Watcher.Event.EventType.NodeDeleted) {
                    cb.operationComplete(0, null);
                }
            }
        };
        try {
            if (null == zkc.exists(basePath + '/'
                    + BookKeeperConstants.DISABLE_NODE, w)) {
                cb.operationComplete(0, null);
                return;
            }
        } catch (KeeperException ke) {
            LOG.error("Error while checking the state of "
                    + "ledger re-replication", ke);
            throw new ReplicationException.UnavailableException(
                    "Error contacting zookeeper", ke);
        } catch (InterruptedException ie) {
            Thread.currentThread().interrupt();
            throw new ReplicationException.UnavailableException(
                    "Interrupted while contacting zookeeper", ie);
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/meta/ZkVersion.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */
package org.apache.bookkeeper.meta;

import org.apache.bookkeeper.versioning.Version;
import org.apache.bookkeeper.versioning.Version.Occurred;

public class ZkVersion implements Version {
    int znodeVersion;

    public ZkVersion(int version) {
        znodeVersion = version;
    }

    @Override
    public Occurred compare(Version v) {
        if (null == v) {
            throw new NullPointerException("Version is not allowed to be null.");
        }
        if (v == Version.NEW) {
            return Occurred.AFTER;
        } else if (v == Version.ANY) {
            return Occurred.CONCURRENTLY;
        } else if (!(v instanceof ZkVersion)) {
            throw new IllegalArgumentException("Invalid version type");
        }
        ZkVersion zv = (ZkVersion)v;
        int res = znodeVersion - zv.znodeVersion;
        if (res == 0) {
            return Occurred.CONCURRENTLY;
        } else if (res < 0) {
            return Occurred.BEFORE;
        } else {
            return Occurred.AFTER;
        }
    }

    public int getZnodeVersion() {
        return znodeVersion;
    }

    public ZkVersion setZnodeVersion(int znodeVersion) {
        this.znodeVersion = znodeVersion;
        return this;
    }

    @Override
    public String toString() {
        return Integer.toString(znodeVersion, 10);
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/metastore/InMemoryMetaStore.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.metastore;

import java.util.HashMap;
import java.util.Map;

import org.apache.commons.configuration.Configuration;

public class InMemoryMetaStore implements MetaStore {

    static final int CUR_VERSION = 1;

    static Map<String, InMemoryMetastoreTable> tables =
        new HashMap<String, InMemoryMetastoreTable>();

    // for test
    public static void reset() {
        tables.clear();
    }

    @Override
    public String getName() {
        return getClass().getName();
    }

    @Override
    public int getVersion() {
        return CUR_VERSION;
    }

    @Override
    public void init(Configuration conf, int msVersion)
    throws MetastoreException {
        // do nothing
    }

    @Override
    public void close() {
        // do nothing
    }

    @Override
    public MetastoreTable createTable(String name) {
        return createInMemoryTable(name);
    }

    @Override
    public MetastoreScannableTable createScannableTable(String name) {
        return createInMemoryTable(name);
    }

    private InMemoryMetastoreTable createInMemoryTable(String name) {
        InMemoryMetastoreTable t = tables.get(name);
        if (t == null) {
            t = new InMemoryMetastoreTable(this, name);
            tables.put(name, t);
        }
        return t;
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/metastore/InMemoryMetastoreCursor.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.metastore;

import static org.apache.bookkeeper.metastore.InMemoryMetastoreTable.cloneValue;

import java.io.IOException;
import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.NavigableMap;
import java.util.Set;
import java.util.concurrent.ScheduledExecutorService;

import org.apache.bookkeeper.metastore.MSException.Code;
import org.apache.bookkeeper.versioning.Versioned;

class InMemoryMetastoreCursor implements MetastoreCursor {

    private final ScheduledExecutorService scheduler;
    private final Iterator<Map.Entry<String, Versioned<Value>>> iter;
    private final Set<String> fields;

    public InMemoryMetastoreCursor(NavigableMap<String, Versioned<Value>> map, Set<String> fields,
            ScheduledExecutorService scheduler) {
        this.iter = map.entrySet().iterator();
        this.fields = fields;
        this.scheduler = scheduler;
    }

    @Override
    public boolean hasMoreEntries() {
        return iter.hasNext();
    }

    @Override
    public Iterator<MetastoreTableItem> readEntries(int numEntries)
    throws MSException {
        if (numEntries < 0) {
            throw MSException.create(Code.IllegalOp);
        }
        return unsafeReadEntries(numEntries);
    }

    @Override
    public void asyncReadEntries(final int numEntries, final ReadEntriesCallback cb, final Object ctx) {
        scheduler.submit(new Runnable() {
            @Override
            public void run() {
                if (numEntries < 0) {
                    cb.complete(Code.IllegalOp.getCode(), null, ctx);
                    return;
                }
                Iterator<MetastoreTableItem> result = unsafeReadEntries(numEntries);
                cb.complete(Code.OK.getCode(), result, ctx);
            }
        });
    }

    private Iterator<MetastoreTableItem> unsafeReadEntries(int numEntries) {
        List<MetastoreTableItem> entries = new ArrayList<MetastoreTableItem>();
        int nCount = 0;
        while (iter.hasNext() && nCount < numEntries) {
            Map.Entry<String, Versioned<Value>> entry = iter.next();
            Versioned<Value> value = entry.getValue();
            Versioned<Value> vv = cloneValue(value.getValue(), value.getVersion(), fields);
            String key = entry.getKey();
            entries.add(new MetastoreTableItem(key, vv));
            ++nCount;
        }
        return entries.iterator();
    }

    @Override
    public void close() throws IOException {
        // do nothing
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/metastore/InMemoryMetastoreTable.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.metastore;

import java.util.NavigableMap;
import java.util.Set;
import java.util.TreeMap;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;

import org.apache.bookkeeper.metastore.MSException.Code;
import org.apache.bookkeeper.versioning.Version;
import org.apache.bookkeeper.versioning.Versioned;

public class InMemoryMetastoreTable implements MetastoreScannableTable {

    public static class MetadataVersion implements Version {
        int version;

        public MetadataVersion(int v) {
            this.version = v;
        }

        public MetadataVersion(MetadataVersion v) {
            this.version = v.version;
        }

        public synchronized MetadataVersion incrementVersion() {
            ++version;
            return this;
        }

        @Override
        public Occurred compare(Version v) {
            if (null == v) {
                throw new NullPointerException("Version is not allowed to be null.");
            }
            if (v == Version.NEW) {
                return Occurred.AFTER;
            } else if (v == Version.ANY) {
                return Occurred.CONCURRENTLY;
            } else if (!(v instanceof MetadataVersion)) {
                throw new IllegalArgumentException("Invalid version type");
            }
            MetadataVersion mv = (MetadataVersion)v;
            int res = version - mv.version;
            if (res == 0) {
                return Occurred.CONCURRENTLY;
            } else if (res < 0) {
                return Occurred.BEFORE;
            } else {
                return Occurred.AFTER;
            }
        }

        @Override
        public boolean equals(Object obj) {
            if (null == obj ||
                !(obj instanceof MetadataVersion)) {
                return false;
            }
            MetadataVersion v = (MetadataVersion)obj;
            return 0 == (version - v.version);
        }

        @Override
        public String toString() {
            return "version=" + version;
        }

        @Override
        public int hashCode() {
            return version;
        }
    }

    private String name;
    private TreeMap<String, Versioned<Value>> map = null;
    private ScheduledExecutorService scheduler;

    public InMemoryMetastoreTable(InMemoryMetaStore metastore, String name) {
        this.map = new TreeMap<String, Versioned<Value>>();
        this.name = name;
        this.scheduler = Executors.newSingleThreadScheduledExecutor();
    }

    @Override
    public String getName () {
        return this.name;
    }

    static Versioned<Value> cloneValue(Value value, Version version, Set<String> fields) {
        if (null != value) {
            Value newValue = new Value();
            if (ALL_FIELDS == fields) {
                fields = value.getFields();
            }
            for (String f : fields) {
                newValue.setField(f, value.getField(f));
            }
            value = newValue;
        }

        if (null == version) {
            throw new NullPointerException("Version isn't allowed to be null.");
        }
        if (Version.ANY != version && Version.NEW != version) {
            if (version instanceof MetadataVersion) {
                version = new MetadataVersion(((MetadataVersion)version).version);
            } else {
                throw new IllegalStateException("Wrong version type.");
            }
        }
        return new Versioned<Value>(value, version);
    }

    @Override
    public void get(final String key, final MetastoreCallback<Versioned<Value>> cb, final Object ctx) {
        scheduler.submit(new Runnable() {
            @Override
            public void run() {
                scheduleGet(key, ALL_FIELDS, cb, ctx);
            }
        });
    }

    @Override
    public void get(final String key, final Set<String> fields, final MetastoreCallback<Versioned<Value>> cb,
            final Object ctx) {
        scheduler.submit(new Runnable() {
            @Override
            public void run() {
                scheduleGet(key, fields, cb, ctx);
            }
        });
    }

    public synchronized void scheduleGet(String key, Set<String> fields, MetastoreCallback<Versioned<Value>> cb,
            Object ctx) {
        if (null == key) {
            cb.complete(Code.IllegalOp.getCode(), null, ctx);
            return;
        }
        Versioned<Value> vv = get(key);
        int rc = null == vv ? Code.NoKey.getCode() : Code.OK.getCode();
        if (vv != null) {
            vv = cloneValue(vv.getValue(), vv.getVersion(), fields);
        }
        cb.complete(rc, vv, ctx);
    }

    @Override
    public void put(final String key, final Value value, final Version version, final MetastoreCallback<Version> cb,
            final Object ctx) {
        scheduler.submit(new Runnable() {
            @Override
            public void run() {
                if (null == key || null == value || null == version) {
                    cb.complete(Code.IllegalOp.getCode(), null, ctx);
                    return;
                }
                Result<Version> result = put(key, value, version);
                cb.complete(result.code.getCode(), result.value, ctx);
            }
        });
    }

    @Override
    public void remove(final String key, final Version version, final MetastoreCallback<Void> cb, final Object ctx) {
        scheduler.submit(new Runnable() {
            @Override
            public void run() {
                if (null == key || null == version) {
                    cb.complete(Code.IllegalOp.getCode(), null, ctx);
                    return;
                }
                Code code = remove(key, version);
                cb.complete(code.getCode(), null, ctx);
            }
        });
    }

    @Override
    public void openCursor(MetastoreCallback<MetastoreCursor> cb, Object ctx) {
        openCursor(EMPTY_START_KEY, true, EMPTY_END_KEY, true, Order.ASC,
                   ALL_FIELDS, cb, ctx);
    }

    @Override
    public void openCursor(Set<String> fields,
                           MetastoreCallback<MetastoreCursor> cb, Object ctx) {
        openCursor(EMPTY_START_KEY, true, EMPTY_END_KEY, true, Order.ASC,
                   fields, cb, ctx);
    }

    @Override
    public void openCursor(String firstKey, boolean firstInclusive,
                           String lastKey, boolean lastInclusive,
                           Order order, MetastoreCallback<MetastoreCursor> cb,
                           Object ctx) {
        openCursor(firstKey, firstInclusive, lastKey, lastInclusive,
                   order, ALL_FIELDS, cb, ctx);
    }

    @Override
    public void openCursor(final String firstKey, final boolean firstInclusive,
                           final String lastKey, final boolean lastInclusive,
                           final Order order, final Set<String> fields,
                           final MetastoreCallback<MetastoreCursor> cb, final Object ctx) {
        scheduler.submit(new Runnable() {
            @Override
            public void run() {
                Result<MetastoreCursor> result = openCursor(firstKey, firstInclusive, lastKey, lastInclusive,
                        order, fields);
                cb.complete(result.code.getCode(), result.value, ctx);
            }
        });
    }

    private synchronized Versioned<Value> get(String key) {
        return map.get(key);
    }

    private synchronized Code remove(String key, Version version) {
        Versioned<Value> vv = map.get(key);
        if (null == vv) {
            return Code.NoKey;
        }
        if (Version.Occurred.CONCURRENTLY != vv.getVersion().compare(version)) {
            return Code.BadVersion;
        }
        map.remove(key);
        return Code.OK;
    }

    static class Result<T> {
        Code code;
        T value;

        public Result(Code code, T value) {
            this.code = code;
            this.value = value;
        }
    }

    private synchronized Result<Version> put(String key, Value value, Version version) {
        Versioned<Value> vv = map.get(key);
        if (vv == null) {
            if (Version.NEW != version) {
                return new Result<Version>(Code.NoKey, null);
            }
            vv = cloneValue(value, version, ALL_FIELDS);
            vv.setVersion(new MetadataVersion(0));
            map.put(key, vv);
            return new Result<Version>(Code.OK, new MetadataVersion(0));
        }
        if (Version.NEW == version) {
            return new Result<Version>(Code.KeyExists, null);
        }
        if (Version.Occurred.CONCURRENTLY != vv.getVersion().compare(version)) {
            return new Result<Version>(Code.BadVersion, null);
        }
        vv.setVersion(((MetadataVersion)vv.getVersion()).incrementVersion());
        vv.setValue(vv.getValue().merge(value));
        return new Result<Version>(Code.OK, new MetadataVersion((MetadataVersion)vv.getVersion()));
    }

    private synchronized Result<MetastoreCursor> openCursor(
            String firstKey, boolean firstInclusive,
            String lastKey, boolean lastInclusive,
            Order order, Set<String> fields) {
        if (0 == map.size()) {
            return new Result<MetastoreCursor>(Code.OK, MetastoreCursor.EMPTY_CURSOR);
        }

        boolean isLegalCursor = false;
        NavigableMap<String, Versioned<Value>> myMap = null;
        if (Order.ASC == order) {
            myMap = map;
            if (EMPTY_END_KEY == lastKey ||
                lastKey.compareTo(myMap.lastKey()) > 0) {
                lastKey = myMap.lastKey();
                lastInclusive = true;
            }
            if (EMPTY_START_KEY == firstKey ||
                firstKey.compareTo(myMap.firstKey()) < 0) {
                firstKey = myMap.firstKey();
                firstInclusive = true;
            }
            if (firstKey.compareTo(lastKey) <= 0) {
                isLegalCursor = true;
            }
        } else if (Order.DESC == order) {
            myMap = map.descendingMap();
            if (EMPTY_START_KEY == lastKey ||
                lastKey.compareTo(myMap.lastKey()) < 0) {
                lastKey = myMap.lastKey();
                lastInclusive = true;
            }
            if (EMPTY_END_KEY == firstKey ||
                firstKey.compareTo(myMap.firstKey()) > 0) {
                firstKey = myMap.firstKey();
                firstInclusive = true;
            }
            if (firstKey.compareTo(lastKey) >= 0) {
                isLegalCursor = true;
            }
        }

        if (!isLegalCursor || null == myMap) {
            return new Result<MetastoreCursor>(Code.IllegalOp, null);
        }
        MetastoreCursor cursor = new InMemoryMetastoreCursor(
                myMap.subMap(firstKey, firstInclusive, lastKey, lastInclusive), fields, scheduler);
        return new Result<MetastoreCursor>(Code.OK, cursor);
    }

    @Override
    public void close() {
        // do nothing
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/metastore/MetaStore.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.metastore;

import org.apache.commons.configuration.Configuration;

/**
 * Metadata Store Interface.
 */
public interface MetaStore {
    /**
     * Return the name of the plugin.
     *
     * @return the plugin name.
     */
    public String getName();

    /**
     * Get the plugin verison.
     *
     * @return the plugin version.
     */
    public int getVersion();

    /**
     * Initialize the meta store.
     *
     * @param config
     *          Configuration object passed to metastore
     * @param msVersion
     *          Version to initialize the metastore
     * @throws MetastoreException when failed to initialize
     */
    public void init(Configuration config, int msVersion)
    throws MetastoreException;

    /**
     * Close the meta store.
     */
    public void close();

    /**
     * Create a metastore table.
     *
     * @param name
     *          Table name.
     * @return a metastore table
     * @throws MetastoreException when failed to create the metastore table.
     */
    public MetastoreTable createTable(String name)
    throws MetastoreException;

    /**
     * Create a scannable metastore table.
     *
     * @param name
     *          Table name.
     * @return a metastore scannable table
     * @throws MetastoreException when failed to create the metastore table.
     */
    public MetastoreScannableTable createScannableTable(String name)
    throws MetastoreException;

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/metastore/MetastoreCallback.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.metastore;

public interface MetastoreCallback<T> {
    /**
     * @see MSException.Code
     */
    public void complete(int rc, T value, Object ctx);
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/metastore/MetastoreCursor.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.metastore;

import java.io.Closeable;
import java.io.IOException;
import java.util.Iterator;

public interface MetastoreCursor extends Closeable {

    public static MetastoreCursor EMPTY_CURSOR = new MetastoreCursor() {
        @Override
        public boolean hasMoreEntries() {
            return false;
        }

        @Override
        public Iterator<MetastoreTableItem> readEntries(int numEntries)
        throws MSException {
            throw new MSException.NoEntriesException("No entries left in the cursor.");
        }

        @Override
        public void asyncReadEntries(int numEntries, ReadEntriesCallback callback, Object ctx) {
            callback.complete(MSException.Code.NoEntries.getCode(), null, ctx);
        }

        @Override
        public void close() throws IOException {
            // do nothing
        }
    };

    public static interface ReadEntriesCallback extends
        MetastoreCallback<Iterator<MetastoreTableItem>> {
    }

    /**
     * Is there any entries left in the cursor to read.
     *
     * @return true if there is entries left, false otherwise.
     */
    public boolean hasMoreEntries();

    /**
     * Read entries from the cursor, up to the specified <code>numEntries</code>.
     * The returned list can be smaller.
     *
     * @param numEntries
     *            maximum number of entries to read
     * @return the iterator of returned entries.
     * @throws MSException when failed to read entries from the cursor.
     */
    public Iterator<MetastoreTableItem> readEntries(int numEntries)
    throws MSException;

    /**
     * Asynchronously read entries from the cursor, up to the specified <code>numEntries</code>.
     *
     * @see #readEntries(int)
     * @param numEntries
     *            maximum number of entries to read
     * @param callback
     *            callback object
     * @param ctx
     *            opaque context
     */
    public void asyncReadEntries(int numEntries, ReadEntriesCallback callback, Object ctx);
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/metastore/MetastoreException.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.metastore;

@SuppressWarnings("serial")
public class MetastoreException extends Exception {

    public MetastoreException(String message) {
        super(message);
    }

    public MetastoreException(String message, Throwable t) {
        super(message, t);
    }

    public MetastoreException(Throwable t) {
        super(t);
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/metastore/MetastoreFactory.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.metastore;

import org.apache.bookkeeper.util.ReflectionUtils;

public class MetastoreFactory {

    public static MetaStore createMetaStore(String name)
    throws MetastoreException {
        try {
            return ReflectionUtils.newInstance(name, MetaStore.class);
        } catch (Throwable t) {
            throw new MetastoreException("Failed to instantiate metastore : " + name);
        }
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/metastore/MetastoreScannableTable.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.metastore;

import java.util.Set;

public interface MetastoreScannableTable extends MetastoreTable {

    // Used by cursor, etc when they want to start at the beginning of a table
    public static final String EMPTY_START_KEY = null;
    // Last row in a table.
    public static final String EMPTY_END_KEY = null;
    // the order to loop over a table
    public static enum Order {
        ASC,
        DESC
    }

    /**
     * Open a cursor to loop over the entries belonging to a key range,
     * which returns all fields for each entry.
     *
     * <p>
     * Return Code:<br/>
     * {@link MSException.Code.OK}: an opened cursor<br/>
     * {@link MSException.Code.IllegalOp}/{@link MSException.Code.ServiceDown}:
     * other issues
     * </p>
     *
     * @param firstKey
     *            Key to start scanning. If it is {@link EMPTY_START_KEY}, it starts
     *            from first key (inclusive).
     * @param firstInclusive
     *            true if firstKey is to be included in the returned view.
     * @param lastKey
     *            Key to stop scanning. If it is {@link EMPTY_END_KEY}, scan ends at
     *            the lastKey of the table (inclusive).
     * @param lastInclusive
     *            true if lastKey is to be included in the returned view.
     * @param order
     *            the order to loop over the entries
     * @param cb
     *            Callback to return an opened cursor.
     * @param ctx
     *            Callback context
     */
    public void openCursor(String firstKey, boolean firstInclusive,
                           String lastKey, boolean lastInclusive,
                           Order order,
                           MetastoreCallback<MetastoreCursor> cb,
                           Object ctx);

    /**
     * Open a cursor to loop over the entries belonging to a key range,
     * which returns the specified <code>fields</code> for each entry.
     *
     * <p>
     * Return Code:<br/>
     * {@link MSException.Code.OK}: an opened cursor<br/>
     * {@link MSException.Code.IllegalOp}/{@link MSException.Code.ServiceDown}:
     * other issues
     * </p>
     *
     * @param firstKey
     *            Key to start scanning. If it is {@link EMPTY_START_KEY}, it starts
     *            from first key (inclusive).
     * @param firstInclusive
     *            true if firstKey is to be included in the returned view.
     * @param lastKey
     *            Key to stop scanning. If it is {@link EMPTY_END_KEY}, scan ends at
     *            the lastKey of the table (inclusive).
     * @param lastInclusive
     *            true if lastKey is to be included in the returned view.
     * @param order
     *            the order to loop over the entries
     * @param fields
     *            Fields to select
     * @param cb
     *            Callback to return an opened cursor.
     * @param ctx
     *            Callback context
     */
    public void openCursor(String firstKey, boolean firstInclusive,
                           String lastKey, boolean lastInclusive,
                           Order order, Set<String> fields,
                           MetastoreCallback<MetastoreCursor> cb,
                           Object ctx);

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/metastore/MetastoreTable.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.metastore;

import java.util.HashSet;
import java.util.Set;
import org.apache.bookkeeper.versioning.Version;
import org.apache.bookkeeper.versioning.Versioned;

public interface MetastoreTable {

    // select all fields when reading or scanning entries
    public static final Set<String> ALL_FIELDS = null;
    // select non fields to return when reading/scanning entries
    public static final Set<String> NON_FIELDS = new HashSet<String>();

    /**
     * Get table name.
     *
     * @return table name
     */
    public String getName();

    /**
     * Get all fields of a key.
     *
     * <p>
     * Return Code:<ul>
     * <li>{@link MSException.Code.OK}: success returning the key</li>
     * <li>{@link MSException.Code.NoKey}: no key found</li>
     * <li>{@link MSException.Code.IllegalOp}/{@link MSException.Code.ServiceDown}: other issues</li>
     * </ul></p>
     *
     * @param key
     *          Key Name
     * @param cb
     *          Callback to return all fields of the key
     * @param ctx
     *          Callback context
     */
    public void get(String key, MetastoreCallback<Versioned<Value>> cb, Object ctx);

    /**
     * Get specified fields of a key.
     *
     * <p>
     * Return Code:<ul>
     * <li>{@link MSException.Code.OK}: success returning the key</li>
     * <li>{@link MSException.Code.NoKey}: no key found</li>
     * <li>{@link MSException.Code.IllegalOp}/{@link MSException.Code.ServiceDown}: other issues</li>
     * </ul></p>
     *
     * @param key
     *          Key Name
     * @param fields
     *          Fields to return
     * @param cb
     *          Callback to return specified fields of the key
     * @param ctx
     *          Callback context
     */
    public void get(String key, Set<String> fields,
                    MetastoreCallback<Versioned<Value>> cb, Object ctx);

    /**
     * Update a key according to its version.
     *
     * <p>
     * Return Code:<ul>
     * <li>{@link MSException.Code.OK}: success updating the key</li>
     * <li>{@link MSException.Code.BadVersion}: failed to update the key due to bad version</li>
     * <li>{@link MSException.Code.NoKey}: no key found to update data, if not provided {@link Version.NEW}</li>
     * <li>{@link MSException.Code.KeyExists}: entry exists providing {@link Version.NEW}</li>
     * <li>{@link MSException.Code.IllegalOp}/{@link MSException.Code.ServiceDown}: other issues</li>
     * </ul></p>
     *
     * The key is updated only when the version matches its current version.
     * In particular, if the provided version is:<ul>
     * <li>{@link Version.ANY}: update the data without comparing its version.
     *      <b>Note this usage is not encouraged since it may mess up data consistency.</b></li>
     * <li>{@link Version.NEW}: create the entry if it doesn't exist before;
     *      Otherwise return {@link MSException.Code.KeyExists}.</li>
     * </ul>
     *
     * @param key
     *          Key Name
     * @param value
     *          Value to update.
     * @param version
     *          Version specified to update.
     * @param cb
     *          Callback to return new version after updated.
     * @param ctx
     *          Callback context
     */
    public void put(String key, Value value, Version version, MetastoreCallback<Version> cb, Object ctx);

    /**
     * Remove a key by its version.
     *
     * The key is removed only when the version matches its current version.
     * If <code>version</code> is {@link Version.ANY}, the key would be removed directly.
     *
     * <p>
     * Return Code:<ul>
     * <li>{@link MSException.Code.OK}: success updating the key</li>
     * <li>{@link MSException.Code.NoKey}: if the key doesn't exist.</li>
     * <li>{@link MSException.Code.BadVersion}: failed to delete the key due to bad version</li>
     * <li>{@link MSException.Code.IllegalOp}/{@link MSException.Code.ServiceDown}: other issues</li>
     * </ul></p>
     *
     * @param key
     *          Key Name.
     * @param version
     *          Version specified to remove.
     * @param cb
     *          Callback to return all fields of the key
     * @param ctx
     *          Callback context
     */
    public void remove(String key, Version version,
                       MetastoreCallback<Void> cb, Object ctx);

    /**
     * Open a cursor to loop over all the entries of the table,
     * which returns all fields for each entry.
     * The returned cursor doesn't need to guarantee any order,
     * since the underlying might be a hash table or an order table.
     *
     * @param cb
     *          Callback to return an opened cursor
     * @param ctx
     *          Callback context
     */
    public void openCursor(MetastoreCallback<MetastoreCursor> cb, Object ctx);

    /**
     * Open a cursor to loop over all the entries of the table,
     * which returns the specified <code>fields</code> for each entry.
     * The returned cursor doesn't need to guarantee any order,
     * since the underlying might be a hash table or an order table.
     *
     * @param fields
     *          Fields to select
     * @param cb
     *          Callback to return an opened cursor
     * @param ctx
     *          Callback context
     */
    public void openCursor(Set<String> fields,
                           MetastoreCallback<MetastoreCursor> cb, Object ctx);

    /**
     * Close the table.
     */
    public void close();
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/metastore/MetastoreTableItem.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.metastore;

import org.apache.bookkeeper.versioning.Versioned;

/**
 * Identify an item in a metastore table.
 */
public class MetastoreTableItem {

    private String key;
    private Versioned<Value> value;

    public MetastoreTableItem(String key, Versioned<Value> value) {
        this.key = key;
        this.value = value;
    }

    /**
     * Get the key of the table item.
     *
     * @return key of table item.
     */
    public String getKey() {
        return key;
    }

    /**
     * Set the key of the item.
     *
     * @param key Key
     */
    public void setKey(String key) {
        this.key = key;
    }

    /**
     * Get the value of the item.
     *
     * @return value of the item.
     */
    public Versioned<Value> getValue() {
        return value;
    }

    /**
     * Set the value of the item.
     *
     * @return value of the item.
     */
    public void setValue(Versioned<Value> value) {
        this.value = value;
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/metastore/MetastoreUtils.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.metastore;

import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.atomic.AtomicInteger;

import org.apache.bookkeeper.metastore.MSException.Code;
import org.apache.bookkeeper.versioning.Version;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Provides utilities for metastore.
 */
public class MetastoreUtils {

    protected final static Logger logger = LoggerFactory.getLogger(MetastoreUtils.class);

    static class MultiMetastoreCallback<T> implements MetastoreCallback<T> {

        int rc = Code.OK.getCode();
        final int numOps;
        final AtomicInteger numFinished = new AtomicInteger(0);
        final CountDownLatch doneLatch = new CountDownLatch(1);

        MultiMetastoreCallback(int numOps) {
            this.numOps = numOps;
        }

        @Override
        public void complete(int rc, T value, Object ctx) {
            if (Code.OK.getCode() != rc) {
                this.rc = rc;
                doneLatch.countDown();
                return;
            }
            if (numFinished.incrementAndGet() == numOps) {
                doneLatch.countDown();
            }
        }

        public void waitUntilAllFinished() throws MSException, InterruptedException {
            doneLatch.await();
            if (Code.OK.getCode() != rc) {
                throw MSException.create(Code.get(rc));
            }
        }
    }

    static class SyncMetastoreCallback<T> implements MetastoreCallback<T> {

        int rc;
        T result;
        final CountDownLatch doneLatch = new CountDownLatch(1);

        @Override
        public void complete(int rc, T value, Object ctx) {
            this.rc = rc;
            result = value;
            doneLatch.countDown();
        }

        public T getResult() throws MSException, InterruptedException {
            doneLatch.await();

            if (Code.OK.getCode() != rc) {
                throw MSException.create(Code.get(rc));
            }
            return result;
        }

    }

    /**
     * Clean the given table.
     *
     * @param table
     *          Metastore Table.
     * @param numEntriesPerScan
     *          Num entries per scan.
     * @throws MSException
     * @throws InterruptedException
     */
    public static void cleanTable(MetastoreTable table, int numEntriesPerScan)
    throws MSException, InterruptedException {
        // open cursor
        SyncMetastoreCallback<MetastoreCursor> openCb = new SyncMetastoreCallback<MetastoreCursor>();
        table.openCursor(MetastoreTable.NON_FIELDS, openCb, null);
        MetastoreCursor cursor = openCb.getResult();
        logger.info("Open cursor for table {} to clean entries.", table.getName());

        List<String> keysToClean = new ArrayList<String>(numEntriesPerScan);
        int numEntriesRemoved = 0;
        while (cursor.hasMoreEntries()) {
            logger.info("Fetching next {} entries from table {} to clean.",
                         numEntriesPerScan, table.getName());
            Iterator<MetastoreTableItem> iter = cursor.readEntries(numEntriesPerScan);
            keysToClean.clear();
            while (iter.hasNext()) {
                MetastoreTableItem item = iter.next();
                String key = item.getKey();
                keysToClean.add(key);
            }
            if (keysToClean.isEmpty()) {
                continue;
            }

            logger.info("Issuing deletes to delete keys {}", keysToClean);
            // issue deletes to delete batch of keys
            MultiMetastoreCallback<Void> mcb = new MultiMetastoreCallback<Void>(keysToClean.size());
            for (String key : keysToClean) {
                table.remove(key, Version.ANY, mcb, null);
            }
            mcb.waitUntilAllFinished();
            numEntriesRemoved += keysToClean.size();
            logger.info("Removed {} entries from table {}.", numEntriesRemoved, table.getName());
        }

        logger.info("Finished cleaning up table {}.", table.getName());
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/metastore/MSException.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.metastore;

import java.util.EnumSet;
import java.util.HashMap;
import java.util.Map;

@SuppressWarnings("serial")
public abstract class MSException extends Exception {

    /**
     * return codes
     */
    public static enum Code {
        OK (0, "OK"),
        BadVersion (-1, "Version conflict"),
        NoKey (-2, "Key does not exist"),
        KeyExists (-3, "Key exists"),
        NoEntries (-4, "No entries found"),

        InterruptedException (-100, "Operation interrupted"),
        IllegalOp (-101, "Illegal operation"),
        ServiceDown (-102, "Metadata service is down"),
        OperationFailure(-103, "Operaion failed on metadata storage server side");

        private static final Map<Integer, Code> codes
            = new HashMap<Integer, Code>();

        static {
            for (Code c : EnumSet.allOf(Code.class)) {
                codes.put(c.code, c);
            }
        }

        private final int code;
        private final String description;

        private Code(int code, String description) {
            this.code = code;
            this.description = description;
        }

        /**
         * Get the int value for a particular Code.
         *
         * @return error code as integer
         */
        public int getCode() {
            return code;
        }

        /**
         * Get the description for a particular Code.
         *
         * @return error description
         */
        public String getDescription() {
            return description;
        }

        /**
         * Get the Code value for a particular integer error code.
         *
         * @param code int error code
         * @return Code value corresponding to specified int code, or null.
         */
        public static Code get(int code) {
            return codes.get(code);
        }
    }

    private final Code code;

    MSException(Code code, String errMsg) {
        super(code.getDescription() + " : " + errMsg);
        this.code = code;
    }

    MSException(Code code, String errMsg, Throwable cause) {
        super(code.getDescription() + " : " + errMsg, cause);
        this.code = code;
    }

    public Code getCode() {
        return this.code;
    }

    public static MSException create(Code code) {
        return create(code, "", null);
    }

    public static MSException create(Code code, String errMsg) {
        return create(code, errMsg, null);
    }

    public static MSException create(Code code, String errMsg, Throwable cause) {
        switch (code) {
            case BadVersion:
                return new BadVersionException(errMsg, cause);
            case NoKey:
                return new NoKeyException(errMsg, cause);
            case KeyExists:
                return new KeyExistsException(errMsg, cause);
            case InterruptedException:
                return new MSInterruptedException(errMsg, cause);
            case IllegalOp:
                return new IllegalOpException(errMsg, cause);
            case ServiceDown:
                return new ServiceDownException(errMsg, cause);
            case OperationFailure:
                return new OperationFailureException(errMsg, cause);
            case OK:
            default:
                throw new IllegalArgumentException("Invalid exception code");
        }
    }

    public static class BadVersionException extends MSException {
        public BadVersionException(String errMsg) {
            super(Code.BadVersion, errMsg);
        }

        public BadVersionException(String errMsg, Throwable cause) {
            super(Code.BadVersion, errMsg, cause);
        }
    }

    public static class NoKeyException extends MSException {
        public NoKeyException(String errMsg) {
            super(Code.NoKey, errMsg);
        }

        public NoKeyException(String errMsg, Throwable cause) {
            super(Code.NoKey, errMsg, cause);
        }
    }

    // Exception would be thrown in a cursor if no entries found
    public static class NoEntriesException extends MSException {
        public NoEntriesException(String errMsg) {
            super(Code.NoEntries, errMsg);
        }

        public NoEntriesException(String errMsg, Throwable cause) {
            super(Code.NoEntries, errMsg, cause);
        }
    }

    public static class KeyExistsException extends MSException {
        public KeyExistsException(String errMsg) {
            super(Code.KeyExists, errMsg);
        }

        public KeyExistsException(String errMsg, Throwable cause) {
            super(Code.KeyExists, errMsg, cause);
        }
    }

    public static class MSInterruptedException extends MSException {
        public MSInterruptedException(String errMsg) {
            super(Code.InterruptedException, errMsg);
        }

        public MSInterruptedException(String errMsg, Throwable cause) {
            super(Code.InterruptedException, errMsg, cause);
        }
    }

    public static class IllegalOpException extends MSException {
        public IllegalOpException(String errMsg) {
            super(Code.IllegalOp, errMsg);
        }

        public IllegalOpException(String errMsg, Throwable cause) {
            super(Code.IllegalOp, errMsg, cause);
        }
    }

    public static class ServiceDownException extends MSException {
        public ServiceDownException(String errMsg) {
            super(Code.ServiceDown, errMsg);
        }

        public ServiceDownException(String errMsg, Throwable cause) {
            super(Code.ServiceDown, errMsg, cause);
        }
    }

    public static class OperationFailureException extends MSException {
        public OperationFailureException(String errMsg) {
            super(Code.OperationFailure, errMsg);
        }

        public OperationFailureException(String errMsg, Throwable cause) {
            super(Code.OperationFailure, errMsg, cause);
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/metastore/Value.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.metastore;

import com.google.common.primitives.UnsignedBytes;
import com.google.common.hash.Hasher;
import com.google.common.hash.HashFunction;
import com.google.common.hash.Hashing;

import java.util.Comparator;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;
import java.util.Collections;

import static org.apache.bookkeeper.metastore.MetastoreTable.ALL_FIELDS;

public class Value {
    static final Comparator<byte[]> comparator =
        UnsignedBytes.lexicographicalComparator();

    protected Map<String, byte[]> fields;

    public Value() {
        fields = new HashMap<String, byte[]>();
    }

    public Value(Value v) {
        fields = new HashMap<String, byte[]>(v.fields);
    }

    public byte[] getField(String field) {
        return fields.get(field);
    }

    public Value setField(String field, byte[] data) {
        fields.put(field, data);
        return this;
    }

    public Value clearFields() {
        fields.clear();
        return this;
    }

    public Set<String> getFields() {
        return fields.keySet();
    }

    public Map<String, byte[]> getFieldsMap() {
        return Collections.unmodifiableMap(fields);
    }

    /**
     * Select parts of fields.
     *
     * @param fields
     *            Parts of fields
     * @return new value with specified fields
     */
    public Value project(Set<String> fields) {
        if (ALL_FIELDS == fields) {
            return new Value(this);
        }
        Value v = new Value();
        for (String f : fields) {
            byte[] data = this.fields.get(f);
            v.setField(f, data);
        }
        return v;
    }

    @Override
    public int hashCode() {
        HashFunction hf = Hashing.murmur3_32();
        Hasher hc = hf.newHasher();
        for (String key : fields.keySet()) {
            hc.putString(key);
        }
        return hc.hash().asInt();
    }

    @Override
    public boolean equals(Object o) {
        if (!(o instanceof Value)) {
            return false;
        }
        Value other = (Value) o;
        if (fields.size() != other.fields.size()) {
            return false;
        }
        for (String f : fields.keySet()) {
            byte[] v1 = fields.get(f);
            byte[] v2 = other.fields.get(f);
            if (0 != comparator.compare(v1, v2)) {
                return false;
            }
        }
        return true;
    }

    /**
     * Merge other value.
     *
     * @param other
     *          Other Value
     */
    public Value merge(Value other) {
        for (Map.Entry<String, byte[]> entry : other.fields.entrySet()) {
            if (null == entry.getValue()) {
                fields.remove(entry.getKey());
            } else {
                fields.put(entry.getKey(), entry.getValue());
            }
        }
        return this;
    }

    @Override
    public String toString() {
        StringBuilder sb = new StringBuilder();
        sb.append("[");
        for (Map.Entry<String, byte[]> entry : fields.entrySet()) {
            String f = entry.getKey();
            if (null == f) {
                f = "NULL";
            }
            String value;
            if (null == entry.getValue()) {
                value = "NONE";
            } else {
                value = new String(entry.getValue());
            }
            sb.append("('").append(f).append("'=").append(value).append(")");
        }
        sb.append("]");
        return sb.toString();
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/proto/BKStats.java,false,"/**
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.proto;

import java.beans.ConstructorProperties;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Bookie Server Stats
 */
public class BKStats {
    private static final Logger LOG = LoggerFactory.getLogger(BKStats.class);
    private static BKStats instance = new BKStats();

    public static BKStats getInstance() {
        return instance;
    }

    /**
     * A read view of stats, also used in CompositeViewData to expose to JMX
     */
    public static class OpStatData {
        private final long maxLatency, minLatency;
        private final double avgLatency;
        private final long numSuccessOps, numFailedOps;
        private final String latencyHist;

        @ConstructorProperties({"maxLatency", "minLatency", "avgLatency",
                                "numSuccessOps", "numFailedOps", "latencyHist"})
        public OpStatData(long maxLatency, long minLatency, double avgLatency,
                          long numSuccessOps, long numFailedOps, String latencyHist) {
            this.maxLatency = maxLatency;
            this.minLatency = minLatency == Long.MAX_VALUE ? 0 : minLatency;
            this.avgLatency = avgLatency;
            this.numSuccessOps = numSuccessOps;
            this.numFailedOps = numFailedOps;
            this.latencyHist = latencyHist;
        }

        public long getMaxLatency() {
            return maxLatency;
        }

        public long getMinLatency() {
            return minLatency;
        }

        public double getAvgLatency() {
            return avgLatency;
        }

        public long getNumSuccessOps() {
            return numSuccessOps;
        }

        public long getNumFailedOps() {
            return numFailedOps;
        }

        public String getLatencyHist() {
            return latencyHist;
        }
    }

    /**
     * Operation Statistics
     */
    public static class OpStats {
        static final int NUM_BUCKETS = 3*9 + 2;

        long maxLatency = 0;
        long minLatency = Long.MAX_VALUE;
        double totalLatency = 0.0f;
        long numSuccessOps = 0;
        long numFailedOps = 0;
        long[] latencyBuckets = new long[NUM_BUCKETS];

        OpStats() {}

        /**
         * Increment number of failed operations
         */
        synchronized public void incrementFailedOps() {
            ++numFailedOps;
        }

        /**
         * Update Latency
         */
        synchronized public void updateLatency(long latency) {
            if (latency < 0) {
                // less than 0ms . Ideally this should not happen.
                // We have seen this latency negative in some cases due to the
                // behaviors of JVM. Ignoring the statistics updation for such
                // cases.
                LOG.warn("Latency time coming negative");
                return;
            }
            totalLatency += latency;
            ++numSuccessOps;
            if (latency < minLatency) {
                minLatency = latency;
            }
            if (latency > maxLatency) {
                maxLatency = latency;
            }
            int bucket;
            if (latency <= 100) { // less than 100ms
                bucket = (int)(latency / 10);
            } else if (latency <= 1000) { // 100ms ~ 1000ms
                bucket = 1 * 9 + (int)(latency / 100);
            } else if (latency <= 10000) { // 1s ~ 10s
                bucket = 2 * 9 + (int)(latency / 1000);
            } else { // more than 10s
                bucket = 3 * 9 + 1;
            }
            ++latencyBuckets[bucket];
        }

        public OpStatData toOpStatData() {
            double avgLatency = numSuccessOps > 0 ? totalLatency / numSuccessOps : 0.0f;
            StringBuilder sb = new StringBuilder();
            for (int i=0; i<NUM_BUCKETS; i++) {
                sb.append(latencyBuckets[i]);
                if (i != NUM_BUCKETS - 1) {
                    sb.append(',');
                }
            }

            return new OpStatData(maxLatency, minLatency, avgLatency, numSuccessOps, numFailedOps, sb.toString());
        }

        /**
         * Diff with base opstats
         *
         * @param base
         *        base opstats
         * @return diff opstats
         */
        public OpStats diff(OpStats base) {
            OpStats diff = new OpStats();
            diff.maxLatency = this.maxLatency > base.maxLatency ? this.maxLatency : base.maxLatency;
            diff.minLatency = this.minLatency > base.minLatency ? base.minLatency : this.minLatency;
            diff.totalLatency = this.totalLatency - base.totalLatency;
            diff.numSuccessOps = this.numSuccessOps - base.numSuccessOps;
            diff.numFailedOps = this.numFailedOps - base.numFailedOps;
            for (int i = 0; i < NUM_BUCKETS; i++) {
                diff.latencyBuckets[i] = this.latencyBuckets[i] - base.latencyBuckets[i];
            }
            return diff;
        }

        /**
         * Copy stats from other OpStats
         *
         * @param other other op stats
         * @return void
         */
        public synchronized void copyOf(OpStats other) {
            this.maxLatency = other.maxLatency;
            this.minLatency = other.minLatency;
            this.totalLatency = other.totalLatency;
            this.numSuccessOps = other.numSuccessOps;
            this.numFailedOps = other.numFailedOps;
            System.arraycopy(other.latencyBuckets, 0, this.latencyBuckets, 0, this.latencyBuckets.length);
        }
    }

    public static final int STATS_ADD = 0;
    public static final int STATS_READ = 1;
    public static final int STATS_UNKNOWN = 2;
    // NOTE: if add other stats, increment NUM_STATS
    public static final int NUM_STATS = 3;

    OpStats[] stats = new OpStats[NUM_STATS];

    private BKStats() {
        for (int i=0; i<NUM_STATS; i++) {
            stats[i] = new OpStats();
        }
    }

    /**
     * Stats of operations
     *
     * @return op stats
     */
    public OpStats getOpStats(int type) {
        return stats[type];
    }

    /**
     * Set stats of a specified operation
     *
     * @param type operation type
     * @param stat operation stats
     */
    public void setOpStats(int type, OpStats stat) {
        stats[type] = stat;
    }

    /**
     * Diff with base stats
     *
     * @param base base stats
     * @return diff stats
     */
    public BKStats diff(BKStats base) {
        BKStats diff = new BKStats();
        for (int i=0; i<NUM_STATS; i++) {
            diff.setOpStats(i, stats[i].diff(base.getOpStats(i)));
        }
        return diff;
    }

    /**
     * Copy stats from other stats
     *
     * @param other other stats
     * @return void
     */
    public void copyOf(BKStats other) {
        for (int i=0; i<NUM_STATS; i++) {
            stats[i].copyOf(other.getOpStats(i));
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/proto/BookieClient.java,true,"package org.apache.bookkeeper.proto;

/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

import java.util.Set;
import java.util.HashSet;
import java.io.IOException;
import java.net.InetSocketAddress;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.Executors;
import java.util.concurrent.atomic.AtomicLong;
import org.apache.bookkeeper.conf.ClientConfiguration;
import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.ReadEntryCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.WriteCallback;
import org.apache.bookkeeper.util.OrderedSafeExecutor;
import org.apache.bookkeeper.util.SafeRunnable;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.buffer.ChannelBuffer;
import org.jboss.netty.buffer.ChannelBuffers;
import org.jboss.netty.channel.socket.ClientSocketChannelFactory;
import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory;
import java.util.concurrent.locks.ReentrantReadWriteLock;

/**
 * Implements the client-side part of the BookKeeper protocol.
 *
 */
public class BookieClient {
    static final Logger LOG = LoggerFactory.getLogger(BookieClient.class);

    // This is global state that should be across all BookieClients
    AtomicLong totalBytesOutstanding = new AtomicLong();

    OrderedSafeExecutor executor;
    ClientSocketChannelFactory channelFactory;
    ConcurrentHashMap<InetSocketAddress, PerChannelBookieClient> channels = new ConcurrentHashMap<InetSocketAddress, PerChannelBookieClient>();

    private final ClientConfiguration conf;
    private volatile boolean closed;
    private ReentrantReadWriteLock closeLock;

    public BookieClient(ClientConfiguration conf, ClientSocketChannelFactory channelFactory, OrderedSafeExecutor executor) {
        this.conf = conf;
        this.channelFactory = channelFactory;
        this.executor = executor;
        this.closed = false;
        this.closeLock = new ReentrantReadWriteLock();
    }

    public PerChannelBookieClient lookupClient(InetSocketAddress addr) {
        PerChannelBookieClient channel = channels.get(addr);

        if (channel == null) {
            closeLock.readLock().lock();
            try {
                if (closed) {
                    return null;
                }
                channel = new PerChannelBookieClient(conf, executor, channelFactory, addr, totalBytesOutstanding);
                PerChannelBookieClient prevChannel = channels.putIfAbsent(addr, channel);
                if (prevChannel != null) {
                    channel = prevChannel;
                }
            } finally {
                closeLock.readLock().unlock();
            }
        }

        return channel;
    }

    public void closeClients(Set<InetSocketAddress> addrs) {
        final HashSet<PerChannelBookieClient> clients = new HashSet<PerChannelBookieClient>();
        for (InetSocketAddress a : addrs) {
            PerChannelBookieClient c = channels.get(a);
            if (c != null) {
                clients.add(c);
            }
        }

        if (clients.size() == 0) {
            return;
        }
        executor.submit(new SafeRunnable() {
                @Override
                public void safeRun() {
                    for (PerChannelBookieClient c : clients) {
                        c.disconnect();
                    }
                }
            });
    }

    public void addEntry(final InetSocketAddress addr, final long ledgerId, final byte[] masterKey, final long entryId,
            final ChannelBuffer toSend, final WriteCallback cb, final Object ctx, final int options) {
        final PerChannelBookieClient client = lookupClient(addr);
        if (client == null) {
            cb.writeComplete(BKException.Code.BookieHandleNotAvailableException,
                             ledgerId, entryId, addr, ctx);
            return;
        }

        client.connectIfNeededAndDoOp(new GenericCallback<Void>() {
            @Override
            public void operationComplete(final int rc, Void result) {
                if (rc != BKException.Code.OK) {
                    executor.submitOrdered(ledgerId, new SafeRunnable() {
                        @Override
                        public void safeRun() {
                            cb.writeComplete(rc, ledgerId, entryId, addr, ctx);
                        }
                    });
                    return;
                }
                client.addEntry(ledgerId, masterKey, entryId, toSend, cb, ctx, options);
            }
        });
    }

    public void readEntryAndFenceLedger(final InetSocketAddress addr,
                                        final long ledgerId,
                                        final byte[] masterKey,
                                        final long entryId,
                                        final ReadEntryCallback cb,
                                        final Object ctx) {
        final PerChannelBookieClient client = lookupClient(addr);
        if (client == null) {
            cb.readEntryComplete(BKException.Code.BookieHandleNotAvailableException,
                                 ledgerId, entryId, null, ctx);
            return;
        }

        client.connectIfNeededAndDoOp(new GenericCallback<Void>() {
            @Override
            public void operationComplete(final int rc, Void result) {
                if (rc != BKException.Code.OK) {
                    executor.submitOrdered(ledgerId, new SafeRunnable() {
                        @Override
                        public void safeRun() {
                            cb.readEntryComplete(rc, ledgerId, entryId, null, ctx);
                        }
                    });
                    return;
                }
                client.readEntryAndFenceLedger(ledgerId, masterKey, entryId, cb, ctx);
            }
        });
    }

    public void readEntry(final InetSocketAddress addr, final long ledgerId, final long entryId,
                          final ReadEntryCallback cb, final Object ctx) {
        final PerChannelBookieClient client = lookupClient(addr);
        if (client == null) {
            cb.readEntryComplete(BKException.Code.BookieHandleNotAvailableException,
                                 ledgerId, entryId, null, ctx);
            return;
        }

        client.connectIfNeededAndDoOp(new GenericCallback<Void>() {
            @Override
            public void operationComplete(final int rc, Void result) {
                if (rc != BKException.Code.OK) {
                    executor.submitOrdered(ledgerId, new SafeRunnable() {
                        @Override
                        public void safeRun() {
                            cb.readEntryComplete(rc, ledgerId, entryId, null, ctx);
                        }
                    });
                    return;
                }
                client.readEntry(ledgerId, entryId, cb, ctx);
            }
        });
    }

    public void close() {
        closeLock.writeLock().lock();
        try {
            closed = true;
            for (PerChannelBookieClient channel: channels.values()) {
                channel.close();
            }
        } finally {
            closeLock.writeLock().unlock();
        }
    }

    private static class Counter {
        int i;
        int total;

        synchronized void inc() {
            i++;
            total++;
        }

        synchronized void dec() {
            i--;
            notifyAll();
        }

        synchronized void wait(int limit) throws InterruptedException {
            while (i > limit) {
                wait();
            }
        }

        synchronized int total() {
            return total;
        }
    }

    /**
     * @param args
     * @throws IOException
     * @throws NumberFormatException
     * @throws InterruptedException
     */
    public static void main(String[] args) throws NumberFormatException, IOException, InterruptedException {
        if (args.length != 3) {
            System.err.println("USAGE: BookieClient bookieHost port ledger#");
            return;
        }
        WriteCallback cb = new WriteCallback() {

            public void writeComplete(int rc, long ledger, long entry, InetSocketAddress addr, Object ctx) {
                Counter counter = (Counter) ctx;
                counter.dec();
                if (rc != 0) {
                    System.out.println("rc = " + rc + " for " + entry + "@" + ledger);
                }
            }
        };
        Counter counter = new Counter();
        byte hello[] = "hello".getBytes();
        long ledger = Long.parseLong(args[2]);
        ClientSocketChannelFactory channelFactory = new NioClientSocketChannelFactory(Executors.newCachedThreadPool(), Executors
                .newCachedThreadPool());
        OrderedSafeExecutor executor = new OrderedSafeExecutor(1);
        BookieClient bc = new BookieClient(new ClientConfiguration(), channelFactory, executor);
        InetSocketAddress addr = new InetSocketAddress(args[0], Integer.parseInt(args[1]));

        for (int i = 0; i < 100000; i++) {
            counter.inc();
            bc.addEntry(addr, ledger, new byte[0], i, ChannelBuffers.wrappedBuffer(hello), cb, counter, 0);
        }
        counter.wait(0);
        System.out.println("Total = " + counter.total());
        channelFactory.releaseExternalResources();
        executor.shutdown();
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/proto/BookieProtocol.java,false,"package org.apache.bookkeeper.proto;

/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

/**
 * The packets of the Bookie protocol all have a 4-byte integer indicating the
 * type of request or response at the very beginning of the packet followed by a
 * payload.
 *
 */
public interface BookieProtocol {

    /**
     * Lowest protocol version which will work with the bookie.
     */
    public static final byte LOWEST_COMPAT_PROTOCOL_VERSION = 0;

    /**
     * Current version of the protocol, which client will use. 
     */
    public static final byte CURRENT_PROTOCOL_VERSION = 2;

    /**
     * Entry Entry ID. To be used when no valid entry id can be assigned.
     */
    public static final long INVALID_ENTRY_ID = -1;

    /**
     * Entry identifier representing a request to obtain the last add entry confirmed
     */
    public static final long LAST_ADD_CONFIRMED = -1;

    /**
     * The length of the master key in add packets. This
     * is fixed at 20 for historic reasons. This is because it
     * is always generated using the MacDigestManager regardless
     * of whether Mac is being used for the digest or not
     */
    public static final int MASTER_KEY_LENGTH = 20;

    /** 
     * The first int of a packet is the header.
     * It contains the version, opCode and flags.
     * The initial versions of BK didn't have this structure
     * and just had an int representing the opCode as the 
     * first int. This handles that case also. 
     */
    static class PacketHeader {
        final byte version;
        final byte opCode;
        final short flags;

        public PacketHeader(byte version, byte opCode, short flags) {
            this.version = version;
            this.opCode = opCode;
            this.flags = flags;
        }
        
        int toInt() {
            if (version == 0) {
                return (int)opCode;
            } else {
                return ((version & 0xFF) << 24) 
                    | ((opCode & 0xFF) << 16)
                    | (flags & 0xFFFF);
            }
        }

        static PacketHeader fromInt(int i) {
            byte version = (byte)(i >> 24); 
            byte opCode = 0;
            short flags = 0;
            if (version == 0) {
                opCode = (byte)i;
            } else {
                opCode = (byte)((i >> 16) & 0xFF);
                flags = (short)(i & 0xFFFF);
            }
            return new PacketHeader(version, opCode, flags);
        }

        byte getVersion() {
            return version;
        }

        byte getOpCode() {
            return opCode;
        }

        short getFlags() {
            return flags;
        }
    }

    /**
     * The Add entry request payload will be a ledger entry exactly as it should
     * be logged. The response payload will be a 4-byte integer that has the
     * error code followed by the 8-byte ledger number and 8-byte entry number
     * of the entry written.
     */
    public static final byte ADDENTRY = 1;
    /**
     * The Read entry request payload will be the ledger number and entry number
     * to read. (The ledger number is an 8-byte integer and the entry number is
     * a 8-byte integer.) The response payload will be a 4-byte integer
     * representing an error code and a ledger entry if the error code is EOK,
     * otherwise it will be the 8-byte ledger number and the 4-byte entry number
     * requested. (Note that the first sixteen bytes of the entry happen to be
     * the ledger number and entry number as well.)
     */
    public static final byte READENTRY = 2;

    /**
     * The error code that indicates success
     */
    public static final int EOK = 0;
    /**
     * The error code that indicates that the ledger does not exist
     */
    public static final int ENOLEDGER = 1;
    /**
     * The error code that indicates that the requested entry does not exist
     */
    public static final int ENOENTRY = 2;
    /**
     * The error code that indicates an invalid request type
     */
    public static final int EBADREQ = 100;
    /**
     * General error occurred at the server
     */
    public static final int EIO = 101;

    /**
     * Unauthorized access to ledger
     */
    public static final int EUA = 102;

    /**
     * The server version is incompatible with the client
     */
    public static final int EBADVERSION = 103;

    /**
     * Attempt to write to fenced ledger
     */
    public static final int EFENCED = 104;

    /**
     * The server is running as read-only mode
     */
    public static final int EREADONLY = 105;

    public static final short FLAG_NONE = 0x0;
    public static final short FLAG_DO_FENCING = 0x0001;
    public static final short FLAG_RECOVERY_ADD = 0x0002;
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/proto/BookieServer.java,true,"/**
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */
package org.apache.bookkeeper.proto;

import java.io.File;
import java.io.IOException;
import java.net.InetSocketAddress;
import java.net.MalformedURLException;
import java.net.UnknownHostException;
import java.nio.ByteBuffer;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.Future;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.TimeoutException;

import org.apache.zookeeper.KeeperException;

import org.apache.bookkeeper.bookie.Bookie;
import org.apache.bookkeeper.bookie.BookieException;
import org.apache.bookkeeper.bookie.ExitCode;
import org.apache.bookkeeper.conf.ServerConfiguration;
import org.apache.bookkeeper.jmx.BKMBeanRegistry;
import org.apache.bookkeeper.proto.NIOServerFactory.Cnxn;
import org.apache.bookkeeper.replication.AutoRecoveryMain;
import org.apache.bookkeeper.replication.ReplicationException.CompatibilityException;
import org.apache.bookkeeper.replication.ReplicationException.UnavailableException;
import org.apache.bookkeeper.util.MathUtils;

import com.google.common.annotations.VisibleForTesting;

import static org.apache.bookkeeper.proto.BookieProtocol.PacketHeader;
import org.apache.commons.configuration.ConfigurationException;
import org.apache.commons.cli.BasicParser;
import org.apache.commons.cli.Options;
import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.HelpFormatter;
import org.apache.commons.cli.ParseException;
import org.apache.commons.codec.binary.Hex;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Implements the server-side part of the BookKeeper protocol.
 *
 */
public class BookieServer implements NIOServerFactory.PacketProcessor, BookkeeperInternalCallbacks.WriteCallback {
    final ServerConfiguration conf;
    NIOServerFactory nioServerFactory;
    private volatile boolean running = false;
    Bookie bookie;
    DeathWatcher deathWatcher;
    static Logger LOG = LoggerFactory.getLogger(BookieServer.class);

    int exitCode = ExitCode.OK;

    // operation stats
    final BKStats bkStats = BKStats.getInstance();
    final boolean isStatsEnabled;
    protected BookieServerBean jmxBkServerBean;
    private AutoRecoveryMain autoRecoveryMain = null;
    private boolean isAutoRecoveryDaemonEnabled;

    public BookieServer(ServerConfiguration conf) throws IOException,
            KeeperException, InterruptedException, BookieException,
            UnavailableException, CompatibilityException {
        this.conf = conf;
        this.bookie = newBookie(conf);
        isAutoRecoveryDaemonEnabled = conf.isAutoRecoveryDaemonEnabled();
        if (isAutoRecoveryDaemonEnabled) {
            this.autoRecoveryMain = new AutoRecoveryMain(conf);
        }
        isStatsEnabled = conf.isStatisticsEnabled();
    }

    protected Bookie newBookie(ServerConfiguration conf)
        throws IOException, KeeperException, InterruptedException, BookieException {
        return new Bookie(conf);
    }

    public void start() throws IOException, UnavailableException {
        nioServerFactory = new NIOServerFactory(conf, this);

        this.bookie.start();
        if (isAutoRecoveryDaemonEnabled && this.autoRecoveryMain != null) {
            this.autoRecoveryMain.start();
        }

        nioServerFactory.start();
        running = true;
        deathWatcher = new DeathWatcher(conf);
        deathWatcher.start();

        // register jmx
        registerJMX();
    }

    public InetSocketAddress getLocalAddress() {
        try {
            return Bookie.getBookieAddress(conf);
        } catch (UnknownHostException uhe) {
            return nioServerFactory.getLocalAddress();
        }
    }

    @VisibleForTesting
    public Bookie getBookie() {
        return bookie;
    }

    /**
     * Suspend processing of requests in the bookie (for testing)
     */
    @VisibleForTesting
    public void suspendProcessing() {
        nioServerFactory.suspendProcessing();
    }

    /**
     * Resume processing requests in the bookie (for testing)
     */
    @VisibleForTesting
    public void resumeProcessing() {
        nioServerFactory.resumeProcessing();
    }

    public synchronized void shutdown() {
        if (!running) {
            return;
        }
        nioServerFactory.shutdown();
        exitCode = bookie.shutdown();
        if (isAutoRecoveryDaemonEnabled && this.autoRecoveryMain != null) {
            this.autoRecoveryMain.shutdown();
        }
        running = false;

        // unregister JMX
        unregisterJMX();
    }

    protected void registerJMX() {
        try {
            jmxBkServerBean = new BookieServerBean(conf, this);
            BKMBeanRegistry.getInstance().register(jmxBkServerBean, null);

            bookie.registerJMX(jmxBkServerBean);
        } catch (Exception e) {
            LOG.warn("Failed to register with JMX", e);
            jmxBkServerBean = null;
        }
    }

    protected void unregisterJMX() {
        try {
            bookie.unregisterJMX();
            if (jmxBkServerBean != null) {
                BKMBeanRegistry.getInstance().unregister(jmxBkServerBean);
            }
        } catch (Exception e) {
            LOG.warn("Failed to unregister with JMX", e);
        }
        jmxBkServerBean = null;
    }

    public boolean isRunning() {
        return bookie.isRunning() && nioServerFactory.isRunning() && running;
    }

    /**
     * Whether bookie is running?
     *
     * @return true if bookie is running, otherwise return false
     */
    public boolean isBookieRunning() {
        return bookie.isRunning();
    }

    /**
     * Whether auto-recovery service running with Bookie?
     *
     * @return true if auto-recovery service is running, otherwise return false
     */
    public boolean isAutoRecoveryRunning() {
        return this.autoRecoveryMain != null
                && this.autoRecoveryMain.isAutoRecoveryRunning();
    }

    /**
     * Whether nio server is running?
     *
     * @return true if nio server is running, otherwise return false
     */
    public boolean isNioServerRunning() {
        return nioServerFactory.isRunning();
    }

    public void join() throws InterruptedException {
        nioServerFactory.join();
    }

    public int getExitCode() {
        return exitCode;
    }

    /**
     * A thread to watch whether bookie & nioserver is still alive
     */
    class DeathWatcher extends Thread {

        final int watchInterval;

        DeathWatcher(ServerConfiguration conf) {
            super("BookieDeathWatcher-" + conf.getBookiePort());
            watchInterval = conf.getDeathWatchInterval();
        }

        @Override
        public void run() {
            while(true) {
                try {
                    Thread.sleep(watchInterval);
                } catch (InterruptedException ie) {
                    // do nothing
                }
                if (!isBookieRunning()
                        || !isNioServerRunning()
                        || (isAutoRecoveryDaemonEnabled && !isAutoRecoveryRunning())) {
                    shutdown();
                    break;
                }
            }
        }
    }

    static final Options bkOpts = new Options();
    static {
        bkOpts.addOption("c", "conf", true, "Configuration for Bookie Server");
        bkOpts.addOption("withAutoRecovery", false,
                "Start Autorecovery service Bookie server");
        bkOpts.addOption("h", "help", false, "Print help message");
    }

    /**
     * Print usage
     */
    private static void printUsage() {
        HelpFormatter hf = new HelpFormatter();
        hf.printHelp("BookieServer [options]\n\tor\n"
                   + "BookieServer <bookie_port> <zk_servers> <journal_dir> <ledger_dir [ledger_dir]>", bkOpts);
    }

    private static void loadConfFile(ServerConfiguration conf, String confFile)
        throws IllegalArgumentException {
        try {
            conf.loadConf(new File(confFile).toURI().toURL());
        } catch (MalformedURLException e) {
            LOG.error("Could not open configuration file: " + confFile, e);
            throw new IllegalArgumentException();
        } catch (ConfigurationException e) {
            LOG.error("Malformed configuration file: " + confFile, e);
            throw new IllegalArgumentException();
        }
        LOG.info("Using configuration file " + confFile);
    }

    private static ServerConfiguration parseArgs(String[] args)
        throws IllegalArgumentException {
        try {
            BasicParser parser = new BasicParser();
            CommandLine cmdLine = parser.parse(bkOpts, args);

            if (cmdLine.hasOption('h')) {
                throw new IllegalArgumentException();
            }

            ServerConfiguration conf = new ServerConfiguration();
            String[] leftArgs = cmdLine.getArgs();

            if (cmdLine.hasOption('c')) {
                if (null != leftArgs && leftArgs.length > 0) {
                    throw new IllegalArgumentException();
                }
                String confFile = cmdLine.getOptionValue("c");
                loadConfFile(conf, confFile);
                return conf;
            }

            if (cmdLine.hasOption("withAutoRecovery")) {
                conf.setAutoRecoveryDaemonEnabled(true);
            }

            if (leftArgs.length < 4) {
                throw new IllegalArgumentException();
            }

            // command line arguments overwrite settings in configuration file
            conf.setBookiePort(Integer.parseInt(leftArgs[0]));
            conf.setZkServers(leftArgs[1]);
            conf.setJournalDirName(leftArgs[2]);
            String[] ledgerDirNames = new String[leftArgs.length - 3];
            System.arraycopy(leftArgs, 3, ledgerDirNames, 0, ledgerDirNames.length);
            conf.setLedgerDirNames(ledgerDirNames);

            return conf;
        } catch (ParseException e) {
            LOG.error("Error parsing command line arguments : ", e);
            throw new IllegalArgumentException(e);
        }
    }

    /**
     * @param args
     * @throws IOException
     * @throws InterruptedException
     */
    public static void main(String[] args) {
        ServerConfiguration conf = null;
        try {
            conf = parseArgs(args);
        } catch (IllegalArgumentException iae) {
            LOG.error("Error parsing command line arguments : ", iae);
            System.err.println(iae.getMessage());
            printUsage();
            System.exit(ExitCode.INVALID_CONF);
        }

        StringBuilder sb = new StringBuilder();
        String[] ledgerDirNames = conf.getLedgerDirNames();
        for (int i = 0; i < ledgerDirNames.length; i++) {
            if (i != 0) {
                sb.append(',');
            }
            sb.append(ledgerDirNames[i]);
        }

        String hello = String.format(
                           "Hello, I'm your bookie, listening on port %1$s. ZKServers are on %2$s. Journals are in %3$s. Ledgers are stored in %4$s.",
                           conf.getBookiePort(), conf.getZkServers(),
                           conf.getJournalDirName(), sb);
        LOG.info(hello);
        try {
            final BookieServer bs = new BookieServer(conf);
            bs.start();
            Runtime.getRuntime().addShutdownHook(new Thread() {
                @Override
                public void run() {
                    bs.shutdown();
                    LOG.info("Shut down bookie server successfully");
                }
            });
            LOG.info("Register shutdown hook successfully");
            bs.join();

            System.exit(bs.getExitCode());
        } catch (Exception e) {
            LOG.error("Exception running bookie server : ", e);
            System.exit(ExitCode.SERVER_EXCEPTION);
        }
    }

    public void processPacket(ByteBuffer packet, Cnxn src) {
        PacketHeader h = PacketHeader.fromInt(packet.getInt());

        boolean success = false;
        int statType = BKStats.STATS_UNKNOWN;
        long startTime = 0;
        if (isStatsEnabled) {
            startTime = MathUtils.now();
        }

        // packet format is different between ADDENTRY and READENTRY
        long ledgerId = -1;
        long entryId = BookieProtocol.INVALID_ENTRY_ID;
        byte[] masterKey = null;
        switch (h.getOpCode()) {
        case BookieProtocol.ADDENTRY:
            // first read master key
            masterKey = new byte[BookieProtocol.MASTER_KEY_LENGTH];
            packet.get(masterKey, 0, BookieProtocol.MASTER_KEY_LENGTH);
            ByteBuffer bb = packet.duplicate();
            ledgerId = bb.getLong();
            entryId = bb.getLong();
            break;
        case BookieProtocol.READENTRY:
            ledgerId = packet.getLong();
            entryId = packet.getLong();
            break;
        }

        if (h.getVersion() < BookieProtocol.LOWEST_COMPAT_PROTOCOL_VERSION
            || h.getVersion() > BookieProtocol.CURRENT_PROTOCOL_VERSION) {
            LOG.error("Invalid protocol version, expected something between "
                      + BookieProtocol.LOWEST_COMPAT_PROTOCOL_VERSION 
                      + " & " + BookieProtocol.CURRENT_PROTOCOL_VERSION
                    + ". got " + h.getVersion());
            src.sendResponse(buildResponse(BookieProtocol.EBADVERSION, 
                                           h.getVersion(), h.getOpCode(), ledgerId, entryId));
            return;
        }
        short flags = h.getFlags();
        switch (h.getOpCode()) {
        case BookieProtocol.ADDENTRY:
            statType = BKStats.STATS_ADD;

            if (bookie.isReadOnly()) {
                LOG.warn("BookieServer is running as readonly mode,"
                        + " so rejecting the request from the client!");
                src.sendResponse(buildResponse(BookieProtocol.EREADONLY,
                        h.getVersion(), h.getOpCode(), ledgerId, entryId));
                break;
            }

            try {
                TimedCnxn tsrc = new TimedCnxn(src, startTime);
                if ((flags & BookieProtocol.FLAG_RECOVERY_ADD) == BookieProtocol.FLAG_RECOVERY_ADD) {
                    bookie.recoveryAddEntry(packet.slice(), this, tsrc, masterKey);
                } else {
                    bookie.addEntry(packet.slice(), this, tsrc, masterKey);
                }
                success = true;
            } catch (IOException e) {
                LOG.error("Error writing " + entryId + "@" + ledgerId, e);
                src.sendResponse(buildResponse(BookieProtocol.EIO, h.getVersion(), h.getOpCode(), ledgerId, entryId));
            } catch (BookieException.LedgerFencedException lfe) {
                LOG.error("Attempt to write to fenced ledger", lfe);
                src.sendResponse(buildResponse(BookieProtocol.EFENCED, h.getVersion(), h.getOpCode(), ledgerId, entryId));
            } catch (BookieException e) {
                LOG.error("Unauthorized access to ledger " + ledgerId, e);
                src.sendResponse(buildResponse(BookieProtocol.EUA, h.getVersion(), h.getOpCode(), ledgerId, entryId));
            }
            break;
        case BookieProtocol.READENTRY:
            statType = BKStats.STATS_READ;
            ByteBuffer[] rsp = new ByteBuffer[2];
            LOG.debug("Received new read request: {}, {}", ledgerId, entryId);
            int errorCode = BookieProtocol.EIO;
            try {
                Future<Boolean> fenceResult = null;
                if ((flags & BookieProtocol.FLAG_DO_FENCING) == BookieProtocol.FLAG_DO_FENCING) {
                    LOG.warn("Ledger " + ledgerId + " fenced by " + src.getPeerName());
                    if (h.getVersion() >= 2) {
                        masterKey = new byte[BookieProtocol.MASTER_KEY_LENGTH];
                        packet.get(masterKey, 0, BookieProtocol.MASTER_KEY_LENGTH);

                        fenceResult = bookie.fenceLedger(ledgerId, masterKey);
                    } else {
                        LOG.error("Password not provided, Not safe to fence {}", ledgerId);
                        throw BookieException.create(BookieException.Code.UnauthorizedAccessException);
                    }
                }
                rsp[1] = bookie.readEntry(ledgerId, entryId);
                LOG.debug("##### Read entry ##### {}", rsp[1].remaining());
                if (null != fenceResult) {
                    // TODO:
                    // currently we don't have readCallback to run in separated read
                    // threads. after BOOKKEEPER-429 is complete, we could improve
                    // following code to make it not wait here
                    //
                    // For now, since we only try to wait after read entry. so writing
                    // to journal and read entry are executed in different thread
                    // it would be fine.
                    try {
                        Boolean fenced = fenceResult.get(1000, TimeUnit.MILLISECONDS);
                        if (null == fenced || !fenced) {
                            // if failed to fence, fail the read request to make it retry.
                            errorCode = BookieProtocol.EIO;
                            success = false;
                            rsp[1] = null;
                        } else {
                            errorCode = BookieProtocol.EOK;
                            success = true;
                        }
                    } catch (InterruptedException ie) {
                        LOG.error("Interrupting fence read entry (lid:" + ledgerId
                                  + ", eid:" + entryId + ") :", ie);
                        errorCode = BookieProtocol.EIO;
                        success = false;
                        rsp[1] = null;
                    } catch (ExecutionException ee) {
                        LOG.error("Failed to fence read entry (lid:" + ledgerId
                                  + ", eid:" + entryId + ") :", ee);
                        errorCode = BookieProtocol.EIO;
                        success = false;
                        rsp[1] = null;
                    } catch (TimeoutException te) {
                        LOG.error("Timeout to fence read entry (lid:" + ledgerId
                                  + ", eid:" + entryId + ") :", te);
                        errorCode = BookieProtocol.EIO;
                        success = false;
                        rsp[1] = null;
                    }
                } else {
                    errorCode = BookieProtocol.EOK;
                    success = true;
                }
            } catch (Bookie.NoLedgerException e) {
                if (LOG.isTraceEnabled()) {
                    LOG.error("Error reading " + entryId + "@" + ledgerId, e);
                }
                errorCode = BookieProtocol.ENOLEDGER;
            } catch (Bookie.NoEntryException e) {
                if (LOG.isTraceEnabled()) {
                    LOG.error("Error reading " + entryId + "@" + ledgerId, e);
                }
                errorCode = BookieProtocol.ENOENTRY;
            } catch (IOException e) {
                if (LOG.isTraceEnabled()) {
                    LOG.error("Error reading " + entryId + "@" + ledgerId, e);
                }
                errorCode = BookieProtocol.EIO;
            } catch (BookieException e) {
                LOG.error("Unauthorized access to ledger " + ledgerId, e);
                errorCode = BookieProtocol.EUA;
            }
            rsp[0] = buildResponse(errorCode, h.getVersion(), h.getOpCode(), ledgerId, entryId);

            if (LOG.isTraceEnabled()) {
                LOG.trace("Read entry rc = " + errorCode + " for " + entryId + "@" + ledgerId);
            }
            if (rsp[1] == null) {
                // We haven't filled in entry data, so we have to send back
                // the ledger and entry ids here
                rsp[1] = ByteBuffer.allocate(16);
                rsp[1].putLong(ledgerId);
                rsp[1].putLong(entryId);
                rsp[1].flip();
            }
            if (LOG.isTraceEnabled()) {
                byte[] content = new byte[rsp[1].remaining()];
                rsp[1].duplicate().get(content);
                LOG.trace("Sending response for: {}, content: {}", entryId, Hex.encodeHexString(content));
            } else {
                LOG.debug("Sending response for: {}, length: {}", entryId, rsp[1].remaining());
            }
            src.sendResponse(rsp);
            break;
        default:
            src.sendResponse(buildResponse(BookieProtocol.EBADREQ, h.getVersion(), h.getOpCode(), ledgerId, entryId));
        }
        if (isStatsEnabled) {
            if (success) {
                // for add operations, we compute latency in writeComplete callbacks.
                if (statType != BKStats.STATS_ADD) {
                    long elapsedTime = MathUtils.now() - startTime;
                    bkStats.getOpStats(statType).updateLatency(elapsedTime);
                }
            } else {
                bkStats.getOpStats(statType).incrementFailedOps();
            }
        }
    }

    private ByteBuffer buildResponse(int errorCode, byte version, byte opCode, long ledgerId, long entryId) {
        ByteBuffer rsp = ByteBuffer.allocate(24);
        rsp.putInt(new PacketHeader(version, 
                                    opCode, (short)0).toInt());
        rsp.putInt(errorCode);
        rsp.putLong(ledgerId);
        rsp.putLong(entryId);

        rsp.flip();
        return rsp;
    }

    public void writeComplete(int rc, long ledgerId, long entryId, InetSocketAddress addr, Object ctx) {
        TimedCnxn tcnxn = (TimedCnxn) ctx;
        Cnxn src = tcnxn.cnxn;
        long startTime = tcnxn.time;
        ByteBuffer bb = ByteBuffer.allocate(24);
        bb.putInt(new PacketHeader(BookieProtocol.CURRENT_PROTOCOL_VERSION, 
                                   BookieProtocol.ADDENTRY, (short)0).toInt());
        bb.putInt(rc);
        bb.putLong(ledgerId);
        bb.putLong(entryId);
        bb.flip();
        if (LOG.isTraceEnabled()) {
            LOG.trace("Add entry rc = " + rc + " for " + entryId + "@" + ledgerId);
        }
        src.sendResponse(new ByteBuffer[] { bb });
        if (isStatsEnabled) {
            // compute the latency
            if (0 == rc) {
                // for add operations, we compute latency in writeComplete callbacks.
                long elapsedTime = MathUtils.now() - startTime;
                bkStats.getOpStats(BKStats.STATS_ADD).updateLatency(elapsedTime);
            } else {
                bkStats.getOpStats(BKStats.STATS_ADD).incrementFailedOps();                
            }
        }
    }

    /**
     * A cnxn wrapper for time
     */
    static class TimedCnxn {
        Cnxn cnxn;
        long time;

        public TimedCnxn(Cnxn cnxn, long startTime) {
            this.cnxn = cnxn;
            this.time = startTime;
        }
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/proto/BookieServerBean.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.bookkeeper.proto;

import java.net.InetAddress;
import java.net.UnknownHostException;

import org.apache.bookkeeper.util.StringUtils;
import org.apache.bookkeeper.bookie.Bookie;
import org.apache.bookkeeper.conf.ServerConfiguration;
import org.apache.bookkeeper.jmx.BKMBeanInfo;
import org.apache.bookkeeper.proto.BKStats;
import org.apache.bookkeeper.proto.BKStats.OpStats;
import org.apache.bookkeeper.proto.BKStats.OpStatData;

/**
 * Bookie Server Bean
 */
public class BookieServerBean implements BookieServerMXBean, BKMBeanInfo {

    protected final BookieServer bks;
    protected final ServerConfiguration conf;
    private final String name;

    public BookieServerBean(ServerConfiguration conf, BookieServer bks) {
        this.conf = conf;
        this.bks = bks;
        name = "BookieServer_" + conf.getBookiePort();
    }

    @Override
    public String getName() {
        return name;
    }

    @Override
    public boolean isHidden() {
        return false;
    }

    @Override
    public long getNumPacketsReceived() {
        return ServerStats.getInstance().getPacketsReceived();
    }

    @Override
    public long getNumPacketsSent() {
        return ServerStats.getInstance().getPacketsSent();
    }

    @Override
    public OpStatData getAddStats() {
        return bks.bkStats.getOpStats(BKStats.STATS_ADD).toOpStatData();
    }

    @Override
    public OpStatData getReadStats() {
        return bks.bkStats.getOpStats(BKStats.STATS_READ).toOpStatData();
    }

    @Override
    public String getServerState() {
        return bks.nioServerFactory.stats.getServerState();
    }

    @Override
    public String getServerPort() {
        try {
            return StringUtils.addrToString(Bookie.getBookieAddress(conf));
        } catch (UnknownHostException e) {
            return "localhost:" + conf.getBookiePort();
        }
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/proto/BookieServerMXBean.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.bookkeeper.proto;

import org.apache.bookkeeper.proto.BKStats.OpStatData;

/**
 * Bookie Server MBean
 */
public interface BookieServerMXBean {

    /**
     * @return packets received
     */
    public long getNumPacketsReceived();

    /**
     * @return packets sent
     */
    public long getNumPacketsSent();

    /**
     * @return add stats
     */
    public OpStatData getAddStats();

    /**
     * @return read stats
     */
    public OpStatData getReadStats();

    /**
     * @return server state
     */
    public String getServerState();

    /**
     * @return server port
     */
    public String getServerPort();

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/proto/BookkeeperInternalCallbacks.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.proto;

import java.net.InetSocketAddress;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.atomic.AtomicInteger;

import org.apache.zookeeper.AsyncCallback;
import org.jboss.netty.buffer.ChannelBuffer;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Declaration of a callback interfaces used in bookkeeper client library but
 * not exposed to the client application.
 */

public class BookkeeperInternalCallbacks {

    static final Logger LOG = LoggerFactory.getLogger(BookkeeperInternalCallbacks.class);

    /**
     * Callback for calls from BookieClient objects. Such calls are for replies
     * of write operations (operations to add an entry to a ledger).
     *
     */

    public interface WriteCallback {
        void writeComplete(int rc, long ledgerId, long entryId, InetSocketAddress addr, Object ctx);
    }

    public interface GenericCallback<T> {
        void operationComplete(int rc, T result);
    }

    /**
     * Declaration of a callback implementation for calls from BookieClient objects.
     * Such calls are for replies of read operations (operations to read an entry
     * from a ledger).
     *
     */

    public interface ReadEntryCallback {
        void readEntryComplete(int rc, long ledgerId, long entryId, ChannelBuffer buffer, Object ctx);
    }

    /**
     * This is a multi callback object that waits for all of
     * the multiple async operations to complete. If any fail, then we invoke
     * the final callback with a provided failureRc
     */
    public static class MultiCallback implements AsyncCallback.VoidCallback {
        // Number of expected callbacks
        final int expected;
        final int failureRc;
        final int successRc;
        // Final callback and the corresponding context to invoke
        final AsyncCallback.VoidCallback cb;
        final Object context;
        // This keeps track of how many operations have completed
        final AtomicInteger done = new AtomicInteger();
        // List of the exceptions from operations that completed unsuccessfully
        final LinkedBlockingQueue<Integer> exceptions = new LinkedBlockingQueue<Integer>();

        public MultiCallback(int expected, AsyncCallback.VoidCallback cb, Object context, int successRc, int failureRc) {
            this.expected = expected;
            this.cb = cb;
            this.context = context;
            this.failureRc = failureRc;
            this.successRc = successRc;
            if (expected == 0) {
                cb.processResult(successRc, null, context);
            }
        }

        private void tick() {
            if (done.incrementAndGet() == expected) {
                if (exceptions.isEmpty()) {
                    cb.processResult(successRc, null, context);
                } else {
                    cb.processResult(failureRc, null, context);
                }
            }
        }

        @Override
        public void processResult(int rc, String path, Object ctx) {
            if (rc != successRc) {
                LOG.error("Error in multi callback : " + rc);
                exceptions.add(rc);
            }
            tick();
        }

    }

    /**
     * Processor to process a specific element
     */
    public static interface Processor<T> {
        /**
         * Process a specific element
         *
         * @param data
         *          data to process
         * @param iterationCallback
         *          Callback to invoke when process has been done.
         */
        public void process(T data, AsyncCallback.VoidCallback cb);
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/proto/DataFormats.java,false,"// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: src/main/proto/DataFormats.proto

package org.apache.bookkeeper.proto;

public final class DataFormats {
  private DataFormats() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
  }
  public interface LedgerMetadataFormatOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // required int32 quorumSize = 1;
    boolean hasQuorumSize();
    int getQuorumSize();
    
    // required int32 ensembleSize = 2;
    boolean hasEnsembleSize();
    int getEnsembleSize();
    
    // required int64 length = 3;
    boolean hasLength();
    long getLength();
    
    // optional int64 lastEntryId = 4;
    boolean hasLastEntryId();
    long getLastEntryId();
    
    // required .LedgerMetadataFormat.State state = 5 [default = OPEN];
    boolean hasState();
    org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.State getState();
    
    // repeated .LedgerMetadataFormat.Segment segment = 6;
    java.util.List<org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment> 
        getSegmentList();
    org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment getSegment(int index);
    int getSegmentCount();
    java.util.List<? extends org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.SegmentOrBuilder> 
        getSegmentOrBuilderList();
    org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.SegmentOrBuilder getSegmentOrBuilder(
        int index);
    
    // optional .LedgerMetadataFormat.DigestType digestType = 7;
    boolean hasDigestType();
    org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.DigestType getDigestType();
    
    // optional bytes password = 8;
    boolean hasPassword();
    com.google.protobuf.ByteString getPassword();
    
    // optional int32 ackQuorumSize = 9;
    boolean hasAckQuorumSize();
    int getAckQuorumSize();
  }
  public static final class LedgerMetadataFormat extends
      com.google.protobuf.GeneratedMessage
      implements LedgerMetadataFormatOrBuilder {
    // Use LedgerMetadataFormat.newBuilder() to construct.
    private LedgerMetadataFormat(Builder builder) {
      super(builder);
    }
    private LedgerMetadataFormat(boolean noInit) {}
    
    private static final LedgerMetadataFormat defaultInstance;
    public static LedgerMetadataFormat getDefaultInstance() {
      return defaultInstance;
    }
    
    public LedgerMetadataFormat getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.bookkeeper.proto.DataFormats.internal_static_LedgerMetadataFormat_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.bookkeeper.proto.DataFormats.internal_static_LedgerMetadataFormat_fieldAccessorTable;
    }
    
    public enum State
        implements com.google.protobuf.ProtocolMessageEnum {
      OPEN(0, 1),
      IN_RECOVERY(1, 2),
      CLOSED(2, 3),
      ;
      
      public static final int OPEN_VALUE = 1;
      public static final int IN_RECOVERY_VALUE = 2;
      public static final int CLOSED_VALUE = 3;
      
      
      public final int getNumber() { return value; }
      
      public static State valueOf(int value) {
        switch (value) {
          case 1: return OPEN;
          case 2: return IN_RECOVERY;
          case 3: return CLOSED;
          default: return null;
        }
      }
      
      public static com.google.protobuf.Internal.EnumLiteMap<State>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static com.google.protobuf.Internal.EnumLiteMap<State>
          internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<State>() {
              public State findValueByNumber(int number) {
                return State.valueOf(number);
              }
            };
      
      public final com.google.protobuf.Descriptors.EnumValueDescriptor
          getValueDescriptor() {
        return getDescriptor().getValues().get(index);
      }
      public final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptorForType() {
        return getDescriptor();
      }
      public static final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptor() {
        return org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.getDescriptor().getEnumTypes().get(0);
      }
      
      private static final State[] VALUES = {
        OPEN, IN_RECOVERY, CLOSED, 
      };
      
      public static State valueOf(
          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
        if (desc.getType() != getDescriptor()) {
          throw new java.lang.IllegalArgumentException(
            "EnumValueDescriptor is not for this type.");
        }
        return VALUES[desc.getIndex()];
      }
      
      private final int index;
      private final int value;
      
      private State(int index, int value) {
        this.index = index;
        this.value = value;
      }
      
      // @@protoc_insertion_point(enum_scope:LedgerMetadataFormat.State)
    }
    
    public enum DigestType
        implements com.google.protobuf.ProtocolMessageEnum {
      CRC32(0, 1),
      HMAC(1, 2),
      ;
      
      public static final int CRC32_VALUE = 1;
      public static final int HMAC_VALUE = 2;
      
      
      public final int getNumber() { return value; }
      
      public static DigestType valueOf(int value) {
        switch (value) {
          case 1: return CRC32;
          case 2: return HMAC;
          default: return null;
        }
      }
      
      public static com.google.protobuf.Internal.EnumLiteMap<DigestType>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static com.google.protobuf.Internal.EnumLiteMap<DigestType>
          internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<DigestType>() {
              public DigestType findValueByNumber(int number) {
                return DigestType.valueOf(number);
              }
            };
      
      public final com.google.protobuf.Descriptors.EnumValueDescriptor
          getValueDescriptor() {
        return getDescriptor().getValues().get(index);
      }
      public final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptorForType() {
        return getDescriptor();
      }
      public static final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptor() {
        return org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.getDescriptor().getEnumTypes().get(1);
      }
      
      private static final DigestType[] VALUES = {
        CRC32, HMAC, 
      };
      
      public static DigestType valueOf(
          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
        if (desc.getType() != getDescriptor()) {
          throw new java.lang.IllegalArgumentException(
            "EnumValueDescriptor is not for this type.");
        }
        return VALUES[desc.getIndex()];
      }
      
      private final int index;
      private final int value;
      
      private DigestType(int index, int value) {
        this.index = index;
        this.value = value;
      }
      
      // @@protoc_insertion_point(enum_scope:LedgerMetadataFormat.DigestType)
    }
    
    public interface SegmentOrBuilder
        extends com.google.protobuf.MessageOrBuilder {
      
      // repeated string ensembleMember = 1;
      java.util.List<String> getEnsembleMemberList();
      int getEnsembleMemberCount();
      String getEnsembleMember(int index);
      
      // required int64 firstEntryId = 2;
      boolean hasFirstEntryId();
      long getFirstEntryId();
    }
    public static final class Segment extends
        com.google.protobuf.GeneratedMessage
        implements SegmentOrBuilder {
      // Use Segment.newBuilder() to construct.
      private Segment(Builder builder) {
        super(builder);
      }
      private Segment(boolean noInit) {}
      
      private static final Segment defaultInstance;
      public static Segment getDefaultInstance() {
        return defaultInstance;
      }
      
      public Segment getDefaultInstanceForType() {
        return defaultInstance;
      }
      
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.bookkeeper.proto.DataFormats.internal_static_LedgerMetadataFormat_Segment_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.bookkeeper.proto.DataFormats.internal_static_LedgerMetadataFormat_Segment_fieldAccessorTable;
      }
      
      private int bitField0_;
      // repeated string ensembleMember = 1;
      public static final int ENSEMBLEMEMBER_FIELD_NUMBER = 1;
      private com.google.protobuf.LazyStringList ensembleMember_;
      public java.util.List<String>
          getEnsembleMemberList() {
        return ensembleMember_;
      }
      public int getEnsembleMemberCount() {
        return ensembleMember_.size();
      }
      public String getEnsembleMember(int index) {
        return ensembleMember_.get(index);
      }
      
      // required int64 firstEntryId = 2;
      public static final int FIRSTENTRYID_FIELD_NUMBER = 2;
      private long firstEntryId_;
      public boolean hasFirstEntryId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public long getFirstEntryId() {
        return firstEntryId_;
      }
      
      private void initFields() {
        ensembleMember_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        firstEntryId_ = 0L;
      }
      private byte memoizedIsInitialized = -1;
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized != -1) return isInitialized == 1;
        
        if (!hasFirstEntryId()) {
          memoizedIsInitialized = 0;
          return false;
        }
        memoizedIsInitialized = 1;
        return true;
      }
      
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        for (int i = 0; i < ensembleMember_.size(); i++) {
          output.writeBytes(1, ensembleMember_.getByteString(i));
        }
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          output.writeInt64(2, firstEntryId_);
        }
        getUnknownFields().writeTo(output);
      }
      
      private int memoizedSerializedSize = -1;
      public int getSerializedSize() {
        int size = memoizedSerializedSize;
        if (size != -1) return size;
      
        size = 0;
        {
          int dataSize = 0;
          for (int i = 0; i < ensembleMember_.size(); i++) {
            dataSize += com.google.protobuf.CodedOutputStream
              .computeBytesSizeNoTag(ensembleMember_.getByteString(i));
          }
          size += dataSize;
          size += 1 * getEnsembleMemberList().size();
        }
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          size += com.google.protobuf.CodedOutputStream
            .computeInt64Size(2, firstEntryId_);
        }
        size += getUnknownFields().getSerializedSize();
        memoizedSerializedSize = size;
        return size;
      }
      
      private static final long serialVersionUID = 0L;
      @java.lang.Override
      protected java.lang.Object writeReplace()
          throws java.io.ObjectStreamException {
        return super.writeReplace();
      }
      
      public static org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return newBuilder().mergeFrom(data).buildParsed();
      }
      public static org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return newBuilder().mergeFrom(data, extensionRegistry)
                 .buildParsed();
      }
      public static org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return newBuilder().mergeFrom(data).buildParsed();
      }
      public static org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return newBuilder().mergeFrom(data, extensionRegistry)
                 .buildParsed();
      }
      public static org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return newBuilder().mergeFrom(input).buildParsed();
      }
      public static org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return newBuilder().mergeFrom(input, extensionRegistry)
                 .buildParsed();
      }
      public static org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        Builder builder = newBuilder();
        if (builder.mergeDelimitedFrom(input)) {
          return builder.buildParsed();
        } else {
          return null;
        }
      }
      public static org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        Builder builder = newBuilder();
        if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
          return builder.buildParsed();
        } else {
          return null;
        }
      }
      public static org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return newBuilder().mergeFrom(input).buildParsed();
      }
      public static org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return newBuilder().mergeFrom(input, extensionRegistry)
                 .buildParsed();
      }
      
      public static Builder newBuilder() { return Builder.create(); }
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder(org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment prototype) {
        return newBuilder().mergeFrom(prototype);
      }
      public Builder toBuilder() { return newBuilder(this); }
      
      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      public static final class Builder extends
          com.google.protobuf.GeneratedMessage.Builder<Builder>
         implements org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.SegmentOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return org.apache.bookkeeper.proto.DataFormats.internal_static_LedgerMetadataFormat_Segment_descriptor;
        }
        
        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return org.apache.bookkeeper.proto.DataFormats.internal_static_LedgerMetadataFormat_Segment_fieldAccessorTable;
        }
        
        // Construct using org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }
        
        private Builder(BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          }
        }
        private static Builder create() {
          return new Builder();
        }
        
        public Builder clear() {
          super.clear();
          ensembleMember_ = com.google.protobuf.LazyStringArrayList.EMPTY;
          bitField0_ = (bitField0_ & ~0x00000001);
          firstEntryId_ = 0L;
          bitField0_ = (bitField0_ & ~0x00000002);
          return this;
        }
        
        public Builder clone() {
          return create().mergeFrom(buildPartial());
        }
        
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment.getDescriptor();
        }
        
        public org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment getDefaultInstanceForType() {
          return org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment.getDefaultInstance();
        }
        
        public org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment build() {
          org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }
        
        private org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment buildParsed()
            throws com.google.protobuf.InvalidProtocolBufferException {
          org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(
              result).asInvalidProtocolBufferException();
          }
          return result;
        }
        
        public org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment buildPartial() {
          org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment result = new org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment(this);
          int from_bitField0_ = bitField0_;
          int to_bitField0_ = 0;
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            ensembleMember_ = new com.google.protobuf.UnmodifiableLazyStringList(
                ensembleMember_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.ensembleMember_ = ensembleMember_;
          if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
            to_bitField0_ |= 0x00000001;
          }
          result.firstEntryId_ = firstEntryId_;
          result.bitField0_ = to_bitField0_;
          onBuilt();
          return result;
        }
        
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment) {
            return mergeFrom((org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }
        
        public Builder mergeFrom(org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment other) {
          if (other == org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment.getDefaultInstance()) return this;
          if (!other.ensembleMember_.isEmpty()) {
            if (ensembleMember_.isEmpty()) {
              ensembleMember_ = other.ensembleMember_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureEnsembleMemberIsMutable();
              ensembleMember_.addAll(other.ensembleMember_);
            }
            onChanged();
          }
          if (other.hasFirstEntryId()) {
            setFirstEntryId(other.getFirstEntryId());
          }
          this.mergeUnknownFields(other.getUnknownFields());
          return this;
        }
        
        public final boolean isInitialized() {
          if (!hasFirstEntryId()) {
            
            return false;
          }
          return true;
        }
        
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder(
              this.getUnknownFields());
          while (true) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              default: {
                if (!parseUnknownField(input, unknownFields,
                                       extensionRegistry, tag)) {
                  this.setUnknownFields(unknownFields.build());
                  onChanged();
                  return this;
                }
                break;
              }
              case 10: {
                ensureEnsembleMemberIsMutable();
                ensembleMember_.add(input.readBytes());
                break;
              }
              case 16: {
                bitField0_ |= 0x00000002;
                firstEntryId_ = input.readInt64();
                break;
              }
            }
          }
        }
        
        private int bitField0_;
        
        // repeated string ensembleMember = 1;
        private com.google.protobuf.LazyStringList ensembleMember_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        private void ensureEnsembleMemberIsMutable() {
          if (!((bitField0_ & 0x00000001) == 0x00000001)) {
            ensembleMember_ = new com.google.protobuf.LazyStringArrayList(ensembleMember_);
            bitField0_ |= 0x00000001;
           }
        }
        public java.util.List<String>
            getEnsembleMemberList() {
          return java.util.Collections.unmodifiableList(ensembleMember_);
        }
        public int getEnsembleMemberCount() {
          return ensembleMember_.size();
        }
        public String getEnsembleMember(int index) {
          return ensembleMember_.get(index);
        }
        public Builder setEnsembleMember(
            int index, String value) {
          if (value == null) {
    throw new NullPointerException();
  }
  ensureEnsembleMemberIsMutable();
          ensembleMember_.set(index, value);
          onChanged();
          return this;
        }
        public Builder addEnsembleMember(String value) {
          if (value == null) {
    throw new NullPointerException();
  }
  ensureEnsembleMemberIsMutable();
          ensembleMember_.add(value);
          onChanged();
          return this;
        }
        public Builder addAllEnsembleMember(
            java.lang.Iterable<String> values) {
          ensureEnsembleMemberIsMutable();
          super.addAll(values, ensembleMember_);
          onChanged();
          return this;
        }
        public Builder clearEnsembleMember() {
          ensembleMember_ = com.google.protobuf.LazyStringArrayList.EMPTY;
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
          return this;
        }
        void addEnsembleMember(com.google.protobuf.ByteString value) {
          ensureEnsembleMemberIsMutable();
          ensembleMember_.add(value);
          onChanged();
        }
        
        // required int64 firstEntryId = 2;
        private long firstEntryId_ ;
        public boolean hasFirstEntryId() {
          return ((bitField0_ & 0x00000002) == 0x00000002);
        }
        public long getFirstEntryId() {
          return firstEntryId_;
        }
        public Builder setFirstEntryId(long value) {
          bitField0_ |= 0x00000002;
          firstEntryId_ = value;
          onChanged();
          return this;
        }
        public Builder clearFirstEntryId() {
          bitField0_ = (bitField0_ & ~0x00000002);
          firstEntryId_ = 0L;
          onChanged();
          return this;
        }
        
        // @@protoc_insertion_point(builder_scope:LedgerMetadataFormat.Segment)
      }
      
      static {
        defaultInstance = new Segment(true);
        defaultInstance.initFields();
      }
      
      // @@protoc_insertion_point(class_scope:LedgerMetadataFormat.Segment)
    }
    
    private int bitField0_;
    // required int32 quorumSize = 1;
    public static final int QUORUMSIZE_FIELD_NUMBER = 1;
    private int quorumSize_;
    public boolean hasQuorumSize() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    public int getQuorumSize() {
      return quorumSize_;
    }
    
    // required int32 ensembleSize = 2;
    public static final int ENSEMBLESIZE_FIELD_NUMBER = 2;
    private int ensembleSize_;
    public boolean hasEnsembleSize() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    public int getEnsembleSize() {
      return ensembleSize_;
    }
    
    // required int64 length = 3;
    public static final int LENGTH_FIELD_NUMBER = 3;
    private long length_;
    public boolean hasLength() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    public long getLength() {
      return length_;
    }
    
    // optional int64 lastEntryId = 4;
    public static final int LASTENTRYID_FIELD_NUMBER = 4;
    private long lastEntryId_;
    public boolean hasLastEntryId() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    public long getLastEntryId() {
      return lastEntryId_;
    }
    
    // required .LedgerMetadataFormat.State state = 5 [default = OPEN];
    public static final int STATE_FIELD_NUMBER = 5;
    private org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.State state_;
    public boolean hasState() {
      return ((bitField0_ & 0x00000010) == 0x00000010);
    }
    public org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.State getState() {
      return state_;
    }
    
    // repeated .LedgerMetadataFormat.Segment segment = 6;
    public static final int SEGMENT_FIELD_NUMBER = 6;
    private java.util.List<org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment> segment_;
    public java.util.List<org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment> getSegmentList() {
      return segment_;
    }
    public java.util.List<? extends org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.SegmentOrBuilder> 
        getSegmentOrBuilderList() {
      return segment_;
    }
    public int getSegmentCount() {
      return segment_.size();
    }
    public org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment getSegment(int index) {
      return segment_.get(index);
    }
    public org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.SegmentOrBuilder getSegmentOrBuilder(
        int index) {
      return segment_.get(index);
    }
    
    // optional .LedgerMetadataFormat.DigestType digestType = 7;
    public static final int DIGESTTYPE_FIELD_NUMBER = 7;
    private org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.DigestType digestType_;
    public boolean hasDigestType() {
      return ((bitField0_ & 0x00000020) == 0x00000020);
    }
    public org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.DigestType getDigestType() {
      return digestType_;
    }
    
    // optional bytes password = 8;
    public static final int PASSWORD_FIELD_NUMBER = 8;
    private com.google.protobuf.ByteString password_;
    public boolean hasPassword() {
      return ((bitField0_ & 0x00000040) == 0x00000040);
    }
    public com.google.protobuf.ByteString getPassword() {
      return password_;
    }
    
    // optional int32 ackQuorumSize = 9;
    public static final int ACKQUORUMSIZE_FIELD_NUMBER = 9;
    private int ackQuorumSize_;
    public boolean hasAckQuorumSize() {
      return ((bitField0_ & 0x00000080) == 0x00000080);
    }
    public int getAckQuorumSize() {
      return ackQuorumSize_;
    }
    
    private void initFields() {
      quorumSize_ = 0;
      ensembleSize_ = 0;
      length_ = 0L;
      lastEntryId_ = 0L;
      state_ = org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.State.OPEN;
      segment_ = java.util.Collections.emptyList();
      digestType_ = org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.DigestType.CRC32;
      password_ = com.google.protobuf.ByteString.EMPTY;
      ackQuorumSize_ = 0;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      if (!hasQuorumSize()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasEnsembleSize()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasLength()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasState()) {
        memoizedIsInitialized = 0;
        return false;
      }
      for (int i = 0; i < getSegmentCount(); i++) {
        if (!getSegment(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeInt32(1, quorumSize_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeInt32(2, ensembleSize_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeInt64(3, length_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeInt64(4, lastEntryId_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        output.writeEnum(5, state_.getNumber());
      }
      for (int i = 0; i < segment_.size(); i++) {
        output.writeMessage(6, segment_.get(i));
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        output.writeEnum(7, digestType_.getNumber());
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        output.writeBytes(8, password_);
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        output.writeInt32(9, ackQuorumSize_);
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(1, quorumSize_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(2, ensembleSize_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(3, length_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(4, lastEntryId_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(5, state_.getNumber());
      }
      for (int i = 0; i < segment_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(6, segment_.get(i));
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(7, digestType_.getNumber());
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(8, password_);
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(9, ackQuorumSize_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormatOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.bookkeeper.proto.DataFormats.internal_static_LedgerMetadataFormat_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.bookkeeper.proto.DataFormats.internal_static_LedgerMetadataFormat_fieldAccessorTable;
      }
      
      // Construct using org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getSegmentFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        quorumSize_ = 0;
        bitField0_ = (bitField0_ & ~0x00000001);
        ensembleSize_ = 0;
        bitField0_ = (bitField0_ & ~0x00000002);
        length_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        lastEntryId_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000008);
        state_ = org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.State.OPEN;
        bitField0_ = (bitField0_ & ~0x00000010);
        if (segmentBuilder_ == null) {
          segment_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
        } else {
          segmentBuilder_.clear();
        }
        digestType_ = org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.DigestType.CRC32;
        bitField0_ = (bitField0_ & ~0x00000040);
        password_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000080);
        ackQuorumSize_ = 0;
        bitField0_ = (bitField0_ & ~0x00000100);
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.getDescriptor();
      }
      
      public org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat getDefaultInstanceForType() {
        return org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.getDefaultInstance();
      }
      
      public org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat build() {
        org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat buildPartial() {
        org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat result = new org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.quorumSize_ = quorumSize_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.ensembleSize_ = ensembleSize_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.length_ = length_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        result.lastEntryId_ = lastEntryId_;
        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
          to_bitField0_ |= 0x00000010;
        }
        result.state_ = state_;
        if (segmentBuilder_ == null) {
          if (((bitField0_ & 0x00000020) == 0x00000020)) {
            segment_ = java.util.Collections.unmodifiableList(segment_);
            bitField0_ = (bitField0_ & ~0x00000020);
          }
          result.segment_ = segment_;
        } else {
          result.segment_ = segmentBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
          to_bitField0_ |= 0x00000020;
        }
        result.digestType_ = digestType_;
        if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
          to_bitField0_ |= 0x00000040;
        }
        result.password_ = password_;
        if (((from_bitField0_ & 0x00000100) == 0x00000100)) {
          to_bitField0_ |= 0x00000080;
        }
        result.ackQuorumSize_ = ackQuorumSize_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat) {
          return mergeFrom((org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat other) {
        if (other == org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.getDefaultInstance()) return this;
        if (other.hasQuorumSize()) {
          setQuorumSize(other.getQuorumSize());
        }
        if (other.hasEnsembleSize()) {
          setEnsembleSize(other.getEnsembleSize());
        }
        if (other.hasLength()) {
          setLength(other.getLength());
        }
        if (other.hasLastEntryId()) {
          setLastEntryId(other.getLastEntryId());
        }
        if (other.hasState()) {
          setState(other.getState());
        }
        if (segmentBuilder_ == null) {
          if (!other.segment_.isEmpty()) {
            if (segment_.isEmpty()) {
              segment_ = other.segment_;
              bitField0_ = (bitField0_ & ~0x00000020);
            } else {
              ensureSegmentIsMutable();
              segment_.addAll(other.segment_);
            }
            onChanged();
          }
        } else {
          if (!other.segment_.isEmpty()) {
            if (segmentBuilder_.isEmpty()) {
              segmentBuilder_.dispose();
              segmentBuilder_ = null;
              segment_ = other.segment_;
              bitField0_ = (bitField0_ & ~0x00000020);
              segmentBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getSegmentFieldBuilder() : null;
            } else {
              segmentBuilder_.addAllMessages(other.segment_);
            }
          }
        }
        if (other.hasDigestType()) {
          setDigestType(other.getDigestType());
        }
        if (other.hasPassword()) {
          setPassword(other.getPassword());
        }
        if (other.hasAckQuorumSize()) {
          setAckQuorumSize(other.getAckQuorumSize());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        if (!hasQuorumSize()) {
          
          return false;
        }
        if (!hasEnsembleSize()) {
          
          return false;
        }
        if (!hasLength()) {
          
          return false;
        }
        if (!hasState()) {
          
          return false;
        }
        for (int i = 0; i < getSegmentCount(); i++) {
          if (!getSegment(i).isInitialized()) {
            
            return false;
          }
        }
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              quorumSize_ = input.readInt32();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              ensembleSize_ = input.readInt32();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              length_ = input.readInt64();
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              lastEntryId_ = input.readInt64();
              break;
            }
            case 40: {
              int rawValue = input.readEnum();
              org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.State value = org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.State.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(5, rawValue);
              } else {
                bitField0_ |= 0x00000010;
                state_ = value;
              }
              break;
            }
            case 50: {
              org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment.Builder subBuilder = org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment.newBuilder();
              input.readMessage(subBuilder, extensionRegistry);
              addSegment(subBuilder.buildPartial());
              break;
            }
            case 56: {
              int rawValue = input.readEnum();
              org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.DigestType value = org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.DigestType.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(7, rawValue);
              } else {
                bitField0_ |= 0x00000040;
                digestType_ = value;
              }
              break;
            }
            case 66: {
              bitField0_ |= 0x00000080;
              password_ = input.readBytes();
              break;
            }
            case 72: {
              bitField0_ |= 0x00000100;
              ackQuorumSize_ = input.readInt32();
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // required int32 quorumSize = 1;
      private int quorumSize_ ;
      public boolean hasQuorumSize() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public int getQuorumSize() {
        return quorumSize_;
      }
      public Builder setQuorumSize(int value) {
        bitField0_ |= 0x00000001;
        quorumSize_ = value;
        onChanged();
        return this;
      }
      public Builder clearQuorumSize() {
        bitField0_ = (bitField0_ & ~0x00000001);
        quorumSize_ = 0;
        onChanged();
        return this;
      }
      
      // required int32 ensembleSize = 2;
      private int ensembleSize_ ;
      public boolean hasEnsembleSize() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      public int getEnsembleSize() {
        return ensembleSize_;
      }
      public Builder setEnsembleSize(int value) {
        bitField0_ |= 0x00000002;
        ensembleSize_ = value;
        onChanged();
        return this;
      }
      public Builder clearEnsembleSize() {
        bitField0_ = (bitField0_ & ~0x00000002);
        ensembleSize_ = 0;
        onChanged();
        return this;
      }
      
      // required int64 length = 3;
      private long length_ ;
      public boolean hasLength() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      public long getLength() {
        return length_;
      }
      public Builder setLength(long value) {
        bitField0_ |= 0x00000004;
        length_ = value;
        onChanged();
        return this;
      }
      public Builder clearLength() {
        bitField0_ = (bitField0_ & ~0x00000004);
        length_ = 0L;
        onChanged();
        return this;
      }
      
      // optional int64 lastEntryId = 4;
      private long lastEntryId_ ;
      public boolean hasLastEntryId() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      public long getLastEntryId() {
        return lastEntryId_;
      }
      public Builder setLastEntryId(long value) {
        bitField0_ |= 0x00000008;
        lastEntryId_ = value;
        onChanged();
        return this;
      }
      public Builder clearLastEntryId() {
        bitField0_ = (bitField0_ & ~0x00000008);
        lastEntryId_ = 0L;
        onChanged();
        return this;
      }
      
      // required .LedgerMetadataFormat.State state = 5 [default = OPEN];
      private org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.State state_ = org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.State.OPEN;
      public boolean hasState() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      public org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.State getState() {
        return state_;
      }
      public Builder setState(org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.State value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000010;
        state_ = value;
        onChanged();
        return this;
      }
      public Builder clearState() {
        bitField0_ = (bitField0_ & ~0x00000010);
        state_ = org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.State.OPEN;
        onChanged();
        return this;
      }
      
      // repeated .LedgerMetadataFormat.Segment segment = 6;
      private java.util.List<org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment> segment_ =
        java.util.Collections.emptyList();
      private void ensureSegmentIsMutable() {
        if (!((bitField0_ & 0x00000020) == 0x00000020)) {
          segment_ = new java.util.ArrayList<org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment>(segment_);
          bitField0_ |= 0x00000020;
         }
      }
      
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment, org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment.Builder, org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.SegmentOrBuilder> segmentBuilder_;
      
      public java.util.List<org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment> getSegmentList() {
        if (segmentBuilder_ == null) {
          return java.util.Collections.unmodifiableList(segment_);
        } else {
          return segmentBuilder_.getMessageList();
        }
      }
      public int getSegmentCount() {
        if (segmentBuilder_ == null) {
          return segment_.size();
        } else {
          return segmentBuilder_.getCount();
        }
      }
      public org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment getSegment(int index) {
        if (segmentBuilder_ == null) {
          return segment_.get(index);
        } else {
          return segmentBuilder_.getMessage(index);
        }
      }
      public Builder setSegment(
          int index, org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment value) {
        if (segmentBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSegmentIsMutable();
          segment_.set(index, value);
          onChanged();
        } else {
          segmentBuilder_.setMessage(index, value);
        }
        return this;
      }
      public Builder setSegment(
          int index, org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment.Builder builderForValue) {
        if (segmentBuilder_ == null) {
          ensureSegmentIsMutable();
          segment_.set(index, builderForValue.build());
          onChanged();
        } else {
          segmentBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      public Builder addSegment(org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment value) {
        if (segmentBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSegmentIsMutable();
          segment_.add(value);
          onChanged();
        } else {
          segmentBuilder_.addMessage(value);
        }
        return this;
      }
      public Builder addSegment(
          int index, org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment value) {
        if (segmentBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureSegmentIsMutable();
          segment_.add(index, value);
          onChanged();
        } else {
          segmentBuilder_.addMessage(index, value);
        }
        return this;
      }
      public Builder addSegment(
          org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment.Builder builderForValue) {
        if (segmentBuilder_ == null) {
          ensureSegmentIsMutable();
          segment_.add(builderForValue.build());
          onChanged();
        } else {
          segmentBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      public Builder addSegment(
          int index, org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment.Builder builderForValue) {
        if (segmentBuilder_ == null) {
          ensureSegmentIsMutable();
          segment_.add(index, builderForValue.build());
          onChanged();
        } else {
          segmentBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      public Builder addAllSegment(
          java.lang.Iterable<? extends org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment> values) {
        if (segmentBuilder_ == null) {
          ensureSegmentIsMutable();
          super.addAll(values, segment_);
          onChanged();
        } else {
          segmentBuilder_.addAllMessages(values);
        }
        return this;
      }
      public Builder clearSegment() {
        if (segmentBuilder_ == null) {
          segment_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000020);
          onChanged();
        } else {
          segmentBuilder_.clear();
        }
        return this;
      }
      public Builder removeSegment(int index) {
        if (segmentBuilder_ == null) {
          ensureSegmentIsMutable();
          segment_.remove(index);
          onChanged();
        } else {
          segmentBuilder_.remove(index);
        }
        return this;
      }
      public org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment.Builder getSegmentBuilder(
          int index) {
        return getSegmentFieldBuilder().getBuilder(index);
      }
      public org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.SegmentOrBuilder getSegmentOrBuilder(
          int index) {
        if (segmentBuilder_ == null) {
          return segment_.get(index);  } else {
          return segmentBuilder_.getMessageOrBuilder(index);
        }
      }
      public java.util.List<? extends org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.SegmentOrBuilder> 
           getSegmentOrBuilderList() {
        if (segmentBuilder_ != null) {
          return segmentBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(segment_);
        }
      }
      public org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment.Builder addSegmentBuilder() {
        return getSegmentFieldBuilder().addBuilder(
            org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment.getDefaultInstance());
      }
      public org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment.Builder addSegmentBuilder(
          int index) {
        return getSegmentFieldBuilder().addBuilder(
            index, org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment.getDefaultInstance());
      }
      public java.util.List<org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment.Builder> 
           getSegmentBuilderList() {
        return getSegmentFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment, org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment.Builder, org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.SegmentOrBuilder> 
          getSegmentFieldBuilder() {
        if (segmentBuilder_ == null) {
          segmentBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment, org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment.Builder, org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.SegmentOrBuilder>(
                  segment_,
                  ((bitField0_ & 0x00000020) == 0x00000020),
                  getParentForChildren(),
                  isClean());
          segment_ = null;
        }
        return segmentBuilder_;
      }
      
      // optional .LedgerMetadataFormat.DigestType digestType = 7;
      private org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.DigestType digestType_ = org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.DigestType.CRC32;
      public boolean hasDigestType() {
        return ((bitField0_ & 0x00000040) == 0x00000040);
      }
      public org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.DigestType getDigestType() {
        return digestType_;
      }
      public Builder setDigestType(org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.DigestType value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000040;
        digestType_ = value;
        onChanged();
        return this;
      }
      public Builder clearDigestType() {
        bitField0_ = (bitField0_ & ~0x00000040);
        digestType_ = org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.DigestType.CRC32;
        onChanged();
        return this;
      }
      
      // optional bytes password = 8;
      private com.google.protobuf.ByteString password_ = com.google.protobuf.ByteString.EMPTY;
      public boolean hasPassword() {
        return ((bitField0_ & 0x00000080) == 0x00000080);
      }
      public com.google.protobuf.ByteString getPassword() {
        return password_;
      }
      public Builder setPassword(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000080;
        password_ = value;
        onChanged();
        return this;
      }
      public Builder clearPassword() {
        bitField0_ = (bitField0_ & ~0x00000080);
        password_ = getDefaultInstance().getPassword();
        onChanged();
        return this;
      }
      
      // optional int32 ackQuorumSize = 9;
      private int ackQuorumSize_ ;
      public boolean hasAckQuorumSize() {
        return ((bitField0_ & 0x00000100) == 0x00000100);
      }
      public int getAckQuorumSize() {
        return ackQuorumSize_;
      }
      public Builder setAckQuorumSize(int value) {
        bitField0_ |= 0x00000100;
        ackQuorumSize_ = value;
        onChanged();
        return this;
      }
      public Builder clearAckQuorumSize() {
        bitField0_ = (bitField0_ & ~0x00000100);
        ackQuorumSize_ = 0;
        onChanged();
        return this;
      }
      
      // @@protoc_insertion_point(builder_scope:LedgerMetadataFormat)
    }
    
    static {
      defaultInstance = new LedgerMetadataFormat(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:LedgerMetadataFormat)
  }
  
  public interface LedgerRereplicationLayoutFormatOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // required string type = 1;
    boolean hasType();
    String getType();
    
    // required int32 version = 2;
    boolean hasVersion();
    int getVersion();
  }
  public static final class LedgerRereplicationLayoutFormat extends
      com.google.protobuf.GeneratedMessage
      implements LedgerRereplicationLayoutFormatOrBuilder {
    // Use LedgerRereplicationLayoutFormat.newBuilder() to construct.
    private LedgerRereplicationLayoutFormat(Builder builder) {
      super(builder);
    }
    private LedgerRereplicationLayoutFormat(boolean noInit) {}
    
    private static final LedgerRereplicationLayoutFormat defaultInstance;
    public static LedgerRereplicationLayoutFormat getDefaultInstance() {
      return defaultInstance;
    }
    
    public LedgerRereplicationLayoutFormat getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.bookkeeper.proto.DataFormats.internal_static_LedgerRereplicationLayoutFormat_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.bookkeeper.proto.DataFormats.internal_static_LedgerRereplicationLayoutFormat_fieldAccessorTable;
    }
    
    private int bitField0_;
    // required string type = 1;
    public static final int TYPE_FIELD_NUMBER = 1;
    private java.lang.Object type_;
    public boolean hasType() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    public String getType() {
      java.lang.Object ref = type_;
      if (ref instanceof String) {
        return (String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        String s = bs.toStringUtf8();
        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
          type_ = s;
        }
        return s;
      }
    }
    private com.google.protobuf.ByteString getTypeBytes() {
      java.lang.Object ref = type_;
      if (ref instanceof String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
        type_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }
    
    // required int32 version = 2;
    public static final int VERSION_FIELD_NUMBER = 2;
    private int version_;
    public boolean hasVersion() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    public int getVersion() {
      return version_;
    }
    
    private void initFields() {
      type_ = "";
      version_ = 0;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      if (!hasType()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasVersion()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getTypeBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeInt32(2, version_);
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getTypeBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(2, version_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.bookkeeper.proto.DataFormats.LedgerRereplicationLayoutFormat parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.LedgerRereplicationLayoutFormat parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.LedgerRereplicationLayoutFormat parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.LedgerRereplicationLayoutFormat parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.LedgerRereplicationLayoutFormat parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.LedgerRereplicationLayoutFormat parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.LedgerRereplicationLayoutFormat parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.bookkeeper.proto.DataFormats.LedgerRereplicationLayoutFormat parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.bookkeeper.proto.DataFormats.LedgerRereplicationLayoutFormat parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.LedgerRereplicationLayoutFormat parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.bookkeeper.proto.DataFormats.LedgerRereplicationLayoutFormat prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.bookkeeper.proto.DataFormats.LedgerRereplicationLayoutFormatOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.bookkeeper.proto.DataFormats.internal_static_LedgerRereplicationLayoutFormat_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.bookkeeper.proto.DataFormats.internal_static_LedgerRereplicationLayoutFormat_fieldAccessorTable;
      }
      
      // Construct using org.apache.bookkeeper.proto.DataFormats.LedgerRereplicationLayoutFormat.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        type_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        version_ = 0;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.bookkeeper.proto.DataFormats.LedgerRereplicationLayoutFormat.getDescriptor();
      }
      
      public org.apache.bookkeeper.proto.DataFormats.LedgerRereplicationLayoutFormat getDefaultInstanceForType() {
        return org.apache.bookkeeper.proto.DataFormats.LedgerRereplicationLayoutFormat.getDefaultInstance();
      }
      
      public org.apache.bookkeeper.proto.DataFormats.LedgerRereplicationLayoutFormat build() {
        org.apache.bookkeeper.proto.DataFormats.LedgerRereplicationLayoutFormat result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.bookkeeper.proto.DataFormats.LedgerRereplicationLayoutFormat buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.bookkeeper.proto.DataFormats.LedgerRereplicationLayoutFormat result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.bookkeeper.proto.DataFormats.LedgerRereplicationLayoutFormat buildPartial() {
        org.apache.bookkeeper.proto.DataFormats.LedgerRereplicationLayoutFormat result = new org.apache.bookkeeper.proto.DataFormats.LedgerRereplicationLayoutFormat(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.type_ = type_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.version_ = version_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.bookkeeper.proto.DataFormats.LedgerRereplicationLayoutFormat) {
          return mergeFrom((org.apache.bookkeeper.proto.DataFormats.LedgerRereplicationLayoutFormat)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.bookkeeper.proto.DataFormats.LedgerRereplicationLayoutFormat other) {
        if (other == org.apache.bookkeeper.proto.DataFormats.LedgerRereplicationLayoutFormat.getDefaultInstance()) return this;
        if (other.hasType()) {
          setType(other.getType());
        }
        if (other.hasVersion()) {
          setVersion(other.getVersion());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        if (!hasType()) {
          
          return false;
        }
        if (!hasVersion()) {
          
          return false;
        }
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              type_ = input.readBytes();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              version_ = input.readInt32();
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // required string type = 1;
      private java.lang.Object type_ = "";
      public boolean hasType() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public String getType() {
        java.lang.Object ref = type_;
        if (!(ref instanceof String)) {
          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
          type_ = s;
          return s;
        } else {
          return (String) ref;
        }
      }
      public Builder setType(String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        type_ = value;
        onChanged();
        return this;
      }
      public Builder clearType() {
        bitField0_ = (bitField0_ & ~0x00000001);
        type_ = getDefaultInstance().getType();
        onChanged();
        return this;
      }
      void setType(com.google.protobuf.ByteString value) {
        bitField0_ |= 0x00000001;
        type_ = value;
        onChanged();
      }
      
      // required int32 version = 2;
      private int version_ ;
      public boolean hasVersion() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      public int getVersion() {
        return version_;
      }
      public Builder setVersion(int value) {
        bitField0_ |= 0x00000002;
        version_ = value;
        onChanged();
        return this;
      }
      public Builder clearVersion() {
        bitField0_ = (bitField0_ & ~0x00000002);
        version_ = 0;
        onChanged();
        return this;
      }
      
      // @@protoc_insertion_point(builder_scope:LedgerRereplicationLayoutFormat)
    }
    
    static {
      defaultInstance = new LedgerRereplicationLayoutFormat(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:LedgerRereplicationLayoutFormat)
  }
  
  public interface UnderreplicatedLedgerFormatOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // repeated string replica = 1;
    java.util.List<String> getReplicaList();
    int getReplicaCount();
    String getReplica(int index);
  }
  public static final class UnderreplicatedLedgerFormat extends
      com.google.protobuf.GeneratedMessage
      implements UnderreplicatedLedgerFormatOrBuilder {
    // Use UnderreplicatedLedgerFormat.newBuilder() to construct.
    private UnderreplicatedLedgerFormat(Builder builder) {
      super(builder);
    }
    private UnderreplicatedLedgerFormat(boolean noInit) {}
    
    private static final UnderreplicatedLedgerFormat defaultInstance;
    public static UnderreplicatedLedgerFormat getDefaultInstance() {
      return defaultInstance;
    }
    
    public UnderreplicatedLedgerFormat getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.bookkeeper.proto.DataFormats.internal_static_UnderreplicatedLedgerFormat_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.bookkeeper.proto.DataFormats.internal_static_UnderreplicatedLedgerFormat_fieldAccessorTable;
    }
    
    // repeated string replica = 1;
    public static final int REPLICA_FIELD_NUMBER = 1;
    private com.google.protobuf.LazyStringList replica_;
    public java.util.List<String>
        getReplicaList() {
      return replica_;
    }
    public int getReplicaCount() {
      return replica_.size();
    }
    public String getReplica(int index) {
      return replica_.get(index);
    }
    
    private void initFields() {
      replica_ = com.google.protobuf.LazyStringArrayList.EMPTY;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < replica_.size(); i++) {
        output.writeBytes(1, replica_.getByteString(i));
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      {
        int dataSize = 0;
        for (int i = 0; i < replica_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeBytesSizeNoTag(replica_.getByteString(i));
        }
        size += dataSize;
        size += 1 * getReplicaList().size();
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.bookkeeper.proto.DataFormats.UnderreplicatedLedgerFormat parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.UnderreplicatedLedgerFormat parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.UnderreplicatedLedgerFormat parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.UnderreplicatedLedgerFormat parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.UnderreplicatedLedgerFormat parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.UnderreplicatedLedgerFormat parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.UnderreplicatedLedgerFormat parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.bookkeeper.proto.DataFormats.UnderreplicatedLedgerFormat parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.bookkeeper.proto.DataFormats.UnderreplicatedLedgerFormat parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.UnderreplicatedLedgerFormat parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.bookkeeper.proto.DataFormats.UnderreplicatedLedgerFormat prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.bookkeeper.proto.DataFormats.UnderreplicatedLedgerFormatOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.bookkeeper.proto.DataFormats.internal_static_UnderreplicatedLedgerFormat_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.bookkeeper.proto.DataFormats.internal_static_UnderreplicatedLedgerFormat_fieldAccessorTable;
      }
      
      // Construct using org.apache.bookkeeper.proto.DataFormats.UnderreplicatedLedgerFormat.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        replica_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.bookkeeper.proto.DataFormats.UnderreplicatedLedgerFormat.getDescriptor();
      }
      
      public org.apache.bookkeeper.proto.DataFormats.UnderreplicatedLedgerFormat getDefaultInstanceForType() {
        return org.apache.bookkeeper.proto.DataFormats.UnderreplicatedLedgerFormat.getDefaultInstance();
      }
      
      public org.apache.bookkeeper.proto.DataFormats.UnderreplicatedLedgerFormat build() {
        org.apache.bookkeeper.proto.DataFormats.UnderreplicatedLedgerFormat result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.bookkeeper.proto.DataFormats.UnderreplicatedLedgerFormat buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.bookkeeper.proto.DataFormats.UnderreplicatedLedgerFormat result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.bookkeeper.proto.DataFormats.UnderreplicatedLedgerFormat buildPartial() {
        org.apache.bookkeeper.proto.DataFormats.UnderreplicatedLedgerFormat result = new org.apache.bookkeeper.proto.DataFormats.UnderreplicatedLedgerFormat(this);
        int from_bitField0_ = bitField0_;
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          replica_ = new com.google.protobuf.UnmodifiableLazyStringList(
              replica_);
          bitField0_ = (bitField0_ & ~0x00000001);
        }
        result.replica_ = replica_;
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.bookkeeper.proto.DataFormats.UnderreplicatedLedgerFormat) {
          return mergeFrom((org.apache.bookkeeper.proto.DataFormats.UnderreplicatedLedgerFormat)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.bookkeeper.proto.DataFormats.UnderreplicatedLedgerFormat other) {
        if (other == org.apache.bookkeeper.proto.DataFormats.UnderreplicatedLedgerFormat.getDefaultInstance()) return this;
        if (!other.replica_.isEmpty()) {
          if (replica_.isEmpty()) {
            replica_ = other.replica_;
            bitField0_ = (bitField0_ & ~0x00000001);
          } else {
            ensureReplicaIsMutable();
            replica_.addAll(other.replica_);
          }
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 10: {
              ensureReplicaIsMutable();
              replica_.add(input.readBytes());
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // repeated string replica = 1;
      private com.google.protobuf.LazyStringList replica_ = com.google.protobuf.LazyStringArrayList.EMPTY;
      private void ensureReplicaIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          replica_ = new com.google.protobuf.LazyStringArrayList(replica_);
          bitField0_ |= 0x00000001;
         }
      }
      public java.util.List<String>
          getReplicaList() {
        return java.util.Collections.unmodifiableList(replica_);
      }
      public int getReplicaCount() {
        return replica_.size();
      }
      public String getReplica(int index) {
        return replica_.get(index);
      }
      public Builder setReplica(
          int index, String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureReplicaIsMutable();
        replica_.set(index, value);
        onChanged();
        return this;
      }
      public Builder addReplica(String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureReplicaIsMutable();
        replica_.add(value);
        onChanged();
        return this;
      }
      public Builder addAllReplica(
          java.lang.Iterable<String> values) {
        ensureReplicaIsMutable();
        super.addAll(values, replica_);
        onChanged();
        return this;
      }
      public Builder clearReplica() {
        replica_ = com.google.protobuf.LazyStringArrayList.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }
      void addReplica(com.google.protobuf.ByteString value) {
        ensureReplicaIsMutable();
        replica_.add(value);
        onChanged();
      }
      
      // @@protoc_insertion_point(builder_scope:UnderreplicatedLedgerFormat)
    }
    
    static {
      defaultInstance = new UnderreplicatedLedgerFormat(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:UnderreplicatedLedgerFormat)
  }
  
  public interface CookieFormatOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // required string bookieHost = 1;
    boolean hasBookieHost();
    String getBookieHost();
    
    // required string journalDir = 2;
    boolean hasJournalDir();
    String getJournalDir();
    
    // required string ledgerDirs = 3;
    boolean hasLedgerDirs();
    String getLedgerDirs();
    
    // optional string instanceId = 4;
    boolean hasInstanceId();
    String getInstanceId();
  }
  public static final class CookieFormat extends
      com.google.protobuf.GeneratedMessage
      implements CookieFormatOrBuilder {
    // Use CookieFormat.newBuilder() to construct.
    private CookieFormat(Builder builder) {
      super(builder);
    }
    private CookieFormat(boolean noInit) {}
    
    private static final CookieFormat defaultInstance;
    public static CookieFormat getDefaultInstance() {
      return defaultInstance;
    }
    
    public CookieFormat getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.bookkeeper.proto.DataFormats.internal_static_CookieFormat_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.bookkeeper.proto.DataFormats.internal_static_CookieFormat_fieldAccessorTable;
    }
    
    private int bitField0_;
    // required string bookieHost = 1;
    public static final int BOOKIEHOST_FIELD_NUMBER = 1;
    private java.lang.Object bookieHost_;
    public boolean hasBookieHost() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    public String getBookieHost() {
      java.lang.Object ref = bookieHost_;
      if (ref instanceof String) {
        return (String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        String s = bs.toStringUtf8();
        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
          bookieHost_ = s;
        }
        return s;
      }
    }
    private com.google.protobuf.ByteString getBookieHostBytes() {
      java.lang.Object ref = bookieHost_;
      if (ref instanceof String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
        bookieHost_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }
    
    // required string journalDir = 2;
    public static final int JOURNALDIR_FIELD_NUMBER = 2;
    private java.lang.Object journalDir_;
    public boolean hasJournalDir() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    public String getJournalDir() {
      java.lang.Object ref = journalDir_;
      if (ref instanceof String) {
        return (String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        String s = bs.toStringUtf8();
        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
          journalDir_ = s;
        }
        return s;
      }
    }
    private com.google.protobuf.ByteString getJournalDirBytes() {
      java.lang.Object ref = journalDir_;
      if (ref instanceof String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
        journalDir_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }
    
    // required string ledgerDirs = 3;
    public static final int LEDGERDIRS_FIELD_NUMBER = 3;
    private java.lang.Object ledgerDirs_;
    public boolean hasLedgerDirs() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    public String getLedgerDirs() {
      java.lang.Object ref = ledgerDirs_;
      if (ref instanceof String) {
        return (String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        String s = bs.toStringUtf8();
        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
          ledgerDirs_ = s;
        }
        return s;
      }
    }
    private com.google.protobuf.ByteString getLedgerDirsBytes() {
      java.lang.Object ref = ledgerDirs_;
      if (ref instanceof String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
        ledgerDirs_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }
    
    // optional string instanceId = 4;
    public static final int INSTANCEID_FIELD_NUMBER = 4;
    private java.lang.Object instanceId_;
    public boolean hasInstanceId() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    public String getInstanceId() {
      java.lang.Object ref = instanceId_;
      if (ref instanceof String) {
        return (String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        String s = bs.toStringUtf8();
        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
          instanceId_ = s;
        }
        return s;
      }
    }
    private com.google.protobuf.ByteString getInstanceIdBytes() {
      java.lang.Object ref = instanceId_;
      if (ref instanceof String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
        instanceId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }
    
    private void initFields() {
      bookieHost_ = "";
      journalDir_ = "";
      ledgerDirs_ = "";
      instanceId_ = "";
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      if (!hasBookieHost()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasJournalDir()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasLedgerDirs()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getBookieHostBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, getJournalDirBytes());
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeBytes(3, getLedgerDirsBytes());
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeBytes(4, getInstanceIdBytes());
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getBookieHostBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, getJournalDirBytes());
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(3, getLedgerDirsBytes());
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(4, getInstanceIdBytes());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.bookkeeper.proto.DataFormats.CookieFormat parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.CookieFormat parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.CookieFormat parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.CookieFormat parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.CookieFormat parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.CookieFormat parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.CookieFormat parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.bookkeeper.proto.DataFormats.CookieFormat parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.bookkeeper.proto.DataFormats.CookieFormat parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.CookieFormat parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.bookkeeper.proto.DataFormats.CookieFormat prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.bookkeeper.proto.DataFormats.CookieFormatOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.bookkeeper.proto.DataFormats.internal_static_CookieFormat_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.bookkeeper.proto.DataFormats.internal_static_CookieFormat_fieldAccessorTable;
      }
      
      // Construct using org.apache.bookkeeper.proto.DataFormats.CookieFormat.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        bookieHost_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        journalDir_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        ledgerDirs_ = "";
        bitField0_ = (bitField0_ & ~0x00000004);
        instanceId_ = "";
        bitField0_ = (bitField0_ & ~0x00000008);
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.bookkeeper.proto.DataFormats.CookieFormat.getDescriptor();
      }
      
      public org.apache.bookkeeper.proto.DataFormats.CookieFormat getDefaultInstanceForType() {
        return org.apache.bookkeeper.proto.DataFormats.CookieFormat.getDefaultInstance();
      }
      
      public org.apache.bookkeeper.proto.DataFormats.CookieFormat build() {
        org.apache.bookkeeper.proto.DataFormats.CookieFormat result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.bookkeeper.proto.DataFormats.CookieFormat buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.bookkeeper.proto.DataFormats.CookieFormat result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.bookkeeper.proto.DataFormats.CookieFormat buildPartial() {
        org.apache.bookkeeper.proto.DataFormats.CookieFormat result = new org.apache.bookkeeper.proto.DataFormats.CookieFormat(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.bookieHost_ = bookieHost_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.journalDir_ = journalDir_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.ledgerDirs_ = ledgerDirs_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        result.instanceId_ = instanceId_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.bookkeeper.proto.DataFormats.CookieFormat) {
          return mergeFrom((org.apache.bookkeeper.proto.DataFormats.CookieFormat)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.bookkeeper.proto.DataFormats.CookieFormat other) {
        if (other == org.apache.bookkeeper.proto.DataFormats.CookieFormat.getDefaultInstance()) return this;
        if (other.hasBookieHost()) {
          setBookieHost(other.getBookieHost());
        }
        if (other.hasJournalDir()) {
          setJournalDir(other.getJournalDir());
        }
        if (other.hasLedgerDirs()) {
          setLedgerDirs(other.getLedgerDirs());
        }
        if (other.hasInstanceId()) {
          setInstanceId(other.getInstanceId());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        if (!hasBookieHost()) {
          
          return false;
        }
        if (!hasJournalDir()) {
          
          return false;
        }
        if (!hasLedgerDirs()) {
          
          return false;
        }
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              bookieHost_ = input.readBytes();
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              journalDir_ = input.readBytes();
              break;
            }
            case 26: {
              bitField0_ |= 0x00000004;
              ledgerDirs_ = input.readBytes();
              break;
            }
            case 34: {
              bitField0_ |= 0x00000008;
              instanceId_ = input.readBytes();
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // required string bookieHost = 1;
      private java.lang.Object bookieHost_ = "";
      public boolean hasBookieHost() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public String getBookieHost() {
        java.lang.Object ref = bookieHost_;
        if (!(ref instanceof String)) {
          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
          bookieHost_ = s;
          return s;
        } else {
          return (String) ref;
        }
      }
      public Builder setBookieHost(String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        bookieHost_ = value;
        onChanged();
        return this;
      }
      public Builder clearBookieHost() {
        bitField0_ = (bitField0_ & ~0x00000001);
        bookieHost_ = getDefaultInstance().getBookieHost();
        onChanged();
        return this;
      }
      void setBookieHost(com.google.protobuf.ByteString value) {
        bitField0_ |= 0x00000001;
        bookieHost_ = value;
        onChanged();
      }
      
      // required string journalDir = 2;
      private java.lang.Object journalDir_ = "";
      public boolean hasJournalDir() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      public String getJournalDir() {
        java.lang.Object ref = journalDir_;
        if (!(ref instanceof String)) {
          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
          journalDir_ = s;
          return s;
        } else {
          return (String) ref;
        }
      }
      public Builder setJournalDir(String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        journalDir_ = value;
        onChanged();
        return this;
      }
      public Builder clearJournalDir() {
        bitField0_ = (bitField0_ & ~0x00000002);
        journalDir_ = getDefaultInstance().getJournalDir();
        onChanged();
        return this;
      }
      void setJournalDir(com.google.protobuf.ByteString value) {
        bitField0_ |= 0x00000002;
        journalDir_ = value;
        onChanged();
      }
      
      // required string ledgerDirs = 3;
      private java.lang.Object ledgerDirs_ = "";
      public boolean hasLedgerDirs() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      public String getLedgerDirs() {
        java.lang.Object ref = ledgerDirs_;
        if (!(ref instanceof String)) {
          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
          ledgerDirs_ = s;
          return s;
        } else {
          return (String) ref;
        }
      }
      public Builder setLedgerDirs(String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        ledgerDirs_ = value;
        onChanged();
        return this;
      }
      public Builder clearLedgerDirs() {
        bitField0_ = (bitField0_ & ~0x00000004);
        ledgerDirs_ = getDefaultInstance().getLedgerDirs();
        onChanged();
        return this;
      }
      void setLedgerDirs(com.google.protobuf.ByteString value) {
        bitField0_ |= 0x00000004;
        ledgerDirs_ = value;
        onChanged();
      }
      
      // optional string instanceId = 4;
      private java.lang.Object instanceId_ = "";
      public boolean hasInstanceId() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      public String getInstanceId() {
        java.lang.Object ref = instanceId_;
        if (!(ref instanceof String)) {
          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
          instanceId_ = s;
          return s;
        } else {
          return (String) ref;
        }
      }
      public Builder setInstanceId(String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        instanceId_ = value;
        onChanged();
        return this;
      }
      public Builder clearInstanceId() {
        bitField0_ = (bitField0_ & ~0x00000008);
        instanceId_ = getDefaultInstance().getInstanceId();
        onChanged();
        return this;
      }
      void setInstanceId(com.google.protobuf.ByteString value) {
        bitField0_ |= 0x00000008;
        instanceId_ = value;
        onChanged();
      }
      
      // @@protoc_insertion_point(builder_scope:CookieFormat)
    }
    
    static {
      defaultInstance = new CookieFormat(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:CookieFormat)
  }
  
  public interface LockDataFormatOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // optional string bookieId = 1;
    boolean hasBookieId();
    String getBookieId();
  }
  public static final class LockDataFormat extends
      com.google.protobuf.GeneratedMessage
      implements LockDataFormatOrBuilder {
    // Use LockDataFormat.newBuilder() to construct.
    private LockDataFormat(Builder builder) {
      super(builder);
    }
    private LockDataFormat(boolean noInit) {}
    
    private static final LockDataFormat defaultInstance;
    public static LockDataFormat getDefaultInstance() {
      return defaultInstance;
    }
    
    public LockDataFormat getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.bookkeeper.proto.DataFormats.internal_static_LockDataFormat_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.bookkeeper.proto.DataFormats.internal_static_LockDataFormat_fieldAccessorTable;
    }
    
    private int bitField0_;
    // optional string bookieId = 1;
    public static final int BOOKIEID_FIELD_NUMBER = 1;
    private java.lang.Object bookieId_;
    public boolean hasBookieId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    public String getBookieId() {
      java.lang.Object ref = bookieId_;
      if (ref instanceof String) {
        return (String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        String s = bs.toStringUtf8();
        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
          bookieId_ = s;
        }
        return s;
      }
    }
    private com.google.protobuf.ByteString getBookieIdBytes() {
      java.lang.Object ref = bookieId_;
      if (ref instanceof String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
        bookieId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }
    
    private void initFields() {
      bookieId_ = "";
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getBookieIdBytes());
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getBookieIdBytes());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.bookkeeper.proto.DataFormats.LockDataFormat parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.LockDataFormat parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.LockDataFormat parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.LockDataFormat parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.LockDataFormat parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.LockDataFormat parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.LockDataFormat parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.bookkeeper.proto.DataFormats.LockDataFormat parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.bookkeeper.proto.DataFormats.LockDataFormat parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.LockDataFormat parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.bookkeeper.proto.DataFormats.LockDataFormat prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.bookkeeper.proto.DataFormats.LockDataFormatOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.bookkeeper.proto.DataFormats.internal_static_LockDataFormat_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.bookkeeper.proto.DataFormats.internal_static_LockDataFormat_fieldAccessorTable;
      }
      
      // Construct using org.apache.bookkeeper.proto.DataFormats.LockDataFormat.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        bookieId_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.bookkeeper.proto.DataFormats.LockDataFormat.getDescriptor();
      }
      
      public org.apache.bookkeeper.proto.DataFormats.LockDataFormat getDefaultInstanceForType() {
        return org.apache.bookkeeper.proto.DataFormats.LockDataFormat.getDefaultInstance();
      }
      
      public org.apache.bookkeeper.proto.DataFormats.LockDataFormat build() {
        org.apache.bookkeeper.proto.DataFormats.LockDataFormat result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.bookkeeper.proto.DataFormats.LockDataFormat buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.bookkeeper.proto.DataFormats.LockDataFormat result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.bookkeeper.proto.DataFormats.LockDataFormat buildPartial() {
        org.apache.bookkeeper.proto.DataFormats.LockDataFormat result = new org.apache.bookkeeper.proto.DataFormats.LockDataFormat(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.bookieId_ = bookieId_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.bookkeeper.proto.DataFormats.LockDataFormat) {
          return mergeFrom((org.apache.bookkeeper.proto.DataFormats.LockDataFormat)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.bookkeeper.proto.DataFormats.LockDataFormat other) {
        if (other == org.apache.bookkeeper.proto.DataFormats.LockDataFormat.getDefaultInstance()) return this;
        if (other.hasBookieId()) {
          setBookieId(other.getBookieId());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              bookieId_ = input.readBytes();
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // optional string bookieId = 1;
      private java.lang.Object bookieId_ = "";
      public boolean hasBookieId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public String getBookieId() {
        java.lang.Object ref = bookieId_;
        if (!(ref instanceof String)) {
          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
          bookieId_ = s;
          return s;
        } else {
          return (String) ref;
        }
      }
      public Builder setBookieId(String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        bookieId_ = value;
        onChanged();
        return this;
      }
      public Builder clearBookieId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        bookieId_ = getDefaultInstance().getBookieId();
        onChanged();
        return this;
      }
      void setBookieId(com.google.protobuf.ByteString value) {
        bitField0_ |= 0x00000001;
        bookieId_ = value;
        onChanged();
      }
      
      // @@protoc_insertion_point(builder_scope:LockDataFormat)
    }
    
    static {
      defaultInstance = new LockDataFormat(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:LockDataFormat)
  }
  
  public interface AuditorVoteFormatOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // optional string bookieId = 1;
    boolean hasBookieId();
    String getBookieId();
  }
  public static final class AuditorVoteFormat extends
      com.google.protobuf.GeneratedMessage
      implements AuditorVoteFormatOrBuilder {
    // Use AuditorVoteFormat.newBuilder() to construct.
    private AuditorVoteFormat(Builder builder) {
      super(builder);
    }
    private AuditorVoteFormat(boolean noInit) {}
    
    private static final AuditorVoteFormat defaultInstance;
    public static AuditorVoteFormat getDefaultInstance() {
      return defaultInstance;
    }
    
    public AuditorVoteFormat getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.bookkeeper.proto.DataFormats.internal_static_AuditorVoteFormat_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.bookkeeper.proto.DataFormats.internal_static_AuditorVoteFormat_fieldAccessorTable;
    }
    
    private int bitField0_;
    // optional string bookieId = 1;
    public static final int BOOKIEID_FIELD_NUMBER = 1;
    private java.lang.Object bookieId_;
    public boolean hasBookieId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    public String getBookieId() {
      java.lang.Object ref = bookieId_;
      if (ref instanceof String) {
        return (String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        String s = bs.toStringUtf8();
        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
          bookieId_ = s;
        }
        return s;
      }
    }
    private com.google.protobuf.ByteString getBookieIdBytes() {
      java.lang.Object ref = bookieId_;
      if (ref instanceof String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
        bookieId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }
    
    private void initFields() {
      bookieId_ = "";
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, getBookieIdBytes());
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, getBookieIdBytes());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.bookkeeper.proto.DataFormats.AuditorVoteFormat parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.AuditorVoteFormat parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.AuditorVoteFormat parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.AuditorVoteFormat parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.AuditorVoteFormat parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.AuditorVoteFormat parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.AuditorVoteFormat parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.bookkeeper.proto.DataFormats.AuditorVoteFormat parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.bookkeeper.proto.DataFormats.AuditorVoteFormat parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.bookkeeper.proto.DataFormats.AuditorVoteFormat parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.bookkeeper.proto.DataFormats.AuditorVoteFormat prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.bookkeeper.proto.DataFormats.AuditorVoteFormatOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.bookkeeper.proto.DataFormats.internal_static_AuditorVoteFormat_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.bookkeeper.proto.DataFormats.internal_static_AuditorVoteFormat_fieldAccessorTable;
      }
      
      // Construct using org.apache.bookkeeper.proto.DataFormats.AuditorVoteFormat.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        bookieId_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.bookkeeper.proto.DataFormats.AuditorVoteFormat.getDescriptor();
      }
      
      public org.apache.bookkeeper.proto.DataFormats.AuditorVoteFormat getDefaultInstanceForType() {
        return org.apache.bookkeeper.proto.DataFormats.AuditorVoteFormat.getDefaultInstance();
      }
      
      public org.apache.bookkeeper.proto.DataFormats.AuditorVoteFormat build() {
        org.apache.bookkeeper.proto.DataFormats.AuditorVoteFormat result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.bookkeeper.proto.DataFormats.AuditorVoteFormat buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.bookkeeper.proto.DataFormats.AuditorVoteFormat result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.bookkeeper.proto.DataFormats.AuditorVoteFormat buildPartial() {
        org.apache.bookkeeper.proto.DataFormats.AuditorVoteFormat result = new org.apache.bookkeeper.proto.DataFormats.AuditorVoteFormat(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.bookieId_ = bookieId_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.bookkeeper.proto.DataFormats.AuditorVoteFormat) {
          return mergeFrom((org.apache.bookkeeper.proto.DataFormats.AuditorVoteFormat)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.bookkeeper.proto.DataFormats.AuditorVoteFormat other) {
        if (other == org.apache.bookkeeper.proto.DataFormats.AuditorVoteFormat.getDefaultInstance()) return this;
        if (other.hasBookieId()) {
          setBookieId(other.getBookieId());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              bookieId_ = input.readBytes();
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // optional string bookieId = 1;
      private java.lang.Object bookieId_ = "";
      public boolean hasBookieId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public String getBookieId() {
        java.lang.Object ref = bookieId_;
        if (!(ref instanceof String)) {
          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
          bookieId_ = s;
          return s;
        } else {
          return (String) ref;
        }
      }
      public Builder setBookieId(String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        bookieId_ = value;
        onChanged();
        return this;
      }
      public Builder clearBookieId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        bookieId_ = getDefaultInstance().getBookieId();
        onChanged();
        return this;
      }
      void setBookieId(com.google.protobuf.ByteString value) {
        bitField0_ |= 0x00000001;
        bookieId_ = value;
        onChanged();
      }
      
      // @@protoc_insertion_point(builder_scope:AuditorVoteFormat)
    }
    
    static {
      defaultInstance = new AuditorVoteFormat(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:AuditorVoteFormat)
  }
  
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_LedgerMetadataFormat_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_LedgerMetadataFormat_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_LedgerMetadataFormat_Segment_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_LedgerMetadataFormat_Segment_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_LedgerRereplicationLayoutFormat_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_LedgerRereplicationLayoutFormat_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_UnderreplicatedLedgerFormat_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_UnderreplicatedLedgerFormat_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_CookieFormat_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_CookieFormat_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_LockDataFormat_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_LockDataFormat_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_AuditorVoteFormat_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_AuditorVoteFormat_fieldAccessorTable;
  
  public static com.google.protobuf.Descriptors.FileDescriptor
      getDescriptor() {
    return descriptor;
  }
  private static com.google.protobuf.Descriptors.FileDescriptor
      descriptor;
  static {
    java.lang.String[] descriptorData = {
      "\n src/main/proto/DataFormats.proto\"\262\003\n\024L" +
      "edgerMetadataFormat\022\022\n\nquorumSize\030\001 \002(\005\022" +
      "\024\n\014ensembleSize\030\002 \002(\005\022\016\n\006length\030\003 \002(\003\022\023\n" +
      "\013lastEntryId\030\004 \001(\003\0220\n\005state\030\005 \002(\0162\033.Ledg" +
      "erMetadataFormat.State:\004OPEN\022.\n\007segment\030" +
      "\006 \003(\0132\035.LedgerMetadataFormat.Segment\0224\n\n" +
      "digestType\030\007 \001(\0162 .LedgerMetadataFormat." +
      "DigestType\022\020\n\010password\030\010 \001(\014\022\025\n\rackQuoru" +
      "mSize\030\t \001(\005\0327\n\007Segment\022\026\n\016ensembleMember" +
      "\030\001 \003(\t\022\024\n\014firstEntryId\030\002 \002(\003\".\n\005State\022\010\n",
      "\004OPEN\020\001\022\017\n\013IN_RECOVERY\020\002\022\n\n\006CLOSED\020\003\"!\n\n" +
      "DigestType\022\t\n\005CRC32\020\001\022\010\n\004HMAC\020\002\"@\n\037Ledge" +
      "rRereplicationLayoutFormat\022\014\n\004type\030\001 \002(\t" +
      "\022\017\n\007version\030\002 \002(\005\".\n\033UnderreplicatedLedg" +
      "erFormat\022\017\n\007replica\030\001 \003(\t\"^\n\014CookieForma" +
      "t\022\022\n\nbookieHost\030\001 \002(\t\022\022\n\njournalDir\030\002 \002(" +
      "\t\022\022\n\nledgerDirs\030\003 \002(\t\022\022\n\ninstanceId\030\004 \001(" +
      "\t\"\"\n\016LockDataFormat\022\020\n\010bookieId\030\001 \001(\t\"%\n" +
      "\021AuditorVoteFormat\022\020\n\010bookieId\030\001 \001(\tB\037\n\033" +
      "org.apache.bookkeeper.protoH\001"
    };
    com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
      new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {
        public com.google.protobuf.ExtensionRegistry assignDescriptors(
            com.google.protobuf.Descriptors.FileDescriptor root) {
          descriptor = root;
          internal_static_LedgerMetadataFormat_descriptor =
            getDescriptor().getMessageTypes().get(0);
          internal_static_LedgerMetadataFormat_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_LedgerMetadataFormat_descriptor,
              new java.lang.String[] { "QuorumSize", "EnsembleSize", "Length", "LastEntryId", "State", "Segment", "DigestType", "Password", "AckQuorumSize", },
              org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.class,
              org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Builder.class);
          internal_static_LedgerMetadataFormat_Segment_descriptor =
            internal_static_LedgerMetadataFormat_descriptor.getNestedTypes().get(0);
          internal_static_LedgerMetadataFormat_Segment_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_LedgerMetadataFormat_Segment_descriptor,
              new java.lang.String[] { "EnsembleMember", "FirstEntryId", },
              org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment.class,
              org.apache.bookkeeper.proto.DataFormats.LedgerMetadataFormat.Segment.Builder.class);
          internal_static_LedgerRereplicationLayoutFormat_descriptor =
            getDescriptor().getMessageTypes().get(1);
          internal_static_LedgerRereplicationLayoutFormat_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_LedgerRereplicationLayoutFormat_descriptor,
              new java.lang.String[] { "Type", "Version", },
              org.apache.bookkeeper.proto.DataFormats.LedgerRereplicationLayoutFormat.class,
              org.apache.bookkeeper.proto.DataFormats.LedgerRereplicationLayoutFormat.Builder.class);
          internal_static_UnderreplicatedLedgerFormat_descriptor =
            getDescriptor().getMessageTypes().get(2);
          internal_static_UnderreplicatedLedgerFormat_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_UnderreplicatedLedgerFormat_descriptor,
              new java.lang.String[] { "Replica", },
              org.apache.bookkeeper.proto.DataFormats.UnderreplicatedLedgerFormat.class,
              org.apache.bookkeeper.proto.DataFormats.UnderreplicatedLedgerFormat.Builder.class);
          internal_static_CookieFormat_descriptor =
            getDescriptor().getMessageTypes().get(3);
          internal_static_CookieFormat_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_CookieFormat_descriptor,
              new java.lang.String[] { "BookieHost", "JournalDir", "LedgerDirs", "InstanceId", },
              org.apache.bookkeeper.proto.DataFormats.CookieFormat.class,
              org.apache.bookkeeper.proto.DataFormats.CookieFormat.Builder.class);
          internal_static_LockDataFormat_descriptor =
            getDescriptor().getMessageTypes().get(4);
          internal_static_LockDataFormat_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_LockDataFormat_descriptor,
              new java.lang.String[] { "BookieId", },
              org.apache.bookkeeper.proto.DataFormats.LockDataFormat.class,
              org.apache.bookkeeper.proto.DataFormats.LockDataFormat.Builder.class);
          internal_static_AuditorVoteFormat_descriptor =
            getDescriptor().getMessageTypes().get(5);
          internal_static_AuditorVoteFormat_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_AuditorVoteFormat_descriptor,
              new java.lang.String[] { "BookieId", },
              org.apache.bookkeeper.proto.DataFormats.AuditorVoteFormat.class,
              org.apache.bookkeeper.proto.DataFormats.AuditorVoteFormat.Builder.class);
          return null;
        }
      };
    com.google.protobuf.Descriptors.FileDescriptor
      .internalBuildGeneratedFileFrom(descriptorData,
        new com.google.protobuf.Descriptors.FileDescriptor[] {
        }, assigner);
  }
  
  // @@protoc_insertion_point(outer_class_scope)
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/proto/NIOServerFactory.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.bookkeeper.proto;

import java.io.IOException;
import java.net.InetSocketAddress;
import java.nio.ByteBuffer;
import java.nio.channels.CancelledKeyException;
import java.nio.channels.Channel;
import java.nio.channels.SelectionKey;
import java.nio.channels.Selector;
import java.nio.channels.ServerSocketChannel;
import java.nio.channels.SocketChannel;
import java.util.ArrayList;
import java.util.Collections;
import java.util.HashSet;
import java.util.Iterator;
import java.util.Set;
import java.util.concurrent.LinkedBlockingQueue;

import org.apache.bookkeeper.conf.ServerConfiguration;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.common.annotations.VisibleForTesting;

/**
 * This class handles communication with clients using NIO. There is one Cnxn
 * per client, but only one thread doing the communication.
 */
public class NIOServerFactory extends Thread {

    public interface PacketProcessor {
        public void processPacket(ByteBuffer packet, Cnxn src);
    }

    ServerStats stats = new ServerStats();

    Logger LOG = LoggerFactory.getLogger(NIOServerFactory.class);

    ServerSocketChannel ss;

    Selector selector = Selector.open();

    /**
     * We use this buffer to do efficient socket I/O. Since there is a single
     * sender thread per NIOServerCnxn instance, we can use a member variable to
     * only allocate it once.
     */
    ByteBuffer directBuffer = ByteBuffer.allocateDirect(64 * 1024);

    HashSet<Cnxn> cnxns = new HashSet<Cnxn>();

    int outstandingLimit = 2000;

    PacketProcessor processor;

    long minLatency = 99999999;

    ServerConfiguration conf;

    private Object suspensionLock = new Object();
    private boolean suspended = false;

    public NIOServerFactory(ServerConfiguration conf, PacketProcessor processor) throws IOException {
        super("NIOServerFactory-" + conf.getBookiePort());
        setDaemon(true);
        this.processor = processor;
        this.conf = conf;
        this.ss = ServerSocketChannel.open();
        ss.socket().bind(new InetSocketAddress(conf.getBookiePort()));
        ss.configureBlocking(false);
        ss.register(selector, SelectionKey.OP_ACCEPT);
    }

    public InetSocketAddress getLocalAddress() {
        return (InetSocketAddress) ss.socket().getLocalSocketAddress();
    }

    private void addCnxn(Cnxn cnxn) {
        synchronized (cnxns) {
            cnxns.add(cnxn);
        }
    }

    public boolean isRunning() {
        return !ss.socket().isClosed();
    }

    /**
     * Stop nio server from processing requests. (for testing)
     */
    @VisibleForTesting
    public void suspendProcessing() {
        synchronized(suspensionLock) {
            suspended = true;
        }
    }

    /**
     * Resume processing requests in nio server. (for testing)
     */
    @VisibleForTesting
    public void resumeProcessing() {
        synchronized(suspensionLock) {
            suspended = false;
            suspensionLock.notify();
        }
    }

    @Override
    public void run() {
        while (!ss.socket().isClosed()) {
            try {
                selector.select(1000);
                synchronized(suspensionLock) {
                    while (suspended) {
                        suspensionLock.wait();
                    }
                }
                Set<SelectionKey> selected;
                synchronized (this) {
                    selected = selector.selectedKeys();
                }
                ArrayList<SelectionKey> selectedList = new ArrayList<SelectionKey>(selected);
                Collections.shuffle(selectedList);
                for (SelectionKey k : selectedList) {
                    if ((k.readyOps() & SelectionKey.OP_ACCEPT) != 0) {
                        SocketChannel sc = ((ServerSocketChannel) k.channel()).accept();
                        sc.configureBlocking(false);
                        SelectionKey sk = sc.register(selector, SelectionKey.OP_READ);
                        Cnxn cnxn = new Cnxn(sc, sk);
                        sk.attach(cnxn);
                        addCnxn(cnxn);
                    } else if ((k.readyOps() & (SelectionKey.OP_READ | SelectionKey.OP_WRITE)) != 0) {
                        Cnxn c = (Cnxn) k.attachment();
                        c.doIO(k);
                    }
                }
                selected.clear();
            } catch (Exception e) {
                LOG.warn("Exception in server socket loop: " + ss.socket().getInetAddress(), e);
            }
        }
        LOG.info("NIOServerCnxn factory exitedloop.");
        clear();
    }

    /**
     * clear all the connections in the selector
     *
     */
    synchronized public void clear() {
        selector.wakeup();
        synchronized (cnxns) {
            // got to clear all the connections that we have in the selector
            for (Iterator<Cnxn> it = cnxns.iterator(); it.hasNext();) {
                Cnxn cnxn = it.next();
                it.remove();
                try {
                    cnxn.close();
                } catch (Exception e) {
                    // Do nothing.
                }
            }
        }

    }

    public void shutdown() {
        try {
            ss.close();
            clear();
            this.interrupt();
            this.join();
        } catch (InterruptedException e) {
            LOG.warn("Interrupted", e);
        } catch (Exception e) {
            LOG.error("Unexpected exception", e);
        }
    }

    /**
     * The buffer will cause the connection to be close when we do a send.
     */
    static final ByteBuffer closeConn = ByteBuffer.allocate(0);

    public class Cnxn {

        private SocketChannel sock;

        private SelectionKey sk;

        boolean initialized;

        ByteBuffer lenBuffer = ByteBuffer.allocate(4);

        ByteBuffer incomingBuffer = lenBuffer;

        LinkedBlockingQueue<ByteBuffer> outgoingBuffers = new LinkedBlockingQueue<ByteBuffer>();

        int sessionTimeout;

        void doIO(SelectionKey k) throws InterruptedException {
            try {
                if (sock == null) {
                    return;
                }
                if (k.isReadable()) {
                    int rc = sock.read(incomingBuffer);
                    if (rc < 0) {
                        throw new IOException("Read error");
                    }
                    if (incomingBuffer.remaining() == 0) {
                        incomingBuffer.flip();
                        if (incomingBuffer == lenBuffer) {
                            readLength(k);
                        } else {
                            cnxnStats.packetsReceived++;
                            ServerStats.getInstance().incrementPacketsReceived();
                            try {
                                readRequest();
                            } finally {
                                lenBuffer.clear();
                                incomingBuffer = lenBuffer;
                            }
                        }
                    }
                }
                if (k.isWritable()) {
                    if (outgoingBuffers.size() > 0) {
                        // ZooLog.logTraceMessage(LOG,
                        // ZooLog.CLIENT_DATA_PACKET_TRACE_MASK,
                        // "sk " + k + " is valid: " +
                        // k.isValid());

                        /*
                         * This is going to reset the buffer position to 0 and
                         * the limit to the size of the buffer, so that we can
                         * fill it with data from the non-direct buffers that we
                         * need to send.
                         */
                        directBuffer.clear();

                        for (ByteBuffer b : outgoingBuffers) {
                            if (directBuffer.remaining() < b.remaining()) {
                                /*
                                 * When we call put later, if the directBuffer
                                 * is to small to hold everything, nothing will
                                 * be copied, so we've got to slice the buffer
                                 * if it's too big.
                                 */
                                b = (ByteBuffer) b.slice().limit(directBuffer.remaining());
                            }
                            /*
                             * put() is going to modify the positions of both
                             * buffers, put we don't want to change the position
                             * of the source buffers (we'll do that after the
                             * send, if needed), so we save and reset the
                             * position after the copy
                             */
                            int p = b.position();
                            directBuffer.put(b);
                            b.position(p);
                            if (directBuffer.remaining() == 0) {
                                break;
                            }
                        }
                        /*
                         * Do the flip: limit becomes position, position gets
                         * set to 0. This sets us up for the write.
                         */
                        directBuffer.flip();

                        int sent = sock.write(directBuffer);
                        ByteBuffer bb;

                        // Remove the buffers that we have sent
                        while (outgoingBuffers.size() > 0) {
                            bb = outgoingBuffers.peek();
                            if (bb == closeConn) {
                                throw new IOException("closing");
                            }
                            int left = bb.remaining() - sent;
                            if (left > 0) {
                                /*
                                 * We only partially sent this buffer, so we
                                 * update the position and exit the loop.
                                 */
                                bb.position(bb.position() + sent);
                                break;
                            }
                            cnxnStats.packetsSent++;
                            /* We've sent the whole buffer, so drop the buffer */
                            sent -= bb.remaining();
                            ServerStats.getInstance().incrementPacketsSent();
                            outgoingBuffers.remove();
                        }
                        // ZooLog.logTraceMessage(LOG,
                        // ZooLog.CLIENT_DATA_PACKET_TRACE_MASK, "after send,
                        // outgoingBuffers.size() = " + outgoingBuffers.size());
                    }
                    synchronized (this) {
                        if (outgoingBuffers.size() == 0) {
                            if (!initialized && (sk.interestOps() & SelectionKey.OP_READ) == 0) {
                                throw new IOException("Responded to info probe");
                            }
                            sk.interestOps(sk.interestOps() & (~SelectionKey.OP_WRITE));
                        } else {
                            sk.interestOps(sk.interestOps() | SelectionKey.OP_WRITE);
                        }
                    }
                }
            } catch (CancelledKeyException e) {
                close();
            } catch (IOException e) {
                // LOG.error("FIXMSG",e);
                close();
            }
        }

        private void readRequest() throws IOException {
            incomingBuffer = incomingBuffer.slice();
            processor.processPacket(incomingBuffer, this);
        }

        public void disableRecv() {
            sk.interestOps(sk.interestOps() & (~SelectionKey.OP_READ));
        }

        public void enableRecv() {
            if (sk.isValid()) {
                int interest = sk.interestOps();
                if ((interest & SelectionKey.OP_READ) == 0) {
                    sk.interestOps(interest | SelectionKey.OP_READ);
                }
            }
        }

        private void readLength(SelectionKey k) throws IOException {
            // Read the length, now get the buffer
            int len = lenBuffer.getInt();
            if (len < 0 || len > 0xfffff) {
                throw new IOException("Len error " + len);
            }
            incomingBuffer = ByteBuffer.allocate(len);
        }

        /**
         * The number of requests that have been submitted but not yet responded
         * to.
         */
        int outstandingRequests;

        /*
         * (non-Javadoc)
         *
         * @see org.apache.zookeeper.server.ServerCnxnIface#getSessionTimeout()
         */
        public int getSessionTimeout() {
            return sessionTimeout;
        }

        String peerName = null;

        public Cnxn(SocketChannel sock, SelectionKey sk) throws IOException {
            this.sock = sock;
            this.sk = sk;
            sock.socket().setTcpNoDelay(conf.getServerTcpNoDelay());
            sock.socket().setSoLinger(true, 2);
            sk.interestOps(SelectionKey.OP_READ);
            if (LOG.isTraceEnabled()) {
                peerName = sock.socket().toString();
            }

            lenBuffer.clear();
            incomingBuffer = lenBuffer;
        }

        @Override
        public String toString() {
            return "NIOServerCnxn object with sock = " + sock + " and sk = " + sk;
        }

        public String getPeerName() {
            if (peerName == null) {
                peerName = sock.socket().toString();
            }
            return peerName;
        }

        boolean closed;

        /*
         * (non-Javadoc)
         *
         * @see org.apache.zookeeper.server.ServerCnxnIface#close()
         */
        public void close() {
            if (closed) {
                return;
            }
            closed = true;
            synchronized (cnxns) {
                cnxns.remove(this);
            }
            LOG.debug("close NIOServerCnxn: {}", sock);
            try {
                /*
                 * The following sequence of code is stupid! You would think
                 * that only sock.close() is needed, but alas, it doesn't work
                 * that way. If you just do sock.close() there are cases where
                 * the socket doesn't actually close...
                 */
                sock.socket().shutdownOutput();
            } catch (IOException e) {
                // This is a relatively common exception that we can't avoid
            }
            try {
                sock.socket().shutdownInput();
            } catch (IOException e) {
            }
            try {
                sock.socket().close();
            } catch (IOException e) {
                LOG.error("FIXMSG", e);
            }
            try {
                sock.close();
                // XXX The next line doesn't seem to be needed, but some posts
                // to forums suggest that it is needed. Keep in mind if errors
                // in
                // this section arise.
                // factory.selector.wakeup();
            } catch (IOException e) {
                LOG.error("FIXMSG", e);
            }
            sock = null;
            if (sk != null) {
                try {
                    // need to cancel this selection key from the selector
                    sk.cancel();
                } catch (Exception e) {
                }
            }
        }

        private void makeWritable(SelectionKey sk) {
            try {
                selector.wakeup();
                if (sk.isValid()) {
                    sk.interestOps(sk.interestOps() | SelectionKey.OP_WRITE);
                }
            } catch (RuntimeException e) {
                LOG.error("Problem setting writable", e);
                throw e;
            }
        }

        private void sendBuffers(ByteBuffer bb[]) {
            ByteBuffer len = ByteBuffer.allocate(4);
            int total = 0;
            for (int i = 0; i < bb.length; i++) {
                if (bb[i] != null) {
                    total += bb[i].remaining();
                }
            }
            LOG.debug("Sending response of size {} to {}", total, peerName);
            len.putInt(total);
            len.flip();
            outgoingBuffers.add(len);
            for (int i = 0; i < bb.length; i++) {
                if (bb[i] != null) {
                    outgoingBuffers.add(bb[i]);
                }
            }
            makeWritable(sk);
        }

        synchronized public void sendResponse(ByteBuffer... bb) {
            if (closed) {
                return;
            }
            sendBuffers(bb);
            synchronized (NIOServerFactory.this) {
                outstandingRequests--;
                // check throttling
                if (outstandingRequests < outstandingLimit) {
                    sk.selector().wakeup();
                    enableRecv();
                }
            }
        }

        public InetSocketAddress getRemoteAddress() {
            return (InetSocketAddress) sock.socket().getRemoteSocketAddress();
        }

        private class CnxnStats {
            long packetsSent = 0;
            long packetsReceived = 0;

            /**
             * The number of requests that have been submitted but not yet
             * responded to.
             */
            public long getOutstandingRequests() {
                synchronized(Cnxn.this) {
                    return outstandingRequests;
                }
            }

            public long getPacketsReceived() {
                return packetsReceived;
            }

            public long getPacketsSent() {
                return packetsSent;
            }

            @Override
            public String toString() {
                StringBuilder sb = new StringBuilder();
                Channel channel = sk.channel();
                if (channel instanceof SocketChannel) {
                    sb.append(" ").append(((SocketChannel) channel).socket().getRemoteSocketAddress()).append("[")
                    .append(Integer.toHexString(sk.interestOps())).append("](queued=").append(
                        getOutstandingRequests()).append(",recved=").append(getPacketsReceived()).append(
                            ",sent=").append(getPacketsSent()).append(")\n");
                }
                return sb.toString();
            }
        }

        private CnxnStats cnxnStats = new CnxnStats();

        public CnxnStats getStats() {
            return cnxnStats;
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/proto/PerChannelBookieClient.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.proto;

import java.io.IOException;
import java.net.InetSocketAddress;
import java.util.ArrayDeque;
import java.util.Queue;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.Semaphore;
import java.util.concurrent.atomic.AtomicLong;

import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.conf.ClientConfiguration;
import org.apache.bookkeeper.proto.BookieProtocol.PacketHeader;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.ReadEntryCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.WriteCallback;
import org.apache.bookkeeper.util.MathUtils;
import org.apache.bookkeeper.util.OrderedSafeExecutor;
import org.apache.bookkeeper.util.SafeRunnable;
import org.jboss.netty.bootstrap.ClientBootstrap;
import org.jboss.netty.buffer.ChannelBuffer;
import org.jboss.netty.buffer.ChannelBuffers;
import org.jboss.netty.channel.Channel;
import org.jboss.netty.channel.ChannelFuture;
import org.jboss.netty.channel.ChannelFutureListener;
import org.jboss.netty.channel.ChannelHandlerContext;
import org.jboss.netty.channel.ChannelPipeline;
import org.jboss.netty.channel.ChannelPipelineCoverage;
import org.jboss.netty.channel.ChannelPipelineFactory;
import org.jboss.netty.channel.ChannelStateEvent;
import org.jboss.netty.channel.Channels;
import org.jboss.netty.channel.ExceptionEvent;
import org.jboss.netty.channel.MessageEvent;
import org.jboss.netty.channel.SimpleChannelHandler;
import org.jboss.netty.channel.socket.ClientSocketChannelFactory;
import org.jboss.netty.handler.codec.frame.CorruptedFrameException;
import org.jboss.netty.handler.codec.frame.LengthFieldBasedFrameDecoder;
import org.jboss.netty.handler.codec.frame.TooLongFrameException;
import org.jboss.netty.handler.timeout.ReadTimeoutException;
import org.jboss.netty.handler.timeout.ReadTimeoutHandler;
import org.jboss.netty.util.HashedWheelTimer;
import org.jboss.netty.util.Timer;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * This class manages all details of connection to a particular bookie. It also
 * has reconnect logic if a connection to a bookie fails.
 *
 */

@ChannelPipelineCoverage("one")
public class PerChannelBookieClient extends SimpleChannelHandler implements ChannelPipelineFactory {

    static final Logger LOG = LoggerFactory.getLogger(PerChannelBookieClient.class);

    static final long maxMemory = Runtime.getRuntime().maxMemory() / 5;
    public static final int MAX_FRAME_LENGTH = 2 * 1024 * 1024; // 2M

    InetSocketAddress addr;
    AtomicLong totalBytesOutstanding;
    ClientSocketChannelFactory channelFactory;
    OrderedSafeExecutor executor;
    private Timer readTimeoutTimer;

    ConcurrentHashMap<CompletionKey, AddCompletion> addCompletions = new ConcurrentHashMap<CompletionKey, AddCompletion>();
    ConcurrentHashMap<CompletionKey, ReadCompletion> readCompletions = new ConcurrentHashMap<CompletionKey, ReadCompletion>();

    /**
     * The following member variables do not need to be concurrent, or volatile
     * because they are always updated under a lock
     */
    Queue<GenericCallback<Void>> pendingOps = new ArrayDeque<GenericCallback<Void>>();
    volatile Channel channel = null;

    private enum ConnectionState {
        DISCONNECTED, CONNECTING, CONNECTED, CLOSED
            };

    private volatile ConnectionState state;
    private final ClientConfiguration conf;

    public PerChannelBookieClient(OrderedSafeExecutor executor, ClientSocketChannelFactory channelFactory,
                                  InetSocketAddress addr, AtomicLong totalBytesOutstanding) {
        this(new ClientConfiguration(), executor, channelFactory, addr, totalBytesOutstanding);
    }
            
    public PerChannelBookieClient(ClientConfiguration conf, OrderedSafeExecutor executor, ClientSocketChannelFactory channelFactory,
                                  InetSocketAddress addr, AtomicLong totalBytesOutstanding) {
        this.conf = conf;
        this.addr = addr;
        this.executor = executor;
        this.totalBytesOutstanding = totalBytesOutstanding;
        this.channelFactory = channelFactory;
        this.state = ConnectionState.DISCONNECTED;
        this.readTimeoutTimer = null;
    }

    private void connect() {
        LOG.info("Connecting to bookie: {}", addr);

        // Set up the ClientBootStrap so we can create a new Channel connection
        // to the bookie.
        ClientBootstrap bootstrap = new ClientBootstrap(channelFactory);
        bootstrap.setPipelineFactory(this);
        bootstrap.setOption("tcpNoDelay", conf.getClientTcpNoDelay());
        bootstrap.setOption("keepAlive", true);

        ChannelFuture future = bootstrap.connect(addr);

        future.addListener(new ChannelFutureListener() {
            @Override
            public void operationComplete(ChannelFuture future) throws Exception {
                int rc;
                Queue<GenericCallback<Void>> oldPendingOps;

                synchronized (PerChannelBookieClient.this) {

                    if (future.isSuccess() && state == ConnectionState.CONNECTING) {
                        LOG.info("Successfully connected to bookie: " + addr);
                        rc = BKException.Code.OK;
                        channel = future.getChannel();
                        state = ConnectionState.CONNECTED;
                    } else if (future.isSuccess() && (state == ConnectionState.CLOSED
                                                      || state == ConnectionState.DISCONNECTED)) {
                        LOG.error("Closed before connection completed, clean up: " + addr);
                        future.getChannel().close();
                        rc = BKException.Code.BookieHandleNotAvailableException;
                        channel = null;
                    } else {
                        LOG.error("Could not connect to bookie: " + addr);
                        rc = BKException.Code.BookieHandleNotAvailableException;
                        channel = null;
                        if (state != ConnectionState.CLOSED) {
                            state = ConnectionState.DISCONNECTED;
                        }
                    }

                    // trick to not do operations under the lock, take the list
                    // of pending ops and assign it to a new variable, while
                    // emptying the pending ops by just assigning it to a new
                    // list
                    oldPendingOps = pendingOps;
                    pendingOps = new ArrayDeque<GenericCallback<Void>>();
                }

                for (GenericCallback<Void> pendingOp : oldPendingOps) {
                    pendingOp.operationComplete(rc, null);
                }
            }
        });
    }

    void connectIfNeededAndDoOp(GenericCallback<Void> op) {
        boolean completeOpNow = false;
        int opRc = BKException.Code.OK;
        // common case without lock first
        if (channel != null && state == ConnectionState.CONNECTED) {
            completeOpNow = true;
        } else {

            synchronized (this) {
                // check the channel status again under lock
                if (channel != null && state == ConnectionState.CONNECTED) {
                    completeOpNow = true;
                    opRc = BKException.Code.OK;
                } else if (state == ConnectionState.CLOSED) {
                    completeOpNow = true;
                    opRc = BKException.Code.BookieHandleNotAvailableException;
                } else {
                    // channel is either null (first connection attempt), or the
                    // channel is disconnected. Connection attempt is still in
                    // progress, queue up this op. Op will be executed when
                    // connection attempt either fails or succeeds
                    pendingOps.add(op);

                    if (state == ConnectionState.CONNECTING) {
                        // just return as connection request has already send
                        // and waiting for the response.
                        return;
                    }
                    // switch state to connecting and do connection attempt
                    state = ConnectionState.CONNECTING;
                }
            }
            if (!completeOpNow) {
                // Start connection attempt to the input server host.
                connect();
            }
        }

        if (completeOpNow) {
            op.operationComplete(opRc, null);
        }

    }

    /**
     * This method should be called only after connection has been checked for
     * {@link #connectIfNeededAndDoOp(GenericCallback)}
     *
     * @param ledgerId
     * @param masterKey
     * @param entryId
     * @param lastConfirmed
     * @param macCode
     * @param data
     * @param cb
     * @param ctx
     */
    void addEntry(final long ledgerId, byte[] masterKey, final long entryId, ChannelBuffer toSend, WriteCallback cb,
                  Object ctx, final int options) {
        final int entrySize = toSend.readableBytes();

        final CompletionKey completionKey = new CompletionKey(ledgerId, entryId);

        addCompletions.put(completionKey, new AddCompletion(cb, entrySize, ctx));

        int totalHeaderSize = 4 // for the length of the packet
                              + 4 // for the type of request
                              + BookieProtocol.MASTER_KEY_LENGTH; // for the master key

        try{
            ChannelBuffer header = channel.getConfig().getBufferFactory().getBuffer(totalHeaderSize);

            header.writeInt(totalHeaderSize - 4 + entrySize);
            header.writeInt(new PacketHeader(BookieProtocol.CURRENT_PROTOCOL_VERSION,
                                             BookieProtocol.ADDENTRY, (short)options).toInt());
            header.writeBytes(masterKey, 0, BookieProtocol.MASTER_KEY_LENGTH);

            ChannelBuffer wrappedBuffer = ChannelBuffers.wrappedBuffer(header, toSend);

            ChannelFuture future = channel.write(wrappedBuffer);
            future.addListener(new ChannelFutureListener() {
                @Override
                public void operationComplete(ChannelFuture future) throws Exception {
                    if (future.isSuccess()) {
                        if (LOG.isDebugEnabled()) {
                            LOG.debug("Successfully wrote request for adding entry: " + entryId + " ledger-id: " + ledgerId
                                                            + " bookie: " + channel.getRemoteAddress() + " entry length: " + entrySize);
                        }
                        // totalBytesOutstanding.addAndGet(entrySize);
                    } else {
                        errorOutAddKey(completionKey);
                    }
                }
            });
        } catch (Throwable e) {
            LOG.warn("Add entry operation failed", e);
            errorOutAddKey(completionKey);
        }
    }

    public void readEntryAndFenceLedger(final long ledgerId, byte[] masterKey,
                                        final long entryId,
                                        ReadEntryCallback cb, Object ctx) {
        final CompletionKey key = new CompletionKey(ledgerId, entryId);
        readCompletions.put(key, new ReadCompletion(cb, ctx));

        int totalHeaderSize = 4 // for the length of the packet
                              + 4 // for request type
                              + 8 // for ledgerId
                              + 8 // for entryId
                              + BookieProtocol.MASTER_KEY_LENGTH; // for masterKey

        ChannelBuffer tmpEntry = channel.getConfig().getBufferFactory().getBuffer(totalHeaderSize);
        tmpEntry.writeInt(totalHeaderSize - 4);

        tmpEntry.writeInt(new PacketHeader(BookieProtocol.CURRENT_PROTOCOL_VERSION,
                                           BookieProtocol.READENTRY,
                                           BookieProtocol.FLAG_DO_FENCING).toInt());
        tmpEntry.writeLong(ledgerId);
        tmpEntry.writeLong(entryId);
        tmpEntry.writeBytes(masterKey, 0, BookieProtocol.MASTER_KEY_LENGTH);

        ChannelFuture future = channel.write(tmpEntry);
        future.addListener(new ChannelFutureListener() {
            @Override
            public void operationComplete(ChannelFuture future) throws Exception {
                if (future.isSuccess()) {
                    if (LOG.isDebugEnabled()) {
                        LOG.debug("Successfully wrote request for reading entry: " + entryId + " ledger-id: "
                                  + ledgerId + " bookie: " + channel.getRemoteAddress());
                    }
                } else {
                    errorOutReadKey(key);
                }
            }
        });
    }

    public void readEntry(final long ledgerId, final long entryId, ReadEntryCallback cb, Object ctx) {
        final CompletionKey key = new CompletionKey(ledgerId, entryId);
        readCompletions.put(key, new ReadCompletion(cb, ctx));

        int totalHeaderSize = 4 // for the length of the packet
                              + 4 // for request type
                              + 8 // for ledgerId
                              + 8; // for entryId

        try{
            ChannelBuffer tmpEntry = channel.getConfig().getBufferFactory().getBuffer(totalHeaderSize);
            tmpEntry.writeInt(totalHeaderSize - 4);

            tmpEntry.writeInt(new PacketHeader(BookieProtocol.CURRENT_PROTOCOL_VERSION,
                                               BookieProtocol.READENTRY, BookieProtocol.FLAG_NONE).toInt());
            tmpEntry.writeLong(ledgerId);
            tmpEntry.writeLong(entryId);

            ChannelFuture future = channel.write(tmpEntry);
            future.addListener(new ChannelFutureListener() {
                @Override
                public void operationComplete(ChannelFuture future) throws Exception {
                    if (future.isSuccess()) {
                        if (LOG.isDebugEnabled()) {
                            LOG.debug("Successfully wrote request for reading entry: " + entryId + " ledger-id: "
                                                            + ledgerId + " bookie: " + channel.getRemoteAddress());
                        }
                    } else {
                        errorOutReadKey(key);
                    }
                }
            });
        } catch(Throwable e) {
            LOG.warn("Read entry operation failed", e);
            errorOutReadKey(key);
        }
    }

    /**
     * Disconnects the bookie client. It can be reused.
     */
    public void disconnect() {
        closeInternal(false);
    }

    /**
     * Closes the bookie client permanently. It cannot be reused.
     */
    public void close() {
        closeInternal(true);
    }

    private void closeInternal(boolean permanent) {
        synchronized (this) {
            if (permanent) {
                state = ConnectionState.CLOSED;
            } else if (state != ConnectionState.CLOSED) {
                state = ConnectionState.DISCONNECTED;
            }
        }
        if (channel != null) {
            channel.close().awaitUninterruptibly();
        }
        if (readTimeoutTimer != null) {
            readTimeoutTimer.stop();
            readTimeoutTimer = null;
        }
    }

    void errorOutReadKey(final CompletionKey key) {
        executor.submitOrdered(key.ledgerId, new SafeRunnable() {
            @Override
            public void safeRun() {

                ReadCompletion readCompletion = readCompletions.remove(key);

                if (readCompletion != null) {
                    LOG.error("Could not write  request for reading entry: " + key.entryId + " ledger-id: "
                              + key.ledgerId + " bookie: " + channel.getRemoteAddress());

                    readCompletion.cb.readEntryComplete(BKException.Code.BookieHandleNotAvailableException,
                                                        key.ledgerId, key.entryId, null, readCompletion.ctx);
                }
            }

        });
    }

    void errorOutAddKey(final CompletionKey key) {
        executor.submitOrdered(key.ledgerId, new SafeRunnable() {
            @Override
            public void safeRun() {

                AddCompletion addCompletion = addCompletions.remove(key);

                if (addCompletion != null) {
                    String bAddress = "null";
                    if(channel != null)
                        bAddress = channel.getRemoteAddress().toString();
                    LOG.error("Could not write request for adding entry: " + key.entryId + " ledger-id: "
                              + key.ledgerId + " bookie: " + bAddress);

                    addCompletion.cb.writeComplete(BKException.Code.BookieHandleNotAvailableException, key.ledgerId,
                                                   key.entryId, addr, addCompletion.ctx);
                    LOG.error("Invoked callback method: " + key.entryId);
                }
            }

        });

    }

    /**
     * Errors out pending entries. We call this method from one thread to avoid
     * concurrent executions to QuorumOpMonitor (implements callbacks). It seems
     * simpler to call it from BookieHandle instead of calling directly from
     * here.
     */

    void errorOutOutstandingEntries() {

        // DO NOT rewrite these using Map.Entry iterations. We want to iterate
        // on keys and see if we are successfully able to remove the key from
        // the map. Because the add and the read methods also do the same thing
        // in case they get a write failure on the socket. The one who
        // successfully removes the key from the map is the one responsible for
        // calling the application callback.

        for (CompletionKey key : addCompletions.keySet()) {
            errorOutAddKey(key);
        }

        for (CompletionKey key : readCompletions.keySet()) {
            errorOutReadKey(key);
        }
    }

    /**
     * In the netty pipeline, we need to split packets based on length, so we
     * use the {@link LengthFieldBasedFrameDecoder}. Other than that all actions
     * are carried out in this class, e.g., making sense of received messages,
     * prepending the length to outgoing packets etc.
     */
    @Override
    public ChannelPipeline getPipeline() throws Exception {
        ChannelPipeline pipeline = Channels.pipeline();

        if (readTimeoutTimer == null) {
            readTimeoutTimer = new HashedWheelTimer();
        }

        pipeline.addLast("readTimeout", new ReadTimeoutHandler(readTimeoutTimer, 
                                                               conf.getReadTimeout()));
        pipeline.addLast("lengthbasedframedecoder", new LengthFieldBasedFrameDecoder(MAX_FRAME_LENGTH, 0, 4, 0, 4));
        pipeline.addLast("mainhandler", this);
        return pipeline;
    }

    /**
     * If our channel has disconnected, we just error out the pending entries
     */
    @Override
    public void channelDisconnected(ChannelHandlerContext ctx, ChannelStateEvent e) throws Exception {
        LOG.info("Disconnected from bookie: " + addr);
        errorOutOutstandingEntries();
        Channel c = this.channel;
        if (c != null) {
            c.close();
        }
        synchronized (this) {
            if (state != ConnectionState.CLOSED) {
                state = ConnectionState.DISCONNECTED;
            }
        }

        // we don't want to reconnect right away. If someone sends a request to
        // this address, we will reconnect.
    }

    /**
     * Called by netty when an exception happens in one of the netty threads
     * (mostly due to what we do in the netty threads)
     */
    @Override
    public void exceptionCaught(ChannelHandlerContext ctx, ExceptionEvent e) throws Exception {
        Throwable t = e.getCause();
        if (t instanceof CorruptedFrameException || t instanceof TooLongFrameException) {
            LOG.error("Corrupted fram received from bookie: "
                      + e.getChannel().getRemoteAddress());
            return;
        }
        if (t instanceof ReadTimeoutException) {
            for (CompletionKey key : addCompletions.keySet()) {
                if (key.shouldTimeout()) {
                    errorOutAddKey(key);
                }
            }
            for (CompletionKey key : readCompletions.keySet()) {
                if (key.shouldTimeout()) {
                    errorOutReadKey(key);
                }
            }
            return;
        }

        if (t instanceof IOException) {
            // these are thrown when a bookie fails, logging them just pollutes
            // the logs (the failure is logged from the listeners on the write
            // operation), so I'll just ignore it here.
            return;
        }

        LOG.error("Unexpected exception caught by bookie client channel handler", t);
        // Since we are a library, cant terminate App here, can we?
    }

    /**
     * Called by netty when a message is received on a channel
     */
    @Override
    public void messageReceived(ChannelHandlerContext ctx, MessageEvent e) throws Exception {
        if (!(e.getMessage() instanceof ChannelBuffer)) {
            ctx.sendUpstream(e);
            return;
        }

        final ChannelBuffer buffer = (ChannelBuffer) e.getMessage();
        final int rc;
        final long ledgerId, entryId;
        final PacketHeader header;

        try {
            header = PacketHeader.fromInt(buffer.readInt());
            rc = buffer.readInt();
            ledgerId = buffer.readLong();
            entryId = buffer.readLong();
        } catch (IndexOutOfBoundsException ex) {
            LOG.error("Unparseable response from bookie: " + addr, ex);
            return;
        }

        executor.submitOrdered(ledgerId, new SafeRunnable() {
            @Override
            public void safeRun() {
                switch (header.getOpCode()) {
                case BookieProtocol.ADDENTRY:
                    handleAddResponse(ledgerId, entryId, rc);
                    break;
                case BookieProtocol.READENTRY:
                    handleReadResponse(ledgerId, entryId, rc, buffer);
                    break;
                default:
                    LOG.error("Unexpected response, type: " + header.getOpCode() 
                              + " received from bookie: " + addr + " , ignoring");
                }
            }
        });
    }

    void handleAddResponse(long ledgerId, long entryId, int rc) {
        if (LOG.isDebugEnabled()) {
            LOG.debug("Got response for add request from bookie: " + addr + " for ledger: " + ledgerId + " entry: "
                      + entryId + " rc: " + rc);
        }

        // convert to BKException code because thats what the uppper
        // layers expect. This is UGLY, there should just be one set of
        // error codes.
        switch (rc) {
        case BookieProtocol.EOK:
            rc = BKException.Code.OK;
            break;
        case BookieProtocol.EBADVERSION:
            rc = BKException.Code.ProtocolVersionException;
            break;
        case BookieProtocol.EFENCED:
            rc = BKException.Code.LedgerFencedException;
            break;
        case BookieProtocol.EUA:
            rc = BKException.Code.UnauthorizedAccessException;
            break;
        case BookieProtocol.EREADONLY:
            rc = BKException.Code.WriteOnReadOnlyBookieException;
            break;
        default:
            LOG.error("Add for ledger: " + ledgerId + ", entry: " + entryId + " failed on bookie: " + addr
                      + " with code: " + rc);
            rc = BKException.Code.WriteException;
            break;
        }

        AddCompletion ac;
        ac = addCompletions.remove(new CompletionKey(ledgerId, entryId));
        if (ac == null) {
            LOG.error("Unexpected add response received from bookie: " + addr + " for ledger: " + ledgerId
                      + ", entry: " + entryId + " , ignoring");
            return;
        }

        // totalBytesOutstanding.addAndGet(-ac.size);

        ac.cb.writeComplete(rc, ledgerId, entryId, addr, ac.ctx);

    }

    void handleReadResponse(long ledgerId, long entryId, int rc, ChannelBuffer buffer) {
        if (LOG.isDebugEnabled()) {
            LOG.debug("Got response for read request from bookie: " + addr + " for ledger: " + ledgerId + " entry: "
                      + entryId + " rc: " + rc + " entry length: " + buffer.readableBytes());
        }

        // convert to BKException code because thats what the uppper
        // layers expect. This is UGLY, there should just be one set of
        // error codes.
        if (rc == BookieProtocol.EOK) {
            rc = BKException.Code.OK;
        } else if (rc == BookieProtocol.ENOENTRY || rc == BookieProtocol.ENOLEDGER) {
            rc = BKException.Code.NoSuchEntryException;
        } else if (rc == BookieProtocol.EBADVERSION) {
            rc = BKException.Code.ProtocolVersionException;
        } else if (rc == BookieProtocol.EUA) {
            rc = BKException.Code.UnauthorizedAccessException;
        } else {
            LOG.error("Read for ledger: " + ledgerId + ", entry: " + entryId + " failed on bookie: " + addr
                      + " with code: " + rc);
            rc = BKException.Code.ReadException;
        }

        CompletionKey key = new CompletionKey(ledgerId, entryId);
        ReadCompletion readCompletion = readCompletions.remove(key);

        if (readCompletion == null) {
            /*
             * This is a special case. When recovering a ledger, a client
             * submits a read request with id -1, and receives a response with a
             * different entry id.
             */
            
            readCompletion = readCompletions.remove(new CompletionKey(ledgerId, BookieProtocol.LAST_ADD_CONFIRMED));
        }

        if (readCompletion == null) {
            LOG.error("Unexpected read response received from bookie: " + addr + " for ledger: " + ledgerId
                      + ", entry: " + entryId + " , ignoring");
            return;
        }

        readCompletion.cb.readEntryComplete(rc, ledgerId, entryId, buffer.slice(), readCompletion.ctx);
    }

    /**
     * Boiler-plate wrapper classes follow
     *
     */
    // visible for testing
    static class ReadCompletion {
        final ReadEntryCallback cb;
        final Object ctx;

        public ReadCompletion(ReadEntryCallback cb, Object ctx) {
            this.cb = cb;
            this.ctx = ctx;
        }
    }

    // visible for testing
    static class AddCompletion {
        final WriteCallback cb;
        //final long size;
        final Object ctx;

        public AddCompletion(WriteCallback cb, long size, Object ctx) {
            this.cb = cb;
            //this.size = size;
            this.ctx = ctx;
        }
    }

    // visable for testing
    CompletionKey newCompletionKey(long ledgerId, long entryId) {
        return new CompletionKey(ledgerId, entryId);
    }

    // visable for testing
    class CompletionKey {
        long ledgerId;
        long entryId;
        final long timeoutAt;

        CompletionKey(long ledgerId, long entryId) {
            this.ledgerId = ledgerId;
            this.entryId = entryId;
            this.timeoutAt = MathUtils.now() + (conf.getReadTimeout()*1000);
        }

        @Override
        public boolean equals(Object obj) {
            if (!(obj instanceof CompletionKey) || obj == null) {
                return false;
            }
            CompletionKey that = (CompletionKey) obj;
            return this.ledgerId == that.ledgerId && this.entryId == that.entryId;
        }

        @Override
        public int hashCode() {
            return ((int) ledgerId << 16) ^ ((int) entryId);
        }

        public String toString() {
            return String.format("LedgerEntry(%d, %d)", ledgerId, entryId);
        }

        public boolean shouldTimeout() {
            return this.timeoutAt <= MathUtils.now();
        }
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/proto/ServerStats.java,true,"/**
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.bookkeeper.proto;

import org.apache.bookkeeper.util.MathUtils;

public class ServerStats {
    private static ServerStats instance = new ServerStats();
    private long packetsSent;
    private long packetsReceived;
    private long maxLatency;
    private long minLatency = Long.MAX_VALUE;
    private long totalLatency = 0;
    private long count = 0;

    public interface Provider {
        public long getOutstandingRequests();

        public long getLastProcessedZxid();
    }

    private Provider provider = null;
    private Object mutex = new Object();

    static public ServerStats getInstance() {
        return instance;
    }

    static public void registerAsConcrete() {
        setInstance(new ServerStats());
    }

    static synchronized public void unregister() {
        instance = null;
    }

    static synchronized protected void setInstance(ServerStats newInstance) {
        assert instance == null;
        instance = newInstance;
    }

    protected ServerStats() {
    }

    // getters
    synchronized public long getMinLatency() {
        return (minLatency == Long.MAX_VALUE) ? 0 : minLatency;
    }

    synchronized public long getAvgLatency() {
        if (count != 0)
            return totalLatency / count;
        return 0;
    }

    synchronized public long getMaxLatency() {
        return maxLatency;
    }

    public long getOutstandingRequests() {
        synchronized (mutex) {
            return (provider != null) ? provider.getOutstandingRequests() : -1;
        }
    }

    public long getLastProcessedZxid() {
        synchronized (mutex) {
            return (provider != null) ? provider.getLastProcessedZxid() : -1;
        }
    }

    synchronized public long getPacketsReceived() {
        return packetsReceived;
    }

    synchronized public long getPacketsSent() {
        return packetsSent;
    }

    public String getServerState() {
        return "standalone";
    }

    @Override
    public String toString() {
        StringBuilder sb = new StringBuilder();
        sb.append("Latency min/avg/max: " + getMinLatency() + "/" + getAvgLatency() + "/" + getMaxLatency() + "\n");
        sb.append("Received: " + getPacketsReceived() + "\n");
        sb.append("Sent: " + getPacketsSent() + "\n");
        if (provider != null) {
            sb.append("Outstanding: " + getOutstandingRequests() + "\n");
            sb.append("Zxid: 0x" + Long.toHexString(getLastProcessedZxid()) + "\n");
        }
        sb.append("Mode: " + getServerState() + "\n");
        return sb.toString();
    }

    // mutators
    public void setStatsProvider(Provider zk) {
        synchronized (mutex) {
            provider = zk;
        }
    }

    synchronized void updateLatency(long requestCreateTime) {
        long latency = MathUtils.now() - requestCreateTime;
        totalLatency += latency;
        count++;
        if (latency < minLatency) {
            minLatency = latency;
        }
        if (latency > maxLatency) {
            maxLatency = latency;
        }
    }

    synchronized public void resetLatency() {
        totalLatency = count = maxLatency = 0;
        minLatency = Long.MAX_VALUE;
    }

    synchronized public void resetMaxLatency() {
        maxLatency = getMinLatency();
    }

    synchronized public void incrementPacketsReceived() {
        packetsReceived++;
    }

    synchronized public void incrementPacketsSent() {
        packetsSent++;
    }

    synchronized public void resetRequestCounters() {
        packetsReceived = packetsSent = 0;
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/replication/Auditor.java,true,"/**
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */
package org.apache.bookkeeper.replication;

import java.io.IOException;
import java.util.Collection;
import java.util.List;
import java.util.ArrayList;
import java.util.Map;
import java.util.Set;

import java.util.concurrent.TimeUnit;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.ThreadFactory;

import java.net.InetSocketAddress;

import java.util.concurrent.CountDownLatch;
import java.util.concurrent.atomic.AtomicInteger;

import org.apache.bookkeeper.conf.ClientConfiguration;
import org.apache.bookkeeper.client.BookKeeperAdmin;
import org.apache.bookkeeper.client.BookKeeper;
import org.apache.bookkeeper.client.LedgerHandle;
import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.client.LedgerChecker;
import org.apache.bookkeeper.client.LedgerFragment;
import org.apache.bookkeeper.util.StringUtils;

import org.apache.bookkeeper.util.ZkUtils;
import org.apache.bookkeeper.zookeeper.ZooKeeperWatcherBase;

import org.apache.bookkeeper.conf.ServerConfiguration;
import org.apache.bookkeeper.meta.LedgerManagerFactory;
import org.apache.bookkeeper.meta.LedgerManager;
import org.apache.bookkeeper.meta.LedgerUnderreplicationManager;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.Processor;

import org.apache.bookkeeper.replication.ReplicationException.BKAuditException;
import org.apache.bookkeeper.replication.ReplicationException.CompatibilityException;
import org.apache.bookkeeper.replication.ReplicationException.UnavailableException;
import org.apache.commons.collections.CollectionUtils;
import com.google.common.collect.Sets;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.WatchedEvent;
import org.apache.zookeeper.Watcher;
import org.apache.zookeeper.ZooKeeper;
import org.apache.zookeeper.AsyncCallback;
import org.apache.zookeeper.Watcher.Event.EventType;
import org.apache.zookeeper.Watcher.Event.KeeperState;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Auditor is a single entity in the entire Bookie cluster and will be watching
 * all the bookies under 'ledgerrootpath/available' zkpath. When any of the
 * bookie failed or disconnected from zk, he will start initiating the
 * re-replication activities by keeping all the corresponding ledgers of the
 * failed bookie as underreplicated znode in zk.
 */
public class Auditor implements Watcher {
    private static final Logger LOG = LoggerFactory.getLogger(Auditor.class);
    private final ServerConfiguration conf;
    private final ZooKeeper zkc;
    private BookieLedgerIndexer bookieLedgerIndexer;
    private LedgerManager ledgerManager;
    private LedgerUnderreplicationManager ledgerUnderreplicationManager;
    private final ScheduledExecutorService executor;
    private List<String> knownBookies = new ArrayList<String>();

    public Auditor(final String bookieIdentifier, ServerConfiguration conf,
                   ZooKeeper zkc) throws UnavailableException {
        this.conf = conf;
        this.zkc = zkc;
        initialize(conf, zkc);

        executor = Executors.newSingleThreadScheduledExecutor(new ThreadFactory() {
                @Override
                public Thread newThread(Runnable r) {
                    Thread t = new Thread(r, "AuditorBookie-" + bookieIdentifier);
                    t.setDaemon(true);
                    return t;
                }
            });
    }

    private void initialize(ServerConfiguration conf, ZooKeeper zkc)
            throws UnavailableException {
        try {
            LedgerManagerFactory ledgerManagerFactory = LedgerManagerFactory
                    .newLedgerManagerFactory(conf, zkc);
            ledgerManager = ledgerManagerFactory.newLedgerManager();
            this.bookieLedgerIndexer = new BookieLedgerIndexer(ledgerManager);

            this.ledgerUnderreplicationManager = ledgerManagerFactory
                    .newLedgerUnderreplicationManager();

        } catch (CompatibilityException ce) {
            throw new UnavailableException(
                    "CompatibilityException while initializing Auditor", ce);
        } catch (IOException ioe) {
            throw new UnavailableException(
                    "IOException while initializing Auditor", ioe);
        } catch (KeeperException ke) {
            throw new UnavailableException(
                    "KeeperException while initializing Auditor", ke);
        } catch (InterruptedException ie) {
            throw new UnavailableException(
                    "Interrupted while initializing Auditor", ie);
        }
    }

    private void submitShutdownTask() {
        synchronized (this) {
            if (executor.isShutdown()) {
                return;
            }
            executor.submit(new Runnable() {
                    public void run() {
                        synchronized (Auditor.this) {
                            executor.shutdown();
                        }
                    }
                });
        }
    }

    private synchronized void submitAuditTask() {
        synchronized (this) {
            if (executor.isShutdown()) {
                return;
            }
            executor.submit(new Runnable() {
                    public void run() {
                        try {
                            waitIfLedgerReplicationDisabled();

                            List<String> availableBookies = getAvailableBookies();

                            // casting to String, as knownBookies and availableBookies
                            // contains only String values
                            // find new bookies(if any) and update the known bookie list
                            Collection<String> newBookies = CollectionUtils.subtract(
                                    availableBookies, knownBookies);
                            knownBookies.addAll(newBookies);

                            // find lost bookies(if any)
                            Collection<String> lostBookies = CollectionUtils.subtract(
                                    knownBookies, availableBookies);

                            if (lostBookies.size() > 0) {
                                knownBookies.removeAll(lostBookies);
                                Map<String, Set<Long>> ledgerDetails = generateBookie2LedgersIndex();
                                handleLostBookies(lostBookies, ledgerDetails);
                            }
                        } catch (KeeperException ke) {
                            LOG.error("Exception while watching available bookies", ke);
                        } catch (InterruptedException ie) {
                            Thread.currentThread().interrupt();
                            LOG.error("Interrupted while watching available bookies ", ie);
                        } catch (BKAuditException bke) {
                            LOG.error("Exception while watching available bookies", bke);
                        } catch (UnavailableException ue) {
                            LOG.error("Exception while watching available bookies", ue);
                        }
                    }
                });
        }
    }

    public void start() {
        LOG.info("I'm starting as Auditor Bookie");
        // on startup watching available bookie and based on the
        // available bookies determining the bookie failures.
        synchronized (this) {
            if (executor.isShutdown()) {
                return;
            }

            long interval = conf.getAuditorPeriodicCheckInterval();
            if (interval > 0) {
                LOG.info("Periodic checking enabled");
                executor.scheduleAtFixedRate(new Runnable() {
                        public void run() {
                            LOG.info("Running periodic check");

                            try {
                                if (!ledgerUnderreplicationManager.isLedgerReplicationEnabled()) {
                                    LOG.info("Ledger replication disabled, skipping");
                                    return;
                                }

                                checkAllLedgers();
                            } catch (KeeperException ke) {
                                LOG.error("Exception while running periodic check", ke);
                            } catch (InterruptedException ie) {
                                Thread.currentThread().interrupt();
                                LOG.error("Interrupted while running periodic check", ie);
                            } catch (BKAuditException bkae) {
                                LOG.error("Exception while running periodic check", bkae);
                            } catch (BKException bke) {
                                LOG.error("Exception running periodic check", bke);
                            } catch (IOException ioe) {
                                LOG.error("I/O exception running periodic check", ioe);
                            } catch (ReplicationException.UnavailableException ue) {
                                LOG.error("Underreplication manager unavailable "
                                          +"running periodic check", ue);
                            }
                        }
                    }, interval, interval, TimeUnit.MILLISECONDS);
            } else {
                LOG.info("Periodic checking disabled");
            }

            executor.submit(new Runnable() {
                    public void run() {
                        try {
                            knownBookies = getAvailableBookies();
                            auditingBookies(knownBookies);
                        } catch (KeeperException ke) {
                            LOG.error("Exception while watching available bookies", ke);
                            submitShutdownTask();
                        } catch (InterruptedException ie) {
                            Thread.currentThread().interrupt();
                            LOG.error("Interrupted while watching available bookies ", ie);
                            submitShutdownTask();
                        } catch (BKAuditException bke) {
                            LOG.error("Exception while watching available bookies", bke);
                            submitShutdownTask();
                        }
                    }
                });
        }
    }

    private void waitIfLedgerReplicationDisabled() throws UnavailableException,
            InterruptedException {
        ReplicationEnableCb cb = new ReplicationEnableCb();
        if (!ledgerUnderreplicationManager.isLedgerReplicationEnabled()) {
            ledgerUnderreplicationManager.notifyLedgerReplicationEnabled(cb);
            cb.await();
        }
    }
    
    private List<String> getAvailableBookies() throws KeeperException,
            InterruptedException {
        return zkc.getChildren(conf.getZkAvailableBookiesPath(), this);
    }

    private void auditingBookies(List<String> availableBookies)
            throws BKAuditException, KeeperException, InterruptedException {

        Map<String, Set<Long>> ledgerDetails = generateBookie2LedgersIndex();

        // find lost bookies
        Set<String> knownBookies = ledgerDetails.keySet();
        Collection<String> lostBookies = CollectionUtils.subtract(knownBookies,
                availableBookies);

        if (lostBookies.size() > 0)
            handleLostBookies(lostBookies, ledgerDetails);
    }

    private Map<String, Set<Long>> generateBookie2LedgersIndex()
            throws BKAuditException {
        return bookieLedgerIndexer.getBookieToLedgerIndex();
    }

    private void handleLostBookies(Collection<String> lostBookies,
            Map<String, Set<Long>> ledgerDetails) throws BKAuditException,
            KeeperException, InterruptedException {
        LOG.info("Following are the failed bookies: " + lostBookies
                + " and searching its ledgers for re-replication");

        for (String bookieIP : lostBookies) {
            // identify all the ledgers in bookieIP and publishing these ledgers
            // as under-replicated.
            publishSuspectedLedgers(bookieIP, ledgerDetails.get(bookieIP));
        }
    }

    private void publishSuspectedLedgers(String bookieIP, Set<Long> ledgers)
            throws KeeperException, InterruptedException, BKAuditException {
        if (null == ledgers || ledgers.size() == 0) {
            // there is no ledgers available for this bookie and just
            // ignoring the bookie failures
            LOG.info("There is no ledgers for the failed bookie: " + bookieIP);
            return;
        }
        LOG.info("Following ledgers: " + ledgers + " of bookie: " + bookieIP
                + " are identified as underreplicated");
        for (Long ledgerId : ledgers) {
            try {
                ledgerUnderreplicationManager.markLedgerUnderreplicated(
                        ledgerId, bookieIP);
            } catch (UnavailableException ue) {
                throw new BKAuditException(
                        "Failed to publish underreplicated ledger: " + ledgerId
                                + " of bookie: " + bookieIP, ue);
            }
        }
    }

    /**
     * Process the result returned from checking a ledger
     */
    private class ProcessLostFragmentsCb implements GenericCallback<Set<LedgerFragment>> {
        final LedgerHandle lh;
        final AsyncCallback.VoidCallback callback;

        ProcessLostFragmentsCb(LedgerHandle lh, AsyncCallback.VoidCallback callback) {
            this.lh = lh;
            this.callback = callback;
        }

        public void operationComplete(int rc, Set<LedgerFragment> fragments) {
            try {
                if (rc == BKException.Code.OK) {
                    Set<InetSocketAddress> bookies = Sets.newHashSet();
                    for (LedgerFragment f : fragments) {
                        bookies.add(f.getAddress());
                    }
                    for (InetSocketAddress bookie : bookies) {
                        publishSuspectedLedgers(StringUtils.addrToString(bookie),
                                                Sets.newHashSet(lh.getId()));
                    }
                }
                lh.close();
            } catch (BKException bke) {
                LOG.error("Error closing lh", bke);
                if (rc == BKException.Code.OK) {
                    rc = BKException.Code.ZKException;
                }
            } catch (KeeperException ke) {
                LOG.error("Couldn't publish suspected ledger", ke);
                if (rc == BKException.Code.OK) {
                    rc = BKException.Code.ZKException;
                }
            } catch (InterruptedException ie) {
                LOG.error("Interrupted publishing suspected ledger", ie);
                Thread.currentThread().interrupt();
                if (rc == BKException.Code.OK) {
                    rc = BKException.Code.InterruptedException;
                }
            } catch (BKAuditException bkae) {
                LOG.error("Auditor exception publishing suspected ledger", bkae);
                if (rc == BKException.Code.OK) {
                    rc = BKException.Code.ZKException;
                }
            }

            callback.processResult(rc, null, null);
        }
    }

    /**
     * List all the ledgers and check them individually. This should not
     * be run very often.
     */
    private void checkAllLedgers() throws BKAuditException, BKException,
            IOException, InterruptedException, KeeperException {
        ZooKeeperWatcherBase w = new ZooKeeperWatcherBase(conf.getZkTimeout());
        ZooKeeper newzk = ZkUtils.createConnectedZookeeperClient(conf.getZkServers(), w);

        final BookKeeper client = new BookKeeper(new ClientConfiguration(conf),
                                                 newzk);
        final BookKeeperAdmin admin = new BookKeeperAdmin(client);

        try {
            final LedgerChecker checker = new LedgerChecker(client);

            final AtomicInteger returnCode = new AtomicInteger(BKException.Code.OK);
            final CountDownLatch processDone = new CountDownLatch(1);

            Processor<Long> checkLedgersProcessor = new Processor<Long>() {
                @Override
                public void process(final Long ledgerId,
                                    final AsyncCallback.VoidCallback callback) {
                    try {
                        if (!ledgerUnderreplicationManager.isLedgerReplicationEnabled()) {
                            LOG.info("Ledger rereplication has been disabled, aborting periodic check");
                            processDone.countDown();
                            return;
                        }
                    } catch (ReplicationException.UnavailableException ue) {
                        LOG.error("Underreplication manager unavailable "
                                  +"running periodic check", ue);
                        processDone.countDown();
                        return;
                    }

                    LedgerHandle lh = null;
                    try {
                        lh = admin.openLedgerNoRecovery(ledgerId);
                        checker.checkLedger(lh, new ProcessLostFragmentsCb(lh, callback));
                    } catch (BKException bke) {
                        LOG.error("Couldn't open ledger " + ledgerId, bke);
                        callback.processResult(BKException.Code.BookieHandleNotAvailableException,
                                         null, null);
                        return;
                    } catch (InterruptedException ie) {
                        LOG.error("Interrupted opening ledger", ie);
                        Thread.currentThread().interrupt();
                        callback.processResult(BKException.Code.InterruptedException, null, null);
                        return;
                    } finally {
                        if (lh != null) {
                            try {
                                lh.close();
                            } catch (BKException bke) {
                                LOG.warn("Couldn't close ledger " + ledgerId, bke);
                            } catch (InterruptedException ie) {
                                LOG.warn("Interrupted closing ledger " + ledgerId, ie);
                                Thread.currentThread().interrupt();
                            }
                        }
                    }
                }
            };

            ledgerManager.asyncProcessLedgers(checkLedgersProcessor,
                    new AsyncCallback.VoidCallback() {
                        @Override
                        public void processResult(int rc, String s, Object obj) {
                            returnCode.set(rc);
                            processDone.countDown();
                        }
                    }, null, BKException.Code.OK, BKException.Code.ReadException);
            try {
                processDone.await();
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
                throw new BKAuditException(
                        "Exception while checking ledgers", e);
            }
            if (returnCode.get() != BKException.Code.OK) {
                throw BKException.create(returnCode.get());
            }
        } finally {
            admin.close();
            client.close();
            newzk.close();
        }
    }

    @Override
    public void process(WatchedEvent event) {
        // listen children changed event from ZooKeeper
        if (event.getState() == KeeperState.Disconnected
                || event.getState() == KeeperState.Expired) {
            submitShutdownTask();
        } else if (event.getType() == EventType.NodeChildrenChanged) {
            submitAuditTask();
        }
    }

    /**
     * Shutdown the auditor
     */
    public void shutdown() {
        LOG.info("Shutting down auditor");
        submitShutdownTask();

        try {
            while (!executor.awaitTermination(30, TimeUnit.SECONDS)) {
                LOG.warn("Executor not shutting down, interrupting");
                executor.shutdownNow();
            }
        } catch (InterruptedException ie) {
            Thread.currentThread().interrupt();
            LOG.warn("Interrupted while shutting down auditor bookie", ie);
        }
    }

    /**
     * Return true if auditor is running otherwise return false
     * 
     * @return auditor status
     */
    public boolean isRunning() {
        return !executor.isShutdown();
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/replication/AuditorElector.java,true,"/**
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */
package org.apache.bookkeeper.replication;

import java.util.Collections;
import java.util.Comparator;
import java.util.List;
import java.io.Serializable;

import org.apache.bookkeeper.proto.DataFormats.AuditorVoteFormat;
import com.google.common.annotations.VisibleForTesting;
import java.util.concurrent.Executors;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.ThreadFactory;
import java.util.concurrent.atomic.AtomicBoolean;

import org.apache.bookkeeper.conf.ServerConfiguration;
import org.apache.bookkeeper.replication.ReplicationException.UnavailableException;
import org.apache.bookkeeper.util.BookKeeperConstants;
import org.apache.commons.lang.StringUtils;
import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.WatchedEvent;
import org.apache.zookeeper.Watcher;
import org.apache.zookeeper.ZooKeeper;
import org.apache.zookeeper.Watcher.Event.EventType;
import org.apache.zookeeper.Watcher.Event.KeeperState;
import org.apache.zookeeper.ZooDefs.Ids;
import com.google.protobuf.TextFormat;
import static com.google.common.base.Charsets.UTF_8;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Performing auditor election using Apache ZooKeeper. Using ZooKeeper as a
 * coordination service, when a bookie bids for auditor, it creates an ephemeral
 * sequential file (znode) on ZooKeeper and considered as their vote. Vote
 * format is 'V_sequencenumber'. Election will be done by comparing the
 * ephemeral sequential numbers and the bookie which has created the least znode
 * will be elected as Auditor. All the other bookies will be watching on their
 * predecessor znode according to the ephemeral sequence numbers.
 */
public class AuditorElector {
    private static final Logger LOG = LoggerFactory
            .getLogger(AuditorElector.class);
    // Represents the index of the auditor node
    private static final int AUDITOR_INDEX = 0;
    // Represents vote prefix
    private static final String VOTE_PREFIX = "V_";
    // Represents path Separator
    private static final String PATH_SEPARATOR = "/";
    // Represents urLedger path in zk
    private final String basePath;
    // Represents auditor election path in zk
    private final String electionPath;

    private final String bookieId;
    private final ServerConfiguration conf;
    private final ZooKeeper zkc;
    private final ExecutorService executor;

    private String myVote;
    Auditor auditor;
    private AtomicBoolean running = new AtomicBoolean(false);


    /**
     * AuditorElector for performing the auditor election
     * 
     * @param bookieId
     *            - bookie identifier, comprises HostAddress:Port
     * @param conf
     *            - configuration
     * @param zkc
     *            - ZK instance
     * @throws UnavailableException
     *             throws unavailable exception while initializing the elector
     */
    public AuditorElector(final String bookieId, ServerConfiguration conf,
                          ZooKeeper zkc) throws UnavailableException {
        this.bookieId = bookieId;
        this.conf = conf;
        this.zkc = zkc;
        basePath = conf.getZkLedgersRootPath() + '/'
                + BookKeeperConstants.UNDER_REPLICATION_NODE;
        electionPath = basePath + "/auditorelection";
        createElectorPath();
        executor = Executors.newSingleThreadExecutor(new ThreadFactory() {
                @Override
                public Thread newThread(Runnable r) {
                    return new Thread(r, "AuditorElector-"+bookieId);
                }
            });
    }



    private void createMyVote() throws KeeperException, InterruptedException {
        if (null == myVote || null == zkc.exists(myVote, false)) {
            AuditorVoteFormat.Builder builder = AuditorVoteFormat.newBuilder()
                .setBookieId(bookieId);
            myVote = zkc.create(getVotePath(PATH_SEPARATOR + VOTE_PREFIX),
                    TextFormat.printToString(builder.build()).getBytes(UTF_8), Ids.OPEN_ACL_UNSAFE,
                    CreateMode.EPHEMERAL_SEQUENTIAL);
        }
    }

    private String getVotePath(String vote) {
        return electionPath + vote;
    }

    private void createElectorPath() throws UnavailableException {
        try {
            if (zkc.exists(basePath, false) == null) {
                try {
                    zkc.create(basePath, new byte[0], Ids.OPEN_ACL_UNSAFE,
                            CreateMode.PERSISTENT);
                } catch (KeeperException.NodeExistsException nee) {
                    // do nothing, someone else could have created it
                }
            }
            if (zkc.exists(getVotePath(""), false) == null) {
                try {
                    zkc.create(getVotePath(""), new byte[0],
                            Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
                } catch (KeeperException.NodeExistsException nee) {
                    // do nothing, someone else could have created it
                }
            }
        } catch (KeeperException ke) {
            throw new UnavailableException(
                    "Failed to initialize Auditor Elector", ke);
        } catch (InterruptedException ie) {
            Thread.currentThread().interrupt();
            throw new UnavailableException(
                    "Failed to initialize Auditor Elector", ie);
        }
    }

    /**
     * Watching the predecessor bookies and will do election on predecessor node
     * deletion or expiration.
     */
    private class ElectionWatcher implements Watcher {
        @Override
        public void process(WatchedEvent event) {
            if (event.getState() == KeeperState.Disconnected
                || event.getState() == KeeperState.Expired) {
                LOG.error("Lost ZK connection, shutting down");
                submitShutdownTask();
            } else if (event.getType() == EventType.NodeDeleted) {
                submitElectionTask();
            }
        }
    }

    public void start() {
        running.set(true);
        submitElectionTask();
    }

    /**
     * Run cleanup operations for the auditor elector.
     */
    private void submitShutdownTask() {
        executor.submit(new Runnable() {
                public void run() {
                    if (!running.compareAndSet(true, false)) {
                        return;
                    }
                    LOG.info("Shutting down AuditorElector");
                    if (myVote != null) {
                        try {
                            zkc.delete(myVote, -1);
                        } catch (InterruptedException ie) {
                            LOG.warn("InterruptedException while deleting myVote: " + myVote,
                                     ie);
                        } catch (KeeperException ke) {
                            LOG.error("Exception while deleting myVote:" + myVote, ke);
                        }
                    }
                }
            });
    }

    /**
     * Performing the auditor election using the ZooKeeper ephemeral sequential
     * znode. The bookie which has created the least sequential will be elect as
     * Auditor.
     */
    @VisibleForTesting
    void submitElectionTask() {

        Runnable r = new Runnable() {
                public void run() {
                    try {
                        // creating my vote in zk. Vote format is 'V_numeric'
                        createMyVote();
                        List<String> children = zkc.getChildren(getVotePath(""), false);

                        if (0 >= children.size()) {
                            throw new IllegalArgumentException(
                                    "Atleast one bookie server should present to elect the Auditor!");
                        }

                        // sorting in ascending order of sequential number
                        Collections.sort(children, new ElectionComparator());
                        String voteNode = StringUtils.substringAfterLast(myVote,
                                                                         PATH_SEPARATOR);

                        // starting Auditing service
                        if (children.get(AUDITOR_INDEX).equals(voteNode)) {
                            // update the auditor bookie id in the election path. This is
                            // done for debugging purpose
                            AuditorVoteFormat.Builder builder = AuditorVoteFormat.newBuilder()
                                .setBookieId(bookieId);

                            zkc.setData(getVotePath(""),
                                        TextFormat.printToString(builder.build()).getBytes(UTF_8), -1);
                            auditor = new Auditor(bookieId, conf, zkc);
                            auditor.start();
                        } else {
                            // If not an auditor, will be watching to my predecessor and
                            // looking the previous node deletion.
                            Watcher electionWatcher = new ElectionWatcher();
                            int myIndex = children.indexOf(voteNode);
                            int prevNodeIndex = myIndex - 1;
                            if (null == zkc.exists(getVotePath(PATH_SEPARATOR)
                                                   + children.get(prevNodeIndex), electionWatcher)) {
                                // While adding, the previous znode doesn't exists.
                                // Again going to election.
                                submitElectionTask();
                            }
                        }
                    } catch (KeeperException e) {
                        LOG.error("Exception while performing auditor election", e);
                        submitShutdownTask();
                    } catch (InterruptedException e) {
                        LOG.error("Interrupted while performing auditor election", e);
                        Thread.currentThread().interrupt();
                        submitShutdownTask();
                    } catch (UnavailableException e) {
                        LOG.error("Ledger underreplication manager unavailable during election", e);
                        submitShutdownTask();
                    }
                }
            };
        executor.submit(r);
    }

    /**
     * Shutting down AuditorElector
     */
    public void shutdown() throws InterruptedException {
        synchronized (this) {
            if (executor.isShutdown()) {
                return;
            }
            submitShutdownTask();
            executor.shutdown();
        }

        if (auditor != null) {
            auditor.shutdown();
            auditor = null;
        }
    }

    /**
     * If current bookie is running as auditor, return the status of the
     * auditor. Otherwise return the status of elector.
     * 
     * @return
     */
    public boolean isRunning() {
        if (auditor != null) {
            return auditor.isRunning();
        }
        return running.get();
    }

    /**
     * Compare the votes in the ascending order of the sequence number. Vote
     * format is 'V_sequencenumber', comparator will do sorting based on the
     * numeric sequence value.
     */
    private static class ElectionComparator
        implements Comparator<String>, Serializable {
        /**
         * Return -1 if the first vote is less than second. Return 1 if the
         * first vote is greater than second. Return 0 if the votes are equal.
         */
        public int compare(String vote1, String vote2) {
            long voteSeqId1 = getVoteSequenceId(vote1);
            long voteSeqId2 = getVoteSequenceId(vote2);
            int result = voteSeqId1 < voteSeqId2 ? -1
                    : (voteSeqId1 > voteSeqId2 ? 1 : 0);
            return result;
        }

        private long getVoteSequenceId(String vote) {
            String voteId = StringUtils.substringAfter(vote, VOTE_PREFIX);
            return Long.parseLong(voteId);
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/replication/AutoRecoveryMain.java,true,"/**
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */
package org.apache.bookkeeper.replication;

import java.io.File;
import java.io.IOException;
import java.net.MalformedURLException;

import org.apache.bookkeeper.bookie.Bookie;
import org.apache.bookkeeper.bookie.ExitCode;
import org.apache.bookkeeper.conf.ServerConfiguration;
import org.apache.bookkeeper.replication.ReplicationException.CompatibilityException;
import org.apache.bookkeeper.replication.ReplicationException.UnavailableException;
import org.apache.bookkeeper.util.ZkUtils;
import org.apache.bookkeeper.util.StringUtils;
import org.apache.bookkeeper.zookeeper.ZooKeeperWatcherBase;
import org.apache.commons.cli.BasicParser;
import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.HelpFormatter;
import org.apache.commons.cli.Options;
import org.apache.commons.cli.ParseException;
import org.apache.commons.configuration.ConfigurationException;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.WatchedEvent;
import org.apache.zookeeper.Watcher;
import org.apache.zookeeper.ZooKeeper;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Class to start/stop the AutoRecovery daemons Auditor and ReplicationWorker
 */
public class AutoRecoveryMain {
    private static final Logger LOG = LoggerFactory
            .getLogger(AutoRecoveryMain.class);

    private ServerConfiguration conf;
    private ZooKeeper zk;
    AuditorElector auditorElector;
    ReplicationWorker replicationWorker;
    private AutoRecoveryDeathWatcher deathWatcher;
    private int exitCode;
    private volatile boolean shuttingDown = false;
    private volatile boolean running = false;

    public AutoRecoveryMain(ServerConfiguration conf) throws IOException,
            InterruptedException, KeeperException, UnavailableException,
            CompatibilityException {
        this.conf = conf;
        ZooKeeperWatcherBase w = new ZooKeeperWatcherBase(conf.getZkTimeout()) {
            @Override
            public void process(WatchedEvent event) {
                // Check for expired connection.
                if (event.getState().equals(Watcher.Event.KeeperState.Expired)) {
                    LOG.error("ZK client connection to the"
                            + " ZK server has expired!");
                    shutdown(ExitCode.ZK_EXPIRED);
                } else {
                    super.process(event);
                }
            }
        };
        zk = ZkUtils.createConnectedZookeeperClient(conf.getZkServers(), w);
        auditorElector = new AuditorElector(
                StringUtils.addrToString(Bookie.getBookieAddress(conf)), conf, zk);
        replicationWorker = new ReplicationWorker(zk, conf,
                Bookie.getBookieAddress(conf));
        deathWatcher = new AutoRecoveryDeathWatcher(this);
    }

    /*
     * Start daemons
     */
    public void start() throws UnavailableException {
        auditorElector.start();
        replicationWorker.start();
        deathWatcher.start();
        running = true;
    }

    /*
     * Waits till all daemons joins
     */
    public void join() throws InterruptedException {
        deathWatcher.join();
    }

    /*
     * Shutdown all daemons gracefully
     */
    public void shutdown() {
        shutdown(ExitCode.OK);
    }

    private void shutdown(int exitCode) {
        if (shuttingDown) {
            return;
        }
        shuttingDown = true;
        running = false;
        this.exitCode = exitCode;
        try {
            deathWatcher.interrupt();
            deathWatcher.join();

            auditorElector.shutdown();
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            LOG.warn("Interrupted shutting down auto recovery", e);
        }

        replicationWorker.shutdown();
        try {
            zk.close();
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            LOG.warn("Interrupted shutting down auto recovery", e);
        }
    }

    private int getExitCode() {
        return exitCode;
    }

    /** Is auto-recovery service running? */
    public boolean isAutoRecoveryRunning() {
        return running;
    }

    /*
     * DeathWatcher for AutoRecovery daemons.
     */
    private static class AutoRecoveryDeathWatcher extends Thread {
        private int watchInterval;
        private AutoRecoveryMain autoRecoveryMain;

        public AutoRecoveryDeathWatcher(AutoRecoveryMain autoRecoveryMain) {
            super("AutoRecoveryDeathWatcher-"
                    + autoRecoveryMain.conf.getBookiePort());
            this.autoRecoveryMain = autoRecoveryMain;
            watchInterval = autoRecoveryMain.conf.getDeathWatchInterval();
        }

        @Override
        public void run() {
            while (true) {
                try {
                    Thread.sleep(watchInterval);
                } catch (InterruptedException ie) {
                    break;
                }
                // If any one service not running, then shutdown peer.
                if (!autoRecoveryMain.auditorElector.isRunning()
                    || !autoRecoveryMain.replicationWorker.isRunning()) {
                    autoRecoveryMain.shutdown();
                    break;
                }
            }
        }
    }

    private static final Options opts = new Options();
    static {
        opts.addOption("c", "conf", true, "Bookie server configuration");
        opts.addOption("h", "help", false, "Print help message");
    }

    /*
     * Print usage
     */
    private static void printUsage() {
        HelpFormatter hf = new HelpFormatter();
        hf.printHelp("AutoRecoveryMain [options]\n", opts);
    }

    /*
     * load configurations from file.
     */
    private static void loadConfFile(ServerConfiguration conf, String confFile)
            throws IllegalArgumentException {
        try {
            conf.loadConf(new File(confFile).toURI().toURL());
        } catch (MalformedURLException e) {
            LOG.error("Could not open configuration file: " + confFile, e);
            throw new IllegalArgumentException();
        } catch (ConfigurationException e) {
            LOG.error("Malformed configuration file: " + confFile, e);
            throw new IllegalArgumentException();
        }
        LOG.info("Using configuration file " + confFile);
    }

    /*
     * Parse console args
     */
    private static ServerConfiguration parseArgs(String[] args)
            throws IllegalArgumentException {
        try {
            BasicParser parser = new BasicParser();
            CommandLine cmdLine = parser.parse(opts, args);

            if (cmdLine.hasOption('h')) {
                throw new IllegalArgumentException();
            }

            ServerConfiguration conf = new ServerConfiguration();
            String[] leftArgs = cmdLine.getArgs();

            if (cmdLine.hasOption('c')) {
                if (null != leftArgs && leftArgs.length > 0) {
                    throw new IllegalArgumentException();
                }
                String confFile = cmdLine.getOptionValue("c");
                loadConfFile(conf, confFile);
            }

            if (null != leftArgs && leftArgs.length > 0) {
                throw new IllegalArgumentException();
            }
            return conf;
        } catch (ParseException e) {
            throw new IllegalArgumentException(e);
        }
    }

    public static void main(String[] args) {
        ServerConfiguration conf = null;
        try {
            conf = parseArgs(args);
        } catch (IllegalArgumentException iae) {
            LOG.error("Error parsing command line arguments : ", iae);
            System.err.println(iae.getMessage());
            printUsage();
            System.exit(ExitCode.INVALID_CONF);
        }

        try {
            final AutoRecoveryMain autoRecoveryMain = new AutoRecoveryMain(conf);
            autoRecoveryMain.start();
            Runtime.getRuntime().addShutdownHook(new Thread() {
                @Override
                public void run() {
                    autoRecoveryMain.shutdown();
                    LOG.info("Shutdown AutoRecoveryMain successfully");
                }
            });
            LOG.info("Register shutdown hook successfully");
            autoRecoveryMain.join();
            System.exit(autoRecoveryMain.getExitCode());
        } catch (Exception e) {
            LOG.error("Exception running AutoRecoveryMain : ", e);
            System.exit(ExitCode.SERVER_EXCEPTION);
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/replication/BookieLedgerIndexer.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.replication;

import java.net.InetSocketAddress;
import java.util.ArrayList;
import java.util.Collections;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.CountDownLatch;

import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.client.LedgerMetadata;
import org.apache.bookkeeper.meta.LedgerManager;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.Processor;
import org.apache.bookkeeper.replication.ReplicationException.BKAuditException;
import org.apache.bookkeeper.util.StringUtils;
import org.apache.zookeeper.AsyncCallback;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Preparing bookie vs its corresponding ledgers. This will always look up the
 * ledgermanager for ledger metadata and will generate indexes.
 */
public class BookieLedgerIndexer {

    private static final Logger LOG = LoggerFactory.getLogger(BookieLedgerIndexer.class);
    private final LedgerManager ledgerManager;

    public BookieLedgerIndexer(LedgerManager ledgerManager) {
        this.ledgerManager = ledgerManager;
    }

    /**
     * Generating bookie vs its ledgers map by reading all the ledgers in each
     * bookie and parsing its metadata.
     * 
     * @return bookie2ledgersMap map of bookie vs ledgers
     * @throws BKAuditException
     *             exception while getting bookie-ledgers
     */
    public Map<String, Set<Long>> getBookieToLedgerIndex()
            throws BKAuditException {
        // bookie vs ledgers map
        final ConcurrentHashMap<String, Set<Long>> bookie2ledgersMap
            = new ConcurrentHashMap<String, Set<Long>>();
        final CountDownLatch ledgerCollectorLatch = new CountDownLatch(1);

        Processor<Long> ledgerProcessor = new Processor<Long>() {
            @Override
            public void process(final Long ledgerId,
                    final AsyncCallback.VoidCallback iterCallback) {
                GenericCallback<LedgerMetadata> genericCallback = new GenericCallback<LedgerMetadata>() {
                    @Override
                    public void operationComplete(final int rc,
                            LedgerMetadata ledgerMetadata) {
                        if (rc == BKException.Code.OK) {
                            for (Map.Entry<Long, ArrayList<InetSocketAddress>> ensemble : ledgerMetadata
                                    .getEnsembles().entrySet()) {
                                for (InetSocketAddress bookie : ensemble
                                        .getValue()) {
                                    putLedger(bookie2ledgersMap,
                                              StringUtils.addrToString(bookie),
                                              ledgerId);
                                }
                            }
                        } else {
                            LOG.warn("Unable to read the ledger:" + ledgerId
                                    + " information");
                        }
                        iterCallback.processResult(rc, null, null);
                    }
                };
                ledgerManager.readLedgerMetadata(ledgerId, genericCallback);
            }
        };
        // Reading the result after processing all the ledgers
        final List<Integer> resultCode = new ArrayList<Integer>(1);
        ledgerManager.asyncProcessLedgers(ledgerProcessor,
                new AsyncCallback.VoidCallback() {

                    @Override
                    public void processResult(int rc, String s, Object obj) {
                        resultCode.add(rc);
                        ledgerCollectorLatch.countDown();
                    }
                }, null, BKException.Code.OK, BKException.Code.ReadException);
        try {
            ledgerCollectorLatch.await();
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            throw new BKAuditException(
                    "Exception while getting the bookie-ledgers", e);
        }
        if (!resultCode.contains(BKException.Code.OK)) {
            throw new BKAuditException(
                    "Exception while getting the bookie-ledgers", BKException
                            .create(resultCode.get(0)));
        }
        return bookie2ledgersMap;
    }

    private void putLedger(ConcurrentHashMap<String, Set<Long>> bookie2ledgersMap,
            String bookie, long ledgerId) {
        Set<Long> ledgers = bookie2ledgersMap.get(bookie);
        // creates an empty list and add to bookie for keeping its ledgers
        if (ledgers == null) {
            ledgers = Collections.synchronizedSet(new HashSet<Long>());
            Set<Long> oldLedgers = bookie2ledgersMap.putIfAbsent(bookie, ledgers);
            if (oldLedgers != null) {
                ledgers = oldLedgers;
            }
        }
        ledgers.add(ledgerId);
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/replication/ReplicationEnableCb.java,false,"/**
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */
package org.apache.bookkeeper.replication;

import java.util.concurrent.CountDownLatch;

import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Callback which is getting notified when the replication process is enabled
 */
public class ReplicationEnableCb implements GenericCallback<Void> {

    private static final Logger LOG = LoggerFactory
            .getLogger(ReplicationEnableCb.class);
    private final CountDownLatch latch = new CountDownLatch(1);

    @Override
    public void operationComplete(int rc, Void result) {
        latch.countDown();
        LOG.debug("Automatic ledger re-replication is enabled");
    }

    /**
     * This is a blocking call and causes the current thread to wait until the
     * replication process is enabled
     * 
     * @throws InterruptedException
     *             interrupted while waiting
     */
    public void await() throws InterruptedException {
        LOG.debug("Automatic ledger re-replication is disabled. "
                + "Hence waiting until its enabled!");
        latch.await();
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/replication/ReplicationException.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.bookkeeper.replication;

/**
 * Exceptions for use within the replication service
 */
public abstract class ReplicationException extends Exception {
    protected ReplicationException(String message, Throwable cause) {
        super(message, cause);
    }

    protected ReplicationException(String message) {
        super(message);
    }

    /**
     * The replication service has become unavailable
     */
    public static class UnavailableException extends ReplicationException {
        private static final long serialVersionUID = 31872209L;

        public UnavailableException(String message, Throwable cause) {
            super(message, cause);
        }

        public UnavailableException(String message) {
            super(message);
        }
    }

    /**
     * Compatibility error. This version of the code, doesn't know how to
     * deal with the metadata it has found.
     */
    public static class CompatibilityException extends ReplicationException {
        private static final long serialVersionUID = 98551903L;

        public CompatibilityException(String message, Throwable cause) {
            super(message, cause);
        }

        public CompatibilityException(String message) {
            super(message);
        }
    }

    /**
     * Exception while auditing bookie-ledgers
    */
    static class BKAuditException extends ReplicationException {
        private static final long serialVersionUID = 95551905L;

        BKAuditException(String message, Throwable cause) {
            super(message, cause);
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/replication/ReplicationWorker.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one 
 * or more contributor license agreements.  See the NOTICE file 
 * distributed with this work for additional information 
 * regarding copyright ownership.  The ASF licenses this file 
 * to you under the Apache License, Version 2.0 (the 
 * "License"); you may not use this file except in compliance 
 * with the License.  You may obtain a copy of the License at 
 * 
 *   http://www.apache.org/licenses/LICENSE-2.0 
 * 
 * Unless required by applicable law or agreed to in writing, 
 * software distributed under the License is distributed on an 
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY 
 * KIND, either express or implied.  See the License for the 
 * specific language governing permissions and limitations 
 * under the License. 
 * 
 */
package org.apache.bookkeeper.replication;

import java.io.IOException;
import java.net.InetSocketAddress;
import java.util.List;
import java.util.Set;
import java.util.Timer;
import java.util.TimerTask;
import java.util.concurrent.CountDownLatch;

import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.client.BookKeeper;
import org.apache.bookkeeper.client.BookKeeperAdmin;
import org.apache.bookkeeper.client.LedgerChecker;
import org.apache.bookkeeper.client.LedgerFragment;
import org.apache.bookkeeper.client.LedgerHandle;
import org.apache.bookkeeper.client.BKException.BKBookieHandleNotAvailableException;
import org.apache.bookkeeper.client.BKException.BKNoSuchLedgerExistsException;
import org.apache.bookkeeper.client.BKException.BKReadException;
import org.apache.bookkeeper.conf.ClientConfiguration;
import org.apache.bookkeeper.conf.ServerConfiguration;
import org.apache.bookkeeper.meta.LedgerManagerFactory;
import org.apache.bookkeeper.meta.LedgerUnderreplicationManager;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;
import org.apache.bookkeeper.replication.ReplicationException.CompatibilityException;
import org.apache.bookkeeper.replication.ReplicationException.UnavailableException;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.ZooKeeper;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * ReplicationWorker will take the fragments one by one from
 * ZKLedgerUnderreplicationManager and replicates to it.
 */
public class ReplicationWorker implements Runnable {
    private static Logger LOG = LoggerFactory
            .getLogger(ReplicationWorker.class);
    final private LedgerUnderreplicationManager underreplicationManager;
    private ServerConfiguration conf;
    private ZooKeeper zkc;
    private volatile boolean workerRunning = false;
    final private BookKeeperAdmin admin;
    private LedgerChecker ledgerChecker;
    private InetSocketAddress targetBookie;
    private BookKeeper bkc;
    private Thread workerThread;
    private long openLedgerRereplicationGracePeriod;
    private Timer pendingReplicationTimer;

    /**
     * Replication worker for replicating the ledger fragments from
     * UnderReplicationManager to the targetBookie. This target bookie will be a
     * local bookie.
     * 
     * @param zkc
     *            - ZK instance
     * @param conf
     *            - configurations
     * @param targetBKAddr
     *            - to where replication should happen. Ideally this will be
     *            local Bookie address.
     */
    public ReplicationWorker(final ZooKeeper zkc,
            final ServerConfiguration conf, InetSocketAddress targetBKAddr)
            throws CompatibilityException, KeeperException,
            InterruptedException, IOException {
        this.zkc = zkc;
        this.conf = conf;
        this.targetBookie = targetBKAddr;
        LedgerManagerFactory mFactory = LedgerManagerFactory
                .newLedgerManagerFactory(this.conf, this.zkc);
        this.underreplicationManager = mFactory
                .newLedgerUnderreplicationManager();
        this.bkc = new BookKeeper(new ClientConfiguration(conf), zkc);
        this.admin = new BookKeeperAdmin(bkc);
        this.ledgerChecker = new LedgerChecker(bkc);
        this.workerThread = new Thread(this, "ReplicationWorker");
        this.openLedgerRereplicationGracePeriod = conf
                .getOpenLedgerRereplicationGracePeriod();
        this.pendingReplicationTimer = new Timer("PendingReplicationTimer");
    }

    /** Start the replication worker */
    public void start() {
        this.workerThread.start();
    }

    @Override
    public void run() {
        workerRunning = true;
        while (workerRunning) {
            try {
                rereplicate();
            } catch (InterruptedException e) {
                shutdown();
                Thread.currentThread().interrupt();
                LOG.info("InterruptedException "
                        + "while replicating fragments", e);
                return;
            } catch (BKException e) {
                shutdown();
                LOG.error("BKException while replicating fragments", e);
                return;
            } catch (UnavailableException e) {
                shutdown();
                LOG.error("UnavailableException "
                        + "while replicating fragments", e);
                return;
            }
        }
    }

    /**
     * Replicates the under replicated fragments from failed bookie ledger to
     * targetBookie
     */
    private void rereplicate() throws InterruptedException, BKException,
            UnavailableException {
        long ledgerIdToReplicate = underreplicationManager
                .getLedgerToRereplicate();
        LOG.info("Going to replicate the fragments of the ledger: "
                + ledgerIdToReplicate);
        LedgerHandle lh;
        try {
            lh = admin.openLedgerNoRecovery(ledgerIdToReplicate);
        } catch (BKNoSuchLedgerExistsException e) {
            // Ledger might have been deleted by user
            LOG.info("BKNoSuchLedgerExistsException while opening "
                    + "ledger for replication. Other clients "
                    + "might have deleted the ledger. "
                    + "So, no harm to continue");
            underreplicationManager.markLedgerReplicated(ledgerIdToReplicate);
            return;
        } catch (BKReadException e) {
            LOG.info("BKReadException while"
                    + " opening ledger for replication."
                    + " Enough Bookies might not have available"
                    + "So, no harm to continue");
            underreplicationManager
                    .releaseUnderreplicatedLedger(ledgerIdToReplicate);
            return;
        } catch (BKBookieHandleNotAvailableException e) {
            LOG.info("BKBookieHandleNotAvailableException while"
                    + " opening ledger for replication."
                    + " Enough Bookies might not have available"
                    + "So, no harm to continue");
            underreplicationManager
                    .releaseUnderreplicatedLedger(ledgerIdToReplicate);
            return;
        }
        Set<LedgerFragment> fragments = getUnderreplicatedFragments(lh);
        LOG.info("Founds fragments " + fragments
                + " for replication from ledger: " + ledgerIdToReplicate);

        boolean foundOpenFragments = false;
        for (LedgerFragment ledgerFragment : fragments) {
            if (!ledgerFragment.isClosed()) {
                foundOpenFragments = true;
                continue;
            } else if (isTargetBookieExistsInFragmentEnsemble(lh,
                    ledgerFragment)) {
                LOG.info("Target Bookie[" + targetBookie
                        + "] found in the fragment ensemble:"
                        + ledgerFragment.getEnsemble());
                continue;
            }
            try {
                admin.replicateLedgerFragment(lh, ledgerFragment, targetBookie);
            } catch (BKException.BKBookieHandleNotAvailableException e) {
                LOG.warn("BKBookieHandleNotAvailableException "
                        + "while replicating the fragment", e);
            } catch (BKException.BKLedgerRecoveryException e) {
                LOG.warn("BKLedgerRecoveryException "
                        + "while replicating the fragment", e);
            }
        }

        if (foundOpenFragments) {
            deferLedgerLockRelease(ledgerIdToReplicate);
            return;
        }
        
        fragments = getUnderreplicatedFragments(lh);
        if (fragments.size() == 0) {
            LOG.info("Ledger replicated successfully. ledger id is: "
                    + ledgerIdToReplicate);
            underreplicationManager.markLedgerReplicated(ledgerIdToReplicate);
        } else {
            // Releasing the underReplication ledger lock and compete
            // for the replication again for the pending fragments
            underreplicationManager
                    .releaseUnderreplicatedLedger(ledgerIdToReplicate);
        }
    }

    /** Gets the under replicated fragments */
    private Set<LedgerFragment> getUnderreplicatedFragments(LedgerHandle lh)
            throws InterruptedException {
        CheckerCallback checkerCb = new CheckerCallback();
        ledgerChecker.checkLedger(lh, checkerCb);
        Set<LedgerFragment> fragments = checkerCb.waitAndGetResult();
        return fragments;
    }

    /**
     * Schedules a timer task for releasing the lock which will be scheduled
     * after open ledger fragment replication time. Ledger will be fenced if it
     * is still in open state when timer task fired
     */
    private void deferLedgerLockRelease(final long ledgerId) {
        long gracePeriod = this.openLedgerRereplicationGracePeriod;
        TimerTask timerTask = new TimerTask() {
            @Override
            public void run() {
                LedgerHandle lh = null;
                try {
                    lh = admin.openLedgerNoRecovery(ledgerId);
                    Set<LedgerFragment> fragments = getUnderreplicatedFragments(lh);
                    for (LedgerFragment fragment : fragments) {
                        if (!fragment.isClosed()) {
                            lh = admin.openLedger(ledgerId);
                            break;
                        }
                    }
                } catch (InterruptedException e) {
                    Thread.currentThread().interrupt();
                    LOG.info("InterruptedException "
                            + "while replicating fragments", e);
                } catch (BKException e) {
                    LOG.error("BKException while fencing the ledger"
                            + " for rereplication of postponed ledgers", e);
                } finally {
                    try {
                        if (lh != null) {
                            lh.close();
                        }
                    } catch (InterruptedException e) {
                        Thread.currentThread().interrupt();
                        LOG.info("InterruptedException while closing "
                                + "ledger", e);
                    } catch (BKException e) {
                        // Lets go ahead and release the lock. Catch actual
                        // exception in normal replication flow and take
                        // action.
                        LOG.warn("BKException while closing ledger ", e);
                    } finally {
                        try {
                            underreplicationManager
                                    .releaseUnderreplicatedLedger(ledgerId);
                        } catch (UnavailableException e) {
                            shutdown();
                            LOG.error("UnavailableException "
                                    + "while replicating fragments", e);
                        }
                    }
                }
            }
        };
        pendingReplicationTimer.schedule(timerTask, gracePeriod);
    }
    
    /**
     * Stop the replication worker service
     */
    public void shutdown() {
        synchronized (this) {
            if (!workerRunning) {
                return;
            }
            workerRunning = false;
        }
        this.pendingReplicationTimer.cancel();
        try {
            this.workerThread.interrupt();
            this.workerThread.join();
        } catch (InterruptedException e) {
            LOG.error("Interrupted during shutting down replication worker : ",
                    e);
            Thread.currentThread().interrupt();
        }
        try {
            bkc.close();
        } catch (InterruptedException e) {
            LOG.warn("Interrupted while closing the Bookie client", e);
            Thread.currentThread().interrupt();
        } catch (BKException e) {
            LOG.warn("Exception while closing the Bookie client", e);
        }
        try {
            underreplicationManager.close();
        } catch (UnavailableException e) {
            LOG.warn("Exception while closing the "
                    + "ZkLedgerUnderrepliationManager", e);
        }
    }

    /**
     * Gives the running status of ReplicationWorker
     */
    boolean isRunning() {
        return workerRunning;
    }

    private boolean isTargetBookieExistsInFragmentEnsemble(LedgerHandle lh,
            LedgerFragment ledgerFragment) {
        List<InetSocketAddress> ensemble = ledgerFragment.getEnsemble();
        for (InetSocketAddress bkAddr : ensemble) {
            if (targetBookie.equals(bkAddr)) {
                return true;
            }
        }
        return false;
    }

    /** Ledger checker call back */
    private static class CheckerCallback implements
            GenericCallback<Set<LedgerFragment>> {
        private Set<LedgerFragment> result = null;
        private CountDownLatch latch = new CountDownLatch(1);

        @Override
        public void operationComplete(int rc, Set<LedgerFragment> result) {
            this.result = result;
            latch.countDown();
        }

        /**
         * Wait until operation complete call back comes and return the ledger
         * fragments set
         */
        Set<LedgerFragment> waitAndGetResult() throws InterruptedException {
            latch.await();
            return result;
        }
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/streaming/LedgerInputStream.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */
package org.apache.bookkeeper.streaming;

import java.io.IOException;
import java.io.InputStream;
import java.nio.ByteBuffer;
import java.util.Enumeration;

import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.client.LedgerEntry;
import org.apache.bookkeeper.client.LedgerHandle;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class LedgerInputStream extends InputStream {
    Logger LOG = LoggerFactory.getLogger(LedgerInputStream.class);
    private LedgerHandle lh;
    private ByteBuffer bytebuff;
    byte[] bbytes;
    long lastEntry = 0;
    int increment = 50;
    int defaultSize = 1024 * 1024; // 1MB default size
    Enumeration<LedgerEntry> ledgerSeq = null;

    /**
     * construct a outputstream from a ledger handle
     *
     * @param lh
     *            ledger handle
     * @throws {@link BKException}, {@link InterruptedException}
     */
    public LedgerInputStream(LedgerHandle lh) throws BKException, InterruptedException {
        this.lh = lh;
        bbytes = new byte[defaultSize];
        this.bytebuff = ByteBuffer.wrap(bbytes);
        this.bytebuff.position(this.bytebuff.limit());
        lastEntry = Math.min(lh.getLastAddConfirmed(), increment);
        ledgerSeq = lh.readEntries(0, lastEntry);
    }

    /**
     * construct a outputstream from a ledger handle
     *
     * @param lh
     *            the ledger handle
     * @param size
     *            the size of the buffer
     * @throws {@link BKException}, {@link InterruptedException}
     */
    public LedgerInputStream(LedgerHandle lh, int size) throws BKException, InterruptedException {
        this.lh = lh;
        bbytes = new byte[size];
        this.bytebuff = ByteBuffer.wrap(bbytes);
        this.bytebuff.position(this.bytebuff.limit());
        lastEntry = Math.min(lh.getLastAddConfirmed(), increment);
        ledgerSeq = lh.readEntries(0, lastEntry);
    }

    /**
     * Method close currently doesn't do anything. The application
     * is supposed to open and close the ledger handle backing up
     * a stream ({@link LedgerHandle}).
     */
    @Override
    public void close() {
        // do nothing
        // let the application
        // close the ledger
    }

    /**
     * refill the buffer, we need to read more bytes
     *
     * @return if we can refill or not
     */
    private synchronized boolean refill() throws IOException {
        bytebuff.clear();
        if (!ledgerSeq.hasMoreElements() && lastEntry >= lh.getLastAddConfirmed()) {
            return false;
        }
        if (!ledgerSeq.hasMoreElements()) {
            // do refill
            long last = Math.min(lastEntry + increment, lh.getLastAddConfirmed());
            try {
                ledgerSeq = lh.readEntries(lastEntry + 1, last);
            } catch (BKException bk) {
                IOException ie = new IOException(bk.getMessage());
                ie.initCause(bk);
                throw ie;
            } catch (InterruptedException ie) {
                Thread.currentThread().interrupt();
            }
            lastEntry = last;
        }
        LedgerEntry le = ledgerSeq.nextElement();
        bbytes = le.getEntry();
        bytebuff = ByteBuffer.wrap(bbytes);
        return true;
    }

    @Override
    public synchronized int read() throws IOException {
        boolean toread = true;
        if (bytebuff.remaining() == 0) {
            // their are no remaining bytes
            toread = refill();
        }
        if (toread) {
            int ret = 0xFF & bytebuff.get();
            return ret;
        }
        return -1;
    }

    @Override
    public synchronized int read(byte[] b) throws IOException {
        // be smart ... just copy the bytes
        // once and return the size
        // user will call it again
        boolean toread = true;
        if (bytebuff.remaining() == 0) {
            toread = refill();
        }
        if (toread) {
            int bcopied = bytebuff.remaining();
            int tocopy = Math.min(bcopied, b.length);
            // cannot used gets because of
            // the underflow/overflow exceptions
            System.arraycopy(bbytes, bytebuff.position(), b, 0, tocopy);
            bytebuff.position(bytebuff.position() + tocopy);
            return tocopy;
        }
        return -1;
    }

    @Override
    public synchronized int read(byte[] b, int off, int len) throws IOException {
        // again dont need ot fully
        // fill b, just return
        // what we have and let the application call read
        // again
        boolean toread = true;
        if (bytebuff.remaining() == 0) {
            toread = refill();
        }
        if (toread) {
            int bcopied = bytebuff.remaining();
            int tocopy = Math.min(bcopied, len);
            System.arraycopy(bbytes, bytebuff.position(), b, off, tocopy);
            bytebuff.position(bytebuff.position() + tocopy);
            return tocopy;
        }
        return -1;
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/streaming/LedgerOutputStream.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */
package org.apache.bookkeeper.streaming;

import java.io.IOException;
import java.io.OutputStream;
import java.nio.ByteBuffer;

import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.client.LedgerHandle;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * this class provides a streaming api to get an output stream from a ledger
 * handle and write to it as a stream of bytes. This is built on top of
 * ledgerhandle api and uses a buffer to cache the data written to it and writes
 * out the entry to the ledger.
 */
public class LedgerOutputStream extends OutputStream {
    Logger LOG = LoggerFactory.getLogger(LedgerOutputStream.class);
    private LedgerHandle lh;
    private ByteBuffer bytebuff;
    byte[] bbytes;
    int defaultSize = 1024 * 1024; // 1MB default size

    /**
     * construct a outputstream from a ledger handle
     *
     * @param lh
     *            ledger handle
     */
    public LedgerOutputStream(LedgerHandle lh) {
        this.lh = lh;
        bbytes = new byte[defaultSize];
        this.bytebuff = ByteBuffer.wrap(bbytes);
    }

    /**
     * construct a outputstream from a ledger handle
     *
     * @param lh
     *            the ledger handle
     * @param size
     *            the size of the buffer
     */
    public LedgerOutputStream(LedgerHandle lh, int size) {
        this.lh = lh;
        bbytes = new byte[size];
        this.bytebuff = ByteBuffer.wrap(bbytes);
    }

    @Override
    public void close() {
        // flush everything
        // we have
        flush();
    }

    @Override
    public synchronized void flush() {
        // lets flush all the data
        // into the ledger entry
        if (bytebuff.position() > 0) {
            // copy the bytes into
            // a new byte buffer and send it out
            byte[] b = new byte[bytebuff.position()];
            LOG.info("Comment: flushing with params " + " " + bytebuff.position());
            System.arraycopy(bbytes, 0, b, 0, bytebuff.position());
            try {
                lh.addEntry(b);
            } catch (InterruptedException ie) {
                LOG.warn("Interrupted while flusing " + ie);
                Thread.currentThread().interrupt();
            } catch (BKException bke) {
                LOG.warn("BookKeeper exception ", bke);
            }
        }
    }

    /**
     * make space for len bytes to be written to the buffer.
     *
     * @param len
     * @return if true then we can make space for len if false we cannot
     */
    private boolean makeSpace(int len) {
        if (bytebuff.remaining() < len) {
            flush();
            bytebuff.clear();
            if (bytebuff.capacity() < len) {
                return false;
            }
        }
        return true;
    }

    @Override
    public synchronized void write(byte[] b) {
        if (makeSpace(b.length)) {
            bytebuff.put(b);
        } else {
            try {
                lh.addEntry(b);
            } catch (InterruptedException ie) {
                LOG.warn("Interrupted while writing", ie);
                Thread.currentThread().interrupt();
            } catch (BKException bke) {
                LOG.warn("BookKeeper exception", bke);
            }
        }
    }

    @Override
    public synchronized void write(byte[] b, int off, int len) {
        if (!makeSpace(len)) {
            // lets try making the buffer bigger
            bbytes = new byte[len];
            bytebuff = ByteBuffer.wrap(bbytes);
        }
        bytebuff.put(b, off, len);
    }

    @Override
    public synchronized void write(int b) throws IOException {
        makeSpace(1);
        byte oneB = (byte) (b & 0xFF);
        bytebuff.put(oneB);
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/tools/BookKeeperTools.java,false,"package org.apache.bookkeeper.tools;

/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

import java.io.IOException;
import org.apache.zookeeper.KeeperException;
import java.net.InetSocketAddress;

import org.apache.bookkeeper.client.BookKeeperAdmin;
import org.apache.bookkeeper.client.BKException;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Provides Admin Tools to manage the BookKeeper cluster.
 *
 */
public class BookKeeperTools {
    private static Logger LOG = LoggerFactory.getLogger(BookKeeperTools.class);

    /**
     * Main method so we can invoke the bookie recovery via command line.
     *
     * @param args
     *            Arguments to BookKeeperTools. 2 are required and the third is
     *            optional. The first is a comma separated list of ZK server
     *            host:port pairs. The second is the host:port socket address
     *            for the bookie we are trying to recover. The third is the
     *            host:port socket address of the optional destination bookie
     *            server we want to replicate the data over to.
     * @throws InterruptedException
     * @throws IOException
     * @throws KeeperException
     * @throws BKException
     */
    public static void main(String[] args) 
            throws InterruptedException, IOException, KeeperException, BKException {
        // Validate the inputs
        if (args.length < 2) {
            System.err.println("USAGE: BookKeeperTools zkServers bookieSrc [bookieDest]");
            return;
        }
        // Parse out the input arguments
        String zkServers = args[0];
        String bookieSrcString[] = args[1].split(":");
        if (bookieSrcString.length < 2) {
            System.err.println("BookieSrc inputted has invalid name format (host:port expected): " + args[1]);
            return;
        }
        final InetSocketAddress bookieSrc = new InetSocketAddress(bookieSrcString[0], Integer
                .parseInt(bookieSrcString[1]));
        InetSocketAddress bookieDest = null;
        if (args.length < 3) {
            String bookieDestString[] = args[2].split(":");
            if (bookieDestString.length < 2) {
                System.err.println("BookieDest inputted has invalid name format (host:port expected): "
                                   + args[2]);
                return;
            }
            bookieDest = new InetSocketAddress(bookieDestString[0], Integer.parseInt(bookieDestString[1]));
        }

        // Create the BookKeeperTools instance and perform the bookie recovery
        // synchronously.
        BookKeeperAdmin bkTools = new BookKeeperAdmin(zkServers);
        bkTools.recoverBookieData(bookieSrc, bookieDest);

        // Shutdown the resources used in the BookKeeperTools instance.
        bkTools.close();
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/util/BookKeeperConstants.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.util;

/**
 * This class contains constants used in BookKeeper
 */
public class BookKeeperConstants {

    // //////////////////////////
    // /////Basic constants//////
    // //////////////////////////
    public static final String LEDGER_NODE_PREFIX = "L";
    public static final String COLON = ":";
    public static final String VERSION_FILENAME = "VERSION";
    public final static String PASSWD = "passwd";
    public static final String CURRENT_DIR = "current";
    public static final String READONLY = "readonly";
    
    // //////////////////////////
    // ///// Znodes//////////////
    // //////////////////////////
    public static final String AVAILABLE_NODE = "available";
    public static final String COOKIE_NODE = "cookies";
    public static final String UNDER_REPLICATION_NODE = "underreplication";
    public static final String DISABLE_NODE = "disable";
    public static final String DEFAULT_ZK_LEDGERS_ROOT_PATH = "/ledgers";
    public static final String LAYOUT_ZNODE = "LAYOUT";
    public static final String INSTANCEID = "INSTANCEID";
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/util/DiskChecker.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.bookkeeper.util;

import java.io.File;
import java.io.IOException;

import com.google.common.annotations.VisibleForTesting;

/**
 * Class that provides utility functions for checking disk problems
 */
public class DiskChecker {
    private float diskUsageThreshold;

    public static class DiskErrorException extends IOException {
        private static final long serialVersionUID = 9091606022449761729L;

        public DiskErrorException(String msg) {
            super(msg);
        }
    }

    public static class DiskOutOfSpaceException extends IOException {
        private static final long serialVersionUID = 160898797915906860L;

        public DiskOutOfSpaceException(String msg) {
            super(msg);
        }
    }

    public DiskChecker(float threshold) {
        validateThreshold(threshold);
        this.diskUsageThreshold = threshold;
    }

    /**
     * The semantics of mkdirsWithExistsCheck method is different from the
     * mkdirs method provided in the Sun's java.io.File class in the following
     * way: While creating the non-existent parent directories, this method
     * checks for the existence of those directories if the mkdir fails at any
     * point (since that directory might have just been created by some other
     * process). If both mkdir() and the exists() check fails for any seemingly
     * non-existent directory, then we signal an error; Sun's mkdir would signal
     * an error (return false) if a directory it is attempting to create already
     * exists or the mkdir fails.
     * 
     * @param dir
     * @return true on success, false on failure
     */
    private static boolean mkdirsWithExistsCheck(File dir) {
        if (dir.mkdir() || dir.exists()) {
            return true;
        }
        File canonDir = null;
        try {
            canonDir = dir.getCanonicalFile();
        } catch (IOException e) {
            return false;
        }
        String parent = canonDir.getParent();
        return (parent != null)
                && (mkdirsWithExistsCheck(new File(parent)) && (canonDir
                        .mkdir() || canonDir.exists()));
    }

    /**
     * Checks the disk space available.
     * 
     * @param dir
     *            Directory to check for the disk space
     * @throws DiskOutOfSpaceException
     *             Throws {@link DiskOutOfSpaceException} if available space is
     *             less than threshhold.
     */
    @VisibleForTesting
    void checkDiskFull(File dir) throws DiskOutOfSpaceException {
        if (null == dir) {
            return;
        }
        if (dir.exists()) {
            long usableSpace = dir.getUsableSpace();
            long totalSpace = dir.getTotalSpace();
            float free = (float) usableSpace / (float) totalSpace;
            float used = 1f - free;
            if (used > diskUsageThreshold) {
                throw new DiskOutOfSpaceException("Space left on device "
                        + usableSpace + " < threshhold " + diskUsageThreshold);
            }
        } else {
            checkDiskFull(dir.getParentFile());
        }
    }

    /**
     * Create the directory if it doesn't exist and
     * 
     * @param dir
     *            Directory to check for the disk error/full.
     * @throws DiskErrorException
     *             If disk having errors
     * @throws DiskOutOfSpaceException
     *             If disk is full or having less space than threshhold
     */
    public void checkDir(File dir) throws DiskErrorException,
            DiskOutOfSpaceException {
        checkDiskFull(dir);
        if (!mkdirsWithExistsCheck(dir))
            throw new DiskErrorException("can not create directory: "
                    + dir.toString());

        if (!dir.isDirectory())
            throw new DiskErrorException("not a directory: " + dir.toString());

        if (!dir.canRead())
            throw new DiskErrorException("directory is not readable: "
                    + dir.toString());

        if (!dir.canWrite())
            throw new DiskErrorException("directory is not writable: "
                    + dir.toString());
    }

    /**
     * Returns the disk space threshold.
     * 
     * @return
     */
    @VisibleForTesting
    float getDiskSpaceThreshold() {
        return diskUsageThreshold;
    }

    /**
     * Set the disk space threshold
     * 
     * @param diskSpaceThreshold
     */
    @VisibleForTesting
    void setDiskSpaceThreshold(float diskSpaceThreshold) {
        validateThreshold(diskSpaceThreshold);
        this.diskUsageThreshold = diskSpaceThreshold;
    }

    private void validateThreshold(float diskSpaceThreshold) {
        if (diskSpaceThreshold <= 0 || diskSpaceThreshold >= 1) {
            throw new IllegalArgumentException("Disk space threashold "
                    + diskSpaceThreshold
                    + " is not valid. Should be > 0 and < 1 ");
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/util/EntryFormatter.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.util;

import java.io.IOException;

import org.apache.commons.configuration.Configuration;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Formatter to format an entry
 */
public abstract class EntryFormatter {

    static Logger LOG = LoggerFactory.getLogger(EntryFormatter.class);

    protected Configuration conf;

    public void setConf(Configuration conf) {
        this.conf = conf;
    }

    /**
     * Format an entry into a readable format
     *
     * @param data
     *          Data Payload
     */
    public abstract void formatEntry(byte[] data);

    /**
     * Format an entry from a string into a readable format
     *
     * @param input
     *          Input Stream
     */
    public abstract void formatEntry(java.io.InputStream input);

    public final static EntryFormatter STRING_FORMATTER = new StringEntryFormatter();

    public static EntryFormatter newEntryFormatter(Configuration conf, String clsProperty) {
        String cls = conf.getString(clsProperty, StringEntryFormatter.class.getName());
        ClassLoader classLoader = EntryFormatter.class.getClassLoader();
        EntryFormatter formatter;
        try {
            Class aCls = classLoader.loadClass(cls);
            formatter = (EntryFormatter) aCls.newInstance();
            formatter.setConf(conf);
        } catch (Exception e) {
            LOG.warn("No formatter class found : " + cls, e);
            LOG.warn("Using Default String Formatter.");
            formatter = STRING_FORMATTER;
        }
        return formatter;
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/util/HardLink.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
/* Copied wholesale from hadoop-common 0.23.1
package org.apache.hadoop.fs;
*/
package org.apache.bookkeeper.util;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.Arrays;

/**
 * Class for creating hardlinks.
 * Supports Unix/Linux, WinXP/2003/Vista via Cygwin, and Mac OS X.
 * 
 * The HardLink class was formerly a static inner class of FSUtil,
 * and the methods provided were blatantly non-thread-safe.
 * To enable volume-parallel Update snapshots, we now provide static 
 * threadsafe methods that allocate new buffer string arrays
 * upon each call.  We also provide an API to hardlink all files in a
 * directory with a single command, which is up to 128 times more 
 * efficient - and minimizes the impact of the extra buffer creations.
 */
public class HardLink { 

  public enum OSType {
    OS_TYPE_UNIX,
    OS_TYPE_WINXP,
    OS_TYPE_SOLARIS,
    OS_TYPE_MAC
  }
  
  public static final OSType osType;
  private static HardLinkCommandGetter getHardLinkCommand;
  
  public final LinkStats linkStats; //not static
  
  //initialize the command "getters" statically, so can use their 
  //methods without instantiating the HardLink object
  static { 
    osType = getOSType();
    if (osType == OSType.OS_TYPE_WINXP) {
      // Windows
      getHardLinkCommand = new HardLinkCGWin();
    } else {
      // Unix
      getHardLinkCommand = new HardLinkCGUnix();
      //override getLinkCountCommand for the particular Unix variant
      //Linux is already set as the default - {"stat","-c%h", null}
      if (osType == OSType.OS_TYPE_MAC) {
        String[] linkCountCmdTemplate = {"stat","-f%l", null};
        HardLinkCGUnix.setLinkCountCmdTemplate(linkCountCmdTemplate);
      } else if (osType == OSType.OS_TYPE_SOLARIS) {
        String[] linkCountCmdTemplate = {"ls","-l", null};
        HardLinkCGUnix.setLinkCountCmdTemplate(linkCountCmdTemplate);        
      }
    }
  }

  public HardLink() {
    linkStats = new LinkStats();
  }
  
  static private OSType getOSType() {
    String osName = System.getProperty("os.name");
    if (osName.contains("Windows") &&
            (osName.contains("XP") 
            || osName.contains("2003") 
            || osName.contains("Vista")
            || osName.contains("Windows_7")
            || osName.contains("Windows 7") 
            || osName.contains("Windows7"))) {
      return OSType.OS_TYPE_WINXP;
    }
    else if (osName.contains("SunOS") 
            || osName.contains("Solaris")) {
       return OSType.OS_TYPE_SOLARIS;
    }
    else if (osName.contains("Mac")) {
       return OSType.OS_TYPE_MAC;
    }
    else {
      return OSType.OS_TYPE_UNIX;
    }
  }
  
  /**
   * This abstract class bridges the OS-dependent implementations of the 
   * needed functionality for creating hardlinks and querying link counts.
   * The particular implementation class is chosen during 
   * static initialization phase of the HardLink class.
   * The "getter" methods construct shell command strings for various purposes.
   */
  private static abstract class HardLinkCommandGetter {

    /**
     * Get the command string needed to hardlink a bunch of files from
     * a single source directory into a target directory.  The source directory
     * is not specified here, but the command will be executed using the source
     * directory as the "current working directory" of the shell invocation.
     * 
     * @param fileBaseNames - array of path-less file names, relative
     *            to the source directory
     * @param linkDir - target directory where the hardlinks will be put
     * @return - an array of Strings suitable for use as a single shell command
     *            with {@link Runtime.exec()}
     * @throws IOException - if any of the file or path names misbehave
     */
    abstract String[] linkMult(String[] fileBaseNames, File linkDir) 
                          throws IOException;
    
    /**
     * Get the command string needed to hardlink a single file
     */
    abstract String[] linkOne(File file, File linkName) throws IOException;
    
    /**
     * Get the command string to query the hardlink count of a file
     */
    abstract String[] linkCount(File file) throws IOException;
    
    /**
     * Calculate the total string length of the shell command
     * resulting from execution of linkMult, plus the length of the
     * source directory name (which will also be provided to the shell)
     * 
     * @param fileDir - source directory, parent of fileBaseNames
     * @param fileBaseNames - array of path-less file names, relative
     *            to the source directory
     * @param linkDir - target directory where the hardlinks will be put
     * @return - total data length (must not exceed maxAllowedCmdArgLength)
     * @throws IOException
     */
    abstract int getLinkMultArgLength(
                     File fileDir, String[] fileBaseNames, File linkDir) 
                     throws IOException;
    
    /**
     * Get the maximum allowed string length of a shell command on this OS,
     * which is just the documented minimum guaranteed supported command
     * length - aprx. 32KB for Unix, and 8KB for Windows.
     */
    abstract int getMaxAllowedCmdArgLength(); 
  }
  
  /**
   * Implementation of HardLinkCommandGetter class for Unix
   */
  static class HardLinkCGUnix extends HardLinkCommandGetter {
    private static String[] hardLinkCommand = {"ln", null, null};
    private static String[] hardLinkMultPrefix = {"ln"};
    private static String[] hardLinkMultSuffix = {null};
    private static String[] getLinkCountCommand = {"stat","-c%h", null};
    //Unix guarantees at least 32K bytes cmd length.
    //Subtract another 64b to allow for Java 'exec' overhead
    private static final int maxAllowedCmdArgLength = 32*1024 - 65;
    
    private static synchronized 
    void setLinkCountCmdTemplate(String[] template) {
      //May update this for specific unix variants, 
      //after static initialization phase
      getLinkCountCommand = template;
    }
    
    /*
     * @see org.apache.hadoop.fs.HardLink.HardLinkCommandGetter#linkOne(java.io.File, java.io.File)
     */
    @Override
    String[] linkOne(File file, File linkName) 
    throws IOException {
      String[] buf = new String[hardLinkCommand.length];
      System.arraycopy(hardLinkCommand, 0, buf, 0, hardLinkCommand.length);
      //unix wants argument order: "ln <existing> <new>"
      buf[1] = makeShellPath(file); 
      buf[2] = makeShellPath(linkName);
      return buf;
    }
    
    /*
     * @see org.apache.hadoop.fs.HardLink.HardLinkCommandGetter#linkMult(java.lang.String[], java.io.File)
     */
    @Override
    String[] linkMult(String[] fileBaseNames, File linkDir) 
    throws IOException {
      String[] buf = new String[fileBaseNames.length 
                                + hardLinkMultPrefix.length 
                                + hardLinkMultSuffix.length];
      int mark=0;
      System.arraycopy(hardLinkMultPrefix, 0, buf, mark, 
                       hardLinkMultPrefix.length);
      mark += hardLinkMultPrefix.length;
      System.arraycopy(fileBaseNames, 0, buf, mark, fileBaseNames.length);
      mark += fileBaseNames.length;
      buf[mark] = makeShellPath(linkDir);
      return buf;
    }
    
    /*
     * @see org.apache.hadoop.fs.HardLink.HardLinkCommandGetter#linkCount(java.io.File)
     */
    @Override
    String[] linkCount(File file) 
    throws IOException {
      String[] buf = new String[getLinkCountCommand.length];
      System.arraycopy(getLinkCountCommand, 0, buf, 0, 
                       getLinkCountCommand.length);
      buf[getLinkCountCommand.length - 1] = makeShellPath(file);
      return buf;
    }
    
    /*
     * @see org.apache.hadoop.fs.HardLink.HardLinkCommandGetter#getLinkMultArgLength(java.io.File, java.lang.String[], java.io.File)
     */
    @Override
    int getLinkMultArgLength(File fileDir, String[] fileBaseNames, File linkDir) 
    throws IOException{
      int sum = 0;
      for (String x : fileBaseNames) {
        // add 1 to account for terminal null or delimiter space
        sum += 1 + ((x == null) ? 0 : x.length());
      }
      sum += 2 + makeShellPath(fileDir).length()
             + makeShellPath(linkDir).length();
      //add the fixed overhead of the hardLinkMult prefix and suffix
      sum += 3; //length("ln") + 1
      return sum;
    }
    
    /*
     * @see org.apache.hadoop.fs.HardLink.HardLinkCommandGetter#getMaxAllowedCmdArgLength()
     */
    @Override
    int getMaxAllowedCmdArgLength() {
      return maxAllowedCmdArgLength;
    }
  }
  
  
  /**
   * Implementation of HardLinkCommandGetter class for Windows
   * 
   * Note that the linkCount shell command for Windows is actually
   * a Cygwin shell command, and depends on ${cygwin}/bin
   * being in the Windows PATH environment variable, so
   * stat.exe can be found.
   */
  static class HardLinkCGWin extends HardLinkCommandGetter {
    //The Windows command getter impl class and its member fields are
    //package-private ("default") access instead of "private" to assist 
    //unit testing (sort of) on non-Win servers

    static String[] hardLinkCommand = {
                        "fsutil","hardlink","create", null, null};
    static String[] hardLinkMultPrefix = {
                        "cmd","/q","/c","for", "%f", "in", "("};
    static String   hardLinkMultDir = "\\%f";
    static String[] hardLinkMultSuffix = {
                        ")", "do", "fsutil", "hardlink", "create", null, 
                        "%f", "1>NUL"};
    static String[] getLinkCountCommand = {"stat","-c%h", null};
    //Windows guarantees only 8K - 1 bytes cmd length.
    //Subtract another 64b to allow for Java 'exec' overhead
    static final int maxAllowedCmdArgLength = 8*1024 - 65;

    /*
     * @see org.apache.hadoop.fs.HardLink.HardLinkCommandGetter#linkOne(java.io.File, java.io.File)
     */
    @Override
    String[] linkOne(File file, File linkName) 
    throws IOException {
      String[] buf = new String[hardLinkCommand.length];
      System.arraycopy(hardLinkCommand, 0, buf, 0, hardLinkCommand.length);
      //windows wants argument order: "create <new> <existing>"
      buf[4] = file.getCanonicalPath(); 
      buf[3] = linkName.getCanonicalPath();
      return buf;
    }
    
    /*
     * @see org.apache.hadoop.fs.HardLink.HardLinkCommandGetter#linkMult(java.lang.String[], java.io.File)
     */
    @Override
    String[] linkMult(String[] fileBaseNames, File linkDir) 
    throws IOException {
      String[] buf = new String[fileBaseNames.length 
                                + hardLinkMultPrefix.length 
                                + hardLinkMultSuffix.length];
      String td = linkDir.getCanonicalPath() + hardLinkMultDir;
      int mark=0;
      System.arraycopy(hardLinkMultPrefix, 0, buf, mark, 
                       hardLinkMultPrefix.length);
      mark += hardLinkMultPrefix.length;
      System.arraycopy(fileBaseNames, 0, buf, mark, fileBaseNames.length);
      mark += fileBaseNames.length;
      System.arraycopy(hardLinkMultSuffix, 0, buf, mark, 
                       hardLinkMultSuffix.length);
      mark += hardLinkMultSuffix.length;
      buf[mark - 3] = td;
      return buf;
    }
    
    /*
     * @see org.apache.hadoop.fs.HardLink.HardLinkCommandGetter#linkCount(java.io.File)
     */
    @Override
    String[] linkCount(File file) 
    throws IOException {
      String[] buf = new String[getLinkCountCommand.length];
      System.arraycopy(getLinkCountCommand, 0, buf, 0, 
                       getLinkCountCommand.length);
      //The linkCount command is actually a Cygwin shell command,
      //not a Windows shell command, so we should use "makeShellPath()"
      //instead of "getCanonicalPath()".  However, that causes another
      //shell exec to "cygpath.exe", and "stat.exe" actually can handle
      //DOS-style paths (it just prints a couple hundred bytes of warning
      //to stderr), so we use the more efficient "getCanonicalPath()".
      buf[getLinkCountCommand.length - 1] = file.getCanonicalPath();
      return buf;
    }
    
    /*
     * @see org.apache.hadoop.fs.HardLink.HardLinkCommandGetter#getLinkMultArgLength(java.io.File, java.lang.String[], java.io.File)
     */
    @Override
    int getLinkMultArgLength(File fileDir, String[] fileBaseNames, File linkDir) 
    throws IOException {
      int sum = 0;
      for (String x : fileBaseNames) {
        // add 1 to account for terminal null or delimiter space
        sum += 1 + ((x == null) ? 0 : x.length());
      }
      sum += 2 + fileDir.getCanonicalPath().length() +
               linkDir.getCanonicalPath().length();
      //add the fixed overhead of the hardLinkMult command 
      //(prefix, suffix, and Dir suffix)
      sum += ("cmd.exe /q /c for %f in ( ) do "
              + "fsutil hardlink create \\%f %f 1>NUL ").length();
      return sum;
    }
    
    /*
     * @see org.apache.hadoop.fs.HardLink.HardLinkCommandGetter#getMaxAllowedCmdArgLength()
     */
    @Override
    int getMaxAllowedCmdArgLength() {
      return maxAllowedCmdArgLength;
    }
  }
  
  
  /**
   * Calculate the nominal length of all contributors to the total 
   * commandstring length, including fixed overhead of the OS-dependent 
   * command.  It's protected rather than private, to assist unit testing,
   * but real clients are not expected to need it -- see the way 
   * createHardLinkMult() uses it internally so the user doesn't need to worry
   * about it.
   * 
   * @param fileDir - source directory, parent of fileBaseNames
   * @param fileBaseNames - array of path-less file names, relative
   *            to the source directory
   * @param linkDir - target directory where the hardlinks will be put
   * @return - total data length (must not exceed maxAllowedCmdArgLength)
   * @throws IOException
   */
  protected static int getLinkMultArgLength(
          File fileDir, String[] fileBaseNames, File linkDir) 
  throws IOException {
    return getHardLinkCommand.getLinkMultArgLength(fileDir, 
          fileBaseNames, linkDir);
  }
  
  /**
   * Return this private value for use by unit tests.
   * Shell commands are not allowed to have a total string length
   * exceeding this size.
   */
  protected static int getMaxAllowedCmdArgLength() {
    return getHardLinkCommand.getMaxAllowedCmdArgLength();
  }
  
  /*
   * ****************************************************
   * Complexity is above.  User-visible functionality is below
   * ****************************************************
   */

  /**
   * Creates a hardlink 
   * @param file - existing source file
   * @param linkName - desired target link file
   */
  public static void createHardLink(File file, File linkName) 
  throws IOException {
    if (file == null) {
      throw new IOException(
          "invalid arguments to createHardLink: source file is null");
    }
    if (linkName == null) {
      throw new IOException(
          "invalid arguments to createHardLink: link name is null");
    }
	  // construct and execute shell command
    String[] hardLinkCommand = getHardLinkCommand.linkOne(file, linkName);
    Process process = Runtime.getRuntime().exec(hardLinkCommand);
    try {
      if (process.waitFor() != 0) {
        String errMsg = new BufferedReader(new InputStreamReader(
            process.getInputStream())).readLine();
        if (errMsg == null)  errMsg = "";
        String inpMsg = new BufferedReader(new InputStreamReader(
            process.getErrorStream())).readLine();
        if (inpMsg == null)  inpMsg = "";
        throw new IOException(errMsg + inpMsg);
      }
    } catch (InterruptedException e) {
      throw new IOException(e);
    } finally {
      process.destroy();
    }
  }

  /**
   * Creates hardlinks from multiple existing files within one parent
   * directory, into one target directory.
   * @param parentDir - directory containing source files
   * @param fileBaseNames - list of path-less file names, as returned by 
   *                        parentDir.list()
   * @param linkDir - where the hardlinks should be put.  It must already exist.
   * 
   * If the list of files is too long (overflows maxAllowedCmdArgLength),
   * we will automatically split it into multiple invocations of the
   * underlying method.
   */
  public static void createHardLinkMult(File parentDir, String[] fileBaseNames, 
      File linkDir) throws IOException {
    //This is the public method all non-test clients are expected to use.
    //Normal case - allow up to maxAllowedCmdArgLength characters in the cmd
    createHardLinkMult(parentDir, fileBaseNames, linkDir, 
                       getHardLinkCommand.getMaxAllowedCmdArgLength());
  }

  /*
   * Implements {@link createHardLinkMult} with added variable  "maxLength",
   * to ease unit testing of the auto-splitting feature for long lists.
   * Likewise why it returns "callCount", the number of sub-arrays that
   * the file list had to be split into.
   * Non-test clients are expected to call the public method instead.
   */
  protected static int createHardLinkMult(File parentDir, 
      String[] fileBaseNames, File linkDir, int maxLength) 
  throws IOException {
    if (parentDir == null) {
      throw new IOException(
          "invalid arguments to createHardLinkMult: parent directory is null");
    }
    if (linkDir == null) {
      throw new IOException(
          "invalid arguments to createHardLinkMult: link directory is null");
    }
    if (fileBaseNames == null) {
      throw new IOException(
          "invalid arguments to createHardLinkMult: "
          + "filename list can be empty but not null");
    }
    if (fileBaseNames.length == 0) {
      //the OS cmds can't handle empty list of filenames, 
      //but it's legal, so just return.
      return 0; 
    }
    if (!linkDir.exists()) {
      throw new FileNotFoundException(linkDir + " not found.");
    }

    //if the list is too long, split into multiple invocations
    int callCount = 0;
    if (getLinkMultArgLength(parentDir, fileBaseNames, linkDir) > maxLength
          && fileBaseNames.length > 1) {
      String[] list1 = Arrays.copyOf(fileBaseNames, fileBaseNames.length/2);
      callCount += createHardLinkMult(parentDir, list1, linkDir, maxLength);
      String[] list2 = Arrays.copyOfRange(fileBaseNames, fileBaseNames.length/2,
          fileBaseNames.length);
      callCount += createHardLinkMult(parentDir, list2, linkDir, maxLength);  
      return callCount;
    } else {
      callCount = 1;
    }
    
    // construct and execute shell command
    String[] hardLinkCommand = getHardLinkCommand.linkMult(fileBaseNames, 
        linkDir);
    Process process = Runtime.getRuntime().exec(hardLinkCommand, null, 
        parentDir);
    try {
      if (process.waitFor() != 0) {
        String errMsg = new BufferedReader(new InputStreamReader(
            process.getInputStream())).readLine();
        if (errMsg == null)  errMsg = "";
        String inpMsg = new BufferedReader(new InputStreamReader(
            process.getErrorStream())).readLine();
        if (inpMsg == null)  inpMsg = "";
        throw new IOException(errMsg + inpMsg);
      }
    } catch (InterruptedException e) {
      throw new IOException(e);
    } finally {
      process.destroy();
    }
    return callCount;
  }

   /**
   * Retrieves the number of links to the specified file.
   */
  public static int getLinkCount(File fileName) throws IOException {
    if (fileName == null) {
      throw new IOException(
          "invalid argument to getLinkCount: file name is null");
    }
    if (!fileName.exists()) {
      throw new FileNotFoundException(fileName + " not found.");
    }

    // construct and execute shell command
    String[] cmd = getHardLinkCommand.linkCount(fileName);
    String inpMsg = null;
    String errMsg = null;
    int exitValue = -1;
    BufferedReader in = null;
    BufferedReader err = null;

    Process process = Runtime.getRuntime().exec(cmd);
    try {
      exitValue = process.waitFor();
      in = new BufferedReader(new InputStreamReader(
                                  process.getInputStream()));
      inpMsg = in.readLine();
      err = new BufferedReader(new InputStreamReader(
                                   process.getErrorStream()));
      errMsg = err.readLine();
      if (inpMsg == null || exitValue != 0) {
        throw createIOException(fileName, inpMsg, errMsg, exitValue, null);
      }
      if (osType == OSType.OS_TYPE_SOLARIS) {
        String[] result = inpMsg.split("\\s+");
        return Integer.parseInt(result[1]);
      } else {
        return Integer.parseInt(inpMsg);
      }
    } catch (NumberFormatException e) {
      throw createIOException(fileName, inpMsg, errMsg, exitValue, e);
    } catch (InterruptedException e) {
      throw createIOException(fileName, inpMsg, errMsg, exitValue, e);
    } finally {
      process.destroy();
      if (in != null) in.close();
      if (err != null) err.close();
    }
  }
  
  /* Create an IOException for failing to get link count. */
  private static IOException createIOException(File f, String message,
      String error, int exitvalue, Exception cause) {
    
    final String winErrMsg = "; Windows errors in getLinkCount are often due "
         + "to Cygwin misconfiguration";

    final String s = "Failed to get link count on file " + f
        + ": message=" + message
        + "; error=" + error
        + ((osType == OSType.OS_TYPE_WINXP) ? winErrMsg : "")
        + "; exit value=" + exitvalue;
    return (cause == null) ? new IOException(s) : new IOException(s, cause);
  }
  
  
  /**
   * HardLink statistics counters and methods.
   * Not multi-thread safe, obviously.
   * Init is called during HardLink instantiation, above.
   * 
   * These are intended for use by knowledgeable clients, not internally, 
   * because many of the internal methods are static and can't update these
   * per-instance counters.
   */
  public static class LinkStats {
    public int countDirs = 0; 
    public int countSingleLinks = 0; 
    public int countMultLinks = 0; 
    public int countFilesMultLinks = 0; 
    public int countEmptyDirs = 0; 
    public int countPhysicalFileCopies = 0;
  
    public void clear() {
      countDirs = 0; 
      countSingleLinks = 0; 
      countMultLinks = 0; 
      countFilesMultLinks = 0; 
      countEmptyDirs = 0; 
      countPhysicalFileCopies = 0;
    }
    
    public String report() {
      return "HardLinkStats: " + countDirs + " Directories, including " 
      + countEmptyDirs + " Empty Directories, " 
      + countSingleLinks 
      + " single Link operations, " + countMultLinks 
      + " multi-Link operations, linking " + countFilesMultLinks 
      + " files, total " + (countSingleLinks + countFilesMultLinks) 
      + " linkable files.  Also physically copied " 
      + countPhysicalFileCopies + " other files.";
    }
  }

  /**
   * Convert a os-native filename to a path that works for the shell.
   * @param filename The filename to convert
   * @return The unix pathname
   * @throws IOException on windows, there can be problems with the subprocess
   */
  public static String makeShellPath(File file) throws IOException {
    String filename = file.getCanonicalPath();
    if (System.getProperty("os.name").startsWith("Windows")) {
      BufferedReader r = null;
      try {
        ProcessBuilder pb = new ProcessBuilder("cygpath", "-u", filename);
        Process p = pb.start();
        int err = p.waitFor();
        if (err != 0) {
            throw new IOException("Couldn't resolve path "
                                  + filename + "(" + err + ")");
        }
        r = new BufferedReader(new InputStreamReader(p.getInputStream()));
        return r.readLine();
      } catch (InterruptedException ie) {
        throw new IOException("Couldn't resolve path " + filename, ie);
      } finally {
        if (r != null) {
          r.close();
        }
      }
    } else {
      return filename;
    }
  }
}

"
bookkeeper-server/src/main/java/org/apache/bookkeeper/util/IOUtils.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */
package org.apache.bookkeeper.util;

import java.io.IOException;

import org.slf4j.Logger;

/**
 * An utility class for I/O related functionality.
 */
public class IOUtils {

    /**
     * Close the Closeable objects and <b>ignore</b> any {@link IOException} or
     * null pointers. Must only be used for cleanup in exception handlers.
     * 
     * @param log
     *            the log to record problems to at debug level. Can be null.
     * @param closeables
     *            the objects to close
     */
    public static void close(Logger log, java.io.Closeable... closeables) {
        for (java.io.Closeable c : closeables) {
            if (c != null) {
                try {
                    c.close();
                } catch (IOException e) {
                    if (log != null && log.isDebugEnabled()) {
                        log.debug("Exception in closing " + c, e);
                    }
                }
            }
        }
    }

    /**
     * Confirm prompt for the console operations.
     * 
     * @param prompt
     *            Prompt message to be displayed on console
     * @return Returns true if confirmed as 'Y', returns false if confirmed as
     *         'N'
     * @throws IOException
     */
    public static boolean confirmPrompt(String prompt) throws IOException {
        while (true) {
            System.out.print(prompt + " (Y or N) ");
            StringBuilder responseBuilder = new StringBuilder();
            while (true) {
                int c = System.in.read();
                if (c == -1 || c == '\r' || c == '\n') {
                    break;
                }
                responseBuilder.append((char) c);
            }

            String response = responseBuilder.toString();
            if (response.equalsIgnoreCase("y")
                    || response.equalsIgnoreCase("yes")) {
                return true;
            } else if (response.equalsIgnoreCase("n")
                    || response.equalsIgnoreCase("no")) {
                return false;
            }
            System.out.println("Invalid input: " + response);
            // else ask them again
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/util/LocalBookKeeper.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.util;

import java.io.BufferedReader;
import java.io.File;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.OutputStream;
import java.net.InetAddress;
import java.net.InetSocketAddress;
import java.net.Socket;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.TimeUnit;

import org.apache.bookkeeper.bookie.BookieException;
import org.apache.bookkeeper.conf.ServerConfiguration;
import org.apache.bookkeeper.proto.BookieServer;
import org.apache.bookkeeper.replication.ReplicationException.CompatibilityException;
import org.apache.bookkeeper.replication.ReplicationException.UnavailableException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.WatchedEvent;
import org.apache.zookeeper.Watcher;
import org.apache.zookeeper.ZooKeeper;
import org.apache.zookeeper.Watcher.Event.KeeperState;
import org.apache.zookeeper.ZooDefs.Ids;
import org.apache.zookeeper.server.NIOServerCnxnFactory;
import org.apache.zookeeper.server.ZooKeeperServer;

public class LocalBookKeeper {
    protected static final Logger LOG = LoggerFactory.getLogger(LocalBookKeeper.class);
    public static final int CONNECTION_TIMEOUT = 30000;

    int numberOfBookies;

    public LocalBookKeeper() {
        numberOfBookies = 3;
    }

    public LocalBookKeeper(int numberOfBookies) {
        this();
        this.numberOfBookies = numberOfBookies;
        LOG.info("Running " + this.numberOfBookies + " bookie(s).");
    }

    private final String HOSTPORT = "127.0.0.1:2181";
    NIOServerCnxnFactory serverFactory;
    ZooKeeperServer zks;
    ZooKeeper zkc;
    int ZooKeeperDefaultPort = 2181;
    static int zkSessionTimeOut = 5000;
    File ZkTmpDir;

    //BookKeeper variables
    File tmpDirs[];
    BookieServer bs[];
    ServerConfiguration bsConfs[];
    Integer initialPort = 5000;

    /**
     * @param args
     */

    private void runZookeeper(int maxCC) throws IOException {
        // create a ZooKeeper server(dataDir, dataLogDir, port)
        LOG.info("Starting ZK server");
        //ServerStats.registerAsConcrete();
        //ClientBase.setupTestEnv();
        ZkTmpDir = File.createTempFile("zookeeper", "test");
        if (!ZkTmpDir.delete() || !ZkTmpDir.mkdir()) {
            throw new IOException("Couldn't create zk directory " + ZkTmpDir);
        }

        try {
            zks = new ZooKeeperServer(ZkTmpDir, ZkTmpDir, ZooKeeperDefaultPort);
            serverFactory =  new NIOServerCnxnFactory();
            serverFactory.configure(new InetSocketAddress(ZooKeeperDefaultPort), maxCC);
            serverFactory.startup(zks);
        } catch (Exception e) {
            // TODO Auto-generated catch block
            LOG.error("Exception while instantiating ZooKeeper", e);
        }

        boolean b = waitForServerUp(HOSTPORT, CONNECTION_TIMEOUT);
        LOG.debug("ZooKeeper server up: {}", b);
    }

    private void initializeZookeper() throws IOException {
        LOG.info("Instantiate ZK Client");
        //initialize the zk client with values
        try {
            ZKConnectionWatcher zkConnectionWatcher = new ZKConnectionWatcher();
            zkc = new ZooKeeper(HOSTPORT, zkSessionTimeOut,
                    zkConnectionWatcher);
            zkConnectionWatcher.waitForConnection();
            zkc.create("/ledgers", new byte[0], Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
            zkc.create("/ledgers/available", new byte[0], Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
            // No need to create an entry for each requested bookie anymore as the
            // BookieServers will register themselves with ZooKeeper on startup.
        } catch (KeeperException e) {
            // TODO Auto-generated catch block
            LOG.error("Exception while creating znodes", e);
        } catch (InterruptedException e) {
            // TODO Auto-generated catch block
            LOG.error("Interrupted while creating znodes", e);
        }
    }

    private void runBookies(ServerConfiguration baseConf) throws IOException,
            KeeperException, InterruptedException, BookieException,
            UnavailableException, CompatibilityException {
        LOG.info("Starting Bookie(s)");
        // Create Bookie Servers (B1, B2, B3)

        tmpDirs = new File[numberOfBookies];
        bs = new BookieServer[numberOfBookies];
        bsConfs = new ServerConfiguration[numberOfBookies];

        for(int i = 0; i < numberOfBookies; i++) {
            tmpDirs[i] = File.createTempFile("bookie" + Integer.toString(i), "test");
            if (!tmpDirs[i].delete() || !tmpDirs[i].mkdir()) {
                throw new IOException("Couldn't create bookie dir " + tmpDirs[i]);
            }

            bsConfs[i] = new ServerConfiguration(baseConf);
            // override settings
            bsConfs[i].setBookiePort(initialPort + i);
            bsConfs[i].setZkServers(InetAddress.getLocalHost().getHostAddress() + ":"
                                  + ZooKeeperDefaultPort);
            bsConfs[i].setJournalDirName(tmpDirs[i].getPath());
            bsConfs[i].setLedgerDirNames(new String[] { tmpDirs[i].getPath() });

            bs[i] = new BookieServer(bsConfs[i]);
            bs[i].start();
        }
    }

    public static void main(String[] args) throws IOException, KeeperException,
            InterruptedException, BookieException, UnavailableException,
            CompatibilityException {
        if(args.length < 1) {
            usage();
            System.exit(-1);
        }
        LocalBookKeeper lb = new LocalBookKeeper(Integer.parseInt(args[0]));

        ServerConfiguration conf = new ServerConfiguration();
        if (args.length >= 2) {
            String confFile = args[1];
            try {
                conf.loadConf(new File(confFile).toURI().toURL());
                LOG.info("Using configuration file " + confFile);
            } catch (Exception e) {
                // load conf failed
                LOG.warn("Error loading configuration file " + confFile, e);
            }
        }

        lb.runZookeeper(1000);
        lb.initializeZookeper();
        lb.runBookies(conf);
        while (true) {
            Thread.sleep(5000);
        }
    }

    private static void usage() {
        System.err.println("Usage: LocalBookKeeper number-of-bookies");
    }

    /* Watching SyncConnected event from ZooKeeper */
    static class ZKConnectionWatcher implements Watcher {
        private CountDownLatch clientConnectLatch = new CountDownLatch(1);

        @Override
        public void process(WatchedEvent event) {
            if (event.getState() == KeeperState.SyncConnected) {
                clientConnectLatch.countDown();
            }
        }

        // Waiting for the SyncConnected event from the ZooKeeper server
        public void waitForConnection() throws IOException {
            try {
                if (!clientConnectLatch.await(zkSessionTimeOut,
                        TimeUnit.MILLISECONDS)) {
                    throw new IOException(
                            "Couldn't connect to zookeeper server");
                }
            } catch (InterruptedException e) {
                throw new IOException(
                        "Interrupted when connecting to zookeeper server", e);
            }
        }
    }

    public static boolean waitForServerUp(String hp, long timeout) {
        long start = MathUtils.now();
        String split[] = hp.split(":");
        String host = split[0];
        int port = Integer.parseInt(split[1]);
        while (true) {
            try {
                Socket sock = new Socket(host, port);
                BufferedReader reader = null;
                try {
                    OutputStream outstream = sock.getOutputStream();
                    outstream.write("stat".getBytes());
                    outstream.flush();

                    reader =
                        new BufferedReader(
                        new InputStreamReader(sock.getInputStream()));
                    String line = reader.readLine();
                    if (line != null && line.startsWith("Zookeeper version:")) {
                        LOG.info("Server UP");
                        return true;
                    }
                } finally {
                    sock.close();
                    if (reader != null) {
                        reader.close();
                    }
                }
            } catch (IOException e) {
                // ignore as this is expected
                LOG.info("server " + hp + " not up " + e);
            }

            if (MathUtils.now() > start + timeout) {
                break;
            }
            try {
                Thread.sleep(250);
            } catch (InterruptedException e) {
                // ignore
            }
        }
        return false;
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/util/Main.java,false,"package org.apache.bookkeeper.util;

/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

import java.io.IOException;

import org.apache.bookkeeper.proto.BookieClient;
import org.apache.bookkeeper.proto.BookieServer;

public class Main {

    static void usage() {
        System.err.println("USAGE: bookeeper client|bookie");
    }

    /**
     * @param args
     * @throws InterruptedException
     * @throws IOException
     */
    public static void main(String[] args) throws Exception {
        if (args.length < 1 || !(args[0].equals("client") || args[0].equals("bookie"))) {
            usage();
            return;
        }
        String newArgs[] = new String[args.length - 1];
        System.arraycopy(args, 1, newArgs, 0, newArgs.length);
        if (args[0].equals("bookie")) {
            BookieServer.main(newArgs);
        } else {
            BookieClient.main(newArgs);
        }
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/util/MathUtils.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.bookkeeper.util;

/**
 * Provides misc math functions that don't come standard
 */
public class MathUtils {
    private static final long NANOSECONDS_PER_MILLISECOND = 1000000;
    public static int signSafeMod(long dividend, int divisor) {
        int mod = (int) (dividend % divisor);

        if (mod < 0) {
            mod += divisor;
        }

        return mod;

    }
    
    /**
     * Current time from some arbitrary time base in the past, counting in
     * milliseconds, and not affected by settimeofday or similar system clock
     * changes. This is appropriate to use when computing how much longer to
     * wait for an interval to expire.
     * 
     * @return current time in milliseconds.
     */
    public static long now() {
        return System.nanoTime() / NANOSECONDS_PER_MILLISECOND;
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/util/OrderedSafeExecutor.java,false,"package org.apache.bookkeeper.util;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.util.Random;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;

import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;

/**
 * This class provides 2 things over the java {@link ScheduledExecutorService}.
 *
 * 1. It takes {@link SafeRunnable objects} instead of plain Runnable objects.
 * This means that exceptions in scheduled tasks wont go unnoticed and will be
 * logged.
 *
 * 2. It supports submitting tasks with an ordering key, so that tasks submitted
 * with the same key will always be executed in order, but tasks across
 * different keys can be unordered. This retains parallelism while retaining the
 * basic amount of ordering we want (e.g. , per ledger handle). Ordering is
 * achieved by hashing the key objects to threads by their {@link #hashCode()}
 * method.
 *
 */
public class OrderedSafeExecutor {
    ExecutorService threads[];
    Random rand = new Random();

    public OrderedSafeExecutor(int numThreads) {
        if (numThreads <= 0) {
            throw new IllegalArgumentException();
        }

        threads = new ExecutorService[numThreads];
        for (int i = 0; i < numThreads; i++) {
            threads[i] = Executors.newSingleThreadExecutor();
        }
    }

    ExecutorService chooseThread() {
        // skip random # generation in this special case
        if (threads.length == 1) {
            return threads[0];
        }

        return threads[rand.nextInt(threads.length)];

    }

    ExecutorService chooseThread(Object orderingKey) {
        // skip hashcode generation in this special case
        if (threads.length == 1) {
            return threads[0];
        }

        return threads[MathUtils.signSafeMod(orderingKey.hashCode(), threads.length)];

    }

    /**
     * schedules a one time action to execute
     */
    public void submit(SafeRunnable r) {
        chooseThread().submit(r);
    }

    /**
     * schedules a one time action to execute with an ordering guarantee on the key
     * @param orderingKey
     * @param r
     */
    public void submitOrdered(Object orderingKey, SafeRunnable r) {
        chooseThread(orderingKey).submit(r);
    }

    public void shutdown() {
        for (int i = 0; i < threads.length; i++) {
            threads[i].shutdown();
        }
    }

    public boolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException {
        boolean ret = true;
        for (int i = 0; i < threads.length; i++) {
            ret = ret && threads[i].awaitTermination(timeout, unit);
        }
        return ret;
    }

    /**
     * Generic callback implementation which will run the
     * callback in the thread which matches the ordering key
     */
    public static abstract class OrderedSafeGenericCallback<T>
            implements GenericCallback<T> {
        private final OrderedSafeExecutor executor;
        private final Object orderingKey;

        /**
         * @param executor The executor on which to run the callback
         * @param orderingKey Key used to decide which thread the callback
         *                    should run on.
         */
        public OrderedSafeGenericCallback(OrderedSafeExecutor executor, Object orderingKey) {
            this.executor = executor;
            this.orderingKey = orderingKey;
        }

        @Override
        public final void operationComplete(final int rc, final T result) {
            executor.submitOrdered(orderingKey, new SafeRunnable() {
                    @Override
                    public void safeRun() {
                        safeOperationComplete(rc, result);
                    }
                });
        }

        public abstract void safeOperationComplete(int rc, T result);
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/util/ReflectionUtils.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */
package org.apache.bookkeeper.util;

import java.lang.reflect.Constructor;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;

import org.apache.commons.configuration.Configuration;
import org.apache.commons.configuration.ConfigurationException;

/**
 * General Class Reflection Utils
 */
public class ReflectionUtils {

    private static final Map<Class<?>, Constructor<?>> constructorCache =
            new ConcurrentHashMap<Class<?>, Constructor<?>>();

    /**
     * Get the value of the <code>name</code> property as a <code>Class</code>.
     * If no such property is specified, then <code>defaultCls</code> is returned.
     *
     * @param conf
     *          Configuration Object.
     * @param name
     *          Class Property Name.
     * @param defaultCls
     *          Default Class to be returned.
     * @param classLoader
     *          Class Loader to load class.
     * @return property value as a <code>Class</code>, or <code>defaultCls</code>.
     * @throws ConfigurationException
     */
    public static Class<?> getClass(Configuration conf, String name,
                                    Class<?> defaultCls, ClassLoader classLoader)
            throws ConfigurationException {
        String valueStr = conf.getString(name);
        if (null == valueStr) {
            return defaultCls;
        }
        try {
            return Class.forName(valueStr, true, classLoader);
        } catch (ClassNotFoundException cnfe) {
            throw new ConfigurationException(cnfe);
        }
    }

    /**
     * Get the value of the <code>name</code> property as a <code>Class</code> implementing
     * the interface specified by <code>xface</code>.
     *
     * If no such property is specified, then <code>defaultValue</code> is returned.
     *
     * An exception is thrown if the returned class does not implement the named interface.
     *
     * @param conf
     *          Configuration Object.
     * @param name
     *          Class Property Name.
     * @param defaultValue
     *          Default Class to be returned.
     * @param xface
     *          The interface implemented by the named class.
     * @param classLoader
     *          Class Loader to load class.
     * @return property value as a <code>Class</code>, or <code>defaultValue</code>.
     * @throws ConfigurationException
     */
    public static <U> Class<? extends U> getClass(Configuration conf,
                                                  String name, Class<? extends U> defaultValue,
                                                  Class<U> xface, ClassLoader classLoader)
        throws ConfigurationException {
        try {
            Class<?> theCls = getClass(conf, name, defaultValue, classLoader);
            if (null != theCls && !xface.isAssignableFrom(theCls)) {
                throw new ConfigurationException(theCls + " not " + xface.getName());
            } else if (null != theCls) {
                return theCls.asSubclass(xface);
            } else {
                return null;
            }
        } catch (Exception e) {
            throw new ConfigurationException(e);
        }
    }

    /**
     * Create an object for the given class.
     *
     * @param theCls
     *          class of which an object is created.
     * @return a new object
     */
    @SuppressWarnings("unchecked")
    public static <T> T newInstance(Class<T> theCls) {
        T result;
        try {
            Constructor<T> meth = (Constructor<T>) constructorCache.get(theCls);
            if (null == meth) {
                meth = theCls.getDeclaredConstructor();
                meth.setAccessible(true);
                constructorCache.put(theCls, meth);
            }
            result = meth.newInstance();
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
        return result;
    }

    /**
     * Create an object using the given class name.
     *
     * @param clsName
     *          class name of which an object is created.
     * @param xface
     *          The interface implemented by the named class.
     * @return a new object
     */
    @SuppressWarnings("unchecked")
    public static <T> T newInstance(String clsName, Class<T> xface) {
        Class<?> theCls;
        try {
            theCls = Class.forName(clsName);
        } catch (ClassNotFoundException cnfe) {
            throw new RuntimeException(cnfe);
        }
        if (!xface.isAssignableFrom(theCls)) {
            throw new RuntimeException(clsName + " not " + xface.getName());
        }
        return newInstance(theCls.asSubclass(xface));
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/util/SafeRunnable.java,false,"package org.apache.bookkeeper.util;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public abstract class SafeRunnable implements Runnable {

    static final Logger logger = LoggerFactory.getLogger(SafeRunnable.class);

    @Override
    public void run() {
        try {
            safeRun();
        } catch(Throwable t) {
            logger.error("Unexpected throwable caught ", t);
        }
    }

    public abstract void safeRun();

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/util/SnapshotMap.java,false,"package org.apache.bookkeeper.util;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.util.concurrent.ConcurrentSkipListMap;
import java.util.concurrent.ConcurrentHashMap;

import java.util.Map;
import java.util.NavigableMap;
import java.util.concurrent.locks.ReentrantReadWriteLock;

/**
 * A snapshotable map.
 */
public class SnapshotMap<K, V> {
    // stores recent updates
    volatile Map<K, V> updates;
    volatile Map<K, V> updatesToMerge;
    // map stores all snapshot data
    volatile NavigableMap<K, V> snapshot;

    final ReentrantReadWriteLock lock = new ReentrantReadWriteLock();

    public SnapshotMap() {
        updates = new ConcurrentHashMap<K, V>();
        updatesToMerge = new ConcurrentHashMap<K, V>();
        snapshot = new ConcurrentSkipListMap<K, V>();
    }

    /**
     * Create a snapshot of current map.
     *
     * @return a snapshot of current map.
     */
    public NavigableMap<K, V> snapshot() {
        this.lock.writeLock().lock();
        try {
            if (updates.isEmpty()) {
                return snapshot;
            }
            // put updates for merge to snapshot
            updatesToMerge = updates;
            updates = new ConcurrentHashMap<K, V>();
        } finally {
            this.lock.writeLock().unlock();
        }
        // merging the updates to snapshot
        for (Map.Entry<K, V> entry : updatesToMerge.entrySet()) {
            snapshot.put(entry.getKey(), entry.getValue());
        }
        // clear updatesToMerge
        this.lock.writeLock().lock();
        try {
            updatesToMerge = new ConcurrentHashMap<K, V>();
        } finally {
            this.lock.writeLock().unlock();
        }
        return snapshot;
    }

    /**
     * Associates the specified value with the specified key in this map.
     *
     * @param key
     *          Key with which the specified value is to be associated.
     * @param value
     *          Value to be associated with the specified key.
     */
    public void put(K key, V value) {
        this.lock.readLock().lock();
        try {
            updates.put(key, value);
        } finally {
            this.lock.readLock().unlock();
        }

    }

    /**
     * Removes the mapping for the key from this map if it is present.
     *
     * @param key
     *          Key whose mapping is to be removed from this map.
     */
    public void remove(K key) {
        this.lock.readLock().lock();
        try {
            // first remove updates
            updates.remove(key);
            updatesToMerge.remove(key);
            // then remove snapshot
            snapshot.remove(key);
        } finally {
            this.lock.readLock().unlock();
        }
    }

    /**
     * Returns true if this map contains a mapping for the specified key.
     *
     * @param key
     *          Key whose presence is in the map to be tested.
     * @return true if the map contains a mapping for the specified key.
     */
    public boolean containsKey(K key) {
        this.lock.readLock().lock();
        try {
            return updates.containsKey(key)
                 | updatesToMerge.containsKey(key)
                 | snapshot.containsKey(key);
        } finally {
            this.lock.readLock().unlock();
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/util/StringEntryFormatter.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.util;

import java.io.IOException;

import org.apache.commons.configuration.Configuration;

import com.google.protobuf.ByteString;

public class StringEntryFormatter extends EntryFormatter {
    @Override
    public void formatEntry(byte[] data) {
        System.out.println(ByteString.copyFrom(data).toStringUtf8());
    }

    @Override
    public void formatEntry(java.io.InputStream input) {
        try {
            byte[] data = new byte[input.available()];
            input.read(data, 0, data.length);
            formatEntry(data);
        } catch (IOException ie) {
            System.out.println("Warn: Unreadable entry : " + ie.getMessage());
        }
    }

};
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/util/StringUtils.java,false,"package org.apache.bookkeeper.util;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.IOException;
import java.net.InetSocketAddress;

/**
 * Provided utilites for parsing network addresses, ledger-id from node paths
 * etc.
 *
 */
public class StringUtils {

    // Ledger Node Prefix
    static public final String LEDGER_NODE_PREFIX = "L";

    /**
     * Parses address into IP and port.
     *
     * @param addr
     *            String
     */

    public static InetSocketAddress parseAddr(String s) throws IOException {

        String parts[] = s.split(":");
        if (parts.length != 2) {
            throw new IOException(s + " does not have the form host:port");
        }
        int port;
        try {
            port = Integer.parseInt(parts[1]);
        } catch (NumberFormatException e) {
            throw new IOException(s + " does not have the form host:port");
        }

        InetSocketAddress addr = new InetSocketAddress(parts[0], port);
        return addr;
    }

    public static String addrToString(InetSocketAddress addr) {
        return addr.getAddress().getHostAddress() + ":" + addr.getPort();
    }

    /**
     * Formats ledger ID according to ZooKeeper rules
     *
     * @param id
     *            znode id
     */
    public static String getZKStringId(long id) {
        return String.format("%010d", id);
    }

    /**
     * Get the hierarchical ledger path according to the ledger id
     *
     * @param ledgerId
     *          ledger id
     * @return the hierarchical path
     */
    public static String getHierarchicalLedgerPath(long ledgerId) {
        String ledgerIdStr = getZKStringId(ledgerId);
        // do 2-4-4 split
        StringBuilder sb = new StringBuilder();
        sb.append("/")
          .append(ledgerIdStr.substring(0, 2)).append("/")
          .append(ledgerIdStr.substring(2, 6)).append("/")
          .append(LEDGER_NODE_PREFIX)
          .append(ledgerIdStr.substring(6, 10));
        return sb.toString();
    }

    /**
     * Parse the hierarchical ledger path to its ledger id
     *
     * @param hierarchicalLedgerPath
     * @return the ledger id
     * @throws IOException
     */
    public static long stringToHierarchicalLedgerId(String hierarchicalLedgerPath)
            throws IOException {
        String[] hierarchicalParts = hierarchicalLedgerPath.split("/");
        if (hierarchicalParts.length != 3) {
            throw new IOException("it is not a valid hierarchical path name : " + hierarchicalLedgerPath);
        }
        hierarchicalParts[2] =
            hierarchicalParts[2].substring(LEDGER_NODE_PREFIX.length());
        return stringToHierarchicalLedgerId(hierarchicalParts);
    }

    /**
     * Get ledger id
     *
     * @param levelNodes
     *          level of the ledger path
     * @return ledger id
     * @throws IOException
     */
    public static long stringToHierarchicalLedgerId(String...levelNodes) throws IOException {
        try {
            StringBuilder sb = new StringBuilder();
            for (String node : levelNodes) {
                sb.append(node);
            }
            return Long.parseLong(sb.toString());
        } catch (NumberFormatException e) {
            throw new IOException(e);
        }
    }

}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/util/Tool.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.bookkeeper.util;

import org.apache.commons.configuration.Configuration;

/**
 * A tool interface that supports handling of generic command-line options.
 */
public interface Tool {
    /**
     * Exectue the command with given arguments
     *
     * @param args command specific arguments
     * @return exit code.
     * @throws Exception
     */
    public int run(String[] args) throws Exception;

    /**
     * Passe a configuration object to the tool.
     *
     * @param conf configuration object
     * @throws Exception
     */
    public void setConf(Configuration conf) throws Exception;
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/util/ZkUtils.java,true,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.bookkeeper.util;

import java.io.File;
import java.io.IOException;
import java.util.List;

import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.GenericCallback;
import org.apache.bookkeeper.zookeeper.ZooKeeperWatcherBase;
import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.AsyncCallback;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.AsyncCallback.StringCallback;
import org.apache.zookeeper.KeeperException.Code;
import org.apache.zookeeper.data.ACL;
import org.apache.zookeeper.ZooKeeper;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Provided utilites for zookeeper access, etc.
 */
public class ZkUtils {
    private static final Logger LOG = LoggerFactory.getLogger(ZkUtils.class);
    /**
     * Create zookeeper path recursively
     *
     * @param zk
     *          Zookeeper client
     * @param originalPath
     *          Zookeeper full path
     * @param data
     *          Zookeeper data
     * @param acl
     *          Acl of the zk path
     * @param createMode
     *          Create mode of zk path
     * @param callback
     *          Callback
     * @param ctx
     *          Context object
     */
    public static void createFullPathOptimistic(
        final ZooKeeper zk, final String originalPath, final byte[] data,
        final List<ACL> acl, final CreateMode createMode,
        final AsyncCallback.StringCallback callback, final Object ctx) {

        zk.create(originalPath, data, acl, createMode, new StringCallback() {
            @Override
            public void processResult(int rc, String path, Object ctx, String name) {

                if (rc != Code.NONODE.intValue()) {
                    callback.processResult(rc, path, ctx, name);
                    return;
                }

                // Since I got a nonode, it means that my parents don't exist
                // create mode is persistent since ephemeral nodes can't be
                // parents
                createFullPathOptimistic(zk, new File(originalPath).getParent().replace("\\", "/"), new byte[0], acl,
                        CreateMode.PERSISTENT, new StringCallback() {

                            @Override
                            public void processResult(int rc, String path, Object ctx, String name) {
                                if (rc == Code.OK.intValue() || rc == Code.NODEEXISTS.intValue()) {
                                    // succeeded in creating the parent, now
                                    // create the original path
                                    createFullPathOptimistic(zk, originalPath, data, acl, createMode, callback,
                                            ctx);
                                } else {
                                    callback.processResult(rc, path, ctx, name);
                                }
                            }
                        }, ctx);
            }
        }, ctx);

    }

    private static class GetChildrenCtx {
        int rc;
        boolean done = false;
        List<String> children = null;
    }

    /**
     * Sync get all children under single zk node
     *
     * @param zk
     *          zookeeper client
     * @param node
     *          node path
     * @return direct children
     * @throws InterruptedException
     * @throws IOException
     */
    public static List<String> getChildrenInSingleNode(final ZooKeeper zk, final String node)
            throws InterruptedException, IOException {
        final GetChildrenCtx ctx = new GetChildrenCtx();
        getChildrenInSingleNode(zk, node, new GenericCallback<List<String>>() {
            @Override
            public void operationComplete(int rc, List<String> ledgers) {
                synchronized (ctx) {
                    if (Code.OK.intValue() == rc) {
                        ctx.children = ledgers;
                    }
                    ctx.rc = rc;
                    ctx.done = true;
                    ctx.notifyAll();
                }
            }
        });

        synchronized (ctx) {
            while (ctx.done == false) {
                ctx.wait();
            }
        }
        if (Code.OK.intValue() != ctx.rc) {
            throw new IOException("Error on getting children from node " + node);
        }
        return ctx.children;

    }

    /**
     * Async get direct children under single node
     *
     * @param zk
     *          zookeeper client
     * @param node
     *          node path
     * @param cb
     *          callback function
     */
    public static void getChildrenInSingleNode(final ZooKeeper zk, final String node,
            final GenericCallback<List<String>> cb) {
        zk.sync(node, new AsyncCallback.VoidCallback() {
            @Override
            public void processResult(int rc, String path, Object ctx) {
                if (rc != Code.OK.intValue()) {
                    LOG.error("ZK error syncing nodes when getting children: ", KeeperException
                            .create(KeeperException.Code.get(rc), path));
                    cb.operationComplete(rc, null);
                    return;
                }
                zk.getChildren(node, false, new AsyncCallback.ChildrenCallback() {
                    @Override
                    public void processResult(int rc, String path, Object ctx, List<String> nodes) {
                        if (rc != Code.OK.intValue()) {
                            LOG.error("Error polling ZK for the available nodes: ", KeeperException
                                    .create(KeeperException.Code.get(rc), path));
                            cb.operationComplete(rc, null);
                            return;
                        }

                        cb.operationComplete(rc, nodes);

                    }
                }, null);
            }
        }, null);
    }

    /**
     * Get new ZooKeeper client. Waits till the connection is complete. If
     * connection is not successful within timeout, then throws back exception.
     *
     * @param servers
     *            ZK servers connection string.
     * @param timeout
     *            Session timeout.
     */
    public static ZooKeeper createConnectedZookeeperClient(String servers,
            ZooKeeperWatcherBase w) throws IOException, InterruptedException,
            KeeperException {
        if (servers == null || servers.isEmpty()) {
            throw new IllegalArgumentException("servers cannot be empty");
        }
        final ZooKeeper newZk = new ZooKeeper(servers, w.getZkSessionTimeOut(),
                w);
        w.waitForConnection();
        // Re-checking zookeeper connection status
        if (!newZk.getState().isConnected()) {
            throw KeeperException.create(KeeperException.Code.CONNECTIONLOSS);
        }
        return newZk;
    }

    /**
     * Utility to create the complete znode path synchronously
     * 
     * @param zkc
     *            - ZK instance
     * @param path
     *            - znode path
     * @param data
     *            - znode data
     * @param acl
     *            - Acl of the zk path
     * @param createMode
     *            - Create mode of zk path
     * @throws KeeperException
     *             if the server returns a non-zero error code, or invalid ACL
     * @throws InterruptedException
     *             if the transaction is interrupted
     */
    public static void createFullPathOptimistic(ZooKeeper zkc, String path,
            byte[] data, final List<ACL> acl, final CreateMode createMode)
            throws KeeperException, InterruptedException {
        try {
            zkc.create(path, data, acl, createMode);
        } catch (KeeperException.NoNodeException nne) {
            int lastSlash = path.lastIndexOf('/');
            if (lastSlash <= 0) {
                throw nne;
            }
            String parent = path.substring(0, lastSlash);
            createFullPathOptimistic(zkc, parent, new byte[0], acl, createMode);
            zkc.create(path, data, acl, createMode);
        }
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/versioning/Version.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */
package org.apache.bookkeeper.versioning;

/**
 * An interface that allows us to determine if a given version happened before or after another version.
 */
public interface Version {

    /**
     * Initial version.
     */
    public static final Version NEW = new Version() {
        @Override
        public Occurred compare(Version v) {
            if (null == v) {
                throw new NullPointerException("Version is not allowed to be null.");
            }
            if (this == v) {
                return Occurred.CONCURRENTLY;
            }
            return Occurred.BEFORE;
        }
    };

    /**
     * Match any version.
     */
    public static final Version ANY = new Version() {
        @Override
        public Occurred compare(Version v) {
            if (null == v) {
                throw new NullPointerException("Version is not allowed to be null.");
            }
            return Occurred.CONCURRENTLY;
        }
    };

    public static enum Occurred {
        BEFORE, AFTER, CONCURRENTLY
    }

    public Occurred compare(Version v);
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/versioning/Versioned.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.bookkeeper.versioning;

public class Versioned<T> {
    T value;
    Version version;

    public Versioned(T value, Version version) {
        this.value = value;
        this.version = version;
    }

    public void setValue(T value) {
        this.value = value;
    }

    public T getValue() {
        return value;
    }

    public void setVersion(Version version) {
        this.version = version;
    }

    public Version getVersion() {
        return version;
    }
}
"
bookkeeper-server/src/main/java/org/apache/bookkeeper/zookeeper/ZooKeeperWatcherBase.java,false,"/**
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */
package org.apache.bookkeeper.zookeeper;

import java.util.concurrent.CountDownLatch;
import java.util.concurrent.TimeUnit;

import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.WatchedEvent;
import org.apache.zookeeper.Watcher;
import org.apache.zookeeper.Watcher.Event.EventType;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Watcher for receiving zookeeper server connection events.
 */
public class ZooKeeperWatcherBase implements Watcher {
    private static final Logger LOG = LoggerFactory
            .getLogger(ZooKeeperWatcherBase.class);

    private final int zkSessionTimeOut;
    private CountDownLatch clientConnectLatch = new CountDownLatch(1);

    public ZooKeeperWatcherBase(int zkSessionTimeOut) {
        this.zkSessionTimeOut = zkSessionTimeOut;
    }

    @Override
    public void process(WatchedEvent event) {
        // If event type is NONE, this is a connection status change
        if (event.getType() != EventType.None) {
            LOG.debug("Recieved event: {}, path: {} from ZooKeeper server",
                    event.getType(), event.getPath());
            return;
        }

        LOG.debug("Recieved {} from ZooKeeper server", event.getState());
        // TODO: Needs to handle AuthFailed, SaslAuthenticated events
        switch (event.getState()) {
        case SyncConnected:
            clientConnectLatch.countDown();
            break;
        case Disconnected:
            LOG.debug("Ignoring Disconnected event from ZooKeeper server");
            break;
        case Expired:
            LOG.error("ZooKeeper client connection to the "
                    + "ZooKeeper server has expired!");
            break;
        }
    }

    /**
     * Waiting for the SyncConnected event from the ZooKeeper server
     * 
     * @throws KeeperException
     *             when there is no connection
     * @throws InterruptedException
     *             interrupted while waiting for connection
     */
    public void waitForConnection() throws KeeperException,
            InterruptedException {
        if (!clientConnectLatch.await(zkSessionTimeOut, TimeUnit.MILLISECONDS)) {
            throw KeeperException.create(KeeperException.Code.CONNECTIONLOSS);
        }
    }

    /**
     * Return zookeeper session time out
     */
    public int getZkSessionTimeOut() {
        return zkSessionTimeOut;
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/HedwigClient.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client;

import org.apache.hedwig.client.api.Client;
import org.apache.hedwig.client.api.Publisher;
import org.apache.hedwig.client.api.Subscriber;
import org.apache.hedwig.client.netty.HedwigClientImpl;

import org.apache.hedwig.client.conf.ClientConfiguration;
import org.jboss.netty.channel.ChannelFactory;

/**
 * Hedwig client uses as starting point for all communications with the Hedwig service.
 * 
 * @see Publisher
 * @see Subscriber
 */
public class HedwigClient implements Client {
    private final Client impl;

    /**
     * Construct a hedwig client object. The configuration object
     * should be an instance of a class which implements ClientConfiguration.
     *
     * @param cfg The client configuration.
     */
    public HedwigClient(ClientConfiguration cfg) {
        impl = HedwigClientImpl.create(cfg);
    }

    /**
     * Construct a hedwig client object, using a preexisting socket factory.
     * This is useful if you need to create many hedwig client instances.
     *
     * @param cfg The client configuration
     * @param socketFactory A netty socket factory.
     */
    public HedwigClient(ClientConfiguration cfg, ChannelFactory socketFactory) {
        impl = HedwigClientImpl.create(cfg, socketFactory);
    }

    @Override
    public Publisher getPublisher() {
        return impl.getPublisher();
    }

    @Override
    public Subscriber getSubscriber() {
        return impl.getSubscriber();
    }

    @Override
    public void close() {
        impl.close();
    }
}"
hedwig-client/src/main/java/org/apache/hedwig/client/api/Client.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.api;

/**
 * Interface defining the client API for Hedwig
 */
public interface Client {
    /**
     * Retrieve the Publisher object for the client.
     * This object can be used to publish messages to a topic on Hedwig.
     * @see Publisher
     */
    public Publisher getPublisher();
    
    /**
     * Retrieve the Subscriber object for the client.
     * This object can be used to subscribe for messages from a topic.
     * @see Subscriber
     */
    public Subscriber getSubscriber();

    /**
     * Close the client and free all associated resources.
     */
    public void close();
}"
hedwig-client/src/main/java/org/apache/hedwig/client/api/MessageHandler.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.api;

import com.google.protobuf.ByteString;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.util.Callback;

/**
 * Interface to define the client handler logic to deliver messages it is
 * subscribed to.
 *
 */
public interface MessageHandler {

    /**
     * Delivers a message which has been published for topic. 
     *
     * @param topic
     *            The topic name where the message came from.
     * @param subscriberId
     *            ID of the subscriber.
     * @param msg
     *            The message object to deliver.
     * @param callback
     *            Callback to invoke when the message delivery has been done.
     * @param context
     *            Calling context that the Callback needs since this is done
     *            asynchronously.
     */
    public void deliver(ByteString topic, ByteString subscriberId, Message msg, Callback<Void> callback, Object context);

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/api/Publisher.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.api;

import com.google.protobuf.ByteString;
import org.apache.hedwig.exceptions.PubSubException.CouldNotConnectException;
import org.apache.hedwig.exceptions.PubSubException.ServiceDownException;
import org.apache.hedwig.protocol.PubSubProtocol;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.util.Callback;

/**
 * Interface to define the client Publisher API.
 *
 */
public interface Publisher {

    /**
     * Publishes a message on the given topic.
     *
     * @param topic
     *            Topic name to publish on
     * @param msg
     *            Message object to serialize and publish
     * @throws CouldNotConnectException
     *             If we are not able to connect to the server host
     * @throws ServiceDownException
     *             If we are unable to publish the message to the topic.
     * @return The PubSubProtocol.PublishResponse of the publish ... can be used to pick seq-id.
     */
    public PubSubProtocol.PublishResponse publish(ByteString topic, Message msg)
        throws CouldNotConnectException, ServiceDownException;

    /**
     * Publishes a message asynchronously on the given topic.
     *
     * @param topic
     *            Topic name to publish on
     * @param msg
     *            Message object to serialize and publish
     * @param callback
     *            Callback to invoke when the publish to the server has actually
     *            gone through. This will have to deal with error conditions on
     *            the async publish request.
     * @param context
     *            Calling context that the Callback needs since this is done
     *            asynchronously.
     */
    public void asyncPublish(ByteString topic, Message msg, Callback<Void> callback, Object context);


  /**
   * Publishes a message asynchronously on the given topic.
   * This method, unlike {@link #asyncPublish(ByteString, PubSubProtocol.Message, Callback, Object)},
   * allows for the callback to retrieve {@link org.apache.hedwig.protocol.PubSubProtocol.PublishResponse}
   * which was returned by the server.
   *
   *
   *
   * @param topic
   *            Topic name to publish on
   * @param msg
   *            Message object to serialize and publish
   * @param callback
   *            Callback to invoke when the publish to the server has actually
   *            gone through. This will have to deal with error conditions on
   *            the async publish request.
   * @param context
   *            Calling context that the Callback needs since this is done
   *            asynchronously.
   */
    public void asyncPublishWithResponse(ByteString topic, Message msg,
                                         Callback<PubSubProtocol.PublishResponse> callback, Object context);
}
"
hedwig-client/src/main/java/org/apache/hedwig/client/api/Subscriber.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.api;

import java.util.List;

import com.google.protobuf.ByteString;
import org.apache.hedwig.client.exceptions.AlreadyStartDeliveryException;
import org.apache.hedwig.client.exceptions.InvalidSubscriberIdException;
import org.apache.hedwig.exceptions.PubSubException.ClientAlreadySubscribedException;
import org.apache.hedwig.exceptions.PubSubException.ClientNotSubscribedException;
import org.apache.hedwig.exceptions.PubSubException.CouldNotConnectException;
import org.apache.hedwig.exceptions.PubSubException.ServiceDownException;
import org.apache.hedwig.filter.ClientMessageFilter;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions;
import org.apache.hedwig.util.Callback;
import org.apache.hedwig.util.SubscriptionListener;

/**
 * Interface to define the client Subscriber API.
 *
 */
public interface Subscriber {

    /**
     * Subscribe to the given topic for the inputted subscriberId.
     *
     * @param topic
     *            Topic name of the subscription
     * @param subscriberId
     *            ID of the subscriber
     * @param mode
     *            Whether to prohibit, tolerate, or require an existing
     *            subscription.
     * @throws CouldNotConnectException
     *             If we are not able to connect to the server host
     * @throws ClientAlreadySubscribedException
     *             If client is already subscribed to the topic
     * @throws ServiceDownException
     *             If unable to subscribe to topic
     * @throws InvalidSubscriberIdException
     *             If the subscriberId is not valid. We may want to set aside
     *             certain formats of subscriberId's for different purposes.
     *             e.g. local vs. hub subscriber
     * @deprecated As of BookKeeper 4.2.0, replaced by
     *             {@link Subscriber#subscribe(com.google.protobuf.ByteString,
     *                                         com.google.protobuf.ByteString,
     *                                         PubSubProtocol.SubscriptionOptions)}
     */
    @Deprecated
    public void subscribe(ByteString topic, ByteString subscriberId, CreateOrAttach mode)
            throws CouldNotConnectException, ClientAlreadySubscribedException, ServiceDownException,
        InvalidSubscriberIdException;

    /**
     * Subscribe to the given topic asynchronously for the inputted subscriberId
     * disregarding if the topic has been created yet or not.
     *
     * @param topic
     *            Topic name of the subscription
     * @param subscriberId
     *            ID of the subscriber
     * @param mode
     *            Whether to prohibit, tolerate, or require an existing
     *            subscription.
     * @param callback
     *            Callback to invoke when the subscribe request to the server
     *            has actually gone through. This will have to deal with error
     *            conditions on the async subscribe request.
     * @param context
     *            Calling context that the Callback needs since this is done
     *            asynchronously.
     * @deprecated As of BookKeeper 4.2.0, replaced by
     *             {@link Subscriber#asyncSubscribe(com.google.protobuf.ByteString,
     *                                              com.google.protobuf.ByteString,
     *                                              PubSubProtocol.SubscriptionOptions,Callback,Object)}
     */
    @Deprecated
    public void asyncSubscribe(ByteString topic, ByteString subscriberId, CreateOrAttach mode, Callback<Void> callback,
                               Object context);


    /**
     * Subscribe to the given topic for the inputted subscriberId.
     *
     * @param topic
     *            Topic name of the subscription
     * @param subscriberId
     *            ID of the subscriber
     * @param options
     *            Options to pass to the subscription. See
     *             {@link Subscriber#asyncSubscribe(com.google.protobuf.ByteString,
     *                                              com.google.protobuf.ByteString,
     *                                              PubSubProtocol.SubscriptionOptions,
     *                                              Callback,Object) asyncSubscribe}
     *            for details on how to set options.
     * @throws CouldNotConnectException
     *             If we are not able to connect to the server host
     * @throws ClientAlreadySubscribedException
     *             If client is already subscribed to the topic
     * @throws ServiceDownException
     *             If unable to subscribe to topic
     * @throws InvalidSubscriberIdException
     *             If the subscriberId is not valid. We may want to set aside
     *             certain formats of subscriberId's for different purposes.
     *             e.g. local vs. hub subscriber
     */
    public void subscribe(ByteString topic, ByteString subscriberId, SubscriptionOptions options)
            throws CouldNotConnectException, ClientAlreadySubscribedException, ServiceDownException,
        InvalidSubscriberIdException;

    /**
     * <p>Subscribe to the given topic asynchronously for the inputted subscriberId.</p>
     *
     * <p>SubscriptionOptions contains parameters for how the hub should make the subscription.
     * The options includes createorattach mode, message bound and message filter.</p>
     *
     * <p>The createorattach mode defines whether the subscription should create a new subscription, or
     * just attach to a preexisting subscription. If it tries to create the subscription, and the
     * subscription already exists, then an error will occur.</p>
     *
     * <p>The message bound defines the maximum number of undelivered messages which will be stored
     * for the subscription. This can be used to ensure that unused subscriptions do not grow
     * in an unbounded fashion. By default, the message bound is infinite, i.e. all undelivered messages
     * will be stored for the subscription. Note that if one subscription on a topic has a infinite
     * message bound, the message bound for all other subscriptions on that topic will effectively be
     * infinite as the messages have to be stored for the first subscription in any case. </p>
     *
     * <p>The message filter defines a {@link org.apache.hedwig.filter.ServerMessageFilter}
     * run in hub server to filter messages delivered to the subscription. The server message
     * filter should be placed in the classpath of hub server before using it.</p>
     *
     * All these subscription options would be stored as SubscriptionPreferences in metadata
     * manager. The next time subscriber attached with difference options, the new options would
     * overwrite the old options.
     *
     * Usage is as follows:
     * <pre>
     * {@code
     * // create a new subscription with a message bound of 5
     * SubscriptionOptions options = SubscriptionOptions.newBuilder()
     *     .setCreateOrAttach(CreateOrAttach.CREATE).setMessageBound(5).build();
     * client.getSubscriber().asyncSubscribe(ByteString.copyFromUtf8("myTopic"),
     *                                       ByteString.copyFromUtf8("mySubscription"),
     *                                       options,
     *                                       myCallback,
     *                                       myContext);
     * }
     * </pre>
     * @param topic
     *            Topic name of the subscription
     * @param subscriberId
     *            ID of the subscriber
     * @param options
     *            Options to pass to the subscription.
     * @param callback
     *            Callback to invoke when the subscribe request to the server
     *            has actually gone through. This will have to deal with error
     *            conditions on the async subscribe request.
     * @param context
     *            Calling context that the Callback needs since this is done
     *            asynchronously.
     */
    public void asyncSubscribe(ByteString topic, ByteString subscriberId, SubscriptionOptions options,
                               Callback<Void> callback, Object context);

    /**
     * Unsubscribe from a topic that the subscriberId user has previously
     * subscribed to.
     *
     * @param topic
     *            Topic name of the subscription
     * @param subscriberId
     *            ID of the subscriber
     * @throws CouldNotConnectException
     *             If we are not able to connect to the server host
     * @throws ClientNotSubscribedException
     *             If the client is not currently subscribed to the topic
     * @throws ServiceDownException
     *             If the server was down and unable to complete the request
     * @throws InvalidSubscriberIdException
     *             If the subscriberId is not valid. We may want to set aside
     *             certain formats of subscriberId's for different purposes.
     *             e.g. local vs. hub subscriber
     */
    public void unsubscribe(ByteString topic, ByteString subscriberId) throws CouldNotConnectException,
        ClientNotSubscribedException, ServiceDownException, InvalidSubscriberIdException;

    /**
     * Unsubscribe from a topic asynchronously that the subscriberId user has
     * previously subscribed to.
     *
     * @param topic
     *            Topic name of the subscription
     * @param subscriberId
     *            ID of the subscriber
     * @param callback
     *            Callback to invoke when the unsubscribe request to the server
     *            has actually gone through. This will have to deal with error
     *            conditions on the async unsubscribe request.
     * @param context
     *            Calling context that the Callback needs since this is done
     *            asynchronously.
     */
    public void asyncUnsubscribe(ByteString topic, ByteString subscriberId, Callback<Void> callback, Object context);

    /**
     * Manually send a consume message to the server for the given inputs.
     *
     * @param topic
     *            Topic name of the subscription
     * @param subscriberId
     *            ID of the subscriber
     * @param messageSeqId
     *            Message Sequence ID for the latest message that the client app
     *            has successfully consumed. All messages up to that point will
     *            also be considered as consumed.
     * @throws ClientNotSubscribedException
     *             If the client is not currently subscribed to the topic based
     *             on the client's local state.
     */
    public void consume(ByteString topic, ByteString subscriberId, MessageSeqId messageSeqId)
            throws ClientNotSubscribedException;

    /**
     * Checks if the subscriberId client is currently subscribed to the given
     * topic.
     *
     * @param topic
     *            Topic name of the subscription.
     * @param subscriberId
     *            ID of the subscriber
     * @throws CouldNotConnectException
     *             If we are not able to connect to the server host
     * @throws ServiceDownException
     *             If there is an error checking the server if the client has a
     *             subscription
     * @return Boolean indicating if the client has a subscription or not.
     */
    public boolean hasSubscription(ByteString topic, ByteString subscriberId) throws CouldNotConnectException,
        ServiceDownException;

    /**
     * Fills the input List with the subscriptions this subscriberId client is
     * subscribed to.
     *
     * @param subscriberId
     *            ID of the subscriber
     * @return List filled with subscription name (topic) strings.
     * @throws CouldNotConnectException
     *             If we are not able to connect to the server host
     * @throws ServiceDownException
     *             If there is an error retrieving the list of topics
     */
    public List<ByteString> getSubscriptionList(ByteString subscriberId) throws CouldNotConnectException,
        ServiceDownException;

    /**
     * Begin delivery of messages from the server to us for this topic and
     * subscriberId.
     *
     * @param topic
     *            Topic name of the subscription
     * @param subscriberId
     *            ID of the subscriber
     * @param messageHandler
     *            Message Handler that will consume the subscribed messages
     * @throws ClientNotSubscribedException
     *             If the client is not currently subscribed to the topic
     * @throws AlreadyStartDeliveryException
     *             If someone started delivery a message handler before stopping existed one.
     */
    public void startDelivery(ByteString topic, ByteString subscriberId, MessageHandler messageHandler)
            throws ClientNotSubscribedException, AlreadyStartDeliveryException;

    /**
     * Begin delivery of messages from the server to us for this topic and
     * subscriberId.
     *
     * Only the messages passed <code>messageFilter</code> could be delivered to
     * <code>messageHandler</code>.
     *
     * @param topic
     *            Topic name of the subscription
     * @param subscriberId
     *            ID of the subscriber
     * @param messageHandler
     *            Message Handler that will consume the subscribed messages
     * @throws ClientNotSubscribedException
     *             If the client is not currently subscribed to the topic
     * @throws AlreadyStartDeliveryException
     *             If someone started delivery a message handler before stopping existed one.
     * @throws NullPointerException
     *             If either <code>messageHandler</code> or <code>messageFilter</code> is null.
     */
    public void startDeliveryWithFilter(ByteString topic, ByteString subscriberId,
                                        MessageHandler messageHandler,
                                        ClientMessageFilter messageFilter)
            throws ClientNotSubscribedException, AlreadyStartDeliveryException;

    /**
     * Stop delivery of messages for this topic and subscriberId.
     *
     * @param topic
     *            Topic name of the subscription
     * @param subscriberId
     *            ID of the subscriber
     * @throws ClientNotSubscribedException
     *             If the client is not currently subscribed to the topic
     */
    public void stopDelivery(ByteString topic, ByteString subscriberId) throws ClientNotSubscribedException;

    /**
     * Closes all of the client side cached data for this subscription without
     * actually sending an unsubscribe request to the server. This will close
     * the subscribe channel synchronously (if it exists) for the topic.
     *
     * @param topic
     *            Topic name of the subscription
     * @param subscriberId
     *            ID of the subscriber
     * @throws ServiceDownException
     *             If the subscribe channel was not able to be closed
     *             successfully
     */
    public void closeSubscription(ByteString topic, ByteString subscriberId) throws ServiceDownException;

    /**
     * Closes all of the client side cached data for this subscription without
     * actually sending an unsubscribe request to the server. This will close
     * the subscribe channel asynchronously (if it exists) for the topic.
     *
     * @param topic
     *            Topic name of the subscription
     * @param subscriberId
     *            ID of the subscriber
     * @param callback
     *            Callback to invoke when the subscribe channel has been closed.
     * @param context
     *            Calling context that the Callback needs since this is done
     *            asynchronously.
     */
    public void asyncCloseSubscription(ByteString topic, ByteString subscriberId, Callback<Void> callback,
                                       Object context);

    /**
     * Register a subscription listener which get notified about subscription
     * event indicating a state of a subscription that subscribed disable
     * resubscribe logic.
     *
     * @param listener
     *          Subscription Listener
     */
    public void addSubscriptionListener(SubscriptionListener listener);

    /**
     * Unregister a subscription listener.
     *
     * @param listener
     *          Subscription Listener
     */
    public void removeSubscriptionListener(SubscriptionListener listener);
}
"
hedwig-client/src/main/java/org/apache/hedwig/client/benchmark/BenchmarkPublisher.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.benchmark;

import com.google.protobuf.ByteString;

import org.apache.bookkeeper.util.MathUtils;
import org.apache.hedwig.client.api.MessageHandler;
import org.apache.hedwig.client.api.Publisher;
import org.apache.hedwig.client.api.Subscriber;
import org.apache.hedwig.client.benchmark.BenchmarkUtils.BenchmarkCallback;
import org.apache.hedwig.client.benchmark.BenchmarkUtils.ThroughputLatencyAggregator;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach;
import org.apache.hedwig.util.Callback;

public class BenchmarkPublisher extends BenchmarkWorker {
    Publisher publisher;
    Subscriber subscriber;
    int msgSize;
    int nParallel;
    double rate;

    public BenchmarkPublisher(int numTopics, int numMessages, int numRegions, int startTopicLabel, int partitionIndex,
                              int numPartitions, Publisher publisher, Subscriber subscriber, int msgSize, int nParallel, int rate) {
        super(numTopics, numMessages, numRegions, startTopicLabel, partitionIndex, numPartitions);
        this.publisher = publisher;
        this.msgSize = msgSize;
        this.subscriber = subscriber;
        this.nParallel = nParallel;

        this.rate = rate / (numRegions * numPartitions + 0.0);
    }

    public void warmup(int nWarmup) throws Exception {
        ByteString topic = ByteString.copyFromUtf8("warmup" + partitionIndex);
        ByteString subId = ByteString.copyFromUtf8("sub");
        subscriber.subscribe(topic, subId, CreateOrAttach.CREATE_OR_ATTACH);

        subscriber.startDelivery(topic, subId, new MessageHandler() {
            @Override
            public void deliver(ByteString topic, ByteString subscriberId, Message msg, Callback<Void> callback,
            Object context) {
                // noop
                callback.operationFinished(context, null);
            }
        });

        // picking constants arbitarily for warmup phase
        ThroughputLatencyAggregator agg = new ThroughputLatencyAggregator("acked pubs", nWarmup, 100);
        agg.startProgress();

        Message msg = getMsg(1024);
        for (int i = 0; i < nWarmup; i++) {
            publisher.asyncPublish(topic, msg, new BenchmarkCallback(agg), null);
        }

        if (agg.tpAgg.queue.take() > 0) {
            throw new RuntimeException("Warmup publishes failed!");
        }

    }

    public Message getMsg(int size) {
        StringBuilder sb = new StringBuilder();
        for (int i = 0; i < size; i++) {
            sb.append('a');
        }
        final ByteString body = ByteString.copyFromUtf8(sb.toString());
        Message msg = Message.newBuilder().setBody(body).build();
        return msg;
    }

    public Void call() throws Exception {
        Message msg = getMsg(msgSize);

        // Single warmup for every topic
        int myPublishCount = 0;
        for (int i = 0; i < numTopics; i++) {
            if (!HedwigBenchmark.amIResponsibleForTopic(startTopicLabel + i, partitionIndex, numPartitions)) {
                continue;
            }
            ByteString topic = ByteString.copyFromUtf8(HedwigBenchmark.TOPIC_PREFIX + (startTopicLabel + i));
            publisher.publish(topic, msg);
            myPublishCount++;
        }

        long startTime = MathUtils.now();
        int myPublishLimit = numMessages / numRegions / numPartitions - myPublishCount;
        myPublishCount = 0;
        ThroughputLatencyAggregator agg = new ThroughputLatencyAggregator("acked pubs", myPublishLimit, nParallel);
        agg.startProgress();

        int topicLabel = 0;

        while (myPublishCount < myPublishLimit) {
            int topicNum = startTopicLabel + topicLabel;
            topicLabel = (topicLabel + 1) % numTopics;

            if (!HedwigBenchmark.amIResponsibleForTopic(topicNum, partitionIndex, numPartitions)) {
                continue;
            }

            ByteString topic = ByteString.copyFromUtf8(HedwigBenchmark.TOPIC_PREFIX + topicNum);

            if (rate > 0) {
                long delay = startTime + (long) (1000 * myPublishCount / rate) - MathUtils.now();
                if (delay > 0)
                    Thread.sleep(delay);
            }
            publisher.asyncPublish(topic, msg, new BenchmarkCallback(agg), null);
            myPublishCount++;
        }

        System.out.println("Finished unacked pubs: tput = " + BenchmarkUtils.calcTp(myPublishLimit, startTime)
                           + " ops/s");
        // Wait till the benchmark test has completed
        agg.tpAgg.queue.take();
        System.out.println(agg.summarize(startTime));
        return null;
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/benchmark/BenchmarkSubscriber.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.benchmark;

import java.util.HashMap;
import java.util.Map;
import java.util.concurrent.Callable;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.protobuf.ByteString;

import org.apache.bookkeeper.util.MathUtils;
import org.apache.hedwig.client.api.MessageHandler;
import org.apache.hedwig.client.api.Subscriber;
import org.apache.hedwig.client.benchmark.BenchmarkUtils.BenchmarkCallback;
import org.apache.hedwig.client.benchmark.BenchmarkUtils.ThroughputAggregator;
import org.apache.hedwig.client.benchmark.BenchmarkUtils.ThroughputLatencyAggregator;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach;
import org.apache.hedwig.util.Callback;

public class BenchmarkSubscriber extends BenchmarkWorker implements Callable<Void> {
    static final Logger logger = LoggerFactory.getLogger(BenchmarkSubscriber.class);
    Subscriber subscriber;
    ByteString subId;


    public BenchmarkSubscriber(int numTopics, int numMessages, int numRegions,
                               int startTopicLabel, int partitionIndex, int numPartitions, Subscriber subscriber, ByteString subId) {
        super(numTopics, numMessages, numRegions, startTopicLabel, partitionIndex, numPartitions);
        this.subscriber = subscriber;
        this.subId = subId;
    }

    public void warmup(int numWarmup) throws InterruptedException {
        /*
         * multiplying the number of ops by numParitions because we end up
         * skipping many because of the partitioning logic
         */
        multiSub("warmup", "warmup", 0, numWarmup, numWarmup * numPartitions);
    }

    public Void call() throws Exception {

        final ThroughputAggregator agg = new ThroughputAggregator("recvs", numMessages);
        agg.startProgress();

        final Map<String, Long> lastSeqIdSeenMap = new HashMap<String, Long>();

        for (int i = startTopicLabel; i < startTopicLabel + numTopics; i++) {

            if (!HedwigBenchmark.amIResponsibleForTopic(i, partitionIndex, numPartitions)) {
                continue;
            }

            final String topic = HedwigBenchmark.TOPIC_PREFIX + i;

            subscriber.subscribe(ByteString.copyFromUtf8(topic), subId, CreateOrAttach.CREATE_OR_ATTACH);
            subscriber.startDelivery(ByteString.copyFromUtf8(topic), subId, new MessageHandler() {

                @Override
                public void deliver(ByteString thisTopic, ByteString subscriberId, Message msg,
                Callback<Void> callback, Object context) {
                    logger.debug("Got message from src-region: {} with seq-id: {}",
                        msg.getSrcRegion(), msg.getMsgId());

                    String mapKey = topic + msg.getSrcRegion().toStringUtf8();
                    Long lastSeqIdSeen = lastSeqIdSeenMap.get(mapKey);
                    if (lastSeqIdSeen == null) {
                        lastSeqIdSeen = (long) 0;
                    }

                    if (getSrcSeqId(msg) <= lastSeqIdSeen) {
                        logger.info("Redelivery of message, src-region: " + msg.getSrcRegion() + "seq-id: "
                                    + msg.getMsgId());
                    } else {
                        agg.ding(false);
                    }

                    callback.operationFinished(context, null);
                }
            });
        }
        System.out.println("Finished subscribing to topics and now waiting for messages to come in...");
        // Wait till the benchmark test has completed
        agg.queue.take();
        System.out.println(agg.summarize(agg.earliest.get()));
        return null;
    }

    long getSrcSeqId(Message msg) {
        if (msg.getMsgId().getRemoteComponentsCount() == 0) {
            return msg.getMsgId().getLocalComponent();
        }

        for (RegionSpecificSeqId rseqId : msg.getMsgId().getRemoteComponentsList()) {
            if (rseqId.getRegion().equals(msg.getSrcRegion()))
                return rseqId.getSeqId();
        }

        return msg.getMsgId().getLocalComponent();
    }

    void multiSub(String label, String topicPrefix, int start, final int npar, final int count)
            throws InterruptedException {
        long startTime = MathUtils.now();
        ThroughputLatencyAggregator agg = new ThroughputLatencyAggregator(label, count / numPartitions, npar);
        agg.startProgress();

        int end = start + count;
        for (int i = start; i < end; ++i) {
            if (!HedwigBenchmark.amIResponsibleForTopic(i, partitionIndex, numPartitions)) {
                continue;
            }
            subscriber.asyncSubscribe(ByteString.copyFromUtf8(topicPrefix + i), subId, CreateOrAttach.CREATE_OR_ATTACH,
                                      new BenchmarkCallback(agg), null);
        }
        // Wait till the benchmark test has completed
        agg.tpAgg.queue.take();
        if (count > 1)
            System.out.println(agg.summarize(startTime));
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/benchmark/BenchmarkUtils.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.benchmark;

import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.Semaphore;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicLong;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.bookkeeper.util.MathUtils;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.util.Callback;

public class BenchmarkUtils {
    static final Logger logger = LoggerFactory.getLogger(BenchmarkUtils.class);

    public static double calcTp(final int count, long startTime) {
        return 1000. * count / (MathUtils.now() - startTime);
    }

    /**
     * Stats aggregator for callback (round-trip) operations. Measures both
     * throughput and latency.
     */
    public static class ThroughputLatencyAggregator {
        int numBuckets;
        final ThroughputAggregator tpAgg;
        final Semaphore outstanding;
        final AtomicLong sum = new AtomicLong();

        final AtomicLong[] latencyBuckets;

        // bucket[i] is count of number of operations that took >= i ms and <
        // (i+1) ms.

        public ThroughputLatencyAggregator(String label, int count, int limit) throws InterruptedException {
            numBuckets = Integer.getInteger("numBuckets", 101);
            latencyBuckets = new AtomicLong[numBuckets];
            tpAgg = new ThroughputAggregator(label, count);
            outstanding = new Semaphore(limit);
            for (int i = 0; i < numBuckets; i++) {
                latencyBuckets[i] = new AtomicLong();
            }
        }

        public void startProgress() {
            tpAgg.startProgress();
        }

        public void reportLatency(long latency) {
            sum.addAndGet(latency);

            int bucketIndex;
            if (latency >= numBuckets) {
                bucketIndex = (int) numBuckets - 1;
            } else {
                bucketIndex = (int) latency;
            }
            latencyBuckets[bucketIndex].incrementAndGet();
        }

        private String getPercentile(double percentile) {
            int numInliersNeeded = (int) (percentile / 100 * tpAgg.count);
            int numInliersFound = 0;
            for (int i = 0; i < numBuckets - 1; i++) {
                numInliersFound += latencyBuckets[i].intValue();
                if (numInliersFound > numInliersNeeded) {
                    return i + "";
                }
            }
            return " >= " + (numBuckets - 1);
        }

        public String summarize(long startTime) {
            double percentile = Double.parseDouble(System.getProperty("percentile", "99.9"));
            return tpAgg.summarize(startTime) + ", avg latency = " + sum.get() / tpAgg.count + ", " + percentile
                   + "%ile latency = " + getPercentile(percentile);
        }
    }

    /**
     * Stats aggregator for non-callback (single-shot) operations. Measures just
     * throughput.
     */
    public static class ThroughputAggregator {
        final String label;
        final int count;
        final AtomicInteger done = new AtomicInteger();
        final AtomicLong earliest = new AtomicLong();
        final AtomicInteger numFailed = new AtomicInteger();
        final Thread progressThread;
        final LinkedBlockingQueue<Integer> queue = new LinkedBlockingQueue<Integer>();

        public ThroughputAggregator(final String label, final int count) {
            this.label = label;
            this.count = count;
            if (count == 0)
                queue.add(0);
            if (Boolean.getBoolean("progress")) {
                progressThread = new Thread(new Runnable() {
                    @Override
                    public void run() {
                        try {
                            for (int doneSnap = 0, prev = 0; doneSnap < count; prev = doneSnap, doneSnap = done.get()) {
                                if (doneSnap > prev) {
                                    System.out.println(label + " progress: " + doneSnap + " of " + count);
                                }
                                Thread.sleep(1000);
                            }
                        } catch (Exception ex) {
                            throw new RuntimeException(ex);
                        }
                    }
                    });
            } else {
                progressThread = null;
            }
        }

        public void startProgress() {
            if (progressThread != null) {
                progressThread.start();
            }
        }

        public void ding(boolean failed) {
            int snapDone = done.incrementAndGet();
            earliest.compareAndSet(0, MathUtils.now());
            if (failed)
                numFailed.incrementAndGet();
            if (logger.isDebugEnabled())
                logger.debug(label + " " + (failed ? "failed" : "succeeded") + ", done so far = " + snapDone);
            if (snapDone == count) {
                queue.add(numFailed.get());
            }
        }

        public String summarize(long startTime) {
            return "Finished " + label + ": count = " + done.get() + ", tput = " + calcTp(count, startTime)
                   + " ops/s, numFailed = " + numFailed;
        }
    }

    public static class BenchmarkCallback implements Callback<Void> {

        final ThroughputLatencyAggregator agg;
        final long startTime;

        public BenchmarkCallback(ThroughputLatencyAggregator agg) throws InterruptedException {
            this.agg = agg;
            agg.outstanding.acquire();
            // Must set the start time *after* taking acquiring on outstanding.
            startTime = MathUtils.now();
        }

        private void finish(boolean failed) {
            agg.reportLatency(MathUtils.now() - startTime);
            agg.tpAgg.ding(failed);
            agg.outstanding.release();
        }

        @Override
        public void operationFinished(Object ctx, Void resultOfOperation) {
            finish(false);
        }

        @Override
        public void operationFailed(Object ctx, PubSubException exception) {
            finish(true);
        }
    };

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/benchmark/BenchmarkWorker.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.benchmark;

public class BenchmarkWorker {
    int numTopics;
    int numMessages;
    int numRegions;
    int startTopicLabel;
    int partitionIndex;
    int numPartitions;

    public BenchmarkWorker(int numTopics, int numMessages, int numRegions,
                           int startTopicLabel, int partitionIndex, int numPartitions) {
        this.numTopics = numTopics;
        this.numMessages = numMessages;
        this.numRegions = numRegions;
        this.startTopicLabel = startTopicLabel;
        this.partitionIndex = partitionIndex;
        this.numPartitions = numPartitions;

        if (numMessages % (numTopics * numRegions) != 0) {
            throw new RuntimeException("Number of messages not equally divisible among regions and topics");
        }

        if (numTopics % numPartitions != 0) {
            throw new RuntimeException("Number of topics not equally divisible among partitions");
        }

    }
}
"
hedwig-client/src/main/java/org/apache/hedwig/client/benchmark/HedwigBenchmark.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.benchmark;

import java.io.File;
import java.util.concurrent.Callable;

import org.apache.commons.configuration.ConfigurationException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.logging.InternalLoggerFactory;
import org.jboss.netty.logging.Log4JLoggerFactory;

import com.google.protobuf.ByteString;
import org.apache.hedwig.util.HedwigSocketAddress;
import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.HedwigClient;
import org.apache.hedwig.client.api.Publisher;
import org.apache.hedwig.client.api.Subscriber;

import org.apache.commons.cli.HelpFormatter;
import org.apache.commons.cli.Option;
import org.apache.commons.cli.Options;
import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.CommandLineParser;
import org.apache.commons.cli.PosixParser;
import org.apache.commons.cli.ParseException;

public class HedwigBenchmark implements Callable<Void> {
    protected static final Logger logger = LoggerFactory.getLogger(HedwigBenchmark.class);

    static final String TOPIC_PREFIX = "topic";

    private final HedwigClient client;
    private final Publisher publisher;
    private final Subscriber subscriber;
    private final CommandLine cmd;

    public HedwigBenchmark(ClientConfiguration cfg, CommandLine cmd) {
        client = new HedwigClient(cfg);
        publisher = client.getPublisher();
        subscriber = client.getSubscriber();
        this.cmd = cmd;
    }

    static boolean amIResponsibleForTopic(int topicNum, int partitionIndex, int numPartitions) {
        return topicNum % numPartitions == partitionIndex;
    }

    @Override
    public Void call() throws Exception {

        //
        // Parameters.
        //

        // What program to run: pub, sub (subscription benchmark), recv.
        final String mode = cmd.getOptionValue("mode","");

        // Number of requests to make (publishes or subscribes).
        int numTopics = Integer.valueOf(cmd.getOptionValue("nTopics", "50"));
        int numMessages = Integer.valueOf(cmd.getOptionValue("nMsgs", "1000"));
        int numRegions = Integer.valueOf(cmd.getOptionValue("nRegions", "1"));
        int startTopicLabel = Integer.valueOf(cmd.getOptionValue("startTopicLabel", "0"));
        int partitionIndex = Integer.valueOf(cmd.getOptionValue("partitionIndex", "0"));
        int numPartitions = Integer.valueOf(cmd.getOptionValue("nPartitions", "1"));

        int replicaIndex = Integer.valueOf(cmd.getOptionValue("replicaIndex", "0"));

        int rate = Integer.valueOf(cmd.getOptionValue("rate", "0"));
        int nParallel = Integer.valueOf(cmd.getOptionValue("npar", "100"));
        int msgSize = Integer.valueOf(cmd.getOptionValue("msgSize", "1024"));

        // Number of warmup subscriptions to make.
        final int nWarmups = Integer.valueOf(cmd.getOptionValue("nwarmups", "1000"));

        if (mode.equals("sub")) {
            BenchmarkSubscriber benchmarkSub = new BenchmarkSubscriber(numTopics, 0, 1, startTopicLabel, 0, 1,
                    subscriber, ByteString.copyFromUtf8("mySub"));

            benchmarkSub.warmup(nWarmups);
            benchmarkSub.call();

        } else if (mode.equals("recv")) {

            BenchmarkSubscriber benchmarkSub = new BenchmarkSubscriber(numTopics, numMessages, numRegions,
                    startTopicLabel, partitionIndex, numPartitions, subscriber, ByteString.copyFromUtf8("sub-"
                            + replicaIndex));

            benchmarkSub.call();

        } else if (mode.equals("pub")) {
            // Offered load in msgs/second.
            BenchmarkPublisher benchmarkPub = new BenchmarkPublisher(numTopics, numMessages, numRegions,
                    startTopicLabel, partitionIndex, numPartitions, publisher, subscriber, msgSize, nParallel, rate);
            benchmarkPub.warmup(nWarmups);
            benchmarkPub.call();

        } else {
            throw new Exception("unknown mode: " + mode);
        }

        return null;
    }

    public static void main(String[] args) throws Exception {
        Options options = new Options();
        options.addOption("mode", true, "sub, recv, or pub");
        options.addOption("nTopics", true, "Number of topics, default 50");
        options.addOption("nMsgs", true, "Number of messages, default 1000");
        options.addOption("nRegions", true, "Number of regsions, default 1");
        options.addOption("startTopicLabel", true,
                          "Prefix of topic labels. Must be numeric. Default 0");
        options.addOption("partitionIndex", true, "If partitioning, the partition index for this client");
        options.addOption("nPartitions", true, "Number of partitions, default 1");
        options.addOption("replicaIndex", true, "default 0");
        options.addOption("rate", true, "default 0");
        options.addOption("npar", true, "default 100");
        options.addOption("msgSize", true, "Size of messages, default 1024");
        options.addOption("nwarmups", true, "Number of warmup messages, default 1000");
        options.addOption("defaultHub", true, "Default hedwig hub to connect to, default localhost:4080");

        CommandLineParser parser = new PosixParser();
        final CommandLine cmd = parser.parse(options, args);

        if (cmd.hasOption("help")) {
            HelpFormatter formatter = new HelpFormatter();
            formatter.printHelp("HedwigBenchmark <options>", options);
            System.exit(-1);
        }

        ClientConfiguration cfg = new ClientConfiguration() {
                public HedwigSocketAddress getDefaultServerHedwigSocketAddress() {
                    return new HedwigSocketAddress(cmd.getOptionValue("defaultHub",
                                                                      "localhost:4080"));
                }

                public boolean isSSLEnabled() {
                    return false;
                }
            };

        InternalLoggerFactory.setDefaultFactory(new Log4JLoggerFactory());

        HedwigBenchmark app = new HedwigBenchmark(cfg, cmd);
        app.call();
        System.exit(0);
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/conf/ClientConfiguration.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.conf;

import java.net.InetSocketAddress;

import org.apache.commons.configuration.ConfigurationException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.hedwig.conf.AbstractConfiguration;
import org.apache.hedwig.util.HedwigSocketAddress;

public class ClientConfiguration extends AbstractConfiguration {
    Logger logger = LoggerFactory.getLogger(ClientConfiguration.class);

    // Protected member variables for configuration parameter names
    protected static final String DEFAULT_SERVER_HOST = "default_server_host";
    protected static final String MAX_MESSAGE_SIZE = "max_message_size";
    protected static final String MAX_SERVER_REDIRECTS = "max_server_redirects";
    protected static final String AUTO_SEND_CONSUME_MESSAGE_ENABLED = "auto_send_consume_message_enabled";
    protected static final String CONSUMED_MESSAGES_BUFFER_SIZE = "consumed_messages_buffer_size";
    protected static final String MESSAGE_CONSUME_RETRY_WAIT_TIME = "message_consume_retry_wait_time";
    protected static final String SUBSCRIBE_RECONNECT_RETRY_WAIT_TIME = "subscribe_reconnect_retry_wait_time";
    protected static final String MAX_OUTSTANDING_MESSAGES = "max_outstanding_messages";
    protected static final String SERVER_ACK_RESPONSE_TIMEOUT = "server_ack_response_timeout";
    protected static final String TIMEOUT_THREAD_RUN_INTERVAL = "timeout_thread_run_interval";
    protected static final String SSL_ENABLED = "ssl_enabled";
    protected static final String SUBSCRIPTION_MESSAGE_BOUND = "subscription_message_bound";
    protected static final String SUBSCRIPTION_CHANNEL_SHARING_ENABLED = "subscription_channel_sharing_enabled";

    // Singletons we want to instantiate only once per ClientConfiguration
    protected HedwigSocketAddress myDefaultServerAddress = null;

    // Getters for the various Client Configuration parameters.
    // This should point to the default server host, or the VIP fronting all of
    // the server hubs. This will return the HedwigSocketAddress which
    // encapsulates both the regular and SSL port connection to the server host.
    protected HedwigSocketAddress getDefaultServerHedwigSocketAddress() {
        if (myDefaultServerAddress == null)
            myDefaultServerAddress = new HedwigSocketAddress(conf.getString(DEFAULT_SERVER_HOST, "localhost:4080:9876"));
        return myDefaultServerAddress;
    }

    // This will get the default server InetSocketAddress based on if SSL is
    // enabled or not.
    public InetSocketAddress getDefaultServerHost() {
        if (isSSLEnabled())
            return getDefaultServerHedwigSocketAddress().getSSLSocketAddress();
        else
            return getDefaultServerHedwigSocketAddress().getSocketAddress();
    }

    public int getMaximumMessageSize() {
        return conf.getInt(MAX_MESSAGE_SIZE, 2 * 1024 * 1024);
    }

    // This parameter is for setting the maximum number of server redirects to
    // allow before we consider it as an error condition. This is to stop
    // infinite redirect loops in case there is a problem with the hub servers
    // topic mastership.
    public int getMaximumServerRedirects() {
        return conf.getInt(MAX_SERVER_REDIRECTS, 2);
    }

    // This parameter is a boolean flag indicating if the client library should
    // automatically send the consume message to the server based on the
    // configured amount of messages consumed by the client app. The client app
    // could choose to override this behavior and instead, manually send the
    // consume message to the server via the client library using its own
    // logic and policy.
    public boolean isAutoSendConsumeMessageEnabled() {
        return conf.getBoolean(AUTO_SEND_CONSUME_MESSAGE_ENABLED, true);
    }

    // This parameter is to set how many consumed messages we'll buffer up
    // before we send the Consume message to the server indicating that all
    // of the messages up to that point have been successfully consumed by
    // the client.
    public int getConsumedMessagesBufferSize() {
        return conf.getInt(CONSUMED_MESSAGES_BUFFER_SIZE, 5);
    }

    // This parameter is used to determine how long we wait before retrying the
    // client app's MessageHandler to consume a subscribed messages sent to us
    // from the server. The time to wait is in milliseconds.
    public long getMessageConsumeRetryWaitTime() {
        return conf.getLong(MESSAGE_CONSUME_RETRY_WAIT_TIME, 10000);
    }

    // This parameter is used to determine how long we wait before retrying the
    // Subscribe Reconnect request. This is done when the connection to a server
    // disconnects and we attempt to connect to it. We'll keep on trying but
    // in case the server(s) is down for a longer time, we want to throttle
    // how often we do the subscribe reconnect request. The time to wait is in
    // milliseconds.
    public long getSubscribeReconnectRetryWaitTime() {
        return conf.getLong(SUBSCRIBE_RECONNECT_RETRY_WAIT_TIME, 10000);
    }

    // This parameter is for setting the maximum number of outstanding messages
    // the client app can be consuming at a time for topic subscription before
    // we throttle things and stop reading from the Netty Channel.
    public int getMaximumOutstandingMessages() {
        return conf.getInt(MAX_OUTSTANDING_MESSAGES, 10);
    }

    // This parameter is used to determine how long we wait (in milliseconds)
    // before we time out outstanding PubSubRequests that were written to the
    // server successfully but haven't yet received the ack response.
    public long getServerAckResponseTimeout() {
        return conf.getLong(SERVER_ACK_RESPONSE_TIMEOUT, 30000);
    }

    // This parameter is used to determine how often we run the server ack
    // response timeout cleaner thread (in milliseconds).
    public long getTimeoutThreadRunInterval() {
        return conf.getLong(TIMEOUT_THREAD_RUN_INTERVAL, 60000);
    }

    // This parameter is a boolean flag indicating if communication with the
    // server should be done via SSL for encryption. This is needed for
    // cross-colo hub clients listening to non-local servers.
    public boolean isSSLEnabled() {
        return conf.getBoolean(SSL_ENABLED, false);
    }

    /**
     * This parameter is a boolean flag indicating if multiplexing subscription
     * channels.
     */
    public boolean isSubscriptionChannelSharingEnabled() {
        return conf.getBoolean(SUBSCRIPTION_CHANNEL_SHARING_ENABLED, false);
    }

    /**
     * The maximum number of messages the hub will queue for subscriptions
     * created using this configuration. The hub will always queue the most
     * recent messages. If there are enough publishes to the topic to hit
     * the bound, then the oldest messages are dropped from the queue.
     *
     * A bound of 0 disabled the bound completely. This is the default.
     */
    public int getSubscriptionMessageBound() {
        return conf.getInt(SUBSCRIPTION_MESSAGE_BOUND, 0);
    }

    // Validate that the configuration properties are valid.
    public void validate() throws ConfigurationException {
        if (isSSLEnabled() && getDefaultServerHedwigSocketAddress().getSSLSocketAddress() == null) {
            throw new ConfigurationException("SSL is enabled but a default server SSL port not given!");
        }
        // Add other validation checks here
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/data/MessageConsumeData.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.data;

import org.apache.hedwig.client.data.TopicSubscriber;
import org.apache.hedwig.protocol.PubSubProtocol.Message;

/**
 * Wrapper class to store all of the data points needed to encapsulate Message
 * Consumption in the Subscribe flow for consuming a message sent from the
 * server for a given TopicSubscriber. This will be used as the Context in the
 * VoidCallback for the MessageHandlers once they've completed consuming the
 * message.
 *
 */
public class MessageConsumeData {

    // Member variables
    public final TopicSubscriber topicSubscriber;
    // This is the Message sent from the server for Subscribes for consumption
    // by the client.
    public final Message msg;

    // Constructor
    public MessageConsumeData(final TopicSubscriber topicSubscriber, final Message msg) {
        this.topicSubscriber = topicSubscriber;
        this.msg = msg;
    }

    @Override
    public String toString() {
        StringBuilder sb = new StringBuilder();
        if (topicSubscriber != null) {
            sb.append("Subscription: ").append(topicSubscriber);
        }
        if (msg != null) {
            sb.append(PubSubData.COMMA).append("Message: ").append(msg);
        }
        return sb.toString();
    }
}
"
hedwig-client/src/main/java/org/apache/hedwig/client/data/PubSubData.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.data;

import java.util.List;

import com.google.protobuf.ByteString;
import org.apache.hedwig.client.netty.HChannel;
import org.apache.hedwig.protocol.PubSubProtocol;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.OperationType;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions;
import org.apache.hedwig.util.Callback;

/**
 * Wrapper class to store all of the data points needed to encapsulate all
 * PubSub type of request operations the client will do. This includes knowing
 * all of the information needed if we need to redo the publish/subscribe
 * request in case of a server redirect. This will be used for all sync/async
 * calls, and for all the known types of request messages to send to the server
 * hubs: Publish, Subscribe, Unsubscribe, and Consume.
 *
 */
public class PubSubData {
    // Static string constants
    protected static final String COMMA = ", ";

    // Member variables needed during object construction time.
    public final ByteString topic;
    public final Message msg;
    public final ByteString subscriberId;
    // Enum to indicate what type of operation this PubSub request data object
    // is for.
    public final OperationType operationType;
    // Options for the subscription
    public final SubscriptionOptions options;

    // These two variables are not final since we might override them
    // in the case of a Subscribe reconnect.
    private Callback<PubSubProtocol.ResponseBody> callback;

    public Object context;

    // Member variables used after object has been constructed.
    // List of all servers we've sent the PubSubRequest to successfully.
    // This is to keep track of redirected servers that responded back to us.
    public List<ByteString> triedServers;
    // List of all servers that we've tried to connect or write to but
    // was unsuccessful. We'll retry sending the PubSubRequest but will
    // quit if we're trying to connect or write to a server that we've
    // attempted to previously.
    public List<ByteString> connectFailedServers;
    public List<ByteString> writeFailedServers;
    // Boolean to the hub server indicating if it should claim ownership
    // of the topic the PubSubRequest is for. This is mainly used after
    // a server redirect. Defaults to false.
    public boolean shouldClaim = false;
    // TxnID for the PubSubData if it was sent as a PubSubRequest to the hub
    // server. This is used in the WriteCallback in case of failure. We want
    // to remove it from the ResponseHandler.txn2PubSubData map since the
    // failed PubSubRequest will not get an ack response from the server.
    // This is set later in the PubSub flows only when we write the actual
    // request. Therefore it is not an argument in the constructor.
    public long txnId;
    // Time in milliseconds using the System.currentTimeMillis() call when the
    // PubSubRequest was written on the netty Channel to the server.
    public long requestWriteTime;
    // For synchronous calls, this variable is used to know when the background
    // async process for it has completed, set in the VoidCallback.
    public boolean isDone = false;
    // Record the original channel for a resubscribe request
    private HChannel origChannel = null;

    // Constructor for all types of PubSub request data to send to the server
    public PubSubData(final ByteString topic, final Message msg, final ByteString subscriberId,
                      final OperationType operationType, final SubscriptionOptions options,
                      final Callback<PubSubProtocol.ResponseBody> callback,
                      final Object context) {
        this.topic = topic;
        this.msg = msg;
        this.subscriberId = subscriberId;
        this.operationType = operationType;
        this.options = options;
        this.callback = callback;
        this.context = context;
    }

    public void setCallback(Callback<PubSubProtocol.ResponseBody> callback) {
        this.callback = callback;
    }

    public Callback<PubSubProtocol.ResponseBody> getCallback() {
        return callback;
    }

    public void operationFinishedToCallback(Object context, PubSubProtocol.ResponseBody response){
        callback.operationFinished(context, response);
    }

    public boolean isResubscribeRequest() {
        return null != origChannel;
    }

    public HChannel getOriginalChannelForResubscribe() {
        return origChannel;
    }

    public void setOriginalChannelForResubscribe(HChannel channel) {
        this.origChannel = channel;
    }

    // Clear all of the stored servers we've contacted or attempted to in this
    // request.
    public void clearServersList() {
        if (triedServers != null)
            triedServers.clear();
        if (connectFailedServers != null)
            connectFailedServers.clear();
        if (writeFailedServers != null)
            writeFailedServers.clear();
    }

    @Override
    public String toString() {
        StringBuilder sb = new StringBuilder();
        if (topic != null)
            sb.append("Topic: " + topic.toStringUtf8());
        if (msg != null)
            sb.append(COMMA).append("Message: " + msg);
        if (subscriberId != null)
            sb.append(COMMA).append("SubscriberId: " + subscriberId.toStringUtf8());
        if (operationType != null)
            sb.append(COMMA).append("Operation Type: " + operationType.toString());
        if (options != null)
            sb.append(COMMA).append("Create Or Attach: " + options.getCreateOrAttach().toString())
                .append(COMMA).append("Message Bound: " + options.getMessageBound());
        if (triedServers != null && triedServers.size() > 0) {
            sb.append(COMMA).append("Tried Servers: ");
            for (ByteString triedServer : triedServers) {
                sb.append(triedServer.toStringUtf8()).append(COMMA);
            }
        }
        if (connectFailedServers != null && connectFailedServers.size() > 0) {
            sb.append(COMMA).append("Connect Failed Servers: ");
            for (ByteString connectFailedServer : connectFailedServers) {
                sb.append(connectFailedServer.toStringUtf8()).append(COMMA);
            }
        }
        if (writeFailedServers != null && writeFailedServers.size() > 0) {
            sb.append(COMMA).append("Write Failed Servers: ");
            for (ByteString writeFailedServer : writeFailedServers) {
                sb.append(writeFailedServer.toStringUtf8()).append(COMMA);
            }
        }
        sb.append(COMMA).append("Should Claim: " + shouldClaim);
        if (txnId != 0)
            sb.append(COMMA).append("TxnID: " + txnId);
        if (requestWriteTime != 0)
            sb.append(COMMA).append("Request Write Time: " + requestWriteTime);
        sb.append(COMMA).append("Is Done: " + isDone);
        return sb.toString();
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/data/TopicSubscriber.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.data;

import org.apache.commons.lang.builder.HashCodeBuilder;

import com.google.protobuf.ByteString;

/**
 * Wrapper class object for the Topic + SubscriberId combination. Since the
 * Subscribe flows always use the Topic + SubscriberId as the logical entity,
 * we'll create a simple class to encapsulate that.
 *
 */
public class TopicSubscriber {
    private final ByteString topic;
    private final ByteString subscriberId;
    private final int hashCode;

    public TopicSubscriber(final ByteString topic, final ByteString subscriberId) {
        this.topic = topic;
        this.subscriberId = subscriberId;
        hashCode = new HashCodeBuilder().append(topic).append(subscriberId).toHashCode();
    }

    @Override
    public boolean equals(final Object o) {
        if (o == this)
            return true;
        if (!(o instanceof TopicSubscriber))
            return false;
        final TopicSubscriber obj = (TopicSubscriber) o;
        return topic.equals(obj.topic) && subscriberId.equals(obj.subscriberId);
    }

    @Override
    public int hashCode() {
        return hashCode;
    }

    @Override
    public String toString() {
        StringBuilder sb = new StringBuilder();
        if (topic != null)
            sb.append("Topic: " + topic.toStringUtf8());
        if (subscriberId != null)
            sb.append(PubSubData.COMMA).append("SubscriberId: " + subscriberId.toStringUtf8());
        return sb.toString();
    }

    public ByteString getTopic() {
        return topic;
    }

    public ByteString getSubscriberId() {
        return subscriberId;
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/exceptions/AlreadyStartDeliveryException.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.exceptions;

/**
 * This is a Hedwig client side exception when the local client wants to
 * startDelivery using another message handler before stopping previous one.
 */
public class AlreadyStartDeliveryException extends Exception {

    private static final long serialVersionUID = 873259807218723524L;

    public AlreadyStartDeliveryException(String message) {
        super(message);
    }

    public AlreadyStartDeliveryException(String message, Throwable t) {
        super(message, t);
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/exceptions/InvalidSubscriberIdException.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.exceptions;

/**
 * This is a Hedwig client side exception when the local client wants to do
 * subscribe type of operations. Currently, to distinguish between local and hub
 * subscribers, the subscriberId will have a specific format.
 */
public class InvalidSubscriberIdException extends Exception {

    private static final long serialVersionUID = 873259807218723523L;

    public InvalidSubscriberIdException(String message) {
        super(message);
    }

    public InvalidSubscriberIdException(String message, Throwable t) {
        super(message, t);
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/exceptions/NoResponseHandlerException.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.exceptions;

/**
 * This is a Hedwig client side exception thrown when it can't get the response
 * handler from the channel pipeline responsible for a PubSubRequest.
 */
public class NoResponseHandlerException extends Exception {
    private static final long serialVersionUID = 1L;

    public NoResponseHandlerException(String message) {
        super(message);
    }

    public NoResponseHandlerException(String message, Throwable t) {
        super(message, t);
    }
}
"
hedwig-client/src/main/java/org/apache/hedwig/client/exceptions/ResubscribeException.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.exceptions;

/**
 * This is a Hedwig client side exception when the client failed to resubscribe
 * when topic moved or subscription is closed.
 */
public class ResubscribeException extends Exception {

    public ResubscribeException(String message) {
        super(message);
    }

    public ResubscribeException(String message, Throwable t) {
        super(message, t);
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/exceptions/ServerRedirectLoopException.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.exceptions;

/**
 * This is a Hedwig client side exception when the PubSubRequest is being
 * redirected to a server where the request has already been sent to previously.
 * To avoid having a cyclical redirect loop, this condition is checked for
 * and this exception will be thrown to the client caller.
 */
public class ServerRedirectLoopException extends Exception {

    private static final long serialVersionUID = 98723508723152897L;

    public ServerRedirectLoopException(String message) {
        super(message);
    }

    public ServerRedirectLoopException(String message, Throwable t) {
        super(message, t);
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/exceptions/TooManyServerRedirectsException.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.exceptions;

/**
 * This is a Hedwig client side exception when there have been too many server
 * redirects during a publish/subscribe call. We only allow a certain number of
 * server redirects to find the topic master. If we have exceeded this
 * configured amount, the publish/subscribe will fail with this exception.
 *
 */
public class TooManyServerRedirectsException extends Exception {

    private static final long serialVersionUID = 2341192937965635310L;

    public TooManyServerRedirectsException(String message) {
        super(message);
    }

    public TooManyServerRedirectsException(String message, Throwable t) {
        super(message, t);
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/handlers/AbstractResponseHandler.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.handlers;

import java.net.InetSocketAddress;
import java.util.LinkedList;

import com.google.protobuf.ByteString;

import org.jboss.netty.channel.Channel;

import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.client.netty.HChannelManager;
import org.apache.hedwig.client.exceptions.ServerRedirectLoopException;
import org.apache.hedwig.client.exceptions.TooManyServerRedirectsException;
import org.apache.hedwig.client.netty.NetUtils;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.ServiceDownException;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse;
import org.apache.hedwig.util.HedwigSocketAddress;
import static org.apache.hedwig.util.VarArgs.va;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public abstract class AbstractResponseHandler {

    private static Logger logger = LoggerFactory.getLogger(AbstractResponseHandler.class);

    protected final ClientConfiguration cfg;
    protected final HChannelManager channelManager;

    protected AbstractResponseHandler(ClientConfiguration cfg,
                                      HChannelManager channelManager) {
        this.cfg = cfg;
        this.channelManager = channelManager;
    }

    /**
     * Logic to handle received response.
     *
     * @param response
     *            PubSubResponse received from hub server.
     * @param pubSubData
     *            PubSubData for the pub/sub request.
     * @param channel
     *            Channel we used to make the request.
     */
    public abstract void handleResponse(PubSubResponse response, PubSubData pubSubData,
                                        Channel channel) throws Exception;

    /**
     * Logic to repost a PubSubRequest when the server responds with a redirect
     * indicating they are not the topic master.
     *
     * @param response
     *            PubSubResponse from the server for the redirect
     * @param pubSubData
     *            PubSubData for the original PubSubRequest made
     * @param channel
     *            Channel Channel we used to make the original PubSubRequest
     * @throws Exception
     *             Throws an exception if there was an error in doing the
     *             redirect repost of the PubSubRequest
     */
    protected void handleRedirectResponse(PubSubResponse response, PubSubData pubSubData,
                                          Channel channel)
            throws Exception {
        if (logger.isDebugEnabled()) {
            logger.debug("Handling a redirect from host: {}, response: {}, pubSubData: {}",
                         va(NetUtils.getHostFromChannel(channel), response, pubSubData));
        }
        // In this case, the PubSub request was done to a server that is not
        // responsible for the topic. First make sure that we haven't
        // exceeded the maximum number of server redirects.
        int curNumServerRedirects = (pubSubData.triedServers == null) ? 0 : pubSubData.triedServers.size();
        if (curNumServerRedirects >= cfg.getMaximumServerRedirects()) {
            // We've already exceeded the maximum number of server redirects
            // so consider this as an error condition for the client.
            // Invoke the operationFailed callback and just return.
            logger.debug("Exceeded the number of server redirects ({}) so error out.",
                         curNumServerRedirects);
            PubSubException exception = new ServiceDownException(
                new TooManyServerRedirectsException("Already reached max number of redirects: "
                                                    + curNumServerRedirects));
            pubSubData.getCallback().operationFailed(pubSubData.context, exception);
            return;
        }

        // We will redirect and try to connect to the correct server
        // stored in the StatusMsg of the response. First store the
        // server that we sent the PubSub request to for the topic.
        ByteString triedServer = ByteString.copyFromUtf8(HedwigSocketAddress.sockAddrStr(
                                                         NetUtils.getHostFromChannel(channel)));
        if (pubSubData.triedServers == null) {
            pubSubData.triedServers = new LinkedList<ByteString>();
        }
        pubSubData.shouldClaim = true;
        pubSubData.triedServers.add(triedServer);

        // Now get the redirected server host (expected format is
        // Hostname:Port:SSLPort) from the server's response message. If one is
        // not given for some reason, then redirect to the default server
        // host/VIP to repost the request.
        String statusMsg = response.getStatusMsg();
        InetSocketAddress redirectedHost;
        boolean redirectToDefaultServer;
        if (statusMsg != null && statusMsg.length() > 0) {
            if (cfg.isSSLEnabled()) {
                redirectedHost = new HedwigSocketAddress(statusMsg).getSSLSocketAddress();
            } else {
                redirectedHost = new HedwigSocketAddress(statusMsg).getSocketAddress();
            }
            redirectToDefaultServer = false;
        } else {
            redirectedHost = cfg.getDefaultServerHost();
            redirectToDefaultServer = true;
        }

        // Make sure the redirected server is not one we've already attempted
        // already before in this PubSub request.
        if (pubSubData.triedServers.contains(ByteString.copyFromUtf8(HedwigSocketAddress.sockAddrStr(redirectedHost)))) {
            logger.error("We've already sent this PubSubRequest before to redirectedHost: {}, pubSubData: {}",
                         va(redirectedHost, pubSubData));
            PubSubException exception = new ServiceDownException(
                new ServerRedirectLoopException("Already made the request before to redirected host: "
                                                + redirectedHost));
            pubSubData.getCallback().operationFailed(pubSubData.context, exception);
            return;
        }

        // submit the pub/sub request to redirected host
        if (redirectToDefaultServer) {
            channelManager.submitOpToDefaultServer(pubSubData);
        } else {
            channelManager.redirectToHost(pubSubData, redirectedHost);
        }
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/handlers/CloseSubscriptionResponseHandler.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.handlers;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;

import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.client.data.TopicSubscriber;
import org.apache.hedwig.client.netty.HChannelManager;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.ClientNotSubscribedException;
import org.apache.hedwig.exceptions.PubSubException.ServiceDownException;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse;
import org.apache.hedwig.protocol.PubSubProtocol.ResponseBody;
import org.apache.hedwig.util.Callback;
import static org.apache.hedwig.util.VarArgs.va;

public class CloseSubscriptionResponseHandler extends AbstractResponseHandler {

    private static Logger logger =
        LoggerFactory.getLogger(CloseSubscriptionResponseHandler.class);

    public CloseSubscriptionResponseHandler(ClientConfiguration cfg,
                                            HChannelManager channelManager) {
        super(cfg, channelManager);
    }

    @Override
    public void handleResponse(final PubSubResponse response, final PubSubData pubSubData,
                               final Channel channel)
            throws Exception {
        switch (response.getStatusCode()) {
        case SUCCESS:
            pubSubData.getCallback().operationFinished(pubSubData.context, null);
            break;
        case CLIENT_NOT_SUBSCRIBED:
            // For closesubscription requests, the server says that the client was
            // never subscribed to the topic.
            pubSubData.getCallback().operationFailed(pubSubData.context, new ClientNotSubscribedException(
                                                    "Client was never subscribed to topic: " +
                                                        pubSubData.topic.toStringUtf8() + ", subscriberId: " +
                                                        pubSubData.subscriberId.toStringUtf8()));
            break;
        case SERVICE_DOWN:
            // Response was service down failure so just invoke the callback's
            // operationFailed method.
            pubSubData.getCallback().operationFailed(pubSubData.context, new ServiceDownException(
                                                    "Server responded with a SERVICE_DOWN status"));
            break;
        case NOT_RESPONSIBLE_FOR_TOPIC:
            // Redirect response so we'll need to repost the original
            // Unsubscribe Request
            handleRedirectResponse(response, pubSubData, channel);
            break;
        default:
            // Consider all other status codes as errors, operation failed
            // cases.
            logger.error("Unexpected error response from server for PubSubResponse: " + response);
            pubSubData.getCallback().operationFailed(pubSubData.context, new ServiceDownException(
                                                    "Server responded with a status code of: " +
                                                        response.getStatusCode()));
            break;
        }
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/handlers/MessageConsumeCallback.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.handlers;

import java.util.TimerTask;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.data.MessageConsumeData;
import org.apache.hedwig.client.netty.HChannelManager;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.protoextensions.MessageIdUtils;
import org.apache.hedwig.util.Callback;
import static org.apache.hedwig.util.VarArgs.va;

/**
 * This is the Callback used by the MessageHandlers on the client app when
 * they've finished consuming a subscription message sent from the server
 * asynchronously. This callback back to the client libs will be stateless so we
 * can use a singleton for the class. The object context used should be the
 * MessageConsumeData type. That will contain all of the information needed to
 * call the message consume logic in the client lib HChannelHandler.
 *
 */
public class MessageConsumeCallback implements Callback<Void> {

    private static Logger logger = LoggerFactory.getLogger(MessageConsumeCallback.class);

    private final HChannelManager channelManager;
    private final long consumeRetryWaitTime;

    public MessageConsumeCallback(ClientConfiguration cfg,
                                  HChannelManager channelManager) {
        this.channelManager = channelManager;
        this.consumeRetryWaitTime =
            cfg.getMessageConsumeRetryWaitTime();
    }

    class MessageConsumeRetryTask extends TimerTask {
        private final MessageConsumeData messageConsumeData;

        public MessageConsumeRetryTask(MessageConsumeData messageConsumeData) {
            this.messageConsumeData = messageConsumeData;
        }

        @Override
        public void run() {
            // Try to consume the message again
            SubscribeResponseHandler subscribeHChannelHandler =
                channelManager.getSubscribeResponseHandler(messageConsumeData.topicSubscriber);
            if (null == subscribeHChannelHandler ||
                !subscribeHChannelHandler.hasSubscription(messageConsumeData.topicSubscriber)) {
                logger.warn("No subscription {} found to retry delivering message {}.",
                            va(messageConsumeData.topicSubscriber,
                               MessageIdUtils.msgIdToReadableString(messageConsumeData.msg.getMsgId())));
                return;
            }

            subscribeHChannelHandler.asyncMessageDeliver(messageConsumeData.topicSubscriber,
                                                         messageConsumeData.msg);
        }
    }

    public void operationFinished(Object ctx, Void resultOfOperation) {
        MessageConsumeData messageConsumeData = (MessageConsumeData) ctx;

        SubscribeResponseHandler subscribeHChannelHandler =
            channelManager.getSubscribeResponseHandler(messageConsumeData.topicSubscriber);
        if (null == subscribeHChannelHandler ||
            !subscribeHChannelHandler.hasSubscription(messageConsumeData.topicSubscriber)) {
            logger.warn("No subscription {} found to consume message {}.",
                        va(messageConsumeData.topicSubscriber,
                           MessageIdUtils.msgIdToReadableString(messageConsumeData.msg.getMsgId())));
            return;
        }

        // Message has been successfully consumed by the client app so callback
        // to the HChannelHandler indicating that the message is consumed.
        subscribeHChannelHandler.messageConsumed(messageConsumeData.topicSubscriber,
                                                 messageConsumeData.msg);
    }

    public void operationFailed(Object ctx, PubSubException exception) {
        // Message has NOT been successfully consumed by the client app so
        // callback to the HChannelHandler to try the async MessageHandler
        // Consume logic again.
        MessageConsumeData messageConsumeData = (MessageConsumeData) ctx;
        logger.error("Message was not consumed successfully by client MessageHandler: {}",
                     messageConsumeData);

        // Sleep a pre-configured amount of time (in milliseconds) before we
        // do the retry. In the future, we can have more dynamic logic on
        // what duration to sleep based on how many times we've retried, or
        // perhaps what the last amount of time we slept was. We could stick
        // some of this meta-data into the MessageConsumeData when we retry.
        channelManager.schedule(new MessageConsumeRetryTask(messageConsumeData),
                                consumeRetryWaitTime);
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/handlers/PublishResponseHandler.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.handlers;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;

import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.client.netty.HChannelManager;
import org.apache.hedwig.exceptions.PubSubException.ServiceDownException;
import org.apache.hedwig.protocol.PubSubProtocol;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse;

public class PublishResponseHandler extends AbstractResponseHandler {

    private static Logger logger = LoggerFactory.getLogger(PublishResponseHandler.class);

    public PublishResponseHandler(ClientConfiguration cfg,
                                  HChannelManager channelManager) {
        super(cfg, channelManager);
    }

    @Override
    public void handleResponse(PubSubResponse response, PubSubData pubSubData,
                               Channel channel) throws Exception {
        switch (response.getStatusCode()) {
        case SUCCESS:
            // Response was success so invoke the callback's operationFinished
            // method.
            pubSubData.operationFinishedToCallback(pubSubData.context,
                response.hasResponseBody() ? response.getResponseBody() : null);
            break;
        case SERVICE_DOWN:
            // Response was service down failure so just invoke the callback's
            // operationFailed method.
            pubSubData.getCallback().operationFailed(pubSubData.context, new ServiceDownException(
                                                    "Server responded with a SERVICE_DOWN status"));
            break;
        case NOT_RESPONSIBLE_FOR_TOPIC:
            // Redirect response so we'll need to repost the original Publish
            // Request
            handleRedirectResponse(response, pubSubData, channel);
            break;
        default:
            // Consider all other status codes as errors, operation failed
            // cases.
            logger.error("Unexpected error response from server for PubSubResponse: " + response);
            pubSubData.getCallback().operationFailed(pubSubData.context, new ServiceDownException(
                                                    "Server responded with a status code of: " +
                                                        response.getStatusCode()));
            break;
        }
    }
}
"
hedwig-client/src/main/java/org/apache/hedwig/client/handlers/PubSubCallback.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.handlers;

import org.apache.hedwig.protocol.PubSubProtocol;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.util.Callback;

/**
 * This class is used when we are doing synchronous type of operations. All
 * underlying client ops in Hedwig are async so this is just a way to make the
 * async calls synchronous.
 *
 */
public class PubSubCallback implements Callback<PubSubProtocol.ResponseBody> {

    private static Logger logger = LoggerFactory.getLogger(PubSubCallback.class);

    // Private member variables
    private final PubSubData pubSubData;
    // Boolean indicator to see if the sync PubSub call was successful or not.
    private boolean isCallSuccessful;
    // For sync callbacks, we'd like to know what the PubSubException is thrown
    // on failure. This is so we can have a handle to the exception and rethrow
    // it later.
    private PubSubException failureException;

    private PubSubProtocol.ResponseBody responseBody;

    // Constructor
    public PubSubCallback(PubSubData pubSubData) {
        this.pubSubData = pubSubData;
    }

    public void operationFinished(Object ctx, PubSubProtocol.ResponseBody resultOfOperation) {
        logger.debug("PubSub call succeeded for pubSubData: {}", pubSubData);
        // Wake up the main sync PubSub thread that is waiting for us to
        // complete.
        synchronized (pubSubData) {
            this.responseBody = resultOfOperation;
            isCallSuccessful = true;
            pubSubData.isDone = true;
            pubSubData.notify();
        }
    }

    public void operationFailed(Object ctx, PubSubException exception) {
        logger.debug("PubSub call failed with exception: {}, pubSubData: {}", exception, pubSubData);
        // Wake up the main sync PubSub thread that is waiting for us to
        // complete.
        synchronized (pubSubData) {
            isCallSuccessful = false;
            failureException = exception;
            pubSubData.isDone = true;
            pubSubData.notify();
        }
    }

    // Public getter to determine if the PubSub callback is successful or not
    // based on the PubSub ack response from the server.
    public boolean getIsCallSuccessful() {
        return isCallSuccessful;
    }

    // Public getter to retrieve what the PubSubException was that occurred when
    // the operation failed.
    public PubSubException getFailureException() {
        return failureException;
    }


    public PubSubProtocol.ResponseBody getResponseBody() {
        return responseBody;
    }
}
"
hedwig-client/src/main/java/org/apache/hedwig/client/handlers/SubscribeResponseHandler.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.handlers;

import java.net.InetSocketAddress;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;

import com.google.protobuf.ByteString;

import org.apache.hedwig.client.api.MessageHandler;
import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.data.TopicSubscriber;
import org.apache.hedwig.client.exceptions.AlreadyStartDeliveryException;
import org.apache.hedwig.client.exceptions.NoResponseHandlerException;
import org.apache.hedwig.client.netty.HChannelManager;
import org.apache.hedwig.exceptions.PubSubException.ClientNotSubscribedException;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse;
import org.apache.hedwig.protocol.PubSubProtocol.ResponseBody;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEvent;
import org.apache.hedwig.protoextensions.SubscriptionStateUtils;
import org.apache.hedwig.util.Callback;

/**
 * An interface provided to manage all subscriptions on a channel.
 *
 * Its responsibility is to handle all subscribe responses received on that channel,
 * clear up subscriptions and retry reconnectin subscriptions when channel disconnected,
 * and handle delivering messages to {@link MessageHandler} and sent consume messages
 * back to hub servers.
 */
public abstract class SubscribeResponseHandler extends AbstractResponseHandler {

    protected SubscribeResponseHandler(ClientConfiguration cfg,
                                       HChannelManager channelManager) {
        super(cfg, channelManager);
    }

    /**
     * Handle Message delivered by the server.
     *
     * @param response
     *          Message received from the server.
     */
    public abstract void handleSubscribeMessage(PubSubResponse response);

    /**
     * Handle a subscription event delivered by the server.
     *
     * @param topic
     *          Topic Name
     * @param subscriberId
     *          Subscriber Id
     * @param event
     *          Subscription Event describes its status
     */
    public abstract void handleSubscriptionEvent(ByteString topic,
                                                 ByteString subscriberId,
                                                 SubscriptionEvent event);

    /**
     * Method called when a message arrives for a subscribe Channel and we want
     * to deliver it asynchronously via the registered MessageHandler (should
     * not be null when called here).
     *
     * @param message
     *            Message from Subscribe Channel we want to consume.
     */
    protected abstract void asyncMessageDeliver(TopicSubscriber topicSubscriber,
                                                Message message);

    /**
     * Method called when the client app's MessageHandler has asynchronously
     * completed consuming a subscribed message sent from the server. The
     * contract with the client app is that messages sent to the handler to be
     * consumed will have the callback response done in the same order. So if we
     * asynchronously call the MessageHandler to consume messages #1-5, that
     * should call the messageConsumed method here via the VoidCallback in the
     * same order. To make this thread safe, since multiple outstanding messages
     * could be consumed by the client app and then called back to here, make
     * this method synchronized.
     *
     * @param topicSubscriber
     *            Topic Subscriber
     * @param message
     *            Message sent from server for topic subscription that has been
     *            consumed by the client.
     */
    protected abstract void messageConsumed(TopicSubscriber topicSubscriber,
                                            Message message);

    /**
     * Start delivering messages for a given topic subscriber.
     *
     * @param topicSubscriber
     *            Topic Subscriber
     * @param messageHandler
     *            MessageHandler to register for this ResponseHandler instance.
     * @throws ClientNotSubscribedException
     *            If the client is not currently subscribed to the topic
     * @throws AlreadyStartDeliveryException
     *            If someone started delivery a message handler before stopping existed one.
     */
    public abstract void startDelivery(TopicSubscriber topicSubscriber,
                                       MessageHandler messageHandler)
    throws ClientNotSubscribedException, AlreadyStartDeliveryException;

    /**
     * Stop delivering messages for a given topic subscriber.
     *
     * @param topicSubscriber
     *            Topic Subscriber
     * @throws ClientNotSubscribedException
     *             If the client is not currently subscribed to the topic
     */
    public abstract void stopDelivery(TopicSubscriber topicSubscriber)
    throws ClientNotSubscribedException;

    /**
     * Whether the given topic subscriber subscribed thru this handler.
     *
     * @param topicSubscriber
     *            Topic Subscriber
     * @return whether the given topic subscriber subscribed thru this handler.
     */
    public abstract boolean hasSubscription(TopicSubscriber topicSubscriber);

    /**
     * Close subscription from this handler.
     *
     * @param topicSubscriber
     *            Topic Subscriber
     * @param callback
     *            Callback when the subscription is closed. 
     * @param context
     *            Callback context.
     */
    public abstract void asyncCloseSubscription(TopicSubscriber topicSubscriber,
                                                Callback<ResponseBody> callback,
                                                Object context);

    /**
     * Consume a given message for given topic subscriber thru this handler.
     *
     * @param topicSubscriber
     *            Topic Subscriber
     */
    public abstract void consume(TopicSubscriber topicSubscriber,
                                 MessageSeqId messageSeqId);

    /**
     * This method is called when the underlying channel is disconnected due to server failure.
     *
     * The implementation should take the responsibility to clear subscriptions and retry
     * reconnecting subscriptions to new hub servers.
     *
     * @param host
     *          Host that channel connected to has disconnected.
     * @param channel
     *          Channel connected to.
     */
    public abstract void onChannelDisconnected(InetSocketAddress host,
                                               Channel channel);
}
"
hedwig-client/src/main/java/org/apache/hedwig/client/handlers/UnsubscribeResponseHandler.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.handlers;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;

import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.client.data.TopicSubscriber;
import org.apache.hedwig.client.netty.HChannelManager;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.ClientNotSubscribedException;
import org.apache.hedwig.exceptions.PubSubException.ServiceDownException;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse;
import org.apache.hedwig.protocol.PubSubProtocol.ResponseBody;
import org.apache.hedwig.util.Callback;
import static org.apache.hedwig.util.VarArgs.va;

public class UnsubscribeResponseHandler extends AbstractResponseHandler {

    private static Logger logger = LoggerFactory.getLogger(UnsubscribeResponseHandler.class);

    public UnsubscribeResponseHandler(ClientConfiguration cfg,
                                      HChannelManager channelManager) {
        super(cfg, channelManager);
    }

    @Override
    public void handleResponse(final PubSubResponse response, final PubSubData pubSubData,
                               final Channel channel)
            throws Exception {
        switch (response.getStatusCode()) {
        case SUCCESS:
            // since for unsubscribe request, we close subscription first
            // for now, we don't need to do anything now.
            pubSubData.getCallback().operationFinished(pubSubData.context, null);
            break;
        case CLIENT_NOT_SUBSCRIBED:
            // For Unsubscribe requests, the server says that the client was
            // never subscribed to the topic.
            pubSubData.getCallback().operationFailed(pubSubData.context, new ClientNotSubscribedException(
                                                    "Client was never subscribed to topic: " +
                                                        pubSubData.topic.toStringUtf8() + ", subscriberId: " +
                                                        pubSubData.subscriberId.toStringUtf8()));
            break;
        case SERVICE_DOWN:
            // Response was service down failure so just invoke the callback's
            // operationFailed method.
            pubSubData.getCallback().operationFailed(pubSubData.context, new ServiceDownException(
                                                    "Server responded with a SERVICE_DOWN status"));
            break;
        case NOT_RESPONSIBLE_FOR_TOPIC:
            // Redirect response so we'll need to repost the original
            // Unsubscribe Request
            handleRedirectResponse(response, pubSubData, channel);
            break;
        default:
            // Consider all other status codes as errors, operation failed
            // cases.
            logger.error("Unexpected error response from server for PubSubResponse: " + response);
            pubSubData.getCallback().operationFailed(pubSubData.context, new ServiceDownException(
                                                    "Server responded with a status code of: " +
                                                        response.getStatusCode()));
            break;
        }
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/CleanupChannelMap.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty;

import java.util.Collection;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.locks.ReentrantReadWriteLock;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class CleanupChannelMap<T> {

    private static Logger logger = LoggerFactory.getLogger(CleanupChannelMap.class);
    
    private final ConcurrentHashMap<T, HChannel> channels;

    // Boolean indicating if the channel map is closed or not.
    protected boolean closed = false;
    protected final ReentrantReadWriteLock closedLock =
        new ReentrantReadWriteLock();

    public CleanupChannelMap() {
        channels = new ConcurrentHashMap<T, HChannel>();
    }

    /**
     * Add channel to the map. If an old channel has been bound
     * to <code>key</code>, the <code>channel</code> would be
     * closed immediately and the old channel is returned. Otherwise,
     * the <code>channel</code> is put in the map for future usage.
     *
     * If the channel map has been closed, the channel would be closed
     * immediately.
     *
     * @param key
     *            Key
     * @param channel
     *            Channel
     * @return the channel instance to use.
     */
    public HChannel addChannel(T key, HChannel channel) {
        this.closedLock.readLock().lock();
        try {
            if (closed) {
                channel.close();
                return channel;
            }
            HChannel oldChannel = channels.putIfAbsent(key, channel);
            if (null != oldChannel) {
                logger.info("Channel for {} already exists, so no need to store it.", key);
                channel.close();
                return oldChannel;
            } else {
                logger.debug("Storing a new channel for {}.", key);
                return channel;
            }
        } finally {
            this.closedLock.readLock().unlock();
        }
    }

    /**
     * Replace channel only if currently mapped to the given <code>oldChannel</code>.
     *
     * @param key
     *            Key
     * @param oldChannel
     *            Old Channel
     * @param newChannel
     *            New Channel
     * @return true if replaced successfully, otherwise false.
     */
    public boolean replaceChannel(T key, HChannel oldChannel, HChannel newChannel) {
        this.closedLock.readLock().lock();
        try {
            if (closed) {
                if (null != oldChannel) oldChannel.close();
                if (null != newChannel) newChannel.close();
                return false;
            }
            if (null == oldChannel) {
                HChannel existedChannel = channels.putIfAbsent(key, newChannel);
                if (null != existedChannel) {
                    logger.info("Channel for {} already exists, so no need to replace it.", key);
                    newChannel.close();
                    return false;
                } else {
                    logger.debug("Storing a new channel for {}.", key);
                    return true;
                }
            } else {
                if (channels.replace(key, oldChannel, newChannel)) {
                    logger.debug("Replacd channel {} for {}.", oldChannel, key);
                    oldChannel.close();
                    return true;
                } else {
                    newChannel.close();
                    return false;
                }
            }
        } finally {
            this.closedLock.readLock().unlock();
        }
    }

    /**
     * Returns the channel bound with <code>key</code>.
     *
     * @param key Key
     * @return the channel bound with <code>key</code>.
     */
    public HChannel getChannel(T key) {
        return channels.get(key);
    }

    /**
     * Remove the channel bound with <code>key</code>.
     *
     * @param key Key
     * @return the channel bound with <code>key</code>, null if no channel
     *         is bound with <code>key</code>.
     */
    public HChannel removeChannel(T key) {
        return channels.remove(key);
    }

    /**
     * Remove the channel bound with <code>key</code>.
     *
     * @param key Key
     * @param channel The channel expected to be bound with <code>key</code>.
     * @return true if the channel is removed, false otherwise.
     */
    public boolean removeChannel(T key, HChannel channel) {
        return channels.remove(key, channel);
    }

    /**
     * Return the channels in the map.
     *
     * @return the set of channels.
     */
    public Collection<HChannel> getChannels() {
        return channels.values();
    }

    /**
     * Close the channels map.
     */
    public void close() {
        closedLock.writeLock().lock();
        try {
            if (closed) {
                return;
            }
            closed = true;
        } finally {
            closedLock.writeLock().unlock();
        }
        logger.debug("Closing channels map.");
        for (HChannel channel : channels.values()) {
            channel.close(true);
        }
        channels.clear();
        logger.debug("Closed channels map.");
    }
}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/FilterableMessageHandler.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty;

import com.google.protobuf.ByteString;

import org.apache.hedwig.client.api.MessageHandler;
import org.apache.hedwig.filter.ClientMessageFilter;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.util.Callback;

/**
 * Handlers used by a subscription.
 */
public class FilterableMessageHandler implements MessageHandler {

    MessageHandler msgHandler;
    ClientMessageFilter  msgFilter;

    public FilterableMessageHandler(MessageHandler msgHandler,
                                    ClientMessageFilter msgFilter) {
        this.msgHandler = msgHandler;
        this.msgFilter = msgFilter;
    }

    public boolean hasMessageHandler() {
        return null != msgHandler;
    }

    public MessageHandler getMessageHandler() {
        return msgHandler;
    }

    public boolean hasMessageFilter() {
        return null != msgFilter;
    }

    public ClientMessageFilter getMessageFilter() {
        return msgFilter;
    }

    @Override
    public void deliver(ByteString topic, ByteString subscriberId, Message msg,
                        Callback<Void> callback, Object context) {
        boolean deliver = true;
        if (hasMessageFilter()) {
            deliver = msgFilter.testMessage(msg);
        }
        if (deliver) {
            msgHandler.deliver(topic, subscriberId, msg, callback, context);
        } else {
            callback.operationFinished(context, null);
        }
    }
}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/HChannel.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty;

import org.jboss.netty.channel.Channel;
import org.apache.hedwig.client.data.PubSubData;

/**
 * A wrapper interface over netty {@link Channel} to submit hedwig's
 * {@link PubSubData} requests.
 */
public interface HChannel {

    /**
     * Submit a pub/sub request.
     *
     * @param op
     *          Pub/Sub Request.
     */
    public void submitOp(PubSubData op);

    /**
     * @return underlying netty channel
     */
    public Channel getChannel();

    /**
     * Close the channel without waiting.
     */
    public void close();

    /**
     * Close the channel
     *
     * @param wait
     *          Whether wait until the channel is closed.
     */
    public void close(boolean wait);
}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/HChannelManager.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty;

import java.net.InetSocketAddress;
import java.util.TimerTask;

import org.apache.hedwig.client.api.MessageHandler;
import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.client.data.TopicSubscriber;
import org.apache.hedwig.client.exceptions.AlreadyStartDeliveryException;
import org.apache.hedwig.client.handlers.SubscribeResponseHandler;
import org.apache.hedwig.exceptions.PubSubException.ClientNotSubscribedException;
import org.apache.hedwig.protocol.PubSubProtocol.ResponseBody;
import org.apache.hedwig.util.Callback;

/**
 * A manager manages 1) all channels established to hub servers,
 * 2) the actions taken by the topic subscribers.
 */
public interface HChannelManager {

    /**
     * Submit a pub/sub request after a given <code>delay</code>.
     *
     * @param op
     *          Pub/Sub Request.
     * @param delay
     *          Delay time in ms.
     */
    public void submitOpAfterDelay(PubSubData op, long delay);

    /**
     * Submit a pub/sub request.
     *
     * @param pubSubData
     *          Pub/Sub Request.
     */
    public void submitOp(PubSubData pubSubData);

    /**
     * Submit a pub/sub request to default server.
     *
     * @param pubSubData
     *           Pub/Sub request.
     */
    public void submitOpToDefaultServer(PubSubData pubSubData);

    /**
     * Submit a pub/sub request to a given host.
     *
     * @param pubSubData
     *          Pub/Sub request.
     * @param host
     *          Given host address.
     */
    public void redirectToHost(PubSubData pubSubData, InetSocketAddress host);

    /**
     * Generate next transaction id for pub/sub request sending thru this manager.
     *
     * @return next transaction id.
     */
    public long nextTxnId();

    /**
     * Schedule a timer task after a given <code>delay</code>.
     *
     * @param task
     *          A timer task
     * @param delay
     *          Delay time in ms.
     */
    public void schedule(TimerTask task, long delay);

    /**
     * Get the subscribe response handler managed the given <code>topicSubscriber</code>.
     *
     * @param topicSubscriber
     *          Topic Subscriber
     * @return subscribe response handler managed it, otherwise return null.
     */
    public SubscribeResponseHandler getSubscribeResponseHandler(
                                    TopicSubscriber topicSubscriber);

    /**
     * Start delivering messages for a given topic subscriber.
     *
     * @param topicSubscriber
     *            Topic Subscriber
     * @param messageHandler
     *            MessageHandler to register for this ResponseHandler instance.
     * @throws ClientNotSubscribedException
     *            If the client is not currently subscribed to the topic
     * @throws AlreadyStartDeliveryException
     *            If someone started delivery a message handler before stopping existed one.
     */
    public void startDelivery(TopicSubscriber topicSubscriber,
                              MessageHandler messageHandler)
    throws ClientNotSubscribedException, AlreadyStartDeliveryException;

    /**
     * Stop delivering messages for a given topic subscriber.
     *
     * @param topicSubscriber
     *            Topic Subscriber
     * @throws ClientNotSubscribedException
     *             If the client is not currently subscribed to the topic
     */
    public void stopDelivery(TopicSubscriber topicSubscriber)
    throws ClientNotSubscribedException;

    /**
     * Close the subscription of the given <code>topicSubscriber</code>.
     *
     * @param topicSubscriber
     *          Topic Subscriber
     * @param callback
     *          Callback
     * @param context
     *          Callback context
     */
    public void asyncCloseSubscription(TopicSubscriber topicSubscriber,
                                       Callback<ResponseBody> callback,
                                       Object context);

    /**
     * Return the subscription event emitter to emit subscription events.
     *
     * @return subscription event emitter.
     */
    public SubscriptionEventEmitter getSubscriptionEventEmitter();

    /**
     * Is the channel manager closed.
     *
     * @return true if the channel manager is closed, otherwise return false.
     */
    public boolean isClosed();

    /**
     * Close the channel manager.
     */
    public void close();
}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/HedwigClientImpl.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty;

import java.util.concurrent.Executors;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.ChannelFactory;
import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory;

import com.google.protobuf.ByteString;

import org.apache.hedwig.client.api.Client;
import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.netty.impl.simple.SimpleHChannelManager;
import org.apache.hedwig.client.netty.impl.multiplex.MultiplexHChannelManager;

/**
 * This is a top level Hedwig Client class that encapsulates the common
 * functionality needed for both Publish and Subscribe operations.
 *
 */
public class HedwigClientImpl implements Client {

    private static final Logger logger = LoggerFactory.getLogger(HedwigClientImpl.class);

    // The Netty socket factory for making connections to the server.
    protected final ChannelFactory socketFactory;
    // Whether the socket factory is one we created or is owned by whoever
    // instantiated us.
    protected boolean ownChannelFactory = false;

    // channel manager manages all the channels established by the client
    protected final HChannelManager channelManager;

    private HedwigSubscriber sub;
    private final HedwigPublisher pub;
    private final ClientConfiguration cfg;

    public static Client create(ClientConfiguration cfg) {
        return new HedwigClientImpl(cfg);
    }

    public static Client create(ClientConfiguration cfg, ChannelFactory socketFactory) {
        return new HedwigClientImpl(cfg, socketFactory);
    }

    // Base constructor that takes in a Configuration object.
    // This will create its own client socket channel factory.
    protected HedwigClientImpl(ClientConfiguration cfg) {
        this(cfg, new NioClientSocketChannelFactory(
                  Executors.newCachedThreadPool(), Executors.newCachedThreadPool()));
        ownChannelFactory = true;
    }

    // Constructor that takes in a Configuration object and a ChannelFactory
    // that has already been instantiated by the caller.
    protected HedwigClientImpl(ClientConfiguration cfg, ChannelFactory socketFactory) {
        this.cfg = cfg;
        this.socketFactory = socketFactory;
        if (cfg.isSubscriptionChannelSharingEnabled()) {
            channelManager = new MultiplexHChannelManager(cfg, socketFactory);
        } else {
            channelManager = new SimpleHChannelManager(cfg, socketFactory);
        }
        pub = new HedwigPublisher(this);
        sub = new HedwigSubscriber(this);
    }

    public ClientConfiguration getConfiguration() {
        return cfg;
    }

    public HChannelManager getHChannelManager() {
        return channelManager;
    }

    public HedwigSubscriber getSubscriber() {
        return sub;
    }

    // Protected method to set the subscriber. This is needed currently for hub
    // versions of the client subscriber.
    protected void setSubscriber(HedwigSubscriber sub) {
        this.sub = sub;
    }

    public HedwigPublisher getPublisher() {
        return pub;
    }

    // When we are done with the client, this is a clean way to gracefully close
    // all channels/sockets created by the client and to also release all
    // resources used by netty.
    public void close() {
        logger.info("Stopping the client!");

        // close channel manager to release all channels
        channelManager.close(); 

        // Release resources used by the ChannelFactory on the client if we are
        // the owner that created it.
        if (ownChannelFactory) {
            socketFactory.releaseExternalResources();
        }
        logger.info("Completed stopping the client!");
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/HedwigPublisher.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.protobuf.ByteString;

import org.apache.hedwig.client.api.Publisher;
import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.client.handlers.PubSubCallback;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.CouldNotConnectException;
import org.apache.hedwig.exceptions.PubSubException.ServiceDownException;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.OperationType;
import org.apache.hedwig.protocol.PubSubProtocol.PublishResponse;
import org.apache.hedwig.protocol.PubSubProtocol.ResponseBody;
import org.apache.hedwig.util.Callback;

/**
 * This is the Hedwig Netty specific implementation of the Publisher interface.
 *
 */
public class HedwigPublisher implements Publisher {

    private static Logger logger = LoggerFactory.getLogger(HedwigPublisher.class);

    private final HChannelManager channelManager;

    protected HedwigPublisher(HedwigClientImpl client) {
        this.channelManager = client.getHChannelManager();
    }

    public PublishResponse publish(ByteString topic, Message msg)
        throws CouldNotConnectException, ServiceDownException {

        if (logger.isDebugEnabled()) {
            logger.debug("Calling a sync publish for topic: {}, msg: {}.",
                         topic.toStringUtf8(), msg);
        }
        PubSubData pubSubData = new PubSubData(topic, msg, null, OperationType.PUBLISH, null, null, null);
        synchronized (pubSubData) {
            PubSubCallback pubSubCallback = new PubSubCallback(pubSubData);
            asyncPublishWithResponseImpl(topic, msg, pubSubCallback, null);
            try {
                while (!pubSubData.isDone)
                    pubSubData.wait();
            } catch (InterruptedException e) {
                throw new ServiceDownException("Interrupted Exception while waiting for async publish call");
            }
            // Check from the PubSubCallback if it was successful or not.
            if (!pubSubCallback.getIsCallSuccessful()) {
                // See what the exception was that was thrown when the operation
                // failed.
                PubSubException failureException = pubSubCallback.getFailureException();
                if (failureException == null) {
                    // This should not happen as the operation failed but a null
                    // PubSubException was passed. Log a warning message but
                    // throw a generic ServiceDownException.
                    logger.error("Sync Publish operation failed but no PubSubException was passed!");
                    throw new ServiceDownException("Server ack response to publish request is not successful");
                }
                // For the expected exceptions that could occur, just rethrow
                // them.
                else if (failureException instanceof CouldNotConnectException) {
                    throw (CouldNotConnectException) failureException;
                } else if (failureException instanceof ServiceDownException) {
                    throw (ServiceDownException) failureException;
                } else {
                    // For other types of PubSubExceptions, just throw a generic
                    // ServiceDownException but log a warning message.
                    logger.error("Unexpected exception type when a sync publish operation failed: ",
                                 failureException);
                    throw new ServiceDownException("Server ack response to publish request is not successful");
                }
            }

            ResponseBody respBody = pubSubCallback.getResponseBody();
            if (null == respBody) {
                return null;
            }
            return respBody.hasPublishResponse() ? respBody.getPublishResponse() : null;
        }
    }

    public void asyncPublish(ByteString topic, Message msg,
                             final Callback<Void> callback, Object context) {
        asyncPublishWithResponseImpl(topic, msg,
                                     new VoidCallbackAdapter<ResponseBody>(callback), context);
    }

    public void asyncPublishWithResponse(ByteString topic, Message msg,
                                         Callback<PublishResponse> callback,
                                         Object context) {
        // adapt the callback.
        asyncPublishWithResponseImpl(topic, msg,
                                     new PublishResponseCallbackAdapter(callback), context);
    }

    private void asyncPublishWithResponseImpl(ByteString topic, Message msg,
                                              Callback<ResponseBody> callback,
                                              Object context) {
        if (logger.isDebugEnabled()) {
            logger.debug("Calling an async publish for topic: {}, msg: {}.",
                         topic.toStringUtf8(), msg);
        }
        PubSubData pubSubData = new PubSubData(topic, msg, null, OperationType.PUBLISH, null,
                                               callback, context);
        channelManager.submitOp(pubSubData);
    }

    private static class PublishResponseCallbackAdapter implements Callback<ResponseBody>{

        private final Callback<PublishResponse> delegate;

        private PublishResponseCallbackAdapter(Callback<PublishResponse> delegate) {
            this.delegate = delegate;
        }

        @Override
        public void operationFinished(Object ctx, ResponseBody resultOfOperation) {
            if (null == resultOfOperation) {
                delegate.operationFinished(ctx, null);
            } else {
                delegate.operationFinished(ctx, resultOfOperation.getPublishResponse());
            }
        }

        @Override
        public void operationFailed(Object ctx, PubSubException exception) {
            delegate.operationFailed(ctx, exception);
        }
    }
}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/HedwigSubscriber.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty;

import java.util.List;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.protobuf.ByteString;

import org.apache.hedwig.client.api.MessageHandler;
import org.apache.hedwig.client.api.Subscriber;
import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.client.data.TopicSubscriber;
import org.apache.hedwig.client.exceptions.AlreadyStartDeliveryException;
import org.apache.hedwig.client.exceptions.InvalidSubscriberIdException;
import org.apache.hedwig.client.handlers.PubSubCallback;
import org.apache.hedwig.client.handlers.SubscribeResponseHandler;
import org.apache.hedwig.filter.ClientMessageFilter;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.ClientAlreadySubscribedException;
import org.apache.hedwig.exceptions.PubSubException.ClientNotSubscribedException;
import org.apache.hedwig.exceptions.PubSubException.CouldNotConnectException;
import org.apache.hedwig.exceptions.PubSubException.ServiceDownException;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.OperationType;
import org.apache.hedwig.protocol.PubSubProtocol.ResponseBody;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions;
import org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach;
import org.apache.hedwig.protoextensions.SubscriptionStateUtils;
import org.apache.hedwig.util.Callback;
import org.apache.hedwig.util.SubscriptionListener;

/**
 * This is the Hedwig Netty specific implementation of the Subscriber interface.
 *
 */
public class HedwigSubscriber implements Subscriber {

    private static Logger logger = LoggerFactory.getLogger(HedwigSubscriber.class);

    protected final ClientConfiguration cfg;
    protected final HChannelManager channelManager;

    public HedwigSubscriber(HedwigClientImpl client) {
        this.cfg = client.getConfiguration();
        this.channelManager = client.getHChannelManager();
    }

    public void addSubscriptionListener(SubscriptionListener listener) {
        channelManager.getSubscriptionEventEmitter()
                      .addSubscriptionListener(listener);
    }

    public void removeSubscriptionListener(SubscriptionListener listener) {
        channelManager.getSubscriptionEventEmitter()
                      .removeSubscriptionListener(listener);
    }

    // Private method that holds the common logic for doing synchronous
    // Subscribe or Unsubscribe requests. This is for code reuse since these
    // two flows are very similar. The assumption is that the input
    // OperationType is either SUBSCRIBE or UNSUBSCRIBE.
    private void subUnsub(ByteString topic, ByteString subscriberId, OperationType operationType,
                          SubscriptionOptions options)
            throws CouldNotConnectException, ClientAlreadySubscribedException,
        ClientNotSubscribedException, ServiceDownException {
        if (logger.isDebugEnabled()) {
            StringBuilder debugMsg = new StringBuilder().append("Calling a sync subUnsub request for topic: ")
                                     .append(topic.toStringUtf8()).append(", subscriberId: ")
                                     .append(subscriberId.toStringUtf8()).append(", operationType: ")
                                     .append(operationType);
            if (null != options) {
                debugMsg.append(", createOrAttach: ").append(options.getCreateOrAttach())
                        .append(", messageBound: ").append(options.getMessageBound());
            }
            logger.debug(debugMsg.toString());
        }
        PubSubData pubSubData = new PubSubData(topic, null, subscriberId, operationType, options, null, null);
        synchronized (pubSubData) {
            PubSubCallback pubSubCallback = new PubSubCallback(pubSubData);
            asyncSubUnsub(topic, subscriberId, pubSubCallback, null, operationType, options);
            try {
                while (!pubSubData.isDone)
                    pubSubData.wait();
            } catch (InterruptedException e) {
                throw new ServiceDownException("Interrupted Exception while waiting for async subUnsub call");
            }
            // Check from the PubSubCallback if it was successful or not.
            if (!pubSubCallback.getIsCallSuccessful()) {
                // See what the exception was that was thrown when the operation
                // failed.
                PubSubException failureException = pubSubCallback.getFailureException();
                if (failureException == null) {
                    // This should not happen as the operation failed but a null
                    // PubSubException was passed. Log a warning message but
                    // throw a generic ServiceDownException.
                    logger.error("Sync SubUnsub operation failed but no PubSubException was passed!");
                    throw new ServiceDownException("Server ack response to SubUnsub request is not successful");
                }
                // For the expected exceptions that could occur, just rethrow
                // them.
                else if (failureException instanceof CouldNotConnectException)
                    throw (CouldNotConnectException) failureException;
                else if (failureException instanceof ClientAlreadySubscribedException)
                    throw (ClientAlreadySubscribedException) failureException;
                else if (failureException instanceof ClientNotSubscribedException)
                    throw (ClientNotSubscribedException) failureException;
                else if (failureException instanceof ServiceDownException)
                    throw (ServiceDownException) failureException;
                else {
                    logger.error("Unexpected PubSubException thrown: ", failureException);
                    // Throw a generic ServiceDownException but wrap the
                    // original PubSubException within it.
                    throw new ServiceDownException(failureException);
                }
            }
        }
    }

    // Private method that holds the common logic for doing asynchronous
    // Subscribe or Unsubscribe requests. This is for code reuse since these two
    // flows are very similar. The assumption is that the input OperationType is
    // either SUBSCRIBE or UNSUBSCRIBE.
    private void asyncSubUnsub(ByteString topic, ByteString subscriberId,
                               Callback<ResponseBody> callback, Object context,
                               OperationType operationType, SubscriptionOptions options) {
        if (logger.isDebugEnabled()) {
            StringBuilder debugMsg = new StringBuilder().append("Calling a async subUnsub request for topic: ")
                                     .append(topic.toStringUtf8()).append(", subscriberId: ")
                                     .append(subscriberId.toStringUtf8()).append(", operationType: ")
                                     .append(operationType);
            if (null != options) {
                debugMsg.append(", createOrAttach: ").append(options.getCreateOrAttach())
                        .append(", messageBound: ").append(options.getMessageBound());
            }
            logger.debug(debugMsg.toString());
        }
        if (OperationType.SUBSCRIBE.equals(operationType)) {
            if (options.getMessageBound() <= 0 &&
                cfg.getSubscriptionMessageBound() > 0) {
                SubscriptionOptions.Builder soBuilder =
                    SubscriptionOptions.newBuilder(options).setMessageBound(
                        cfg.getSubscriptionMessageBound());
                options = soBuilder.build();
            }
        }
        PubSubData pubSubData = new PubSubData(topic, null, subscriberId, operationType,
                                               options, callback, context);
        channelManager.submitOp(pubSubData);
    }

    public void subscribe(ByteString topic, ByteString subscriberId, CreateOrAttach mode)
            throws CouldNotConnectException, ClientAlreadySubscribedException, ServiceDownException,
        InvalidSubscriberIdException {
        SubscriptionOptions options = SubscriptionOptions.newBuilder().setCreateOrAttach(mode).build();
        subscribe(topic, subscriberId, options, false);
    }

    public void subscribe(ByteString topic, ByteString subscriberId, SubscriptionOptions options)
            throws CouldNotConnectException, ClientAlreadySubscribedException, ServiceDownException,
         InvalidSubscriberIdException {
        subscribe(topic, subscriberId, options, false);
    }

    protected void subscribe(ByteString topic, ByteString subscriberId, SubscriptionOptions options, boolean isHub)
            throws CouldNotConnectException, ClientAlreadySubscribedException, ServiceDownException,
        InvalidSubscriberIdException {
        // Validate that the format of the subscriberId is valid either as a
        // local or hub subscriber.
        if (!isValidSubscriberId(subscriberId, isHub)) {
            throw new InvalidSubscriberIdException("SubscriberId passed is not valid: " + subscriberId.toStringUtf8()
                                                   + ", isHub: " + isHub);
        }
        try {
            subUnsub(topic, subscriberId, OperationType.SUBSCRIBE, options);
        } catch (ClientNotSubscribedException e) {
            logger.error("Unexpected Exception thrown: ", e);
            // This exception should never be thrown here. But just in case,
            // throw a generic ServiceDownException but wrap the original
            // Exception within it.
            throw new ServiceDownException(e);
        }
    }

    public void asyncSubscribe(ByteString topic, ByteString subscriberId, CreateOrAttach mode, Callback<Void> callback,
                               Object context) {
        SubscriptionOptions options = SubscriptionOptions.newBuilder().setCreateOrAttach(mode).build();
        asyncSubscribe(topic, subscriberId, options, callback, context, false);
    }

    public void asyncSubscribe(ByteString topic, ByteString subscriberId, SubscriptionOptions options,
                               Callback<Void> callback, Object context) {
        asyncSubscribe(topic, subscriberId, options, callback, context, false);
    }

    protected void asyncSubscribe(ByteString topic, ByteString subscriberId,
                                  SubscriptionOptions options,
                                  Callback<Void> callback, Object context, boolean isHub) {
        // Validate that the format of the subscriberId is valid either as a
        // local or hub subscriber.
        if (!isValidSubscriberId(subscriberId, isHub)) {
            callback.operationFailed(context, new ServiceDownException(new InvalidSubscriberIdException(
                                         "SubscriberId passed is not valid: " + subscriberId.toStringUtf8() + ", isHub: " + isHub)));
            return;
        }
        asyncSubUnsub(topic, subscriberId,
                      new VoidCallbackAdapter<ResponseBody>(callback), context,
                      OperationType.SUBSCRIBE, options);
    }

    public void unsubscribe(ByteString topic, ByteString subscriberId) throws CouldNotConnectException,
        ClientNotSubscribedException, ServiceDownException, InvalidSubscriberIdException {
        unsubscribe(topic, subscriberId, false);
    }

    protected void unsubscribe(ByteString topic, ByteString subscriberId, boolean isHub)
            throws CouldNotConnectException, ClientNotSubscribedException, ServiceDownException,
        InvalidSubscriberIdException {
        // Validate that the format of the subscriberId is valid either as a
        // local or hub subscriber.
        if (!isValidSubscriberId(subscriberId, isHub)) {
            throw new InvalidSubscriberIdException("SubscriberId passed is not valid: " + subscriberId.toStringUtf8()
                                                   + ", isHub: " + isHub);
        }
        // Synchronously close the subscription on the client side. Even
        // if the unsubscribe request to the server errors out, we won't be
        // delivering messages for this subscription to the client. The client
        // can later retry the unsubscribe request to the server so they are
        // "fully" unsubscribed from the given topic.
        closeSubscription(topic, subscriberId);
        try {
            subUnsub(topic, subscriberId, OperationType.UNSUBSCRIBE, null);
        } catch (ClientAlreadySubscribedException e) {
            logger.error("Unexpected Exception thrown: ", e);
            // This exception should never be thrown here. But just in case,
            // throw a generic ServiceDownException but wrap the original
            // Exception within it.
            throw new ServiceDownException(e);
        }
    }

    public void asyncUnsubscribe(final ByteString topic, final ByteString subscriberId,
                                 final Callback<Void> callback, final Object context) {
        doAsyncUnsubscribe(topic, subscriberId,
                           new VoidCallbackAdapter<ResponseBody>(callback),
                           context, false);
    }

    protected void asyncUnsubscribe(final ByteString topic, final ByteString subscriberId,
                                    final Callback<Void> callback, final Object context, boolean isHub) {
        doAsyncUnsubscribe(topic, subscriberId,
                           new VoidCallbackAdapter<ResponseBody>(callback),
                           context, isHub);
    }

    private void doAsyncUnsubscribe(final ByteString topic, final ByteString subscriberId,
                                    final Callback<ResponseBody> callback,
                                    final Object context, boolean isHub) {
        // Validate that the format of the subscriberId is valid either as a
        // local or hub subscriber.
        if (!isValidSubscriberId(subscriberId, isHub)) {
            callback.operationFailed(context, new ServiceDownException(new InvalidSubscriberIdException(
                                         "SubscriberId passed is not valid: " + subscriberId.toStringUtf8() + ", isHub: " + isHub)));
            return;
        }
        // Asynchronously close the subscription. On the callback to that
        // operation once it completes, post the async unsubscribe request.
        doAsyncCloseSubscription(topic, subscriberId, new Callback<ResponseBody>() {
            @Override
            public void operationFinished(Object ctx, ResponseBody resultOfOperation) {
                asyncSubUnsub(topic, subscriberId, callback, context, OperationType.UNSUBSCRIBE, null);
            }

            @Override
            public void operationFailed(Object ctx, PubSubException exception) {
                callback.operationFailed(context, exception);
            }
        }, null);
    }

    // This is a helper method to determine if a subscriberId is valid as either
    // a hub or local subscriber
    private boolean isValidSubscriberId(ByteString subscriberId, boolean isHub) {
        if ((isHub && !SubscriptionStateUtils.isHubSubscriber(subscriberId))
                || (!isHub && SubscriptionStateUtils.isHubSubscriber(subscriberId)))
            return false;
        else
            return true;
    }

    public void consume(ByteString topic, ByteString subscriberId, MessageSeqId messageSeqId)
            throws ClientNotSubscribedException {
        TopicSubscriber topicSubscriber = new TopicSubscriber(topic, subscriberId);
        logger.debug("Calling consume for {}, messageSeqId: {}.",
                     topicSubscriber, messageSeqId);

        SubscribeResponseHandler subscribeResponseHandler =
            channelManager.getSubscribeResponseHandler(topicSubscriber);
        // Check that this topic subscription on the client side exists.
        if (null == subscribeResponseHandler ||
            !subscribeResponseHandler.hasSubscription(topicSubscriber)) {
            throw new ClientNotSubscribedException(
                "Cannot send consume message since client is not subscribed to topic: "
                + topic.toStringUtf8() + ", subscriberId: " + subscriberId.toStringUtf8());
        }
        // Send the consume message to the server using the same subscribe
        // channel that the topic subscription uses.
        subscribeResponseHandler.consume(topicSubscriber, messageSeqId);
    }

    public boolean hasSubscription(ByteString topic, ByteString subscriberId) throws CouldNotConnectException,
        ServiceDownException {
        // The subscription type of info should be stored on the server end, not
        // the client side. Eventually, the server will have the Subscription
        // Manager part that ties into Zookeeper to manage this info.
        // Commenting out these type of API's related to that here for now until
        // this data is available on the server. Will figure out what the
        // correct way to contact the server to get this info is then.
        // The client side just has soft memory state for client subscription
        // information.
        TopicSubscriber topicSubscriber = new TopicSubscriber(topic, subscriberId);
        SubscribeResponseHandler subscribeResponseHandler =
            channelManager.getSubscribeResponseHandler(topicSubscriber);
        return !(null == subscribeResponseHandler ||
                 !subscribeResponseHandler.hasSubscription(topicSubscriber));
    }

    public List<ByteString> getSubscriptionList(ByteString subscriberId) throws CouldNotConnectException,
        ServiceDownException {
        // Same as the previous hasSubscription method, this data should reside
        // on the server end, not the client side.
        return null;
    }

    public void startDelivery(final ByteString topic, final ByteString subscriberId,
                              MessageHandler messageHandler)
            throws ClientNotSubscribedException, AlreadyStartDeliveryException {
        TopicSubscriber topicSubscriber = new TopicSubscriber(topic, subscriberId);
        logger.debug("Starting delivery for {}.", topicSubscriber);
        channelManager.startDelivery(topicSubscriber, messageHandler); 
    }

    public void startDeliveryWithFilter(final ByteString topic, final ByteString subscriberId,
                                        MessageHandler messageHandler,
                                        ClientMessageFilter messageFilter)
            throws ClientNotSubscribedException, AlreadyStartDeliveryException {
        if (null == messageHandler || null == messageFilter) {
            throw new NullPointerException("Null message handler or message filter is       provided.");
        }
        TopicSubscriber topicSubscriber = new TopicSubscriber(topic, subscriberId);
        messageHandler = new FilterableMessageHandler(messageHandler, messageFilter);
        logger.debug("Starting delivery with filter for {}.", topicSubscriber);
        channelManager.startDelivery(topicSubscriber, messageHandler);
    }

    public void stopDelivery(final ByteString topic, final ByteString subscriberId)
    throws ClientNotSubscribedException {
        TopicSubscriber topicSubscriber = new TopicSubscriber(topic, subscriberId);
        logger.debug("Stopping delivery for {}.", topicSubscriber);
        channelManager.stopDelivery(topicSubscriber); 
    }

    public void closeSubscription(ByteString topic, ByteString subscriberId) throws ServiceDownException {
        PubSubData pubSubData = new PubSubData(topic, null, subscriberId, null, null, null, null);
        synchronized (pubSubData) {
            PubSubCallback pubSubCallback = new PubSubCallback(pubSubData);
            doAsyncCloseSubscription(topic, subscriberId, pubSubCallback, null);
            try {
                while (!pubSubData.isDone)
                    pubSubData.wait();
            } catch (InterruptedException e) {
                throw new ServiceDownException("Interrupted Exception while waiting for asyncCloseSubscription call");
            }
            // Check from the PubSubCallback if it was successful or not.
            if (!pubSubCallback.getIsCallSuccessful()) {
                throw new ServiceDownException("Exception while trying to close the subscription for topic: "
                                               + topic.toStringUtf8() + ", subscriberId: " + subscriberId.toStringUtf8());
            }
        }
    }

    public void asyncCloseSubscription(final ByteString topic, final ByteString subscriberId,
                                       final Callback<Void> callback, final Object context) {
        doAsyncCloseSubscription(topic, subscriberId,
                                 new VoidCallbackAdapter<ResponseBody>(callback), context);
    }

    private void doAsyncCloseSubscription(final ByteString topic, final ByteString subscriberId,
                                          final Callback<ResponseBody> callback, final Object context) {
        TopicSubscriber topicSubscriber = new TopicSubscriber(topic, subscriberId);
        logger.debug("Stopping delivery for {} before closing subscription.", topicSubscriber);
        // We only stop delivery here not in channel manager
        // Because channelManager#asyncCloseSubscription will called
        // when subscription channel disconnected to clear local subscription
        try {
            channelManager.stopDelivery(topicSubscriber); 
        } catch (ClientNotSubscribedException cnse) {
            // it is OK to ignore the exception when closing subscription
        }
        logger.debug("Closing subscription asynchronously for {}.", topicSubscriber);
        channelManager.asyncCloseSubscription(topicSubscriber, callback, context);
    }
}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/NetUtils.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty;

import java.net.InetSocketAddress;

import org.jboss.netty.channel.Channel;

import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.client.data.TopicSubscriber;
import org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest;
import org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.OperationType;
import org.apache.hedwig.protocol.PubSubProtocol.PublishRequest;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;
import org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion;
import org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences;
import org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest;

/**
 * Utilities for network operations.
 */
public class NetUtils {

    /**
     * Helper static method to get the String Hostname:Port from a netty
     * Channel. Assumption is that the netty Channel was originally created with
     * an InetSocketAddress. This is true with the Hedwig netty implementation.
     *
     * @param channel
     *            Netty channel to extract the hostname and port from.
     * @return String representation of the Hostname:Port from the Netty Channel
     */
    public static InetSocketAddress getHostFromChannel(Channel channel) {
        return (InetSocketAddress) channel.getRemoteAddress();
    }

    /**
     * This is a helper method to build the actual pub/sub message.
     *
     * @param txnId
     *            Transaction Id.
     * @param pubSubData
     *            Publish call's data wrapper object.
     * @return pub sub request to send
     */
    public static PubSubRequest.Builder buildPubSubRequest(long txnId,
                                                           PubSubData pubSubData) {
        // Create a PubSubRequest
        PubSubRequest.Builder pubsubRequestBuilder = PubSubRequest.newBuilder();
        pubsubRequestBuilder.setProtocolVersion(ProtocolVersion.VERSION_ONE);
        pubsubRequestBuilder.setType(pubSubData.operationType);
        // for consume request, we don't need to care about tried servers list
        if (OperationType.CONSUME != pubSubData.operationType) {
            if (pubSubData.triedServers != null && pubSubData.triedServers.size() > 0) {
                pubsubRequestBuilder.addAllTriedServers(pubSubData.triedServers);
            }
        }
        pubsubRequestBuilder.setTxnId(txnId);
        pubsubRequestBuilder.setShouldClaim(pubSubData.shouldClaim);
        pubsubRequestBuilder.setTopic(pubSubData.topic);

        switch (pubSubData.operationType) {
        case PUBLISH:
            // Set the PublishRequest into the outer PubSubRequest
            pubsubRequestBuilder.setPublishRequest(buildPublishRequest(pubSubData));
            break;
        case SUBSCRIBE:
            // Set the SubscribeRequest into the outer PubSubRequest
            pubsubRequestBuilder.setSubscribeRequest(buildSubscribeRequest(pubSubData));
            break;
        case UNSUBSCRIBE:
            // Set the UnsubscribeRequest into the outer PubSubRequest
            pubsubRequestBuilder.setUnsubscribeRequest(buildUnsubscribeRequest(pubSubData));
            break;
        case CLOSESUBSCRIPTION:
            // Set the CloseSubscriptionRequest into the outer PubSubRequest
            pubsubRequestBuilder.setCloseSubscriptionRequest(
                buildCloseSubscriptionRequest(pubSubData));
            break;
        }

        // Update the PubSubData with the txnId and the requestWriteTime
        pubSubData.txnId = txnId;
        pubSubData.requestWriteTime = System.currentTimeMillis();

        return pubsubRequestBuilder;
    }

    // build publish request
    private static PublishRequest.Builder buildPublishRequest(PubSubData pubSubData) {
        PublishRequest.Builder publishRequestBuilder = PublishRequest.newBuilder();
        publishRequestBuilder.setMsg(pubSubData.msg);
        return publishRequestBuilder;
    }

    // build subscribe request
    private static SubscribeRequest.Builder buildSubscribeRequest(PubSubData pubSubData) { SubscribeRequest.Builder subscribeRequestBuilder = SubscribeRequest.newBuilder();
        subscribeRequestBuilder.setSubscriberId(pubSubData.subscriberId);
        subscribeRequestBuilder.setCreateOrAttach(pubSubData.options.getCreateOrAttach());
        subscribeRequestBuilder.setForceAttach(pubSubData.options.getForceAttach());
        // For now, all subscribes should wait for all cross-regional
        // subscriptions to be established before returning.
        subscribeRequestBuilder.setSynchronous(true);
        // set subscription preferences
        SubscriptionPreferences.Builder preferencesBuilder =
            options2Preferences(pubSubData.options);
        // backward compatable with 4.1.0
        if (preferencesBuilder.hasMessageBound()) {
            subscribeRequestBuilder.setMessageBound(preferencesBuilder.getMessageBound());
        } 
        subscribeRequestBuilder.setPreferences(preferencesBuilder);
        return subscribeRequestBuilder;
    }

    // build unsubscribe request
    private static UnsubscribeRequest.Builder buildUnsubscribeRequest(PubSubData pubSubData) {
        // Create the UnSubscribeRequest
        UnsubscribeRequest.Builder unsubscribeRequestBuilder = UnsubscribeRequest.newBuilder();
        unsubscribeRequestBuilder.setSubscriberId(pubSubData.subscriberId);
        return unsubscribeRequestBuilder;
    }

    // build closesubscription request
    private static CloseSubscriptionRequest.Builder
        buildCloseSubscriptionRequest(PubSubData pubSubData) {
        // Create the CloseSubscriptionRequest
        CloseSubscriptionRequest.Builder closeSubscriptionRequestBuilder =
            CloseSubscriptionRequest.newBuilder();
        closeSubscriptionRequestBuilder.setSubscriberId(pubSubData.subscriberId);
        return closeSubscriptionRequestBuilder;
    }

    /**
     * Build consume request
     *
     * @param txnId
     *          Transaction Id.
     * @param topicSubscriber
     *          Topic Subscriber.
     * @param messageSeqId
     *          Message Seq Id.
     * @return pub/sub request.
     */
    public static PubSubRequest.Builder buildConsumeRequest(long txnId,
                                                            TopicSubscriber topicSubscriber,
                                                            MessageSeqId messageSeqId) {
        // Create a PubSubRequest
        PubSubRequest.Builder pubsubRequestBuilder = PubSubRequest.newBuilder();
        pubsubRequestBuilder.setProtocolVersion(ProtocolVersion.VERSION_ONE);
        pubsubRequestBuilder.setType(OperationType.CONSUME);

        pubsubRequestBuilder.setTxnId(txnId);
        pubsubRequestBuilder.setTopic(topicSubscriber.getTopic());

        // Create the ConsumeRequest
        ConsumeRequest.Builder consumeRequestBuilder = ConsumeRequest.newBuilder();
        consumeRequestBuilder.setSubscriberId(topicSubscriber.getSubscriberId());
        consumeRequestBuilder.setMsgId(messageSeqId);

        pubsubRequestBuilder.setConsumeRequest(consumeRequestBuilder);

        return pubsubRequestBuilder;
    }

    /**
     * Convert client-side subscription options to subscription preferences
     *
     * @param options
     *          Client-Side subscription options
     * @return subscription preferences
     */
    private static SubscriptionPreferences.Builder options2Preferences(SubscriptionOptions options) {
        // prepare subscription preferences
        SubscriptionPreferences.Builder preferencesBuilder =
            SubscriptionPreferences.newBuilder();

        // set message bound
        if (options.getMessageBound() > 0) {
            preferencesBuilder.setMessageBound(options.getMessageBound());
        }

        // set message filter
        if (options.hasMessageFilter()) {
            preferencesBuilder.setMessageFilter(options.getMessageFilter());
        }

        // set user options
        if (options.hasOptions()) {
            preferencesBuilder.setOptions(options.getOptions());
        }

        // set message window size if set
        if (options.hasMessageWindowSize() && options.getMessageWindowSize() > 0) {
            preferencesBuilder.setMessageWindowSize(options.getMessageWindowSize());
        }

        return preferencesBuilder;
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/package-info.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

/**
 * A Netty based Hedwig client implementation.
 *
 * <h3>Components</h3>
 *
 * The netty based implementation contains following components:
 * <ul>
 *   <li>{@link HChannel}: A interface wrapper of netty {@link org.jboss.netty.channel.Channel}
 *       to submit hedwig's {@link org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest}s
 *       to target host.</li>
 *   <li>{@link HChanneHandler}: A wrapper of netty {@link org.jboss.netty.channel.ChannelHandler}
 *       to handle events of its underlying netty channel, such as responses received, channel
 *       disconnected, etc. A {@link HChannelHandler} is bound with a {@link HChannel}.</li>
 *   <li>{@link HChannelManager}: A manager manages all established {@link HChannel}s.
 *       It provides a clean interface for publisher/subscriber to send
 *       {@link org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest}s</li>
 * </ul>
 *
 * <h3>Main Flow</h3>
 *
 * <ul>
 *   <li>{@link HedwigPublisher}/{@link HedwigSubscriber} delegates {@link HChannelManager}
 *       to submit pub/sub requests.</li>
 *   <li>{@link HChannelManager} find the owner hubs, establish a {@link HChannel} to hub servers
 *       and send the requests to them.</li>
 *   <li>{@link HChannelHandler} dispatches responses to target
 *       {@link org.apache.hedwig.client.handlers.AbstractResponseHandler} to process.</li>
 *   <li>{@link HChannelHandler} detects an underlying netty {@link org.jboss.netty.channel.Channel}
 *       disconnected. It calles {@link HChannelManager} to clear cached {@link HChannel} that
 *       it bound with. For non-subscritpion channels, it would fail all pending requests;
 *       For subscription channels, it would fail all pending requests and retry to reconnect
 *       those successful subscriptions.</li>
 * </ul>
 *
 * <h3>HChannel</h3>
 *
 * Two kinds of {@link HChannel}s provided in current implementation. {@link HChannelImpl}
 * provides the ability to multiplex pub/sub requests in an underlying netty
 * {@link org.jboss.netty.channel.Channel}, while {@link DefaultServerChannel} provides the
 * ability to establish a netty channel {@link org.jboss.netty.channel.Channel} for a pub/sub
 * request. After the underlying netty channel is estabilished, it would be converted into
 * a {@link HChannelImpl} by {@link HChannelManager#submitOpThruChannel(pubSubData, channel)}.
 *
 * Although {@link HChannelImpl} provides multiplexing ability, it still could be used for
 * one-channel-per-subscription case, which just sent only one subscribe request thru the
 * underlying channel.
 *
 * <h3>HChannelHandler</h3>
 *
 * {@link HChannelHandler} is generic netty {@link org.jboss.netty.channel.ChannelHandler},
 * which handles events from the underlying channel. A <i>HChannelHandler</i> is bound with
 * a {@link HChannel} as channel pipeplien when the underlying channel is established. It
 * takes the responsibility of dispatching response to target response handler. For a
 * non-subscription channel, it just handles <b>PUBLISH</b> and <b>UNSUBSCRIBE</b> responses.
 * For a subscription channel, it handles <b>SUBSCRIBE</b> response. For consume requests,
 * we treated them in a fire-and-forget way, so they are not need to be handled by any response
 * handler.
 *
 * <h3>HChannelManager</h3>
 *
 * {@link HChannelManager} manages all outstanding connections to target hub servers for a client.
 * Since a subscription channel acts quite different from a non-subscription channel, the basic
 * implementation {@link AbstractHChannelManager} manages non-subscription channels and
 * subscription channels in different channel sets. Currently hedwig client provides
 * {@link SimpleHChannelManager} which manages subscription channels in one-channel-per-subscription
 * way. In future, if we want to multiplex multiple subscriptions in one channel, we just need
 * to provide an multiplexing version of {@link AbstractHChannelManager} which manages channels
 * in multiplexing way, and a multiplexing version of {@link org.apache.hedwig.client.handlers.SubscribeResponseHandler}
 * which handles multiple subscriptions in one channel.
 */
package org.apache.hedwig.client.netty;
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/SubscriptionEventEmitter.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty;

import java.util.concurrent.CopyOnWriteArraySet;

import com.google.protobuf.ByteString;

import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEvent;
import org.apache.hedwig.util.SubscriptionListener;

public class SubscriptionEventEmitter {

    private final CopyOnWriteArraySet<SubscriptionListener> listeners;

    public SubscriptionEventEmitter() {
        listeners = new CopyOnWriteArraySet<SubscriptionListener>();
    }

    public void addSubscriptionListener(SubscriptionListener listener) {
        listeners.add(listener); 
    }

    public void removeSubscriptionListener(SubscriptionListener listener) {
        listeners.remove(listener);
    }

    public void emitSubscriptionEvent(ByteString topic, ByteString subscriberId,
                                      SubscriptionEvent event) {
        for (SubscriptionListener listener : listeners) {
            listener.processEvent(topic, subscriberId, event);
        }
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/VoidCallbackAdapter.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty;

import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.util.Callback;

/**
 * Adapts from Callback&lt;T> to Callback&lt;Void>. (Ignores the &lt;T> parameter).
 */
public class VoidCallbackAdapter<T> implements Callback<T> {
    private final Callback<Void> delegate;

    public VoidCallbackAdapter(Callback<Void> delegate){
        this.delegate = delegate;
    }

    @Override
    public void operationFinished(Object ctx, T resultOfOperation) {
        delegate.operationFinished(ctx, null);
    }

    @Override
    public void operationFailed(Object ctx, PubSubException exception) {
        delegate.operationFailed(ctx, exception);
    }
}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/impl/AbstractHChannelManager.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty.impl;

import java.net.InetSocketAddress;
import java.util.HashSet;
import java.util.Set;
import java.util.Timer;
import java.util.TimerTask;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ConcurrentMap;
import java.util.concurrent.atomic.AtomicLong;
import java.util.concurrent.locks.ReentrantReadWriteLock;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;
import org.jboss.netty.channel.ChannelFactory;
import org.jboss.netty.channel.ChannelFuture;
import org.jboss.netty.channel.ChannelFutureListener;

import com.google.protobuf.ByteString;

import org.apache.hedwig.client.api.MessageHandler;
import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.client.data.TopicSubscriber;
import org.apache.hedwig.client.exceptions.AlreadyStartDeliveryException;
import org.apache.hedwig.client.exceptions.NoResponseHandlerException;
import org.apache.hedwig.client.handlers.MessageConsumeCallback;
import org.apache.hedwig.client.handlers.SubscribeResponseHandler;
import org.apache.hedwig.client.netty.CleanupChannelMap;
import org.apache.hedwig.client.netty.HChannel;
import org.apache.hedwig.client.netty.HChannelManager;
import org.apache.hedwig.client.netty.NetUtils;
import org.apache.hedwig.client.netty.SubscriptionEventEmitter;
import org.apache.hedwig.client.ssl.SslClientContextFactory;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.ClientNotSubscribedException;
import org.apache.hedwig.exceptions.PubSubException.ServiceDownException;
import org.apache.hedwig.filter.ClientMessageFilter;
import org.apache.hedwig.protocol.PubSubProtocol;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.MessageHeader;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.OperationType;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;
import org.apache.hedwig.protocol.PubSubProtocol.ResponseBody;
import org.apache.hedwig.util.Callback;
import static org.apache.hedwig.util.VarArgs.va;

/**
 * Basic HChannel Manager Implementation
 */
public abstract class AbstractHChannelManager implements HChannelManager {

    private static Logger logger = LoggerFactory.getLogger(AbstractHChannelManager.class);

    // Empty Topic List
    private final static Set<ByteString> EMPTY_TOPIC_SET =
        new HashSet<ByteString>();

    // Boolean indicating if the channel manager is running or has been closed.
    // Once we stop the manager, we should sidestep all of the connect, write callback
    // and channel disconnected logic.
    protected boolean closed = false;
    protected final ReentrantReadWriteLock closedLock =
        new ReentrantReadWriteLock();

    // Global counter used for generating unique transaction ID's for
    // publish and subscribe requests
    protected final AtomicLong globalCounter = new AtomicLong();

    // Concurrent Map to store the mapping from the Topic to the Host.
    // This could change over time since servers can drop mastership of topics
    // for load balancing or failover. If a server host ever goes down, we'd
    // also want to remove all topic mappings the host was responsible for.
    // The second Map is used as the inverted version of the first one.
    protected final ConcurrentMap<ByteString, InetSocketAddress> topic2Host =
        new ConcurrentHashMap<ByteString, InetSocketAddress>();
    // The inverse mapping is used only when clearing all topics. For performance
    // consideration, we don't guarantee host2Topics to be consistent with
    // topic2Host. it would be better to not rely on this mapping for anything
    // significant.
    protected final ConcurrentMap<InetSocketAddress, Set<ByteString>> host2Topics =
        new ConcurrentHashMap<InetSocketAddress, Set<ByteString>>();

    // This channels will be used for publish and unsubscribe requests
    protected final CleanupChannelMap<InetSocketAddress> host2NonSubscriptionChannels =
        new CleanupChannelMap<InetSocketAddress>();

    private final ClientConfiguration cfg;
    // The Netty socket factory for making connections to the server.
    protected final ChannelFactory socketFactory;
    // PipelineFactory to create non-subscription netty channels to the appropriate server
    private final ClientChannelPipelineFactory nonSubscriptionChannelPipelineFactory;
    // ssl context factory
    private SslClientContextFactory sslFactory = null;

    // default server channel
    private final HChannel defaultServerChannel;

    // Each client instantiation will have a Timer for running recurring
    // threads. One such timer task thread to is to timeout long running
    // PubSubRequests that are waiting for an ack response from the server.
    private final Timer clientTimer = new Timer(true);
    // a common consume callback for all consume requests.
    private final MessageConsumeCallback consumeCb;
    // A event emitter to emit subscription events
    private final SubscriptionEventEmitter eventEmitter;

    protected AbstractHChannelManager(ClientConfiguration cfg,
                                      ChannelFactory socketFactory) {
        this.cfg = cfg;
        this.socketFactory = socketFactory;
        this.nonSubscriptionChannelPipelineFactory =
            new NonSubscriptionChannelPipelineFactory(cfg, this);

        // create a default server channel
        defaultServerChannel =
            new DefaultServerChannel(cfg.getDefaultServerHost(), this);

        if (cfg.isSSLEnabled()) {
            sslFactory = new SslClientContextFactory(cfg);
        }

        consumeCb = new MessageConsumeCallback(cfg, this);
        eventEmitter = new SubscriptionEventEmitter();

        // Schedule Request Timeout task.
        clientTimer.schedule(new PubSubRequestTimeoutTask(), 0,
                             cfg.getTimeoutThreadRunInterval());
    }

    @Override
    public SubscriptionEventEmitter getSubscriptionEventEmitter() {
        return eventEmitter;
    }

    public MessageConsumeCallback getConsumeCallback() {
        return consumeCb;
    }

    public SslClientContextFactory getSslFactory() {
        return sslFactory;
    }

    protected ChannelFactory getChannelFactory() {
        return socketFactory;
    }

    protected ClientChannelPipelineFactory getNonSubscriptionChannelPipelineFactory() {
        return this.nonSubscriptionChannelPipelineFactory;
    }

    protected abstract ClientChannelPipelineFactory getSubscriptionChannelPipelineFactory();

    @Override
    public void schedule(final TimerTask task, final long delay) {
        this.closedLock.readLock().lock();
        try {
            if (closed) {
                logger.warn("Task {} is not scheduled due to the channel manager is closed.",
                            task);
                return;
            }
            clientTimer.schedule(task, delay);
        } finally {
            this.closedLock.readLock().unlock();
        }
    }

    @Override
    public void submitOpAfterDelay(final PubSubData pubSubData, final long delay) {
        this.closedLock.readLock().lock();
        try {
            if (closed) {
                pubSubData.getCallback().operationFailed(pubSubData.context,
                    new ServiceDownException("Client has been closed."));
                return;
            }
            clientTimer.schedule(new TimerTask() {
                @Override
                public void run() {
                    logger.debug("Submit request {} in {} ms later.",
                                 va(pubSubData, delay));
                    submitOp(pubSubData);
                }
            }, delay);
        } finally {
            closedLock.readLock().unlock();
        }
    }

    @Override
    public void submitOp(PubSubData pubSubData) {
        HChannel hChannel;
        if (OperationType.PUBLISH.equals(pubSubData.operationType) ||
            OperationType.UNSUBSCRIBE.equals(pubSubData.operationType)) {
            hChannel = getNonSubscriptionChannelByTopic(pubSubData.topic);
        } else {
            TopicSubscriber ts = new TopicSubscriber(pubSubData.topic,
                                                     pubSubData.subscriberId);
            hChannel = getSubscriptionChannelByTopicSubscriber(ts);
        }
        // no channel found to submit pubsub data
        // choose the default server
        if (null == hChannel) {
            hChannel = defaultServerChannel;
        }
        hChannel.submitOp(pubSubData);
    }

    @Override
    public void redirectToHost(PubSubData pubSubData, InetSocketAddress host) {
        logger.debug("Submit operation {} to host {}.",
                     va(pubSubData, host));
        HChannel hChannel;
        if (OperationType.PUBLISH.equals(pubSubData.operationType) ||
            OperationType.UNSUBSCRIBE.equals(pubSubData.operationType)) {
            hChannel = getNonSubscriptionChannel(host);
            if (null == hChannel) {
                // create a channel to connect to specified host
                hChannel = createAndStoreNonSubscriptionChannel(host);
            }
        } else {
            hChannel = getSubscriptionChannel(host);
            if (null == hChannel) {
                // create a subscription channel to specified host
                hChannel = createAndStoreSubscriptionChannel(host);
            }
        }
        // no channel found to submit pubsub data
        // choose the default server
        if (null == hChannel) {
            hChannel = defaultServerChannel;
        }
        hChannel.submitOp(pubSubData);
    }

    void submitOpThruChannel(PubSubData pubSubData, Channel channel) {
        logger.debug("Submit operation {} to thru channel {}.",
                     va(pubSubData, channel));
        HChannel hChannel;
        if (OperationType.PUBLISH.equals(pubSubData.operationType) ||
            OperationType.UNSUBSCRIBE.equals(pubSubData.operationType)) {
            hChannel = createAndStoreNonSubscriptionChannel(channel);
        } else {
            hChannel = createAndStoreSubscriptionChannel(channel);
        }
        hChannel.submitOp(pubSubData);
    }

    @Override
    public void submitOpToDefaultServer(PubSubData pubSubData) {
        logger.debug("Submit operation {} to default server {}.",
                     va(pubSubData, defaultServerChannel));
        defaultServerChannel.submitOp(pubSubData);
    }

    // Synchronized method to store the host2Channel mapping (if it doesn't
    // exist yet). Retrieve the hostname info from the Channel created via the
    // RemoteAddress tied to it.
    private HChannel createAndStoreNonSubscriptionChannel(Channel channel) {
        InetSocketAddress host = NetUtils.getHostFromChannel(channel);
        HChannel newHChannel = new HChannelImpl(host, channel, this,
                                                getNonSubscriptionChannelPipelineFactory());
        return storeNonSubscriptionChannel(host, newHChannel);
    }

    private HChannel createAndStoreNonSubscriptionChannel(InetSocketAddress host) {
        HChannel newHChannel = new HChannelImpl(host, this,
                                                getNonSubscriptionChannelPipelineFactory());
        return storeNonSubscriptionChannel(host, newHChannel);
    }

    private HChannel storeNonSubscriptionChannel(InetSocketAddress host,
                                                 HChannel newHChannel) {
        return host2NonSubscriptionChannels.addChannel(host, newHChannel);
    }

    /**
     * Is there a {@link HChannel} existed for a given host.
     *
     * @param host
     *          Target host address.
     */
    private HChannel getNonSubscriptionChannel(InetSocketAddress host) {
        return host2NonSubscriptionChannels.getChannel(host);
    }

    /**
     * Get a non-subscription channel for a given <code>topic</code>.
     *
     * @param topic
     *          Topic Name
     * @return if <code>topic</code>'s owner is unknown, return null.
     *         if <code>topic</code>'s owner is know and there is a channel
     *         existed before, return the existed channel, otherwise created
     *         a new one.
     */
    private HChannel getNonSubscriptionChannelByTopic(ByteString topic) {
        InetSocketAddress host = topic2Host.get(topic);
        if (null == host) {
            // we don't know where is the topic
            return null;
        } else {
            // we had know which server owned the topic
            HChannel channel = getNonSubscriptionChannel(host);
            if (null == channel) {
                // create a channel to connect to specified host
                channel = createAndStoreNonSubscriptionChannel(host);
            }
            return channel;
        }
    }

    /**
     * Handle the disconnected event from a non-subscription {@link HChannel}.
     *
     * @param host
     *          Which host is disconnected.
     * @param channel
     *          The underlying established channel.
     */
    protected void onNonSubscriptionChannelDisconnected(InetSocketAddress host,
                                                        Channel channel) {
        // Only remove the Channel from the mapping if this current
        // disconnected channel is the same as the cached entry.
        // Due to race concurrency situations, it is possible to
        // create multiple channels to the same host for publish
        // and unsubscribe requests.
        HChannel hChannel = host2NonSubscriptionChannels.getChannel(host);
        if (null == hChannel) {
            return;
        }
        Channel underlyingChannel = hChannel.getChannel();
        if (null == underlyingChannel ||
            !underlyingChannel.equals(channel)) {
            return;
        }
        logger.info("NonSubscription Channel {} to {} disconnected.",
                    va(channel, host));
        // remove existed channel
        if (host2NonSubscriptionChannels.removeChannel(host, hChannel)) {
            clearAllTopicsForHost(host);
        }
    }

    /**
     * Create and store a subscription {@link HChannel} thru the underlying established
     * <code>channel</code>
     *
     * @param channel
     *          The underlying established subscription channel.
     */
    protected abstract HChannel createAndStoreSubscriptionChannel(Channel channel);

    /**
     * Create and store a subscription {@link HChannel} to target host.
     *
     * @param host
     *          Target host address.
     */
    protected abstract HChannel createAndStoreSubscriptionChannel(InetSocketAddress host);

    /**
     * Is there a subscription {@link HChannel} existed for a given host.
     *
     * @param host
     *          Target host address.
     */
    protected abstract HChannel getSubscriptionChannel(InetSocketAddress host);

    /**
     * Get a subscription channel for a given <code>topicSubscriber</code>.
     *
     * @param topicSubscriber
     *          Topic Subscriber
     * @return if <code>topic</code>'s owner is unknown, return null.
     *         if <code>topic</code>'s owner is know and there is a channel
     *         existed before, return the existed channel, otherwise created
     *         a new one for the <code>topicSubscriber</code>.
     */
    protected abstract HChannel getSubscriptionChannelByTopicSubscriber(TopicSubscriber topicSubscriber);

    /**
     * Handle the disconnected event from a subscription {@link HChannel}.
     *
     * @param host
     *          Which host is disconnected.
     * @param channel
     *          The underlying established channel.
     */
    protected abstract void onSubscriptionChannelDisconnected(InetSocketAddress host,
                                                              Channel channel);

    private void sendConsumeRequest(final TopicSubscriber topicSubscriber,
                                    final MessageSeqId messageSeqId,
                                    final Channel channel) {
        PubSubRequest.Builder pubsubRequestBuilder =
            NetUtils.buildConsumeRequest(nextTxnId(), topicSubscriber, messageSeqId);  

        // For Consume requests, we will send them from the client in a fire and
        // forget manner. We are not expecting the server to send back an ack
        // response so no need to register this in the ResponseHandler. There
        // are no callbacks to invoke since this isn't a client initiated
        // action. Instead, just have a future listener that will log an error
        // message if there was a problem writing the consume request.
        logger.debug("Writing a Consume request to host: {} with messageSeqId: {} for {}",
                     va(NetUtils.getHostFromChannel(channel), messageSeqId, topicSubscriber));
        ChannelFuture future = channel.write(pubsubRequestBuilder.build());
        future.addListener(new ChannelFutureListener() {
            @Override
            public void operationComplete(ChannelFuture future) throws Exception {
                if (!future.isSuccess()) {
                    logger.error("Error writing a Consume request to host: {} with messageSeqId: {} for {}",
                                 va(NetUtils.getHostFromChannel(channel),
                                    messageSeqId, topicSubscriber));
                }
            }
        });
    }

    /**
     * Helper method to store the topic2Host mapping in the channel manager cache
     * map. This method is assumed to be called when we've done a successful
     * connection to the correct server topic master.
     *
     * @param topic
     *            Topic Name
     * @param host
     *            Host Address
     */
    protected void storeTopic2HostMapping(ByteString topic, InetSocketAddress host) {
        InetSocketAddress oldHost = topic2Host.putIfAbsent(topic, host);
        if (null != oldHost && oldHost.equals(host)) {
            // Entry in map exists for the topic but it is the same as the
            // current host. In this case there is nothing to do.
            return;
        }

        if (null != oldHost) {
            if (topic2Host.replace(topic, oldHost, host)) {
                // Store the relevant mappings for this topic and host combination.
                logger.debug("Storing info for topic: {}, old host: {}, new host: {}.",
                             va(topic.toStringUtf8(), oldHost, host));
                clearHostForTopic(topic, oldHost);
            } else {
                logger.warn("Ownership of topic: {} has been changed from {} to {} when storeing host: {}",
                            va(topic.toStringUtf8(), oldHost, topic2Host.get(topic), host));
                return;
            }
        } else {
            logger.debug("Storing info for topic: {}, host: {}.",
                         va(topic.toStringUtf8(), host));
        }
        Set<ByteString> topicsForHost = host2Topics.get(host);
        if (null == topicsForHost) {
            Set<ByteString> newTopicsSet = new HashSet<ByteString>();
            topicsForHost = host2Topics.putIfAbsent(host, newTopicsSet);
            if (null == topicsForHost) {
              topicsForHost = newTopicsSet;
            }
        }
        synchronized (topicsForHost) {
            // check whether the ownership changed, since it might happened
            // after replace succeed
            if (host.equals(topic2Host.get(topic))) {
                topicsForHost.add(topic);
            }
        }
    }

    // If a server host goes down or the channel to it gets disconnected,
    // we want to clear out all relevant cached information. We'll
    // need to remove all of the topic mappings that the host was
    // responsible for.
    protected void clearAllTopicsForHost(InetSocketAddress host) {
        logger.debug("Clearing all topics for host: {}", host);
        // For each of the topics that the host was responsible for,
        // remove it from the topic2Host mapping.
        Set<ByteString> topicsForHost = host2Topics.get(host);
        if (null != topicsForHost) {
            synchronized (topicsForHost) {
                for (ByteString topic : topicsForHost) {
                    logger.debug("Removing mapping for topic: {} from host: {}.",
                                 va(topic.toStringUtf8(), host));
                    topic2Host.remove(topic, host);
                }
            }
            // Now it is safe to remove the host2Topics mapping entry.
            host2Topics.remove(host, topicsForHost);
        }
    }

    // If a subscribe channel goes down, the topic might have moved.
    // We only clear out that topic for the host and not all cached information.
    public void clearHostForTopic(ByteString topic, InetSocketAddress host) {
        logger.debug("Clearing topic: {} from host: {}.",
                     va(topic.toStringUtf8(), host));
        if (topic2Host.remove(topic, host)) {
            logger.debug("Removed topic to host mapping for topic: {} and host: {}.",
                         va(topic.toStringUtf8(), host));
        }
        Set<ByteString> topicsForHost = host2Topics.get(host);
        if (null != topicsForHost) {
            boolean removed;
            synchronized (topicsForHost) {
                removed = topicsForHost.remove(topic);
            }
            if (removed) {
                logger.debug("Removed topic: {} from host: {}.",
                             topic.toStringUtf8(), host);
                if (topicsForHost.isEmpty()) {
                    // remove only topic list is empty
                    host2Topics.remove(host, EMPTY_TOPIC_SET);
                }
            }
        }
    }

    @Override
    public long nextTxnId() {
        return globalCounter.incrementAndGet();
    }

    // We need to deal with the possible problem of a PubSub request being
    // written to successfully to the server host but for some reason, the
    // ack message back never comes. What could happen is that the VoidCallback
    // stored in the ResponseHandler.txn2PublishData map will never be called.
    // We should have a configured timeout so if that passes from the time a
    // write was successfully done to the server, we can fail this async PubSub
    // transaction. The caller could possibly redo the transaction if needed at
    // a later time. Creating a timeout cleaner TimerTask to do this here.
    class PubSubRequestTimeoutTask extends TimerTask {
        /**
         * Implement the TimerTask's abstract run method.
         */
        @Override
        public void run() {
            if (isClosed()) {
                return;
            }
            logger.debug("Running the PubSubRequest Timeout Task");
            // First check those non-subscription channels
            for (HChannel channel : host2NonSubscriptionChannels.getChannels()) {
                try {
                    HChannelHandler channelHandler =
                        HChannelImpl.getHChannelHandlerFromChannel(channel.getChannel());
                    channelHandler.checkTimeoutRequests();
                } catch (NoResponseHandlerException nrhe) {
                    continue;
                }
            }
            // Then check those subscription channels
            checkTimeoutRequestsOnSubscriptionChannels();
        }
    }

    protected abstract void restartDelivery(TopicSubscriber topicSubscriber)
        throws ClientNotSubscribedException, AlreadyStartDeliveryException;

    /**
     * Chekout the pub/sub requests on subscription channels.
     */
    protected abstract void checkTimeoutRequestsOnSubscriptionChannels();

    @Override
    public boolean isClosed() {
        closedLock.readLock().lock();
        try {
            return closed; 
        } finally {
            closedLock.readLock().unlock();
        }
    }

    /**
     * Close all subscription channels when close channel manager.
     */
    protected abstract void closeSubscriptionChannels();

    @Override
    public void close() {
        logger.info("Shutting down the channels manager.");
        closedLock.writeLock().lock();
        try {
            // Not first time to close
            if (closed) {
                return;
            }
            closed = true;
        } finally {
            closedLock.writeLock().unlock();
        }
        clientTimer.cancel();
        // Clear all existed channels
        host2NonSubscriptionChannels.close();

        // clear all subscription channels
        closeSubscriptionChannels();

        // Clear out all Maps
        topic2Host.clear();
        host2Topics.clear();
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/impl/AbstractSubscribeResponseHandler.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty.impl;

import java.net.InetSocketAddress;
import java.util.LinkedList;
import java.util.Queue;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ConcurrentMap;
import java.util.concurrent.locks.ReentrantReadWriteLock;

import com.google.protobuf.ByteString;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;
import org.jboss.netty.channel.ChannelFuture;
import org.jboss.netty.channel.ChannelFutureListener;

import org.apache.hedwig.client.api.MessageHandler;
import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.data.MessageConsumeData;
import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.client.data.TopicSubscriber;
import org.apache.hedwig.client.exceptions.AlreadyStartDeliveryException;
import org.apache.hedwig.client.handlers.SubscribeResponseHandler;
import org.apache.hedwig.client.netty.HChannelManager;
import org.apache.hedwig.client.netty.HChannel;
import org.apache.hedwig.client.netty.NetUtils;
import org.apache.hedwig.client.netty.FilterableMessageHandler;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.ClientAlreadySubscribedException;
import org.apache.hedwig.exceptions.PubSubException.ClientNotSubscribedException;
import org.apache.hedwig.exceptions.PubSubException.ServiceDownException;
import org.apache.hedwig.exceptions.PubSubException.UnexpectedConditionException;
import org.apache.hedwig.filter.ClientMessageFilter;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.OperationType;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse;
import org.apache.hedwig.protocol.PubSubProtocol.ResponseBody;
import org.apache.hedwig.protocol.PubSubProtocol.StatusCode;
import org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEvent;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences;
import org.apache.hedwig.protoextensions.MessageIdUtils;
import org.apache.hedwig.protoextensions.SubscriptionStateUtils;
import org.apache.hedwig.util.Callback;
import org.apache.hedwig.util.Either;
import org.apache.hedwig.util.SubscriptionListener;
import static org.apache.hedwig.util.VarArgs.va;

public abstract class AbstractSubscribeResponseHandler extends SubscribeResponseHandler {

    private static Logger logger =
        LoggerFactory.getLogger(AbstractSubscribeResponseHandler.class);

    protected final ReentrantReadWriteLock disconnectLock =
        new ReentrantReadWriteLock();

    protected final ConcurrentMap<TopicSubscriber, ActiveSubscriber> subscriptions
        = new ConcurrentHashMap<TopicSubscriber, ActiveSubscriber>();
    protected final AbstractHChannelManager aChannelManager;

    protected AbstractSubscribeResponseHandler(ClientConfiguration cfg,
                                               HChannelManager channelManager) {
        super(cfg, channelManager);
        this.aChannelManager = (AbstractHChannelManager) channelManager;
    }

    protected HChannelManager getHChannelManager() {
        return this.channelManager;
    }

    protected ClientConfiguration getConfiguration() {
        return cfg;
    }

    protected ActiveSubscriber getActiveSubscriber(TopicSubscriber ts) {
        return subscriptions.get(ts);
    }

    protected ActiveSubscriber createActiveSubscriber(
        ClientConfiguration cfg, AbstractHChannelManager channelManager,
        TopicSubscriber ts, PubSubData op, SubscriptionPreferences preferences,
        Channel channel, HChannel hChannel) {
        return new ActiveSubscriber(cfg, channelManager, ts, op, preferences, channel, hChannel);
    }

    @Override
    public void handleResponse(PubSubResponse response, PubSubData pubSubData,
                               Channel channel) throws Exception {
        if (logger.isDebugEnabled()) {
            logger.debug("Handling a Subscribe response: {}, pubSubData: {}, host: {}.",
                         va(response, pubSubData, NetUtils.getHostFromChannel(channel)));
        }
        switch (response.getStatusCode()) {
        case SUCCESS:
            TopicSubscriber ts = new TopicSubscriber(pubSubData.topic,
                                                     pubSubData.subscriberId);
            SubscriptionPreferences preferences = null;
            if (response.hasResponseBody()) {
                ResponseBody respBody = response.getResponseBody();
                if (respBody.hasSubscribeResponse()) {
                    SubscribeResponse resp = respBody.getSubscribeResponse();
                    if (resp.hasPreferences()) {
                        preferences = resp.getPreferences();
                        if (logger.isDebugEnabled()) {
                            logger.debug("Receive subscription preferences for {} : {}",
                                         va(ts,
                                            SubscriptionStateUtils.toString(preferences)));
                        }
                    }
                }
            }

            Either<StatusCode, HChannel> result;
            StatusCode statusCode;
            ActiveSubscriber ss = null;
            // Store the Subscribe state
            disconnectLock.readLock().lock();
            try {
                result = handleSuccessResponse(ts, pubSubData, channel);
                statusCode = result.left();
                if (StatusCode.SUCCESS == statusCode) {
                    ss = createActiveSubscriber(
                        cfg, aChannelManager, ts, pubSubData, preferences, channel, result.right());
                    statusCode = addSubscription(ts, ss);
                }
            } finally {
                disconnectLock.readLock().unlock();
            }
            if (StatusCode.SUCCESS == statusCode) {
                postHandleSuccessResponse(ts, ss);
                // Response was success so invoke the callback's operationFinished
                // method.
                pubSubData.getCallback().operationFinished(pubSubData.context, null);
            } else {
                PubSubException exception = PubSubException.create(statusCode,
                    "Client is already subscribed for " + ts);
                pubSubData.getCallback().operationFailed(pubSubData.context, exception);
            }
            break;
        case CLIENT_ALREADY_SUBSCRIBED:
            // For Subscribe requests, the server says that the client is
            // already subscribed to it.
            pubSubData.getCallback().operationFailed(pubSubData.context,
                    new ClientAlreadySubscribedException("Client is already subscribed for topic: "
                                                         + pubSubData.topic.toStringUtf8() + ", subscriberId: "
                                                         + pubSubData.subscriberId.toStringUtf8()));
            break;
        case SERVICE_DOWN:
            // Response was service down failure so just invoke the callback's
            // operationFailed method.
            pubSubData.getCallback().operationFailed(pubSubData.context, new ServiceDownException(
                                                     "Server responded with a SERVICE_DOWN status"));
            break;
        case NOT_RESPONSIBLE_FOR_TOPIC:
            // Redirect response so we'll need to repost the original Subscribe
            // Request
            handleRedirectResponse(response, pubSubData, channel);
            break;
        default:
            // Consider all other status codes as errors, operation failed
            // cases.
            logger.error("Unexpected error response from server for PubSubResponse: " + response);
            pubSubData.getCallback().operationFailed(pubSubData.context,
                    new ServiceDownException("Server responded with a status code of: "
                            + response.getStatusCode(),
                            PubSubException.create(response.getStatusCode(),
                                                   "Original Exception")));
            break;
        }
    }

    /**
     * Handle success response for a specific TopicSubscriber <code>ts</code>. The method
     * is triggered after subscribed successfully.
     *
     * @param ts
     *          Topic Subscriber.
     * @param pubSubData
     *          Pub/Sub Request data for this subscribe request.
     * @param channel
     *          Subscription Channel.
     * @return status code to indicate what happened
     */
    protected abstract Either<StatusCode, HChannel> handleSuccessResponse(
        TopicSubscriber ts, PubSubData pubSubData, Channel channel);

    protected void postHandleSuccessResponse(TopicSubscriber ts, ActiveSubscriber ss) {
        // do nothing now
    }

    private StatusCode addSubscription(TopicSubscriber ts, ActiveSubscriber ss) {
        ActiveSubscriber oldSS = subscriptions.putIfAbsent(ts, ss);
        if (null != oldSS) {
            return StatusCode.CLIENT_ALREADY_SUBSCRIBED;
        } else {
            return StatusCode.SUCCESS;
        }
    }

    @Override
    public void handleSubscribeMessage(PubSubResponse response) {
        Message message = response.getMessage();
        TopicSubscriber ts = new TopicSubscriber(response.getTopic(),
                                                 response.getSubscriberId());
        if (logger.isDebugEnabled()) {
            logger.debug("Handling a Subscribe message in response: {}, {}",
                         va(response, ts));
        }
        ActiveSubscriber ss = getActiveSubscriber(ts);
        if (null == ss) {
            logger.error("Subscriber {} is not found receiving its message {}.",
                         va(ts, MessageIdUtils.msgIdToReadableString(message.getMsgId())));
            return;
        }
        ss.handleMessage(message);
    }

    @Override
    protected void asyncMessageDeliver(TopicSubscriber topicSubscriber,
                                       Message message) {
        ActiveSubscriber ss = getActiveSubscriber(topicSubscriber);
        if (null == ss) {
            logger.error("Subscriber {} is not found delivering its message {}.",
                         va(topicSubscriber,
                            MessageIdUtils.msgIdToReadableString(message.getMsgId())));
            return;
        }
        ss.asyncMessageDeliver(message);
    }

    @Override
    protected void messageConsumed(TopicSubscriber topicSubscriber,
                                   Message message) {
        ActiveSubscriber ss = getActiveSubscriber(topicSubscriber);
        if (null == ss) {
            logger.warn("Subscriber {} is not found consumed its message {}.",
                        va(topicSubscriber,
                           MessageIdUtils.msgIdToReadableString(message.getMsgId())));
            return;
        }
        if (logger.isDebugEnabled()) {
            logger.debug("Message has been successfully consumed by the client app : {}, {}",
                         va(message, topicSubscriber));
        }
        ss.messageConsumed(message);
    }

    @Override
    public void handleSubscriptionEvent(ByteString topic, ByteString subscriberId,
                                        SubscriptionEvent event) {
        TopicSubscriber ts = new TopicSubscriber(topic, subscriberId);
        ActiveSubscriber ss = getActiveSubscriber(ts);
        if (null == ss) {
            logger.warn("No subscription {} found receiving subscription event {}.",
                        va(ts, event));
            return;
        }
        if (logger.isDebugEnabled()) {
            logger.debug("Received subscription event {} for ({}).",
                         va(event, ts));
        }
        processSubscriptionEvent(ss, event);
    }

    protected void processSubscriptionEvent(ActiveSubscriber as, SubscriptionEvent event) {
        switch (event) {
        // for all cases we need to resubscribe for the subscription
        case TOPIC_MOVED:
        case SUBSCRIPTION_FORCED_CLOSED:
            resubscribeIfNecessary(as, event);
            break;
        default:
            logger.error("Receive unknown subscription event {} for {}.",
                         va(event, as.getTopicSubscriber()));
        }
    }

    @Override
    public void startDelivery(final TopicSubscriber topicSubscriber,
                              MessageHandler messageHandler)
    throws ClientNotSubscribedException, AlreadyStartDeliveryException {
        ActiveSubscriber ss = getActiveSubscriber(topicSubscriber);
        if (null == ss) {
            throw new ClientNotSubscribedException("Client is not yet subscribed to " + topicSubscriber);
        }
        if (logger.isDebugEnabled()) {
            logger.debug("Start delivering message for {} using message handler {}",
                         va(topicSubscriber, messageHandler));
        }
        ss.startDelivery(messageHandler);
    }

    @Override
    public void stopDelivery(final TopicSubscriber topicSubscriber)
    throws ClientNotSubscribedException {
        ActiveSubscriber ss = getActiveSubscriber(topicSubscriber);
        if (null == ss) {
            throw new ClientNotSubscribedException("Client is not yet subscribed to " + topicSubscriber);
        }
        if (logger.isDebugEnabled()) {
            logger.debug("Stop delivering messages for {}", topicSubscriber);
        }
        ss.stopDelivery();
    }

    @Override
    public boolean hasSubscription(TopicSubscriber topicSubscriber) {
        return subscriptions.containsKey(topicSubscriber);
    }

    @Override
    public void consume(final TopicSubscriber topicSubscriber,
                        final MessageSeqId messageSeqId) {
        ActiveSubscriber ss = getActiveSubscriber(topicSubscriber);
        if (null == ss) {
            logger.warn("Subscriber {} is not found consuming message {}.",
                        va(topicSubscriber,
                           MessageIdUtils.msgIdToReadableString(messageSeqId)));
            return;
        }
        ss.consume(messageSeqId);
    }

    @Override
    public void onChannelDisconnected(InetSocketAddress host, Channel channel) {
        disconnectLock.writeLock().lock();
        try {
            onDisconnect(host);
        } finally {
            disconnectLock.writeLock().unlock();
        }
    }

    private void onDisconnect(InetSocketAddress host) {
        for (ActiveSubscriber ss : subscriptions.values()) {
            onDisconnect(ss, host);
        }
    }

    private void onDisconnect(ActiveSubscriber ss, InetSocketAddress host) {
        logger.info("Subscription channel for ({}) is disconnected.", ss);
        resubscribeIfNecessary(ss, SubscriptionEvent.TOPIC_MOVED);
    }

    protected boolean removeSubscription(TopicSubscriber ts, ActiveSubscriber ss) {
        return subscriptions.remove(ts, ss);
    }

    protected void resubscribeIfNecessary(ActiveSubscriber ss, SubscriptionEvent event) {
        // if subscriber has been changed, we don't need to resubscribe
        if (!removeSubscription(ss.getTopicSubscriber(), ss)) {
            return;
        }
        ss.resubscribeIfNecessary(event);
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/impl/ActiveSubscriber.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty.impl;

import java.util.LinkedList;
import java.util.Queue;

import com.google.protobuf.ByteString;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;
import org.jboss.netty.channel.ChannelFuture;
import org.jboss.netty.channel.ChannelFutureListener;

import org.apache.hedwig.client.api.MessageHandler;
import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.data.MessageConsumeData;
import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.client.data.TopicSubscriber;
import org.apache.hedwig.client.exceptions.AlreadyStartDeliveryException;
import org.apache.hedwig.client.netty.HChannel;
import org.apache.hedwig.client.netty.NetUtils;
import org.apache.hedwig.client.netty.FilterableMessageHandler;
import org.apache.hedwig.exceptions.PubSubException.ClientNotSubscribedException;
import org.apache.hedwig.filter.ClientMessageFilter;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEvent;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences;
import org.apache.hedwig.protoextensions.MessageIdUtils;
import org.apache.hedwig.protoextensions.SubscriptionStateUtils;
import static org.apache.hedwig.util.VarArgs.va;

/**
 * an active subscriber handles subscription actions in a channel.
 */
public class ActiveSubscriber {

    private static final Logger logger = LoggerFactory.getLogger(ActiveSubscriber.class);

    protected final ClientConfiguration cfg;
    protected final AbstractHChannelManager channelManager;

    // Subscriber related variables
    protected final TopicSubscriber topicSubscriber;
    protected final PubSubData op;
    protected final SubscriptionPreferences preferences;

    // the underlying netty channel to send request
    protected final Channel channel;
    protected final HChannel hChannel;

    // Counter for the number of consumed messages so far to buffer up before we
    // send the Consume message back to the server along with the last/largest
    // message seq ID seen so far in that batch.
    private int numConsumedMessagesInBuffer = 0;
    private MessageSeqId lastMessageSeqId = null;

    // Message Handler
    private MessageHandler msgHandler = null;

    // Queue used for subscribes when the MessageHandler hasn't been registered
    // yet but we've already received subscription messages from the server.
    // This will be lazily created as needed.
    private final Queue<Message> msgQueue = new LinkedList<Message>();

    /**
     * Construct an active subscriber instance.
     *
     * @param cfg
     *          Client configuration object.
     * @param channelManager
     *          Channel manager instance.
     * @param ts
     *          Topic subscriber.
     * @param op
     *          Pub/Sub request.
     * @param preferences
     *          Subscription preferences for the subscriber.
     * @param channel
     *          Netty channel the subscriber lived.
     */
    public ActiveSubscriber(ClientConfiguration cfg,
                            AbstractHChannelManager channelManager,
                            TopicSubscriber ts, PubSubData op,
                            SubscriptionPreferences preferences,
                            Channel channel,
                            HChannel hChannel) {
        this.cfg = cfg;
        this.channelManager = channelManager;
        this.topicSubscriber = ts;
        this.op = op;
        this.preferences = preferences;
        this.channel = channel;
        this.hChannel = hChannel;
    }

    /**
     * @return pub/sub request for the subscription.
     */
    public PubSubData getPubSubData() {
        return this.op;
    }

    /**
     * @return topic subscriber id for the active subscriber.
     */
    public TopicSubscriber getTopicSubscriber() {
        return this.topicSubscriber;
    }

    /**
     * Start delivering messages using given message handler.
     *
     * @param messageHandler
     *          Message handler to deliver messages
     * @throws AlreadyStartDeliveryException if someone already started delivery.
     * @throws ClientNotSubscribedException when start delivery before subscribe.
     */
    public synchronized void startDelivery(MessageHandler messageHandler)
    throws AlreadyStartDeliveryException, ClientNotSubscribedException {
        if (null != this.msgHandler) {
            throw new AlreadyStartDeliveryException("A message handler " + msgHandler
                + " has been started for " + topicSubscriber);
        }
        if (null != messageHandler && messageHandler instanceof FilterableMessageHandler) {
            FilterableMessageHandler filterMsgHandler =
                (FilterableMessageHandler) messageHandler;
            if (filterMsgHandler.hasMessageFilter()) {
                if (null == preferences) {
                    // no preferences means talking to an old version hub server
                    logger.warn("Start delivering messages with filter but no subscription "
                              + "preferences found. It might due to talking to an old version"
                              + " hub server.");
                    // use the original message handler.
                    messageHandler = filterMsgHandler.getMessageHandler();
                } else {
                    // pass subscription preferences to message filter
                    if (logger.isDebugEnabled()) {
                        logger.debug("Start delivering messages with filter on {}, preferences: {}",
                                     va(topicSubscriber,
                                        SubscriptionStateUtils.toString(preferences)));
                    }
                    ClientMessageFilter msgFilter = filterMsgHandler.getMessageFilter();
                    msgFilter.setSubscriptionPreferences(topicSubscriber.getTopic(),
                                                         topicSubscriber.getSubscriberId(),
                                                         preferences);
                }
            }
        }

        this.msgHandler = messageHandler;
        // Once the MessageHandler is registered, see if we have any queued up
        // subscription messages sent to us already from the server. If so,
        // consume those first. Do this only if the MessageHandler registered is
        // not null (since that would be the HedwigSubscriber.stopDelivery
        // call).
        if (null == msgHandler) {
            return;
        }
        if (msgQueue.size() > 0) {
            if (logger.isDebugEnabled()) {
                logger.debug("Consuming {} queued up messages for {}",
                             va(msgQueue.size(), topicSubscriber));
            }
            for (Message message : msgQueue) {
                asyncMessageDeliver(message);
            }
            // Now we can remove the queued up messages since they are all
            // consumed.
            msgQueue.clear();
        }
    }

    /**
     * Stop delivering messages to the subscriber.
     */
    public synchronized void stopDelivery() {
        this.msgHandler = null;
    }

    /**
     * Handle received message.
     *
     * @param message
     *          Received message.
     */
    public synchronized void handleMessage(Message message) {
        if (null != msgHandler) {
            asyncMessageDeliver(message);
        } else {
            // MessageHandler has not yet been registered so queue up these
            // messages for the Topic Subscription. Make the initial lazy
            // creation of the message queue thread safe just so we don't
            // run into a race condition where two simultaneous threads process
            // a received message and both try to create a new instance of
            // the message queue. Performance overhead should be okay
            // because the delivery of the topic has not even started yet
            // so these messages are not consumed and just buffered up here.
            if (logger.isDebugEnabled()) {
                logger.debug("Message {} has arrived but no MessageHandler provided for {}"
                             + " yet so queueing up the message.",
                             va(MessageIdUtils.msgIdToReadableString(message.getMsgId()),
                                topicSubscriber));
            }
            msgQueue.add(message);
        }
    }

    /**
     * Deliver message to the client.
     *
     * @param message
     *          Message to deliver.
     */
    public synchronized void asyncMessageDeliver(Message message) {
        if (null == msgHandler) {
            logger.error("No message handler found to deliver message {} to {}.",
                         va(MessageIdUtils.msgIdToReadableString(message.getMsgId()),
                            topicSubscriber));
            return;
        }
        if (logger.isDebugEnabled()) {
            logger.debug("Call the client app's MessageHandler asynchronously to deliver the message {} to {}",
                         va(message, topicSubscriber));
        }
        unsafeDeliverMessage(message);
    }

    /**
     * Unsafe version to deliver message to a message handler.
     * Caller need to handle synchronization issue.
     *
     * @param message
     *          Message to deliver.
     */
    protected void unsafeDeliverMessage(Message message) {
        MessageConsumeData messageConsumeData =
            new MessageConsumeData(topicSubscriber, message);
        msgHandler.deliver(topicSubscriber.getTopic(), topicSubscriber.getSubscriberId(),
                           message, channelManager.getConsumeCallback(),
                           messageConsumeData);
    }

    private synchronized boolean updateLastMessageSeqId(MessageSeqId seqId) {
        if (null != lastMessageSeqId &&
            seqId.getLocalComponent() <= lastMessageSeqId.getLocalComponent()) {
            return false;
        }
        ++numConsumedMessagesInBuffer;
        lastMessageSeqId = seqId;
        if (numConsumedMessagesInBuffer >= cfg.getConsumedMessagesBufferSize()) {
            numConsumedMessagesInBuffer = 0;
            lastMessageSeqId = null;
            return true;
        }
        return false;
    }

    /**
     * Consume a specific message.
     *
     * @param messageSeqId
     *          Message seq id.
     */
    public void consume(final MessageSeqId messageSeqId) {
        PubSubRequest.Builder pubsubRequestBuilder =
            NetUtils.buildConsumeRequest(channelManager.nextTxnId(),
                                         topicSubscriber, messageSeqId);

        // For Consume requests, we will send them from the client in a fire and
        // forget manner. We are not expecting the server to send back an ack
        // response so no need to register this in the ResponseHandler. There
        // are no callbacks to invoke since this isn't a client initiated
        // action. Instead, just have a future listener that will log an error
        // message if there was a problem writing the consume request.
        if (logger.isDebugEnabled()) {
            logger.debug("Writing a Consume request to channel: {} with messageSeqId: {} for {}",
                         va(channel, messageSeqId, topicSubscriber));
        }
        ChannelFuture future = channel.write(pubsubRequestBuilder.build());
        future.addListener(new ChannelFutureListener() {
            @Override
            public void operationComplete(ChannelFuture future) throws Exception {
                if (!future.isSuccess()) {
                    logger.error("Error writing a Consume request to channel: {} with messageSeqId: {} for {}",
                                 va(channel, messageSeqId, topicSubscriber));
                }
            }
        });
    }

    /**
     * Application acked to consume message.
     *
     * @param message
     *          Message consumed by application.
     */
    public void messageConsumed(Message message) {
        // For consume response to server, there is a config param on how many
        // messages to consume and buffer up before sending the consume request.
        // We just need to keep a count of the number of messages consumed
        // and the largest/latest msg ID seen so far in this batch. Messages
        // should be delivered in order and without gaps. Do this only if
        // auto-sending of consume messages is enabled.
        if (cfg.isAutoSendConsumeMessageEnabled()) {
            // Update these variables only if we are auto-sending consume
            // messages to the server. Otherwise the onus is on the client app
            // to call the Subscriber consume API to let the server know which
            // messages it has successfully consumed.
            if (updateLastMessageSeqId(message.getMsgId())) {
                // Send the consume request and reset the consumed messages buffer
                // variables. We will use the same Channel created from the
                // subscribe request for the TopicSubscriber.
                if (logger.isDebugEnabled()) {
                    logger.debug("Consume message {} when reaching consumed message buffer limit.",
                                 message.getMsgId());
                }
                consume(message.getMsgId());
            }
        }
    }

    /**
     * Resubscribe a subscriber if necessary.
     *
     * @param event
     *          Subscription Event.
     */
    public void resubscribeIfNecessary(SubscriptionEvent event) {
        // clear topic ownership
        if (SubscriptionEvent.TOPIC_MOVED == event) {
            channelManager.clearHostForTopic(topicSubscriber.getTopic(),
                                             NetUtils.getHostFromChannel(channel));
        }
        if (!op.options.getEnableResubscribe()) {
            channelManager.getSubscriptionEventEmitter().emitSubscriptionEvent(
                topicSubscriber.getTopic(), topicSubscriber.getSubscriberId(), event);
            return;
        }
        // Since the connection to the server host that was responsible
        // for the topic died, we are not sure about the state of that
        // server. Resend the original subscribe request data to the default
        // server host/VIP. Also clear out all of the servers we've
        // contacted or attempted to from this request as we are starting a
        // "fresh" subscribe request.
        op.clearServersList();
        // Set a new type of VoidCallback for this async call. We need this
        // hook so after the resubscribe has completed, delivery for
        // that topic subscriber should also be restarted (if it was that
        // case before the channel disconnect).
        final long retryWaitTime = cfg.getSubscribeReconnectRetryWaitTime();
        ResubscribeCallback resubscribeCb =
            new ResubscribeCallback(topicSubscriber, op,
                                    channelManager, retryWaitTime);
        op.setCallback(resubscribeCb);
        op.context = null;
        op.setOriginalChannelForResubscribe(hChannel);
        if (logger.isDebugEnabled()) {
            logger.debug("Resubscribe {} with origSubData {}",
                         va(topicSubscriber, op));
        }
        // resubmit the request
        channelManager.submitOp(op);
    }
}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/impl/ClientChannelPipelineFactory.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty.impl;

import java.util.Map;

import org.jboss.netty.channel.ChannelPipeline;
import org.jboss.netty.channel.ChannelPipelineFactory;
import org.jboss.netty.channel.Channels;
import org.jboss.netty.handler.codec.frame.LengthFieldBasedFrameDecoder;
import org.jboss.netty.handler.codec.frame.LengthFieldPrepender;
import org.jboss.netty.handler.codec.protobuf.ProtobufDecoder;
import org.jboss.netty.handler.codec.protobuf.ProtobufEncoder;
import org.jboss.netty.handler.ssl.SslHandler;

import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.handlers.AbstractResponseHandler;
import org.apache.hedwig.protocol.PubSubProtocol;
import org.apache.hedwig.protocol.PubSubProtocol.OperationType;

public abstract class ClientChannelPipelineFactory implements ChannelPipelineFactory {

    protected ClientConfiguration cfg;
    protected AbstractHChannelManager channelManager;

    public ClientChannelPipelineFactory(ClientConfiguration cfg,
                                        AbstractHChannelManager channelManager) {
        this.cfg = cfg;
        this.channelManager = channelManager;
    }

    protected abstract Map<OperationType, AbstractResponseHandler> createResponseHandlers();

    private HChannelHandler createHChannelHandler() {
        return new HChannelHandler(cfg, channelManager, createResponseHandlers());
    }

    // Retrieve a ChannelPipeline from the factory.
    public ChannelPipeline getPipeline() throws Exception {
        // Create a new ChannelPipline using the factory method from the
        // Channels helper class.
        ChannelPipeline pipeline = Channels.pipeline();
        if (channelManager.getSslFactory() != null) {
            pipeline.addLast("ssl", new SslHandler(channelManager.getSslFactory().getEngine()));
        }
        pipeline.addLast("lengthbaseddecoder", new LengthFieldBasedFrameDecoder(
                         cfg.getMaximumMessageSize(), 0, 4, 0, 4));
        pipeline.addLast("lengthprepender", new LengthFieldPrepender(4));

        pipeline.addLast("protobufdecoder", new ProtobufDecoder(PubSubProtocol.PubSubResponse.getDefaultInstance()));
        pipeline.addLast("protobufencoder", new ProtobufEncoder());

        pipeline.addLast("responsehandler", createHChannelHandler());
        return pipeline;
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/impl/DefaultServerChannel.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty.impl;

import java.net.InetSocketAddress;

import org.jboss.netty.channel.ChannelFuture;
import org.jboss.netty.channel.ChannelFutureListener;

import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.protocol.PubSubProtocol.OperationType;
import static org.apache.hedwig.util.VarArgs.va;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Handle requests sent to default hub server. <b>DefaultServerChannel</b> would not
 * be used as a channel to send requests directly. It just takes the responsibility to
 * connect to the default server. After the underlying netty channel is established,
 * it would call {@link HChannelManager#submitOpThruChannel()} to send requests thru
 * the underlying netty channel.
 */
class DefaultServerChannel extends HChannelImpl {

    private static Logger logger = LoggerFactory.getLogger(DefaultServerChannel.class);

    DefaultServerChannel(InetSocketAddress host, AbstractHChannelManager channelManager) {
        super(host, channelManager);
    }

    @Override
    public String toString() {
        StringBuilder sb = new StringBuilder();
        sb.append("[DefaultServer: ").append(host).append("]");
        return sb.toString();
    }

    @Override
    public void submitOp(final PubSubData pubSubData) {
        // for each pub/sub request sent to default hub server
        // we would establish a fresh connection for it
        ClientChannelPipelineFactory pipelineFactory;
        if (OperationType.PUBLISH.equals(pubSubData.operationType) ||
            OperationType.UNSUBSCRIBE.equals(pubSubData.operationType)) {
            pipelineFactory = channelManager.getNonSubscriptionChannelPipelineFactory();
        } else {
            pipelineFactory = channelManager.getSubscriptionChannelPipelineFactory();
        }
        ChannelFuture future = connect(host, pipelineFactory);
        future.addListener(new ChannelFutureListener() {
            @Override
            public void operationComplete(ChannelFuture future) throws Exception {
                // If the channel has been closed, there is no need to proceed with any callback
                // logic here.
                if (closed) {
                    future.getChannel().close();
                    return;
                }

                // Check if the connection to the server was done successfully.
                if (!future.isSuccess()) {
                    logger.error("Error connecting to host {}.", host);
                    future.getChannel().close();

                    retryOrFailOp(pubSubData);
                    // Finished with failure logic so just return.
                    return;
                }
                logger.debug("Connected to host {} for pubSubData: {}",
                             va(host, pubSubData));
                channelManager.submitOpThruChannel(pubSubData, future.getChannel());
            }
        });
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/impl/HChannelHandler.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty.impl;

import java.net.InetSocketAddress;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ConcurrentMap;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;
import org.jboss.netty.channel.ChannelHandlerContext;
import org.jboss.netty.channel.ChannelPipelineCoverage;
import org.jboss.netty.channel.ChannelStateEvent;
import org.jboss.netty.channel.ExceptionEvent;
import org.jboss.netty.channel.MessageEvent;
import org.jboss.netty.channel.SimpleChannelHandler;
import org.jboss.netty.handler.ssl.SslHandler;

import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.client.exceptions.NoResponseHandlerException;
import org.apache.hedwig.client.netty.NetUtils;
import org.apache.hedwig.client.handlers.AbstractResponseHandler;
import org.apache.hedwig.client.handlers.SubscribeResponseHandler;
import org.apache.hedwig.exceptions.PubSubException.UncertainStateException;
import org.apache.hedwig.exceptions.PubSubException.UnexpectedConditionException;
import org.apache.hedwig.protocol.PubSubProtocol.OperationType;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse;
import org.apache.hedwig.protocol.PubSubProtocol.ResponseBody;
import org.apache.hedwig.protocol.PubSubProtocol.StatusCode;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse;
import static org.apache.hedwig.util.VarArgs.va;

@ChannelPipelineCoverage("all")
public class HChannelHandler extends SimpleChannelHandler {

    private static Logger logger = LoggerFactory.getLogger(HChannelHandler.class);

    // Concurrent Map to store for each async PubSub request, the txn ID
    // and the corresponding PubSub call's data which stores the VoidCallback to
    // invoke when we receive a PubSub ack response from the server.
    // This is specific to this instance of the HChannelHandler which is
    // tied to a specific netty Channel Pipeline.
    private final ConcurrentMap<Long, PubSubData> txn2PubSubData =
        new ConcurrentHashMap<Long, PubSubData>();

    // Boolean indicating if we closed the channel this HChannelHandler is
    // attached to explicitly or not. If so, we do not need to do the
    // channel disconnected logic here.
    private volatile boolean channelClosedExplicitly = false;

    private final AbstractHChannelManager channelManager;
    private final ClientConfiguration cfg;

    private final Map<OperationType, AbstractResponseHandler> handlers;
    private final SubscribeResponseHandler subHandler;

    public HChannelHandler(ClientConfiguration cfg,
                           AbstractHChannelManager channelManager,
                           Map<OperationType, AbstractResponseHandler> handlers) {
        this.cfg = cfg;
        this.channelManager = channelManager;
        this.handlers = handlers;
        subHandler = (SubscribeResponseHandler) handlers.get(OperationType.SUBSCRIBE);
    }

    public SubscribeResponseHandler getSubscribeResponseHandler() {
        return subHandler;
    }

    public void removeTxn(long txnId) {
        txn2PubSubData.remove(txnId);
    }

    public void addTxn(long txnId, PubSubData pubSubData) {
        txn2PubSubData.put(txnId, pubSubData);
    }

    @Override
    public void messageReceived(ChannelHandlerContext ctx, MessageEvent e) throws Exception {
        // If the Message is not a PubSubResponse, just send it upstream and let
        // something else handle it.
        if (!(e.getMessage() instanceof PubSubResponse)) {
            ctx.sendUpstream(e);
            return;
        }
        // Retrieve the PubSubResponse from the Message that was sent by the
        // server.
        PubSubResponse response = (PubSubResponse) e.getMessage();
        logger.debug("Response received from host: {}, response: {}.",
                     va(NetUtils.getHostFromChannel(ctx.getChannel()), response));

        // Determine if this PubSubResponse is an ack response for a PubSub
        // Request or if it is a message being pushed to the client subscriber.
        if (response.hasMessage()) {
            // Subscribed messages being pushed to the client so handle/consume
            // it and return.
            if (null == subHandler) {
                logger.error("Received message from a non-subscription channel : {}",
                             response);
            } else {
                subHandler.handleSubscribeMessage(response);
            }
            return;
        }

        // Process Subscription Events
        if (response.hasResponseBody()) {
            ResponseBody resp = response.getResponseBody();
            // A special subscription event indicates the state of a subscriber
            if (resp.hasSubscriptionEvent()) {
                if (null == subHandler) {
                    logger.error("Received subscription event from a non-subscription channel : {}",
                                 response); 
                } else {
                    SubscriptionEventResponse eventResp = resp.getSubscriptionEvent();
                    logger.debug("Received subscription event {} for (topic:{}, subscriber:{}).",
                                 va(eventResp.getEvent(), response.getTopic(),
                                    response.getSubscriberId()));
                    subHandler.handleSubscriptionEvent(response.getTopic(),
                                                       response.getSubscriberId(),
                                                       eventResp.getEvent());
                }
                return;
            }
        }

        // Response is an ack to a prior PubSubRequest so first retrieve the
        // PubSub data for this txn.
        PubSubData pubSubData = txn2PubSubData.remove(response.getTxnId());

        // Validate that the PubSub data for this txn is stored. If not, just
        // log an error message and return since we don't know how to handle
        // this.
        if (pubSubData == null) {
            logger.error("PubSub Data was not found for PubSubResponse: {}", response);
            return;
        }

        // Store the topic2Host mapping if this wasn't a server redirect. We'll
        // assume that if the server was able to have an open Channel connection
        // to the client, and responded with an ack message other than the
        // NOT_RESPONSIBLE_FOR_TOPIC one, it is the correct topic master.
        if (!response.getStatusCode().equals(StatusCode.NOT_RESPONSIBLE_FOR_TOPIC)) {
            // Retrieve the server host that we've connected to and store the
            // mapping from the topic to this host. For all other non-redirected
            // server statuses, we consider that as a successful connection to the
            // correct topic master.
            InetSocketAddress host = NetUtils.getHostFromChannel(ctx.getChannel());
            channelManager.storeTopic2HostMapping(pubSubData.topic, host);
        }

        // Depending on the operation type, call the appropriate handler.
        logger.debug("Handling a {} response: {}, pubSubData: {}, host: {}.",
                     va(pubSubData.operationType, response, pubSubData, ctx.getChannel()));
        AbstractResponseHandler respHandler = handlers.get(pubSubData.operationType);
        if (null == respHandler) {
            // The above are the only expected PubSubResponse messages received
            // from the server for the various client side requests made.
            logger.error("Response received from server is for an unhandled operation {}, txnId: {}.",
                         va(pubSubData.operationType, response.getTxnId()));
            pubSubData.getCallback().operationFailed(pubSubData.context,
                new UnexpectedConditionException("Can't find response handler for operation "
                                                 + pubSubData.operationType));
            return;
        }
        respHandler.handleResponse(response, pubSubData, ctx.getChannel());
    }

    public void checkTimeoutRequests() {
        long curTime = System.currentTimeMillis();
        long timeoutInterval = cfg.getServerAckResponseTimeout();
        for (PubSubData pubSubData : txn2PubSubData.values()) {
            checkTimeoutRequest(pubSubData, curTime, timeoutInterval);
        }
    }

    private void checkTimeoutRequest(PubSubData pubSubData,
                                     long curTime, long timeoutInterval) {
        if (curTime > pubSubData.requestWriteTime + timeoutInterval) {
            // Current PubSubRequest has timed out so remove it from the
            // ResponseHandler's map and invoke the VoidCallback's
            // operationFailed method.
            logger.error("Current PubSubRequest has timed out for pubSubData: " + pubSubData);
            txn2PubSubData.remove(pubSubData.txnId);
            pubSubData.getCallback().operationFailed(pubSubData.context,
                new UncertainStateException("Server ack response never received so PubSubRequest has timed out!"));
        }
    }

    // Logic to deal with what happens when a Channel to a server host is
    // disconnected.
    @Override
    public void channelDisconnected(ChannelHandlerContext ctx, ChannelStateEvent e) throws Exception {
        // If this channel was closed explicitly by the client code,
        // we do not need to do any of this logic. This could happen
        // for redundant Publish channels created or redirected subscribe
        // channels that are not used anymore or when we shutdown the
        // client and manually close all of the open channels.
        // Also don't do any of the disconnect logic if the client has stopped.
        if (channelClosedExplicitly || channelManager.isClosed()) {
            return;
        }

        // Make sure the host retrieved is not null as there could be some weird
        // channel disconnect events happening during a client shutdown.
        // If it is, just return as there shouldn't be anything we need to do.
        InetSocketAddress host = NetUtils.getHostFromChannel(ctx.getChannel());
        if (host == null) {
            return;
        }

        logger.info("Channel {} was disconnected to host {}.",
                    va(ctx.getChannel(), host));

        // If this Channel was used for Publish and Unsubscribe flows, just
        // remove it from the HewdigPublisher's host2Channel map. We will
        // re-establish a Channel connection to that server when the next
        // publish/unsubscribe request to a topic that the server owns occurs.

        // Now determine what type of operation this channel was used for.
        if (null == subHandler) {
            channelManager.onNonSubscriptionChannelDisconnected(host, ctx.getChannel());
        } else {
            channelManager.onSubscriptionChannelDisconnected(host, ctx.getChannel());
        }

        // Finally, all of the PubSubRequests that are still waiting for an ack
        // response from the server need to be removed and timed out. Invoke the
        // operationFailed callbacks on all of them. Use the
        // UncertainStateException since the server did receive the request but
        // we're not sure of the state of the request since the ack response was
        // never received.
        for (PubSubData pubSubData : txn2PubSubData.values()) {
            logger.debug("Channel disconnected so invoking the operationFailed callback for pubSubData: {}",
                         pubSubData);
            pubSubData.getCallback().operationFailed(pubSubData.context, new UncertainStateException(
                                                     "Server ack response never received before server connection disconnected!"));
        }
        txn2PubSubData.clear();
    }

    // Logic to deal with what happens when a Channel to a server host is
    // connected. This is needed if the client is using an SSL port to
    // communicate with the server. If so, we need to do the SSL handshake here
    // when the channel is first connected.
    @Override
    public void channelConnected(ChannelHandlerContext ctx, ChannelStateEvent e) throws Exception {
        // No need to initiate the SSL handshake if we are closing this channel
        // explicitly or the client has been stopped.
        if (cfg.isSSLEnabled() && !channelClosedExplicitly && !channelManager.isClosed()) {
            logger.debug("Initiating the SSL handshake");
            ctx.getPipeline().get(SslHandler.class).handshake(e.getChannel());
        }
    }

    @Override
    public void exceptionCaught(ChannelHandlerContext ctx, ExceptionEvent e) {
        logger.error("Exception caught on client channel", e.getCause());
        e.getChannel().close();
    }

    public void closeExplicitly() {
        // TODO: BOOKKEEPER-350 : Handle consume buffering, etc here - in a different patch
        channelClosedExplicitly = true;
    }
}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/impl/HChannelImpl.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty.impl;

import java.net.InetSocketAddress;
import java.util.ArrayDeque;
import java.util.LinkedList;
import java.util.Queue;

import com.google.protobuf.ByteString;

import org.jboss.netty.bootstrap.ClientBootstrap;
import org.jboss.netty.channel.Channel;
import org.jboss.netty.channel.ChannelFuture;
import org.jboss.netty.channel.ChannelFutureListener;

import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.client.exceptions.NoResponseHandlerException;
import org.apache.hedwig.client.netty.HChannel;
import org.apache.hedwig.client.netty.NetUtils;
import org.apache.hedwig.exceptions.PubSubException.CouldNotConnectException;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;
import org.apache.hedwig.util.HedwigSocketAddress;
import static org.apache.hedwig.util.VarArgs.va;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Provide a wrapper over netty channel for Hedwig operations.
 */
public class HChannelImpl implements HChannel {

    private static Logger logger = LoggerFactory.getLogger(HChannelImpl.class);

    enum State {
        DISCONNECTED,
        CONNECTING,
        CONNECTED,
    };

    InetSocketAddress host;
    final AbstractHChannelManager channelManager;
    final ClientChannelPipelineFactory pipelineFactory;
    volatile Channel channel;
    volatile State state;

    // Indicates whether the channel is closed or not.
    volatile boolean closed = false;
    // Queue the pubsub requests when the channel is not connected.
    Queue<PubSubData> pendingOps = new ArrayDeque<PubSubData>();

    /**
     * Create a un-established channel with provided target <code>host</code>.
     *
     * @param host
     *          Target host address.
     * @param channelManager
     *          Channel manager manages the channels.
     */
    protected HChannelImpl(InetSocketAddress host, AbstractHChannelManager channelManager) {
        this(host, channelManager, null);
    }

    public HChannelImpl(InetSocketAddress host, AbstractHChannelManager channelManager,
                        ClientChannelPipelineFactory pipelineFactory) {
        this(host, null, channelManager, pipelineFactory);
        state = State.DISCONNECTED;
    }

    /**
     * Create a <code>HChannel</code> with an established netty channel.
     *
     * @param host
     *          Target host address.
     * @param channel
     *          Established Netty channel.
     * @param channelManager
     *          Channel manager manages the channels.
     */
    public HChannelImpl(InetSocketAddress host, Channel channel,
                        AbstractHChannelManager channelManager,
                        ClientChannelPipelineFactory pipelineFactory) {
        this.host = host;
        this.channel = channel;
        this.channelManager = channelManager;
        this.pipelineFactory = pipelineFactory;
        state = State.CONNECTED;
    }

    @Override
    public void submitOp(PubSubData pubSubData) {
        boolean doOpNow = false;

        // common case without lock first
        if (null != channel && State.CONNECTED == state) {
            doOpNow = true;
        } else {
            synchronized (this) {
                // check channel & state again under lock
                if (null != channel && State.CONNECTED == state) {
                    doOpNow = true;
                } else {
                    // if reached here, channel is either null (first connection attempt),
                    // or the channel is disconnected. Connection attempt is still in progress,
                    // queue up this op. Op will be executed when connection attempt either
                    // fails or succeeds
                    pendingOps.add(pubSubData);
                }
            }
            if (!doOpNow) {
                // start connection attempt to server
                connect();
            }
        }
        if (doOpNow) {
            executeOpAfterConnected(pubSubData); 
        }
    }

    /**
     * Execute pub/sub operation after the underlying channel is connected.
     *
     * @param pubSubData
     *          Pub/Sub Operation
     */
    private void executeOpAfterConnected(PubSubData pubSubData) {
        PubSubRequest.Builder reqBuilder =
            NetUtils.buildPubSubRequest(channelManager.nextTxnId(), pubSubData);
        writePubSubRequest(pubSubData, reqBuilder.build());
    }

    @Override
    public Channel getChannel() {
        return channel;
    }

    private void writePubSubRequest(PubSubData pubSubData, PubSubRequest pubSubRequest) {
        if (closed || null == channel || State.CONNECTED != state) {
            retryOrFailOp(pubSubData);
            return;
        }

        // Before we do the write, store this information into the
        // ResponseHandler so when the server responds, we know what
        // appropriate Callback Data to invoke for the given txn ID.
        try {
            getHChannelHandlerFromChannel(channel)
                .addTxn(pubSubData.txnId, pubSubData);
        } catch (NoResponseHandlerException nrhe) {
            logger.warn("No Channel Handler found for channel {} when writing request."
                        + " It might already disconnect.", channel);
            return;
        }

        // Finally, write the pub/sub request through the Channel.
        logger.debug("Writing a {} request to host: {} for pubSubData: {}.",
                     va(pubSubData.operationType, host, pubSubData));
        ChannelFuture future = channel.write(pubSubRequest);
        future.addListener(new WriteCallback(pubSubData, channelManager));
    }

    /**
     * Re-submit operation to default server or fail it.
     *
     * @param pubSubData
     *          Pub/Sub Operation
     */
    protected void retryOrFailOp(PubSubData pubSubData) {
        // if we were not able to connect to the host, it could be down
        ByteString hostString = ByteString.copyFromUtf8(HedwigSocketAddress.sockAddrStr(host));
        if (pubSubData.connectFailedServers != null &&
            pubSubData.connectFailedServers.contains(hostString)) {
            // We've already tried to connect to this host before so just
            // invoke the operationFailed callback.
            logger.error("Error connecting to host {} more than once so fail the request: {}",
                         va(host, pubSubData));
            pubSubData.getCallback().operationFailed(pubSubData.context,
                new CouldNotConnectException("Could not connect to host: " + host));
        } else {
            logger.error("Retry to connect to default hub server again for pubSubData: {}",
                         pubSubData);
            // Keep track of this current server that we failed to connect
            // to but retry the request on the default server host/VIP.
            if (pubSubData.connectFailedServers == null) {
                pubSubData.connectFailedServers = new LinkedList<ByteString>();
            }
            pubSubData.connectFailedServers.add(hostString);
            channelManager.submitOpToDefaultServer(pubSubData);
        }
    }

    private void onChannelConnected(ChannelFuture future) {
        Queue<PubSubData> oldPendingOps;
        synchronized (this) {
            // if the channel is closed by client, do nothing
            if (closed) {
                future.getChannel().close();
                return;
            }
            state = State.CONNECTED;
            channel = future.getChannel();
            host = NetUtils.getHostFromChannel(channel);
            oldPendingOps = pendingOps;
            pendingOps = new ArrayDeque<PubSubData>();
        }
        for (PubSubData op : oldPendingOps) {
            executeOpAfterConnected(op);
        }
    }

    private void onChannelConnectFailure() {
        Queue<PubSubData> oldPendingOps;
        synchronized (this) {
            state = State.DISCONNECTED;
            channel = null;
            oldPendingOps = pendingOps;
            pendingOps = new ArrayDeque<PubSubData>();
        }
        for (PubSubData op : oldPendingOps) {
            retryOrFailOp(op);
        }
    }

    private void connect() {
        synchronized (this) {
            if (State.CONNECTING == state ||
                State.CONNECTED == state) {
                return;
            }
            state = State.CONNECTING;
        }
        // Start the connection attempt to the input server host.
        ChannelFuture future = connect(host, pipelineFactory);
        future.addListener(new ChannelFutureListener() {

            @Override
            public void operationComplete(ChannelFuture future) throws Exception {
                // If the channel has been closed, there is no need to proceed with any
                // callback logic here.
                if (closed) {
                    future.getChannel().close();
                    return;
                }

                if (!future.isSuccess()) {
                    logger.error("Error connecting to host {}.", host);
                    future.getChannel().close();

                    // if we were not able to connect to the host, it could be down.
                    onChannelConnectFailure();
                    return;
                }
                logger.debug("Connected to server {}.", host);
                // Now that we have connected successfully to the server, execute all queueing
                // requests.
                onChannelConnected(future);
            }

        });
    }

    /**
     * This is a helper method to do the connect attempt to the server given the
     * inputted host/port. This can be used to connect to the default server
     * host/port which is the VIP. That will pick a server in the cluster at
     * random to connect to for the initial PubSub attempt (with redirect logic
     * being done at the server side). Additionally, this could be called after
     * the client makes an initial PubSub attempt at a server, and is redirected
     * to the one that is responsible for the topic. Once the connect to the
     * server is done, we will perform the corresponding PubSub write on that
     * channel.
     *
     * @param serverHost
     *            Input server host to connect to of type InetSocketAddress
     * @param pipelineFactory
     *            PipelineFactory to create response handler to handle responses from
     *            underlying channel.
     */
    protected ChannelFuture connect(InetSocketAddress serverHost,
                                    ClientChannelPipelineFactory pipelineFactory) {
        logger.debug("Connecting to host {} ...", serverHost);
        // Set up the ClientBootStrap so we can create a new Channel connection
        // to the server.
        ClientBootstrap bootstrap = new ClientBootstrap(channelManager.getChannelFactory());
        bootstrap.setPipelineFactory(pipelineFactory);
        bootstrap.setOption("tcpNoDelay", true);
        bootstrap.setOption("keepAlive", true);

        // Start the connection attempt to the input server host.
        return bootstrap.connect(serverHost);
    }

    @Override
    public void close(boolean wait) {
        synchronized (this) {
            if (closed) {
                return;
            }
            closed = true;
        }
        if (null == channel) {
            return;
        }
        try {
            getHChannelHandlerFromChannel(channel).closeExplicitly();
        } catch (NoResponseHandlerException nrhe) {
            logger.warn("No channel handler found for channel {} when closing it.",
                        channel);
        }
        if (wait) {
            channel.close().awaitUninterruptibly();
        } else {
            channel.close();
        }
        channel = null;
    }

    @Override
    public String toString() {
        StringBuilder sb = new StringBuilder();
        sb.append("[HChannel: host - ").append(host)
          .append(", channel - ").append(channel)
          .append(", pending reqs - ").append(pendingOps.size())
          .append(", closed - ").append(closed).append("]");
        return sb.toString();
    }

    @Override
    public void close() {
        close(false);
    }

    /**
     * Helper static method to get the ResponseHandler instance from a Channel
     * via the ChannelPipeline it is associated with. The assumption is that the
     * last ChannelHandler tied to the ChannelPipeline is the ResponseHandler.
     *
     * @param channel
     *            Channel we are retrieving the ResponseHandler instance for
     * @return ResponseHandler Instance tied to the Channel's Pipeline
     */
    public static HChannelHandler getHChannelHandlerFromChannel(Channel channel)
    throws NoResponseHandlerException {
        if (null == channel) {
            throw new NoResponseHandlerException("Received a null value for the channel. Cannot retrieve the response handler");
        }

        HChannelHandler handler = (HChannelHandler) channel.getPipeline().getLast();
        if (null == handler) {
            throw new NoResponseHandlerException("Could not retrieve the response handler from the channel's pipeline.");
        }
        return handler;
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/impl/NonSubscriptionChannelPipelineFactory.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty.impl;

import java.util.HashMap;
import java.util.Map;

import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.handlers.AbstractResponseHandler;
import org.apache.hedwig.client.handlers.PublishResponseHandler;
import org.apache.hedwig.client.handlers.UnsubscribeResponseHandler;
import org.apache.hedwig.protocol.PubSubProtocol.OperationType;

public class NonSubscriptionChannelPipelineFactory extends ClientChannelPipelineFactory {

    public NonSubscriptionChannelPipelineFactory(ClientConfiguration cfg,
                                                 AbstractHChannelManager channelManager) {
        super(cfg, channelManager);
    }

    @Override
    protected Map<OperationType, AbstractResponseHandler> createResponseHandlers() {
        Map<OperationType, AbstractResponseHandler> handlers =
            new HashMap<OperationType, AbstractResponseHandler>();
        handlers.put(OperationType.PUBLISH,
                     new PublishResponseHandler(cfg, channelManager));
        handlers.put(OperationType.UNSUBSCRIBE,
                     new UnsubscribeResponseHandler(cfg, channelManager));
        return handlers;
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/impl/ResubscribeCallback.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty.impl;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.client.data.TopicSubscriber;
import org.apache.hedwig.client.exceptions.AlreadyStartDeliveryException;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.ClientNotSubscribedException;
import org.apache.hedwig.exceptions.PubSubException.ResubscribeException;
import org.apache.hedwig.protocol.PubSubProtocol.ResponseBody;
import org.apache.hedwig.util.Callback;
import static org.apache.hedwig.util.VarArgs.va;

/**
 * This class is used when a Subscribe channel gets disconnected and we attempt
 * to resubmit subscribe requests existed in that channel. Once the resubscribe
 * the topic is completed, we need to restart delivery for that topic.
 */
class ResubscribeCallback implements Callback<ResponseBody> {

    private static Logger logger = LoggerFactory.getLogger(ResubscribeCallback.class);

    // Private member variables
    private final TopicSubscriber origTopicSubscriber;
    private final PubSubData origSubData;
    private final AbstractHChannelManager channelManager;
    private final long retryWaitTime;

    // Constructor
    ResubscribeCallback(TopicSubscriber origTopicSubscriber,
                        PubSubData origSubData,
                        AbstractHChannelManager channelManager,
                        long retryWaitTime) {
        this.origTopicSubscriber = origTopicSubscriber;
        this.origSubData = origSubData;
        this.channelManager = channelManager;
        this.retryWaitTime = retryWaitTime;
    }

    @Override
    public void operationFinished(Object ctx, ResponseBody resultOfOperation) {
        if (logger.isDebugEnabled())
            logger.debug("Resubscribe succeeded for origSubData: " + origSubData);
        // Now we want to restart delivery for the subscription channel only
        // if delivery was started at the time the original subscribe channel
        // was disconnected.
        try {
            channelManager.restartDelivery(origTopicSubscriber);
        } catch (ClientNotSubscribedException e) {
            // This exception should never be thrown here but just in case,
            // log an error and just keep retrying the subscribe request.
            logger.error("Subscribe was successful but error starting delivery for {} : {}",
                         va(origTopicSubscriber, e.getMessage()));
            retrySubscribeRequest();
        } catch (AlreadyStartDeliveryException asde) {
            // should not reach here
        }
    }

    @Override
    public void operationFailed(Object ctx, PubSubException exception) {
        if (exception instanceof ResubscribeException) {
            // it might be caused by closesub when resubscribing.
            // so we don't need to retry resubscribe again
            logger.warn("Failed to resubscribe {} : but it is caused by closesub when resubscribing. "
                        + "so we don't need to retry subscribe again.", origSubData);
        }
        // If the resubscribe fails, just keep retrying the subscribe
        // request. There isn't a way to flag to the application layer that
        // a topic subscription has failed. So instead, we'll just keep
        // retrying in the background until success.
        logger.error("Resubscribe failed with error: " + exception.getMessage());
        // we don't retry subscribe request is channel manager is closing
        // otherwise it might overflow the stack.
        if (!channelManager.isClosed()) {
            retrySubscribeRequest();
        }
    }

    private void retrySubscribeRequest() {
        if (channelManager.isClosed()) {
            return;
        }
        origSubData.clearServersList();
        logger.debug("Resubmit subscribe request for {} in {} ms later.",
                     va(origTopicSubscriber, retryWaitTime));
        channelManager.submitOpAfterDelay(origSubData, retryWaitTime);
    }
}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/impl/WriteCallback.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty.impl;

import java.net.InetSocketAddress;
import java.util.LinkedList;

import org.apache.hedwig.client.exceptions.NoResponseHandlerException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.ChannelFuture;
import org.jboss.netty.channel.ChannelFutureListener;

import com.google.protobuf.ByteString;
import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.client.netty.HChannelManager;
import org.apache.hedwig.client.netty.NetUtils;
import org.apache.hedwig.exceptions.PubSubException.ServiceDownException;
import org.apache.hedwig.util.HedwigSocketAddress;

public class WriteCallback implements ChannelFutureListener {

    private static Logger logger = LoggerFactory.getLogger(WriteCallback.class);

    // Private member variables
    private PubSubData pubSubData;
    private final HChannelManager channelManager;

    // Constructor
    public WriteCallback(PubSubData pubSubData,
                         HChannelManager channelManager) {
        super();
        this.pubSubData = pubSubData;
        this.channelManager = channelManager;
    }

    public void operationComplete(ChannelFuture future) throws Exception {
        // If the client has stopped, there is no need to proceed
        // with any callback logic here.
        if (channelManager.isClosed()) {
            future.getChannel().close();
            return;
        }

        // When the write operation to the server is done, we just need to check
        // if it was successful or not.
        InetSocketAddress host = NetUtils.getHostFromChannel(future.getChannel());
        if (!future.isSuccess()) {
            logger.error("Error writing on channel to host: {}", host);
            // On a write failure for a PubSubRequest, we also want to remove
            // the saved txnId to PubSubData in the ResponseHandler. These
            // requests will not receive an ack response from the server
            // so there is no point storing that information there anymore.
            try {
                HChannelHandler channelHandler = 
                    HChannelImpl.getHChannelHandlerFromChannel(future.getChannel());
                channelHandler.removeTxn(pubSubData.txnId);
                channelHandler.closeExplicitly();
            } catch (NoResponseHandlerException e) {
                // We just couldn't remove the transaction ID's mapping.
                // The handler was null, so this has been reset anyway.
                logger.warn("Could not find response handler to remove txnId mapping to pubsub data. Ignoring.");
            }

            future.getChannel().close();

            // If we were not able to write on the channel to the server host,
            // the host could have died or something is wrong with the channel
            // connection where we can connect to the host, but not write to it.
            ByteString hostString = (host == null) ? null : ByteString.copyFromUtf8(HedwigSocketAddress.sockAddrStr(host));
            if (pubSubData.writeFailedServers != null && pubSubData.writeFailedServers.contains(hostString)) {
                // We've already tried to write to this server previously and
                // failed, so invoke the operationFailed callback.
                logger.error("Error writing to host more than once so just invoke the operationFailed callback!");
                pubSubData.getCallback().operationFailed(pubSubData.context, new ServiceDownException(
                                                        "Error while writing message to server: " + hostString));
            } else {
                logger.debug("Try to send the PubSubRequest again to the default server host/VIP for pubSubData: {}",
                    pubSubData);
                // Keep track of this current server that we failed to write to
                // but retry the request on the default server host/VIP.
                if (pubSubData.writeFailedServers == null)
                    pubSubData.writeFailedServers = new LinkedList<ByteString>();
                pubSubData.writeFailedServers.add(hostString);
                channelManager.submitOpToDefaultServer(pubSubData);
            }
        } else {
            // Now that the write to the server is done, we have to wait for it
            // to respond. The ResponseHandler will take care of the ack
            // response from the server before we can determine if the async
            // PubSub call has really completed successfully or not.
            logger.debug("Successfully wrote to host: {} for pubSubData: {}", host, pubSubData);
        }
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/impl/multiplex/MultiplexHChannelManager.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty.impl.multiplex;

import java.net.InetSocketAddress;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ConcurrentMap;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;
import org.jboss.netty.channel.ChannelFactory;
import org.jboss.netty.channel.ChannelFuture;
import org.jboss.netty.channel.ChannelFutureListener;

import org.apache.hedwig.client.api.MessageHandler;
import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.client.data.TopicSubscriber;
import org.apache.hedwig.client.exceptions.AlreadyStartDeliveryException;
import org.apache.hedwig.client.exceptions.NoResponseHandlerException;
import org.apache.hedwig.client.handlers.SubscribeResponseHandler;
import org.apache.hedwig.client.netty.CleanupChannelMap;
import org.apache.hedwig.client.netty.HChannel;
import org.apache.hedwig.client.netty.NetUtils;
import org.apache.hedwig.client.netty.impl.AbstractHChannelManager;
import org.apache.hedwig.client.netty.impl.ClientChannelPipelineFactory;
import org.apache.hedwig.client.netty.impl.HChannelHandler;
import org.apache.hedwig.client.netty.impl.HChannelImpl;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.ClientNotSubscribedException;
import org.apache.hedwig.exceptions.PubSubException.ServiceDownException;
import org.apache.hedwig.protocol.PubSubProtocol.OperationType;
import org.apache.hedwig.protocol.PubSubProtocol.ResponseBody;
import org.apache.hedwig.util.Callback;
import org.apache.hedwig.util.Either;
import static org.apache.hedwig.util.VarArgs.va;


/**
 * Multiplex HChannel Manager which establish a connection for multi subscriptions.
 */
public class MultiplexHChannelManager extends AbstractHChannelManager {

    static final Logger logger = LoggerFactory.getLogger(MultiplexHChannelManager.class);

    // Find which HChannel that a given TopicSubscriber used.
    protected final CleanupChannelMap<InetSocketAddress> subscriptionChannels;

    // A index map for each topic subscriber is served by which subscription channel
    protected final CleanupChannelMap<TopicSubscriber> sub2Channels;

    // Concurrent Map to store Message handler for each topic + sub id combination.
    // Store it here instead of in SubscriberResponseHandler as we don't want to lose the handler
    // user set when connection is recovered
    protected final ConcurrentMap<TopicSubscriber, MessageHandler> topicSubscriber2MessageHandler
        = new ConcurrentHashMap<TopicSubscriber, MessageHandler>();

    // PipelineFactory to create subscription netty channels to the appropriate server
    private final ClientChannelPipelineFactory subscriptionChannelPipelineFactory;

    public MultiplexHChannelManager(ClientConfiguration cfg,
                                    ChannelFactory socketFactory) {
        super(cfg, socketFactory);
        subscriptionChannels = new CleanupChannelMap<InetSocketAddress>();
        sub2Channels = new CleanupChannelMap<TopicSubscriber>();
        subscriptionChannelPipelineFactory =
            new MultiplexSubscriptionChannelPipelineFactory(cfg, this);
    }

    @Override
    protected ClientChannelPipelineFactory getSubscriptionChannelPipelineFactory() {
        return subscriptionChannelPipelineFactory;
    }

    @Override
    protected HChannel createAndStoreSubscriptionChannel(Channel channel) {
        // store the channel connected to target host for future usage
        InetSocketAddress host = NetUtils.getHostFromChannel(channel);
        HChannel newHChannel = new HChannelImpl(host, channel, this,
                                                getSubscriptionChannelPipelineFactory());
        return storeSubscriptionChannel(host, newHChannel);
    }

    @Override
    protected HChannel createAndStoreSubscriptionChannel(InetSocketAddress host) {
        HChannel newHChannel = new HChannelImpl(host, this,
                                                getSubscriptionChannelPipelineFactory());
        return storeSubscriptionChannel(host, newHChannel);
    }

    private HChannel storeSubscriptionChannel(InetSocketAddress host,
                                              HChannel newHChannel) {
        // here, we guarantee there is only one channel used to communicate with target
        // host.
        return subscriptionChannels.addChannel(host, newHChannel);
    }

    @Override
    protected HChannel getSubscriptionChannel(InetSocketAddress host) {
        return subscriptionChannels.getChannel(host);
    }

    protected HChannel getSubscriptionChannel(TopicSubscriber subscriber) {
        InetSocketAddress host = topic2Host.get(subscriber.getTopic());
        if (null == host) {
            // we don't know where is the owner of the topic
            return null;
        } else {
            return getSubscriptionChannel(host);
        }
    }

    @Override
    protected HChannel getSubscriptionChannelByTopicSubscriber(TopicSubscriber subscriber) {
        InetSocketAddress host = topic2Host.get(subscriber.getTopic());
        if (null == host) {
            // we don't know where is the topic
            return null;
        } else {
            // we had know which server owned the topic
            HChannel channel = getSubscriptionChannel(host);
            if (null == channel) {
                // create a channel to connect to sepcified host
                channel = createAndStoreSubscriptionChannel(host);
            }
            return channel;
        }
    }

    @Override
    protected void onSubscriptionChannelDisconnected(InetSocketAddress host,
                                                     Channel channel) {
        HChannel hChannel = subscriptionChannels.getChannel(host);
        if (null == hChannel) {
            return;
        }
        Channel underlyingChannel = hChannel.getChannel();
        if (null == underlyingChannel ||
            !underlyingChannel.equals(channel)) {
            return;
        }
        logger.info("Subscription Channel {} disconnected from {}.",
                    va(channel, host));
        // remove existed channel
        if (subscriptionChannels.removeChannel(host, hChannel)) {
            try {
                HChannelHandler channelHandler =
                    HChannelImpl.getHChannelHandlerFromChannel(channel);
                channelHandler.getSubscribeResponseHandler()
                              .onChannelDisconnected(host, channel);
            } catch (NoResponseHandlerException nrhe) {
                logger.warn("No Channel Handler found for channel {} when it disconnected.",
                            channel);
            }
        }
    }

    @Override
    public SubscribeResponseHandler getSubscribeResponseHandler(TopicSubscriber topicSubscriber) {
        HChannel hChannel = getSubscriptionChannel(topicSubscriber);
        if (null == hChannel) {
            return null;
        }
        Channel channel = hChannel.getChannel();
        if (null == channel) {
            return null;
        }
        try {
            HChannelHandler channelHandler =
                HChannelImpl.getHChannelHandlerFromChannel(channel);
            return channelHandler.getSubscribeResponseHandler();
        } catch (NoResponseHandlerException nrhe) {
            logger.warn("No Channel Handler found for channel {}, topic subscriber {}.",
                        channel, topicSubscriber);
            return null;
        }
    }

    @Override
    public void startDelivery(TopicSubscriber topicSubscriber,
                              MessageHandler messageHandler)
        throws ClientNotSubscribedException, AlreadyStartDeliveryException {
        startDelivery(topicSubscriber, messageHandler, false);
    }

    @Override
    protected void restartDelivery(TopicSubscriber topicSubscriber)
        throws ClientNotSubscribedException, AlreadyStartDeliveryException {
        startDelivery(topicSubscriber, null, true);
    }

    private void startDelivery(TopicSubscriber topicSubscriber,
                               MessageHandler messageHandler,
                               boolean restart)
        throws ClientNotSubscribedException, AlreadyStartDeliveryException {
        // Make sure we know about this topic subscription on the client side
        // exists. The assumption is that the client should have in memory the
        // Channel created for the TopicSubscriber once the server has sent
        // an ack response to the initial subscribe request.
        SubscribeResponseHandler subscribeResponseHandler =
            getSubscribeResponseHandler(topicSubscriber);
        if (null == subscribeResponseHandler ||
            !subscribeResponseHandler.hasSubscription(topicSubscriber)) {
            logger.error("Client is not yet subscribed to {}.", topicSubscriber);
            throw new ClientNotSubscribedException("Client is not yet subscribed to "
                                                   + topicSubscriber);
        }

        MessageHandler existedMsgHandler = topicSubscriber2MessageHandler.get(topicSubscriber);
        if (restart) {
            // restart using existing msg handler 
            messageHandler = existedMsgHandler;
        } else {
            // some has started delivery but not stop it
            if (null != existedMsgHandler) {
                throw new AlreadyStartDeliveryException("A message handler has been started for topic subscriber " + topicSubscriber);
            }
            if (messageHandler != null) {
                if (null != topicSubscriber2MessageHandler.putIfAbsent(topicSubscriber, messageHandler)) {
                    throw new AlreadyStartDeliveryException("Someone is also starting delivery for topic subscriber " + topicSubscriber);
                }
            }
        }

        // tell subscribe response handler to start delivering messages for topicSubscriber
        subscribeResponseHandler.startDelivery(topicSubscriber, messageHandler);
    }

    public void stopDelivery(TopicSubscriber topicSubscriber)
    throws ClientNotSubscribedException {
        // Make sure we know that this topic subscription on the client side
        // exists. The assumption is that the client should have in memory the
        // Channel created for the TopicSubscriber once the server has sent
        // an ack response to the initial subscribe request.
        SubscribeResponseHandler subscribeResponseHandler =
            getSubscribeResponseHandler(topicSubscriber);
        if (null == subscribeResponseHandler ||
            !subscribeResponseHandler.hasSubscription(topicSubscriber)) {
            logger.error("Client is not yet subscribed to {}.", topicSubscriber);
            throw new ClientNotSubscribedException("Client is not yet subscribed to "
                                                   + topicSubscriber);
        }

        // tell subscribe response handler to stop delivering messages for a given topic subscriber
        topicSubscriber2MessageHandler.remove(topicSubscriber);
        subscribeResponseHandler.stopDelivery(topicSubscriber);
    }
                            

    @Override
    public void asyncCloseSubscription(final TopicSubscriber topicSubscriber,
                                       final Callback<ResponseBody> callback,
                                       final Object context) {
        SubscribeResponseHandler subscribeResponseHandler =
            getSubscribeResponseHandler(topicSubscriber);
        if (null == subscribeResponseHandler ||
            !subscribeResponseHandler.hasSubscription(topicSubscriber)) {
            logger.warn("Trying to close a subscription when we don't have a subscription channel cached for {}",
                        topicSubscriber);
            callback.operationFinished(context, (ResponseBody)null);
            return;
        }
        subscribeResponseHandler.asyncCloseSubscription(topicSubscriber, callback, context);
    }

    @Override
    protected void checkTimeoutRequestsOnSubscriptionChannels() {
        // timeout task may be started before constructing subscriptionChannels
        if (null == subscriptionChannels) {
            return;
        }
        for (HChannel channel : subscriptionChannels.getChannels()) {
            try {
                HChannelHandler channelHandler =
                    HChannelImpl.getHChannelHandlerFromChannel(channel.getChannel());
                channelHandler.checkTimeoutRequests();
            } catch (NoResponseHandlerException nrhe) {
                continue;
            }
        }
    }

    @Override
    protected void closeSubscriptionChannels() {
        subscriptionChannels.close();
    }

    protected Either<Boolean, HChannel> storeSubscriptionChannel(
        TopicSubscriber topicSubscriber, PubSubData txn, HChannel channel) {
        boolean replaced = sub2Channels.replaceChannel(
            topicSubscriber, txn.getOriginalChannelForResubscribe(), channel);
        if (replaced) {
            return Either.of(replaced, channel);
        } else {
            return Either.of(replaced, null);
        }
    }

    protected boolean removeSubscriptionChannel(
        TopicSubscriber topicSubscriber, HChannel channel) {
        return sub2Channels.removeChannel(topicSubscriber, channel);
    }
}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/impl/multiplex/MultiplexSubscribeResponseHandler.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty.impl.multiplex;

import java.net.InetSocketAddress;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;

import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.client.data.TopicSubscriber;
import org.apache.hedwig.client.netty.HChannelManager;
import org.apache.hedwig.client.netty.HChannel;
import org.apache.hedwig.client.netty.NetUtils;
import org.apache.hedwig.client.netty.impl.AbstractSubscribeResponseHandler;
import org.apache.hedwig.client.netty.impl.ActiveSubscriber;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.UnexpectedConditionException;
import org.apache.hedwig.protocol.PubSubProtocol.OperationType;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse;
import org.apache.hedwig.protocol.PubSubProtocol.ResponseBody;
import org.apache.hedwig.protocol.PubSubProtocol.StatusCode;
import org.apache.hedwig.util.Callback;
import org.apache.hedwig.util.Either;
import static org.apache.hedwig.util.VarArgs.va;

public class MultiplexSubscribeResponseHandler extends AbstractSubscribeResponseHandler {

    private static Logger logger =
        LoggerFactory.getLogger(MultiplexSubscribeResponseHandler.class);

    // the underlying subscription channel
    volatile HChannel hChannel;
    private final MultiplexHChannelManager sChannelManager;

    protected MultiplexSubscribeResponseHandler(ClientConfiguration cfg,
                                                HChannelManager channelManager) {
        super(cfg, channelManager);
        sChannelManager = (MultiplexHChannelManager) channelManager;
    }

    @Override
    public void handleResponse(PubSubResponse response, PubSubData pubSubData,
                               Channel channel) throws Exception {
        if (null == hChannel) {
            InetSocketAddress host = NetUtils.getHostFromChannel(channel);
            hChannel = sChannelManager.getSubscriptionChannel(host);
            if (null == hChannel ||
                !channel.equals(hChannel.getChannel())) {
                PubSubException pse =
                    new UnexpectedConditionException("Failed to get subscription channel of " + host);
                pubSubData.getCallback().operationFailed(pubSubData.context, pse);
                return;
            }
        }
        super.handleResponse(response, pubSubData, channel);
    }

    @Override
    protected Either<StatusCode, HChannel> handleSuccessResponse(
        TopicSubscriber ts, PubSubData pubSubData, Channel channel) {
        // Store the mapping for the TopicSubscriber to the Channel.
        // This is so we can control the starting and stopping of
        // message deliveries from the server on that Channel. Store
        // this only on a successful ack response from the server.
        Either<Boolean, HChannel> result =
            sChannelManager.storeSubscriptionChannel(ts, pubSubData, hChannel);
        if (result.left()) {
            return Either.of(StatusCode.SUCCESS, result.right());
        } else {
            StatusCode code;
            if (pubSubData.isResubscribeRequest()) {
                code = StatusCode.RESUBSCRIBE_EXCEPTION;
            } else {
                code = StatusCode.CLIENT_ALREADY_SUBSCRIBED;
            }
            return Either.of(code, null);
        }
    }

    @Override
    public void asyncCloseSubscription(final TopicSubscriber topicSubscriber,
                                       final Callback<ResponseBody> callback,
                                       final Object context) {
        final ActiveSubscriber ss = getActiveSubscriber(topicSubscriber);
        if (null == ss || null == hChannel) {
            logger.debug("No subscription {} found when closing its subscription from {}.",
                         va(topicSubscriber, hChannel));
            callback.operationFinished(context, (ResponseBody)null);
            return;
        }
        Callback<ResponseBody> closeCb = new Callback<ResponseBody>() {
            @Override
            public void operationFinished(Object ctx, ResponseBody respBody) {
                removeSubscription(topicSubscriber, ss);
                sChannelManager.removeSubscriptionChannel(topicSubscriber, hChannel);
                callback.operationFinished(context, null);
            }

            @Override
            public void operationFailed(Object ctx, PubSubException exception) {
                callback.operationFailed(context, exception);
            }
        };
        PubSubData closeOp = new PubSubData(topicSubscriber.getTopic(), null,
                                            topicSubscriber.getSubscriberId(),
                                            OperationType.CLOSESUBSCRIPTION,
                                            null, closeCb, context);
        hChannel.submitOp(closeOp);
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/impl/multiplex/MultiplexSubscriptionChannelPipelineFactory.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty.impl.multiplex;

import java.util.HashMap;
import java.util.Map;

import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.handlers.AbstractResponseHandler;
import org.apache.hedwig.client.handlers.CloseSubscriptionResponseHandler;
import org.apache.hedwig.client.netty.impl.AbstractHChannelManager;
import org.apache.hedwig.client.netty.impl.ClientChannelPipelineFactory;
import org.apache.hedwig.client.netty.impl.HChannelHandler;
import org.apache.hedwig.protocol.PubSubProtocol.OperationType;

public class MultiplexSubscriptionChannelPipelineFactory extends ClientChannelPipelineFactory {

    public MultiplexSubscriptionChannelPipelineFactory(ClientConfiguration cfg,
                                                       MultiplexHChannelManager channelManager) {
        super(cfg, channelManager);
    }

    @Override
    protected Map<OperationType, AbstractResponseHandler> createResponseHandlers() {
        Map<OperationType, AbstractResponseHandler> handlers =
            new HashMap<OperationType, AbstractResponseHandler>();
        handlers.put(OperationType.SUBSCRIBE,
                     new MultiplexSubscribeResponseHandler(cfg, channelManager));
        handlers.put(OperationType.CLOSESUBSCRIPTION,
                     new CloseSubscriptionResponseHandler(cfg, channelManager));
        return handlers;
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/impl/simple/SimpleHChannelManager.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty.impl.simple;

import java.net.InetSocketAddress;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ConcurrentMap;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;
import org.jboss.netty.channel.ChannelFactory;
import org.jboss.netty.channel.ChannelFuture;
import org.jboss.netty.channel.ChannelFutureListener;

import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.client.api.MessageHandler;
import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.data.TopicSubscriber;
import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.client.exceptions.AlreadyStartDeliveryException;
import org.apache.hedwig.client.exceptions.NoResponseHandlerException;
import org.apache.hedwig.client.handlers.SubscribeResponseHandler;
import org.apache.hedwig.client.netty.CleanupChannelMap;
import org.apache.hedwig.client.netty.HChannel;
import org.apache.hedwig.client.netty.NetUtils;
import org.apache.hedwig.client.netty.impl.AbstractHChannelManager;
import org.apache.hedwig.client.netty.impl.ClientChannelPipelineFactory;
import org.apache.hedwig.client.netty.impl.HChannelHandler;
import org.apache.hedwig.client.netty.impl.HChannelImpl;
import org.apache.hedwig.exceptions.PubSubException.ClientNotSubscribedException;
import org.apache.hedwig.exceptions.PubSubException.ServiceDownException;
import org.apache.hedwig.exceptions.PubSubException.TopicBusyException;
import org.apache.hedwig.protocol.PubSubProtocol.ResponseBody;
import org.apache.hedwig.protocol.PubSubProtocol.OperationType;
import org.apache.hedwig.util.Callback;
import org.apache.hedwig.util.Either;
import static org.apache.hedwig.util.VarArgs.va;

/**
 * Simple HChannel Manager which establish a connection for each subscription.
 */
public class SimpleHChannelManager extends AbstractHChannelManager {

    private static Logger logger = LoggerFactory.getLogger(SimpleHChannelManager.class);

    // Concurrent Map to store the cached Channel connections on the client side
    // to a server host for a given Topic + SubscriberId combination. For each
    // TopicSubscriber, we want a unique Channel connection to the server for
    // it. We can also get the ResponseHandler tied to the Channel via the
    // Channel Pipeline.
    protected final CleanupChannelMap<TopicSubscriber> topicSubscriber2Channel;

    // Concurrent Map to store Message handler for each topic + sub id combination.
    // Store it here instead of in SubscriberResponseHandler as we don't want to lose the handler
    // user set when connection is recovered
    protected final ConcurrentMap<TopicSubscriber, MessageHandler> topicSubscriber2MessageHandler
        = new ConcurrentHashMap<TopicSubscriber, MessageHandler>();

    // PipelineFactory to create subscription netty channels to the appropriate server
    private final ClientChannelPipelineFactory subscriptionChannelPipelineFactory;

    public SimpleHChannelManager(ClientConfiguration cfg,
                                 ChannelFactory socketFactory) {
        super(cfg, socketFactory);
        topicSubscriber2Channel = new CleanupChannelMap<TopicSubscriber>();
        this.subscriptionChannelPipelineFactory =
            new SimpleSubscriptionChannelPipelineFactory(cfg, this);
    }

    @Override
    public void submitOp(final PubSubData pubSubData) {
        /**
         * In the simple hchannel implementation that if a client closes a subscription
         * and tries to attach to it immediately, it could get a TOPIC_BUSY response. This
         * is because, a subscription is closed simply by closing the channel, and the hub
         * side may not have been notified of the channel disconnection event by the time
         * the new subscription request comes in. To solve this, retry up to 5 times.
         * {@link https://issues.apache.org/jira/browse/BOOKKEEPER-513}
         */
        if (OperationType.SUBSCRIBE.equals(pubSubData.operationType)) {
            final Callback<ResponseBody> origCb = pubSubData.getCallback();
            final AtomicInteger retries = new AtomicInteger(5);
            final Callback<ResponseBody> wrapperCb
                = new Callback<ResponseBody>() {
                @Override
                public void operationFinished(Object ctx,
                                              ResponseBody resultOfOperation) {
                    origCb.operationFinished(ctx, resultOfOperation);
                }

                @Override
                public void operationFailed(Object ctx, PubSubException exception) {
                    if (exception instanceof ServiceDownException
                        && exception.getCause() instanceof TopicBusyException
                        && retries.decrementAndGet() > 0) {
                        logger.warn("TOPIC_DOWN from server using simple channel scheme."
                                    + "This could be due to the channel disconnection from a close"
                                    + " not having been triggered on the server side. Retrying");
                        SimpleHChannelManager.super.submitOp(pubSubData);
                        return;
                    }
                    origCb.operationFailed(ctx, exception);
                }
            };
            pubSubData.setCallback(wrapperCb);
        }
        super.submitOp(pubSubData);
    }

    @Override
    protected ClientChannelPipelineFactory getSubscriptionChannelPipelineFactory() {
        return subscriptionChannelPipelineFactory;
    }

    @Override
    protected HChannel createAndStoreSubscriptionChannel(Channel channel) {
        // for simple channel, we don't store subscription channel now
        // we store it until we received success response
        InetSocketAddress host = NetUtils.getHostFromChannel(channel);
        return new HChannelImpl(host, channel, this,
                                getSubscriptionChannelPipelineFactory());
    }

    @Override
    protected HChannel createAndStoreSubscriptionChannel(InetSocketAddress host) {
        // for simple channel, we don't store subscription channel now
        // we store it until we received success response
        return new HChannelImpl(host, this,
                                getSubscriptionChannelPipelineFactory());
    }

    protected Either<Boolean, HChannel> storeSubscriptionChannel(
        TopicSubscriber topicSubscriber, PubSubData txn, Channel channel) {
        InetSocketAddress host = NetUtils.getHostFromChannel(channel);
        HChannel newHChannel = new HChannelImpl(host, channel, this,
                                                getSubscriptionChannelPipelineFactory());
        boolean replaced = topicSubscriber2Channel.replaceChannel(
            topicSubscriber, txn.getOriginalChannelForResubscribe(), newHChannel);
        if (replaced) {
            return Either.of(replaced, newHChannel);
        } else {
            return Either.of(replaced, null);
        }
    }

    @Override
    protected HChannel getSubscriptionChannel(InetSocketAddress host) {
        return null;
    }

    @Override
    protected HChannel getSubscriptionChannelByTopicSubscriber(TopicSubscriber subscriber) {
        HChannel channel = topicSubscriber2Channel.getChannel(subscriber);
        if (null != channel) {
            // there is no channel established for this subscription
            return channel;
        } else {
            InetSocketAddress host = topic2Host.get(subscriber.getTopic());
            if (null == host) {
                return null;
            } else {
                channel = getSubscriptionChannel(host);
                if (null == channel) {
                    channel = createAndStoreSubscriptionChannel(host);
                }
                return channel;
            }
        }
    }

    @Override
    protected void onSubscriptionChannelDisconnected(InetSocketAddress host,
                                                     Channel channel) {
        logger.info("Subscription Channel {} disconnected from {}.",
                    va(channel, host));
        try {
            // get hchannel handler
            HChannelHandler channelHandler =
                HChannelImpl.getHChannelHandlerFromChannel(channel);
            channelHandler.getSubscribeResponseHandler()
                          .onChannelDisconnected(host, channel);
        } catch (NoResponseHandlerException nrhe) {
            logger.warn("No Channel Handler found for channel {} when it disconnected.",
                        channel);
        }
    }

    @Override
    public SubscribeResponseHandler getSubscribeResponseHandler(TopicSubscriber topicSubscriber) {
        HChannel hChannel = topicSubscriber2Channel.getChannel(topicSubscriber);
        if (null == hChannel) {
            return null;
        }
        Channel channel = hChannel.getChannel();
        if (null == channel) {
            return null;
        }
        try {
            HChannelHandler channelHandler =
                HChannelImpl.getHChannelHandlerFromChannel(channel);
            return channelHandler.getSubscribeResponseHandler();
        } catch (NoResponseHandlerException nrhe) {
            logger.warn("No Channel Handler found for channel {}, topic subscriber {}.",
                        channel, topicSubscriber);
            return null;
        }

    }

    @Override
    public void startDelivery(TopicSubscriber topicSubscriber,
                              MessageHandler messageHandler)
        throws ClientNotSubscribedException, AlreadyStartDeliveryException {
        startDelivery(topicSubscriber, messageHandler, false);
    }

    @Override
    protected void restartDelivery(TopicSubscriber topicSubscriber)
        throws ClientNotSubscribedException, AlreadyStartDeliveryException {
        startDelivery(topicSubscriber, null, true);
    }

    private void startDelivery(TopicSubscriber topicSubscriber,
                               MessageHandler messageHandler, boolean restart)
        throws ClientNotSubscribedException, AlreadyStartDeliveryException {
        // Make sure we know about this topic subscription on the client side
        // exists. The assumption is that the client should have in memory the
        // Channel created for the TopicSubscriber once the server has sent
        // an ack response to the initial subscribe request.
        SubscribeResponseHandler subscribeResponseHandler =
            getSubscribeResponseHandler(topicSubscriber);
        if (null == subscribeResponseHandler ||
            !subscribeResponseHandler.hasSubscription(topicSubscriber)) {
            logger.error("Client is not yet subscribed to {}.", topicSubscriber);
            throw new ClientNotSubscribedException("Client is not yet subscribed to "
                                                   + topicSubscriber);
        }

        MessageHandler existedMsgHandler = topicSubscriber2MessageHandler.get(topicSubscriber);
        if (restart) {
            // restart using existing msg handler 
            messageHandler = existedMsgHandler;
        } else {
            // some has started delivery but not stop it
            if (null != existedMsgHandler) {
                throw new AlreadyStartDeliveryException("A message handler has been started for topic subscriber " + topicSubscriber);
            }
            if (messageHandler != null) {
                if (null != topicSubscriber2MessageHandler.putIfAbsent(topicSubscriber, messageHandler)) {
                    throw new AlreadyStartDeliveryException("Someone is also starting delivery for topic subscriber " + topicSubscriber);
                }
            }
        }

        // tell subscribe response handler to start delivering messages for topicSubscriber
        subscribeResponseHandler.startDelivery(topicSubscriber, messageHandler);
    }

    public void stopDelivery(TopicSubscriber topicSubscriber)
    throws ClientNotSubscribedException {
        // Make sure we know that this topic subscription on the client side
        // exists. The assumption is that the client should have in memory the
        // Channel created for the TopicSubscriber once the server has sent
        // an ack response to the initial subscribe request.
        SubscribeResponseHandler subscribeResponseHandler =
            getSubscribeResponseHandler(topicSubscriber);
        if (null == subscribeResponseHandler ||
            !subscribeResponseHandler.hasSubscription(topicSubscriber)) {
            logger.error("Client is not yet subscribed to {}.", topicSubscriber);
            throw new ClientNotSubscribedException("Client is not yet subscribed to "
                                                   + topicSubscriber);
        }

        // tell subscribe response handler to stop delivering messages for a given topic subscriber
        topicSubscriber2MessageHandler.remove(topicSubscriber);
        subscribeResponseHandler.stopDelivery(topicSubscriber);
    }
                            

    @Override
    public void asyncCloseSubscription(final TopicSubscriber topicSubscriber,
                                       final Callback<ResponseBody> callback,
                                       final Object context) {
        HChannel hChannel = topicSubscriber2Channel.removeChannel(topicSubscriber);
        if (null == hChannel) {
            logger.warn("Trying to close a subscription when we don't have a subscribe channel cached for {}",
                        topicSubscriber);
            callback.operationFinished(context, (ResponseBody)null);
            return;
        }

        Channel channel = hChannel.getChannel();
        if (null == channel) {
            callback.operationFinished(context, (ResponseBody)null);
            return;
        }

        try {
            HChannelImpl.getHChannelHandlerFromChannel(channel).closeExplicitly();
        } catch (NoResponseHandlerException nrhe) {
            logger.warn("No Channel Handler found when closing {}'s channel {}.",
                        channel, topicSubscriber);
        }
        ChannelFuture future = channel.close();
        future.addListener(new ChannelFutureListener() {
            @Override
            public void operationComplete(ChannelFuture future) throws Exception {
                if (!future.isSuccess()) {
                    logger.error("Failed to close the subscription channel for {}",
                                 topicSubscriber);
                    callback.operationFailed(context, new ServiceDownException(
                        "Failed to close the subscription channel for " + topicSubscriber));
                } else {
                    callback.operationFinished(context, (ResponseBody)null);
                }
            }
        });
    }

    @Override
    protected void checkTimeoutRequestsOnSubscriptionChannels() {
        // timeout task may be started before constructing topicSubscriber2Channel
        if (null == topicSubscriber2Channel) {
            return;
        }
        for (HChannel channel : topicSubscriber2Channel.getChannels()) {
            try {
                HChannelHandler channelHandler =
                    HChannelImpl.getHChannelHandlerFromChannel(channel.getChannel());
                channelHandler.checkTimeoutRequests();
            } catch (NoResponseHandlerException nrhe) {
                continue;
            }
        }
    }

    @Override
    protected void closeSubscriptionChannels() {
        topicSubscriber2Channel.close();
    }
}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/impl/simple/SimpleSubscribeResponseHandler.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty.impl.simple;

import java.net.InetSocketAddress;
import java.util.Set;
import java.util.Collections;
import java.util.concurrent.ConcurrentHashMap;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;
import org.jboss.netty.channel.ChannelFuture;
import org.jboss.netty.channel.ChannelFutureListener;

import com.google.protobuf.ByteString;

import org.apache.hedwig.client.api.MessageHandler;
import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.data.PubSubData;
import org.apache.hedwig.client.data.TopicSubscriber;
import org.apache.hedwig.client.exceptions.AlreadyStartDeliveryException;
import org.apache.hedwig.client.handlers.SubscribeResponseHandler;
import org.apache.hedwig.client.netty.HChannel;
import org.apache.hedwig.client.netty.HChannelManager;
import org.apache.hedwig.client.netty.impl.AbstractHChannelManager;
import org.apache.hedwig.client.netty.impl.AbstractSubscribeResponseHandler;
import org.apache.hedwig.client.netty.impl.ActiveSubscriber;
import org.apache.hedwig.client.netty.impl.HChannelImpl;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.ClientNotSubscribedException;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse;
import org.apache.hedwig.protocol.PubSubProtocol.ResponseBody;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEvent;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences;
import org.apache.hedwig.protocol.PubSubProtocol.StatusCode;
import org.apache.hedwig.protoextensions.MessageIdUtils;
import org.apache.hedwig.util.Callback;
import org.apache.hedwig.util.Either;

public class SimpleSubscribeResponseHandler extends AbstractSubscribeResponseHandler {

    private static Logger logger = LoggerFactory.getLogger(SimpleSubscribeResponseHandler.class);

    /**
     * Simple Active Subscriber enabling client-side throttling.
     */
    static class SimpleActiveSubscriber extends ActiveSubscriber {

        // Set to store all of the outstanding subscribed messages that are pending
        // to be consumed by the client app's MessageHandler. If this ever grows too
        // big (e.g. problem at the client end for message consumption), we can
        // throttle things by temporarily setting the Subscribe Netty Channel
        // to not be readable. When the Set has shrunk sufficiently, we can turn the
        // channel back on to read new messages.
        private final Set<Message> outstandingMsgSet;

        public SimpleActiveSubscriber(ClientConfiguration cfg,
                                      AbstractHChannelManager channelManager,
                                      TopicSubscriber ts, PubSubData op,
                                      SubscriptionPreferences preferences,
                                      Channel channel,
                                      HChannel hChannel) {
            super(cfg, channelManager, ts, op, preferences, channel, hChannel);
            outstandingMsgSet = Collections.newSetFromMap(
                    new ConcurrentHashMap<Message, Boolean>(
                            cfg.getMaximumOutstandingMessages(), 1.0f));
        }

        @Override
        protected void unsafeDeliverMessage(Message message) {
            // Add this "pending to be consumed" message to the outstandingMsgSet.
            outstandingMsgSet.add(message);
            // Check if we've exceeded the max size for the outstanding message set.
            if (outstandingMsgSet.size() >= cfg.getMaximumOutstandingMessages() &&
                channel.isReadable()) {
                // Too many outstanding messages so throttle it by setting the Netty
                // Channel to not be readable.
                if (logger.isDebugEnabled()) {
                    logger.debug("Too many outstanding messages ({}) so throttling the subscribe netty Channel",
                                 outstandingMsgSet.size());
                }
                channel.setReadable(false);
            }
            super.unsafeDeliverMessage(message);
        }

        @Override
        public synchronized void messageConsumed(Message message) {
            super.messageConsumed(message);
            // Remove this consumed message from the outstanding Message Set.
            outstandingMsgSet.remove(message);
            // Check if we throttled message consumption previously when the
            // outstanding message limit was reached. For now, only turn the
            // delivery back on if there are no more outstanding messages to
            // consume. We could make this a configurable parameter if needed.
            if (!channel.isReadable() && outstandingMsgSet.size() == 0) {
                if (logger.isDebugEnabled())
                    logger.debug("Message consumption has caught up so okay to turn off"
                                 + " throttling of messages on the subscribe channel for {}",
                                 topicSubscriber);
                channel.setReadable(true);
            }
        }

        @Override
        public synchronized void startDelivery(MessageHandler messageHandler)
        throws AlreadyStartDeliveryException, ClientNotSubscribedException {
            super.startDelivery(messageHandler);
            // Now make the TopicSubscriber Channel readable (it is set to not be
            // readable when the initial subscription is done). Note that this is an
            // asynchronous call. If this fails (not likely), the futureListener
            // will just log an error message for now.
            ChannelFuture future = channel.setReadable(true);
            future.addListener(new ChannelFutureListener() {
                @Override
                public void operationComplete(ChannelFuture future) throws Exception {
                    if (!future.isSuccess()) {
                        logger.error("Unable to make subscriber Channel readable in startDelivery call for {}",
                                     topicSubscriber);
                    }
                }
            });
        }

        @Override
        public synchronized void stopDelivery() {
            super.stopDelivery();
            // Now make the TopicSubscriber channel not-readable. This will buffer
            // up messages if any are sent from the server. Note that this is an
            // asynchronous call. If this fails (not likely), the futureListener
            // will just log an error message for now.
            ChannelFuture future = channel.setReadable(false);
            future.addListener(new ChannelFutureListener() {
                @Override
                public void operationComplete(ChannelFuture future) throws Exception {
                    if (!future.isSuccess()) {
                        logger.error("Unable to make subscriber Channel not readable in stopDelivery call for {}",
                                     topicSubscriber);
                    }
                }
            });
        }

    }

    // Track which subscriber is alive in this response handler
    // Which is used for backward compat, since old version hub
    // server doesn't carry (topic, subscriberid) in each message.
    private volatile TopicSubscriber origTopicSubscriber;
    private volatile ActiveSubscriber origActiveSubscriber;

    private SimpleHChannelManager sChannelManager;

    protected SimpleSubscribeResponseHandler(ClientConfiguration cfg,
                                             HChannelManager channelManager) {
        super(cfg, channelManager);
        sChannelManager = (SimpleHChannelManager) channelManager;
    }

    @Override
    protected ActiveSubscriber createActiveSubscriber(
        ClientConfiguration cfg, AbstractHChannelManager channelManager,
        TopicSubscriber ts, PubSubData op, SubscriptionPreferences preferences,
        Channel channel, HChannel hChannel) {
        return new SimpleActiveSubscriber(cfg, channelManager, ts, op, preferences, channel, hChannel);
    }

    @Override
    protected synchronized ActiveSubscriber getActiveSubscriber(TopicSubscriber ts) {
        if (null == origTopicSubscriber || !origTopicSubscriber.equals(ts)) {
            return null;
        }
        return origActiveSubscriber;
    }

    private synchronized ActiveSubscriber getActiveSubscriber() {
        return origActiveSubscriber;
    }

    @Override
    public synchronized boolean hasSubscription(TopicSubscriber ts) {
        if (null == origTopicSubscriber) {
            return false;
        }
        return origTopicSubscriber.equals(ts);
    }

    @Override
    protected synchronized boolean removeSubscription(TopicSubscriber ts, ActiveSubscriber ss) {
        if (null != origTopicSubscriber && !origTopicSubscriber.equals(ts)) {
            return false;
        }
        origTopicSubscriber = null;
        origActiveSubscriber = null;
        return super.removeSubscription(ts, ss);
    }

    @Override
    public void handleResponse(PubSubResponse response, PubSubData pubSubData,
                               Channel channel) throws Exception {
        // If this was not a successful response to the Subscribe request, we
        // won't be using the Netty Channel created so just close it.
        if (!response.getStatusCode().equals(StatusCode.SUCCESS)) {
            HChannelImpl.getHChannelHandlerFromChannel(channel).closeExplicitly();
            channel.close();
        }
        super.handleResponse(response, pubSubData, channel);
    }

    @Override
    public void handleSubscribeMessage(PubSubResponse response) {
        Message message = response.getMessage();
        ActiveSubscriber ss = getActiveSubscriber();
        if (null == ss) {
            logger.error("No Subscriber is alive receiving its message {}.",
                         MessageIdUtils.msgIdToReadableString(message.getMsgId()));
            return;
        }
        ss.handleMessage(message);
    }

    @Override
    protected Either<StatusCode, HChannel> handleSuccessResponse(
        TopicSubscriber ts, PubSubData pubSubData, Channel channel) {
        // Store the mapping for the TopicSubscriber to the Channel.
        // This is so we can control the starting and stopping of
        // message deliveries from the server on that Channel. Store
        // this only on a successful ack response from the server.
        Either<Boolean, HChannel> result =
            sChannelManager.storeSubscriptionChannel(ts, pubSubData, channel);
        if (result.left()) {
            return Either.of(StatusCode.SUCCESS, result.right());
        } else {
            StatusCode code;
            if (pubSubData.isResubscribeRequest()) {
                code = StatusCode.RESUBSCRIBE_EXCEPTION;
            } else {
                code = StatusCode.CLIENT_ALREADY_SUBSCRIBED;
            }
            return Either.of(code, null);
        }
    }

    @Override
    protected synchronized void postHandleSuccessResponse(
        TopicSubscriber ts, ActiveSubscriber as) {
        origTopicSubscriber = ts;
        origActiveSubscriber = as;
    }

    @Override
    public void asyncCloseSubscription(final TopicSubscriber topicSubscriber,
                                       final Callback<ResponseBody> callback,
                                       final Object context) {
        // nothing to do just clear status
        // channel manager takes the responsibility to close the channel
        callback.operationFinished(context, (ResponseBody)null);
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/netty/impl/simple/SimpleSubscriptionChannelPipelineFactory.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.netty.impl.simple;

import java.util.HashMap;
import java.util.Map;

import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.handlers.AbstractResponseHandler;
import org.apache.hedwig.client.handlers.CloseSubscriptionResponseHandler;
import org.apache.hedwig.client.netty.impl.AbstractHChannelManager;
import org.apache.hedwig.client.netty.impl.ClientChannelPipelineFactory;
import org.apache.hedwig.client.netty.impl.HChannelHandler;
import org.apache.hedwig.protocol.PubSubProtocol.OperationType;

public class SimpleSubscriptionChannelPipelineFactory extends ClientChannelPipelineFactory {

    public SimpleSubscriptionChannelPipelineFactory(ClientConfiguration cfg,
                                                    SimpleHChannelManager channelManager) {
        super(cfg, channelManager);
    }

    @Override
    protected Map<OperationType, AbstractResponseHandler> createResponseHandlers() {
        Map<OperationType, AbstractResponseHandler> handlers =
            new HashMap<OperationType, AbstractResponseHandler>();
        handlers.put(OperationType.SUBSCRIBE,
                     new SimpleSubscribeResponseHandler(cfg, channelManager));
        handlers.put(OperationType.CLOSESUBSCRIPTION,
                     new CloseSubscriptionResponseHandler(cfg, channelManager));
        return handlers;
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/ssl/SslClientContextFactory.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.ssl;

import javax.net.ssl.SSLContext;

import org.apache.hedwig.client.conf.ClientConfiguration;

public class SslClientContextFactory extends SslContextFactory {

    public SslClientContextFactory(ClientConfiguration cfg) {
        try {
            // Create the SSL context.
            ctx = SSLContext.getInstance("TLS");
            ctx.init(null, getTrustManagers(), null);
        } catch (Exception ex) {
            throw new RuntimeException(ex);
        }
    }

    @Override
    protected boolean isClient() {
        return true;
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/client/ssl/SslContextFactory.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.client.ssl;

import java.security.cert.CertificateException;
import java.security.cert.X509Certificate;

import javax.net.ssl.SSLContext;
import javax.net.ssl.SSLEngine;
import javax.net.ssl.TrustManager;
import javax.net.ssl.X509TrustManager;

public abstract class SslContextFactory {

    protected SSLContext ctx;

    public SSLContext getContext() {
        return ctx;
    }

    protected abstract boolean isClient();

    public SSLEngine getEngine() {
        SSLEngine engine = ctx.createSSLEngine();
        engine.setUseClientMode(isClient());
        return engine;
    }

    protected TrustManager[] getTrustManagers() {
        return new TrustManager[] { new X509TrustManager() {
                // Always trust, even if invalid.

                @Override
                public X509Certificate[] getAcceptedIssuers() {
                    return new X509Certificate[0];
                }

                @Override
                public void checkServerTrusted(X509Certificate[] chain, String authType) throws CertificateException {
                    // Always trust.
                }

                @Override
                public void checkClientTrusted(X509Certificate[] chain, String authType) throws CertificateException {
                    // Always trust.
                }
            }
        };
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/conf/AbstractConfiguration.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.conf;

import java.net.URL;

import org.apache.commons.configuration.CompositeConfiguration;
import org.apache.commons.configuration.Configuration;
import org.apache.commons.configuration.ConfigurationException;
import org.apache.commons.configuration.PropertiesConfiguration;

public abstract class AbstractConfiguration {
    protected CompositeConfiguration conf;

    protected AbstractConfiguration() {
        conf = new CompositeConfiguration();
    }

    /**
     * Return real configuration object
     *
     * @return configuration
     */
    public Configuration getConf() {
        return conf;
    }

    /**
     * You can load configurations in precedence order. The first one takes
     * precedence over any loaded later.
     *
     * @param confURL
     */
    public void loadConf(URL confURL) throws ConfigurationException {
        Configuration loadedConf = new PropertiesConfiguration(confURL);
        conf.addConfiguration(loadedConf);

    }

    /**
     * Add configuration object.
     *
     * @param conf configuration object
     */
    public void addConf(Configuration otherConf) throws ConfigurationException {
        conf.addConfiguration(otherConf);
    }
}
"
hedwig-client/src/main/java/org/apache/hedwig/filter/ClientMessageFilter.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.filter;

/**
 * Message Filter running in client-side.
 */
public interface ClientMessageFilter extends MessageFilterBase {
}
"
hedwig-client/src/main/java/org/apache/hedwig/filter/MessageFilterBase.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.filter;

import com.google.protobuf.ByteString;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences;

public interface MessageFilterBase {

    /**
     * Set subscription preferences.
     *
     * <code>preferences</code> of the subscriber will be passed to message filter when
     * the message filter attaches to its subscription either in server-side or client-side.
     *
     * @param topic
     *          Topic Name.
     * @param subscriberId
     *          Subscriber Id.
     * @param preferences
     *          Subscription Preferences.
     * @return message filter
     */
    public MessageFilterBase setSubscriptionPreferences(ByteString topic, ByteString subscriberId,
                                                        SubscriptionPreferences preferences);

    /**
     * Tests whether a particular message passes the filter or not
     *
     * @param message
     * @return
     */
    public boolean testMessage(Message message);
}
"
hedwig-client/src/main/java/org/apache/hedwig/filter/PipelineFilter.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.filter;

import java.io.IOException;
import java.util.List;
import java.util.LinkedList;

import com.google.protobuf.ByteString;
import org.apache.commons.configuration.Configuration;
import org.apache.commons.configuration.ConfigurationException;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences;

/**
 * A filter filters messages in pipeline.
 */
public class PipelineFilter extends LinkedList<ServerMessageFilter>
implements ServerMessageFilter {

    @Override
    public ServerMessageFilter initialize(Configuration conf)
    throws ConfigurationException, IOException {
        for (ServerMessageFilter filter : this) {
            filter.initialize(conf);
        }
        return this;
    }

    @Override
    public void uninitialize() {
        while (!isEmpty()) {
            ServerMessageFilter filter = removeLast();
            filter.uninitialize();
        }
    }

    @Override
    public MessageFilterBase setSubscriptionPreferences(ByteString topic, ByteString subscriberId,
                                                        SubscriptionPreferences preferences) {
        for (ServerMessageFilter filter : this) {
            filter.setSubscriptionPreferences(topic, subscriberId, preferences);
        }
        return this;
    }

    @Override
    public boolean testMessage(Message message) {
        for (ServerMessageFilter filter : this) {
            if (!filter.testMessage(message)) {
                return false;
            }
        }
        return true;
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/filter/ServerMessageFilter.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.filter;

import java.io.IOException;

import org.apache.commons.configuration.Configuration;
import org.apache.commons.configuration.ConfigurationException;

/**
 * Message Filter running in server-side. Hub server uses reflection to
 * instantiate a message filter to filter messages.
 */
public interface ServerMessageFilter extends MessageFilterBase {

    /**
     * Initialize the message filter.
     *
     * @param conf
     *          Configuration Object. An <i>MessageFilter</i> might read settings from it.
     * @return message filter
     * @throws IOException when failed to initialize message filter
     */
    public ServerMessageFilter initialize(Configuration conf)
    throws ConfigurationException, IOException;

    /**
     * Uninitialize the message filter.
     */
    public void uninitialize();

}
"
hedwig-client/src/main/java/org/apache/hedwig/util/Callback.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.util;

import org.apache.hedwig.exceptions.PubSubException;

/**
 * This class is used for callbacks for asynchronous operations
 *
 */
public interface Callback<T> {

    /**
     * This method is called when the asynchronous operation finishes
     *
     * @param ctx
     * @param resultOfOperation
     */
    public abstract void operationFinished(Object ctx, T resultOfOperation);

    /**
     * This method is called when the operation failed due to some reason. The
     * reason for failure is passed in.
     *
     * @param ctx
     *            The context for the callback
     * @param exception
     *            The reason for the failure of the scan
     */
    public abstract void operationFailed(Object ctx, PubSubException exception);

}
"
hedwig-client/src/main/java/org/apache/hedwig/util/CallbackUtils.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.util;

import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.atomic.AtomicInteger;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.CompositeException;

public class CallbackUtils {

    /**
     * A callback that waits for all of a number of events to fire. If any fail,
     * then fail the final callback with a composite exception.
     *
     * TODO: change this to use any Exception and make CompositeException
     * generic, not a PubSubException.
     *
     * @param expected
     *            Number of expected callbacks.
     * @param cb
     *            The final callback to call.
     * @param ctx
     * @param logger
     *            May be null.
     * @param successMsg
     *            If not null, then this is logged on success.
     * @param failureMsg
     *            If not null, then this is logged on failure.
     * @param eagerErrorHandler
     *            If not null, then this will be executed after the first
     *            failure (but before the final failure callback). Useful for
     *            releasing resources, etc. as soon as we know the composite
     *            operation is doomed.
     * @return the generated callback
     */
    public static Callback<Void> multiCallback(final int expected, final Callback<Void> cb, final Object ctx,
            final Logger logger, final String successMsg, final String failureMsg,
            Runnable eagerErrorHandler) {
        if (expected == 0) {
            cb.operationFinished(ctx, null);
            return null;
        } else {
            return new Callback<Void>() {

                final AtomicInteger done = new AtomicInteger();
                final LinkedBlockingQueue<PubSubException> exceptions = new LinkedBlockingQueue<PubSubException>();

                private void tick() {
                    if (done.incrementAndGet() == expected) {
                        if (exceptions.isEmpty()) {
                            cb.operationFinished(ctx, null);
                        } else {
                            cb.operationFailed(ctx, new CompositeException(exceptions));
                        }
                    }
                }

                @Override
                public void operationFailed(Object ctx, PubSubException exception) {
                    if (logger != null && failureMsg != null)
                        logger.error(failureMsg, exception);
                    exceptions.add(exception);
                    tick();
                }

                @Override
                public void operationFinished(Object ctx, Void resultOfOperation) {
                    if (logger != null && successMsg != null)
                        logger.info(successMsg);
                    tick();
                }

            };
        }
    }

    /**
     * A callback that waits for all of a number of events to fire. If any fail,
     * then fail the final callback with a composite exception.
     */
    public static Callback<Void> multiCallback(int expected, Callback<Void> cb, Object ctx) {
        return multiCallback(expected, cb, ctx, null, null, null, null);
    }

    /**
     * A callback that waits for all of a number of events to fire. If any fail,
     * then fail the final callback with a composite exception.
     */
    public static Callback<Void> multinCallback(int expected, Callback<Void> cb, Object ctx, Runnable eagerErrorHandler) {
        return multiCallback(expected, cb, ctx, null, null, null, eagerErrorHandler);
    }

    private static Callback<Void> nop = new Callback<Void>() {

        @Override
        public void operationFailed(Object ctx, PubSubException exception) {
        }

        @Override
        public void operationFinished(Object ctx, Void resultOfOperation) {
        }

    };

    /**
     * A do-nothing callback.
     */
    public static Callback<Void> nop() {
        return nop;
    }

    /**
     * Logs what happened before continuing the callback chain.
     */
    public static <T> Callback<T> logger(final Logger logger, final String successMsg,
                                         final String failureMsg, final Callback<T> cont) {
        return new Callback<T>() {

            @Override
            public void operationFailed(Object ctx, PubSubException exception) {
                logger.error(failureMsg, exception);
                if (cont != null)
                    cont.operationFailed(ctx, exception);
            }

            @Override
            public void operationFinished(Object ctx, T resultOfOperation) {
                logger.info(successMsg);
                if (cont != null)
                    cont.operationFinished(ctx, resultOfOperation);
            }

        };
    }

    /**
     * Logs what happened (no continuation).
     */
    public static Callback<Void> logger(Logger logger, String successMsg, String failureMsg) {
        return logger(logger, successMsg, failureMsg, nop());
    }

    /**
     * Return a Callback<Void> that just calls the given Callback cb with the
     * bound result.
     */
    public static <T> Callback<Void> curry(final Callback<T> cb, final T result) {
        return new Callback<Void>() {

            @Override
            public void operationFailed(Object ctx, PubSubException exception) {
                cb.operationFailed(ctx, exception);
            }

            @Override
            public void operationFinished(Object ctx, Void resultOfOperation) {
                cb.operationFinished(ctx, result);
            }

        };
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/util/ConcurrencyUtils.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.util;

import java.util.concurrent.BlockingQueue;
import java.util.concurrent.CyclicBarrier;

public class ConcurrencyUtils {

    public static <T, U extends T, V extends BlockingQueue<T>> void put(V queue, U value) {
        try {
            queue.put(value);
        } catch (Exception ex) {
            throw new RuntimeException(ex);
        }
    }

    public static <T> T take(BlockingQueue<T> queue) {
        try {
            return queue.take();
        } catch (Exception ex) {
            throw new RuntimeException(ex);
        }
    }

    public static void await(CyclicBarrier barrier) {
        try {
            barrier.await();
        } catch (Exception ex) {
            throw new RuntimeException(ex);
        }
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/util/Either.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.util;

public class Either<T, U> {

    private T x;
    private U y;

    private Either(T x, U y) {
        this.x = x;
        this.y = y;
    }

    public static <T, U> Either<T, U> of(T x, U y) {
        return new Either<T, U>(x, y);
    }

    public static <T, U> Either<T, U> left(T x) {
        return new Either<T, U>(x, null);
    }

    public static <T, U> Either<T, U> right(U y) {
        return new Either<T, U>(null, y);
    }

    public T left() {
        return x;
    }

    public U right() {
        return y;
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/util/FileUtils.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.util;

import java.io.File;
import java.io.IOException;
import java.util.LinkedList;
import java.util.List;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class FileUtils {

    static DirDeleterThred dirDeleterThread;
    static Logger log = LoggerFactory.getLogger(FileUtils.class);

    static {
        dirDeleterThread = new DirDeleterThred();
        Runtime.getRuntime().addShutdownHook(dirDeleterThread);
    }

    public static File createTempDirectory(String prefix) throws IOException {
        return createTempDirectory(prefix, null);
    }

    public static File createTempDirectory(String prefix, String suffix) throws IOException {
        File tempDir = File.createTempFile(prefix, suffix);
        if (!tempDir.delete()) {
            throw new IOException("Could not delete temp file: " + tempDir.getAbsolutePath());
        }

        if (!tempDir.mkdir()) {
            throw new IOException("Could not create temp directory: " + tempDir.getAbsolutePath());
        }

        dirDeleterThread.addDirToDelete(tempDir);
        return tempDir;

    }

    static class DirDeleterThred extends Thread {
        List<File> dirsToDelete = new LinkedList<File>();

        public synchronized void addDirToDelete(File dir) {
            dirsToDelete.add(dir);
        }

        @Override
        public void run() {
            synchronized (this) {
                for (File dir : dirsToDelete) {
                    deleteDirectory(dir);
                }
            }
        }

        protected void deleteDirectory(File dir) {
            if (dir.isFile()) {
                if (!dir.delete()) {
                    log.error("Could not delete " + dir.getAbsolutePath());
                }
                return;
            }

            File[] files = dir.listFiles();
            if (files == null) {
                return;
            }

            for (File f : files) {
                deleteDirectory(f);
            }

            if (!dir.delete()) {
                log.error("Could not delete directory: " + dir.getAbsolutePath());
            }

        }

    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/util/HedwigSocketAddress.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.util;

import java.net.InetSocketAddress;

/**
 * This is a data wrapper class that is basically an InetSocketAddress with one
 * extra piece of information for the SSL port (optional). This is used by
 * Hedwig so we can encapsulate both regular and SSL port information in one
 * data structure. Hedwig hub servers can be configured to listen on the
 * standard regular port and additionally on an optional SSL port. The String
 * representation of a HedwigSocketAddress is: <hostname>:<port>:<SSL
 * port(optional)>
 */
public class HedwigSocketAddress {

    // Member fields that make up this class.
    private final String hostname;
    private final int port;
    private final int sslPort;

    private final InetSocketAddress socketAddress;
    private final InetSocketAddress sslSocketAddress;

    // Constants used by this class.
    public static final String COLON = ":";
    private static final int NO_SSL_PORT = -1;

    // Constructor that takes in both a regular and SSL port.
    public HedwigSocketAddress(String hostname, int port, int sslPort) {
        this.hostname = hostname;
        this.port = port;
        this.sslPort = sslPort;
        socketAddress = new InetSocketAddress(hostname, port);
        if (sslPort != NO_SSL_PORT)
            sslSocketAddress = new InetSocketAddress(hostname, sslPort);
        else
            sslSocketAddress = null;
    }

    // Constructor that only takes in a regular port.
    public HedwigSocketAddress(String hostname, int port) {
        this(hostname, port, NO_SSL_PORT);
    }

    // Constructor from a String "serialized" version of this class.
    public HedwigSocketAddress(String addr) {
        String[] parts = addr.split(COLON);
        this.hostname = parts[0];
        this.port = Integer.parseInt(parts[1]);
        if (parts.length > 2)
            this.sslPort = Integer.parseInt(parts[2]);
        else
            this.sslPort = NO_SSL_PORT;
        socketAddress = new InetSocketAddress(hostname, port);
        if (sslPort != NO_SSL_PORT)
            sslSocketAddress = new InetSocketAddress(hostname, sslPort);
        else
            sslSocketAddress = null;
    }

    // Public getters
    public String getHostname() {
        return hostname;
    }

    public int getPort() {
        return port;
    }

    public int getSSLPort() {
        return sslPort;
    }

    // Method to return an InetSocketAddress for the regular port.
    public InetSocketAddress getSocketAddress() {
        return socketAddress;
    }

    // Method to return an InetSocketAddress for the SSL port.
    // Note that if no SSL port (or an invalid value) was passed
    // during object creation, this call will throw an IllegalArgumentException
    // (runtime exception).
    public InetSocketAddress getSSLSocketAddress() {
        return sslSocketAddress;
    }

    // Method to determine if this object instance is SSL enabled or not
    // (contains a valid SSL port).
    public boolean isSSLEnabled() {
        return sslPort != NO_SSL_PORT;
    }

    // Return the String "serialized" version of this object.
    @Override
    public String toString() {
        StringBuilder sb = new StringBuilder();
        sb.append(hostname).append(COLON).append(port).append(COLON).append(sslPort);
        return sb.toString();
    }

    // Implement an equals method comparing two HedwigSocketAddress objects.
    @Override
    public boolean equals(Object obj) {
        if (!(obj instanceof HedwigSocketAddress))
            return false;
        HedwigSocketAddress that = (HedwigSocketAddress) obj;
        return (this.hostname.equals(that.hostname) && (this.port == that.port) && (this.sslPort == that.sslPort));
    }

    @Override
    public int hashCode() {
        return (this.hostname + this.port + this.sslPort).hashCode();
    }

    // Static helper method to return the string representation for an
    // InetSocketAddress. The HedwigClient can only operate in SSL or non-SSL
    // mode. So the server hosts it connects to will just be an
    // InetSocketAddress instead of a HedwigSocketAddress. This utility method
    // can be used so we can store these server hosts as strings (ByteStrings)
    // in various places (e.g. list of server hosts we've connected to
    // or wrote to unsuccessfully).
    public static String sockAddrStr(InetSocketAddress addr) {
        return addr.getAddress().getHostAddress() + ":" + addr.getPort();
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/util/Option.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.util;

public class Option<T> {

    private T x;

    public static <T> Option<T> of(T x) {
        return new Option<T>(x);
    }

    public static <T> Option<T> of() {
        return new Option<T>();
    }

    public Option() {
    }

    public Option(T x) {
        this.x = x;
    }

    public T get() {
        return x;
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/util/Pair.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.util;

public class Pair<T, U> {

    private T x;
    private U y;

    public Pair(T x, U y) {
        this.x = x;
        this.y = y;
    }

    public static <T, U> Pair<T, U> of(T x, U y) {
        return new Pair<T, U>(x, y);
    }

    public T first() {
        return x;
    }

    public U second() {
        return y;
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/util/PathUtils.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.util;

import java.io.File;
import java.util.ArrayList;
import java.util.List;

public class PathUtils {

    /** Generate all prefixes for a path. "/a/b/c" -> ["/a","/a/b","/a/b/c"] */
    public static List<String> prefixes(String path) {
        List<String> prefixes = new ArrayList<String>();
        StringBuilder prefix = new StringBuilder();
        for (String comp : path.split("/+")) {
            // Skip the first (empty) path component.
            if (!comp.equals("")) {
                prefix.append("/").append(comp);
                prefixes.add(prefix.toString());
            }
        }
        return prefixes;
    }

    /** Return true iff prefix is a prefix of path. */
    public static boolean isPrefix(String prefix, String path) {
        String[] as = prefix.split("/+"), bs = path.split("/+");
        if (as.length > bs.length)
            return false;
        for (int i = 0; i < as.length; i++)
            if (!as[i].equals(bs[i]))
                return false;
        return true;
    }

    /** Like File.getParent but always uses the / separator. */
    public static String parent(String path) {
        return new File(path).getParent().replace("\\", "/");
    }

}
"
hedwig-client/src/main/java/org/apache/hedwig/util/SubscriptionListener.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.util;

import com.google.protobuf.ByteString;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEvent;

/**
 * This class is used for subscriber to listen on subscription event.
 */
public interface SubscriptionListener {

    /**
     * Process an event from a subscription.
     * <p>
     * NOTE: It would be better to not run blocking operations in a
     *       listener implementation.
     * </p>
     *
     * @param topic
     *          Topic Name
     * @param subscriberId
     *          Subscriber Id
     * @param event
     *          Event tell what happened to the subscription.
     */
    public void processEvent(ByteString topic, ByteString subscriberId,
                             SubscriptionEvent event);
}
"
hedwig-client/src/main/java/org/apache/hedwig/util/VarArgs.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.util;

public class VarArgs {

    public static Object[] va(Object...args) {
        return args;
    }

}
"
hedwig-protocol/src/main/java/org/apache/hedwig/exceptions/PubSubException.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.exceptions;

import java.util.Collection;
import java.util.Iterator;

import org.apache.hedwig.protocol.PubSubProtocol.StatusCode;

@SuppressWarnings("serial")
public abstract class PubSubException extends Exception {
    protected StatusCode code;

    protected PubSubException(StatusCode code, String msg) {
        super(msg);
        this.code = code;
    }

    protected PubSubException(StatusCode code, Throwable t) {
        super(t);
        this.code = code;
    }

    protected PubSubException(StatusCode code, String msg, Throwable t) {
        super(msg, t);
        this.code = code;
    }

    public static PubSubException create(StatusCode code, String msg) {
        if (code == StatusCode.CLIENT_ALREADY_SUBSCRIBED) {
            return new ClientAlreadySubscribedException(msg);
        } else if (code == StatusCode.CLIENT_NOT_SUBSCRIBED) {
            return new ClientNotSubscribedException(msg);
        } else if (code == StatusCode.MALFORMED_REQUEST) {
            return new MalformedRequestException(msg);
        } else if (code == StatusCode.NO_SUCH_TOPIC) {
            return new NoSuchTopicException(msg);
        } else if (code == StatusCode.NOT_RESPONSIBLE_FOR_TOPIC) {
            return new ServerNotResponsibleForTopicException(msg);
        } else if (code == StatusCode.SERVICE_DOWN) {
            return new ServiceDownException(msg);
        } else if (code == StatusCode.COULD_NOT_CONNECT) {
            return new CouldNotConnectException(msg);
        } else if (code == StatusCode.TOPIC_BUSY) {
            return new TopicBusyException(msg);
        } else if (code == StatusCode.BAD_VERSION) {
            return new BadVersionException(msg);
        } else if (code == StatusCode.NO_TOPIC_PERSISTENCE_INFO) {
            return new NoTopicPersistenceInfoException(msg);
        } else if (code == StatusCode.TOPIC_PERSISTENCE_INFO_EXISTS) {
            return new TopicPersistenceInfoExistsException(msg);
        } else if (code == StatusCode.NO_SUBSCRIPTION_STATE) {
            return new NoSubscriptionStateException(msg);
        } else if (code == StatusCode.SUBSCRIPTION_STATE_EXISTS) {
            return new SubscriptionStateExistsException(msg);
        } else if (code == StatusCode.NO_TOPIC_OWNER_INFO) {
            return new NoTopicOwnerInfoException(msg);
        } else if (code == StatusCode.TOPIC_OWNER_INFO_EXISTS) {
            return new TopicOwnerInfoExistsException(msg);
        } else if (code == StatusCode.INVALID_MESSAGE_FILTER) {
            return new InvalidMessageFilterException(msg);
        } else if (code == StatusCode.RESUBSCRIBE_EXCEPTION) {
            return new ResubscribeException(msg);
        }
        /*
         * Insert new ones here
         */
        else if (code == StatusCode.UNCERTAIN_STATE) {
            return new UncertainStateException(msg);
        }
        // Finally the catch all exception (for unexpected error conditions)
        else {
            return new UnexpectedConditionException("Unknow status code:" + code.getNumber() + ", msg: " + msg);
        }
    }

    public StatusCode getCode() {
        return code;
    }

    public static class ClientAlreadySubscribedException extends PubSubException {
        public ClientAlreadySubscribedException(String msg) {
            super(StatusCode.CLIENT_ALREADY_SUBSCRIBED, msg);
        }
    }

    public static class ClientNotSubscribedException extends PubSubException {
        public ClientNotSubscribedException(String msg) {
            super(StatusCode.CLIENT_NOT_SUBSCRIBED, msg);
        }
    }

    public static class ResubscribeException extends PubSubException {
        public ResubscribeException(String msg) {
            super(StatusCode.RESUBSCRIBE_EXCEPTION, msg);
        }
    }

    public static class MalformedRequestException extends PubSubException {
        public MalformedRequestException(String msg) {
            super(StatusCode.MALFORMED_REQUEST, msg);
        }
    }

    public static class NoSuchTopicException extends PubSubException {
        public NoSuchTopicException(String msg) {
            super(StatusCode.NO_SUCH_TOPIC, msg);
        }
    }

    public static class ServerNotResponsibleForTopicException extends PubSubException {
        // Note the exception message serves as the name of the responsible host
        public ServerNotResponsibleForTopicException(String responsibleHost) {
            super(StatusCode.NOT_RESPONSIBLE_FOR_TOPIC, responsibleHost);
        }
    }

    public static class TopicBusyException extends PubSubException {
        public TopicBusyException(String msg) {
            super(StatusCode.TOPIC_BUSY, msg);
        }
    }

    public static class ServiceDownException extends PubSubException {
        public ServiceDownException(String msg) {
            super(StatusCode.SERVICE_DOWN, msg);
        }

        public ServiceDownException(Exception e) {
            super(StatusCode.SERVICE_DOWN, e);
        }

        public ServiceDownException(String msg, Throwable t) {
            super(StatusCode.SERVICE_DOWN, msg, t);
        }
    }

    public static class CouldNotConnectException extends PubSubException {
        public CouldNotConnectException(String msg) {
            super(StatusCode.COULD_NOT_CONNECT, msg);
        }
    }

    public static class BadVersionException extends PubSubException {
        public BadVersionException(String msg) {
            super(StatusCode.BAD_VERSION, msg);
        }
    }

    public static class NoTopicPersistenceInfoException extends PubSubException {
        public NoTopicPersistenceInfoException(String msg) {
            super(StatusCode.NO_TOPIC_PERSISTENCE_INFO, msg);
        }
    }

    public static class TopicPersistenceInfoExistsException extends PubSubException {
        public TopicPersistenceInfoExistsException(String msg) {
            super(StatusCode.TOPIC_PERSISTENCE_INFO_EXISTS, msg);
        }
    }

    public static class NoSubscriptionStateException extends PubSubException {
        public NoSubscriptionStateException(String msg) {
            super(StatusCode.NO_SUBSCRIPTION_STATE, msg);
        }
    }

    public static class SubscriptionStateExistsException extends PubSubException {
        public SubscriptionStateExistsException(String msg) {
            super(StatusCode.SUBSCRIPTION_STATE_EXISTS, msg);
        }
    }

    public static class NoTopicOwnerInfoException extends PubSubException {
        public NoTopicOwnerInfoException(String msg) {
            super(StatusCode.NO_TOPIC_OWNER_INFO, msg);
        }
    }

    public static class TopicOwnerInfoExistsException extends PubSubException {
        public TopicOwnerInfoExistsException(String msg) {
            super(StatusCode.TOPIC_OWNER_INFO_EXISTS, msg);
        }
    }

    public static class InvalidMessageFilterException extends PubSubException {
        public InvalidMessageFilterException(String msg) {
            super(StatusCode.INVALID_MESSAGE_FILTER, msg);
        }

        public InvalidMessageFilterException(String msg, Throwable t) {
            super(StatusCode.INVALID_MESSAGE_FILTER, msg, t);
        }
    }

    public static class UncertainStateException extends PubSubException {
        public UncertainStateException(String msg) {
            super(StatusCode.UNCERTAIN_STATE, msg);
        }
    }

    // The catch all exception (for unexpected error conditions)
    public static class UnexpectedConditionException extends PubSubException {
        public UnexpectedConditionException(String msg) {
            super(StatusCode.UNEXPECTED_CONDITION, msg);
        }
        public UnexpectedConditionException(String msg, Throwable t) {
            super(StatusCode.UNEXPECTED_CONDITION, msg, t);
        }
    }

    // The composite exception (for concurrent operations).
    public static class CompositeException extends PubSubException {
        private final Collection<PubSubException> exceptions;
        public CompositeException(Collection<PubSubException> exceptions) {
            super(StatusCode.COMPOSITE, compositeMessage(exceptions));
            this.exceptions = exceptions;
        }

        public Collection<PubSubException> getExceptions() {
            return exceptions;
        }

        /** Merges the message fields of the given Exceptions into a one line string. */
        private static String compositeMessage(Collection<PubSubException> exceptions) {
            StringBuilder builder = new StringBuilder("Composite exception: [");
            Iterator<PubSubException> iter = exceptions.iterator();
            if (iter.hasNext())
                builder.append(iter.next().getMessage());
            while (iter.hasNext())
                builder.append(" :: ").append(iter.next().getMessage());
            return builder.append("]").toString();
        }
    }

    public static class ClientNotSubscribedRuntimeException extends RuntimeException {
    }

}
"
hedwig-protocol/src/main/java/org/apache/hedwig/protocol/PubSubProtocol.java,false,"// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: src/main/protobuf/PubSubProtocol.proto

package org.apache.hedwig.protocol;

public final class PubSubProtocol {
  private PubSubProtocol() {}
  public static void registerAllExtensions(
      com.google.protobuf.ExtensionRegistry registry) {
  }
  public enum ProtocolVersion
      implements com.google.protobuf.ProtocolMessageEnum {
    VERSION_ONE(0, 1),
    ;
    
    public static final int VERSION_ONE_VALUE = 1;
    
    
    public final int getNumber() { return value; }
    
    public static ProtocolVersion valueOf(int value) {
      switch (value) {
        case 1: return VERSION_ONE;
        default: return null;
      }
    }
    
    public static com.google.protobuf.Internal.EnumLiteMap<ProtocolVersion>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static com.google.protobuf.Internal.EnumLiteMap<ProtocolVersion>
        internalValueMap =
          new com.google.protobuf.Internal.EnumLiteMap<ProtocolVersion>() {
            public ProtocolVersion findValueByNumber(int number) {
              return ProtocolVersion.valueOf(number);
            }
          };
    
    public final com.google.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      return getDescriptor().getValues().get(index);
    }
    public final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return org.apache.hedwig.protocol.PubSubProtocol.getDescriptor().getEnumTypes().get(0);
    }
    
    private static final ProtocolVersion[] VALUES = {
      VERSION_ONE, 
    };
    
    public static ProtocolVersion valueOf(
        com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      return VALUES[desc.getIndex()];
    }
    
    private final int index;
    private final int value;
    
    private ProtocolVersion(int index, int value) {
      this.index = index;
      this.value = value;
    }
    
    // @@protoc_insertion_point(enum_scope:Hedwig.ProtocolVersion)
  }
  
  public enum OperationType
      implements com.google.protobuf.ProtocolMessageEnum {
    PUBLISH(0, 0),
    SUBSCRIBE(1, 1),
    CONSUME(2, 2),
    UNSUBSCRIBE(3, 3),
    START_DELIVERY(4, 4),
    STOP_DELIVERY(5, 5),
    CLOSESUBSCRIPTION(6, 6),
    ;
    
    public static final int PUBLISH_VALUE = 0;
    public static final int SUBSCRIBE_VALUE = 1;
    public static final int CONSUME_VALUE = 2;
    public static final int UNSUBSCRIBE_VALUE = 3;
    public static final int START_DELIVERY_VALUE = 4;
    public static final int STOP_DELIVERY_VALUE = 5;
    public static final int CLOSESUBSCRIPTION_VALUE = 6;
    
    
    public final int getNumber() { return value; }
    
    public static OperationType valueOf(int value) {
      switch (value) {
        case 0: return PUBLISH;
        case 1: return SUBSCRIBE;
        case 2: return CONSUME;
        case 3: return UNSUBSCRIBE;
        case 4: return START_DELIVERY;
        case 5: return STOP_DELIVERY;
        case 6: return CLOSESUBSCRIPTION;
        default: return null;
      }
    }
    
    public static com.google.protobuf.Internal.EnumLiteMap<OperationType>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static com.google.protobuf.Internal.EnumLiteMap<OperationType>
        internalValueMap =
          new com.google.protobuf.Internal.EnumLiteMap<OperationType>() {
            public OperationType findValueByNumber(int number) {
              return OperationType.valueOf(number);
            }
          };
    
    public final com.google.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      return getDescriptor().getValues().get(index);
    }
    public final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return org.apache.hedwig.protocol.PubSubProtocol.getDescriptor().getEnumTypes().get(1);
    }
    
    private static final OperationType[] VALUES = {
      PUBLISH, SUBSCRIBE, CONSUME, UNSUBSCRIBE, START_DELIVERY, STOP_DELIVERY, CLOSESUBSCRIPTION, 
    };
    
    public static OperationType valueOf(
        com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      return VALUES[desc.getIndex()];
    }
    
    private final int index;
    private final int value;
    
    private OperationType(int index, int value) {
      this.index = index;
      this.value = value;
    }
    
    // @@protoc_insertion_point(enum_scope:Hedwig.OperationType)
  }
  
  public enum SubscriptionEvent
      implements com.google.protobuf.ProtocolMessageEnum {
    TOPIC_MOVED(0, 1),
    SUBSCRIPTION_FORCED_CLOSED(1, 2),
    ;
    
    public static final int TOPIC_MOVED_VALUE = 1;
    public static final int SUBSCRIPTION_FORCED_CLOSED_VALUE = 2;
    
    
    public final int getNumber() { return value; }
    
    public static SubscriptionEvent valueOf(int value) {
      switch (value) {
        case 1: return TOPIC_MOVED;
        case 2: return SUBSCRIPTION_FORCED_CLOSED;
        default: return null;
      }
    }
    
    public static com.google.protobuf.Internal.EnumLiteMap<SubscriptionEvent>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static com.google.protobuf.Internal.EnumLiteMap<SubscriptionEvent>
        internalValueMap =
          new com.google.protobuf.Internal.EnumLiteMap<SubscriptionEvent>() {
            public SubscriptionEvent findValueByNumber(int number) {
              return SubscriptionEvent.valueOf(number);
            }
          };
    
    public final com.google.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      return getDescriptor().getValues().get(index);
    }
    public final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return org.apache.hedwig.protocol.PubSubProtocol.getDescriptor().getEnumTypes().get(2);
    }
    
    private static final SubscriptionEvent[] VALUES = {
      TOPIC_MOVED, SUBSCRIPTION_FORCED_CLOSED, 
    };
    
    public static SubscriptionEvent valueOf(
        com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      return VALUES[desc.getIndex()];
    }
    
    private final int index;
    private final int value;
    
    private SubscriptionEvent(int index, int value) {
      this.index = index;
      this.value = value;
    }
    
    // @@protoc_insertion_point(enum_scope:Hedwig.SubscriptionEvent)
  }
  
  public enum StatusCode
      implements com.google.protobuf.ProtocolMessageEnum {
    SUCCESS(0, 0),
    MALFORMED_REQUEST(1, 401),
    NO_SUCH_TOPIC(2, 402),
    CLIENT_ALREADY_SUBSCRIBED(3, 403),
    CLIENT_NOT_SUBSCRIBED(4, 404),
    COULD_NOT_CONNECT(5, 405),
    TOPIC_BUSY(6, 406),
    RESUBSCRIBE_EXCEPTION(7, 407),
    NOT_RESPONSIBLE_FOR_TOPIC(8, 501),
    SERVICE_DOWN(9, 502),
    UNCERTAIN_STATE(10, 503),
    INVALID_MESSAGE_FILTER(11, 504),
    BAD_VERSION(12, 520),
    NO_TOPIC_PERSISTENCE_INFO(13, 521),
    TOPIC_PERSISTENCE_INFO_EXISTS(14, 522),
    NO_SUBSCRIPTION_STATE(15, 523),
    SUBSCRIPTION_STATE_EXISTS(16, 524),
    NO_TOPIC_OWNER_INFO(17, 525),
    TOPIC_OWNER_INFO_EXISTS(18, 526),
    UNEXPECTED_CONDITION(19, 600),
    COMPOSITE(20, 700),
    ;
    
    public static final int SUCCESS_VALUE = 0;
    public static final int MALFORMED_REQUEST_VALUE = 401;
    public static final int NO_SUCH_TOPIC_VALUE = 402;
    public static final int CLIENT_ALREADY_SUBSCRIBED_VALUE = 403;
    public static final int CLIENT_NOT_SUBSCRIBED_VALUE = 404;
    public static final int COULD_NOT_CONNECT_VALUE = 405;
    public static final int TOPIC_BUSY_VALUE = 406;
    public static final int RESUBSCRIBE_EXCEPTION_VALUE = 407;
    public static final int NOT_RESPONSIBLE_FOR_TOPIC_VALUE = 501;
    public static final int SERVICE_DOWN_VALUE = 502;
    public static final int UNCERTAIN_STATE_VALUE = 503;
    public static final int INVALID_MESSAGE_FILTER_VALUE = 504;
    public static final int BAD_VERSION_VALUE = 520;
    public static final int NO_TOPIC_PERSISTENCE_INFO_VALUE = 521;
    public static final int TOPIC_PERSISTENCE_INFO_EXISTS_VALUE = 522;
    public static final int NO_SUBSCRIPTION_STATE_VALUE = 523;
    public static final int SUBSCRIPTION_STATE_EXISTS_VALUE = 524;
    public static final int NO_TOPIC_OWNER_INFO_VALUE = 525;
    public static final int TOPIC_OWNER_INFO_EXISTS_VALUE = 526;
    public static final int UNEXPECTED_CONDITION_VALUE = 600;
    public static final int COMPOSITE_VALUE = 700;
    
    
    public final int getNumber() { return value; }
    
    public static StatusCode valueOf(int value) {
      switch (value) {
        case 0: return SUCCESS;
        case 401: return MALFORMED_REQUEST;
        case 402: return NO_SUCH_TOPIC;
        case 403: return CLIENT_ALREADY_SUBSCRIBED;
        case 404: return CLIENT_NOT_SUBSCRIBED;
        case 405: return COULD_NOT_CONNECT;
        case 406: return TOPIC_BUSY;
        case 407: return RESUBSCRIBE_EXCEPTION;
        case 501: return NOT_RESPONSIBLE_FOR_TOPIC;
        case 502: return SERVICE_DOWN;
        case 503: return UNCERTAIN_STATE;
        case 504: return INVALID_MESSAGE_FILTER;
        case 520: return BAD_VERSION;
        case 521: return NO_TOPIC_PERSISTENCE_INFO;
        case 522: return TOPIC_PERSISTENCE_INFO_EXISTS;
        case 523: return NO_SUBSCRIPTION_STATE;
        case 524: return SUBSCRIPTION_STATE_EXISTS;
        case 525: return NO_TOPIC_OWNER_INFO;
        case 526: return TOPIC_OWNER_INFO_EXISTS;
        case 600: return UNEXPECTED_CONDITION;
        case 700: return COMPOSITE;
        default: return null;
      }
    }
    
    public static com.google.protobuf.Internal.EnumLiteMap<StatusCode>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static com.google.protobuf.Internal.EnumLiteMap<StatusCode>
        internalValueMap =
          new com.google.protobuf.Internal.EnumLiteMap<StatusCode>() {
            public StatusCode findValueByNumber(int number) {
              return StatusCode.valueOf(number);
            }
          };
    
    public final com.google.protobuf.Descriptors.EnumValueDescriptor
        getValueDescriptor() {
      return getDescriptor().getValues().get(index);
    }
    public final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptorForType() {
      return getDescriptor();
    }
    public static final com.google.protobuf.Descriptors.EnumDescriptor
        getDescriptor() {
      return org.apache.hedwig.protocol.PubSubProtocol.getDescriptor().getEnumTypes().get(3);
    }
    
    private static final StatusCode[] VALUES = {
      SUCCESS, MALFORMED_REQUEST, NO_SUCH_TOPIC, CLIENT_ALREADY_SUBSCRIBED, CLIENT_NOT_SUBSCRIBED, COULD_NOT_CONNECT, TOPIC_BUSY, RESUBSCRIBE_EXCEPTION, NOT_RESPONSIBLE_FOR_TOPIC, SERVICE_DOWN, UNCERTAIN_STATE, INVALID_MESSAGE_FILTER, BAD_VERSION, NO_TOPIC_PERSISTENCE_INFO, TOPIC_PERSISTENCE_INFO_EXISTS, NO_SUBSCRIPTION_STATE, SUBSCRIPTION_STATE_EXISTS, NO_TOPIC_OWNER_INFO, TOPIC_OWNER_INFO_EXISTS, UNEXPECTED_CONDITION, COMPOSITE, 
    };
    
    public static StatusCode valueOf(
        com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
      if (desc.getType() != getDescriptor()) {
        throw new java.lang.IllegalArgumentException(
          "EnumValueDescriptor is not for this type.");
      }
      return VALUES[desc.getIndex()];
    }
    
    private final int index;
    private final int value;
    
    private StatusCode(int index, int value) {
      this.index = index;
      this.value = value;
    }
    
    // @@protoc_insertion_point(enum_scope:Hedwig.StatusCode)
  }
  
  public interface MapOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // repeated .Hedwig.Map.Entry entries = 1;
    java.util.List<org.apache.hedwig.protocol.PubSubProtocol.Map.Entry> 
        getEntriesList();
    org.apache.hedwig.protocol.PubSubProtocol.Map.Entry getEntries(int index);
    int getEntriesCount();
    java.util.List<? extends org.apache.hedwig.protocol.PubSubProtocol.Map.EntryOrBuilder> 
        getEntriesOrBuilderList();
    org.apache.hedwig.protocol.PubSubProtocol.Map.EntryOrBuilder getEntriesOrBuilder(
        int index);
  }
  public static final class Map extends
      com.google.protobuf.GeneratedMessage
      implements MapOrBuilder {
    // Use Map.newBuilder() to construct.
    private Map(Builder builder) {
      super(builder);
    }
    private Map(boolean noInit) {}
    
    private static final Map defaultInstance;
    public static Map getDefaultInstance() {
      return defaultInstance;
    }
    
    public Map getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_Map_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_Map_fieldAccessorTable;
    }
    
    public interface EntryOrBuilder
        extends com.google.protobuf.MessageOrBuilder {
      
      // optional string key = 1;
      boolean hasKey();
      String getKey();
      
      // optional bytes value = 2;
      boolean hasValue();
      com.google.protobuf.ByteString getValue();
    }
    public static final class Entry extends
        com.google.protobuf.GeneratedMessage
        implements EntryOrBuilder {
      // Use Entry.newBuilder() to construct.
      private Entry(Builder builder) {
        super(builder);
      }
      private Entry(boolean noInit) {}
      
      private static final Entry defaultInstance;
      public static Entry getDefaultInstance() {
        return defaultInstance;
      }
      
      public Entry getDefaultInstanceForType() {
        return defaultInstance;
      }
      
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_Map_Entry_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_Map_Entry_fieldAccessorTable;
      }
      
      private int bitField0_;
      // optional string key = 1;
      public static final int KEY_FIELD_NUMBER = 1;
      private java.lang.Object key_;
      public boolean hasKey() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public String getKey() {
        java.lang.Object ref = key_;
        if (ref instanceof String) {
          return (String) ref;
        } else {
          com.google.protobuf.ByteString bs = 
              (com.google.protobuf.ByteString) ref;
          String s = bs.toStringUtf8();
          if (com.google.protobuf.Internal.isValidUtf8(bs)) {
            key_ = s;
          }
          return s;
        }
      }
      private com.google.protobuf.ByteString getKeyBytes() {
        java.lang.Object ref = key_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8((String) ref);
          key_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      
      // optional bytes value = 2;
      public static final int VALUE_FIELD_NUMBER = 2;
      private com.google.protobuf.ByteString value_;
      public boolean hasValue() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      public com.google.protobuf.ByteString getValue() {
        return value_;
      }
      
      private void initFields() {
        key_ = "";
        value_ = com.google.protobuf.ByteString.EMPTY;
      }
      private byte memoizedIsInitialized = -1;
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized != -1) return isInitialized == 1;
        
        memoizedIsInitialized = 1;
        return true;
      }
      
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        getSerializedSize();
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          output.writeBytes(1, getKeyBytes());
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          output.writeBytes(2, value_);
        }
        getUnknownFields().writeTo(output);
      }
      
      private int memoizedSerializedSize = -1;
      public int getSerializedSize() {
        int size = memoizedSerializedSize;
        if (size != -1) return size;
      
        size = 0;
        if (((bitField0_ & 0x00000001) == 0x00000001)) {
          size += com.google.protobuf.CodedOutputStream
            .computeBytesSize(1, getKeyBytes());
        }
        if (((bitField0_ & 0x00000002) == 0x00000002)) {
          size += com.google.protobuf.CodedOutputStream
            .computeBytesSize(2, value_);
        }
        size += getUnknownFields().getSerializedSize();
        memoizedSerializedSize = size;
        return size;
      }
      
      private static final long serialVersionUID = 0L;
      @java.lang.Override
      protected java.lang.Object writeReplace()
          throws java.io.ObjectStreamException {
        return super.writeReplace();
      }
      
      public static org.apache.hedwig.protocol.PubSubProtocol.Map.Entry parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return newBuilder().mergeFrom(data).buildParsed();
      }
      public static org.apache.hedwig.protocol.PubSubProtocol.Map.Entry parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return newBuilder().mergeFrom(data, extensionRegistry)
                 .buildParsed();
      }
      public static org.apache.hedwig.protocol.PubSubProtocol.Map.Entry parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return newBuilder().mergeFrom(data).buildParsed();
      }
      public static org.apache.hedwig.protocol.PubSubProtocol.Map.Entry parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return newBuilder().mergeFrom(data, extensionRegistry)
                 .buildParsed();
      }
      public static org.apache.hedwig.protocol.PubSubProtocol.Map.Entry parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return newBuilder().mergeFrom(input).buildParsed();
      }
      public static org.apache.hedwig.protocol.PubSubProtocol.Map.Entry parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return newBuilder().mergeFrom(input, extensionRegistry)
                 .buildParsed();
      }
      public static org.apache.hedwig.protocol.PubSubProtocol.Map.Entry parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        Builder builder = newBuilder();
        if (builder.mergeDelimitedFrom(input)) {
          return builder.buildParsed();
        } else {
          return null;
        }
      }
      public static org.apache.hedwig.protocol.PubSubProtocol.Map.Entry parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        Builder builder = newBuilder();
        if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
          return builder.buildParsed();
        } else {
          return null;
        }
      }
      public static org.apache.hedwig.protocol.PubSubProtocol.Map.Entry parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return newBuilder().mergeFrom(input).buildParsed();
      }
      public static org.apache.hedwig.protocol.PubSubProtocol.Map.Entry parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return newBuilder().mergeFrom(input, extensionRegistry)
                 .buildParsed();
      }
      
      public static Builder newBuilder() { return Builder.create(); }
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.Map.Entry prototype) {
        return newBuilder().mergeFrom(prototype);
      }
      public Builder toBuilder() { return newBuilder(this); }
      
      @java.lang.Override
      protected Builder newBuilderForType(
          com.google.protobuf.GeneratedMessage.BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      public static final class Builder extends
          com.google.protobuf.GeneratedMessage.Builder<Builder>
         implements org.apache.hedwig.protocol.PubSubProtocol.Map.EntryOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_Map_Entry_descriptor;
        }
        
        protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
            internalGetFieldAccessorTable() {
          return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_Map_Entry_fieldAccessorTable;
        }
        
        // Construct using org.apache.hedwig.protocol.PubSubProtocol.Map.Entry.newBuilder()
        private Builder() {
          maybeForceBuilderInitialization();
        }
        
        private Builder(BuilderParent parent) {
          super(parent);
          maybeForceBuilderInitialization();
        }
        private void maybeForceBuilderInitialization() {
          if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          }
        }
        private static Builder create() {
          return new Builder();
        }
        
        public Builder clear() {
          super.clear();
          key_ = "";
          bitField0_ = (bitField0_ & ~0x00000001);
          value_ = com.google.protobuf.ByteString.EMPTY;
          bitField0_ = (bitField0_ & ~0x00000002);
          return this;
        }
        
        public Builder clone() {
          return create().mergeFrom(buildPartial());
        }
        
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return org.apache.hedwig.protocol.PubSubProtocol.Map.Entry.getDescriptor();
        }
        
        public org.apache.hedwig.protocol.PubSubProtocol.Map.Entry getDefaultInstanceForType() {
          return org.apache.hedwig.protocol.PubSubProtocol.Map.Entry.getDefaultInstance();
        }
        
        public org.apache.hedwig.protocol.PubSubProtocol.Map.Entry build() {
          org.apache.hedwig.protocol.PubSubProtocol.Map.Entry result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }
        
        private org.apache.hedwig.protocol.PubSubProtocol.Map.Entry buildParsed()
            throws com.google.protobuf.InvalidProtocolBufferException {
          org.apache.hedwig.protocol.PubSubProtocol.Map.Entry result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(
              result).asInvalidProtocolBufferException();
          }
          return result;
        }
        
        public org.apache.hedwig.protocol.PubSubProtocol.Map.Entry buildPartial() {
          org.apache.hedwig.protocol.PubSubProtocol.Map.Entry result = new org.apache.hedwig.protocol.PubSubProtocol.Map.Entry(this);
          int from_bitField0_ = bitField0_;
          int to_bitField0_ = 0;
          if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
            to_bitField0_ |= 0x00000001;
          }
          result.key_ = key_;
          if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
            to_bitField0_ |= 0x00000002;
          }
          result.value_ = value_;
          result.bitField0_ = to_bitField0_;
          onBuilt();
          return result;
        }
        
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.Map.Entry) {
            return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.Map.Entry)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }
        
        public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.Map.Entry other) {
          if (other == org.apache.hedwig.protocol.PubSubProtocol.Map.Entry.getDefaultInstance()) return this;
          if (other.hasKey()) {
            setKey(other.getKey());
          }
          if (other.hasValue()) {
            setValue(other.getValue());
          }
          this.mergeUnknownFields(other.getUnknownFields());
          return this;
        }
        
        public final boolean isInitialized() {
          return true;
        }
        
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          com.google.protobuf.UnknownFieldSet.Builder unknownFields =
            com.google.protobuf.UnknownFieldSet.newBuilder(
              this.getUnknownFields());
          while (true) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              default: {
                if (!parseUnknownField(input, unknownFields,
                                       extensionRegistry, tag)) {
                  this.setUnknownFields(unknownFields.build());
                  onChanged();
                  return this;
                }
                break;
              }
              case 10: {
                bitField0_ |= 0x00000001;
                key_ = input.readBytes();
                break;
              }
              case 18: {
                bitField0_ |= 0x00000002;
                value_ = input.readBytes();
                break;
              }
            }
          }
        }
        
        private int bitField0_;
        
        // optional string key = 1;
        private java.lang.Object key_ = "";
        public boolean hasKey() {
          return ((bitField0_ & 0x00000001) == 0x00000001);
        }
        public String getKey() {
          java.lang.Object ref = key_;
          if (!(ref instanceof String)) {
            String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
            key_ = s;
            return s;
          } else {
            return (String) ref;
          }
        }
        public Builder setKey(String value) {
          if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
          key_ = value;
          onChanged();
          return this;
        }
        public Builder clearKey() {
          bitField0_ = (bitField0_ & ~0x00000001);
          key_ = getDefaultInstance().getKey();
          onChanged();
          return this;
        }
        void setKey(com.google.protobuf.ByteString value) {
          bitField0_ |= 0x00000001;
          key_ = value;
          onChanged();
        }
        
        // optional bytes value = 2;
        private com.google.protobuf.ByteString value_ = com.google.protobuf.ByteString.EMPTY;
        public boolean hasValue() {
          return ((bitField0_ & 0x00000002) == 0x00000002);
        }
        public com.google.protobuf.ByteString getValue() {
          return value_;
        }
        public Builder setValue(com.google.protobuf.ByteString value) {
          if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
          value_ = value;
          onChanged();
          return this;
        }
        public Builder clearValue() {
          bitField0_ = (bitField0_ & ~0x00000002);
          value_ = getDefaultInstance().getValue();
          onChanged();
          return this;
        }
        
        // @@protoc_insertion_point(builder_scope:Hedwig.Map.Entry)
      }
      
      static {
        defaultInstance = new Entry(true);
        defaultInstance.initFields();
      }
      
      // @@protoc_insertion_point(class_scope:Hedwig.Map.Entry)
    }
    
    // repeated .Hedwig.Map.Entry entries = 1;
    public static final int ENTRIES_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hedwig.protocol.PubSubProtocol.Map.Entry> entries_;
    public java.util.List<org.apache.hedwig.protocol.PubSubProtocol.Map.Entry> getEntriesList() {
      return entries_;
    }
    public java.util.List<? extends org.apache.hedwig.protocol.PubSubProtocol.Map.EntryOrBuilder> 
        getEntriesOrBuilderList() {
      return entries_;
    }
    public int getEntriesCount() {
      return entries_.size();
    }
    public org.apache.hedwig.protocol.PubSubProtocol.Map.Entry getEntries(int index) {
      return entries_.get(index);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.Map.EntryOrBuilder getEntriesOrBuilder(
        int index) {
      return entries_.get(index);
    }
    
    private void initFields() {
      entries_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < entries_.size(); i++) {
        output.writeMessage(1, entries_.get(i));
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      for (int i = 0; i < entries_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, entries_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.hedwig.protocol.PubSubProtocol.Map parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.Map parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.Map parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.Map parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.Map parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.Map parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.Map parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.Map parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.Map parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.Map parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.Map prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hedwig.protocol.PubSubProtocol.MapOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_Map_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_Map_fieldAccessorTable;
      }
      
      // Construct using org.apache.hedwig.protocol.PubSubProtocol.Map.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getEntriesFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        if (entriesBuilder_ == null) {
          entries_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          entriesBuilder_.clear();
        }
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.Map.getDescriptor();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.Map getDefaultInstanceForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.Map.getDefaultInstance();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.Map build() {
        org.apache.hedwig.protocol.PubSubProtocol.Map result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.hedwig.protocol.PubSubProtocol.Map buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.hedwig.protocol.PubSubProtocol.Map result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.Map buildPartial() {
        org.apache.hedwig.protocol.PubSubProtocol.Map result = new org.apache.hedwig.protocol.PubSubProtocol.Map(this);
        int from_bitField0_ = bitField0_;
        if (entriesBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            entries_ = java.util.Collections.unmodifiableList(entries_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.entries_ = entries_;
        } else {
          result.entries_ = entriesBuilder_.build();
        }
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.Map) {
          return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.Map)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.Map other) {
        if (other == org.apache.hedwig.protocol.PubSubProtocol.Map.getDefaultInstance()) return this;
        if (entriesBuilder_ == null) {
          if (!other.entries_.isEmpty()) {
            if (entries_.isEmpty()) {
              entries_ = other.entries_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureEntriesIsMutable();
              entries_.addAll(other.entries_);
            }
            onChanged();
          }
        } else {
          if (!other.entries_.isEmpty()) {
            if (entriesBuilder_.isEmpty()) {
              entriesBuilder_.dispose();
              entriesBuilder_ = null;
              entries_ = other.entries_;
              bitField0_ = (bitField0_ & ~0x00000001);
              entriesBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getEntriesFieldBuilder() : null;
            } else {
              entriesBuilder_.addAllMessages(other.entries_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 10: {
              org.apache.hedwig.protocol.PubSubProtocol.Map.Entry.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.Map.Entry.newBuilder();
              input.readMessage(subBuilder, extensionRegistry);
              addEntries(subBuilder.buildPartial());
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // repeated .Hedwig.Map.Entry entries = 1;
      private java.util.List<org.apache.hedwig.protocol.PubSubProtocol.Map.Entry> entries_ =
        java.util.Collections.emptyList();
      private void ensureEntriesIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          entries_ = new java.util.ArrayList<org.apache.hedwig.protocol.PubSubProtocol.Map.Entry>(entries_);
          bitField0_ |= 0x00000001;
         }
      }
      
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.Map.Entry, org.apache.hedwig.protocol.PubSubProtocol.Map.Entry.Builder, org.apache.hedwig.protocol.PubSubProtocol.Map.EntryOrBuilder> entriesBuilder_;
      
      public java.util.List<org.apache.hedwig.protocol.PubSubProtocol.Map.Entry> getEntriesList() {
        if (entriesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(entries_);
        } else {
          return entriesBuilder_.getMessageList();
        }
      }
      public int getEntriesCount() {
        if (entriesBuilder_ == null) {
          return entries_.size();
        } else {
          return entriesBuilder_.getCount();
        }
      }
      public org.apache.hedwig.protocol.PubSubProtocol.Map.Entry getEntries(int index) {
        if (entriesBuilder_ == null) {
          return entries_.get(index);
        } else {
          return entriesBuilder_.getMessage(index);
        }
      }
      public Builder setEntries(
          int index, org.apache.hedwig.protocol.PubSubProtocol.Map.Entry value) {
        if (entriesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureEntriesIsMutable();
          entries_.set(index, value);
          onChanged();
        } else {
          entriesBuilder_.setMessage(index, value);
        }
        return this;
      }
      public Builder setEntries(
          int index, org.apache.hedwig.protocol.PubSubProtocol.Map.Entry.Builder builderForValue) {
        if (entriesBuilder_ == null) {
          ensureEntriesIsMutable();
          entries_.set(index, builderForValue.build());
          onChanged();
        } else {
          entriesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      public Builder addEntries(org.apache.hedwig.protocol.PubSubProtocol.Map.Entry value) {
        if (entriesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureEntriesIsMutable();
          entries_.add(value);
          onChanged();
        } else {
          entriesBuilder_.addMessage(value);
        }
        return this;
      }
      public Builder addEntries(
          int index, org.apache.hedwig.protocol.PubSubProtocol.Map.Entry value) {
        if (entriesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureEntriesIsMutable();
          entries_.add(index, value);
          onChanged();
        } else {
          entriesBuilder_.addMessage(index, value);
        }
        return this;
      }
      public Builder addEntries(
          org.apache.hedwig.protocol.PubSubProtocol.Map.Entry.Builder builderForValue) {
        if (entriesBuilder_ == null) {
          ensureEntriesIsMutable();
          entries_.add(builderForValue.build());
          onChanged();
        } else {
          entriesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      public Builder addEntries(
          int index, org.apache.hedwig.protocol.PubSubProtocol.Map.Entry.Builder builderForValue) {
        if (entriesBuilder_ == null) {
          ensureEntriesIsMutable();
          entries_.add(index, builderForValue.build());
          onChanged();
        } else {
          entriesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      public Builder addAllEntries(
          java.lang.Iterable<? extends org.apache.hedwig.protocol.PubSubProtocol.Map.Entry> values) {
        if (entriesBuilder_ == null) {
          ensureEntriesIsMutable();
          super.addAll(values, entries_);
          onChanged();
        } else {
          entriesBuilder_.addAllMessages(values);
        }
        return this;
      }
      public Builder clearEntries() {
        if (entriesBuilder_ == null) {
          entries_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          entriesBuilder_.clear();
        }
        return this;
      }
      public Builder removeEntries(int index) {
        if (entriesBuilder_ == null) {
          ensureEntriesIsMutable();
          entries_.remove(index);
          onChanged();
        } else {
          entriesBuilder_.remove(index);
        }
        return this;
      }
      public org.apache.hedwig.protocol.PubSubProtocol.Map.Entry.Builder getEntriesBuilder(
          int index) {
        return getEntriesFieldBuilder().getBuilder(index);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.Map.EntryOrBuilder getEntriesOrBuilder(
          int index) {
        if (entriesBuilder_ == null) {
          return entries_.get(index);  } else {
          return entriesBuilder_.getMessageOrBuilder(index);
        }
      }
      public java.util.List<? extends org.apache.hedwig.protocol.PubSubProtocol.Map.EntryOrBuilder> 
           getEntriesOrBuilderList() {
        if (entriesBuilder_ != null) {
          return entriesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(entries_);
        }
      }
      public org.apache.hedwig.protocol.PubSubProtocol.Map.Entry.Builder addEntriesBuilder() {
        return getEntriesFieldBuilder().addBuilder(
            org.apache.hedwig.protocol.PubSubProtocol.Map.Entry.getDefaultInstance());
      }
      public org.apache.hedwig.protocol.PubSubProtocol.Map.Entry.Builder addEntriesBuilder(
          int index) {
        return getEntriesFieldBuilder().addBuilder(
            index, org.apache.hedwig.protocol.PubSubProtocol.Map.Entry.getDefaultInstance());
      }
      public java.util.List<org.apache.hedwig.protocol.PubSubProtocol.Map.Entry.Builder> 
           getEntriesBuilderList() {
        return getEntriesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.Map.Entry, org.apache.hedwig.protocol.PubSubProtocol.Map.Entry.Builder, org.apache.hedwig.protocol.PubSubProtocol.Map.EntryOrBuilder> 
          getEntriesFieldBuilder() {
        if (entriesBuilder_ == null) {
          entriesBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hedwig.protocol.PubSubProtocol.Map.Entry, org.apache.hedwig.protocol.PubSubProtocol.Map.Entry.Builder, org.apache.hedwig.protocol.PubSubProtocol.Map.EntryOrBuilder>(
                  entries_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          entries_ = null;
        }
        return entriesBuilder_;
      }
      
      // @@protoc_insertion_point(builder_scope:Hedwig.Map)
    }
    
    static {
      defaultInstance = new Map(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:Hedwig.Map)
  }
  
  public interface MessageHeaderOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // optional .Hedwig.Map properties = 1;
    boolean hasProperties();
    org.apache.hedwig.protocol.PubSubProtocol.Map getProperties();
    org.apache.hedwig.protocol.PubSubProtocol.MapOrBuilder getPropertiesOrBuilder();
    
    // optional string messageType = 2;
    boolean hasMessageType();
    String getMessageType();
  }
  public static final class MessageHeader extends
      com.google.protobuf.GeneratedMessage
      implements MessageHeaderOrBuilder {
    // Use MessageHeader.newBuilder() to construct.
    private MessageHeader(Builder builder) {
      super(builder);
    }
    private MessageHeader(boolean noInit) {}
    
    private static final MessageHeader defaultInstance;
    public static MessageHeader getDefaultInstance() {
      return defaultInstance;
    }
    
    public MessageHeader getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_MessageHeader_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_MessageHeader_fieldAccessorTable;
    }
    
    private int bitField0_;
    // optional .Hedwig.Map properties = 1;
    public static final int PROPERTIES_FIELD_NUMBER = 1;
    private org.apache.hedwig.protocol.PubSubProtocol.Map properties_;
    public boolean hasProperties() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.Map getProperties() {
      return properties_;
    }
    public org.apache.hedwig.protocol.PubSubProtocol.MapOrBuilder getPropertiesOrBuilder() {
      return properties_;
    }
    
    // optional string messageType = 2;
    public static final int MESSAGETYPE_FIELD_NUMBER = 2;
    private java.lang.Object messageType_;
    public boolean hasMessageType() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    public String getMessageType() {
      java.lang.Object ref = messageType_;
      if (ref instanceof String) {
        return (String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        String s = bs.toStringUtf8();
        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
          messageType_ = s;
        }
        return s;
      }
    }
    private com.google.protobuf.ByteString getMessageTypeBytes() {
      java.lang.Object ref = messageType_;
      if (ref instanceof String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
        messageType_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }
    
    private void initFields() {
      properties_ = org.apache.hedwig.protocol.PubSubProtocol.Map.getDefaultInstance();
      messageType_ = "";
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, properties_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, getMessageTypeBytes());
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, properties_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, getMessageTypeBytes());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.hedwig.protocol.PubSubProtocol.MessageHeader parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.MessageHeader parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.MessageHeader parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.MessageHeader parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.MessageHeader parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.MessageHeader parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.MessageHeader parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.MessageHeader parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.MessageHeader parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.MessageHeader parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.MessageHeader prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hedwig.protocol.PubSubProtocol.MessageHeaderOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_MessageHeader_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_MessageHeader_fieldAccessorTable;
      }
      
      // Construct using org.apache.hedwig.protocol.PubSubProtocol.MessageHeader.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getPropertiesFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        if (propertiesBuilder_ == null) {
          properties_ = org.apache.hedwig.protocol.PubSubProtocol.Map.getDefaultInstance();
        } else {
          propertiesBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        messageType_ = "";
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.MessageHeader.getDescriptor();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.MessageHeader getDefaultInstanceForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.MessageHeader.getDefaultInstance();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.MessageHeader build() {
        org.apache.hedwig.protocol.PubSubProtocol.MessageHeader result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.hedwig.protocol.PubSubProtocol.MessageHeader buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.hedwig.protocol.PubSubProtocol.MessageHeader result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.MessageHeader buildPartial() {
        org.apache.hedwig.protocol.PubSubProtocol.MessageHeader result = new org.apache.hedwig.protocol.PubSubProtocol.MessageHeader(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (propertiesBuilder_ == null) {
          result.properties_ = properties_;
        } else {
          result.properties_ = propertiesBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.messageType_ = messageType_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.MessageHeader) {
          return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.MessageHeader)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.MessageHeader other) {
        if (other == org.apache.hedwig.protocol.PubSubProtocol.MessageHeader.getDefaultInstance()) return this;
        if (other.hasProperties()) {
          mergeProperties(other.getProperties());
        }
        if (other.hasMessageType()) {
          setMessageType(other.getMessageType());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 10: {
              org.apache.hedwig.protocol.PubSubProtocol.Map.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.Map.newBuilder();
              if (hasProperties()) {
                subBuilder.mergeFrom(getProperties());
              }
              input.readMessage(subBuilder, extensionRegistry);
              setProperties(subBuilder.buildPartial());
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              messageType_ = input.readBytes();
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // optional .Hedwig.Map properties = 1;
      private org.apache.hedwig.protocol.PubSubProtocol.Map properties_ = org.apache.hedwig.protocol.PubSubProtocol.Map.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.Map, org.apache.hedwig.protocol.PubSubProtocol.Map.Builder, org.apache.hedwig.protocol.PubSubProtocol.MapOrBuilder> propertiesBuilder_;
      public boolean hasProperties() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.Map getProperties() {
        if (propertiesBuilder_ == null) {
          return properties_;
        } else {
          return propertiesBuilder_.getMessage();
        }
      }
      public Builder setProperties(org.apache.hedwig.protocol.PubSubProtocol.Map value) {
        if (propertiesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          properties_ = value;
          onChanged();
        } else {
          propertiesBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      public Builder setProperties(
          org.apache.hedwig.protocol.PubSubProtocol.Map.Builder builderForValue) {
        if (propertiesBuilder_ == null) {
          properties_ = builderForValue.build();
          onChanged();
        } else {
          propertiesBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      public Builder mergeProperties(org.apache.hedwig.protocol.PubSubProtocol.Map value) {
        if (propertiesBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              properties_ != org.apache.hedwig.protocol.PubSubProtocol.Map.getDefaultInstance()) {
            properties_ =
              org.apache.hedwig.protocol.PubSubProtocol.Map.newBuilder(properties_).mergeFrom(value).buildPartial();
          } else {
            properties_ = value;
          }
          onChanged();
        } else {
          propertiesBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      public Builder clearProperties() {
        if (propertiesBuilder_ == null) {
          properties_ = org.apache.hedwig.protocol.PubSubProtocol.Map.getDefaultInstance();
          onChanged();
        } else {
          propertiesBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      public org.apache.hedwig.protocol.PubSubProtocol.Map.Builder getPropertiesBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getPropertiesFieldBuilder().getBuilder();
      }
      public org.apache.hedwig.protocol.PubSubProtocol.MapOrBuilder getPropertiesOrBuilder() {
        if (propertiesBuilder_ != null) {
          return propertiesBuilder_.getMessageOrBuilder();
        } else {
          return properties_;
        }
      }
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.Map, org.apache.hedwig.protocol.PubSubProtocol.Map.Builder, org.apache.hedwig.protocol.PubSubProtocol.MapOrBuilder> 
          getPropertiesFieldBuilder() {
        if (propertiesBuilder_ == null) {
          propertiesBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hedwig.protocol.PubSubProtocol.Map, org.apache.hedwig.protocol.PubSubProtocol.Map.Builder, org.apache.hedwig.protocol.PubSubProtocol.MapOrBuilder>(
                  properties_,
                  getParentForChildren(),
                  isClean());
          properties_ = null;
        }
        return propertiesBuilder_;
      }
      
      // optional string messageType = 2;
      private java.lang.Object messageType_ = "";
      public boolean hasMessageType() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      public String getMessageType() {
        java.lang.Object ref = messageType_;
        if (!(ref instanceof String)) {
          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
          messageType_ = s;
          return s;
        } else {
          return (String) ref;
        }
      }
      public Builder setMessageType(String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        messageType_ = value;
        onChanged();
        return this;
      }
      public Builder clearMessageType() {
        bitField0_ = (bitField0_ & ~0x00000002);
        messageType_ = getDefaultInstance().getMessageType();
        onChanged();
        return this;
      }
      void setMessageType(com.google.protobuf.ByteString value) {
        bitField0_ |= 0x00000002;
        messageType_ = value;
        onChanged();
      }
      
      // @@protoc_insertion_point(builder_scope:Hedwig.MessageHeader)
    }
    
    static {
      defaultInstance = new MessageHeader(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:Hedwig.MessageHeader)
  }
  
  public interface MessageOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // required bytes body = 1;
    boolean hasBody();
    com.google.protobuf.ByteString getBody();
    
    // optional bytes srcRegion = 2;
    boolean hasSrcRegion();
    com.google.protobuf.ByteString getSrcRegion();
    
    // optional .Hedwig.MessageSeqId msgId = 3;
    boolean hasMsgId();
    org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId getMsgId();
    org.apache.hedwig.protocol.PubSubProtocol.MessageSeqIdOrBuilder getMsgIdOrBuilder();
    
    // optional .Hedwig.MessageHeader header = 4;
    boolean hasHeader();
    org.apache.hedwig.protocol.PubSubProtocol.MessageHeader getHeader();
    org.apache.hedwig.protocol.PubSubProtocol.MessageHeaderOrBuilder getHeaderOrBuilder();
  }
  public static final class Message extends
      com.google.protobuf.GeneratedMessage
      implements MessageOrBuilder {
    // Use Message.newBuilder() to construct.
    private Message(Builder builder) {
      super(builder);
    }
    private Message(boolean noInit) {}
    
    private static final Message defaultInstance;
    public static Message getDefaultInstance() {
      return defaultInstance;
    }
    
    public Message getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_Message_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_Message_fieldAccessorTable;
    }
    
    private int bitField0_;
    // required bytes body = 1;
    public static final int BODY_FIELD_NUMBER = 1;
    private com.google.protobuf.ByteString body_;
    public boolean hasBody() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    public com.google.protobuf.ByteString getBody() {
      return body_;
    }
    
    // optional bytes srcRegion = 2;
    public static final int SRCREGION_FIELD_NUMBER = 2;
    private com.google.protobuf.ByteString srcRegion_;
    public boolean hasSrcRegion() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    public com.google.protobuf.ByteString getSrcRegion() {
      return srcRegion_;
    }
    
    // optional .Hedwig.MessageSeqId msgId = 3;
    public static final int MSGID_FIELD_NUMBER = 3;
    private org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId msgId_;
    public boolean hasMsgId() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId getMsgId() {
      return msgId_;
    }
    public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqIdOrBuilder getMsgIdOrBuilder() {
      return msgId_;
    }
    
    // optional .Hedwig.MessageHeader header = 4;
    public static final int HEADER_FIELD_NUMBER = 4;
    private org.apache.hedwig.protocol.PubSubProtocol.MessageHeader header_;
    public boolean hasHeader() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.MessageHeader getHeader() {
      return header_;
    }
    public org.apache.hedwig.protocol.PubSubProtocol.MessageHeaderOrBuilder getHeaderOrBuilder() {
      return header_;
    }
    
    private void initFields() {
      body_ = com.google.protobuf.ByteString.EMPTY;
      srcRegion_ = com.google.protobuf.ByteString.EMPTY;
      msgId_ = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance();
      header_ = org.apache.hedwig.protocol.PubSubProtocol.MessageHeader.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      if (!hasBody()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (hasMsgId()) {
        if (!getMsgId().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, body_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeBytes(2, srcRegion_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeMessage(3, msgId_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeMessage(4, header_);
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, body_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, srcRegion_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, msgId_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(4, header_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.hedwig.protocol.PubSubProtocol.Message parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.Message parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.Message parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.Message parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.Message parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.Message parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.Message parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.Message parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.Message parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.Message parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.Message prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hedwig.protocol.PubSubProtocol.MessageOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_Message_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_Message_fieldAccessorTable;
      }
      
      // Construct using org.apache.hedwig.protocol.PubSubProtocol.Message.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getMsgIdFieldBuilder();
          getHeaderFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        body_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        srcRegion_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000002);
        if (msgIdBuilder_ == null) {
          msgId_ = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance();
        } else {
          msgIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        if (headerBuilder_ == null) {
          header_ = org.apache.hedwig.protocol.PubSubProtocol.MessageHeader.getDefaultInstance();
        } else {
          headerBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000008);
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.Message.getDescriptor();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.Message getDefaultInstanceForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.Message.getDefaultInstance();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.Message build() {
        org.apache.hedwig.protocol.PubSubProtocol.Message result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.hedwig.protocol.PubSubProtocol.Message buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.hedwig.protocol.PubSubProtocol.Message result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.Message buildPartial() {
        org.apache.hedwig.protocol.PubSubProtocol.Message result = new org.apache.hedwig.protocol.PubSubProtocol.Message(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.body_ = body_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.srcRegion_ = srcRegion_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        if (msgIdBuilder_ == null) {
          result.msgId_ = msgId_;
        } else {
          result.msgId_ = msgIdBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        if (headerBuilder_ == null) {
          result.header_ = header_;
        } else {
          result.header_ = headerBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.Message) {
          return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.Message)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.Message other) {
        if (other == org.apache.hedwig.protocol.PubSubProtocol.Message.getDefaultInstance()) return this;
        if (other.hasBody()) {
          setBody(other.getBody());
        }
        if (other.hasSrcRegion()) {
          setSrcRegion(other.getSrcRegion());
        }
        if (other.hasMsgId()) {
          mergeMsgId(other.getMsgId());
        }
        if (other.hasHeader()) {
          mergeHeader(other.getHeader());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        if (!hasBody()) {
          
          return false;
        }
        if (hasMsgId()) {
          if (!getMsgId().isInitialized()) {
            
            return false;
          }
        }
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              body_ = input.readBytes();
              break;
            }
            case 18: {
              bitField0_ |= 0x00000002;
              srcRegion_ = input.readBytes();
              break;
            }
            case 26: {
              org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.newBuilder();
              if (hasMsgId()) {
                subBuilder.mergeFrom(getMsgId());
              }
              input.readMessage(subBuilder, extensionRegistry);
              setMsgId(subBuilder.buildPartial());
              break;
            }
            case 34: {
              org.apache.hedwig.protocol.PubSubProtocol.MessageHeader.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.MessageHeader.newBuilder();
              if (hasHeader()) {
                subBuilder.mergeFrom(getHeader());
              }
              input.readMessage(subBuilder, extensionRegistry);
              setHeader(subBuilder.buildPartial());
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // required bytes body = 1;
      private com.google.protobuf.ByteString body_ = com.google.protobuf.ByteString.EMPTY;
      public boolean hasBody() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public com.google.protobuf.ByteString getBody() {
        return body_;
      }
      public Builder setBody(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        body_ = value;
        onChanged();
        return this;
      }
      public Builder clearBody() {
        bitField0_ = (bitField0_ & ~0x00000001);
        body_ = getDefaultInstance().getBody();
        onChanged();
        return this;
      }
      
      // optional bytes srcRegion = 2;
      private com.google.protobuf.ByteString srcRegion_ = com.google.protobuf.ByteString.EMPTY;
      public boolean hasSrcRegion() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      public com.google.protobuf.ByteString getSrcRegion() {
        return srcRegion_;
      }
      public Builder setSrcRegion(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000002;
        srcRegion_ = value;
        onChanged();
        return this;
      }
      public Builder clearSrcRegion() {
        bitField0_ = (bitField0_ & ~0x00000002);
        srcRegion_ = getDefaultInstance().getSrcRegion();
        onChanged();
        return this;
      }
      
      // optional .Hedwig.MessageSeqId msgId = 3;
      private org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId msgId_ = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId, org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder, org.apache.hedwig.protocol.PubSubProtocol.MessageSeqIdOrBuilder> msgIdBuilder_;
      public boolean hasMsgId() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId getMsgId() {
        if (msgIdBuilder_ == null) {
          return msgId_;
        } else {
          return msgIdBuilder_.getMessage();
        }
      }
      public Builder setMsgId(org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId value) {
        if (msgIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          msgId_ = value;
          onChanged();
        } else {
          msgIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      public Builder setMsgId(
          org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder builderForValue) {
        if (msgIdBuilder_ == null) {
          msgId_ = builderForValue.build();
          onChanged();
        } else {
          msgIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      public Builder mergeMsgId(org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId value) {
        if (msgIdBuilder_ == null) {
          if (((bitField0_ & 0x00000004) == 0x00000004) &&
              msgId_ != org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance()) {
            msgId_ =
              org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.newBuilder(msgId_).mergeFrom(value).buildPartial();
          } else {
            msgId_ = value;
          }
          onChanged();
        } else {
          msgIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      public Builder clearMsgId() {
        if (msgIdBuilder_ == null) {
          msgId_ = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance();
          onChanged();
        } else {
          msgIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }
      public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder getMsgIdBuilder() {
        bitField0_ |= 0x00000004;
        onChanged();
        return getMsgIdFieldBuilder().getBuilder();
      }
      public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqIdOrBuilder getMsgIdOrBuilder() {
        if (msgIdBuilder_ != null) {
          return msgIdBuilder_.getMessageOrBuilder();
        } else {
          return msgId_;
        }
      }
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId, org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder, org.apache.hedwig.protocol.PubSubProtocol.MessageSeqIdOrBuilder> 
          getMsgIdFieldBuilder() {
        if (msgIdBuilder_ == null) {
          msgIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId, org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder, org.apache.hedwig.protocol.PubSubProtocol.MessageSeqIdOrBuilder>(
                  msgId_,
                  getParentForChildren(),
                  isClean());
          msgId_ = null;
        }
        return msgIdBuilder_;
      }
      
      // optional .Hedwig.MessageHeader header = 4;
      private org.apache.hedwig.protocol.PubSubProtocol.MessageHeader header_ = org.apache.hedwig.protocol.PubSubProtocol.MessageHeader.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.MessageHeader, org.apache.hedwig.protocol.PubSubProtocol.MessageHeader.Builder, org.apache.hedwig.protocol.PubSubProtocol.MessageHeaderOrBuilder> headerBuilder_;
      public boolean hasHeader() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.MessageHeader getHeader() {
        if (headerBuilder_ == null) {
          return header_;
        } else {
          return headerBuilder_.getMessage();
        }
      }
      public Builder setHeader(org.apache.hedwig.protocol.PubSubProtocol.MessageHeader value) {
        if (headerBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          header_ = value;
          onChanged();
        } else {
          headerBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000008;
        return this;
      }
      public Builder setHeader(
          org.apache.hedwig.protocol.PubSubProtocol.MessageHeader.Builder builderForValue) {
        if (headerBuilder_ == null) {
          header_ = builderForValue.build();
          onChanged();
        } else {
          headerBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000008;
        return this;
      }
      public Builder mergeHeader(org.apache.hedwig.protocol.PubSubProtocol.MessageHeader value) {
        if (headerBuilder_ == null) {
          if (((bitField0_ & 0x00000008) == 0x00000008) &&
              header_ != org.apache.hedwig.protocol.PubSubProtocol.MessageHeader.getDefaultInstance()) {
            header_ =
              org.apache.hedwig.protocol.PubSubProtocol.MessageHeader.newBuilder(header_).mergeFrom(value).buildPartial();
          } else {
            header_ = value;
          }
          onChanged();
        } else {
          headerBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000008;
        return this;
      }
      public Builder clearHeader() {
        if (headerBuilder_ == null) {
          header_ = org.apache.hedwig.protocol.PubSubProtocol.MessageHeader.getDefaultInstance();
          onChanged();
        } else {
          headerBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000008);
        return this;
      }
      public org.apache.hedwig.protocol.PubSubProtocol.MessageHeader.Builder getHeaderBuilder() {
        bitField0_ |= 0x00000008;
        onChanged();
        return getHeaderFieldBuilder().getBuilder();
      }
      public org.apache.hedwig.protocol.PubSubProtocol.MessageHeaderOrBuilder getHeaderOrBuilder() {
        if (headerBuilder_ != null) {
          return headerBuilder_.getMessageOrBuilder();
        } else {
          return header_;
        }
      }
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.MessageHeader, org.apache.hedwig.protocol.PubSubProtocol.MessageHeader.Builder, org.apache.hedwig.protocol.PubSubProtocol.MessageHeaderOrBuilder> 
          getHeaderFieldBuilder() {
        if (headerBuilder_ == null) {
          headerBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hedwig.protocol.PubSubProtocol.MessageHeader, org.apache.hedwig.protocol.PubSubProtocol.MessageHeader.Builder, org.apache.hedwig.protocol.PubSubProtocol.MessageHeaderOrBuilder>(
                  header_,
                  getParentForChildren(),
                  isClean());
          header_ = null;
        }
        return headerBuilder_;
      }
      
      // @@protoc_insertion_point(builder_scope:Hedwig.Message)
    }
    
    static {
      defaultInstance = new Message(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:Hedwig.Message)
  }
  
  public interface RegionSpecificSeqIdOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // required bytes region = 1;
    boolean hasRegion();
    com.google.protobuf.ByteString getRegion();
    
    // required uint64 seqId = 2;
    boolean hasSeqId();
    long getSeqId();
  }
  public static final class RegionSpecificSeqId extends
      com.google.protobuf.GeneratedMessage
      implements RegionSpecificSeqIdOrBuilder {
    // Use RegionSpecificSeqId.newBuilder() to construct.
    private RegionSpecificSeqId(Builder builder) {
      super(builder);
    }
    private RegionSpecificSeqId(boolean noInit) {}
    
    private static final RegionSpecificSeqId defaultInstance;
    public static RegionSpecificSeqId getDefaultInstance() {
      return defaultInstance;
    }
    
    public RegionSpecificSeqId getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_RegionSpecificSeqId_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_RegionSpecificSeqId_fieldAccessorTable;
    }
    
    private int bitField0_;
    // required bytes region = 1;
    public static final int REGION_FIELD_NUMBER = 1;
    private com.google.protobuf.ByteString region_;
    public boolean hasRegion() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    public com.google.protobuf.ByteString getRegion() {
      return region_;
    }
    
    // required uint64 seqId = 2;
    public static final int SEQID_FIELD_NUMBER = 2;
    private long seqId_;
    public boolean hasSeqId() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    public long getSeqId() {
      return seqId_;
    }
    
    private void initFields() {
      region_ = com.google.protobuf.ByteString.EMPTY;
      seqId_ = 0L;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      if (!hasRegion()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasSeqId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(1, region_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeUInt64(2, seqId_);
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(1, region_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(2, seqId_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqIdOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_RegionSpecificSeqId_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_RegionSpecificSeqId_fieldAccessorTable;
      }
      
      // Construct using org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        region_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        seqId_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId.getDescriptor();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId getDefaultInstanceForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId.getDefaultInstance();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId build() {
        org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId buildPartial() {
        org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId result = new org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.region_ = region_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.seqId_ = seqId_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId) {
          return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId other) {
        if (other == org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId.getDefaultInstance()) return this;
        if (other.hasRegion()) {
          setRegion(other.getRegion());
        }
        if (other.hasSeqId()) {
          setSeqId(other.getSeqId());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        if (!hasRegion()) {
          
          return false;
        }
        if (!hasSeqId()) {
          
          return false;
        }
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 10: {
              bitField0_ |= 0x00000001;
              region_ = input.readBytes();
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              seqId_ = input.readUInt64();
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // required bytes region = 1;
      private com.google.protobuf.ByteString region_ = com.google.protobuf.ByteString.EMPTY;
      public boolean hasRegion() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public com.google.protobuf.ByteString getRegion() {
        return region_;
      }
      public Builder setRegion(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        region_ = value;
        onChanged();
        return this;
      }
      public Builder clearRegion() {
        bitField0_ = (bitField0_ & ~0x00000001);
        region_ = getDefaultInstance().getRegion();
        onChanged();
        return this;
      }
      
      // required uint64 seqId = 2;
      private long seqId_ ;
      public boolean hasSeqId() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      public long getSeqId() {
        return seqId_;
      }
      public Builder setSeqId(long value) {
        bitField0_ |= 0x00000002;
        seqId_ = value;
        onChanged();
        return this;
      }
      public Builder clearSeqId() {
        bitField0_ = (bitField0_ & ~0x00000002);
        seqId_ = 0L;
        onChanged();
        return this;
      }
      
      // @@protoc_insertion_point(builder_scope:Hedwig.RegionSpecificSeqId)
    }
    
    static {
      defaultInstance = new RegionSpecificSeqId(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:Hedwig.RegionSpecificSeqId)
  }
  
  public interface MessageSeqIdOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // optional uint64 localComponent = 1;
    boolean hasLocalComponent();
    long getLocalComponent();
    
    // repeated .Hedwig.RegionSpecificSeqId remoteComponents = 2;
    java.util.List<org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId> 
        getRemoteComponentsList();
    org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId getRemoteComponents(int index);
    int getRemoteComponentsCount();
    java.util.List<? extends org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqIdOrBuilder> 
        getRemoteComponentsOrBuilderList();
    org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqIdOrBuilder getRemoteComponentsOrBuilder(
        int index);
  }
  public static final class MessageSeqId extends
      com.google.protobuf.GeneratedMessage
      implements MessageSeqIdOrBuilder {
    // Use MessageSeqId.newBuilder() to construct.
    private MessageSeqId(Builder builder) {
      super(builder);
    }
    private MessageSeqId(boolean noInit) {}
    
    private static final MessageSeqId defaultInstance;
    public static MessageSeqId getDefaultInstance() {
      return defaultInstance;
    }
    
    public MessageSeqId getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_MessageSeqId_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_MessageSeqId_fieldAccessorTable;
    }
    
    private int bitField0_;
    // optional uint64 localComponent = 1;
    public static final int LOCALCOMPONENT_FIELD_NUMBER = 1;
    private long localComponent_;
    public boolean hasLocalComponent() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    public long getLocalComponent() {
      return localComponent_;
    }
    
    // repeated .Hedwig.RegionSpecificSeqId remoteComponents = 2;
    public static final int REMOTECOMPONENTS_FIELD_NUMBER = 2;
    private java.util.List<org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId> remoteComponents_;
    public java.util.List<org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId> getRemoteComponentsList() {
      return remoteComponents_;
    }
    public java.util.List<? extends org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqIdOrBuilder> 
        getRemoteComponentsOrBuilderList() {
      return remoteComponents_;
    }
    public int getRemoteComponentsCount() {
      return remoteComponents_.size();
    }
    public org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId getRemoteComponents(int index) {
      return remoteComponents_.get(index);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqIdOrBuilder getRemoteComponentsOrBuilder(
        int index) {
      return remoteComponents_.get(index);
    }
    
    private void initFields() {
      localComponent_ = 0L;
      remoteComponents_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      for (int i = 0; i < getRemoteComponentsCount(); i++) {
        if (!getRemoteComponents(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeUInt64(1, localComponent_);
      }
      for (int i = 0; i < remoteComponents_.size(); i++) {
        output.writeMessage(2, remoteComponents_.get(i));
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(1, localComponent_);
      }
      for (int i = 0; i < remoteComponents_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, remoteComponents_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hedwig.protocol.PubSubProtocol.MessageSeqIdOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_MessageSeqId_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_MessageSeqId_fieldAccessorTable;
      }
      
      // Construct using org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getRemoteComponentsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        localComponent_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000001);
        if (remoteComponentsBuilder_ == null) {
          remoteComponents_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
        } else {
          remoteComponentsBuilder_.clear();
        }
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDescriptor();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId getDefaultInstanceForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId build() {
        org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId buildPartial() {
        org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId result = new org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.localComponent_ = localComponent_;
        if (remoteComponentsBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002)) {
            remoteComponents_ = java.util.Collections.unmodifiableList(remoteComponents_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.remoteComponents_ = remoteComponents_;
        } else {
          result.remoteComponents_ = remoteComponentsBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId) {
          return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId other) {
        if (other == org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance()) return this;
        if (other.hasLocalComponent()) {
          setLocalComponent(other.getLocalComponent());
        }
        if (remoteComponentsBuilder_ == null) {
          if (!other.remoteComponents_.isEmpty()) {
            if (remoteComponents_.isEmpty()) {
              remoteComponents_ = other.remoteComponents_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureRemoteComponentsIsMutable();
              remoteComponents_.addAll(other.remoteComponents_);
            }
            onChanged();
          }
        } else {
          if (!other.remoteComponents_.isEmpty()) {
            if (remoteComponentsBuilder_.isEmpty()) {
              remoteComponentsBuilder_.dispose();
              remoteComponentsBuilder_ = null;
              remoteComponents_ = other.remoteComponents_;
              bitField0_ = (bitField0_ & ~0x00000002);
              remoteComponentsBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getRemoteComponentsFieldBuilder() : null;
            } else {
              remoteComponentsBuilder_.addAllMessages(other.remoteComponents_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        for (int i = 0; i < getRemoteComponentsCount(); i++) {
          if (!getRemoteComponents(i).isInitialized()) {
            
            return false;
          }
        }
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              localComponent_ = input.readUInt64();
              break;
            }
            case 18: {
              org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId.newBuilder();
              input.readMessage(subBuilder, extensionRegistry);
              addRemoteComponents(subBuilder.buildPartial());
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // optional uint64 localComponent = 1;
      private long localComponent_ ;
      public boolean hasLocalComponent() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public long getLocalComponent() {
        return localComponent_;
      }
      public Builder setLocalComponent(long value) {
        bitField0_ |= 0x00000001;
        localComponent_ = value;
        onChanged();
        return this;
      }
      public Builder clearLocalComponent() {
        bitField0_ = (bitField0_ & ~0x00000001);
        localComponent_ = 0L;
        onChanged();
        return this;
      }
      
      // repeated .Hedwig.RegionSpecificSeqId remoteComponents = 2;
      private java.util.List<org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId> remoteComponents_ =
        java.util.Collections.emptyList();
      private void ensureRemoteComponentsIsMutable() {
        if (!((bitField0_ & 0x00000002) == 0x00000002)) {
          remoteComponents_ = new java.util.ArrayList<org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId>(remoteComponents_);
          bitField0_ |= 0x00000002;
         }
      }
      
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId, org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId.Builder, org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqIdOrBuilder> remoteComponentsBuilder_;
      
      public java.util.List<org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId> getRemoteComponentsList() {
        if (remoteComponentsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(remoteComponents_);
        } else {
          return remoteComponentsBuilder_.getMessageList();
        }
      }
      public int getRemoteComponentsCount() {
        if (remoteComponentsBuilder_ == null) {
          return remoteComponents_.size();
        } else {
          return remoteComponentsBuilder_.getCount();
        }
      }
      public org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId getRemoteComponents(int index) {
        if (remoteComponentsBuilder_ == null) {
          return remoteComponents_.get(index);
        } else {
          return remoteComponentsBuilder_.getMessage(index);
        }
      }
      public Builder setRemoteComponents(
          int index, org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId value) {
        if (remoteComponentsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureRemoteComponentsIsMutable();
          remoteComponents_.set(index, value);
          onChanged();
        } else {
          remoteComponentsBuilder_.setMessage(index, value);
        }
        return this;
      }
      public Builder setRemoteComponents(
          int index, org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId.Builder builderForValue) {
        if (remoteComponentsBuilder_ == null) {
          ensureRemoteComponentsIsMutable();
          remoteComponents_.set(index, builderForValue.build());
          onChanged();
        } else {
          remoteComponentsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      public Builder addRemoteComponents(org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId value) {
        if (remoteComponentsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureRemoteComponentsIsMutable();
          remoteComponents_.add(value);
          onChanged();
        } else {
          remoteComponentsBuilder_.addMessage(value);
        }
        return this;
      }
      public Builder addRemoteComponents(
          int index, org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId value) {
        if (remoteComponentsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureRemoteComponentsIsMutable();
          remoteComponents_.add(index, value);
          onChanged();
        } else {
          remoteComponentsBuilder_.addMessage(index, value);
        }
        return this;
      }
      public Builder addRemoteComponents(
          org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId.Builder builderForValue) {
        if (remoteComponentsBuilder_ == null) {
          ensureRemoteComponentsIsMutable();
          remoteComponents_.add(builderForValue.build());
          onChanged();
        } else {
          remoteComponentsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      public Builder addRemoteComponents(
          int index, org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId.Builder builderForValue) {
        if (remoteComponentsBuilder_ == null) {
          ensureRemoteComponentsIsMutable();
          remoteComponents_.add(index, builderForValue.build());
          onChanged();
        } else {
          remoteComponentsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      public Builder addAllRemoteComponents(
          java.lang.Iterable<? extends org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId> values) {
        if (remoteComponentsBuilder_ == null) {
          ensureRemoteComponentsIsMutable();
          super.addAll(values, remoteComponents_);
          onChanged();
        } else {
          remoteComponentsBuilder_.addAllMessages(values);
        }
        return this;
      }
      public Builder clearRemoteComponents() {
        if (remoteComponentsBuilder_ == null) {
          remoteComponents_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
        } else {
          remoteComponentsBuilder_.clear();
        }
        return this;
      }
      public Builder removeRemoteComponents(int index) {
        if (remoteComponentsBuilder_ == null) {
          ensureRemoteComponentsIsMutable();
          remoteComponents_.remove(index);
          onChanged();
        } else {
          remoteComponentsBuilder_.remove(index);
        }
        return this;
      }
      public org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId.Builder getRemoteComponentsBuilder(
          int index) {
        return getRemoteComponentsFieldBuilder().getBuilder(index);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqIdOrBuilder getRemoteComponentsOrBuilder(
          int index) {
        if (remoteComponentsBuilder_ == null) {
          return remoteComponents_.get(index);  } else {
          return remoteComponentsBuilder_.getMessageOrBuilder(index);
        }
      }
      public java.util.List<? extends org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqIdOrBuilder> 
           getRemoteComponentsOrBuilderList() {
        if (remoteComponentsBuilder_ != null) {
          return remoteComponentsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(remoteComponents_);
        }
      }
      public org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId.Builder addRemoteComponentsBuilder() {
        return getRemoteComponentsFieldBuilder().addBuilder(
            org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId.getDefaultInstance());
      }
      public org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId.Builder addRemoteComponentsBuilder(
          int index) {
        return getRemoteComponentsFieldBuilder().addBuilder(
            index, org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId.getDefaultInstance());
      }
      public java.util.List<org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId.Builder> 
           getRemoteComponentsBuilderList() {
        return getRemoteComponentsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId, org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId.Builder, org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqIdOrBuilder> 
          getRemoteComponentsFieldBuilder() {
        if (remoteComponentsBuilder_ == null) {
          remoteComponentsBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId, org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId.Builder, org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqIdOrBuilder>(
                  remoteComponents_,
                  ((bitField0_ & 0x00000002) == 0x00000002),
                  getParentForChildren(),
                  isClean());
          remoteComponents_ = null;
        }
        return remoteComponentsBuilder_;
      }
      
      // @@protoc_insertion_point(builder_scope:Hedwig.MessageSeqId)
    }
    
    static {
      defaultInstance = new MessageSeqId(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:Hedwig.MessageSeqId)
  }
  
  public interface PubSubRequestOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // required .Hedwig.ProtocolVersion protocolVersion = 1;
    boolean hasProtocolVersion();
    org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion getProtocolVersion();
    
    // required .Hedwig.OperationType type = 2;
    boolean hasType();
    org.apache.hedwig.protocol.PubSubProtocol.OperationType getType();
    
    // repeated bytes triedServers = 3;
    java.util.List<com.google.protobuf.ByteString> getTriedServersList();
    int getTriedServersCount();
    com.google.protobuf.ByteString getTriedServers(int index);
    
    // required uint64 txnId = 4;
    boolean hasTxnId();
    long getTxnId();
    
    // optional bool shouldClaim = 5;
    boolean hasShouldClaim();
    boolean getShouldClaim();
    
    // required bytes topic = 6;
    boolean hasTopic();
    com.google.protobuf.ByteString getTopic();
    
    // optional .Hedwig.PublishRequest publishRequest = 52;
    boolean hasPublishRequest();
    org.apache.hedwig.protocol.PubSubProtocol.PublishRequest getPublishRequest();
    org.apache.hedwig.protocol.PubSubProtocol.PublishRequestOrBuilder getPublishRequestOrBuilder();
    
    // optional .Hedwig.SubscribeRequest subscribeRequest = 53;
    boolean hasSubscribeRequest();
    org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest getSubscribeRequest();
    org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequestOrBuilder getSubscribeRequestOrBuilder();
    
    // optional .Hedwig.ConsumeRequest consumeRequest = 54;
    boolean hasConsumeRequest();
    org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest getConsumeRequest();
    org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequestOrBuilder getConsumeRequestOrBuilder();
    
    // optional .Hedwig.UnsubscribeRequest unsubscribeRequest = 55;
    boolean hasUnsubscribeRequest();
    org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest getUnsubscribeRequest();
    org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequestOrBuilder getUnsubscribeRequestOrBuilder();
    
    // optional .Hedwig.StopDeliveryRequest stopDeliveryRequest = 56;
    boolean hasStopDeliveryRequest();
    org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest getStopDeliveryRequest();
    org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequestOrBuilder getStopDeliveryRequestOrBuilder();
    
    // optional .Hedwig.StartDeliveryRequest startDeliveryRequest = 57;
    boolean hasStartDeliveryRequest();
    org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest getStartDeliveryRequest();
    org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequestOrBuilder getStartDeliveryRequestOrBuilder();
    
    // optional .Hedwig.CloseSubscriptionRequest closeSubscriptionRequest = 58;
    boolean hasCloseSubscriptionRequest();
    org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest getCloseSubscriptionRequest();
    org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequestOrBuilder getCloseSubscriptionRequestOrBuilder();
  }
  public static final class PubSubRequest extends
      com.google.protobuf.GeneratedMessage
      implements PubSubRequestOrBuilder {
    // Use PubSubRequest.newBuilder() to construct.
    private PubSubRequest(Builder builder) {
      super(builder);
    }
    private PubSubRequest(boolean noInit) {}
    
    private static final PubSubRequest defaultInstance;
    public static PubSubRequest getDefaultInstance() {
      return defaultInstance;
    }
    
    public PubSubRequest getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_PubSubRequest_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_PubSubRequest_fieldAccessorTable;
    }
    
    private int bitField0_;
    // required .Hedwig.ProtocolVersion protocolVersion = 1;
    public static final int PROTOCOLVERSION_FIELD_NUMBER = 1;
    private org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion protocolVersion_;
    public boolean hasProtocolVersion() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion getProtocolVersion() {
      return protocolVersion_;
    }
    
    // required .Hedwig.OperationType type = 2;
    public static final int TYPE_FIELD_NUMBER = 2;
    private org.apache.hedwig.protocol.PubSubProtocol.OperationType type_;
    public boolean hasType() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.OperationType getType() {
      return type_;
    }
    
    // repeated bytes triedServers = 3;
    public static final int TRIEDSERVERS_FIELD_NUMBER = 3;
    private java.util.List<com.google.protobuf.ByteString> triedServers_;
    public java.util.List<com.google.protobuf.ByteString>
        getTriedServersList() {
      return triedServers_;
    }
    public int getTriedServersCount() {
      return triedServers_.size();
    }
    public com.google.protobuf.ByteString getTriedServers(int index) {
      return triedServers_.get(index);
    }
    
    // required uint64 txnId = 4;
    public static final int TXNID_FIELD_NUMBER = 4;
    private long txnId_;
    public boolean hasTxnId() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    public long getTxnId() {
      return txnId_;
    }
    
    // optional bool shouldClaim = 5;
    public static final int SHOULDCLAIM_FIELD_NUMBER = 5;
    private boolean shouldClaim_;
    public boolean hasShouldClaim() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    public boolean getShouldClaim() {
      return shouldClaim_;
    }
    
    // required bytes topic = 6;
    public static final int TOPIC_FIELD_NUMBER = 6;
    private com.google.protobuf.ByteString topic_;
    public boolean hasTopic() {
      return ((bitField0_ & 0x00000010) == 0x00000010);
    }
    public com.google.protobuf.ByteString getTopic() {
      return topic_;
    }
    
    // optional .Hedwig.PublishRequest publishRequest = 52;
    public static final int PUBLISHREQUEST_FIELD_NUMBER = 52;
    private org.apache.hedwig.protocol.PubSubProtocol.PublishRequest publishRequest_;
    public boolean hasPublishRequest() {
      return ((bitField0_ & 0x00000020) == 0x00000020);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.PublishRequest getPublishRequest() {
      return publishRequest_;
    }
    public org.apache.hedwig.protocol.PubSubProtocol.PublishRequestOrBuilder getPublishRequestOrBuilder() {
      return publishRequest_;
    }
    
    // optional .Hedwig.SubscribeRequest subscribeRequest = 53;
    public static final int SUBSCRIBEREQUEST_FIELD_NUMBER = 53;
    private org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest subscribeRequest_;
    public boolean hasSubscribeRequest() {
      return ((bitField0_ & 0x00000040) == 0x00000040);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest getSubscribeRequest() {
      return subscribeRequest_;
    }
    public org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequestOrBuilder getSubscribeRequestOrBuilder() {
      return subscribeRequest_;
    }
    
    // optional .Hedwig.ConsumeRequest consumeRequest = 54;
    public static final int CONSUMEREQUEST_FIELD_NUMBER = 54;
    private org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest consumeRequest_;
    public boolean hasConsumeRequest() {
      return ((bitField0_ & 0x00000080) == 0x00000080);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest getConsumeRequest() {
      return consumeRequest_;
    }
    public org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequestOrBuilder getConsumeRequestOrBuilder() {
      return consumeRequest_;
    }
    
    // optional .Hedwig.UnsubscribeRequest unsubscribeRequest = 55;
    public static final int UNSUBSCRIBEREQUEST_FIELD_NUMBER = 55;
    private org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest unsubscribeRequest_;
    public boolean hasUnsubscribeRequest() {
      return ((bitField0_ & 0x00000100) == 0x00000100);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest getUnsubscribeRequest() {
      return unsubscribeRequest_;
    }
    public org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequestOrBuilder getUnsubscribeRequestOrBuilder() {
      return unsubscribeRequest_;
    }
    
    // optional .Hedwig.StopDeliveryRequest stopDeliveryRequest = 56;
    public static final int STOPDELIVERYREQUEST_FIELD_NUMBER = 56;
    private org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest stopDeliveryRequest_;
    public boolean hasStopDeliveryRequest() {
      return ((bitField0_ & 0x00000200) == 0x00000200);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest getStopDeliveryRequest() {
      return stopDeliveryRequest_;
    }
    public org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequestOrBuilder getStopDeliveryRequestOrBuilder() {
      return stopDeliveryRequest_;
    }
    
    // optional .Hedwig.StartDeliveryRequest startDeliveryRequest = 57;
    public static final int STARTDELIVERYREQUEST_FIELD_NUMBER = 57;
    private org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest startDeliveryRequest_;
    public boolean hasStartDeliveryRequest() {
      return ((bitField0_ & 0x00000400) == 0x00000400);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest getStartDeliveryRequest() {
      return startDeliveryRequest_;
    }
    public org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequestOrBuilder getStartDeliveryRequestOrBuilder() {
      return startDeliveryRequest_;
    }
    
    // optional .Hedwig.CloseSubscriptionRequest closeSubscriptionRequest = 58;
    public static final int CLOSESUBSCRIPTIONREQUEST_FIELD_NUMBER = 58;
    private org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest closeSubscriptionRequest_;
    public boolean hasCloseSubscriptionRequest() {
      return ((bitField0_ & 0x00000800) == 0x00000800);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest getCloseSubscriptionRequest() {
      return closeSubscriptionRequest_;
    }
    public org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequestOrBuilder getCloseSubscriptionRequestOrBuilder() {
      return closeSubscriptionRequest_;
    }
    
    private void initFields() {
      protocolVersion_ = org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion.VERSION_ONE;
      type_ = org.apache.hedwig.protocol.PubSubProtocol.OperationType.PUBLISH;
      triedServers_ = java.util.Collections.emptyList();;
      txnId_ = 0L;
      shouldClaim_ = false;
      topic_ = com.google.protobuf.ByteString.EMPTY;
      publishRequest_ = org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.getDefaultInstance();
      subscribeRequest_ = org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.getDefaultInstance();
      consumeRequest_ = org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.getDefaultInstance();
      unsubscribeRequest_ = org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.getDefaultInstance();
      stopDeliveryRequest_ = org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.getDefaultInstance();
      startDeliveryRequest_ = org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.getDefaultInstance();
      closeSubscriptionRequest_ = org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      if (!hasProtocolVersion()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasType()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasTxnId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasTopic()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (hasPublishRequest()) {
        if (!getPublishRequest().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      if (hasSubscribeRequest()) {
        if (!getSubscribeRequest().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      if (hasConsumeRequest()) {
        if (!getConsumeRequest().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      if (hasUnsubscribeRequest()) {
        if (!getUnsubscribeRequest().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      if (hasStopDeliveryRequest()) {
        if (!getStopDeliveryRequest().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      if (hasStartDeliveryRequest()) {
        if (!getStartDeliveryRequest().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      if (hasCloseSubscriptionRequest()) {
        if (!getCloseSubscriptionRequest().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeEnum(1, protocolVersion_.getNumber());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeEnum(2, type_.getNumber());
      }
      for (int i = 0; i < triedServers_.size(); i++) {
        output.writeBytes(3, triedServers_.get(i));
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeUInt64(4, txnId_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeBool(5, shouldClaim_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        output.writeBytes(6, topic_);
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        output.writeMessage(52, publishRequest_);
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        output.writeMessage(53, subscribeRequest_);
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        output.writeMessage(54, consumeRequest_);
      }
      if (((bitField0_ & 0x00000100) == 0x00000100)) {
        output.writeMessage(55, unsubscribeRequest_);
      }
      if (((bitField0_ & 0x00000200) == 0x00000200)) {
        output.writeMessage(56, stopDeliveryRequest_);
      }
      if (((bitField0_ & 0x00000400) == 0x00000400)) {
        output.writeMessage(57, startDeliveryRequest_);
      }
      if (((bitField0_ & 0x00000800) == 0x00000800)) {
        output.writeMessage(58, closeSubscriptionRequest_);
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(1, protocolVersion_.getNumber());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(2, type_.getNumber());
      }
      {
        int dataSize = 0;
        for (int i = 0; i < triedServers_.size(); i++) {
          dataSize += com.google.protobuf.CodedOutputStream
            .computeBytesSizeNoTag(triedServers_.get(i));
        }
        size += dataSize;
        size += 1 * getTriedServersList().size();
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(4, txnId_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(5, shouldClaim_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(6, topic_);
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(52, publishRequest_);
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(53, subscribeRequest_);
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(54, consumeRequest_);
      }
      if (((bitField0_ & 0x00000100) == 0x00000100)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(55, unsubscribeRequest_);
      }
      if (((bitField0_ & 0x00000200) == 0x00000200)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(56, stopDeliveryRequest_);
      }
      if (((bitField0_ & 0x00000400) == 0x00000400)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(57, startDeliveryRequest_);
      }
      if (((bitField0_ & 0x00000800) == 0x00000800)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(58, closeSubscriptionRequest_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hedwig.protocol.PubSubProtocol.PubSubRequestOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_PubSubRequest_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_PubSubRequest_fieldAccessorTable;
      }
      
      // Construct using org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getPublishRequestFieldBuilder();
          getSubscribeRequestFieldBuilder();
          getConsumeRequestFieldBuilder();
          getUnsubscribeRequestFieldBuilder();
          getStopDeliveryRequestFieldBuilder();
          getStartDeliveryRequestFieldBuilder();
          getCloseSubscriptionRequestFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        protocolVersion_ = org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion.VERSION_ONE;
        bitField0_ = (bitField0_ & ~0x00000001);
        type_ = org.apache.hedwig.protocol.PubSubProtocol.OperationType.PUBLISH;
        bitField0_ = (bitField0_ & ~0x00000002);
        triedServers_ = java.util.Collections.emptyList();;
        bitField0_ = (bitField0_ & ~0x00000004);
        txnId_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000008);
        shouldClaim_ = false;
        bitField0_ = (bitField0_ & ~0x00000010);
        topic_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000020);
        if (publishRequestBuilder_ == null) {
          publishRequest_ = org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.getDefaultInstance();
        } else {
          publishRequestBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000040);
        if (subscribeRequestBuilder_ == null) {
          subscribeRequest_ = org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.getDefaultInstance();
        } else {
          subscribeRequestBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000080);
        if (consumeRequestBuilder_ == null) {
          consumeRequest_ = org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.getDefaultInstance();
        } else {
          consumeRequestBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000100);
        if (unsubscribeRequestBuilder_ == null) {
          unsubscribeRequest_ = org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.getDefaultInstance();
        } else {
          unsubscribeRequestBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000200);
        if (stopDeliveryRequestBuilder_ == null) {
          stopDeliveryRequest_ = org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.getDefaultInstance();
        } else {
          stopDeliveryRequestBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000400);
        if (startDeliveryRequestBuilder_ == null) {
          startDeliveryRequest_ = org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.getDefaultInstance();
        } else {
          startDeliveryRequestBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000800);
        if (closeSubscriptionRequestBuilder_ == null) {
          closeSubscriptionRequest_ = org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest.getDefaultInstance();
        } else {
          closeSubscriptionRequestBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00001000);
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest.getDescriptor();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest getDefaultInstanceForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest.getDefaultInstance();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest build() {
        org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest buildPartial() {
        org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest result = new org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.protocolVersion_ = protocolVersion_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.type_ = type_;
        if (((bitField0_ & 0x00000004) == 0x00000004)) {
          triedServers_ = java.util.Collections.unmodifiableList(triedServers_);
          bitField0_ = (bitField0_ & ~0x00000004);
        }
        result.triedServers_ = triedServers_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000004;
        }
        result.txnId_ = txnId_;
        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
          to_bitField0_ |= 0x00000008;
        }
        result.shouldClaim_ = shouldClaim_;
        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
          to_bitField0_ |= 0x00000010;
        }
        result.topic_ = topic_;
        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
          to_bitField0_ |= 0x00000020;
        }
        if (publishRequestBuilder_ == null) {
          result.publishRequest_ = publishRequest_;
        } else {
          result.publishRequest_ = publishRequestBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
          to_bitField0_ |= 0x00000040;
        }
        if (subscribeRequestBuilder_ == null) {
          result.subscribeRequest_ = subscribeRequest_;
        } else {
          result.subscribeRequest_ = subscribeRequestBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000100) == 0x00000100)) {
          to_bitField0_ |= 0x00000080;
        }
        if (consumeRequestBuilder_ == null) {
          result.consumeRequest_ = consumeRequest_;
        } else {
          result.consumeRequest_ = consumeRequestBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000200) == 0x00000200)) {
          to_bitField0_ |= 0x00000100;
        }
        if (unsubscribeRequestBuilder_ == null) {
          result.unsubscribeRequest_ = unsubscribeRequest_;
        } else {
          result.unsubscribeRequest_ = unsubscribeRequestBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000400) == 0x00000400)) {
          to_bitField0_ |= 0x00000200;
        }
        if (stopDeliveryRequestBuilder_ == null) {
          result.stopDeliveryRequest_ = stopDeliveryRequest_;
        } else {
          result.stopDeliveryRequest_ = stopDeliveryRequestBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000800) == 0x00000800)) {
          to_bitField0_ |= 0x00000400;
        }
        if (startDeliveryRequestBuilder_ == null) {
          result.startDeliveryRequest_ = startDeliveryRequest_;
        } else {
          result.startDeliveryRequest_ = startDeliveryRequestBuilder_.build();
        }
        if (((from_bitField0_ & 0x00001000) == 0x00001000)) {
          to_bitField0_ |= 0x00000800;
        }
        if (closeSubscriptionRequestBuilder_ == null) {
          result.closeSubscriptionRequest_ = closeSubscriptionRequest_;
        } else {
          result.closeSubscriptionRequest_ = closeSubscriptionRequestBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest) {
          return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest other) {
        if (other == org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest.getDefaultInstance()) return this;
        if (other.hasProtocolVersion()) {
          setProtocolVersion(other.getProtocolVersion());
        }
        if (other.hasType()) {
          setType(other.getType());
        }
        if (!other.triedServers_.isEmpty()) {
          if (triedServers_.isEmpty()) {
            triedServers_ = other.triedServers_;
            bitField0_ = (bitField0_ & ~0x00000004);
          } else {
            ensureTriedServersIsMutable();
            triedServers_.addAll(other.triedServers_);
          }
          onChanged();
        }
        if (other.hasTxnId()) {
          setTxnId(other.getTxnId());
        }
        if (other.hasShouldClaim()) {
          setShouldClaim(other.getShouldClaim());
        }
        if (other.hasTopic()) {
          setTopic(other.getTopic());
        }
        if (other.hasPublishRequest()) {
          mergePublishRequest(other.getPublishRequest());
        }
        if (other.hasSubscribeRequest()) {
          mergeSubscribeRequest(other.getSubscribeRequest());
        }
        if (other.hasConsumeRequest()) {
          mergeConsumeRequest(other.getConsumeRequest());
        }
        if (other.hasUnsubscribeRequest()) {
          mergeUnsubscribeRequest(other.getUnsubscribeRequest());
        }
        if (other.hasStopDeliveryRequest()) {
          mergeStopDeliveryRequest(other.getStopDeliveryRequest());
        }
        if (other.hasStartDeliveryRequest()) {
          mergeStartDeliveryRequest(other.getStartDeliveryRequest());
        }
        if (other.hasCloseSubscriptionRequest()) {
          mergeCloseSubscriptionRequest(other.getCloseSubscriptionRequest());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        if (!hasProtocolVersion()) {
          
          return false;
        }
        if (!hasType()) {
          
          return false;
        }
        if (!hasTxnId()) {
          
          return false;
        }
        if (!hasTopic()) {
          
          return false;
        }
        if (hasPublishRequest()) {
          if (!getPublishRequest().isInitialized()) {
            
            return false;
          }
        }
        if (hasSubscribeRequest()) {
          if (!getSubscribeRequest().isInitialized()) {
            
            return false;
          }
        }
        if (hasConsumeRequest()) {
          if (!getConsumeRequest().isInitialized()) {
            
            return false;
          }
        }
        if (hasUnsubscribeRequest()) {
          if (!getUnsubscribeRequest().isInitialized()) {
            
            return false;
          }
        }
        if (hasStopDeliveryRequest()) {
          if (!getStopDeliveryRequest().isInitialized()) {
            
            return false;
          }
        }
        if (hasStartDeliveryRequest()) {
          if (!getStartDeliveryRequest().isInitialized()) {
            
            return false;
          }
        }
        if (hasCloseSubscriptionRequest()) {
          if (!getCloseSubscriptionRequest().isInitialized()) {
            
            return false;
          }
        }
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 8: {
              int rawValue = input.readEnum();
              org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion value = org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(1, rawValue);
              } else {
                bitField0_ |= 0x00000001;
                protocolVersion_ = value;
              }
              break;
            }
            case 16: {
              int rawValue = input.readEnum();
              org.apache.hedwig.protocol.PubSubProtocol.OperationType value = org.apache.hedwig.protocol.PubSubProtocol.OperationType.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(2, rawValue);
              } else {
                bitField0_ |= 0x00000002;
                type_ = value;
              }
              break;
            }
            case 26: {
              ensureTriedServersIsMutable();
              triedServers_.add(input.readBytes());
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              txnId_ = input.readUInt64();
              break;
            }
            case 40: {
              bitField0_ |= 0x00000010;
              shouldClaim_ = input.readBool();
              break;
            }
            case 50: {
              bitField0_ |= 0x00000020;
              topic_ = input.readBytes();
              break;
            }
            case 418: {
              org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.newBuilder();
              if (hasPublishRequest()) {
                subBuilder.mergeFrom(getPublishRequest());
              }
              input.readMessage(subBuilder, extensionRegistry);
              setPublishRequest(subBuilder.buildPartial());
              break;
            }
            case 426: {
              org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.newBuilder();
              if (hasSubscribeRequest()) {
                subBuilder.mergeFrom(getSubscribeRequest());
              }
              input.readMessage(subBuilder, extensionRegistry);
              setSubscribeRequest(subBuilder.buildPartial());
              break;
            }
            case 434: {
              org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.newBuilder();
              if (hasConsumeRequest()) {
                subBuilder.mergeFrom(getConsumeRequest());
              }
              input.readMessage(subBuilder, extensionRegistry);
              setConsumeRequest(subBuilder.buildPartial());
              break;
            }
            case 442: {
              org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.newBuilder();
              if (hasUnsubscribeRequest()) {
                subBuilder.mergeFrom(getUnsubscribeRequest());
              }
              input.readMessage(subBuilder, extensionRegistry);
              setUnsubscribeRequest(subBuilder.buildPartial());
              break;
            }
            case 450: {
              org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.newBuilder();
              if (hasStopDeliveryRequest()) {
                subBuilder.mergeFrom(getStopDeliveryRequest());
              }
              input.readMessage(subBuilder, extensionRegistry);
              setStopDeliveryRequest(subBuilder.buildPartial());
              break;
            }
            case 458: {
              org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.newBuilder();
              if (hasStartDeliveryRequest()) {
                subBuilder.mergeFrom(getStartDeliveryRequest());
              }
              input.readMessage(subBuilder, extensionRegistry);
              setStartDeliveryRequest(subBuilder.buildPartial());
              break;
            }
            case 466: {
              org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest.newBuilder();
              if (hasCloseSubscriptionRequest()) {
                subBuilder.mergeFrom(getCloseSubscriptionRequest());
              }
              input.readMessage(subBuilder, extensionRegistry);
              setCloseSubscriptionRequest(subBuilder.buildPartial());
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // required .Hedwig.ProtocolVersion protocolVersion = 1;
      private org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion protocolVersion_ = org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion.VERSION_ONE;
      public boolean hasProtocolVersion() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion getProtocolVersion() {
        return protocolVersion_;
      }
      public Builder setProtocolVersion(org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000001;
        protocolVersion_ = value;
        onChanged();
        return this;
      }
      public Builder clearProtocolVersion() {
        bitField0_ = (bitField0_ & ~0x00000001);
        protocolVersion_ = org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion.VERSION_ONE;
        onChanged();
        return this;
      }
      
      // required .Hedwig.OperationType type = 2;
      private org.apache.hedwig.protocol.PubSubProtocol.OperationType type_ = org.apache.hedwig.protocol.PubSubProtocol.OperationType.PUBLISH;
      public boolean hasType() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.OperationType getType() {
        return type_;
      }
      public Builder setType(org.apache.hedwig.protocol.PubSubProtocol.OperationType value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000002;
        type_ = value;
        onChanged();
        return this;
      }
      public Builder clearType() {
        bitField0_ = (bitField0_ & ~0x00000002);
        type_ = org.apache.hedwig.protocol.PubSubProtocol.OperationType.PUBLISH;
        onChanged();
        return this;
      }
      
      // repeated bytes triedServers = 3;
      private java.util.List<com.google.protobuf.ByteString> triedServers_ = java.util.Collections.emptyList();;
      private void ensureTriedServersIsMutable() {
        if (!((bitField0_ & 0x00000004) == 0x00000004)) {
          triedServers_ = new java.util.ArrayList<com.google.protobuf.ByteString>(triedServers_);
          bitField0_ |= 0x00000004;
         }
      }
      public java.util.List<com.google.protobuf.ByteString>
          getTriedServersList() {
        return java.util.Collections.unmodifiableList(triedServers_);
      }
      public int getTriedServersCount() {
        return triedServers_.size();
      }
      public com.google.protobuf.ByteString getTriedServers(int index) {
        return triedServers_.get(index);
      }
      public Builder setTriedServers(
          int index, com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureTriedServersIsMutable();
        triedServers_.set(index, value);
        onChanged();
        return this;
      }
      public Builder addTriedServers(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  ensureTriedServersIsMutable();
        triedServers_.add(value);
        onChanged();
        return this;
      }
      public Builder addAllTriedServers(
          java.lang.Iterable<? extends com.google.protobuf.ByteString> values) {
        ensureTriedServersIsMutable();
        super.addAll(values, triedServers_);
        onChanged();
        return this;
      }
      public Builder clearTriedServers() {
        triedServers_ = java.util.Collections.emptyList();;
        bitField0_ = (bitField0_ & ~0x00000004);
        onChanged();
        return this;
      }
      
      // required uint64 txnId = 4;
      private long txnId_ ;
      public boolean hasTxnId() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      public long getTxnId() {
        return txnId_;
      }
      public Builder setTxnId(long value) {
        bitField0_ |= 0x00000008;
        txnId_ = value;
        onChanged();
        return this;
      }
      public Builder clearTxnId() {
        bitField0_ = (bitField0_ & ~0x00000008);
        txnId_ = 0L;
        onChanged();
        return this;
      }
      
      // optional bool shouldClaim = 5;
      private boolean shouldClaim_ ;
      public boolean hasShouldClaim() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      public boolean getShouldClaim() {
        return shouldClaim_;
      }
      public Builder setShouldClaim(boolean value) {
        bitField0_ |= 0x00000010;
        shouldClaim_ = value;
        onChanged();
        return this;
      }
      public Builder clearShouldClaim() {
        bitField0_ = (bitField0_ & ~0x00000010);
        shouldClaim_ = false;
        onChanged();
        return this;
      }
      
      // required bytes topic = 6;
      private com.google.protobuf.ByteString topic_ = com.google.protobuf.ByteString.EMPTY;
      public boolean hasTopic() {
        return ((bitField0_ & 0x00000020) == 0x00000020);
      }
      public com.google.protobuf.ByteString getTopic() {
        return topic_;
      }
      public Builder setTopic(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000020;
        topic_ = value;
        onChanged();
        return this;
      }
      public Builder clearTopic() {
        bitField0_ = (bitField0_ & ~0x00000020);
        topic_ = getDefaultInstance().getTopic();
        onChanged();
        return this;
      }
      
      // optional .Hedwig.PublishRequest publishRequest = 52;
      private org.apache.hedwig.protocol.PubSubProtocol.PublishRequest publishRequest_ = org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.PublishRequest, org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.Builder, org.apache.hedwig.protocol.PubSubProtocol.PublishRequestOrBuilder> publishRequestBuilder_;
      public boolean hasPublishRequest() {
        return ((bitField0_ & 0x00000040) == 0x00000040);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.PublishRequest getPublishRequest() {
        if (publishRequestBuilder_ == null) {
          return publishRequest_;
        } else {
          return publishRequestBuilder_.getMessage();
        }
      }
      public Builder setPublishRequest(org.apache.hedwig.protocol.PubSubProtocol.PublishRequest value) {
        if (publishRequestBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          publishRequest_ = value;
          onChanged();
        } else {
          publishRequestBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000040;
        return this;
      }
      public Builder setPublishRequest(
          org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.Builder builderForValue) {
        if (publishRequestBuilder_ == null) {
          publishRequest_ = builderForValue.build();
          onChanged();
        } else {
          publishRequestBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000040;
        return this;
      }
      public Builder mergePublishRequest(org.apache.hedwig.protocol.PubSubProtocol.PublishRequest value) {
        if (publishRequestBuilder_ == null) {
          if (((bitField0_ & 0x00000040) == 0x00000040) &&
              publishRequest_ != org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.getDefaultInstance()) {
            publishRequest_ =
              org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.newBuilder(publishRequest_).mergeFrom(value).buildPartial();
          } else {
            publishRequest_ = value;
          }
          onChanged();
        } else {
          publishRequestBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000040;
        return this;
      }
      public Builder clearPublishRequest() {
        if (publishRequestBuilder_ == null) {
          publishRequest_ = org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.getDefaultInstance();
          onChanged();
        } else {
          publishRequestBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000040);
        return this;
      }
      public org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.Builder getPublishRequestBuilder() {
        bitField0_ |= 0x00000040;
        onChanged();
        return getPublishRequestFieldBuilder().getBuilder();
      }
      public org.apache.hedwig.protocol.PubSubProtocol.PublishRequestOrBuilder getPublishRequestOrBuilder() {
        if (publishRequestBuilder_ != null) {
          return publishRequestBuilder_.getMessageOrBuilder();
        } else {
          return publishRequest_;
        }
      }
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.PublishRequest, org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.Builder, org.apache.hedwig.protocol.PubSubProtocol.PublishRequestOrBuilder> 
          getPublishRequestFieldBuilder() {
        if (publishRequestBuilder_ == null) {
          publishRequestBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hedwig.protocol.PubSubProtocol.PublishRequest, org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.Builder, org.apache.hedwig.protocol.PubSubProtocol.PublishRequestOrBuilder>(
                  publishRequest_,
                  getParentForChildren(),
                  isClean());
          publishRequest_ = null;
        }
        return publishRequestBuilder_;
      }
      
      // optional .Hedwig.SubscribeRequest subscribeRequest = 53;
      private org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest subscribeRequest_ = org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest, org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.Builder, org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequestOrBuilder> subscribeRequestBuilder_;
      public boolean hasSubscribeRequest() {
        return ((bitField0_ & 0x00000080) == 0x00000080);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest getSubscribeRequest() {
        if (subscribeRequestBuilder_ == null) {
          return subscribeRequest_;
        } else {
          return subscribeRequestBuilder_.getMessage();
        }
      }
      public Builder setSubscribeRequest(org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest value) {
        if (subscribeRequestBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          subscribeRequest_ = value;
          onChanged();
        } else {
          subscribeRequestBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      public Builder setSubscribeRequest(
          org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.Builder builderForValue) {
        if (subscribeRequestBuilder_ == null) {
          subscribeRequest_ = builderForValue.build();
          onChanged();
        } else {
          subscribeRequestBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      public Builder mergeSubscribeRequest(org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest value) {
        if (subscribeRequestBuilder_ == null) {
          if (((bitField0_ & 0x00000080) == 0x00000080) &&
              subscribeRequest_ != org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.getDefaultInstance()) {
            subscribeRequest_ =
              org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.newBuilder(subscribeRequest_).mergeFrom(value).buildPartial();
          } else {
            subscribeRequest_ = value;
          }
          onChanged();
        } else {
          subscribeRequestBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      public Builder clearSubscribeRequest() {
        if (subscribeRequestBuilder_ == null) {
          subscribeRequest_ = org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.getDefaultInstance();
          onChanged();
        } else {
          subscribeRequestBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000080);
        return this;
      }
      public org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.Builder getSubscribeRequestBuilder() {
        bitField0_ |= 0x00000080;
        onChanged();
        return getSubscribeRequestFieldBuilder().getBuilder();
      }
      public org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequestOrBuilder getSubscribeRequestOrBuilder() {
        if (subscribeRequestBuilder_ != null) {
          return subscribeRequestBuilder_.getMessageOrBuilder();
        } else {
          return subscribeRequest_;
        }
      }
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest, org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.Builder, org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequestOrBuilder> 
          getSubscribeRequestFieldBuilder() {
        if (subscribeRequestBuilder_ == null) {
          subscribeRequestBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest, org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.Builder, org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequestOrBuilder>(
                  subscribeRequest_,
                  getParentForChildren(),
                  isClean());
          subscribeRequest_ = null;
        }
        return subscribeRequestBuilder_;
      }
      
      // optional .Hedwig.ConsumeRequest consumeRequest = 54;
      private org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest consumeRequest_ = org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest, org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.Builder, org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequestOrBuilder> consumeRequestBuilder_;
      public boolean hasConsumeRequest() {
        return ((bitField0_ & 0x00000100) == 0x00000100);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest getConsumeRequest() {
        if (consumeRequestBuilder_ == null) {
          return consumeRequest_;
        } else {
          return consumeRequestBuilder_.getMessage();
        }
      }
      public Builder setConsumeRequest(org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest value) {
        if (consumeRequestBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          consumeRequest_ = value;
          onChanged();
        } else {
          consumeRequestBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000100;
        return this;
      }
      public Builder setConsumeRequest(
          org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.Builder builderForValue) {
        if (consumeRequestBuilder_ == null) {
          consumeRequest_ = builderForValue.build();
          onChanged();
        } else {
          consumeRequestBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000100;
        return this;
      }
      public Builder mergeConsumeRequest(org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest value) {
        if (consumeRequestBuilder_ == null) {
          if (((bitField0_ & 0x00000100) == 0x00000100) &&
              consumeRequest_ != org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.getDefaultInstance()) {
            consumeRequest_ =
              org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.newBuilder(consumeRequest_).mergeFrom(value).buildPartial();
          } else {
            consumeRequest_ = value;
          }
          onChanged();
        } else {
          consumeRequestBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000100;
        return this;
      }
      public Builder clearConsumeRequest() {
        if (consumeRequestBuilder_ == null) {
          consumeRequest_ = org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.getDefaultInstance();
          onChanged();
        } else {
          consumeRequestBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000100);
        return this;
      }
      public org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.Builder getConsumeRequestBuilder() {
        bitField0_ |= 0x00000100;
        onChanged();
        return getConsumeRequestFieldBuilder().getBuilder();
      }
      public org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequestOrBuilder getConsumeRequestOrBuilder() {
        if (consumeRequestBuilder_ != null) {
          return consumeRequestBuilder_.getMessageOrBuilder();
        } else {
          return consumeRequest_;
        }
      }
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest, org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.Builder, org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequestOrBuilder> 
          getConsumeRequestFieldBuilder() {
        if (consumeRequestBuilder_ == null) {
          consumeRequestBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest, org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.Builder, org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequestOrBuilder>(
                  consumeRequest_,
                  getParentForChildren(),
                  isClean());
          consumeRequest_ = null;
        }
        return consumeRequestBuilder_;
      }
      
      // optional .Hedwig.UnsubscribeRequest unsubscribeRequest = 55;
      private org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest unsubscribeRequest_ = org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest, org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.Builder, org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequestOrBuilder> unsubscribeRequestBuilder_;
      public boolean hasUnsubscribeRequest() {
        return ((bitField0_ & 0x00000200) == 0x00000200);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest getUnsubscribeRequest() {
        if (unsubscribeRequestBuilder_ == null) {
          return unsubscribeRequest_;
        } else {
          return unsubscribeRequestBuilder_.getMessage();
        }
      }
      public Builder setUnsubscribeRequest(org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest value) {
        if (unsubscribeRequestBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          unsubscribeRequest_ = value;
          onChanged();
        } else {
          unsubscribeRequestBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000200;
        return this;
      }
      public Builder setUnsubscribeRequest(
          org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.Builder builderForValue) {
        if (unsubscribeRequestBuilder_ == null) {
          unsubscribeRequest_ = builderForValue.build();
          onChanged();
        } else {
          unsubscribeRequestBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000200;
        return this;
      }
      public Builder mergeUnsubscribeRequest(org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest value) {
        if (unsubscribeRequestBuilder_ == null) {
          if (((bitField0_ & 0x00000200) == 0x00000200) &&
              unsubscribeRequest_ != org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.getDefaultInstance()) {
            unsubscribeRequest_ =
              org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.newBuilder(unsubscribeRequest_).mergeFrom(value).buildPartial();
          } else {
            unsubscribeRequest_ = value;
          }
          onChanged();
        } else {
          unsubscribeRequestBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000200;
        return this;
      }
      public Builder clearUnsubscribeRequest() {
        if (unsubscribeRequestBuilder_ == null) {
          unsubscribeRequest_ = org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.getDefaultInstance();
          onChanged();
        } else {
          unsubscribeRequestBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000200);
        return this;
      }
      public org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.Builder getUnsubscribeRequestBuilder() {
        bitField0_ |= 0x00000200;
        onChanged();
        return getUnsubscribeRequestFieldBuilder().getBuilder();
      }
      public org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequestOrBuilder getUnsubscribeRequestOrBuilder() {
        if (unsubscribeRequestBuilder_ != null) {
          return unsubscribeRequestBuilder_.getMessageOrBuilder();
        } else {
          return unsubscribeRequest_;
        }
      }
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest, org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.Builder, org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequestOrBuilder> 
          getUnsubscribeRequestFieldBuilder() {
        if (unsubscribeRequestBuilder_ == null) {
          unsubscribeRequestBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest, org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.Builder, org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequestOrBuilder>(
                  unsubscribeRequest_,
                  getParentForChildren(),
                  isClean());
          unsubscribeRequest_ = null;
        }
        return unsubscribeRequestBuilder_;
      }
      
      // optional .Hedwig.StopDeliveryRequest stopDeliveryRequest = 56;
      private org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest stopDeliveryRequest_ = org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest, org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.Builder, org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequestOrBuilder> stopDeliveryRequestBuilder_;
      public boolean hasStopDeliveryRequest() {
        return ((bitField0_ & 0x00000400) == 0x00000400);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest getStopDeliveryRequest() {
        if (stopDeliveryRequestBuilder_ == null) {
          return stopDeliveryRequest_;
        } else {
          return stopDeliveryRequestBuilder_.getMessage();
        }
      }
      public Builder setStopDeliveryRequest(org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest value) {
        if (stopDeliveryRequestBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          stopDeliveryRequest_ = value;
          onChanged();
        } else {
          stopDeliveryRequestBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000400;
        return this;
      }
      public Builder setStopDeliveryRequest(
          org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.Builder builderForValue) {
        if (stopDeliveryRequestBuilder_ == null) {
          stopDeliveryRequest_ = builderForValue.build();
          onChanged();
        } else {
          stopDeliveryRequestBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000400;
        return this;
      }
      public Builder mergeStopDeliveryRequest(org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest value) {
        if (stopDeliveryRequestBuilder_ == null) {
          if (((bitField0_ & 0x00000400) == 0x00000400) &&
              stopDeliveryRequest_ != org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.getDefaultInstance()) {
            stopDeliveryRequest_ =
              org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.newBuilder(stopDeliveryRequest_).mergeFrom(value).buildPartial();
          } else {
            stopDeliveryRequest_ = value;
          }
          onChanged();
        } else {
          stopDeliveryRequestBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000400;
        return this;
      }
      public Builder clearStopDeliveryRequest() {
        if (stopDeliveryRequestBuilder_ == null) {
          stopDeliveryRequest_ = org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.getDefaultInstance();
          onChanged();
        } else {
          stopDeliveryRequestBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000400);
        return this;
      }
      public org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.Builder getStopDeliveryRequestBuilder() {
        bitField0_ |= 0x00000400;
        onChanged();
        return getStopDeliveryRequestFieldBuilder().getBuilder();
      }
      public org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequestOrBuilder getStopDeliveryRequestOrBuilder() {
        if (stopDeliveryRequestBuilder_ != null) {
          return stopDeliveryRequestBuilder_.getMessageOrBuilder();
        } else {
          return stopDeliveryRequest_;
        }
      }
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest, org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.Builder, org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequestOrBuilder> 
          getStopDeliveryRequestFieldBuilder() {
        if (stopDeliveryRequestBuilder_ == null) {
          stopDeliveryRequestBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest, org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.Builder, org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequestOrBuilder>(
                  stopDeliveryRequest_,
                  getParentForChildren(),
                  isClean());
          stopDeliveryRequest_ = null;
        }
        return stopDeliveryRequestBuilder_;
      }
      
      // optional .Hedwig.StartDeliveryRequest startDeliveryRequest = 57;
      private org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest startDeliveryRequest_ = org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest, org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.Builder, org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequestOrBuilder> startDeliveryRequestBuilder_;
      public boolean hasStartDeliveryRequest() {
        return ((bitField0_ & 0x00000800) == 0x00000800);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest getStartDeliveryRequest() {
        if (startDeliveryRequestBuilder_ == null) {
          return startDeliveryRequest_;
        } else {
          return startDeliveryRequestBuilder_.getMessage();
        }
      }
      public Builder setStartDeliveryRequest(org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest value) {
        if (startDeliveryRequestBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          startDeliveryRequest_ = value;
          onChanged();
        } else {
          startDeliveryRequestBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000800;
        return this;
      }
      public Builder setStartDeliveryRequest(
          org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.Builder builderForValue) {
        if (startDeliveryRequestBuilder_ == null) {
          startDeliveryRequest_ = builderForValue.build();
          onChanged();
        } else {
          startDeliveryRequestBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000800;
        return this;
      }
      public Builder mergeStartDeliveryRequest(org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest value) {
        if (startDeliveryRequestBuilder_ == null) {
          if (((bitField0_ & 0x00000800) == 0x00000800) &&
              startDeliveryRequest_ != org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.getDefaultInstance()) {
            startDeliveryRequest_ =
              org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.newBuilder(startDeliveryRequest_).mergeFrom(value).buildPartial();
          } else {
            startDeliveryRequest_ = value;
          }
          onChanged();
        } else {
          startDeliveryRequestBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000800;
        return this;
      }
      public Builder clearStartDeliveryRequest() {
        if (startDeliveryRequestBuilder_ == null) {
          startDeliveryRequest_ = org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.getDefaultInstance();
          onChanged();
        } else {
          startDeliveryRequestBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000800);
        return this;
      }
      public org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.Builder getStartDeliveryRequestBuilder() {
        bitField0_ |= 0x00000800;
        onChanged();
        return getStartDeliveryRequestFieldBuilder().getBuilder();
      }
      public org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequestOrBuilder getStartDeliveryRequestOrBuilder() {
        if (startDeliveryRequestBuilder_ != null) {
          return startDeliveryRequestBuilder_.getMessageOrBuilder();
        } else {
          return startDeliveryRequest_;
        }
      }
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest, org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.Builder, org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequestOrBuilder> 
          getStartDeliveryRequestFieldBuilder() {
        if (startDeliveryRequestBuilder_ == null) {
          startDeliveryRequestBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest, org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.Builder, org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequestOrBuilder>(
                  startDeliveryRequest_,
                  getParentForChildren(),
                  isClean());
          startDeliveryRequest_ = null;
        }
        return startDeliveryRequestBuilder_;
      }
      
      // optional .Hedwig.CloseSubscriptionRequest closeSubscriptionRequest = 58;
      private org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest closeSubscriptionRequest_ = org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest, org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest.Builder, org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequestOrBuilder> closeSubscriptionRequestBuilder_;
      public boolean hasCloseSubscriptionRequest() {
        return ((bitField0_ & 0x00001000) == 0x00001000);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest getCloseSubscriptionRequest() {
        if (closeSubscriptionRequestBuilder_ == null) {
          return closeSubscriptionRequest_;
        } else {
          return closeSubscriptionRequestBuilder_.getMessage();
        }
      }
      public Builder setCloseSubscriptionRequest(org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest value) {
        if (closeSubscriptionRequestBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          closeSubscriptionRequest_ = value;
          onChanged();
        } else {
          closeSubscriptionRequestBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00001000;
        return this;
      }
      public Builder setCloseSubscriptionRequest(
          org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest.Builder builderForValue) {
        if (closeSubscriptionRequestBuilder_ == null) {
          closeSubscriptionRequest_ = builderForValue.build();
          onChanged();
        } else {
          closeSubscriptionRequestBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00001000;
        return this;
      }
      public Builder mergeCloseSubscriptionRequest(org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest value) {
        if (closeSubscriptionRequestBuilder_ == null) {
          if (((bitField0_ & 0x00001000) == 0x00001000) &&
              closeSubscriptionRequest_ != org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest.getDefaultInstance()) {
            closeSubscriptionRequest_ =
              org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest.newBuilder(closeSubscriptionRequest_).mergeFrom(value).buildPartial();
          } else {
            closeSubscriptionRequest_ = value;
          }
          onChanged();
        } else {
          closeSubscriptionRequestBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00001000;
        return this;
      }
      public Builder clearCloseSubscriptionRequest() {
        if (closeSubscriptionRequestBuilder_ == null) {
          closeSubscriptionRequest_ = org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest.getDefaultInstance();
          onChanged();
        } else {
          closeSubscriptionRequestBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00001000);
        return this;
      }
      public org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest.Builder getCloseSubscriptionRequestBuilder() {
        bitField0_ |= 0x00001000;
        onChanged();
        return getCloseSubscriptionRequestFieldBuilder().getBuilder();
      }
      public org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequestOrBuilder getCloseSubscriptionRequestOrBuilder() {
        if (closeSubscriptionRequestBuilder_ != null) {
          return closeSubscriptionRequestBuilder_.getMessageOrBuilder();
        } else {
          return closeSubscriptionRequest_;
        }
      }
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest, org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest.Builder, org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequestOrBuilder> 
          getCloseSubscriptionRequestFieldBuilder() {
        if (closeSubscriptionRequestBuilder_ == null) {
          closeSubscriptionRequestBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest, org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest.Builder, org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequestOrBuilder>(
                  closeSubscriptionRequest_,
                  getParentForChildren(),
                  isClean());
          closeSubscriptionRequest_ = null;
        }
        return closeSubscriptionRequestBuilder_;
      }
      
      // @@protoc_insertion_point(builder_scope:Hedwig.PubSubRequest)
    }
    
    static {
      defaultInstance = new PubSubRequest(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:Hedwig.PubSubRequest)
  }
  
  public interface PublishRequestOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // required .Hedwig.Message msg = 2;
    boolean hasMsg();
    org.apache.hedwig.protocol.PubSubProtocol.Message getMsg();
    org.apache.hedwig.protocol.PubSubProtocol.MessageOrBuilder getMsgOrBuilder();
  }
  public static final class PublishRequest extends
      com.google.protobuf.GeneratedMessage
      implements PublishRequestOrBuilder {
    // Use PublishRequest.newBuilder() to construct.
    private PublishRequest(Builder builder) {
      super(builder);
    }
    private PublishRequest(boolean noInit) {}
    
    private static final PublishRequest defaultInstance;
    public static PublishRequest getDefaultInstance() {
      return defaultInstance;
    }
    
    public PublishRequest getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_PublishRequest_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_PublishRequest_fieldAccessorTable;
    }
    
    private int bitField0_;
    // required .Hedwig.Message msg = 2;
    public static final int MSG_FIELD_NUMBER = 2;
    private org.apache.hedwig.protocol.PubSubProtocol.Message msg_;
    public boolean hasMsg() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.Message getMsg() {
      return msg_;
    }
    public org.apache.hedwig.protocol.PubSubProtocol.MessageOrBuilder getMsgOrBuilder() {
      return msg_;
    }
    
    private void initFields() {
      msg_ = org.apache.hedwig.protocol.PubSubProtocol.Message.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      if (!hasMsg()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getMsg().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(2, msg_);
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, msg_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.hedwig.protocol.PubSubProtocol.PublishRequest parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PublishRequest parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PublishRequest parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PublishRequest parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PublishRequest parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PublishRequest parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PublishRequest parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PublishRequest parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PublishRequest parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PublishRequest parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.PublishRequest prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hedwig.protocol.PubSubProtocol.PublishRequestOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_PublishRequest_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_PublishRequest_fieldAccessorTable;
      }
      
      // Construct using org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getMsgFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        if (msgBuilder_ == null) {
          msg_ = org.apache.hedwig.protocol.PubSubProtocol.Message.getDefaultInstance();
        } else {
          msgBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.getDescriptor();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.PublishRequest getDefaultInstanceForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.getDefaultInstance();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.PublishRequest build() {
        org.apache.hedwig.protocol.PubSubProtocol.PublishRequest result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.hedwig.protocol.PubSubProtocol.PublishRequest buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.hedwig.protocol.PubSubProtocol.PublishRequest result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.PublishRequest buildPartial() {
        org.apache.hedwig.protocol.PubSubProtocol.PublishRequest result = new org.apache.hedwig.protocol.PubSubProtocol.PublishRequest(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (msgBuilder_ == null) {
          result.msg_ = msg_;
        } else {
          result.msg_ = msgBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.PublishRequest) {
          return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.PublishRequest)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.PublishRequest other) {
        if (other == org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.getDefaultInstance()) return this;
        if (other.hasMsg()) {
          mergeMsg(other.getMsg());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        if (!hasMsg()) {
          
          return false;
        }
        if (!getMsg().isInitialized()) {
          
          return false;
        }
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 18: {
              org.apache.hedwig.protocol.PubSubProtocol.Message.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.Message.newBuilder();
              if (hasMsg()) {
                subBuilder.mergeFrom(getMsg());
              }
              input.readMessage(subBuilder, extensionRegistry);
              setMsg(subBuilder.buildPartial());
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // required .Hedwig.Message msg = 2;
      private org.apache.hedwig.protocol.PubSubProtocol.Message msg_ = org.apache.hedwig.protocol.PubSubProtocol.Message.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.Message, org.apache.hedwig.protocol.PubSubProtocol.Message.Builder, org.apache.hedwig.protocol.PubSubProtocol.MessageOrBuilder> msgBuilder_;
      public boolean hasMsg() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.Message getMsg() {
        if (msgBuilder_ == null) {
          return msg_;
        } else {
          return msgBuilder_.getMessage();
        }
      }
      public Builder setMsg(org.apache.hedwig.protocol.PubSubProtocol.Message value) {
        if (msgBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          msg_ = value;
          onChanged();
        } else {
          msgBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      public Builder setMsg(
          org.apache.hedwig.protocol.PubSubProtocol.Message.Builder builderForValue) {
        if (msgBuilder_ == null) {
          msg_ = builderForValue.build();
          onChanged();
        } else {
          msgBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      public Builder mergeMsg(org.apache.hedwig.protocol.PubSubProtocol.Message value) {
        if (msgBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              msg_ != org.apache.hedwig.protocol.PubSubProtocol.Message.getDefaultInstance()) {
            msg_ =
              org.apache.hedwig.protocol.PubSubProtocol.Message.newBuilder(msg_).mergeFrom(value).buildPartial();
          } else {
            msg_ = value;
          }
          onChanged();
        } else {
          msgBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      public Builder clearMsg() {
        if (msgBuilder_ == null) {
          msg_ = org.apache.hedwig.protocol.PubSubProtocol.Message.getDefaultInstance();
          onChanged();
        } else {
          msgBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      public org.apache.hedwig.protocol.PubSubProtocol.Message.Builder getMsgBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getMsgFieldBuilder().getBuilder();
      }
      public org.apache.hedwig.protocol.PubSubProtocol.MessageOrBuilder getMsgOrBuilder() {
        if (msgBuilder_ != null) {
          return msgBuilder_.getMessageOrBuilder();
        } else {
          return msg_;
        }
      }
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.Message, org.apache.hedwig.protocol.PubSubProtocol.Message.Builder, org.apache.hedwig.protocol.PubSubProtocol.MessageOrBuilder> 
          getMsgFieldBuilder() {
        if (msgBuilder_ == null) {
          msgBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hedwig.protocol.PubSubProtocol.Message, org.apache.hedwig.protocol.PubSubProtocol.Message.Builder, org.apache.hedwig.protocol.PubSubProtocol.MessageOrBuilder>(
                  msg_,
                  getParentForChildren(),
                  isClean());
          msg_ = null;
        }
        return msgBuilder_;
      }
      
      // @@protoc_insertion_point(builder_scope:Hedwig.PublishRequest)
    }
    
    static {
      defaultInstance = new PublishRequest(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:Hedwig.PublishRequest)
  }
  
  public interface SubscriptionPreferencesOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // optional .Hedwig.Map options = 1;
    boolean hasOptions();
    org.apache.hedwig.protocol.PubSubProtocol.Map getOptions();
    org.apache.hedwig.protocol.PubSubProtocol.MapOrBuilder getOptionsOrBuilder();
    
    // optional uint32 messageBound = 2;
    boolean hasMessageBound();
    int getMessageBound();
    
    // optional string messageFilter = 3;
    boolean hasMessageFilter();
    String getMessageFilter();
    
    // optional uint32 messageWindowSize = 4;
    boolean hasMessageWindowSize();
    int getMessageWindowSize();
  }
  public static final class SubscriptionPreferences extends
      com.google.protobuf.GeneratedMessage
      implements SubscriptionPreferencesOrBuilder {
    // Use SubscriptionPreferences.newBuilder() to construct.
    private SubscriptionPreferences(Builder builder) {
      super(builder);
    }
    private SubscriptionPreferences(boolean noInit) {}
    
    private static final SubscriptionPreferences defaultInstance;
    public static SubscriptionPreferences getDefaultInstance() {
      return defaultInstance;
    }
    
    public SubscriptionPreferences getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscriptionPreferences_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscriptionPreferences_fieldAccessorTable;
    }
    
    private int bitField0_;
    // optional .Hedwig.Map options = 1;
    public static final int OPTIONS_FIELD_NUMBER = 1;
    private org.apache.hedwig.protocol.PubSubProtocol.Map options_;
    public boolean hasOptions() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.Map getOptions() {
      return options_;
    }
    public org.apache.hedwig.protocol.PubSubProtocol.MapOrBuilder getOptionsOrBuilder() {
      return options_;
    }
    
    // optional uint32 messageBound = 2;
    public static final int MESSAGEBOUND_FIELD_NUMBER = 2;
    private int messageBound_;
    public boolean hasMessageBound() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    public int getMessageBound() {
      return messageBound_;
    }
    
    // optional string messageFilter = 3;
    public static final int MESSAGEFILTER_FIELD_NUMBER = 3;
    private java.lang.Object messageFilter_;
    public boolean hasMessageFilter() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    public String getMessageFilter() {
      java.lang.Object ref = messageFilter_;
      if (ref instanceof String) {
        return (String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        String s = bs.toStringUtf8();
        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
          messageFilter_ = s;
        }
        return s;
      }
    }
    private com.google.protobuf.ByteString getMessageFilterBytes() {
      java.lang.Object ref = messageFilter_;
      if (ref instanceof String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
        messageFilter_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }
    
    // optional uint32 messageWindowSize = 4;
    public static final int MESSAGEWINDOWSIZE_FIELD_NUMBER = 4;
    private int messageWindowSize_;
    public boolean hasMessageWindowSize() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    public int getMessageWindowSize() {
      return messageWindowSize_;
    }
    
    private void initFields() {
      options_ = org.apache.hedwig.protocol.PubSubProtocol.Map.getDefaultInstance();
      messageBound_ = 0;
      messageFilter_ = "";
      messageWindowSize_ = 0;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, options_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeUInt32(2, messageBound_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeBytes(3, getMessageFilterBytes());
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeUInt32(4, messageWindowSize_);
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, options_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(2, messageBound_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(3, getMessageFilterBytes());
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(4, messageWindowSize_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferencesOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscriptionPreferences_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscriptionPreferences_fieldAccessorTable;
      }
      
      // Construct using org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getOptionsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        if (optionsBuilder_ == null) {
          options_ = org.apache.hedwig.protocol.PubSubProtocol.Map.getDefaultInstance();
        } else {
          optionsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        messageBound_ = 0;
        bitField0_ = (bitField0_ & ~0x00000002);
        messageFilter_ = "";
        bitField0_ = (bitField0_ & ~0x00000004);
        messageWindowSize_ = 0;
        bitField0_ = (bitField0_ & ~0x00000008);
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.getDescriptor();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences getDefaultInstanceForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.getDefaultInstance();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences build() {
        org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences buildPartial() {
        org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences result = new org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (optionsBuilder_ == null) {
          result.options_ = options_;
        } else {
          result.options_ = optionsBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.messageBound_ = messageBound_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.messageFilter_ = messageFilter_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        result.messageWindowSize_ = messageWindowSize_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences) {
          return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences other) {
        if (other == org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.getDefaultInstance()) return this;
        if (other.hasOptions()) {
          mergeOptions(other.getOptions());
        }
        if (other.hasMessageBound()) {
          setMessageBound(other.getMessageBound());
        }
        if (other.hasMessageFilter()) {
          setMessageFilter(other.getMessageFilter());
        }
        if (other.hasMessageWindowSize()) {
          setMessageWindowSize(other.getMessageWindowSize());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 10: {
              org.apache.hedwig.protocol.PubSubProtocol.Map.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.Map.newBuilder();
              if (hasOptions()) {
                subBuilder.mergeFrom(getOptions());
              }
              input.readMessage(subBuilder, extensionRegistry);
              setOptions(subBuilder.buildPartial());
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              messageBound_ = input.readUInt32();
              break;
            }
            case 26: {
              bitField0_ |= 0x00000004;
              messageFilter_ = input.readBytes();
              break;
            }
            case 32: {
              bitField0_ |= 0x00000008;
              messageWindowSize_ = input.readUInt32();
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // optional .Hedwig.Map options = 1;
      private org.apache.hedwig.protocol.PubSubProtocol.Map options_ = org.apache.hedwig.protocol.PubSubProtocol.Map.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.Map, org.apache.hedwig.protocol.PubSubProtocol.Map.Builder, org.apache.hedwig.protocol.PubSubProtocol.MapOrBuilder> optionsBuilder_;
      public boolean hasOptions() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.Map getOptions() {
        if (optionsBuilder_ == null) {
          return options_;
        } else {
          return optionsBuilder_.getMessage();
        }
      }
      public Builder setOptions(org.apache.hedwig.protocol.PubSubProtocol.Map value) {
        if (optionsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          options_ = value;
          onChanged();
        } else {
          optionsBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      public Builder setOptions(
          org.apache.hedwig.protocol.PubSubProtocol.Map.Builder builderForValue) {
        if (optionsBuilder_ == null) {
          options_ = builderForValue.build();
          onChanged();
        } else {
          optionsBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      public Builder mergeOptions(org.apache.hedwig.protocol.PubSubProtocol.Map value) {
        if (optionsBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              options_ != org.apache.hedwig.protocol.PubSubProtocol.Map.getDefaultInstance()) {
            options_ =
              org.apache.hedwig.protocol.PubSubProtocol.Map.newBuilder(options_).mergeFrom(value).buildPartial();
          } else {
            options_ = value;
          }
          onChanged();
        } else {
          optionsBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      public Builder clearOptions() {
        if (optionsBuilder_ == null) {
          options_ = org.apache.hedwig.protocol.PubSubProtocol.Map.getDefaultInstance();
          onChanged();
        } else {
          optionsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      public org.apache.hedwig.protocol.PubSubProtocol.Map.Builder getOptionsBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getOptionsFieldBuilder().getBuilder();
      }
      public org.apache.hedwig.protocol.PubSubProtocol.MapOrBuilder getOptionsOrBuilder() {
        if (optionsBuilder_ != null) {
          return optionsBuilder_.getMessageOrBuilder();
        } else {
          return options_;
        }
      }
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.Map, org.apache.hedwig.protocol.PubSubProtocol.Map.Builder, org.apache.hedwig.protocol.PubSubProtocol.MapOrBuilder> 
          getOptionsFieldBuilder() {
        if (optionsBuilder_ == null) {
          optionsBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hedwig.protocol.PubSubProtocol.Map, org.apache.hedwig.protocol.PubSubProtocol.Map.Builder, org.apache.hedwig.protocol.PubSubProtocol.MapOrBuilder>(
                  options_,
                  getParentForChildren(),
                  isClean());
          options_ = null;
        }
        return optionsBuilder_;
      }
      
      // optional uint32 messageBound = 2;
      private int messageBound_ ;
      public boolean hasMessageBound() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      public int getMessageBound() {
        return messageBound_;
      }
      public Builder setMessageBound(int value) {
        bitField0_ |= 0x00000002;
        messageBound_ = value;
        onChanged();
        return this;
      }
      public Builder clearMessageBound() {
        bitField0_ = (bitField0_ & ~0x00000002);
        messageBound_ = 0;
        onChanged();
        return this;
      }
      
      // optional string messageFilter = 3;
      private java.lang.Object messageFilter_ = "";
      public boolean hasMessageFilter() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      public String getMessageFilter() {
        java.lang.Object ref = messageFilter_;
        if (!(ref instanceof String)) {
          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
          messageFilter_ = s;
          return s;
        } else {
          return (String) ref;
        }
      }
      public Builder setMessageFilter(String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000004;
        messageFilter_ = value;
        onChanged();
        return this;
      }
      public Builder clearMessageFilter() {
        bitField0_ = (bitField0_ & ~0x00000004);
        messageFilter_ = getDefaultInstance().getMessageFilter();
        onChanged();
        return this;
      }
      void setMessageFilter(com.google.protobuf.ByteString value) {
        bitField0_ |= 0x00000004;
        messageFilter_ = value;
        onChanged();
      }
      
      // optional uint32 messageWindowSize = 4;
      private int messageWindowSize_ ;
      public boolean hasMessageWindowSize() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      public int getMessageWindowSize() {
        return messageWindowSize_;
      }
      public Builder setMessageWindowSize(int value) {
        bitField0_ |= 0x00000008;
        messageWindowSize_ = value;
        onChanged();
        return this;
      }
      public Builder clearMessageWindowSize() {
        bitField0_ = (bitField0_ & ~0x00000008);
        messageWindowSize_ = 0;
        onChanged();
        return this;
      }
      
      // @@protoc_insertion_point(builder_scope:Hedwig.SubscriptionPreferences)
    }
    
    static {
      defaultInstance = new SubscriptionPreferences(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:Hedwig.SubscriptionPreferences)
  }
  
  public interface SubscribeRequestOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // required bytes subscriberId = 2;
    boolean hasSubscriberId();
    com.google.protobuf.ByteString getSubscriberId();
    
    // optional .Hedwig.SubscribeRequest.CreateOrAttach createOrAttach = 3 [default = CREATE_OR_ATTACH];
    boolean hasCreateOrAttach();
    org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach getCreateOrAttach();
    
    // optional bool synchronous = 4 [default = false];
    boolean hasSynchronous();
    boolean getSynchronous();
    
    // optional uint32 messageBound = 5;
    boolean hasMessageBound();
    int getMessageBound();
    
    // optional .Hedwig.SubscriptionPreferences preferences = 6;
    boolean hasPreferences();
    org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences getPreferences();
    org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferencesOrBuilder getPreferencesOrBuilder();
    
    // optional bool forceAttach = 7 [default = false];
    boolean hasForceAttach();
    boolean getForceAttach();
  }
  public static final class SubscribeRequest extends
      com.google.protobuf.GeneratedMessage
      implements SubscribeRequestOrBuilder {
    // Use SubscribeRequest.newBuilder() to construct.
    private SubscribeRequest(Builder builder) {
      super(builder);
    }
    private SubscribeRequest(boolean noInit) {}
    
    private static final SubscribeRequest defaultInstance;
    public static SubscribeRequest getDefaultInstance() {
      return defaultInstance;
    }
    
    public SubscribeRequest getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscribeRequest_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscribeRequest_fieldAccessorTable;
    }
    
    public enum CreateOrAttach
        implements com.google.protobuf.ProtocolMessageEnum {
      CREATE(0, 0),
      ATTACH(1, 1),
      CREATE_OR_ATTACH(2, 2),
      ;
      
      public static final int CREATE_VALUE = 0;
      public static final int ATTACH_VALUE = 1;
      public static final int CREATE_OR_ATTACH_VALUE = 2;
      
      
      public final int getNumber() { return value; }
      
      public static CreateOrAttach valueOf(int value) {
        switch (value) {
          case 0: return CREATE;
          case 1: return ATTACH;
          case 2: return CREATE_OR_ATTACH;
          default: return null;
        }
      }
      
      public static com.google.protobuf.Internal.EnumLiteMap<CreateOrAttach>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static com.google.protobuf.Internal.EnumLiteMap<CreateOrAttach>
          internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<CreateOrAttach>() {
              public CreateOrAttach findValueByNumber(int number) {
                return CreateOrAttach.valueOf(number);
              }
            };
      
      public final com.google.protobuf.Descriptors.EnumValueDescriptor
          getValueDescriptor() {
        return getDescriptor().getValues().get(index);
      }
      public final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptorForType() {
        return getDescriptor();
      }
      public static final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptor() {
        return org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.getDescriptor().getEnumTypes().get(0);
      }
      
      private static final CreateOrAttach[] VALUES = {
        CREATE, ATTACH, CREATE_OR_ATTACH, 
      };
      
      public static CreateOrAttach valueOf(
          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
        if (desc.getType() != getDescriptor()) {
          throw new java.lang.IllegalArgumentException(
            "EnumValueDescriptor is not for this type.");
        }
        return VALUES[desc.getIndex()];
      }
      
      private final int index;
      private final int value;
      
      private CreateOrAttach(int index, int value) {
        this.index = index;
        this.value = value;
      }
      
      // @@protoc_insertion_point(enum_scope:Hedwig.SubscribeRequest.CreateOrAttach)
    }
    
    private int bitField0_;
    // required bytes subscriberId = 2;
    public static final int SUBSCRIBERID_FIELD_NUMBER = 2;
    private com.google.protobuf.ByteString subscriberId_;
    public boolean hasSubscriberId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    public com.google.protobuf.ByteString getSubscriberId() {
      return subscriberId_;
    }
    
    // optional .Hedwig.SubscribeRequest.CreateOrAttach createOrAttach = 3 [default = CREATE_OR_ATTACH];
    public static final int CREATEORATTACH_FIELD_NUMBER = 3;
    private org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach createOrAttach_;
    public boolean hasCreateOrAttach() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach getCreateOrAttach() {
      return createOrAttach_;
    }
    
    // optional bool synchronous = 4 [default = false];
    public static final int SYNCHRONOUS_FIELD_NUMBER = 4;
    private boolean synchronous_;
    public boolean hasSynchronous() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    public boolean getSynchronous() {
      return synchronous_;
    }
    
    // optional uint32 messageBound = 5;
    public static final int MESSAGEBOUND_FIELD_NUMBER = 5;
    private int messageBound_;
    public boolean hasMessageBound() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    public int getMessageBound() {
      return messageBound_;
    }
    
    // optional .Hedwig.SubscriptionPreferences preferences = 6;
    public static final int PREFERENCES_FIELD_NUMBER = 6;
    private org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences preferences_;
    public boolean hasPreferences() {
      return ((bitField0_ & 0x00000010) == 0x00000010);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences getPreferences() {
      return preferences_;
    }
    public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferencesOrBuilder getPreferencesOrBuilder() {
      return preferences_;
    }
    
    // optional bool forceAttach = 7 [default = false];
    public static final int FORCEATTACH_FIELD_NUMBER = 7;
    private boolean forceAttach_;
    public boolean hasForceAttach() {
      return ((bitField0_ & 0x00000020) == 0x00000020);
    }
    public boolean getForceAttach() {
      return forceAttach_;
    }
    
    private void initFields() {
      subscriberId_ = com.google.protobuf.ByteString.EMPTY;
      createOrAttach_ = org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach.CREATE_OR_ATTACH;
      synchronous_ = false;
      messageBound_ = 0;
      preferences_ = org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.getDefaultInstance();
      forceAttach_ = false;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      if (!hasSubscriberId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(2, subscriberId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeEnum(3, createOrAttach_.getNumber());
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeBool(4, synchronous_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeUInt32(5, messageBound_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        output.writeMessage(6, preferences_);
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        output.writeBool(7, forceAttach_);
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, subscriberId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(3, createOrAttach_.getNumber());
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(4, synchronous_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(5, messageBound_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(6, preferences_);
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(7, forceAttach_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequestOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscribeRequest_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscribeRequest_fieldAccessorTable;
      }
      
      // Construct using org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getPreferencesFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        subscriberId_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        createOrAttach_ = org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach.CREATE_OR_ATTACH;
        bitField0_ = (bitField0_ & ~0x00000002);
        synchronous_ = false;
        bitField0_ = (bitField0_ & ~0x00000004);
        messageBound_ = 0;
        bitField0_ = (bitField0_ & ~0x00000008);
        if (preferencesBuilder_ == null) {
          preferences_ = org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.getDefaultInstance();
        } else {
          preferencesBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000010);
        forceAttach_ = false;
        bitField0_ = (bitField0_ & ~0x00000020);
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.getDescriptor();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest getDefaultInstanceForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.getDefaultInstance();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest build() {
        org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest buildPartial() {
        org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest result = new org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.subscriberId_ = subscriberId_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.createOrAttach_ = createOrAttach_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.synchronous_ = synchronous_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        result.messageBound_ = messageBound_;
        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
          to_bitField0_ |= 0x00000010;
        }
        if (preferencesBuilder_ == null) {
          result.preferences_ = preferences_;
        } else {
          result.preferences_ = preferencesBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
          to_bitField0_ |= 0x00000020;
        }
        result.forceAttach_ = forceAttach_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest) {
          return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest other) {
        if (other == org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.getDefaultInstance()) return this;
        if (other.hasSubscriberId()) {
          setSubscriberId(other.getSubscriberId());
        }
        if (other.hasCreateOrAttach()) {
          setCreateOrAttach(other.getCreateOrAttach());
        }
        if (other.hasSynchronous()) {
          setSynchronous(other.getSynchronous());
        }
        if (other.hasMessageBound()) {
          setMessageBound(other.getMessageBound());
        }
        if (other.hasPreferences()) {
          mergePreferences(other.getPreferences());
        }
        if (other.hasForceAttach()) {
          setForceAttach(other.getForceAttach());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        if (!hasSubscriberId()) {
          
          return false;
        }
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 18: {
              bitField0_ |= 0x00000001;
              subscriberId_ = input.readBytes();
              break;
            }
            case 24: {
              int rawValue = input.readEnum();
              org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach value = org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(3, rawValue);
              } else {
                bitField0_ |= 0x00000002;
                createOrAttach_ = value;
              }
              break;
            }
            case 32: {
              bitField0_ |= 0x00000004;
              synchronous_ = input.readBool();
              break;
            }
            case 40: {
              bitField0_ |= 0x00000008;
              messageBound_ = input.readUInt32();
              break;
            }
            case 50: {
              org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.newBuilder();
              if (hasPreferences()) {
                subBuilder.mergeFrom(getPreferences());
              }
              input.readMessage(subBuilder, extensionRegistry);
              setPreferences(subBuilder.buildPartial());
              break;
            }
            case 56: {
              bitField0_ |= 0x00000020;
              forceAttach_ = input.readBool();
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // required bytes subscriberId = 2;
      private com.google.protobuf.ByteString subscriberId_ = com.google.protobuf.ByteString.EMPTY;
      public boolean hasSubscriberId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public com.google.protobuf.ByteString getSubscriberId() {
        return subscriberId_;
      }
      public Builder setSubscriberId(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        subscriberId_ = value;
        onChanged();
        return this;
      }
      public Builder clearSubscriberId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        subscriberId_ = getDefaultInstance().getSubscriberId();
        onChanged();
        return this;
      }
      
      // optional .Hedwig.SubscribeRequest.CreateOrAttach createOrAttach = 3 [default = CREATE_OR_ATTACH];
      private org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach createOrAttach_ = org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach.CREATE_OR_ATTACH;
      public boolean hasCreateOrAttach() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach getCreateOrAttach() {
        return createOrAttach_;
      }
      public Builder setCreateOrAttach(org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000002;
        createOrAttach_ = value;
        onChanged();
        return this;
      }
      public Builder clearCreateOrAttach() {
        bitField0_ = (bitField0_ & ~0x00000002);
        createOrAttach_ = org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach.CREATE_OR_ATTACH;
        onChanged();
        return this;
      }
      
      // optional bool synchronous = 4 [default = false];
      private boolean synchronous_ ;
      public boolean hasSynchronous() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      public boolean getSynchronous() {
        return synchronous_;
      }
      public Builder setSynchronous(boolean value) {
        bitField0_ |= 0x00000004;
        synchronous_ = value;
        onChanged();
        return this;
      }
      public Builder clearSynchronous() {
        bitField0_ = (bitField0_ & ~0x00000004);
        synchronous_ = false;
        onChanged();
        return this;
      }
      
      // optional uint32 messageBound = 5;
      private int messageBound_ ;
      public boolean hasMessageBound() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      public int getMessageBound() {
        return messageBound_;
      }
      public Builder setMessageBound(int value) {
        bitField0_ |= 0x00000008;
        messageBound_ = value;
        onChanged();
        return this;
      }
      public Builder clearMessageBound() {
        bitField0_ = (bitField0_ & ~0x00000008);
        messageBound_ = 0;
        onChanged();
        return this;
      }
      
      // optional .Hedwig.SubscriptionPreferences preferences = 6;
      private org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences preferences_ = org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences, org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.Builder, org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferencesOrBuilder> preferencesBuilder_;
      public boolean hasPreferences() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences getPreferences() {
        if (preferencesBuilder_ == null) {
          return preferences_;
        } else {
          return preferencesBuilder_.getMessage();
        }
      }
      public Builder setPreferences(org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences value) {
        if (preferencesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          preferences_ = value;
          onChanged();
        } else {
          preferencesBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000010;
        return this;
      }
      public Builder setPreferences(
          org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.Builder builderForValue) {
        if (preferencesBuilder_ == null) {
          preferences_ = builderForValue.build();
          onChanged();
        } else {
          preferencesBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000010;
        return this;
      }
      public Builder mergePreferences(org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences value) {
        if (preferencesBuilder_ == null) {
          if (((bitField0_ & 0x00000010) == 0x00000010) &&
              preferences_ != org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.getDefaultInstance()) {
            preferences_ =
              org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.newBuilder(preferences_).mergeFrom(value).buildPartial();
          } else {
            preferences_ = value;
          }
          onChanged();
        } else {
          preferencesBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000010;
        return this;
      }
      public Builder clearPreferences() {
        if (preferencesBuilder_ == null) {
          preferences_ = org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.getDefaultInstance();
          onChanged();
        } else {
          preferencesBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000010);
        return this;
      }
      public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.Builder getPreferencesBuilder() {
        bitField0_ |= 0x00000010;
        onChanged();
        return getPreferencesFieldBuilder().getBuilder();
      }
      public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferencesOrBuilder getPreferencesOrBuilder() {
        if (preferencesBuilder_ != null) {
          return preferencesBuilder_.getMessageOrBuilder();
        } else {
          return preferences_;
        }
      }
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences, org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.Builder, org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferencesOrBuilder> 
          getPreferencesFieldBuilder() {
        if (preferencesBuilder_ == null) {
          preferencesBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences, org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.Builder, org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferencesOrBuilder>(
                  preferences_,
                  getParentForChildren(),
                  isClean());
          preferences_ = null;
        }
        return preferencesBuilder_;
      }
      
      // optional bool forceAttach = 7 [default = false];
      private boolean forceAttach_ ;
      public boolean hasForceAttach() {
        return ((bitField0_ & 0x00000020) == 0x00000020);
      }
      public boolean getForceAttach() {
        return forceAttach_;
      }
      public Builder setForceAttach(boolean value) {
        bitField0_ |= 0x00000020;
        forceAttach_ = value;
        onChanged();
        return this;
      }
      public Builder clearForceAttach() {
        bitField0_ = (bitField0_ & ~0x00000020);
        forceAttach_ = false;
        onChanged();
        return this;
      }
      
      // @@protoc_insertion_point(builder_scope:Hedwig.SubscribeRequest)
    }
    
    static {
      defaultInstance = new SubscribeRequest(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:Hedwig.SubscribeRequest)
  }
  
  public interface SubscriptionOptionsOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // optional bool forceAttach = 1 [default = false];
    boolean hasForceAttach();
    boolean getForceAttach();
    
    // optional .Hedwig.SubscribeRequest.CreateOrAttach createOrAttach = 2 [default = CREATE_OR_ATTACH];
    boolean hasCreateOrAttach();
    org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach getCreateOrAttach();
    
    // optional uint32 messageBound = 3 [default = 0];
    boolean hasMessageBound();
    int getMessageBound();
    
    // optional .Hedwig.Map options = 4;
    boolean hasOptions();
    org.apache.hedwig.protocol.PubSubProtocol.Map getOptions();
    org.apache.hedwig.protocol.PubSubProtocol.MapOrBuilder getOptionsOrBuilder();
    
    // optional string messageFilter = 5;
    boolean hasMessageFilter();
    String getMessageFilter();
    
    // optional uint32 messageWindowSize = 6;
    boolean hasMessageWindowSize();
    int getMessageWindowSize();
    
    // optional bool enableResubscribe = 7 [default = true];
    boolean hasEnableResubscribe();
    boolean getEnableResubscribe();
  }
  public static final class SubscriptionOptions extends
      com.google.protobuf.GeneratedMessage
      implements SubscriptionOptionsOrBuilder {
    // Use SubscriptionOptions.newBuilder() to construct.
    private SubscriptionOptions(Builder builder) {
      super(builder);
    }
    private SubscriptionOptions(boolean noInit) {}
    
    private static final SubscriptionOptions defaultInstance;
    public static SubscriptionOptions getDefaultInstance() {
      return defaultInstance;
    }
    
    public SubscriptionOptions getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscriptionOptions_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscriptionOptions_fieldAccessorTable;
    }
    
    private int bitField0_;
    // optional bool forceAttach = 1 [default = false];
    public static final int FORCEATTACH_FIELD_NUMBER = 1;
    private boolean forceAttach_;
    public boolean hasForceAttach() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    public boolean getForceAttach() {
      return forceAttach_;
    }
    
    // optional .Hedwig.SubscribeRequest.CreateOrAttach createOrAttach = 2 [default = CREATE_OR_ATTACH];
    public static final int CREATEORATTACH_FIELD_NUMBER = 2;
    private org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach createOrAttach_;
    public boolean hasCreateOrAttach() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach getCreateOrAttach() {
      return createOrAttach_;
    }
    
    // optional uint32 messageBound = 3 [default = 0];
    public static final int MESSAGEBOUND_FIELD_NUMBER = 3;
    private int messageBound_;
    public boolean hasMessageBound() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    public int getMessageBound() {
      return messageBound_;
    }
    
    // optional .Hedwig.Map options = 4;
    public static final int OPTIONS_FIELD_NUMBER = 4;
    private org.apache.hedwig.protocol.PubSubProtocol.Map options_;
    public boolean hasOptions() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.Map getOptions() {
      return options_;
    }
    public org.apache.hedwig.protocol.PubSubProtocol.MapOrBuilder getOptionsOrBuilder() {
      return options_;
    }
    
    // optional string messageFilter = 5;
    public static final int MESSAGEFILTER_FIELD_NUMBER = 5;
    private java.lang.Object messageFilter_;
    public boolean hasMessageFilter() {
      return ((bitField0_ & 0x00000010) == 0x00000010);
    }
    public String getMessageFilter() {
      java.lang.Object ref = messageFilter_;
      if (ref instanceof String) {
        return (String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        String s = bs.toStringUtf8();
        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
          messageFilter_ = s;
        }
        return s;
      }
    }
    private com.google.protobuf.ByteString getMessageFilterBytes() {
      java.lang.Object ref = messageFilter_;
      if (ref instanceof String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
        messageFilter_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }
    
    // optional uint32 messageWindowSize = 6;
    public static final int MESSAGEWINDOWSIZE_FIELD_NUMBER = 6;
    private int messageWindowSize_;
    public boolean hasMessageWindowSize() {
      return ((bitField0_ & 0x00000020) == 0x00000020);
    }
    public int getMessageWindowSize() {
      return messageWindowSize_;
    }
    
    // optional bool enableResubscribe = 7 [default = true];
    public static final int ENABLERESUBSCRIBE_FIELD_NUMBER = 7;
    private boolean enableResubscribe_;
    public boolean hasEnableResubscribe() {
      return ((bitField0_ & 0x00000040) == 0x00000040);
    }
    public boolean getEnableResubscribe() {
      return enableResubscribe_;
    }
    
    private void initFields() {
      forceAttach_ = false;
      createOrAttach_ = org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach.CREATE_OR_ATTACH;
      messageBound_ = 0;
      options_ = org.apache.hedwig.protocol.PubSubProtocol.Map.getDefaultInstance();
      messageFilter_ = "";
      messageWindowSize_ = 0;
      enableResubscribe_ = true;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBool(1, forceAttach_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeEnum(2, createOrAttach_.getNumber());
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeUInt32(3, messageBound_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeMessage(4, options_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        output.writeBytes(5, getMessageFilterBytes());
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        output.writeUInt32(6, messageWindowSize_);
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        output.writeBool(7, enableResubscribe_);
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(1, forceAttach_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(2, createOrAttach_.getNumber());
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(3, messageBound_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(4, options_);
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(5, getMessageFilterBytes());
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(6, messageWindowSize_);
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(7, enableResubscribe_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptionsOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscriptionOptions_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscriptionOptions_fieldAccessorTable;
      }
      
      // Construct using org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getOptionsFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        forceAttach_ = false;
        bitField0_ = (bitField0_ & ~0x00000001);
        createOrAttach_ = org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach.CREATE_OR_ATTACH;
        bitField0_ = (bitField0_ & ~0x00000002);
        messageBound_ = 0;
        bitField0_ = (bitField0_ & ~0x00000004);
        if (optionsBuilder_ == null) {
          options_ = org.apache.hedwig.protocol.PubSubProtocol.Map.getDefaultInstance();
        } else {
          optionsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000008);
        messageFilter_ = "";
        bitField0_ = (bitField0_ & ~0x00000010);
        messageWindowSize_ = 0;
        bitField0_ = (bitField0_ & ~0x00000020);
        enableResubscribe_ = true;
        bitField0_ = (bitField0_ & ~0x00000040);
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions.getDescriptor();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions getDefaultInstanceForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions.getDefaultInstance();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions build() {
        org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions buildPartial() {
        org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions result = new org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.forceAttach_ = forceAttach_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.createOrAttach_ = createOrAttach_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.messageBound_ = messageBound_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        if (optionsBuilder_ == null) {
          result.options_ = options_;
        } else {
          result.options_ = optionsBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
          to_bitField0_ |= 0x00000010;
        }
        result.messageFilter_ = messageFilter_;
        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
          to_bitField0_ |= 0x00000020;
        }
        result.messageWindowSize_ = messageWindowSize_;
        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
          to_bitField0_ |= 0x00000040;
        }
        result.enableResubscribe_ = enableResubscribe_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions) {
          return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions other) {
        if (other == org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions.getDefaultInstance()) return this;
        if (other.hasForceAttach()) {
          setForceAttach(other.getForceAttach());
        }
        if (other.hasCreateOrAttach()) {
          setCreateOrAttach(other.getCreateOrAttach());
        }
        if (other.hasMessageBound()) {
          setMessageBound(other.getMessageBound());
        }
        if (other.hasOptions()) {
          mergeOptions(other.getOptions());
        }
        if (other.hasMessageFilter()) {
          setMessageFilter(other.getMessageFilter());
        }
        if (other.hasMessageWindowSize()) {
          setMessageWindowSize(other.getMessageWindowSize());
        }
        if (other.hasEnableResubscribe()) {
          setEnableResubscribe(other.getEnableResubscribe());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              forceAttach_ = input.readBool();
              break;
            }
            case 16: {
              int rawValue = input.readEnum();
              org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach value = org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(2, rawValue);
              } else {
                bitField0_ |= 0x00000002;
                createOrAttach_ = value;
              }
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              messageBound_ = input.readUInt32();
              break;
            }
            case 34: {
              org.apache.hedwig.protocol.PubSubProtocol.Map.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.Map.newBuilder();
              if (hasOptions()) {
                subBuilder.mergeFrom(getOptions());
              }
              input.readMessage(subBuilder, extensionRegistry);
              setOptions(subBuilder.buildPartial());
              break;
            }
            case 42: {
              bitField0_ |= 0x00000010;
              messageFilter_ = input.readBytes();
              break;
            }
            case 48: {
              bitField0_ |= 0x00000020;
              messageWindowSize_ = input.readUInt32();
              break;
            }
            case 56: {
              bitField0_ |= 0x00000040;
              enableResubscribe_ = input.readBool();
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // optional bool forceAttach = 1 [default = false];
      private boolean forceAttach_ ;
      public boolean hasForceAttach() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public boolean getForceAttach() {
        return forceAttach_;
      }
      public Builder setForceAttach(boolean value) {
        bitField0_ |= 0x00000001;
        forceAttach_ = value;
        onChanged();
        return this;
      }
      public Builder clearForceAttach() {
        bitField0_ = (bitField0_ & ~0x00000001);
        forceAttach_ = false;
        onChanged();
        return this;
      }
      
      // optional .Hedwig.SubscribeRequest.CreateOrAttach createOrAttach = 2 [default = CREATE_OR_ATTACH];
      private org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach createOrAttach_ = org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach.CREATE_OR_ATTACH;
      public boolean hasCreateOrAttach() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach getCreateOrAttach() {
        return createOrAttach_;
      }
      public Builder setCreateOrAttach(org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000002;
        createOrAttach_ = value;
        onChanged();
        return this;
      }
      public Builder clearCreateOrAttach() {
        bitField0_ = (bitField0_ & ~0x00000002);
        createOrAttach_ = org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach.CREATE_OR_ATTACH;
        onChanged();
        return this;
      }
      
      // optional uint32 messageBound = 3 [default = 0];
      private int messageBound_ ;
      public boolean hasMessageBound() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      public int getMessageBound() {
        return messageBound_;
      }
      public Builder setMessageBound(int value) {
        bitField0_ |= 0x00000004;
        messageBound_ = value;
        onChanged();
        return this;
      }
      public Builder clearMessageBound() {
        bitField0_ = (bitField0_ & ~0x00000004);
        messageBound_ = 0;
        onChanged();
        return this;
      }
      
      // optional .Hedwig.Map options = 4;
      private org.apache.hedwig.protocol.PubSubProtocol.Map options_ = org.apache.hedwig.protocol.PubSubProtocol.Map.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.Map, org.apache.hedwig.protocol.PubSubProtocol.Map.Builder, org.apache.hedwig.protocol.PubSubProtocol.MapOrBuilder> optionsBuilder_;
      public boolean hasOptions() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.Map getOptions() {
        if (optionsBuilder_ == null) {
          return options_;
        } else {
          return optionsBuilder_.getMessage();
        }
      }
      public Builder setOptions(org.apache.hedwig.protocol.PubSubProtocol.Map value) {
        if (optionsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          options_ = value;
          onChanged();
        } else {
          optionsBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000008;
        return this;
      }
      public Builder setOptions(
          org.apache.hedwig.protocol.PubSubProtocol.Map.Builder builderForValue) {
        if (optionsBuilder_ == null) {
          options_ = builderForValue.build();
          onChanged();
        } else {
          optionsBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000008;
        return this;
      }
      public Builder mergeOptions(org.apache.hedwig.protocol.PubSubProtocol.Map value) {
        if (optionsBuilder_ == null) {
          if (((bitField0_ & 0x00000008) == 0x00000008) &&
              options_ != org.apache.hedwig.protocol.PubSubProtocol.Map.getDefaultInstance()) {
            options_ =
              org.apache.hedwig.protocol.PubSubProtocol.Map.newBuilder(options_).mergeFrom(value).buildPartial();
          } else {
            options_ = value;
          }
          onChanged();
        } else {
          optionsBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000008;
        return this;
      }
      public Builder clearOptions() {
        if (optionsBuilder_ == null) {
          options_ = org.apache.hedwig.protocol.PubSubProtocol.Map.getDefaultInstance();
          onChanged();
        } else {
          optionsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000008);
        return this;
      }
      public org.apache.hedwig.protocol.PubSubProtocol.Map.Builder getOptionsBuilder() {
        bitField0_ |= 0x00000008;
        onChanged();
        return getOptionsFieldBuilder().getBuilder();
      }
      public org.apache.hedwig.protocol.PubSubProtocol.MapOrBuilder getOptionsOrBuilder() {
        if (optionsBuilder_ != null) {
          return optionsBuilder_.getMessageOrBuilder();
        } else {
          return options_;
        }
      }
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.Map, org.apache.hedwig.protocol.PubSubProtocol.Map.Builder, org.apache.hedwig.protocol.PubSubProtocol.MapOrBuilder> 
          getOptionsFieldBuilder() {
        if (optionsBuilder_ == null) {
          optionsBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hedwig.protocol.PubSubProtocol.Map, org.apache.hedwig.protocol.PubSubProtocol.Map.Builder, org.apache.hedwig.protocol.PubSubProtocol.MapOrBuilder>(
                  options_,
                  getParentForChildren(),
                  isClean());
          options_ = null;
        }
        return optionsBuilder_;
      }
      
      // optional string messageFilter = 5;
      private java.lang.Object messageFilter_ = "";
      public boolean hasMessageFilter() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      public String getMessageFilter() {
        java.lang.Object ref = messageFilter_;
        if (!(ref instanceof String)) {
          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
          messageFilter_ = s;
          return s;
        } else {
          return (String) ref;
        }
      }
      public Builder setMessageFilter(String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000010;
        messageFilter_ = value;
        onChanged();
        return this;
      }
      public Builder clearMessageFilter() {
        bitField0_ = (bitField0_ & ~0x00000010);
        messageFilter_ = getDefaultInstance().getMessageFilter();
        onChanged();
        return this;
      }
      void setMessageFilter(com.google.protobuf.ByteString value) {
        bitField0_ |= 0x00000010;
        messageFilter_ = value;
        onChanged();
      }
      
      // optional uint32 messageWindowSize = 6;
      private int messageWindowSize_ ;
      public boolean hasMessageWindowSize() {
        return ((bitField0_ & 0x00000020) == 0x00000020);
      }
      public int getMessageWindowSize() {
        return messageWindowSize_;
      }
      public Builder setMessageWindowSize(int value) {
        bitField0_ |= 0x00000020;
        messageWindowSize_ = value;
        onChanged();
        return this;
      }
      public Builder clearMessageWindowSize() {
        bitField0_ = (bitField0_ & ~0x00000020);
        messageWindowSize_ = 0;
        onChanged();
        return this;
      }
      
      // optional bool enableResubscribe = 7 [default = true];
      private boolean enableResubscribe_ = true;
      public boolean hasEnableResubscribe() {
        return ((bitField0_ & 0x00000040) == 0x00000040);
      }
      public boolean getEnableResubscribe() {
        return enableResubscribe_;
      }
      public Builder setEnableResubscribe(boolean value) {
        bitField0_ |= 0x00000040;
        enableResubscribe_ = value;
        onChanged();
        return this;
      }
      public Builder clearEnableResubscribe() {
        bitField0_ = (bitField0_ & ~0x00000040);
        enableResubscribe_ = true;
        onChanged();
        return this;
      }
      
      // @@protoc_insertion_point(builder_scope:Hedwig.SubscriptionOptions)
    }
    
    static {
      defaultInstance = new SubscriptionOptions(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:Hedwig.SubscriptionOptions)
  }
  
  public interface ConsumeRequestOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // required bytes subscriberId = 2;
    boolean hasSubscriberId();
    com.google.protobuf.ByteString getSubscriberId();
    
    // required .Hedwig.MessageSeqId msgId = 3;
    boolean hasMsgId();
    org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId getMsgId();
    org.apache.hedwig.protocol.PubSubProtocol.MessageSeqIdOrBuilder getMsgIdOrBuilder();
  }
  public static final class ConsumeRequest extends
      com.google.protobuf.GeneratedMessage
      implements ConsumeRequestOrBuilder {
    // Use ConsumeRequest.newBuilder() to construct.
    private ConsumeRequest(Builder builder) {
      super(builder);
    }
    private ConsumeRequest(boolean noInit) {}
    
    private static final ConsumeRequest defaultInstance;
    public static ConsumeRequest getDefaultInstance() {
      return defaultInstance;
    }
    
    public ConsumeRequest getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_ConsumeRequest_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_ConsumeRequest_fieldAccessorTable;
    }
    
    private int bitField0_;
    // required bytes subscriberId = 2;
    public static final int SUBSCRIBERID_FIELD_NUMBER = 2;
    private com.google.protobuf.ByteString subscriberId_;
    public boolean hasSubscriberId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    public com.google.protobuf.ByteString getSubscriberId() {
      return subscriberId_;
    }
    
    // required .Hedwig.MessageSeqId msgId = 3;
    public static final int MSGID_FIELD_NUMBER = 3;
    private org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId msgId_;
    public boolean hasMsgId() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId getMsgId() {
      return msgId_;
    }
    public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqIdOrBuilder getMsgIdOrBuilder() {
      return msgId_;
    }
    
    private void initFields() {
      subscriberId_ = com.google.protobuf.ByteString.EMPTY;
      msgId_ = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      if (!hasSubscriberId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasMsgId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getMsgId().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(2, subscriberId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeMessage(3, msgId_);
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, subscriberId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, msgId_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequestOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_ConsumeRequest_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_ConsumeRequest_fieldAccessorTable;
      }
      
      // Construct using org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getMsgIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        subscriberId_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        if (msgIdBuilder_ == null) {
          msgId_ = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance();
        } else {
          msgIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.getDescriptor();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest getDefaultInstanceForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.getDefaultInstance();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest build() {
        org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest buildPartial() {
        org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest result = new org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.subscriberId_ = subscriberId_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        if (msgIdBuilder_ == null) {
          result.msgId_ = msgId_;
        } else {
          result.msgId_ = msgIdBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest) {
          return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest other) {
        if (other == org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.getDefaultInstance()) return this;
        if (other.hasSubscriberId()) {
          setSubscriberId(other.getSubscriberId());
        }
        if (other.hasMsgId()) {
          mergeMsgId(other.getMsgId());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        if (!hasSubscriberId()) {
          
          return false;
        }
        if (!hasMsgId()) {
          
          return false;
        }
        if (!getMsgId().isInitialized()) {
          
          return false;
        }
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 18: {
              bitField0_ |= 0x00000001;
              subscriberId_ = input.readBytes();
              break;
            }
            case 26: {
              org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.newBuilder();
              if (hasMsgId()) {
                subBuilder.mergeFrom(getMsgId());
              }
              input.readMessage(subBuilder, extensionRegistry);
              setMsgId(subBuilder.buildPartial());
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // required bytes subscriberId = 2;
      private com.google.protobuf.ByteString subscriberId_ = com.google.protobuf.ByteString.EMPTY;
      public boolean hasSubscriberId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public com.google.protobuf.ByteString getSubscriberId() {
        return subscriberId_;
      }
      public Builder setSubscriberId(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        subscriberId_ = value;
        onChanged();
        return this;
      }
      public Builder clearSubscriberId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        subscriberId_ = getDefaultInstance().getSubscriberId();
        onChanged();
        return this;
      }
      
      // required .Hedwig.MessageSeqId msgId = 3;
      private org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId msgId_ = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId, org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder, org.apache.hedwig.protocol.PubSubProtocol.MessageSeqIdOrBuilder> msgIdBuilder_;
      public boolean hasMsgId() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId getMsgId() {
        if (msgIdBuilder_ == null) {
          return msgId_;
        } else {
          return msgIdBuilder_.getMessage();
        }
      }
      public Builder setMsgId(org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId value) {
        if (msgIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          msgId_ = value;
          onChanged();
        } else {
          msgIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      public Builder setMsgId(
          org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder builderForValue) {
        if (msgIdBuilder_ == null) {
          msgId_ = builderForValue.build();
          onChanged();
        } else {
          msgIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      public Builder mergeMsgId(org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId value) {
        if (msgIdBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002) &&
              msgId_ != org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance()) {
            msgId_ =
              org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.newBuilder(msgId_).mergeFrom(value).buildPartial();
          } else {
            msgId_ = value;
          }
          onChanged();
        } else {
          msgIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      public Builder clearMsgId() {
        if (msgIdBuilder_ == null) {
          msgId_ = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance();
          onChanged();
        } else {
          msgIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder getMsgIdBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getMsgIdFieldBuilder().getBuilder();
      }
      public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqIdOrBuilder getMsgIdOrBuilder() {
        if (msgIdBuilder_ != null) {
          return msgIdBuilder_.getMessageOrBuilder();
        } else {
          return msgId_;
        }
      }
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId, org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder, org.apache.hedwig.protocol.PubSubProtocol.MessageSeqIdOrBuilder> 
          getMsgIdFieldBuilder() {
        if (msgIdBuilder_ == null) {
          msgIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId, org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder, org.apache.hedwig.protocol.PubSubProtocol.MessageSeqIdOrBuilder>(
                  msgId_,
                  getParentForChildren(),
                  isClean());
          msgId_ = null;
        }
        return msgIdBuilder_;
      }
      
      // @@protoc_insertion_point(builder_scope:Hedwig.ConsumeRequest)
    }
    
    static {
      defaultInstance = new ConsumeRequest(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:Hedwig.ConsumeRequest)
  }
  
  public interface UnsubscribeRequestOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // required bytes subscriberId = 2;
    boolean hasSubscriberId();
    com.google.protobuf.ByteString getSubscriberId();
  }
  public static final class UnsubscribeRequest extends
      com.google.protobuf.GeneratedMessage
      implements UnsubscribeRequestOrBuilder {
    // Use UnsubscribeRequest.newBuilder() to construct.
    private UnsubscribeRequest(Builder builder) {
      super(builder);
    }
    private UnsubscribeRequest(boolean noInit) {}
    
    private static final UnsubscribeRequest defaultInstance;
    public static UnsubscribeRequest getDefaultInstance() {
      return defaultInstance;
    }
    
    public UnsubscribeRequest getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_UnsubscribeRequest_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_UnsubscribeRequest_fieldAccessorTable;
    }
    
    private int bitField0_;
    // required bytes subscriberId = 2;
    public static final int SUBSCRIBERID_FIELD_NUMBER = 2;
    private com.google.protobuf.ByteString subscriberId_;
    public boolean hasSubscriberId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    public com.google.protobuf.ByteString getSubscriberId() {
      return subscriberId_;
    }
    
    private void initFields() {
      subscriberId_ = com.google.protobuf.ByteString.EMPTY;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      if (!hasSubscriberId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(2, subscriberId_);
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, subscriberId_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequestOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_UnsubscribeRequest_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_UnsubscribeRequest_fieldAccessorTable;
      }
      
      // Construct using org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        subscriberId_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.getDescriptor();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest getDefaultInstanceForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.getDefaultInstance();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest build() {
        org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest buildPartial() {
        org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest result = new org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.subscriberId_ = subscriberId_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest) {
          return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest other) {
        if (other == org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.getDefaultInstance()) return this;
        if (other.hasSubscriberId()) {
          setSubscriberId(other.getSubscriberId());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        if (!hasSubscriberId()) {
          
          return false;
        }
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 18: {
              bitField0_ |= 0x00000001;
              subscriberId_ = input.readBytes();
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // required bytes subscriberId = 2;
      private com.google.protobuf.ByteString subscriberId_ = com.google.protobuf.ByteString.EMPTY;
      public boolean hasSubscriberId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public com.google.protobuf.ByteString getSubscriberId() {
        return subscriberId_;
      }
      public Builder setSubscriberId(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        subscriberId_ = value;
        onChanged();
        return this;
      }
      public Builder clearSubscriberId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        subscriberId_ = getDefaultInstance().getSubscriberId();
        onChanged();
        return this;
      }
      
      // @@protoc_insertion_point(builder_scope:Hedwig.UnsubscribeRequest)
    }
    
    static {
      defaultInstance = new UnsubscribeRequest(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:Hedwig.UnsubscribeRequest)
  }
  
  public interface CloseSubscriptionRequestOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // required bytes subscriberId = 2;
    boolean hasSubscriberId();
    com.google.protobuf.ByteString getSubscriberId();
  }
  public static final class CloseSubscriptionRequest extends
      com.google.protobuf.GeneratedMessage
      implements CloseSubscriptionRequestOrBuilder {
    // Use CloseSubscriptionRequest.newBuilder() to construct.
    private CloseSubscriptionRequest(Builder builder) {
      super(builder);
    }
    private CloseSubscriptionRequest(boolean noInit) {}
    
    private static final CloseSubscriptionRequest defaultInstance;
    public static CloseSubscriptionRequest getDefaultInstance() {
      return defaultInstance;
    }
    
    public CloseSubscriptionRequest getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_CloseSubscriptionRequest_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_CloseSubscriptionRequest_fieldAccessorTable;
    }
    
    private int bitField0_;
    // required bytes subscriberId = 2;
    public static final int SUBSCRIBERID_FIELD_NUMBER = 2;
    private com.google.protobuf.ByteString subscriberId_;
    public boolean hasSubscriberId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    public com.google.protobuf.ByteString getSubscriberId() {
      return subscriberId_;
    }
    
    private void initFields() {
      subscriberId_ = com.google.protobuf.ByteString.EMPTY;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      if (!hasSubscriberId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(2, subscriberId_);
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, subscriberId_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequestOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_CloseSubscriptionRequest_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_CloseSubscriptionRequest_fieldAccessorTable;
      }
      
      // Construct using org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        subscriberId_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest.getDescriptor();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest getDefaultInstanceForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest.getDefaultInstance();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest build() {
        org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest buildPartial() {
        org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest result = new org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.subscriberId_ = subscriberId_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest) {
          return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest other) {
        if (other == org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest.getDefaultInstance()) return this;
        if (other.hasSubscriberId()) {
          setSubscriberId(other.getSubscriberId());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        if (!hasSubscriberId()) {
          
          return false;
        }
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 18: {
              bitField0_ |= 0x00000001;
              subscriberId_ = input.readBytes();
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // required bytes subscriberId = 2;
      private com.google.protobuf.ByteString subscriberId_ = com.google.protobuf.ByteString.EMPTY;
      public boolean hasSubscriberId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public com.google.protobuf.ByteString getSubscriberId() {
        return subscriberId_;
      }
      public Builder setSubscriberId(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        subscriberId_ = value;
        onChanged();
        return this;
      }
      public Builder clearSubscriberId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        subscriberId_ = getDefaultInstance().getSubscriberId();
        onChanged();
        return this;
      }
      
      // @@protoc_insertion_point(builder_scope:Hedwig.CloseSubscriptionRequest)
    }
    
    static {
      defaultInstance = new CloseSubscriptionRequest(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:Hedwig.CloseSubscriptionRequest)
  }
  
  public interface StopDeliveryRequestOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // required bytes subscriberId = 2;
    boolean hasSubscriberId();
    com.google.protobuf.ByteString getSubscriberId();
  }
  public static final class StopDeliveryRequest extends
      com.google.protobuf.GeneratedMessage
      implements StopDeliveryRequestOrBuilder {
    // Use StopDeliveryRequest.newBuilder() to construct.
    private StopDeliveryRequest(Builder builder) {
      super(builder);
    }
    private StopDeliveryRequest(boolean noInit) {}
    
    private static final StopDeliveryRequest defaultInstance;
    public static StopDeliveryRequest getDefaultInstance() {
      return defaultInstance;
    }
    
    public StopDeliveryRequest getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_StopDeliveryRequest_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_StopDeliveryRequest_fieldAccessorTable;
    }
    
    private int bitField0_;
    // required bytes subscriberId = 2;
    public static final int SUBSCRIBERID_FIELD_NUMBER = 2;
    private com.google.protobuf.ByteString subscriberId_;
    public boolean hasSubscriberId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    public com.google.protobuf.ByteString getSubscriberId() {
      return subscriberId_;
    }
    
    private void initFields() {
      subscriberId_ = com.google.protobuf.ByteString.EMPTY;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      if (!hasSubscriberId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(2, subscriberId_);
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, subscriberId_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequestOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_StopDeliveryRequest_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_StopDeliveryRequest_fieldAccessorTable;
      }
      
      // Construct using org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        subscriberId_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.getDescriptor();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest getDefaultInstanceForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.getDefaultInstance();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest build() {
        org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest buildPartial() {
        org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest result = new org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.subscriberId_ = subscriberId_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest) {
          return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest other) {
        if (other == org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.getDefaultInstance()) return this;
        if (other.hasSubscriberId()) {
          setSubscriberId(other.getSubscriberId());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        if (!hasSubscriberId()) {
          
          return false;
        }
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 18: {
              bitField0_ |= 0x00000001;
              subscriberId_ = input.readBytes();
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // required bytes subscriberId = 2;
      private com.google.protobuf.ByteString subscriberId_ = com.google.protobuf.ByteString.EMPTY;
      public boolean hasSubscriberId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public com.google.protobuf.ByteString getSubscriberId() {
        return subscriberId_;
      }
      public Builder setSubscriberId(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        subscriberId_ = value;
        onChanged();
        return this;
      }
      public Builder clearSubscriberId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        subscriberId_ = getDefaultInstance().getSubscriberId();
        onChanged();
        return this;
      }
      
      // @@protoc_insertion_point(builder_scope:Hedwig.StopDeliveryRequest)
    }
    
    static {
      defaultInstance = new StopDeliveryRequest(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:Hedwig.StopDeliveryRequest)
  }
  
  public interface StartDeliveryRequestOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // required bytes subscriberId = 2;
    boolean hasSubscriberId();
    com.google.protobuf.ByteString getSubscriberId();
  }
  public static final class StartDeliveryRequest extends
      com.google.protobuf.GeneratedMessage
      implements StartDeliveryRequestOrBuilder {
    // Use StartDeliveryRequest.newBuilder() to construct.
    private StartDeliveryRequest(Builder builder) {
      super(builder);
    }
    private StartDeliveryRequest(boolean noInit) {}
    
    private static final StartDeliveryRequest defaultInstance;
    public static StartDeliveryRequest getDefaultInstance() {
      return defaultInstance;
    }
    
    public StartDeliveryRequest getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_StartDeliveryRequest_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_StartDeliveryRequest_fieldAccessorTable;
    }
    
    private int bitField0_;
    // required bytes subscriberId = 2;
    public static final int SUBSCRIBERID_FIELD_NUMBER = 2;
    private com.google.protobuf.ByteString subscriberId_;
    public boolean hasSubscriberId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    public com.google.protobuf.ByteString getSubscriberId() {
      return subscriberId_;
    }
    
    private void initFields() {
      subscriberId_ = com.google.protobuf.ByteString.EMPTY;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      if (!hasSubscriberId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(2, subscriberId_);
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, subscriberId_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequestOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_StartDeliveryRequest_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_StartDeliveryRequest_fieldAccessorTable;
      }
      
      // Construct using org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        subscriberId_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.getDescriptor();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest getDefaultInstanceForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.getDefaultInstance();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest build() {
        org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest buildPartial() {
        org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest result = new org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.subscriberId_ = subscriberId_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest) {
          return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest other) {
        if (other == org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.getDefaultInstance()) return this;
        if (other.hasSubscriberId()) {
          setSubscriberId(other.getSubscriberId());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        if (!hasSubscriberId()) {
          
          return false;
        }
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 18: {
              bitField0_ |= 0x00000001;
              subscriberId_ = input.readBytes();
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // required bytes subscriberId = 2;
      private com.google.protobuf.ByteString subscriberId_ = com.google.protobuf.ByteString.EMPTY;
      public boolean hasSubscriberId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public com.google.protobuf.ByteString getSubscriberId() {
        return subscriberId_;
      }
      public Builder setSubscriberId(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        subscriberId_ = value;
        onChanged();
        return this;
      }
      public Builder clearSubscriberId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        subscriberId_ = getDefaultInstance().getSubscriberId();
        onChanged();
        return this;
      }
      
      // @@protoc_insertion_point(builder_scope:Hedwig.StartDeliveryRequest)
    }
    
    static {
      defaultInstance = new StartDeliveryRequest(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:Hedwig.StartDeliveryRequest)
  }
  
  public interface SubscriptionEventResponseOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // optional .Hedwig.SubscriptionEvent event = 1;
    boolean hasEvent();
    org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEvent getEvent();
  }
  public static final class SubscriptionEventResponse extends
      com.google.protobuf.GeneratedMessage
      implements SubscriptionEventResponseOrBuilder {
    // Use SubscriptionEventResponse.newBuilder() to construct.
    private SubscriptionEventResponse(Builder builder) {
      super(builder);
    }
    private SubscriptionEventResponse(boolean noInit) {}
    
    private static final SubscriptionEventResponse defaultInstance;
    public static SubscriptionEventResponse getDefaultInstance() {
      return defaultInstance;
    }
    
    public SubscriptionEventResponse getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscriptionEventResponse_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscriptionEventResponse_fieldAccessorTable;
    }
    
    private int bitField0_;
    // optional .Hedwig.SubscriptionEvent event = 1;
    public static final int EVENT_FIELD_NUMBER = 1;
    private org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEvent event_;
    public boolean hasEvent() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEvent getEvent() {
      return event_;
    }
    
    private void initFields() {
      event_ = org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEvent.TOPIC_MOVED;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeEnum(1, event_.getNumber());
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(1, event_.getNumber());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponseOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscriptionEventResponse_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscriptionEventResponse_fieldAccessorTable;
      }
      
      // Construct using org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        event_ = org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEvent.TOPIC_MOVED;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse.getDescriptor();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse getDefaultInstanceForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse.getDefaultInstance();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse build() {
        org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse buildPartial() {
        org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse result = new org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.event_ = event_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse) {
          return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse other) {
        if (other == org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse.getDefaultInstance()) return this;
        if (other.hasEvent()) {
          setEvent(other.getEvent());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 8: {
              int rawValue = input.readEnum();
              org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEvent value = org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEvent.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(1, rawValue);
              } else {
                bitField0_ |= 0x00000001;
                event_ = value;
              }
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // optional .Hedwig.SubscriptionEvent event = 1;
      private org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEvent event_ = org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEvent.TOPIC_MOVED;
      public boolean hasEvent() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEvent getEvent() {
        return event_;
      }
      public Builder setEvent(org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEvent value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000001;
        event_ = value;
        onChanged();
        return this;
      }
      public Builder clearEvent() {
        bitField0_ = (bitField0_ & ~0x00000001);
        event_ = org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEvent.TOPIC_MOVED;
        onChanged();
        return this;
      }
      
      // @@protoc_insertion_point(builder_scope:Hedwig.SubscriptionEventResponse)
    }
    
    static {
      defaultInstance = new SubscriptionEventResponse(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:Hedwig.SubscriptionEventResponse)
  }
  
  public interface PubSubResponseOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // required .Hedwig.ProtocolVersion protocolVersion = 1;
    boolean hasProtocolVersion();
    org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion getProtocolVersion();
    
    // required .Hedwig.StatusCode statusCode = 2;
    boolean hasStatusCode();
    org.apache.hedwig.protocol.PubSubProtocol.StatusCode getStatusCode();
    
    // required uint64 txnId = 3;
    boolean hasTxnId();
    long getTxnId();
    
    // optional string statusMsg = 4;
    boolean hasStatusMsg();
    String getStatusMsg();
    
    // optional .Hedwig.Message message = 5;
    boolean hasMessage();
    org.apache.hedwig.protocol.PubSubProtocol.Message getMessage();
    org.apache.hedwig.protocol.PubSubProtocol.MessageOrBuilder getMessageOrBuilder();
    
    // optional bytes topic = 6;
    boolean hasTopic();
    com.google.protobuf.ByteString getTopic();
    
    // optional bytes subscriberId = 7;
    boolean hasSubscriberId();
    com.google.protobuf.ByteString getSubscriberId();
    
    // optional .Hedwig.ResponseBody responseBody = 8;
    boolean hasResponseBody();
    org.apache.hedwig.protocol.PubSubProtocol.ResponseBody getResponseBody();
    org.apache.hedwig.protocol.PubSubProtocol.ResponseBodyOrBuilder getResponseBodyOrBuilder();
  }
  public static final class PubSubResponse extends
      com.google.protobuf.GeneratedMessage
      implements PubSubResponseOrBuilder {
    // Use PubSubResponse.newBuilder() to construct.
    private PubSubResponse(Builder builder) {
      super(builder);
    }
    private PubSubResponse(boolean noInit) {}
    
    private static final PubSubResponse defaultInstance;
    public static PubSubResponse getDefaultInstance() {
      return defaultInstance;
    }
    
    public PubSubResponse getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_PubSubResponse_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_PubSubResponse_fieldAccessorTable;
    }
    
    private int bitField0_;
    // required .Hedwig.ProtocolVersion protocolVersion = 1;
    public static final int PROTOCOLVERSION_FIELD_NUMBER = 1;
    private org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion protocolVersion_;
    public boolean hasProtocolVersion() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion getProtocolVersion() {
      return protocolVersion_;
    }
    
    // required .Hedwig.StatusCode statusCode = 2;
    public static final int STATUSCODE_FIELD_NUMBER = 2;
    private org.apache.hedwig.protocol.PubSubProtocol.StatusCode statusCode_;
    public boolean hasStatusCode() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.StatusCode getStatusCode() {
      return statusCode_;
    }
    
    // required uint64 txnId = 3;
    public static final int TXNID_FIELD_NUMBER = 3;
    private long txnId_;
    public boolean hasTxnId() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    public long getTxnId() {
      return txnId_;
    }
    
    // optional string statusMsg = 4;
    public static final int STATUSMSG_FIELD_NUMBER = 4;
    private java.lang.Object statusMsg_;
    public boolean hasStatusMsg() {
      return ((bitField0_ & 0x00000008) == 0x00000008);
    }
    public String getStatusMsg() {
      java.lang.Object ref = statusMsg_;
      if (ref instanceof String) {
        return (String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        String s = bs.toStringUtf8();
        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
          statusMsg_ = s;
        }
        return s;
      }
    }
    private com.google.protobuf.ByteString getStatusMsgBytes() {
      java.lang.Object ref = statusMsg_;
      if (ref instanceof String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
        statusMsg_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }
    
    // optional .Hedwig.Message message = 5;
    public static final int MESSAGE_FIELD_NUMBER = 5;
    private org.apache.hedwig.protocol.PubSubProtocol.Message message_;
    public boolean hasMessage() {
      return ((bitField0_ & 0x00000010) == 0x00000010);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.Message getMessage() {
      return message_;
    }
    public org.apache.hedwig.protocol.PubSubProtocol.MessageOrBuilder getMessageOrBuilder() {
      return message_;
    }
    
    // optional bytes topic = 6;
    public static final int TOPIC_FIELD_NUMBER = 6;
    private com.google.protobuf.ByteString topic_;
    public boolean hasTopic() {
      return ((bitField0_ & 0x00000020) == 0x00000020);
    }
    public com.google.protobuf.ByteString getTopic() {
      return topic_;
    }
    
    // optional bytes subscriberId = 7;
    public static final int SUBSCRIBERID_FIELD_NUMBER = 7;
    private com.google.protobuf.ByteString subscriberId_;
    public boolean hasSubscriberId() {
      return ((bitField0_ & 0x00000040) == 0x00000040);
    }
    public com.google.protobuf.ByteString getSubscriberId() {
      return subscriberId_;
    }
    
    // optional .Hedwig.ResponseBody responseBody = 8;
    public static final int RESPONSEBODY_FIELD_NUMBER = 8;
    private org.apache.hedwig.protocol.PubSubProtocol.ResponseBody responseBody_;
    public boolean hasResponseBody() {
      return ((bitField0_ & 0x00000080) == 0x00000080);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.ResponseBody getResponseBody() {
      return responseBody_;
    }
    public org.apache.hedwig.protocol.PubSubProtocol.ResponseBodyOrBuilder getResponseBodyOrBuilder() {
      return responseBody_;
    }
    
    private void initFields() {
      protocolVersion_ = org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion.VERSION_ONE;
      statusCode_ = org.apache.hedwig.protocol.PubSubProtocol.StatusCode.SUCCESS;
      txnId_ = 0L;
      statusMsg_ = "";
      message_ = org.apache.hedwig.protocol.PubSubProtocol.Message.getDefaultInstance();
      topic_ = com.google.protobuf.ByteString.EMPTY;
      subscriberId_ = com.google.protobuf.ByteString.EMPTY;
      responseBody_ = org.apache.hedwig.protocol.PubSubProtocol.ResponseBody.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      if (!hasProtocolVersion()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasStatusCode()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasTxnId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (hasMessage()) {
        if (!getMessage().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      if (hasResponseBody()) {
        if (!getResponseBody().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeEnum(1, protocolVersion_.getNumber());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeEnum(2, statusCode_.getNumber());
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeUInt64(3, txnId_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        output.writeBytes(4, getStatusMsgBytes());
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        output.writeMessage(5, message_);
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        output.writeBytes(6, topic_);
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        output.writeBytes(7, subscriberId_);
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        output.writeMessage(8, responseBody_);
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(1, protocolVersion_.getNumber());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(2, statusCode_.getNumber());
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(3, txnId_);
      }
      if (((bitField0_ & 0x00000008) == 0x00000008)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(4, getStatusMsgBytes());
      }
      if (((bitField0_ & 0x00000010) == 0x00000010)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(5, message_);
      }
      if (((bitField0_ & 0x00000020) == 0x00000020)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(6, topic_);
      }
      if (((bitField0_ & 0x00000040) == 0x00000040)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(7, subscriberId_);
      }
      if (((bitField0_ & 0x00000080) == 0x00000080)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(8, responseBody_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hedwig.protocol.PubSubProtocol.PubSubResponseOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_PubSubResponse_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_PubSubResponse_fieldAccessorTable;
      }
      
      // Construct using org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getMessageFieldBuilder();
          getResponseBodyFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        protocolVersion_ = org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion.VERSION_ONE;
        bitField0_ = (bitField0_ & ~0x00000001);
        statusCode_ = org.apache.hedwig.protocol.PubSubProtocol.StatusCode.SUCCESS;
        bitField0_ = (bitField0_ & ~0x00000002);
        txnId_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        statusMsg_ = "";
        bitField0_ = (bitField0_ & ~0x00000008);
        if (messageBuilder_ == null) {
          message_ = org.apache.hedwig.protocol.PubSubProtocol.Message.getDefaultInstance();
        } else {
          messageBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000010);
        topic_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000020);
        subscriberId_ = com.google.protobuf.ByteString.EMPTY;
        bitField0_ = (bitField0_ & ~0x00000040);
        if (responseBodyBuilder_ == null) {
          responseBody_ = org.apache.hedwig.protocol.PubSubProtocol.ResponseBody.getDefaultInstance();
        } else {
          responseBodyBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000080);
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse.getDescriptor();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse getDefaultInstanceForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse.getDefaultInstance();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse build() {
        org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse buildPartial() {
        org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse result = new org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.protocolVersion_ = protocolVersion_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.statusCode_ = statusCode_;
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.txnId_ = txnId_;
        if (((from_bitField0_ & 0x00000008) == 0x00000008)) {
          to_bitField0_ |= 0x00000008;
        }
        result.statusMsg_ = statusMsg_;
        if (((from_bitField0_ & 0x00000010) == 0x00000010)) {
          to_bitField0_ |= 0x00000010;
        }
        if (messageBuilder_ == null) {
          result.message_ = message_;
        } else {
          result.message_ = messageBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000020) == 0x00000020)) {
          to_bitField0_ |= 0x00000020;
        }
        result.topic_ = topic_;
        if (((from_bitField0_ & 0x00000040) == 0x00000040)) {
          to_bitField0_ |= 0x00000040;
        }
        result.subscriberId_ = subscriberId_;
        if (((from_bitField0_ & 0x00000080) == 0x00000080)) {
          to_bitField0_ |= 0x00000080;
        }
        if (responseBodyBuilder_ == null) {
          result.responseBody_ = responseBody_;
        } else {
          result.responseBody_ = responseBodyBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse) {
          return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse other) {
        if (other == org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse.getDefaultInstance()) return this;
        if (other.hasProtocolVersion()) {
          setProtocolVersion(other.getProtocolVersion());
        }
        if (other.hasStatusCode()) {
          setStatusCode(other.getStatusCode());
        }
        if (other.hasTxnId()) {
          setTxnId(other.getTxnId());
        }
        if (other.hasStatusMsg()) {
          setStatusMsg(other.getStatusMsg());
        }
        if (other.hasMessage()) {
          mergeMessage(other.getMessage());
        }
        if (other.hasTopic()) {
          setTopic(other.getTopic());
        }
        if (other.hasSubscriberId()) {
          setSubscriberId(other.getSubscriberId());
        }
        if (other.hasResponseBody()) {
          mergeResponseBody(other.getResponseBody());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        if (!hasProtocolVersion()) {
          
          return false;
        }
        if (!hasStatusCode()) {
          
          return false;
        }
        if (!hasTxnId()) {
          
          return false;
        }
        if (hasMessage()) {
          if (!getMessage().isInitialized()) {
            
            return false;
          }
        }
        if (hasResponseBody()) {
          if (!getResponseBody().isInitialized()) {
            
            return false;
          }
        }
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 8: {
              int rawValue = input.readEnum();
              org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion value = org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(1, rawValue);
              } else {
                bitField0_ |= 0x00000001;
                protocolVersion_ = value;
              }
              break;
            }
            case 16: {
              int rawValue = input.readEnum();
              org.apache.hedwig.protocol.PubSubProtocol.StatusCode value = org.apache.hedwig.protocol.PubSubProtocol.StatusCode.valueOf(rawValue);
              if (value == null) {
                unknownFields.mergeVarintField(2, rawValue);
              } else {
                bitField0_ |= 0x00000002;
                statusCode_ = value;
              }
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              txnId_ = input.readUInt64();
              break;
            }
            case 34: {
              bitField0_ |= 0x00000008;
              statusMsg_ = input.readBytes();
              break;
            }
            case 42: {
              org.apache.hedwig.protocol.PubSubProtocol.Message.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.Message.newBuilder();
              if (hasMessage()) {
                subBuilder.mergeFrom(getMessage());
              }
              input.readMessage(subBuilder, extensionRegistry);
              setMessage(subBuilder.buildPartial());
              break;
            }
            case 50: {
              bitField0_ |= 0x00000020;
              topic_ = input.readBytes();
              break;
            }
            case 58: {
              bitField0_ |= 0x00000040;
              subscriberId_ = input.readBytes();
              break;
            }
            case 66: {
              org.apache.hedwig.protocol.PubSubProtocol.ResponseBody.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.ResponseBody.newBuilder();
              if (hasResponseBody()) {
                subBuilder.mergeFrom(getResponseBody());
              }
              input.readMessage(subBuilder, extensionRegistry);
              setResponseBody(subBuilder.buildPartial());
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // required .Hedwig.ProtocolVersion protocolVersion = 1;
      private org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion protocolVersion_ = org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion.VERSION_ONE;
      public boolean hasProtocolVersion() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion getProtocolVersion() {
        return protocolVersion_;
      }
      public Builder setProtocolVersion(org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000001;
        protocolVersion_ = value;
        onChanged();
        return this;
      }
      public Builder clearProtocolVersion() {
        bitField0_ = (bitField0_ & ~0x00000001);
        protocolVersion_ = org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion.VERSION_ONE;
        onChanged();
        return this;
      }
      
      // required .Hedwig.StatusCode statusCode = 2;
      private org.apache.hedwig.protocol.PubSubProtocol.StatusCode statusCode_ = org.apache.hedwig.protocol.PubSubProtocol.StatusCode.SUCCESS;
      public boolean hasStatusCode() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.StatusCode getStatusCode() {
        return statusCode_;
      }
      public Builder setStatusCode(org.apache.hedwig.protocol.PubSubProtocol.StatusCode value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000002;
        statusCode_ = value;
        onChanged();
        return this;
      }
      public Builder clearStatusCode() {
        bitField0_ = (bitField0_ & ~0x00000002);
        statusCode_ = org.apache.hedwig.protocol.PubSubProtocol.StatusCode.SUCCESS;
        onChanged();
        return this;
      }
      
      // required uint64 txnId = 3;
      private long txnId_ ;
      public boolean hasTxnId() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      public long getTxnId() {
        return txnId_;
      }
      public Builder setTxnId(long value) {
        bitField0_ |= 0x00000004;
        txnId_ = value;
        onChanged();
        return this;
      }
      public Builder clearTxnId() {
        bitField0_ = (bitField0_ & ~0x00000004);
        txnId_ = 0L;
        onChanged();
        return this;
      }
      
      // optional string statusMsg = 4;
      private java.lang.Object statusMsg_ = "";
      public boolean hasStatusMsg() {
        return ((bitField0_ & 0x00000008) == 0x00000008);
      }
      public String getStatusMsg() {
        java.lang.Object ref = statusMsg_;
        if (!(ref instanceof String)) {
          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
          statusMsg_ = s;
          return s;
        } else {
          return (String) ref;
        }
      }
      public Builder setStatusMsg(String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000008;
        statusMsg_ = value;
        onChanged();
        return this;
      }
      public Builder clearStatusMsg() {
        bitField0_ = (bitField0_ & ~0x00000008);
        statusMsg_ = getDefaultInstance().getStatusMsg();
        onChanged();
        return this;
      }
      void setStatusMsg(com.google.protobuf.ByteString value) {
        bitField0_ |= 0x00000008;
        statusMsg_ = value;
        onChanged();
      }
      
      // optional .Hedwig.Message message = 5;
      private org.apache.hedwig.protocol.PubSubProtocol.Message message_ = org.apache.hedwig.protocol.PubSubProtocol.Message.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.Message, org.apache.hedwig.protocol.PubSubProtocol.Message.Builder, org.apache.hedwig.protocol.PubSubProtocol.MessageOrBuilder> messageBuilder_;
      public boolean hasMessage() {
        return ((bitField0_ & 0x00000010) == 0x00000010);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.Message getMessage() {
        if (messageBuilder_ == null) {
          return message_;
        } else {
          return messageBuilder_.getMessage();
        }
      }
      public Builder setMessage(org.apache.hedwig.protocol.PubSubProtocol.Message value) {
        if (messageBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          message_ = value;
          onChanged();
        } else {
          messageBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000010;
        return this;
      }
      public Builder setMessage(
          org.apache.hedwig.protocol.PubSubProtocol.Message.Builder builderForValue) {
        if (messageBuilder_ == null) {
          message_ = builderForValue.build();
          onChanged();
        } else {
          messageBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000010;
        return this;
      }
      public Builder mergeMessage(org.apache.hedwig.protocol.PubSubProtocol.Message value) {
        if (messageBuilder_ == null) {
          if (((bitField0_ & 0x00000010) == 0x00000010) &&
              message_ != org.apache.hedwig.protocol.PubSubProtocol.Message.getDefaultInstance()) {
            message_ =
              org.apache.hedwig.protocol.PubSubProtocol.Message.newBuilder(message_).mergeFrom(value).buildPartial();
          } else {
            message_ = value;
          }
          onChanged();
        } else {
          messageBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000010;
        return this;
      }
      public Builder clearMessage() {
        if (messageBuilder_ == null) {
          message_ = org.apache.hedwig.protocol.PubSubProtocol.Message.getDefaultInstance();
          onChanged();
        } else {
          messageBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000010);
        return this;
      }
      public org.apache.hedwig.protocol.PubSubProtocol.Message.Builder getMessageBuilder() {
        bitField0_ |= 0x00000010;
        onChanged();
        return getMessageFieldBuilder().getBuilder();
      }
      public org.apache.hedwig.protocol.PubSubProtocol.MessageOrBuilder getMessageOrBuilder() {
        if (messageBuilder_ != null) {
          return messageBuilder_.getMessageOrBuilder();
        } else {
          return message_;
        }
      }
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.Message, org.apache.hedwig.protocol.PubSubProtocol.Message.Builder, org.apache.hedwig.protocol.PubSubProtocol.MessageOrBuilder> 
          getMessageFieldBuilder() {
        if (messageBuilder_ == null) {
          messageBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hedwig.protocol.PubSubProtocol.Message, org.apache.hedwig.protocol.PubSubProtocol.Message.Builder, org.apache.hedwig.protocol.PubSubProtocol.MessageOrBuilder>(
                  message_,
                  getParentForChildren(),
                  isClean());
          message_ = null;
        }
        return messageBuilder_;
      }
      
      // optional bytes topic = 6;
      private com.google.protobuf.ByteString topic_ = com.google.protobuf.ByteString.EMPTY;
      public boolean hasTopic() {
        return ((bitField0_ & 0x00000020) == 0x00000020);
      }
      public com.google.protobuf.ByteString getTopic() {
        return topic_;
      }
      public Builder setTopic(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000020;
        topic_ = value;
        onChanged();
        return this;
      }
      public Builder clearTopic() {
        bitField0_ = (bitField0_ & ~0x00000020);
        topic_ = getDefaultInstance().getTopic();
        onChanged();
        return this;
      }
      
      // optional bytes subscriberId = 7;
      private com.google.protobuf.ByteString subscriberId_ = com.google.protobuf.ByteString.EMPTY;
      public boolean hasSubscriberId() {
        return ((bitField0_ & 0x00000040) == 0x00000040);
      }
      public com.google.protobuf.ByteString getSubscriberId() {
        return subscriberId_;
      }
      public Builder setSubscriberId(com.google.protobuf.ByteString value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000040;
        subscriberId_ = value;
        onChanged();
        return this;
      }
      public Builder clearSubscriberId() {
        bitField0_ = (bitField0_ & ~0x00000040);
        subscriberId_ = getDefaultInstance().getSubscriberId();
        onChanged();
        return this;
      }
      
      // optional .Hedwig.ResponseBody responseBody = 8;
      private org.apache.hedwig.protocol.PubSubProtocol.ResponseBody responseBody_ = org.apache.hedwig.protocol.PubSubProtocol.ResponseBody.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.ResponseBody, org.apache.hedwig.protocol.PubSubProtocol.ResponseBody.Builder, org.apache.hedwig.protocol.PubSubProtocol.ResponseBodyOrBuilder> responseBodyBuilder_;
      public boolean hasResponseBody() {
        return ((bitField0_ & 0x00000080) == 0x00000080);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.ResponseBody getResponseBody() {
        if (responseBodyBuilder_ == null) {
          return responseBody_;
        } else {
          return responseBodyBuilder_.getMessage();
        }
      }
      public Builder setResponseBody(org.apache.hedwig.protocol.PubSubProtocol.ResponseBody value) {
        if (responseBodyBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          responseBody_ = value;
          onChanged();
        } else {
          responseBodyBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      public Builder setResponseBody(
          org.apache.hedwig.protocol.PubSubProtocol.ResponseBody.Builder builderForValue) {
        if (responseBodyBuilder_ == null) {
          responseBody_ = builderForValue.build();
          onChanged();
        } else {
          responseBodyBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      public Builder mergeResponseBody(org.apache.hedwig.protocol.PubSubProtocol.ResponseBody value) {
        if (responseBodyBuilder_ == null) {
          if (((bitField0_ & 0x00000080) == 0x00000080) &&
              responseBody_ != org.apache.hedwig.protocol.PubSubProtocol.ResponseBody.getDefaultInstance()) {
            responseBody_ =
              org.apache.hedwig.protocol.PubSubProtocol.ResponseBody.newBuilder(responseBody_).mergeFrom(value).buildPartial();
          } else {
            responseBody_ = value;
          }
          onChanged();
        } else {
          responseBodyBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000080;
        return this;
      }
      public Builder clearResponseBody() {
        if (responseBodyBuilder_ == null) {
          responseBody_ = org.apache.hedwig.protocol.PubSubProtocol.ResponseBody.getDefaultInstance();
          onChanged();
        } else {
          responseBodyBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000080);
        return this;
      }
      public org.apache.hedwig.protocol.PubSubProtocol.ResponseBody.Builder getResponseBodyBuilder() {
        bitField0_ |= 0x00000080;
        onChanged();
        return getResponseBodyFieldBuilder().getBuilder();
      }
      public org.apache.hedwig.protocol.PubSubProtocol.ResponseBodyOrBuilder getResponseBodyOrBuilder() {
        if (responseBodyBuilder_ != null) {
          return responseBodyBuilder_.getMessageOrBuilder();
        } else {
          return responseBody_;
        }
      }
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.ResponseBody, org.apache.hedwig.protocol.PubSubProtocol.ResponseBody.Builder, org.apache.hedwig.protocol.PubSubProtocol.ResponseBodyOrBuilder> 
          getResponseBodyFieldBuilder() {
        if (responseBodyBuilder_ == null) {
          responseBodyBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hedwig.protocol.PubSubProtocol.ResponseBody, org.apache.hedwig.protocol.PubSubProtocol.ResponseBody.Builder, org.apache.hedwig.protocol.PubSubProtocol.ResponseBodyOrBuilder>(
                  responseBody_,
                  getParentForChildren(),
                  isClean());
          responseBody_ = null;
        }
        return responseBodyBuilder_;
      }
      
      // @@protoc_insertion_point(builder_scope:Hedwig.PubSubResponse)
    }
    
    static {
      defaultInstance = new PubSubResponse(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:Hedwig.PubSubResponse)
  }
  
  public interface PublishResponseOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // required .Hedwig.MessageSeqId publishedMsgId = 1;
    boolean hasPublishedMsgId();
    org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId getPublishedMsgId();
    org.apache.hedwig.protocol.PubSubProtocol.MessageSeqIdOrBuilder getPublishedMsgIdOrBuilder();
  }
  public static final class PublishResponse extends
      com.google.protobuf.GeneratedMessage
      implements PublishResponseOrBuilder {
    // Use PublishResponse.newBuilder() to construct.
    private PublishResponse(Builder builder) {
      super(builder);
    }
    private PublishResponse(boolean noInit) {}
    
    private static final PublishResponse defaultInstance;
    public static PublishResponse getDefaultInstance() {
      return defaultInstance;
    }
    
    public PublishResponse getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_PublishResponse_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_PublishResponse_fieldAccessorTable;
    }
    
    private int bitField0_;
    // required .Hedwig.MessageSeqId publishedMsgId = 1;
    public static final int PUBLISHEDMSGID_FIELD_NUMBER = 1;
    private org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId publishedMsgId_;
    public boolean hasPublishedMsgId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId getPublishedMsgId() {
      return publishedMsgId_;
    }
    public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqIdOrBuilder getPublishedMsgIdOrBuilder() {
      return publishedMsgId_;
    }
    
    private void initFields() {
      publishedMsgId_ = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      if (!hasPublishedMsgId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getPublishedMsgId().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, publishedMsgId_);
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, publishedMsgId_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.hedwig.protocol.PubSubProtocol.PublishResponse parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PublishResponse parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PublishResponse parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PublishResponse parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PublishResponse parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PublishResponse parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PublishResponse parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PublishResponse parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PublishResponse parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.PublishResponse parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.PublishResponse prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hedwig.protocol.PubSubProtocol.PublishResponseOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_PublishResponse_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_PublishResponse_fieldAccessorTable;
      }
      
      // Construct using org.apache.hedwig.protocol.PubSubProtocol.PublishResponse.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getPublishedMsgIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        if (publishedMsgIdBuilder_ == null) {
          publishedMsgId_ = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance();
        } else {
          publishedMsgIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.PublishResponse.getDescriptor();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.PublishResponse getDefaultInstanceForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.PublishResponse.getDefaultInstance();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.PublishResponse build() {
        org.apache.hedwig.protocol.PubSubProtocol.PublishResponse result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.hedwig.protocol.PubSubProtocol.PublishResponse buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.hedwig.protocol.PubSubProtocol.PublishResponse result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.PublishResponse buildPartial() {
        org.apache.hedwig.protocol.PubSubProtocol.PublishResponse result = new org.apache.hedwig.protocol.PubSubProtocol.PublishResponse(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (publishedMsgIdBuilder_ == null) {
          result.publishedMsgId_ = publishedMsgId_;
        } else {
          result.publishedMsgId_ = publishedMsgIdBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.PublishResponse) {
          return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.PublishResponse)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.PublishResponse other) {
        if (other == org.apache.hedwig.protocol.PubSubProtocol.PublishResponse.getDefaultInstance()) return this;
        if (other.hasPublishedMsgId()) {
          mergePublishedMsgId(other.getPublishedMsgId());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        if (!hasPublishedMsgId()) {
          
          return false;
        }
        if (!getPublishedMsgId().isInitialized()) {
          
          return false;
        }
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 10: {
              org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.newBuilder();
              if (hasPublishedMsgId()) {
                subBuilder.mergeFrom(getPublishedMsgId());
              }
              input.readMessage(subBuilder, extensionRegistry);
              setPublishedMsgId(subBuilder.buildPartial());
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // required .Hedwig.MessageSeqId publishedMsgId = 1;
      private org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId publishedMsgId_ = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId, org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder, org.apache.hedwig.protocol.PubSubProtocol.MessageSeqIdOrBuilder> publishedMsgIdBuilder_;
      public boolean hasPublishedMsgId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId getPublishedMsgId() {
        if (publishedMsgIdBuilder_ == null) {
          return publishedMsgId_;
        } else {
          return publishedMsgIdBuilder_.getMessage();
        }
      }
      public Builder setPublishedMsgId(org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId value) {
        if (publishedMsgIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          publishedMsgId_ = value;
          onChanged();
        } else {
          publishedMsgIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      public Builder setPublishedMsgId(
          org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder builderForValue) {
        if (publishedMsgIdBuilder_ == null) {
          publishedMsgId_ = builderForValue.build();
          onChanged();
        } else {
          publishedMsgIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      public Builder mergePublishedMsgId(org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId value) {
        if (publishedMsgIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              publishedMsgId_ != org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance()) {
            publishedMsgId_ =
              org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.newBuilder(publishedMsgId_).mergeFrom(value).buildPartial();
          } else {
            publishedMsgId_ = value;
          }
          onChanged();
        } else {
          publishedMsgIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      public Builder clearPublishedMsgId() {
        if (publishedMsgIdBuilder_ == null) {
          publishedMsgId_ = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance();
          onChanged();
        } else {
          publishedMsgIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder getPublishedMsgIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getPublishedMsgIdFieldBuilder().getBuilder();
      }
      public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqIdOrBuilder getPublishedMsgIdOrBuilder() {
        if (publishedMsgIdBuilder_ != null) {
          return publishedMsgIdBuilder_.getMessageOrBuilder();
        } else {
          return publishedMsgId_;
        }
      }
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId, org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder, org.apache.hedwig.protocol.PubSubProtocol.MessageSeqIdOrBuilder> 
          getPublishedMsgIdFieldBuilder() {
        if (publishedMsgIdBuilder_ == null) {
          publishedMsgIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId, org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder, org.apache.hedwig.protocol.PubSubProtocol.MessageSeqIdOrBuilder>(
                  publishedMsgId_,
                  getParentForChildren(),
                  isClean());
          publishedMsgId_ = null;
        }
        return publishedMsgIdBuilder_;
      }
      
      // @@protoc_insertion_point(builder_scope:Hedwig.PublishResponse)
    }
    
    static {
      defaultInstance = new PublishResponse(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:Hedwig.PublishResponse)
  }
  
  public interface SubscribeResponseOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // optional .Hedwig.SubscriptionPreferences preferences = 2;
    boolean hasPreferences();
    org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences getPreferences();
    org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferencesOrBuilder getPreferencesOrBuilder();
  }
  public static final class SubscribeResponse extends
      com.google.protobuf.GeneratedMessage
      implements SubscribeResponseOrBuilder {
    // Use SubscribeResponse.newBuilder() to construct.
    private SubscribeResponse(Builder builder) {
      super(builder);
    }
    private SubscribeResponse(boolean noInit) {}
    
    private static final SubscribeResponse defaultInstance;
    public static SubscribeResponse getDefaultInstance() {
      return defaultInstance;
    }
    
    public SubscribeResponse getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscribeResponse_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscribeResponse_fieldAccessorTable;
    }
    
    private int bitField0_;
    // optional .Hedwig.SubscriptionPreferences preferences = 2;
    public static final int PREFERENCES_FIELD_NUMBER = 2;
    private org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences preferences_;
    public boolean hasPreferences() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences getPreferences() {
      return preferences_;
    }
    public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferencesOrBuilder getPreferencesOrBuilder() {
      return preferences_;
    }
    
    private void initFields() {
      preferences_ = org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(2, preferences_);
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, preferences_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponseOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscribeResponse_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscribeResponse_fieldAccessorTable;
      }
      
      // Construct using org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getPreferencesFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        if (preferencesBuilder_ == null) {
          preferences_ = org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.getDefaultInstance();
        } else {
          preferencesBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse.getDescriptor();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse getDefaultInstanceForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse.getDefaultInstance();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse build() {
        org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse buildPartial() {
        org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse result = new org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (preferencesBuilder_ == null) {
          result.preferences_ = preferences_;
        } else {
          result.preferences_ = preferencesBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse) {
          return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse other) {
        if (other == org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse.getDefaultInstance()) return this;
        if (other.hasPreferences()) {
          mergePreferences(other.getPreferences());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 18: {
              org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.newBuilder();
              if (hasPreferences()) {
                subBuilder.mergeFrom(getPreferences());
              }
              input.readMessage(subBuilder, extensionRegistry);
              setPreferences(subBuilder.buildPartial());
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // optional .Hedwig.SubscriptionPreferences preferences = 2;
      private org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences preferences_ = org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences, org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.Builder, org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferencesOrBuilder> preferencesBuilder_;
      public boolean hasPreferences() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences getPreferences() {
        if (preferencesBuilder_ == null) {
          return preferences_;
        } else {
          return preferencesBuilder_.getMessage();
        }
      }
      public Builder setPreferences(org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences value) {
        if (preferencesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          preferences_ = value;
          onChanged();
        } else {
          preferencesBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      public Builder setPreferences(
          org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.Builder builderForValue) {
        if (preferencesBuilder_ == null) {
          preferences_ = builderForValue.build();
          onChanged();
        } else {
          preferencesBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      public Builder mergePreferences(org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences value) {
        if (preferencesBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              preferences_ != org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.getDefaultInstance()) {
            preferences_ =
              org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.newBuilder(preferences_).mergeFrom(value).buildPartial();
          } else {
            preferences_ = value;
          }
          onChanged();
        } else {
          preferencesBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      public Builder clearPreferences() {
        if (preferencesBuilder_ == null) {
          preferences_ = org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.getDefaultInstance();
          onChanged();
        } else {
          preferencesBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.Builder getPreferencesBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getPreferencesFieldBuilder().getBuilder();
      }
      public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferencesOrBuilder getPreferencesOrBuilder() {
        if (preferencesBuilder_ != null) {
          return preferencesBuilder_.getMessageOrBuilder();
        } else {
          return preferences_;
        }
      }
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences, org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.Builder, org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferencesOrBuilder> 
          getPreferencesFieldBuilder() {
        if (preferencesBuilder_ == null) {
          preferencesBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences, org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.Builder, org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferencesOrBuilder>(
                  preferences_,
                  getParentForChildren(),
                  isClean());
          preferences_ = null;
        }
        return preferencesBuilder_;
      }
      
      // @@protoc_insertion_point(builder_scope:Hedwig.SubscribeResponse)
    }
    
    static {
      defaultInstance = new SubscribeResponse(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:Hedwig.SubscribeResponse)
  }
  
  public interface ResponseBodyOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // optional .Hedwig.PublishResponse publishResponse = 1;
    boolean hasPublishResponse();
    org.apache.hedwig.protocol.PubSubProtocol.PublishResponse getPublishResponse();
    org.apache.hedwig.protocol.PubSubProtocol.PublishResponseOrBuilder getPublishResponseOrBuilder();
    
    // optional .Hedwig.SubscribeResponse subscribeResponse = 2;
    boolean hasSubscribeResponse();
    org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse getSubscribeResponse();
    org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponseOrBuilder getSubscribeResponseOrBuilder();
    
    // optional .Hedwig.SubscriptionEventResponse subscriptionEvent = 3;
    boolean hasSubscriptionEvent();
    org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse getSubscriptionEvent();
    org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponseOrBuilder getSubscriptionEventOrBuilder();
  }
  public static final class ResponseBody extends
      com.google.protobuf.GeneratedMessage
      implements ResponseBodyOrBuilder {
    // Use ResponseBody.newBuilder() to construct.
    private ResponseBody(Builder builder) {
      super(builder);
    }
    private ResponseBody(boolean noInit) {}
    
    private static final ResponseBody defaultInstance;
    public static ResponseBody getDefaultInstance() {
      return defaultInstance;
    }
    
    public ResponseBody getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_ResponseBody_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_ResponseBody_fieldAccessorTable;
    }
    
    private int bitField0_;
    // optional .Hedwig.PublishResponse publishResponse = 1;
    public static final int PUBLISHRESPONSE_FIELD_NUMBER = 1;
    private org.apache.hedwig.protocol.PubSubProtocol.PublishResponse publishResponse_;
    public boolean hasPublishResponse() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.PublishResponse getPublishResponse() {
      return publishResponse_;
    }
    public org.apache.hedwig.protocol.PubSubProtocol.PublishResponseOrBuilder getPublishResponseOrBuilder() {
      return publishResponse_;
    }
    
    // optional .Hedwig.SubscribeResponse subscribeResponse = 2;
    public static final int SUBSCRIBERESPONSE_FIELD_NUMBER = 2;
    private org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse subscribeResponse_;
    public boolean hasSubscribeResponse() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse getSubscribeResponse() {
      return subscribeResponse_;
    }
    public org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponseOrBuilder getSubscribeResponseOrBuilder() {
      return subscribeResponse_;
    }
    
    // optional .Hedwig.SubscriptionEventResponse subscriptionEvent = 3;
    public static final int SUBSCRIPTIONEVENT_FIELD_NUMBER = 3;
    private org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse subscriptionEvent_;
    public boolean hasSubscriptionEvent() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse getSubscriptionEvent() {
      return subscriptionEvent_;
    }
    public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponseOrBuilder getSubscriptionEventOrBuilder() {
      return subscriptionEvent_;
    }
    
    private void initFields() {
      publishResponse_ = org.apache.hedwig.protocol.PubSubProtocol.PublishResponse.getDefaultInstance();
      subscribeResponse_ = org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse.getDefaultInstance();
      subscriptionEvent_ = org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      if (hasPublishResponse()) {
        if (!getPublishResponse().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, publishResponse_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeMessage(2, subscribeResponse_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeMessage(3, subscriptionEvent_);
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, publishResponse_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, subscribeResponse_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(3, subscriptionEvent_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.hedwig.protocol.PubSubProtocol.ResponseBody parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.ResponseBody parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.ResponseBody parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.ResponseBody parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.ResponseBody parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.ResponseBody parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.ResponseBody parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.ResponseBody parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.ResponseBody parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.ResponseBody parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.ResponseBody prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hedwig.protocol.PubSubProtocol.ResponseBodyOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_ResponseBody_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_ResponseBody_fieldAccessorTable;
      }
      
      // Construct using org.apache.hedwig.protocol.PubSubProtocol.ResponseBody.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getPublishResponseFieldBuilder();
          getSubscribeResponseFieldBuilder();
          getSubscriptionEventFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        if (publishResponseBuilder_ == null) {
          publishResponse_ = org.apache.hedwig.protocol.PubSubProtocol.PublishResponse.getDefaultInstance();
        } else {
          publishResponseBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        if (subscribeResponseBuilder_ == null) {
          subscribeResponse_ = org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse.getDefaultInstance();
        } else {
          subscribeResponseBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        if (subscriptionEventBuilder_ == null) {
          subscriptionEvent_ = org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse.getDefaultInstance();
        } else {
          subscriptionEventBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.ResponseBody.getDescriptor();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.ResponseBody getDefaultInstanceForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.ResponseBody.getDefaultInstance();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.ResponseBody build() {
        org.apache.hedwig.protocol.PubSubProtocol.ResponseBody result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.hedwig.protocol.PubSubProtocol.ResponseBody buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.hedwig.protocol.PubSubProtocol.ResponseBody result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.ResponseBody buildPartial() {
        org.apache.hedwig.protocol.PubSubProtocol.ResponseBody result = new org.apache.hedwig.protocol.PubSubProtocol.ResponseBody(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (publishResponseBuilder_ == null) {
          result.publishResponse_ = publishResponse_;
        } else {
          result.publishResponse_ = publishResponseBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        if (subscribeResponseBuilder_ == null) {
          result.subscribeResponse_ = subscribeResponse_;
        } else {
          result.subscribeResponse_ = subscribeResponseBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        if (subscriptionEventBuilder_ == null) {
          result.subscriptionEvent_ = subscriptionEvent_;
        } else {
          result.subscriptionEvent_ = subscriptionEventBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.ResponseBody) {
          return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.ResponseBody)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.ResponseBody other) {
        if (other == org.apache.hedwig.protocol.PubSubProtocol.ResponseBody.getDefaultInstance()) return this;
        if (other.hasPublishResponse()) {
          mergePublishResponse(other.getPublishResponse());
        }
        if (other.hasSubscribeResponse()) {
          mergeSubscribeResponse(other.getSubscribeResponse());
        }
        if (other.hasSubscriptionEvent()) {
          mergeSubscriptionEvent(other.getSubscriptionEvent());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        if (hasPublishResponse()) {
          if (!getPublishResponse().isInitialized()) {
            
            return false;
          }
        }
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 10: {
              org.apache.hedwig.protocol.PubSubProtocol.PublishResponse.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.PublishResponse.newBuilder();
              if (hasPublishResponse()) {
                subBuilder.mergeFrom(getPublishResponse());
              }
              input.readMessage(subBuilder, extensionRegistry);
              setPublishResponse(subBuilder.buildPartial());
              break;
            }
            case 18: {
              org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse.newBuilder();
              if (hasSubscribeResponse()) {
                subBuilder.mergeFrom(getSubscribeResponse());
              }
              input.readMessage(subBuilder, extensionRegistry);
              setSubscribeResponse(subBuilder.buildPartial());
              break;
            }
            case 26: {
              org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse.newBuilder();
              if (hasSubscriptionEvent()) {
                subBuilder.mergeFrom(getSubscriptionEvent());
              }
              input.readMessage(subBuilder, extensionRegistry);
              setSubscriptionEvent(subBuilder.buildPartial());
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // optional .Hedwig.PublishResponse publishResponse = 1;
      private org.apache.hedwig.protocol.PubSubProtocol.PublishResponse publishResponse_ = org.apache.hedwig.protocol.PubSubProtocol.PublishResponse.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.PublishResponse, org.apache.hedwig.protocol.PubSubProtocol.PublishResponse.Builder, org.apache.hedwig.protocol.PubSubProtocol.PublishResponseOrBuilder> publishResponseBuilder_;
      public boolean hasPublishResponse() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.PublishResponse getPublishResponse() {
        if (publishResponseBuilder_ == null) {
          return publishResponse_;
        } else {
          return publishResponseBuilder_.getMessage();
        }
      }
      public Builder setPublishResponse(org.apache.hedwig.protocol.PubSubProtocol.PublishResponse value) {
        if (publishResponseBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          publishResponse_ = value;
          onChanged();
        } else {
          publishResponseBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      public Builder setPublishResponse(
          org.apache.hedwig.protocol.PubSubProtocol.PublishResponse.Builder builderForValue) {
        if (publishResponseBuilder_ == null) {
          publishResponse_ = builderForValue.build();
          onChanged();
        } else {
          publishResponseBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      public Builder mergePublishResponse(org.apache.hedwig.protocol.PubSubProtocol.PublishResponse value) {
        if (publishResponseBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              publishResponse_ != org.apache.hedwig.protocol.PubSubProtocol.PublishResponse.getDefaultInstance()) {
            publishResponse_ =
              org.apache.hedwig.protocol.PubSubProtocol.PublishResponse.newBuilder(publishResponse_).mergeFrom(value).buildPartial();
          } else {
            publishResponse_ = value;
          }
          onChanged();
        } else {
          publishResponseBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      public Builder clearPublishResponse() {
        if (publishResponseBuilder_ == null) {
          publishResponse_ = org.apache.hedwig.protocol.PubSubProtocol.PublishResponse.getDefaultInstance();
          onChanged();
        } else {
          publishResponseBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      public org.apache.hedwig.protocol.PubSubProtocol.PublishResponse.Builder getPublishResponseBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getPublishResponseFieldBuilder().getBuilder();
      }
      public org.apache.hedwig.protocol.PubSubProtocol.PublishResponseOrBuilder getPublishResponseOrBuilder() {
        if (publishResponseBuilder_ != null) {
          return publishResponseBuilder_.getMessageOrBuilder();
        } else {
          return publishResponse_;
        }
      }
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.PublishResponse, org.apache.hedwig.protocol.PubSubProtocol.PublishResponse.Builder, org.apache.hedwig.protocol.PubSubProtocol.PublishResponseOrBuilder> 
          getPublishResponseFieldBuilder() {
        if (publishResponseBuilder_ == null) {
          publishResponseBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hedwig.protocol.PubSubProtocol.PublishResponse, org.apache.hedwig.protocol.PubSubProtocol.PublishResponse.Builder, org.apache.hedwig.protocol.PubSubProtocol.PublishResponseOrBuilder>(
                  publishResponse_,
                  getParentForChildren(),
                  isClean());
          publishResponse_ = null;
        }
        return publishResponseBuilder_;
      }
      
      // optional .Hedwig.SubscribeResponse subscribeResponse = 2;
      private org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse subscribeResponse_ = org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse, org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse.Builder, org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponseOrBuilder> subscribeResponseBuilder_;
      public boolean hasSubscribeResponse() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse getSubscribeResponse() {
        if (subscribeResponseBuilder_ == null) {
          return subscribeResponse_;
        } else {
          return subscribeResponseBuilder_.getMessage();
        }
      }
      public Builder setSubscribeResponse(org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse value) {
        if (subscribeResponseBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          subscribeResponse_ = value;
          onChanged();
        } else {
          subscribeResponseBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      public Builder setSubscribeResponse(
          org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse.Builder builderForValue) {
        if (subscribeResponseBuilder_ == null) {
          subscribeResponse_ = builderForValue.build();
          onChanged();
        } else {
          subscribeResponseBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      public Builder mergeSubscribeResponse(org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse value) {
        if (subscribeResponseBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002) &&
              subscribeResponse_ != org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse.getDefaultInstance()) {
            subscribeResponse_ =
              org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse.newBuilder(subscribeResponse_).mergeFrom(value).buildPartial();
          } else {
            subscribeResponse_ = value;
          }
          onChanged();
        } else {
          subscribeResponseBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      public Builder clearSubscribeResponse() {
        if (subscribeResponseBuilder_ == null) {
          subscribeResponse_ = org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse.getDefaultInstance();
          onChanged();
        } else {
          subscribeResponseBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      public org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse.Builder getSubscribeResponseBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getSubscribeResponseFieldBuilder().getBuilder();
      }
      public org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponseOrBuilder getSubscribeResponseOrBuilder() {
        if (subscribeResponseBuilder_ != null) {
          return subscribeResponseBuilder_.getMessageOrBuilder();
        } else {
          return subscribeResponse_;
        }
      }
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse, org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse.Builder, org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponseOrBuilder> 
          getSubscribeResponseFieldBuilder() {
        if (subscribeResponseBuilder_ == null) {
          subscribeResponseBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse, org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse.Builder, org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponseOrBuilder>(
                  subscribeResponse_,
                  getParentForChildren(),
                  isClean());
          subscribeResponse_ = null;
        }
        return subscribeResponseBuilder_;
      }
      
      // optional .Hedwig.SubscriptionEventResponse subscriptionEvent = 3;
      private org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse subscriptionEvent_ = org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse, org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse.Builder, org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponseOrBuilder> subscriptionEventBuilder_;
      public boolean hasSubscriptionEvent() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse getSubscriptionEvent() {
        if (subscriptionEventBuilder_ == null) {
          return subscriptionEvent_;
        } else {
          return subscriptionEventBuilder_.getMessage();
        }
      }
      public Builder setSubscriptionEvent(org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse value) {
        if (subscriptionEventBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          subscriptionEvent_ = value;
          onChanged();
        } else {
          subscriptionEventBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      public Builder setSubscriptionEvent(
          org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse.Builder builderForValue) {
        if (subscriptionEventBuilder_ == null) {
          subscriptionEvent_ = builderForValue.build();
          onChanged();
        } else {
          subscriptionEventBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      public Builder mergeSubscriptionEvent(org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse value) {
        if (subscriptionEventBuilder_ == null) {
          if (((bitField0_ & 0x00000004) == 0x00000004) &&
              subscriptionEvent_ != org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse.getDefaultInstance()) {
            subscriptionEvent_ =
              org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse.newBuilder(subscriptionEvent_).mergeFrom(value).buildPartial();
          } else {
            subscriptionEvent_ = value;
          }
          onChanged();
        } else {
          subscriptionEventBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000004;
        return this;
      }
      public Builder clearSubscriptionEvent() {
        if (subscriptionEventBuilder_ == null) {
          subscriptionEvent_ = org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse.getDefaultInstance();
          onChanged();
        } else {
          subscriptionEventBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }
      public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse.Builder getSubscriptionEventBuilder() {
        bitField0_ |= 0x00000004;
        onChanged();
        return getSubscriptionEventFieldBuilder().getBuilder();
      }
      public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponseOrBuilder getSubscriptionEventOrBuilder() {
        if (subscriptionEventBuilder_ != null) {
          return subscriptionEventBuilder_.getMessageOrBuilder();
        } else {
          return subscriptionEvent_;
        }
      }
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse, org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse.Builder, org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponseOrBuilder> 
          getSubscriptionEventFieldBuilder() {
        if (subscriptionEventBuilder_ == null) {
          subscriptionEventBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse, org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse.Builder, org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponseOrBuilder>(
                  subscriptionEvent_,
                  getParentForChildren(),
                  isClean());
          subscriptionEvent_ = null;
        }
        return subscriptionEventBuilder_;
      }
      
      // @@protoc_insertion_point(builder_scope:Hedwig.ResponseBody)
    }
    
    static {
      defaultInstance = new ResponseBody(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:Hedwig.ResponseBody)
  }
  
  public interface SubscriptionStateOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // required .Hedwig.MessageSeqId msgId = 1;
    boolean hasMsgId();
    org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId getMsgId();
    org.apache.hedwig.protocol.PubSubProtocol.MessageSeqIdOrBuilder getMsgIdOrBuilder();
    
    // optional uint32 messageBound = 2;
    boolean hasMessageBound();
    int getMessageBound();
  }
  public static final class SubscriptionState extends
      com.google.protobuf.GeneratedMessage
      implements SubscriptionStateOrBuilder {
    // Use SubscriptionState.newBuilder() to construct.
    private SubscriptionState(Builder builder) {
      super(builder);
    }
    private SubscriptionState(boolean noInit) {}
    
    private static final SubscriptionState defaultInstance;
    public static SubscriptionState getDefaultInstance() {
      return defaultInstance;
    }
    
    public SubscriptionState getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscriptionState_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscriptionState_fieldAccessorTable;
    }
    
    private int bitField0_;
    // required .Hedwig.MessageSeqId msgId = 1;
    public static final int MSGID_FIELD_NUMBER = 1;
    private org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId msgId_;
    public boolean hasMsgId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId getMsgId() {
      return msgId_;
    }
    public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqIdOrBuilder getMsgIdOrBuilder() {
      return msgId_;
    }
    
    // optional uint32 messageBound = 2;
    public static final int MESSAGEBOUND_FIELD_NUMBER = 2;
    private int messageBound_;
    public boolean hasMessageBound() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    public int getMessageBound() {
      return messageBound_;
    }
    
    private void initFields() {
      msgId_ = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance();
      messageBound_ = 0;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      if (!hasMsgId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!getMsgId().isInitialized()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, msgId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeUInt32(2, messageBound_);
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, msgId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(2, messageBound_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hedwig.protocol.PubSubProtocol.SubscriptionStateOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscriptionState_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscriptionState_fieldAccessorTable;
      }
      
      // Construct using org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getMsgIdFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        if (msgIdBuilder_ == null) {
          msgId_ = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance();
        } else {
          msgIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        messageBound_ = 0;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState.getDescriptor();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState getDefaultInstanceForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState.getDefaultInstance();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState build() {
        org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState buildPartial() {
        org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState result = new org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (msgIdBuilder_ == null) {
          result.msgId_ = msgId_;
        } else {
          result.msgId_ = msgIdBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.messageBound_ = messageBound_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState) {
          return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState other) {
        if (other == org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState.getDefaultInstance()) return this;
        if (other.hasMsgId()) {
          mergeMsgId(other.getMsgId());
        }
        if (other.hasMessageBound()) {
          setMessageBound(other.getMessageBound());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        if (!hasMsgId()) {
          
          return false;
        }
        if (!getMsgId().isInitialized()) {
          
          return false;
        }
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 10: {
              org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.newBuilder();
              if (hasMsgId()) {
                subBuilder.mergeFrom(getMsgId());
              }
              input.readMessage(subBuilder, extensionRegistry);
              setMsgId(subBuilder.buildPartial());
              break;
            }
            case 16: {
              bitField0_ |= 0x00000002;
              messageBound_ = input.readUInt32();
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // required .Hedwig.MessageSeqId msgId = 1;
      private org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId msgId_ = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId, org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder, org.apache.hedwig.protocol.PubSubProtocol.MessageSeqIdOrBuilder> msgIdBuilder_;
      public boolean hasMsgId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId getMsgId() {
        if (msgIdBuilder_ == null) {
          return msgId_;
        } else {
          return msgIdBuilder_.getMessage();
        }
      }
      public Builder setMsgId(org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId value) {
        if (msgIdBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          msgId_ = value;
          onChanged();
        } else {
          msgIdBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      public Builder setMsgId(
          org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder builderForValue) {
        if (msgIdBuilder_ == null) {
          msgId_ = builderForValue.build();
          onChanged();
        } else {
          msgIdBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      public Builder mergeMsgId(org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId value) {
        if (msgIdBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              msgId_ != org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance()) {
            msgId_ =
              org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.newBuilder(msgId_).mergeFrom(value).buildPartial();
          } else {
            msgId_ = value;
          }
          onChanged();
        } else {
          msgIdBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      public Builder clearMsgId() {
        if (msgIdBuilder_ == null) {
          msgId_ = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance();
          onChanged();
        } else {
          msgIdBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder getMsgIdBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getMsgIdFieldBuilder().getBuilder();
      }
      public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqIdOrBuilder getMsgIdOrBuilder() {
        if (msgIdBuilder_ != null) {
          return msgIdBuilder_.getMessageOrBuilder();
        } else {
          return msgId_;
        }
      }
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId, org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder, org.apache.hedwig.protocol.PubSubProtocol.MessageSeqIdOrBuilder> 
          getMsgIdFieldBuilder() {
        if (msgIdBuilder_ == null) {
          msgIdBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId, org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder, org.apache.hedwig.protocol.PubSubProtocol.MessageSeqIdOrBuilder>(
                  msgId_,
                  getParentForChildren(),
                  isClean());
          msgId_ = null;
        }
        return msgIdBuilder_;
      }
      
      // optional uint32 messageBound = 2;
      private int messageBound_ ;
      public boolean hasMessageBound() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      public int getMessageBound() {
        return messageBound_;
      }
      public Builder setMessageBound(int value) {
        bitField0_ |= 0x00000002;
        messageBound_ = value;
        onChanged();
        return this;
      }
      public Builder clearMessageBound() {
        bitField0_ = (bitField0_ & ~0x00000002);
        messageBound_ = 0;
        onChanged();
        return this;
      }
      
      // @@protoc_insertion_point(builder_scope:Hedwig.SubscriptionState)
    }
    
    static {
      defaultInstance = new SubscriptionState(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:Hedwig.SubscriptionState)
  }
  
  public interface SubscriptionDataOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // optional .Hedwig.SubscriptionState state = 1;
    boolean hasState();
    org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState getState();
    org.apache.hedwig.protocol.PubSubProtocol.SubscriptionStateOrBuilder getStateOrBuilder();
    
    // optional .Hedwig.SubscriptionPreferences preferences = 2;
    boolean hasPreferences();
    org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences getPreferences();
    org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferencesOrBuilder getPreferencesOrBuilder();
  }
  public static final class SubscriptionData extends
      com.google.protobuf.GeneratedMessage
      implements SubscriptionDataOrBuilder {
    // Use SubscriptionData.newBuilder() to construct.
    private SubscriptionData(Builder builder) {
      super(builder);
    }
    private SubscriptionData(boolean noInit) {}
    
    private static final SubscriptionData defaultInstance;
    public static SubscriptionData getDefaultInstance() {
      return defaultInstance;
    }
    
    public SubscriptionData getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscriptionData_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscriptionData_fieldAccessorTable;
    }
    
    private int bitField0_;
    // optional .Hedwig.SubscriptionState state = 1;
    public static final int STATE_FIELD_NUMBER = 1;
    private org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState state_;
    public boolean hasState() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState getState() {
      return state_;
    }
    public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionStateOrBuilder getStateOrBuilder() {
      return state_;
    }
    
    // optional .Hedwig.SubscriptionPreferences preferences = 2;
    public static final int PREFERENCES_FIELD_NUMBER = 2;
    private org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences preferences_;
    public boolean hasPreferences() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences getPreferences() {
      return preferences_;
    }
    public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferencesOrBuilder getPreferencesOrBuilder() {
      return preferences_;
    }
    
    private void initFields() {
      state_ = org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState.getDefaultInstance();
      preferences_ = org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.getDefaultInstance();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      if (hasState()) {
        if (!getState().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeMessage(1, state_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeMessage(2, preferences_);
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, state_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, preferences_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hedwig.protocol.PubSubProtocol.SubscriptionDataOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscriptionData_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_SubscriptionData_fieldAccessorTable;
      }
      
      // Construct using org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getStateFieldBuilder();
          getPreferencesFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        if (stateBuilder_ == null) {
          state_ = org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState.getDefaultInstance();
        } else {
          stateBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        if (preferencesBuilder_ == null) {
          preferences_ = org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.getDefaultInstance();
        } else {
          preferencesBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData.getDescriptor();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData getDefaultInstanceForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData.getDefaultInstance();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData build() {
        org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData buildPartial() {
        org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData result = new org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        if (stateBuilder_ == null) {
          result.state_ = state_;
        } else {
          result.state_ = stateBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        if (preferencesBuilder_ == null) {
          result.preferences_ = preferences_;
        } else {
          result.preferences_ = preferencesBuilder_.build();
        }
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData) {
          return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData other) {
        if (other == org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData.getDefaultInstance()) return this;
        if (other.hasState()) {
          mergeState(other.getState());
        }
        if (other.hasPreferences()) {
          mergePreferences(other.getPreferences());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        if (hasState()) {
          if (!getState().isInitialized()) {
            
            return false;
          }
        }
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 10: {
              org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState.newBuilder();
              if (hasState()) {
                subBuilder.mergeFrom(getState());
              }
              input.readMessage(subBuilder, extensionRegistry);
              setState(subBuilder.buildPartial());
              break;
            }
            case 18: {
              org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.newBuilder();
              if (hasPreferences()) {
                subBuilder.mergeFrom(getPreferences());
              }
              input.readMessage(subBuilder, extensionRegistry);
              setPreferences(subBuilder.buildPartial());
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // optional .Hedwig.SubscriptionState state = 1;
      private org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState state_ = org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState, org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState.Builder, org.apache.hedwig.protocol.PubSubProtocol.SubscriptionStateOrBuilder> stateBuilder_;
      public boolean hasState() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState getState() {
        if (stateBuilder_ == null) {
          return state_;
        } else {
          return stateBuilder_.getMessage();
        }
      }
      public Builder setState(org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState value) {
        if (stateBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          state_ = value;
          onChanged();
        } else {
          stateBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      public Builder setState(
          org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState.Builder builderForValue) {
        if (stateBuilder_ == null) {
          state_ = builderForValue.build();
          onChanged();
        } else {
          stateBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      public Builder mergeState(org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState value) {
        if (stateBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001) &&
              state_ != org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState.getDefaultInstance()) {
            state_ =
              org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState.newBuilder(state_).mergeFrom(value).buildPartial();
          } else {
            state_ = value;
          }
          onChanged();
        } else {
          stateBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        return this;
      }
      public Builder clearState() {
        if (stateBuilder_ == null) {
          state_ = org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState.getDefaultInstance();
          onChanged();
        } else {
          stateBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState.Builder getStateBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getStateFieldBuilder().getBuilder();
      }
      public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionStateOrBuilder getStateOrBuilder() {
        if (stateBuilder_ != null) {
          return stateBuilder_.getMessageOrBuilder();
        } else {
          return state_;
        }
      }
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState, org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState.Builder, org.apache.hedwig.protocol.PubSubProtocol.SubscriptionStateOrBuilder> 
          getStateFieldBuilder() {
        if (stateBuilder_ == null) {
          stateBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState, org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState.Builder, org.apache.hedwig.protocol.PubSubProtocol.SubscriptionStateOrBuilder>(
                  state_,
                  getParentForChildren(),
                  isClean());
          state_ = null;
        }
        return stateBuilder_;
      }
      
      // optional .Hedwig.SubscriptionPreferences preferences = 2;
      private org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences preferences_ = org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences, org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.Builder, org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferencesOrBuilder> preferencesBuilder_;
      public boolean hasPreferences() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences getPreferences() {
        if (preferencesBuilder_ == null) {
          return preferences_;
        } else {
          return preferencesBuilder_.getMessage();
        }
      }
      public Builder setPreferences(org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences value) {
        if (preferencesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          preferences_ = value;
          onChanged();
        } else {
          preferencesBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      public Builder setPreferences(
          org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.Builder builderForValue) {
        if (preferencesBuilder_ == null) {
          preferences_ = builderForValue.build();
          onChanged();
        } else {
          preferencesBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      public Builder mergePreferences(org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences value) {
        if (preferencesBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002) &&
              preferences_ != org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.getDefaultInstance()) {
            preferences_ =
              org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.newBuilder(preferences_).mergeFrom(value).buildPartial();
          } else {
            preferences_ = value;
          }
          onChanged();
        } else {
          preferencesBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      public Builder clearPreferences() {
        if (preferencesBuilder_ == null) {
          preferences_ = org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.getDefaultInstance();
          onChanged();
        } else {
          preferencesBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.Builder getPreferencesBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getPreferencesFieldBuilder().getBuilder();
      }
      public org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferencesOrBuilder getPreferencesOrBuilder() {
        if (preferencesBuilder_ != null) {
          return preferencesBuilder_.getMessageOrBuilder();
        } else {
          return preferences_;
        }
      }
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences, org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.Builder, org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferencesOrBuilder> 
          getPreferencesFieldBuilder() {
        if (preferencesBuilder_ == null) {
          preferencesBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences, org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.Builder, org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferencesOrBuilder>(
                  preferences_,
                  getParentForChildren(),
                  isClean());
          preferences_ = null;
        }
        return preferencesBuilder_;
      }
      
      // @@protoc_insertion_point(builder_scope:Hedwig.SubscriptionData)
    }
    
    static {
      defaultInstance = new SubscriptionData(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:Hedwig.SubscriptionData)
  }
  
  public interface LedgerRangeOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // required uint64 ledgerId = 1;
    boolean hasLedgerId();
    long getLedgerId();
    
    // optional .Hedwig.MessageSeqId endSeqIdIncluded = 2;
    boolean hasEndSeqIdIncluded();
    org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId getEndSeqIdIncluded();
    org.apache.hedwig.protocol.PubSubProtocol.MessageSeqIdOrBuilder getEndSeqIdIncludedOrBuilder();
    
    // optional uint64 startSeqIdIncluded = 3;
    boolean hasStartSeqIdIncluded();
    long getStartSeqIdIncluded();
  }
  public static final class LedgerRange extends
      com.google.protobuf.GeneratedMessage
      implements LedgerRangeOrBuilder {
    // Use LedgerRange.newBuilder() to construct.
    private LedgerRange(Builder builder) {
      super(builder);
    }
    private LedgerRange(boolean noInit) {}
    
    private static final LedgerRange defaultInstance;
    public static LedgerRange getDefaultInstance() {
      return defaultInstance;
    }
    
    public LedgerRange getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_LedgerRange_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_LedgerRange_fieldAccessorTable;
    }
    
    private int bitField0_;
    // required uint64 ledgerId = 1;
    public static final int LEDGERID_FIELD_NUMBER = 1;
    private long ledgerId_;
    public boolean hasLedgerId() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    public long getLedgerId() {
      return ledgerId_;
    }
    
    // optional .Hedwig.MessageSeqId endSeqIdIncluded = 2;
    public static final int ENDSEQIDINCLUDED_FIELD_NUMBER = 2;
    private org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId endSeqIdIncluded_;
    public boolean hasEndSeqIdIncluded() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId getEndSeqIdIncluded() {
      return endSeqIdIncluded_;
    }
    public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqIdOrBuilder getEndSeqIdIncludedOrBuilder() {
      return endSeqIdIncluded_;
    }
    
    // optional uint64 startSeqIdIncluded = 3;
    public static final int STARTSEQIDINCLUDED_FIELD_NUMBER = 3;
    private long startSeqIdIncluded_;
    public boolean hasStartSeqIdIncluded() {
      return ((bitField0_ & 0x00000004) == 0x00000004);
    }
    public long getStartSeqIdIncluded() {
      return startSeqIdIncluded_;
    }
    
    private void initFields() {
      ledgerId_ = 0L;
      endSeqIdIncluded_ = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance();
      startSeqIdIncluded_ = 0L;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      if (!hasLedgerId()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (hasEndSeqIdIncluded()) {
        if (!getEndSeqIdIncluded().isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeUInt64(1, ledgerId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeMessage(2, endSeqIdIncluded_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        output.writeUInt64(3, startSeqIdIncluded_);
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(1, ledgerId_);
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, endSeqIdIncluded_);
      }
      if (((bitField0_ & 0x00000004) == 0x00000004)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(3, startSeqIdIncluded_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRange parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRange parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRange parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRange parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRange parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRange parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRange parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRange parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRange parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRange parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.LedgerRange prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hedwig.protocol.PubSubProtocol.LedgerRangeOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_LedgerRange_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_LedgerRange_fieldAccessorTable;
      }
      
      // Construct using org.apache.hedwig.protocol.PubSubProtocol.LedgerRange.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getEndSeqIdIncludedFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        ledgerId_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000001);
        if (endSeqIdIncludedBuilder_ == null) {
          endSeqIdIncluded_ = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance();
        } else {
          endSeqIdIncludedBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        startSeqIdIncluded_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000004);
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.LedgerRange.getDescriptor();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.LedgerRange getDefaultInstanceForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.LedgerRange.getDefaultInstance();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.LedgerRange build() {
        org.apache.hedwig.protocol.PubSubProtocol.LedgerRange result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.hedwig.protocol.PubSubProtocol.LedgerRange buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.hedwig.protocol.PubSubProtocol.LedgerRange result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.LedgerRange buildPartial() {
        org.apache.hedwig.protocol.PubSubProtocol.LedgerRange result = new org.apache.hedwig.protocol.PubSubProtocol.LedgerRange(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.ledgerId_ = ledgerId_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        if (endSeqIdIncludedBuilder_ == null) {
          result.endSeqIdIncluded_ = endSeqIdIncluded_;
        } else {
          result.endSeqIdIncluded_ = endSeqIdIncludedBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000004) == 0x00000004)) {
          to_bitField0_ |= 0x00000004;
        }
        result.startSeqIdIncluded_ = startSeqIdIncluded_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.LedgerRange) {
          return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.LedgerRange)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.LedgerRange other) {
        if (other == org.apache.hedwig.protocol.PubSubProtocol.LedgerRange.getDefaultInstance()) return this;
        if (other.hasLedgerId()) {
          setLedgerId(other.getLedgerId());
        }
        if (other.hasEndSeqIdIncluded()) {
          mergeEndSeqIdIncluded(other.getEndSeqIdIncluded());
        }
        if (other.hasStartSeqIdIncluded()) {
          setStartSeqIdIncluded(other.getStartSeqIdIncluded());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        if (!hasLedgerId()) {
          
          return false;
        }
        if (hasEndSeqIdIncluded()) {
          if (!getEndSeqIdIncluded().isInitialized()) {
            
            return false;
          }
        }
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 8: {
              bitField0_ |= 0x00000001;
              ledgerId_ = input.readUInt64();
              break;
            }
            case 18: {
              org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.newBuilder();
              if (hasEndSeqIdIncluded()) {
                subBuilder.mergeFrom(getEndSeqIdIncluded());
              }
              input.readMessage(subBuilder, extensionRegistry);
              setEndSeqIdIncluded(subBuilder.buildPartial());
              break;
            }
            case 24: {
              bitField0_ |= 0x00000004;
              startSeqIdIncluded_ = input.readUInt64();
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // required uint64 ledgerId = 1;
      private long ledgerId_ ;
      public boolean hasLedgerId() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public long getLedgerId() {
        return ledgerId_;
      }
      public Builder setLedgerId(long value) {
        bitField0_ |= 0x00000001;
        ledgerId_ = value;
        onChanged();
        return this;
      }
      public Builder clearLedgerId() {
        bitField0_ = (bitField0_ & ~0x00000001);
        ledgerId_ = 0L;
        onChanged();
        return this;
      }
      
      // optional .Hedwig.MessageSeqId endSeqIdIncluded = 2;
      private org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId endSeqIdIncluded_ = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance();
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId, org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder, org.apache.hedwig.protocol.PubSubProtocol.MessageSeqIdOrBuilder> endSeqIdIncludedBuilder_;
      public boolean hasEndSeqIdIncluded() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId getEndSeqIdIncluded() {
        if (endSeqIdIncludedBuilder_ == null) {
          return endSeqIdIncluded_;
        } else {
          return endSeqIdIncludedBuilder_.getMessage();
        }
      }
      public Builder setEndSeqIdIncluded(org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId value) {
        if (endSeqIdIncludedBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          endSeqIdIncluded_ = value;
          onChanged();
        } else {
          endSeqIdIncludedBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      public Builder setEndSeqIdIncluded(
          org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder builderForValue) {
        if (endSeqIdIncludedBuilder_ == null) {
          endSeqIdIncluded_ = builderForValue.build();
          onChanged();
        } else {
          endSeqIdIncludedBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      public Builder mergeEndSeqIdIncluded(org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId value) {
        if (endSeqIdIncludedBuilder_ == null) {
          if (((bitField0_ & 0x00000002) == 0x00000002) &&
              endSeqIdIncluded_ != org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance()) {
            endSeqIdIncluded_ =
              org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.newBuilder(endSeqIdIncluded_).mergeFrom(value).buildPartial();
          } else {
            endSeqIdIncluded_ = value;
          }
          onChanged();
        } else {
          endSeqIdIncludedBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        return this;
      }
      public Builder clearEndSeqIdIncluded() {
        if (endSeqIdIncludedBuilder_ == null) {
          endSeqIdIncluded_ = org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.getDefaultInstance();
          onChanged();
        } else {
          endSeqIdIncludedBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder getEndSeqIdIncludedBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getEndSeqIdIncludedFieldBuilder().getBuilder();
      }
      public org.apache.hedwig.protocol.PubSubProtocol.MessageSeqIdOrBuilder getEndSeqIdIncludedOrBuilder() {
        if (endSeqIdIncludedBuilder_ != null) {
          return endSeqIdIncludedBuilder_.getMessageOrBuilder();
        } else {
          return endSeqIdIncluded_;
        }
      }
      private com.google.protobuf.SingleFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId, org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder, org.apache.hedwig.protocol.PubSubProtocol.MessageSeqIdOrBuilder> 
          getEndSeqIdIncludedFieldBuilder() {
        if (endSeqIdIncludedBuilder_ == null) {
          endSeqIdIncludedBuilder_ = new com.google.protobuf.SingleFieldBuilder<
              org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId, org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder, org.apache.hedwig.protocol.PubSubProtocol.MessageSeqIdOrBuilder>(
                  endSeqIdIncluded_,
                  getParentForChildren(),
                  isClean());
          endSeqIdIncluded_ = null;
        }
        return endSeqIdIncludedBuilder_;
      }
      
      // optional uint64 startSeqIdIncluded = 3;
      private long startSeqIdIncluded_ ;
      public boolean hasStartSeqIdIncluded() {
        return ((bitField0_ & 0x00000004) == 0x00000004);
      }
      public long getStartSeqIdIncluded() {
        return startSeqIdIncluded_;
      }
      public Builder setStartSeqIdIncluded(long value) {
        bitField0_ |= 0x00000004;
        startSeqIdIncluded_ = value;
        onChanged();
        return this;
      }
      public Builder clearStartSeqIdIncluded() {
        bitField0_ = (bitField0_ & ~0x00000004);
        startSeqIdIncluded_ = 0L;
        onChanged();
        return this;
      }
      
      // @@protoc_insertion_point(builder_scope:Hedwig.LedgerRange)
    }
    
    static {
      defaultInstance = new LedgerRange(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:Hedwig.LedgerRange)
  }
  
  public interface LedgerRangesOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // repeated .Hedwig.LedgerRange ranges = 1;
    java.util.List<org.apache.hedwig.protocol.PubSubProtocol.LedgerRange> 
        getRangesList();
    org.apache.hedwig.protocol.PubSubProtocol.LedgerRange getRanges(int index);
    int getRangesCount();
    java.util.List<? extends org.apache.hedwig.protocol.PubSubProtocol.LedgerRangeOrBuilder> 
        getRangesOrBuilderList();
    org.apache.hedwig.protocol.PubSubProtocol.LedgerRangeOrBuilder getRangesOrBuilder(
        int index);
  }
  public static final class LedgerRanges extends
      com.google.protobuf.GeneratedMessage
      implements LedgerRangesOrBuilder {
    // Use LedgerRanges.newBuilder() to construct.
    private LedgerRanges(Builder builder) {
      super(builder);
    }
    private LedgerRanges(boolean noInit) {}
    
    private static final LedgerRanges defaultInstance;
    public static LedgerRanges getDefaultInstance() {
      return defaultInstance;
    }
    
    public LedgerRanges getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_LedgerRanges_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_LedgerRanges_fieldAccessorTable;
    }
    
    // repeated .Hedwig.LedgerRange ranges = 1;
    public static final int RANGES_FIELD_NUMBER = 1;
    private java.util.List<org.apache.hedwig.protocol.PubSubProtocol.LedgerRange> ranges_;
    public java.util.List<org.apache.hedwig.protocol.PubSubProtocol.LedgerRange> getRangesList() {
      return ranges_;
    }
    public java.util.List<? extends org.apache.hedwig.protocol.PubSubProtocol.LedgerRangeOrBuilder> 
        getRangesOrBuilderList() {
      return ranges_;
    }
    public int getRangesCount() {
      return ranges_.size();
    }
    public org.apache.hedwig.protocol.PubSubProtocol.LedgerRange getRanges(int index) {
      return ranges_.get(index);
    }
    public org.apache.hedwig.protocol.PubSubProtocol.LedgerRangeOrBuilder getRangesOrBuilder(
        int index) {
      return ranges_.get(index);
    }
    
    private void initFields() {
      ranges_ = java.util.Collections.emptyList();
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      for (int i = 0; i < getRangesCount(); i++) {
        if (!getRanges(i).isInitialized()) {
          memoizedIsInitialized = 0;
          return false;
        }
      }
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      for (int i = 0; i < ranges_.size(); i++) {
        output.writeMessage(1, ranges_.get(i));
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      for (int i = 0; i < ranges_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, ranges_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hedwig.protocol.PubSubProtocol.LedgerRangesOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_LedgerRanges_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_LedgerRanges_fieldAccessorTable;
      }
      
      // Construct using org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
          getRangesFieldBuilder();
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        if (rangesBuilder_ == null) {
          ranges_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
        } else {
          rangesBuilder_.clear();
        }
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges.getDescriptor();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges getDefaultInstanceForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges.getDefaultInstance();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges build() {
        org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges buildPartial() {
        org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges result = new org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges(this);
        int from_bitField0_ = bitField0_;
        if (rangesBuilder_ == null) {
          if (((bitField0_ & 0x00000001) == 0x00000001)) {
            ranges_ = java.util.Collections.unmodifiableList(ranges_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.ranges_ = ranges_;
        } else {
          result.ranges_ = rangesBuilder_.build();
        }
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges) {
          return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges other) {
        if (other == org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges.getDefaultInstance()) return this;
        if (rangesBuilder_ == null) {
          if (!other.ranges_.isEmpty()) {
            if (ranges_.isEmpty()) {
              ranges_ = other.ranges_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureRangesIsMutable();
              ranges_.addAll(other.ranges_);
            }
            onChanged();
          }
        } else {
          if (!other.ranges_.isEmpty()) {
            if (rangesBuilder_.isEmpty()) {
              rangesBuilder_.dispose();
              rangesBuilder_ = null;
              ranges_ = other.ranges_;
              bitField0_ = (bitField0_ & ~0x00000001);
              rangesBuilder_ = 
                com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders ?
                   getRangesFieldBuilder() : null;
            } else {
              rangesBuilder_.addAllMessages(other.ranges_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        for (int i = 0; i < getRangesCount(); i++) {
          if (!getRanges(i).isInitialized()) {
            
            return false;
          }
        }
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 10: {
              org.apache.hedwig.protocol.PubSubProtocol.LedgerRange.Builder subBuilder = org.apache.hedwig.protocol.PubSubProtocol.LedgerRange.newBuilder();
              input.readMessage(subBuilder, extensionRegistry);
              addRanges(subBuilder.buildPartial());
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // repeated .Hedwig.LedgerRange ranges = 1;
      private java.util.List<org.apache.hedwig.protocol.PubSubProtocol.LedgerRange> ranges_ =
        java.util.Collections.emptyList();
      private void ensureRangesIsMutable() {
        if (!((bitField0_ & 0x00000001) == 0x00000001)) {
          ranges_ = new java.util.ArrayList<org.apache.hedwig.protocol.PubSubProtocol.LedgerRange>(ranges_);
          bitField0_ |= 0x00000001;
         }
      }
      
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.LedgerRange, org.apache.hedwig.protocol.PubSubProtocol.LedgerRange.Builder, org.apache.hedwig.protocol.PubSubProtocol.LedgerRangeOrBuilder> rangesBuilder_;
      
      public java.util.List<org.apache.hedwig.protocol.PubSubProtocol.LedgerRange> getRangesList() {
        if (rangesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(ranges_);
        } else {
          return rangesBuilder_.getMessageList();
        }
      }
      public int getRangesCount() {
        if (rangesBuilder_ == null) {
          return ranges_.size();
        } else {
          return rangesBuilder_.getCount();
        }
      }
      public org.apache.hedwig.protocol.PubSubProtocol.LedgerRange getRanges(int index) {
        if (rangesBuilder_ == null) {
          return ranges_.get(index);
        } else {
          return rangesBuilder_.getMessage(index);
        }
      }
      public Builder setRanges(
          int index, org.apache.hedwig.protocol.PubSubProtocol.LedgerRange value) {
        if (rangesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureRangesIsMutable();
          ranges_.set(index, value);
          onChanged();
        } else {
          rangesBuilder_.setMessage(index, value);
        }
        return this;
      }
      public Builder setRanges(
          int index, org.apache.hedwig.protocol.PubSubProtocol.LedgerRange.Builder builderForValue) {
        if (rangesBuilder_ == null) {
          ensureRangesIsMutable();
          ranges_.set(index, builderForValue.build());
          onChanged();
        } else {
          rangesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      public Builder addRanges(org.apache.hedwig.protocol.PubSubProtocol.LedgerRange value) {
        if (rangesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureRangesIsMutable();
          ranges_.add(value);
          onChanged();
        } else {
          rangesBuilder_.addMessage(value);
        }
        return this;
      }
      public Builder addRanges(
          int index, org.apache.hedwig.protocol.PubSubProtocol.LedgerRange value) {
        if (rangesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureRangesIsMutable();
          ranges_.add(index, value);
          onChanged();
        } else {
          rangesBuilder_.addMessage(index, value);
        }
        return this;
      }
      public Builder addRanges(
          org.apache.hedwig.protocol.PubSubProtocol.LedgerRange.Builder builderForValue) {
        if (rangesBuilder_ == null) {
          ensureRangesIsMutable();
          ranges_.add(builderForValue.build());
          onChanged();
        } else {
          rangesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      public Builder addRanges(
          int index, org.apache.hedwig.protocol.PubSubProtocol.LedgerRange.Builder builderForValue) {
        if (rangesBuilder_ == null) {
          ensureRangesIsMutable();
          ranges_.add(index, builderForValue.build());
          onChanged();
        } else {
          rangesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      public Builder addAllRanges(
          java.lang.Iterable<? extends org.apache.hedwig.protocol.PubSubProtocol.LedgerRange> values) {
        if (rangesBuilder_ == null) {
          ensureRangesIsMutable();
          super.addAll(values, ranges_);
          onChanged();
        } else {
          rangesBuilder_.addAllMessages(values);
        }
        return this;
      }
      public Builder clearRanges() {
        if (rangesBuilder_ == null) {
          ranges_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          rangesBuilder_.clear();
        }
        return this;
      }
      public Builder removeRanges(int index) {
        if (rangesBuilder_ == null) {
          ensureRangesIsMutable();
          ranges_.remove(index);
          onChanged();
        } else {
          rangesBuilder_.remove(index);
        }
        return this;
      }
      public org.apache.hedwig.protocol.PubSubProtocol.LedgerRange.Builder getRangesBuilder(
          int index) {
        return getRangesFieldBuilder().getBuilder(index);
      }
      public org.apache.hedwig.protocol.PubSubProtocol.LedgerRangeOrBuilder getRangesOrBuilder(
          int index) {
        if (rangesBuilder_ == null) {
          return ranges_.get(index);  } else {
          return rangesBuilder_.getMessageOrBuilder(index);
        }
      }
      public java.util.List<? extends org.apache.hedwig.protocol.PubSubProtocol.LedgerRangeOrBuilder> 
           getRangesOrBuilderList() {
        if (rangesBuilder_ != null) {
          return rangesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(ranges_);
        }
      }
      public org.apache.hedwig.protocol.PubSubProtocol.LedgerRange.Builder addRangesBuilder() {
        return getRangesFieldBuilder().addBuilder(
            org.apache.hedwig.protocol.PubSubProtocol.LedgerRange.getDefaultInstance());
      }
      public org.apache.hedwig.protocol.PubSubProtocol.LedgerRange.Builder addRangesBuilder(
          int index) {
        return getRangesFieldBuilder().addBuilder(
            index, org.apache.hedwig.protocol.PubSubProtocol.LedgerRange.getDefaultInstance());
      }
      public java.util.List<org.apache.hedwig.protocol.PubSubProtocol.LedgerRange.Builder> 
           getRangesBuilderList() {
        return getRangesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilder<
          org.apache.hedwig.protocol.PubSubProtocol.LedgerRange, org.apache.hedwig.protocol.PubSubProtocol.LedgerRange.Builder, org.apache.hedwig.protocol.PubSubProtocol.LedgerRangeOrBuilder> 
          getRangesFieldBuilder() {
        if (rangesBuilder_ == null) {
          rangesBuilder_ = new com.google.protobuf.RepeatedFieldBuilder<
              org.apache.hedwig.protocol.PubSubProtocol.LedgerRange, org.apache.hedwig.protocol.PubSubProtocol.LedgerRange.Builder, org.apache.hedwig.protocol.PubSubProtocol.LedgerRangeOrBuilder>(
                  ranges_,
                  ((bitField0_ & 0x00000001) == 0x00000001),
                  getParentForChildren(),
                  isClean());
          ranges_ = null;
        }
        return rangesBuilder_;
      }
      
      // @@protoc_insertion_point(builder_scope:Hedwig.LedgerRanges)
    }
    
    static {
      defaultInstance = new LedgerRanges(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:Hedwig.LedgerRanges)
  }
  
  public interface ManagerMetaOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // required string managerImpl = 2;
    boolean hasManagerImpl();
    String getManagerImpl();
    
    // required uint32 managerVersion = 3;
    boolean hasManagerVersion();
    int getManagerVersion();
  }
  public static final class ManagerMeta extends
      com.google.protobuf.GeneratedMessage
      implements ManagerMetaOrBuilder {
    // Use ManagerMeta.newBuilder() to construct.
    private ManagerMeta(Builder builder) {
      super(builder);
    }
    private ManagerMeta(boolean noInit) {}
    
    private static final ManagerMeta defaultInstance;
    public static ManagerMeta getDefaultInstance() {
      return defaultInstance;
    }
    
    public ManagerMeta getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_ManagerMeta_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_ManagerMeta_fieldAccessorTable;
    }
    
    private int bitField0_;
    // required string managerImpl = 2;
    public static final int MANAGERIMPL_FIELD_NUMBER = 2;
    private java.lang.Object managerImpl_;
    public boolean hasManagerImpl() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    public String getManagerImpl() {
      java.lang.Object ref = managerImpl_;
      if (ref instanceof String) {
        return (String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        String s = bs.toStringUtf8();
        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
          managerImpl_ = s;
        }
        return s;
      }
    }
    private com.google.protobuf.ByteString getManagerImplBytes() {
      java.lang.Object ref = managerImpl_;
      if (ref instanceof String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
        managerImpl_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }
    
    // required uint32 managerVersion = 3;
    public static final int MANAGERVERSION_FIELD_NUMBER = 3;
    private int managerVersion_;
    public boolean hasManagerVersion() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    public int getManagerVersion() {
      return managerVersion_;
    }
    
    private void initFields() {
      managerImpl_ = "";
      managerVersion_ = 0;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      if (!hasManagerImpl()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasManagerVersion()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(2, getManagerImplBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeUInt32(3, managerVersion_);
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, getManagerImplBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt32Size(3, managerVersion_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.hedwig.protocol.PubSubProtocol.ManagerMeta parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.ManagerMeta parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.ManagerMeta parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.ManagerMeta parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.ManagerMeta parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.ManagerMeta parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.ManagerMeta parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.ManagerMeta parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.ManagerMeta parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.ManagerMeta parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.ManagerMeta prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hedwig.protocol.PubSubProtocol.ManagerMetaOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_ManagerMeta_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_ManagerMeta_fieldAccessorTable;
      }
      
      // Construct using org.apache.hedwig.protocol.PubSubProtocol.ManagerMeta.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        managerImpl_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        managerVersion_ = 0;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.ManagerMeta.getDescriptor();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.ManagerMeta getDefaultInstanceForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.ManagerMeta.getDefaultInstance();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.ManagerMeta build() {
        org.apache.hedwig.protocol.PubSubProtocol.ManagerMeta result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.hedwig.protocol.PubSubProtocol.ManagerMeta buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.hedwig.protocol.PubSubProtocol.ManagerMeta result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.ManagerMeta buildPartial() {
        org.apache.hedwig.protocol.PubSubProtocol.ManagerMeta result = new org.apache.hedwig.protocol.PubSubProtocol.ManagerMeta(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.managerImpl_ = managerImpl_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.managerVersion_ = managerVersion_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.ManagerMeta) {
          return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.ManagerMeta)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.ManagerMeta other) {
        if (other == org.apache.hedwig.protocol.PubSubProtocol.ManagerMeta.getDefaultInstance()) return this;
        if (other.hasManagerImpl()) {
          setManagerImpl(other.getManagerImpl());
        }
        if (other.hasManagerVersion()) {
          setManagerVersion(other.getManagerVersion());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        if (!hasManagerImpl()) {
          
          return false;
        }
        if (!hasManagerVersion()) {
          
          return false;
        }
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 18: {
              bitField0_ |= 0x00000001;
              managerImpl_ = input.readBytes();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000002;
              managerVersion_ = input.readUInt32();
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // required string managerImpl = 2;
      private java.lang.Object managerImpl_ = "";
      public boolean hasManagerImpl() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public String getManagerImpl() {
        java.lang.Object ref = managerImpl_;
        if (!(ref instanceof String)) {
          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
          managerImpl_ = s;
          return s;
        } else {
          return (String) ref;
        }
      }
      public Builder setManagerImpl(String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        managerImpl_ = value;
        onChanged();
        return this;
      }
      public Builder clearManagerImpl() {
        bitField0_ = (bitField0_ & ~0x00000001);
        managerImpl_ = getDefaultInstance().getManagerImpl();
        onChanged();
        return this;
      }
      void setManagerImpl(com.google.protobuf.ByteString value) {
        bitField0_ |= 0x00000001;
        managerImpl_ = value;
        onChanged();
      }
      
      // required uint32 managerVersion = 3;
      private int managerVersion_ ;
      public boolean hasManagerVersion() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      public int getManagerVersion() {
        return managerVersion_;
      }
      public Builder setManagerVersion(int value) {
        bitField0_ |= 0x00000002;
        managerVersion_ = value;
        onChanged();
        return this;
      }
      public Builder clearManagerVersion() {
        bitField0_ = (bitField0_ & ~0x00000002);
        managerVersion_ = 0;
        onChanged();
        return this;
      }
      
      // @@protoc_insertion_point(builder_scope:Hedwig.ManagerMeta)
    }
    
    static {
      defaultInstance = new ManagerMeta(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:Hedwig.ManagerMeta)
  }
  
  public interface HubInfoDataOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // required string hostname = 2;
    boolean hasHostname();
    String getHostname();
    
    // required uint64 czxid = 3;
    boolean hasCzxid();
    long getCzxid();
  }
  public static final class HubInfoData extends
      com.google.protobuf.GeneratedMessage
      implements HubInfoDataOrBuilder {
    // Use HubInfoData.newBuilder() to construct.
    private HubInfoData(Builder builder) {
      super(builder);
    }
    private HubInfoData(boolean noInit) {}
    
    private static final HubInfoData defaultInstance;
    public static HubInfoData getDefaultInstance() {
      return defaultInstance;
    }
    
    public HubInfoData getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_HubInfoData_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_HubInfoData_fieldAccessorTable;
    }
    
    private int bitField0_;
    // required string hostname = 2;
    public static final int HOSTNAME_FIELD_NUMBER = 2;
    private java.lang.Object hostname_;
    public boolean hasHostname() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    public String getHostname() {
      java.lang.Object ref = hostname_;
      if (ref instanceof String) {
        return (String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        String s = bs.toStringUtf8();
        if (com.google.protobuf.Internal.isValidUtf8(bs)) {
          hostname_ = s;
        }
        return s;
      }
    }
    private com.google.protobuf.ByteString getHostnameBytes() {
      java.lang.Object ref = hostname_;
      if (ref instanceof String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8((String) ref);
        hostname_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }
    
    // required uint64 czxid = 3;
    public static final int CZXID_FIELD_NUMBER = 3;
    private long czxid_;
    public boolean hasCzxid() {
      return ((bitField0_ & 0x00000002) == 0x00000002);
    }
    public long getCzxid() {
      return czxid_;
    }
    
    private void initFields() {
      hostname_ = "";
      czxid_ = 0L;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      if (!hasHostname()) {
        memoizedIsInitialized = 0;
        return false;
      }
      if (!hasCzxid()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeBytes(2, getHostnameBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        output.writeUInt64(3, czxid_);
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, getHostnameBytes());
      }
      if (((bitField0_ & 0x00000002) == 0x00000002)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(3, czxid_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.hedwig.protocol.PubSubProtocol.HubInfoData parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.HubInfoData parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.HubInfoData parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.HubInfoData parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.HubInfoData parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.HubInfoData parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.HubInfoData parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.HubInfoData parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.HubInfoData parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.HubInfoData parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.HubInfoData prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hedwig.protocol.PubSubProtocol.HubInfoDataOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_HubInfoData_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_HubInfoData_fieldAccessorTable;
      }
      
      // Construct using org.apache.hedwig.protocol.PubSubProtocol.HubInfoData.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        hostname_ = "";
        bitField0_ = (bitField0_ & ~0x00000001);
        czxid_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.HubInfoData.getDescriptor();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.HubInfoData getDefaultInstanceForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.HubInfoData.getDefaultInstance();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.HubInfoData build() {
        org.apache.hedwig.protocol.PubSubProtocol.HubInfoData result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.hedwig.protocol.PubSubProtocol.HubInfoData buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.hedwig.protocol.PubSubProtocol.HubInfoData result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.HubInfoData buildPartial() {
        org.apache.hedwig.protocol.PubSubProtocol.HubInfoData result = new org.apache.hedwig.protocol.PubSubProtocol.HubInfoData(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.hostname_ = hostname_;
        if (((from_bitField0_ & 0x00000002) == 0x00000002)) {
          to_bitField0_ |= 0x00000002;
        }
        result.czxid_ = czxid_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.HubInfoData) {
          return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.HubInfoData)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.HubInfoData other) {
        if (other == org.apache.hedwig.protocol.PubSubProtocol.HubInfoData.getDefaultInstance()) return this;
        if (other.hasHostname()) {
          setHostname(other.getHostname());
        }
        if (other.hasCzxid()) {
          setCzxid(other.getCzxid());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        if (!hasHostname()) {
          
          return false;
        }
        if (!hasCzxid()) {
          
          return false;
        }
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 18: {
              bitField0_ |= 0x00000001;
              hostname_ = input.readBytes();
              break;
            }
            case 24: {
              bitField0_ |= 0x00000002;
              czxid_ = input.readUInt64();
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // required string hostname = 2;
      private java.lang.Object hostname_ = "";
      public boolean hasHostname() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public String getHostname() {
        java.lang.Object ref = hostname_;
        if (!(ref instanceof String)) {
          String s = ((com.google.protobuf.ByteString) ref).toStringUtf8();
          hostname_ = s;
          return s;
        } else {
          return (String) ref;
        }
      }
      public Builder setHostname(String value) {
        if (value == null) {
    throw new NullPointerException();
  }
  bitField0_ |= 0x00000001;
        hostname_ = value;
        onChanged();
        return this;
      }
      public Builder clearHostname() {
        bitField0_ = (bitField0_ & ~0x00000001);
        hostname_ = getDefaultInstance().getHostname();
        onChanged();
        return this;
      }
      void setHostname(com.google.protobuf.ByteString value) {
        bitField0_ |= 0x00000001;
        hostname_ = value;
        onChanged();
      }
      
      // required uint64 czxid = 3;
      private long czxid_ ;
      public boolean hasCzxid() {
        return ((bitField0_ & 0x00000002) == 0x00000002);
      }
      public long getCzxid() {
        return czxid_;
      }
      public Builder setCzxid(long value) {
        bitField0_ |= 0x00000002;
        czxid_ = value;
        onChanged();
        return this;
      }
      public Builder clearCzxid() {
        bitField0_ = (bitField0_ & ~0x00000002);
        czxid_ = 0L;
        onChanged();
        return this;
      }
      
      // @@protoc_insertion_point(builder_scope:Hedwig.HubInfoData)
    }
    
    static {
      defaultInstance = new HubInfoData(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:Hedwig.HubInfoData)
  }
  
  public interface HubLoadDataOrBuilder
      extends com.google.protobuf.MessageOrBuilder {
    
    // required uint64 numTopics = 2;
    boolean hasNumTopics();
    long getNumTopics();
  }
  public static final class HubLoadData extends
      com.google.protobuf.GeneratedMessage
      implements HubLoadDataOrBuilder {
    // Use HubLoadData.newBuilder() to construct.
    private HubLoadData(Builder builder) {
      super(builder);
    }
    private HubLoadData(boolean noInit) {}
    
    private static final HubLoadData defaultInstance;
    public static HubLoadData getDefaultInstance() {
      return defaultInstance;
    }
    
    public HubLoadData getDefaultInstanceForType() {
      return defaultInstance;
    }
    
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_HubLoadData_descriptor;
    }
    
    protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_HubLoadData_fieldAccessorTable;
    }
    
    private int bitField0_;
    // required uint64 numTopics = 2;
    public static final int NUMTOPICS_FIELD_NUMBER = 2;
    private long numTopics_;
    public boolean hasNumTopics() {
      return ((bitField0_ & 0x00000001) == 0x00000001);
    }
    public long getNumTopics() {
      return numTopics_;
    }
    
    private void initFields() {
      numTopics_ = 0L;
    }
    private byte memoizedIsInitialized = -1;
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized != -1) return isInitialized == 1;
      
      if (!hasNumTopics()) {
        memoizedIsInitialized = 0;
        return false;
      }
      memoizedIsInitialized = 1;
      return true;
    }
    
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getSerializedSize();
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        output.writeUInt64(2, numTopics_);
      }
      getUnknownFields().writeTo(output);
    }
    
    private int memoizedSerializedSize = -1;
    public int getSerializedSize() {
      int size = memoizedSerializedSize;
      if (size != -1) return size;
    
      size = 0;
      if (((bitField0_ & 0x00000001) == 0x00000001)) {
        size += com.google.protobuf.CodedOutputStream
          .computeUInt64Size(2, numTopics_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSerializedSize = size;
      return size;
    }
    
    private static final long serialVersionUID = 0L;
    @java.lang.Override
    protected java.lang.Object writeReplace()
        throws java.io.ObjectStreamException {
      return super.writeReplace();
    }
    
    public static org.apache.hedwig.protocol.PubSubProtocol.HubLoadData parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.HubLoadData parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.HubLoadData parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.HubLoadData parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return newBuilder().mergeFrom(data, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.HubLoadData parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.HubLoadData parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.HubLoadData parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.HubLoadData parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      Builder builder = newBuilder();
      if (builder.mergeDelimitedFrom(input, extensionRegistry)) {
        return builder.buildParsed();
      } else {
        return null;
      }
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.HubLoadData parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input).buildParsed();
    }
    public static org.apache.hedwig.protocol.PubSubProtocol.HubLoadData parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return newBuilder().mergeFrom(input, extensionRegistry)
               .buildParsed();
    }
    
    public static Builder newBuilder() { return Builder.create(); }
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder(org.apache.hedwig.protocol.PubSubProtocol.HubLoadData prototype) {
      return newBuilder().mergeFrom(prototype);
    }
    public Builder toBuilder() { return newBuilder(this); }
    
    @java.lang.Override
    protected Builder newBuilderForType(
        com.google.protobuf.GeneratedMessage.BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    public static final class Builder extends
        com.google.protobuf.GeneratedMessage.Builder<Builder>
       implements org.apache.hedwig.protocol.PubSubProtocol.HubLoadDataOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_HubLoadData_descriptor;
      }
      
      protected com.google.protobuf.GeneratedMessage.FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.hedwig.protocol.PubSubProtocol.internal_static_Hedwig_HubLoadData_fieldAccessorTable;
      }
      
      // Construct using org.apache.hedwig.protocol.PubSubProtocol.HubLoadData.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }
      
      private Builder(BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessage.alwaysUseFieldBuilders) {
        }
      }
      private static Builder create() {
        return new Builder();
      }
      
      public Builder clear() {
        super.clear();
        numTopics_ = 0L;
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }
      
      public Builder clone() {
        return create().mergeFrom(buildPartial());
      }
      
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.HubLoadData.getDescriptor();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.HubLoadData getDefaultInstanceForType() {
        return org.apache.hedwig.protocol.PubSubProtocol.HubLoadData.getDefaultInstance();
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.HubLoadData build() {
        org.apache.hedwig.protocol.PubSubProtocol.HubLoadData result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }
      
      private org.apache.hedwig.protocol.PubSubProtocol.HubLoadData buildParsed()
          throws com.google.protobuf.InvalidProtocolBufferException {
        org.apache.hedwig.protocol.PubSubProtocol.HubLoadData result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(
            result).asInvalidProtocolBufferException();
        }
        return result;
      }
      
      public org.apache.hedwig.protocol.PubSubProtocol.HubLoadData buildPartial() {
        org.apache.hedwig.protocol.PubSubProtocol.HubLoadData result = new org.apache.hedwig.protocol.PubSubProtocol.HubLoadData(this);
        int from_bitField0_ = bitField0_;
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000001) == 0x00000001)) {
          to_bitField0_ |= 0x00000001;
        }
        result.numTopics_ = numTopics_;
        result.bitField0_ = to_bitField0_;
        onBuilt();
        return result;
      }
      
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.hedwig.protocol.PubSubProtocol.HubLoadData) {
          return mergeFrom((org.apache.hedwig.protocol.PubSubProtocol.HubLoadData)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }
      
      public Builder mergeFrom(org.apache.hedwig.protocol.PubSubProtocol.HubLoadData other) {
        if (other == org.apache.hedwig.protocol.PubSubProtocol.HubLoadData.getDefaultInstance()) return this;
        if (other.hasNumTopics()) {
          setNumTopics(other.getNumTopics());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        return this;
      }
      
      public final boolean isInitialized() {
        if (!hasNumTopics()) {
          
          return false;
        }
        return true;
      }
      
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        com.google.protobuf.UnknownFieldSet.Builder unknownFields =
          com.google.protobuf.UnknownFieldSet.newBuilder(
            this.getUnknownFields());
        while (true) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              this.setUnknownFields(unknownFields.build());
              onChanged();
              return this;
            default: {
              if (!parseUnknownField(input, unknownFields,
                                     extensionRegistry, tag)) {
                this.setUnknownFields(unknownFields.build());
                onChanged();
                return this;
              }
              break;
            }
            case 16: {
              bitField0_ |= 0x00000001;
              numTopics_ = input.readUInt64();
              break;
            }
          }
        }
      }
      
      private int bitField0_;
      
      // required uint64 numTopics = 2;
      private long numTopics_ ;
      public boolean hasNumTopics() {
        return ((bitField0_ & 0x00000001) == 0x00000001);
      }
      public long getNumTopics() {
        return numTopics_;
      }
      public Builder setNumTopics(long value) {
        bitField0_ |= 0x00000001;
        numTopics_ = value;
        onChanged();
        return this;
      }
      public Builder clearNumTopics() {
        bitField0_ = (bitField0_ & ~0x00000001);
        numTopics_ = 0L;
        onChanged();
        return this;
      }
      
      // @@protoc_insertion_point(builder_scope:Hedwig.HubLoadData)
    }
    
    static {
      defaultInstance = new HubLoadData(true);
      defaultInstance.initFields();
    }
    
    // @@protoc_insertion_point(class_scope:Hedwig.HubLoadData)
  }
  
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_Map_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_Hedwig_Map_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_Map_Entry_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_Hedwig_Map_Entry_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_MessageHeader_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_Hedwig_MessageHeader_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_Message_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_Hedwig_Message_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_RegionSpecificSeqId_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_Hedwig_RegionSpecificSeqId_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_MessageSeqId_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_Hedwig_MessageSeqId_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_PubSubRequest_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_Hedwig_PubSubRequest_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_PublishRequest_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_Hedwig_PublishRequest_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_SubscriptionPreferences_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_Hedwig_SubscriptionPreferences_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_SubscribeRequest_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_Hedwig_SubscribeRequest_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_SubscriptionOptions_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_Hedwig_SubscriptionOptions_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_ConsumeRequest_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_Hedwig_ConsumeRequest_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_UnsubscribeRequest_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_Hedwig_UnsubscribeRequest_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_CloseSubscriptionRequest_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_Hedwig_CloseSubscriptionRequest_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_StopDeliveryRequest_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_Hedwig_StopDeliveryRequest_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_StartDeliveryRequest_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_Hedwig_StartDeliveryRequest_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_SubscriptionEventResponse_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_Hedwig_SubscriptionEventResponse_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_PubSubResponse_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_Hedwig_PubSubResponse_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_PublishResponse_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_Hedwig_PublishResponse_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_SubscribeResponse_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_Hedwig_SubscribeResponse_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_ResponseBody_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_Hedwig_ResponseBody_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_SubscriptionState_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_Hedwig_SubscriptionState_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_SubscriptionData_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_Hedwig_SubscriptionData_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_LedgerRange_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_Hedwig_LedgerRange_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_LedgerRanges_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_Hedwig_LedgerRanges_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_ManagerMeta_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_Hedwig_ManagerMeta_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_HubInfoData_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_Hedwig_HubInfoData_fieldAccessorTable;
  private static com.google.protobuf.Descriptors.Descriptor
    internal_static_Hedwig_HubLoadData_descriptor;
  private static
    com.google.protobuf.GeneratedMessage.FieldAccessorTable
      internal_static_Hedwig_HubLoadData_fieldAccessorTable;
  
  public static com.google.protobuf.Descriptors.FileDescriptor
      getDescriptor() {
    return descriptor;
  }
  private static com.google.protobuf.Descriptors.FileDescriptor
      descriptor;
  static {
    java.lang.String[] descriptorData = {
      "\n&src/main/protobuf/PubSubProtocol.proto" +
      "\022\006Hedwig\"N\n\003Map\022\"\n\007entries\030\001 \003(\0132\021.Hedwi" +
      "g.Map.Entry\032#\n\005Entry\022\013\n\003key\030\001 \001(\t\022\r\n\005val" +
      "ue\030\002 \001(\014\"E\n\rMessageHeader\022\037\n\nproperties\030" +
      "\001 \001(\0132\013.Hedwig.Map\022\023\n\013messageType\030\002 \001(\t\"" +
      "v\n\007Message\022\014\n\004body\030\001 \002(\014\022\021\n\tsrcRegion\030\002 " +
      "\001(\014\022#\n\005msgId\030\003 \001(\0132\024.Hedwig.MessageSeqId" +
      "\022%\n\006header\030\004 \001(\0132\025.Hedwig.MessageHeader\"" +
      "4\n\023RegionSpecificSeqId\022\016\n\006region\030\001 \002(\014\022\r" +
      "\n\005seqId\030\002 \002(\004\"]\n\014MessageSeqId\022\026\n\016localCo",
      "mponent\030\001 \001(\004\0225\n\020remoteComponents\030\002 \003(\0132" +
      "\033.Hedwig.RegionSpecificSeqId\"\265\004\n\rPubSubR" +
      "equest\0220\n\017protocolVersion\030\001 \002(\0162\027.Hedwig" +
      ".ProtocolVersion\022#\n\004type\030\002 \002(\0162\025.Hedwig." +
      "OperationType\022\024\n\014triedServers\030\003 \003(\014\022\r\n\005t" +
      "xnId\030\004 \002(\004\022\023\n\013shouldClaim\030\005 \001(\010\022\r\n\005topic" +
      "\030\006 \002(\014\022.\n\016publishRequest\0304 \001(\0132\026.Hedwig." +
      "PublishRequest\0222\n\020subscribeRequest\0305 \001(\013" +
      "2\030.Hedwig.SubscribeRequest\022.\n\016consumeReq" +
      "uest\0306 \001(\0132\026.Hedwig.ConsumeRequest\0226\n\022un",
      "subscribeRequest\0307 \001(\0132\032.Hedwig.Unsubscr" +
      "ibeRequest\0228\n\023stopDeliveryRequest\0308 \001(\0132" +
      "\033.Hedwig.StopDeliveryRequest\022:\n\024startDel" +
      "iveryRequest\0309 \001(\0132\034.Hedwig.StartDeliver" +
      "yRequest\022B\n\030closeSubscriptionRequest\030: \001" +
      "(\0132 .Hedwig.CloseSubscriptionRequest\".\n\016" +
      "PublishRequest\022\034\n\003msg\030\002 \002(\0132\017.Hedwig.Mes" +
      "sage\"\177\n\027SubscriptionPreferences\022\034\n\007optio" +
      "ns\030\001 \001(\0132\013.Hedwig.Map\022\024\n\014messageBound\030\002 " +
      "\001(\r\022\025\n\rmessageFilter\030\003 \001(\t\022\031\n\021messageWin",
      "dowSize\030\004 \001(\r\"\277\002\n\020SubscribeRequest\022\024\n\014su" +
      "bscriberId\030\002 \002(\014\022Q\n\016createOrAttach\030\003 \001(\016" +
      "2\'.Hedwig.SubscribeRequest.CreateOrAttac" +
      "h:\020CREATE_OR_ATTACH\022\032\n\013synchronous\030\004 \001(\010" +
      ":\005false\022\024\n\014messageBound\030\005 \001(\r\0224\n\013prefere" +
      "nces\030\006 \001(\0132\037.Hedwig.SubscriptionPreferen" +
      "ces\022\032\n\013forceAttach\030\007 \001(\010:\005false\">\n\016Creat" +
      "eOrAttach\022\n\n\006CREATE\020\000\022\n\n\006ATTACH\020\001\022\024\n\020CRE" +
      "ATE_OR_ATTACH\020\002\"\216\002\n\023SubscriptionOptions\022" +
      "\032\n\013forceAttach\030\001 \001(\010:\005false\022Q\n\016createOrA",
      "ttach\030\002 \001(\0162\'.Hedwig.SubscribeRequest.Cr" +
      "eateOrAttach:\020CREATE_OR_ATTACH\022\027\n\014messag" +
      "eBound\030\003 \001(\r:\0010\022\034\n\007options\030\004 \001(\0132\013.Hedwi" +
      "g.Map\022\025\n\rmessageFilter\030\005 \001(\t\022\031\n\021messageW" +
      "indowSize\030\006 \001(\r\022\037\n\021enableResubscribe\030\007 \001" +
      "(\010:\004true\"K\n\016ConsumeRequest\022\024\n\014subscriber" +
      "Id\030\002 \002(\014\022#\n\005msgId\030\003 \002(\0132\024.Hedwig.Message" +
      "SeqId\"*\n\022UnsubscribeRequest\022\024\n\014subscribe" +
      "rId\030\002 \002(\014\"0\n\030CloseSubscriptionRequest\022\024\n" +
      "\014subscriberId\030\002 \002(\014\"+\n\023StopDeliveryReque",
      "st\022\024\n\014subscriberId\030\002 \002(\014\",\n\024StartDeliver" +
      "yRequest\022\024\n\014subscriberId\030\002 \002(\014\"E\n\031Subscr" +
      "iptionEventResponse\022(\n\005event\030\001 \001(\0162\031.Hed" +
      "wig.SubscriptionEvent\"\377\001\n\016PubSubResponse" +
      "\0220\n\017protocolVersion\030\001 \002(\0162\027.Hedwig.Proto" +
      "colVersion\022&\n\nstatusCode\030\002 \002(\0162\022.Hedwig." +
      "StatusCode\022\r\n\005txnId\030\003 \002(\004\022\021\n\tstatusMsg\030\004" +
      " \001(\t\022 \n\007message\030\005 \001(\0132\017.Hedwig.Message\022\r" +
      "\n\005topic\030\006 \001(\014\022\024\n\014subscriberId\030\007 \001(\014\022*\n\014r" +
      "esponseBody\030\010 \001(\0132\024.Hedwig.ResponseBody\"",
      "?\n\017PublishResponse\022,\n\016publishedMsgId\030\001 \002" +
      "(\0132\024.Hedwig.MessageSeqId\"I\n\021SubscribeRes" +
      "ponse\0224\n\013preferences\030\002 \001(\0132\037.Hedwig.Subs" +
      "criptionPreferences\"\264\001\n\014ResponseBody\0220\n\017" +
      "publishResponse\030\001 \001(\0132\027.Hedwig.PublishRe" +
      "sponse\0224\n\021subscribeResponse\030\002 \001(\0132\031.Hedw" +
      "ig.SubscribeResponse\022<\n\021subscriptionEven" +
      "t\030\003 \001(\0132!.Hedwig.SubscriptionEventRespon" +
      "se\"N\n\021SubscriptionState\022#\n\005msgId\030\001 \002(\0132\024" +
      ".Hedwig.MessageSeqId\022\024\n\014messageBound\030\002 \001",
      "(\r\"r\n\020SubscriptionData\022(\n\005state\030\001 \001(\0132\031." +
      "Hedwig.SubscriptionState\0224\n\013preferences\030" +
      "\002 \001(\0132\037.Hedwig.SubscriptionPreferences\"k" +
      "\n\013LedgerRange\022\020\n\010ledgerId\030\001 \002(\004\022.\n\020endSe" +
      "qIdIncluded\030\002 \001(\0132\024.Hedwig.MessageSeqId\022" +
      "\032\n\022startSeqIdIncluded\030\003 \001(\004\"3\n\014LedgerRan" +
      "ges\022#\n\006ranges\030\001 \003(\0132\023.Hedwig.LedgerRange" +
      "\":\n\013ManagerMeta\022\023\n\013managerImpl\030\002 \002(\t\022\026\n\016" +
      "managerVersion\030\003 \002(\r\".\n\013HubInfoData\022\020\n\010h" +
      "ostname\030\002 \002(\t\022\r\n\005czxid\030\003 \002(\004\" \n\013HubLoadD",
      "ata\022\021\n\tnumTopics\030\002 \002(\004*\"\n\017ProtocolVersio" +
      "n\022\017\n\013VERSION_ONE\020\001*\207\001\n\rOperationType\022\013\n\007" +
      "PUBLISH\020\000\022\r\n\tSUBSCRIBE\020\001\022\013\n\007CONSUME\020\002\022\017\n" +
      "\013UNSUBSCRIBE\020\003\022\022\n\016START_DELIVERY\020\004\022\021\n\rST" +
      "OP_DELIVERY\020\005\022\025\n\021CLOSESUBSCRIPTION\020\006*D\n\021" +
      "SubscriptionEvent\022\017\n\013TOPIC_MOVED\020\001\022\036\n\032SU" +
      "BSCRIPTION_FORCED_CLOSED\020\002*\241\004\n\nStatusCod" +
      "e\022\013\n\007SUCCESS\020\000\022\026\n\021MALFORMED_REQUEST\020\221\003\022\022" +
      "\n\rNO_SUCH_TOPIC\020\222\003\022\036\n\031CLIENT_ALREADY_SUB" +
      "SCRIBED\020\223\003\022\032\n\025CLIENT_NOT_SUBSCRIBED\020\224\003\022\026",
      "\n\021COULD_NOT_CONNECT\020\225\003\022\017\n\nTOPIC_BUSY\020\226\003\022" +
      "\032\n\025RESUBSCRIBE_EXCEPTION\020\227\003\022\036\n\031NOT_RESPO" +
      "NSIBLE_FOR_TOPIC\020\365\003\022\021\n\014SERVICE_DOWN\020\366\003\022\024" +
      "\n\017UNCERTAIN_STATE\020\367\003\022\033\n\026INVALID_MESSAGE_" +
      "FILTER\020\370\003\022\020\n\013BAD_VERSION\020\210\004\022\036\n\031NO_TOPIC_" +
      "PERSISTENCE_INFO\020\211\004\022\"\n\035TOPIC_PERSISTENCE" +
      "_INFO_EXISTS\020\212\004\022\032\n\025NO_SUBSCRIPTION_STATE" +
      "\020\213\004\022\036\n\031SUBSCRIPTION_STATE_EXISTS\020\214\004\022\030\n\023N" +
      "O_TOPIC_OWNER_INFO\020\215\004\022\034\n\027TOPIC_OWNER_INF" +
      "O_EXISTS\020\216\004\022\031\n\024UNEXPECTED_CONDITION\020\330\004\022\016",
      "\n\tCOMPOSITE\020\274\005B\036\n\032org.apache.hedwig.prot" +
      "ocolH\001"
    };
    com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =
      new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {
        public com.google.protobuf.ExtensionRegistry assignDescriptors(
            com.google.protobuf.Descriptors.FileDescriptor root) {
          descriptor = root;
          internal_static_Hedwig_Map_descriptor =
            getDescriptor().getMessageTypes().get(0);
          internal_static_Hedwig_Map_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_Hedwig_Map_descriptor,
              new java.lang.String[] { "Entries", },
              org.apache.hedwig.protocol.PubSubProtocol.Map.class,
              org.apache.hedwig.protocol.PubSubProtocol.Map.Builder.class);
          internal_static_Hedwig_Map_Entry_descriptor =
            internal_static_Hedwig_Map_descriptor.getNestedTypes().get(0);
          internal_static_Hedwig_Map_Entry_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_Hedwig_Map_Entry_descriptor,
              new java.lang.String[] { "Key", "Value", },
              org.apache.hedwig.protocol.PubSubProtocol.Map.Entry.class,
              org.apache.hedwig.protocol.PubSubProtocol.Map.Entry.Builder.class);
          internal_static_Hedwig_MessageHeader_descriptor =
            getDescriptor().getMessageTypes().get(1);
          internal_static_Hedwig_MessageHeader_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_Hedwig_MessageHeader_descriptor,
              new java.lang.String[] { "Properties", "MessageType", },
              org.apache.hedwig.protocol.PubSubProtocol.MessageHeader.class,
              org.apache.hedwig.protocol.PubSubProtocol.MessageHeader.Builder.class);
          internal_static_Hedwig_Message_descriptor =
            getDescriptor().getMessageTypes().get(2);
          internal_static_Hedwig_Message_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_Hedwig_Message_descriptor,
              new java.lang.String[] { "Body", "SrcRegion", "MsgId", "Header", },
              org.apache.hedwig.protocol.PubSubProtocol.Message.class,
              org.apache.hedwig.protocol.PubSubProtocol.Message.Builder.class);
          internal_static_Hedwig_RegionSpecificSeqId_descriptor =
            getDescriptor().getMessageTypes().get(3);
          internal_static_Hedwig_RegionSpecificSeqId_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_Hedwig_RegionSpecificSeqId_descriptor,
              new java.lang.String[] { "Region", "SeqId", },
              org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId.class,
              org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId.Builder.class);
          internal_static_Hedwig_MessageSeqId_descriptor =
            getDescriptor().getMessageTypes().get(4);
          internal_static_Hedwig_MessageSeqId_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_Hedwig_MessageSeqId_descriptor,
              new java.lang.String[] { "LocalComponent", "RemoteComponents", },
              org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.class,
              org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId.Builder.class);
          internal_static_Hedwig_PubSubRequest_descriptor =
            getDescriptor().getMessageTypes().get(5);
          internal_static_Hedwig_PubSubRequest_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_Hedwig_PubSubRequest_descriptor,
              new java.lang.String[] { "ProtocolVersion", "Type", "TriedServers", "TxnId", "ShouldClaim", "Topic", "PublishRequest", "SubscribeRequest", "ConsumeRequest", "UnsubscribeRequest", "StopDeliveryRequest", "StartDeliveryRequest", "CloseSubscriptionRequest", },
              org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest.class,
              org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest.Builder.class);
          internal_static_Hedwig_PublishRequest_descriptor =
            getDescriptor().getMessageTypes().get(6);
          internal_static_Hedwig_PublishRequest_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_Hedwig_PublishRequest_descriptor,
              new java.lang.String[] { "Msg", },
              org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.class,
              org.apache.hedwig.protocol.PubSubProtocol.PublishRequest.Builder.class);
          internal_static_Hedwig_SubscriptionPreferences_descriptor =
            getDescriptor().getMessageTypes().get(7);
          internal_static_Hedwig_SubscriptionPreferences_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_Hedwig_SubscriptionPreferences_descriptor,
              new java.lang.String[] { "Options", "MessageBound", "MessageFilter", "MessageWindowSize", },
              org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.class,
              org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences.Builder.class);
          internal_static_Hedwig_SubscribeRequest_descriptor =
            getDescriptor().getMessageTypes().get(8);
          internal_static_Hedwig_SubscribeRequest_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_Hedwig_SubscribeRequest_descriptor,
              new java.lang.String[] { "SubscriberId", "CreateOrAttach", "Synchronous", "MessageBound", "Preferences", "ForceAttach", },
              org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.class,
              org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.Builder.class);
          internal_static_Hedwig_SubscriptionOptions_descriptor =
            getDescriptor().getMessageTypes().get(9);
          internal_static_Hedwig_SubscriptionOptions_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_Hedwig_SubscriptionOptions_descriptor,
              new java.lang.String[] { "ForceAttach", "CreateOrAttach", "MessageBound", "Options", "MessageFilter", "MessageWindowSize", "EnableResubscribe", },
              org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions.class,
              org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions.Builder.class);
          internal_static_Hedwig_ConsumeRequest_descriptor =
            getDescriptor().getMessageTypes().get(10);
          internal_static_Hedwig_ConsumeRequest_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_Hedwig_ConsumeRequest_descriptor,
              new java.lang.String[] { "SubscriberId", "MsgId", },
              org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.class,
              org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest.Builder.class);
          internal_static_Hedwig_UnsubscribeRequest_descriptor =
            getDescriptor().getMessageTypes().get(11);
          internal_static_Hedwig_UnsubscribeRequest_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_Hedwig_UnsubscribeRequest_descriptor,
              new java.lang.String[] { "SubscriberId", },
              org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.class,
              org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest.Builder.class);
          internal_static_Hedwig_CloseSubscriptionRequest_descriptor =
            getDescriptor().getMessageTypes().get(12);
          internal_static_Hedwig_CloseSubscriptionRequest_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_Hedwig_CloseSubscriptionRequest_descriptor,
              new java.lang.String[] { "SubscriberId", },
              org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest.class,
              org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest.Builder.class);
          internal_static_Hedwig_StopDeliveryRequest_descriptor =
            getDescriptor().getMessageTypes().get(13);
          internal_static_Hedwig_StopDeliveryRequest_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_Hedwig_StopDeliveryRequest_descriptor,
              new java.lang.String[] { "SubscriberId", },
              org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.class,
              org.apache.hedwig.protocol.PubSubProtocol.StopDeliveryRequest.Builder.class);
          internal_static_Hedwig_StartDeliveryRequest_descriptor =
            getDescriptor().getMessageTypes().get(14);
          internal_static_Hedwig_StartDeliveryRequest_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_Hedwig_StartDeliveryRequest_descriptor,
              new java.lang.String[] { "SubscriberId", },
              org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.class,
              org.apache.hedwig.protocol.PubSubProtocol.StartDeliveryRequest.Builder.class);
          internal_static_Hedwig_SubscriptionEventResponse_descriptor =
            getDescriptor().getMessageTypes().get(15);
          internal_static_Hedwig_SubscriptionEventResponse_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_Hedwig_SubscriptionEventResponse_descriptor,
              new java.lang.String[] { "Event", },
              org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse.class,
              org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse.Builder.class);
          internal_static_Hedwig_PubSubResponse_descriptor =
            getDescriptor().getMessageTypes().get(16);
          internal_static_Hedwig_PubSubResponse_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_Hedwig_PubSubResponse_descriptor,
              new java.lang.String[] { "ProtocolVersion", "StatusCode", "TxnId", "StatusMsg", "Message", "Topic", "SubscriberId", "ResponseBody", },
              org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse.class,
              org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse.Builder.class);
          internal_static_Hedwig_PublishResponse_descriptor =
            getDescriptor().getMessageTypes().get(17);
          internal_static_Hedwig_PublishResponse_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_Hedwig_PublishResponse_descriptor,
              new java.lang.String[] { "PublishedMsgId", },
              org.apache.hedwig.protocol.PubSubProtocol.PublishResponse.class,
              org.apache.hedwig.protocol.PubSubProtocol.PublishResponse.Builder.class);
          internal_static_Hedwig_SubscribeResponse_descriptor =
            getDescriptor().getMessageTypes().get(18);
          internal_static_Hedwig_SubscribeResponse_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_Hedwig_SubscribeResponse_descriptor,
              new java.lang.String[] { "Preferences", },
              org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse.class,
              org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse.Builder.class);
          internal_static_Hedwig_ResponseBody_descriptor =
            getDescriptor().getMessageTypes().get(19);
          internal_static_Hedwig_ResponseBody_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_Hedwig_ResponseBody_descriptor,
              new java.lang.String[] { "PublishResponse", "SubscribeResponse", "SubscriptionEvent", },
              org.apache.hedwig.protocol.PubSubProtocol.ResponseBody.class,
              org.apache.hedwig.protocol.PubSubProtocol.ResponseBody.Builder.class);
          internal_static_Hedwig_SubscriptionState_descriptor =
            getDescriptor().getMessageTypes().get(20);
          internal_static_Hedwig_SubscriptionState_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_Hedwig_SubscriptionState_descriptor,
              new java.lang.String[] { "MsgId", "MessageBound", },
              org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState.class,
              org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState.Builder.class);
          internal_static_Hedwig_SubscriptionData_descriptor =
            getDescriptor().getMessageTypes().get(21);
          internal_static_Hedwig_SubscriptionData_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_Hedwig_SubscriptionData_descriptor,
              new java.lang.String[] { "State", "Preferences", },
              org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData.class,
              org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData.Builder.class);
          internal_static_Hedwig_LedgerRange_descriptor =
            getDescriptor().getMessageTypes().get(22);
          internal_static_Hedwig_LedgerRange_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_Hedwig_LedgerRange_descriptor,
              new java.lang.String[] { "LedgerId", "EndSeqIdIncluded", "StartSeqIdIncluded", },
              org.apache.hedwig.protocol.PubSubProtocol.LedgerRange.class,
              org.apache.hedwig.protocol.PubSubProtocol.LedgerRange.Builder.class);
          internal_static_Hedwig_LedgerRanges_descriptor =
            getDescriptor().getMessageTypes().get(23);
          internal_static_Hedwig_LedgerRanges_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_Hedwig_LedgerRanges_descriptor,
              new java.lang.String[] { "Ranges", },
              org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges.class,
              org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges.Builder.class);
          internal_static_Hedwig_ManagerMeta_descriptor =
            getDescriptor().getMessageTypes().get(24);
          internal_static_Hedwig_ManagerMeta_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_Hedwig_ManagerMeta_descriptor,
              new java.lang.String[] { "ManagerImpl", "ManagerVersion", },
              org.apache.hedwig.protocol.PubSubProtocol.ManagerMeta.class,
              org.apache.hedwig.protocol.PubSubProtocol.ManagerMeta.Builder.class);
          internal_static_Hedwig_HubInfoData_descriptor =
            getDescriptor().getMessageTypes().get(25);
          internal_static_Hedwig_HubInfoData_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_Hedwig_HubInfoData_descriptor,
              new java.lang.String[] { "Hostname", "Czxid", },
              org.apache.hedwig.protocol.PubSubProtocol.HubInfoData.class,
              org.apache.hedwig.protocol.PubSubProtocol.HubInfoData.Builder.class);
          internal_static_Hedwig_HubLoadData_descriptor =
            getDescriptor().getMessageTypes().get(26);
          internal_static_Hedwig_HubLoadData_fieldAccessorTable = new
            com.google.protobuf.GeneratedMessage.FieldAccessorTable(
              internal_static_Hedwig_HubLoadData_descriptor,
              new java.lang.String[] { "NumTopics", },
              org.apache.hedwig.protocol.PubSubProtocol.HubLoadData.class,
              org.apache.hedwig.protocol.PubSubProtocol.HubLoadData.Builder.class);
          return null;
        }
      };
    com.google.protobuf.Descriptors.FileDescriptor
      .internalBuildGeneratedFileFrom(descriptorData,
        new com.google.protobuf.Descriptors.FileDescriptor[] {
        }, assigner);
  }
  
  // @@protoc_insertion_point(outer_class_scope)
}
"
hedwig-protocol/src/main/java/org/apache/hedwig/protoextensions/MapUtils.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.protoextensions;

import java.util.HashMap;
import java.util.Map;

import com.google.protobuf.ByteString;
import org.apache.hedwig.protocol.PubSubProtocol;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class MapUtils {

    static final Logger logger = LoggerFactory.getLogger(MapUtils.class);

    public static String toString(PubSubProtocol.Map map) {
        StringBuilder sb = new StringBuilder();
        int numEntries = map.getEntriesCount();
        for (int i=0; i<numEntries; i++) {
            PubSubProtocol.Map.Entry entry = map.getEntries(i);
            String key = entry.getKey();
            ByteString value = entry.getValue();
            sb.append(key).append('=').append(value.toStringUtf8());
            if (i != (numEntries - 1)) {
                sb.append(',');
            }
        }
        return sb.toString();
    }

    public static Map<String, ByteString> buildMap(PubSubProtocol.Map protoMap) {
        Map<String, ByteString> javaMap = new HashMap<String, ByteString>();

        int numEntries = protoMap.getEntriesCount();
        for (int i=0; i<numEntries; i++) {
            PubSubProtocol.Map.Entry entry = protoMap.getEntries(i);
            String key = entry.getKey();
            if (javaMap.containsKey(key)) {
                ByteString preValue = javaMap.get(key);
                logger.warn("Key " + key + " has already been defined as value : " + preValue.toStringUtf8());
            } else {
                javaMap.put(key, entry.getValue());
            }
        }
        return javaMap;
    }

    public static PubSubProtocol.Map.Builder buildMapBuilder(Map<String, ByteString> javaMap) {
        PubSubProtocol.Map.Builder mapBuilder = PubSubProtocol.Map.newBuilder();

        for (Map.Entry<String, ByteString> entry : javaMap.entrySet()) {
            mapBuilder.addEntries(PubSubProtocol.Map.Entry.newBuilder().setKey(entry.getKey())
                                                .setValue(entry.getValue()));
        }
        return mapBuilder;
    }
}
"
hedwig-protocol/src/main/java/org/apache/hedwig/protoextensions/MessageIdUtils.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.protoextensions;

import java.util.HashMap;
import java.util.List;
import java.util.Map;

import com.google.protobuf.ByteString;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.exceptions.PubSubException.UnexpectedConditionException;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId;

public class MessageIdUtils {

    public static String msgIdToReadableString(MessageSeqId seqId) {
        StringBuilder sb = new StringBuilder();
        sb.append("local:");
        sb.append(seqId.getLocalComponent());

        String separator = ";";
        for (RegionSpecificSeqId regionId : seqId.getRemoteComponentsList()) {
            sb.append(separator);
            sb.append(regionId.getRegion().toStringUtf8());
            sb.append(':');
            sb.append(regionId.getSeqId());
        }
        return sb.toString();
    }

    public static Map<ByteString, RegionSpecificSeqId> inMapForm(MessageSeqId msi) {
        Map<ByteString, RegionSpecificSeqId> map = new HashMap<ByteString, RegionSpecificSeqId>();

        for (RegionSpecificSeqId lmsid : msi.getRemoteComponentsList()) {
            map.put(lmsid.getRegion(), lmsid);
        }

        return map;
    }

    public static boolean areEqual(MessageSeqId m1, MessageSeqId m2) {

        if (m1.getLocalComponent() != m2.getLocalComponent()) {
            return false;
        }

        if (m1.getRemoteComponentsCount() != m2.getRemoteComponentsCount()) {
            return false;
        }

        Map<ByteString, RegionSpecificSeqId> m2map = inMapForm(m2);

        for (RegionSpecificSeqId lmsid1 : m1.getRemoteComponentsList()) {
            RegionSpecificSeqId lmsid2 = m2map.get(lmsid1.getRegion());
            if (lmsid2 == null) {
                return false;
            }
            if (lmsid1.getSeqId() != lmsid2.getSeqId()) {
                return false;
            }
        }

        return true;

    }

    public static Message mergeLocalSeqId(Message.Builder messageBuilder, long localSeqId) {
        MessageSeqId.Builder msidBuilder = MessageSeqId.newBuilder(messageBuilder.getMsgId());
        msidBuilder.setLocalComponent(localSeqId);
        messageBuilder.setMsgId(msidBuilder);
        return messageBuilder.build();
    }

    public static Message mergeLocalSeqId(Message orginalMessage, long localSeqId) {
        return mergeLocalSeqId(Message.newBuilder(orginalMessage), localSeqId);
    }

    /**
     * Compares two seq numbers represented as lists of longs.
     *
     * @param l1
     * @param l2
     * @return 1 if the l1 is greater, 0 if they are equal, -1 if l2 is greater
     * @throws UnexpectedConditionException
     *             If the lists are of unequal length
     */
    public static int compare(List<Long> l1, List<Long> l2) throws UnexpectedConditionException {
        if (l1.size() != l2.size()) {
            throw new UnexpectedConditionException("Seq-ids being compared have different sizes: " + l1.size()
                                                   + " and " + l2.size());
        }

        for (int i = 0; i < l1.size(); i++) {
            long v1 = l1.get(i);
            long v2 = l2.get(i);

            if (v1 == v2) {
                continue;
            }

            return v1 > v2 ? 1 : -1;
        }

        // All components equal
        return 0;
    }

    /**
     * Returns the element-wise vector maximum of the two vectors id1 and id2,
     * if we imagine them to be sparse representations of vectors.
     */
    public static void takeRegionMaximum(MessageSeqId.Builder newIdBuilder, MessageSeqId id1, MessageSeqId id2) {
        Map<ByteString, RegionSpecificSeqId> id2Map = MessageIdUtils.inMapForm(id2);

        for (RegionSpecificSeqId rrsid1 : id1.getRemoteComponentsList()) {
            ByteString region = rrsid1.getRegion();

            RegionSpecificSeqId rssid2 = id2Map.get(region);

            if (rssid2 == null) {
                newIdBuilder.addRemoteComponents(rrsid1);
                continue;
            }

            newIdBuilder.addRemoteComponents((rrsid1.getSeqId() > rssid2.getSeqId()) ? rrsid1 : rssid2);

            // remove from map
            id2Map.remove(region);
        }

        // now take the remaining components in the map and add them
        for (RegionSpecificSeqId rssid2 : id2Map.values()) {
            newIdBuilder.addRemoteComponents(rssid2);
        }

    }
}
"
hedwig-protocol/src/main/java/org/apache/hedwig/protoextensions/PubSubResponseUtils.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.protoextensions;

import com.google.protobuf.ByteString;

import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse;
import org.apache.hedwig.protocol.PubSubProtocol.ResponseBody;
import org.apache.hedwig.protocol.PubSubProtocol.StatusCode;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEvent;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEventResponse;

public class PubSubResponseUtils {

    /**
     * Change here if bumping up the version number that the server sends back
     */
    public final static ProtocolVersion serverVersion = ProtocolVersion.VERSION_ONE;

    static PubSubResponse.Builder getBasicBuilder(StatusCode status) {
        return PubSubResponse.newBuilder().setProtocolVersion(serverVersion).setStatusCode(status);
    }

    public static PubSubResponse getSuccessResponse(long txnId) {
        return getBasicBuilder(StatusCode.SUCCESS).setTxnId(txnId).build();
    }

    public static PubSubResponse getSuccessResponse(long txnId, ResponseBody respBody) {
        return getBasicBuilder(StatusCode.SUCCESS).setTxnId(txnId)
               .setResponseBody(respBody).build();
    }

    public static PubSubResponse getResponseForException(PubSubException e, long txnId) {
        return getBasicBuilder(e.getCode()).setStatusMsg(e.getMessage()).setTxnId(txnId).build();
    }

    public static PubSubResponse getResponseForSubscriptionEvent(ByteString topic,
                                                                 ByteString subscriberId,
                                                                 SubscriptionEvent event) {
        SubscriptionEventResponse.Builder eventBuilder =
            SubscriptionEventResponse.newBuilder().setEvent(event);
        ResponseBody.Builder respBuilder =
            ResponseBody.newBuilder().setSubscriptionEvent(eventBuilder);
        PubSubResponse response = PubSubResponse.newBuilder()
                                  .setProtocolVersion(ProtocolVersion.VERSION_ONE)
                                  .setStatusCode(StatusCode.SUCCESS).setTxnId(0)
                                  .setTopic(topic).setSubscriberId(subscriberId)
                                  .setResponseBody(respBuilder).build();
        return response;
    }
}
"
hedwig-protocol/src/main/java/org/apache/hedwig/protoextensions/SubscriptionStateUtils.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.protoextensions;

import java.util.HashMap;
import java.util.Map;

import com.google.protobuf.ByteString;
import com.google.protobuf.InvalidProtocolBufferException;
import org.apache.hedwig.protocol.PubSubProtocol;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class SubscriptionStateUtils {

    static final Logger logger = LoggerFactory.getLogger(SubscriptionStateUtils.class);

    // For now, to differentiate hub subscribers from local ones, the
    // subscriberId will be prepended with a hard-coded prefix. Local
    // subscribers will validate that the subscriberId used cannot start with
    // this prefix. This is only used internally by the hub subscribers.
    public static final String HUB_SUBSCRIBER_PREFIX = "__";

    public static SubscriptionData parseSubscriptionData(byte[] data)
    throws InvalidProtocolBufferException {
        try {
            return SubscriptionData.parseFrom(data);
        } catch (InvalidProtocolBufferException ex) {
            logger.info("Failed to parse data as SubscriptionData. Fall backward to parse it as SubscriptionState for backward compatability.");
            // backward compability
            SubscriptionState state = SubscriptionState.parseFrom(data);
            return SubscriptionData.newBuilder().setState(state).build();
        }
    }

    public static String toString(SubscriptionData data) {
        StringBuilder sb = new StringBuilder();
        if (data.hasState()) {
            sb.append("State : { ").append(toString(data.getState())).append(" };");
        }
        if (data.hasPreferences()) {
            sb.append("Preferences : { ").append(toString(data.getPreferences())).append(" };");
        }
        return sb.toString();
    }

    public static String toString(SubscriptionState state) {
        StringBuilder sb = new StringBuilder();
        sb.append("consumeSeqId: " + MessageIdUtils.msgIdToReadableString(state.getMsgId()));
        return sb.toString();
    }

    public static String toString(SubscriptionPreferences preferences) {
        StringBuilder sb = new StringBuilder();
        sb.append("System Preferences : [");
        if (preferences.hasMessageBound()) {
            sb.append("(messageBound=").append(preferences.getMessageBound())
              .append(")");
        }
        sb.append("]");
        if (preferences.hasOptions()) {
            sb.append(", Customized Preferences : [");
            sb.append(MapUtils.toString(preferences.getOptions()));
            sb.append("]");
        }
        return sb.toString();
    }

    public static boolean isHubSubscriber(ByteString subscriberId) {
        return subscriberId.toStringUtf8().startsWith(HUB_SUBSCRIBER_PREFIX);
    }

    public static Map<String, ByteString> buildUserOptions(SubscriptionPreferences preferences) {
        if (preferences.hasOptions()) {
            return MapUtils.buildMap(preferences.getOptions());
        } else {
            return new HashMap<String, ByteString>();
        }
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/admin/HedwigAdmin.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hedwig.admin;

import java.util.Arrays;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.TimeUnit;

import org.apache.bookkeeper.conf.ClientConfiguration;
import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.client.BookKeeper;
import org.apache.bookkeeper.client.BookKeeper.DigestType;
import org.apache.bookkeeper.client.LedgerHandle;
import org.apache.bookkeeper.versioning.Versioned;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.protocol.PubSubProtocol.LedgerRange;
import org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.meta.MetadataManagerFactory;
import org.apache.hedwig.server.meta.FactoryLayout;
import org.apache.hedwig.server.meta.SubscriptionDataManager;
import org.apache.hedwig.server.meta.TopicOwnershipManager;
import org.apache.hedwig.server.meta.TopicPersistenceManager;
import org.apache.hedwig.server.subscriptions.InMemorySubscriptionState;
import org.apache.hedwig.server.topics.HubInfo;
import org.apache.hedwig.server.topics.HubLoad;
import org.apache.hedwig.util.Callback;
import org.apache.hedwig.util.HedwigSocketAddress;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.WatchedEvent;
import org.apache.zookeeper.Watcher;
import org.apache.zookeeper.ZooKeeper;
import org.apache.zookeeper.data.Stat;

import com.google.protobuf.ByteString;
import com.google.protobuf.InvalidProtocolBufferException;

/**
 * Hedwig Admin
 */
public class HedwigAdmin {
    static final Logger LOG = LoggerFactory.getLogger(HedwigAdmin.class);

    // NOTE: now it is fixed passwd used in hedwig
    static byte[] passwd = "sillysecret".getBytes();

    protected final ZooKeeper zk;
    protected final BookKeeper bk;
    protected final MetadataManagerFactory mmFactory;
    protected final SubscriptionDataManager sdm;
    protected final TopicOwnershipManager tom;
    protected final TopicPersistenceManager tpm;

    // hub configurations
    protected final ServerConfiguration serverConf;
    // bookkeeper configurations
    protected final ClientConfiguration bkClientConf;

    protected final CountDownLatch zkReadyLatch = new CountDownLatch(1);

    // Empty watcher
    private class MyWatcher implements Watcher {
        public void process(WatchedEvent event) {
            if (Event.KeeperState.SyncConnected.equals(event.getState())) {
                zkReadyLatch.countDown();
            }
        }
    }

    static class SyncObj<T> {
        boolean finished = false;
        boolean success = false;
        T value = null;
        PubSubException exception = null;

        synchronized void success(T v) {
            finished = true;
            success = true;
            value = v;
            notify();
        }

        synchronized void fail(PubSubException pse) {
            finished = true;
            success = false;
            exception = pse;
            notify();
        }

        synchronized void block() {
            try {
                while (!finished) {
                    wait();
                }
            } catch (InterruptedException ie) {
            }
        }

        synchronized boolean isSuccess() {
            return success;
        }
    }

    /**
     * Stats of a hub
     */
    public static class HubStats {
        HubInfo hubInfo;
        HubLoad hubLoad;

        public HubStats(HubInfo info, HubLoad load) {
            this.hubInfo = info;
            this.hubLoad = load;
        }

        @Override
        public String toString() {
            StringBuilder sb = new StringBuilder();
            sb.append("info : [").append(hubInfo.toString().trim().replaceAll("\n", ", "))
              .append("], load : [").append(hubLoad.toString().trim().replaceAll("\n", ", "))
              .append("]");
            return sb.toString();
        }
    }

    /**
     * Hedwig Admin Constructor
     *
     * @param bkConf
     *          BookKeeper Client Configuration.
     * @param hubConf
     *          Hub Server Configuration.
     * @throws Exception
     */
    public HedwigAdmin(ClientConfiguration bkConf, ServerConfiguration hubConf) throws Exception {
        this.serverConf = hubConf;
        this.bkClientConf = bkConf;

        // connect to zookeeper
        zk = new ZooKeeper(hubConf.getZkHost(), hubConf.getZkTimeout(), new MyWatcher());
        LOG.debug("Connecting to zookeeper {}, timeout = {}",
                hubConf.getZkHost(), hubConf.getZkTimeout());
        // wait until connection is ready
        if (!zkReadyLatch.await(hubConf.getZkTimeout() * 2, TimeUnit.MILLISECONDS)) {
            throw new Exception("Count not establish connection with ZooKeeper after " + hubConf.getZkTimeout() * 2 + " ms.");
        }

        // construct the metadata manager factory
        mmFactory = MetadataManagerFactory.newMetadataManagerFactory(hubConf, zk);
        tpm = mmFactory.newTopicPersistenceManager();
        tom = mmFactory.newTopicOwnershipManager();
        sdm = mmFactory.newSubscriptionDataManager();

        // connect to bookkeeper
        bk = new BookKeeper(bkClientConf, zk);
        LOG.debug("Connecting to bookkeeper");
    }

    /**
     * Close the hedwig admin.
     *
     * @throws Exception
     */
    public void close() throws Exception {
        tpm.close();
        tom.close();
        sdm.close();
        mmFactory.shutdown();
        bk.close();
        zk.close();
    }

    /**
     * Return zookeeper handle used in hedwig admin.
     *
     * @return zookeeper handle
     */
    public ZooKeeper getZkHandle() {
        return zk;
    }

    /**
     * Return bookkeeper handle used in hedwig admin.
     *
     * @return bookkeeper handle
     */
    public BookKeeper getBkHandle() {
        return bk;
    }

    /**
     * Return hub server configuration used in hedwig admin
     *
     * @return hub server configuration
     */
    public ServerConfiguration getHubServerConf() {
        return serverConf;
    }

    /**
     * Return metadata manager factory.
     *
     * @return metadata manager factory instance.
     */
    public MetadataManagerFactory getMetadataManagerFactory() {
        return mmFactory;
    }

    /**
     * Return bookeeper passwd used in hedwig admin
     *
     * @return bookeeper passwd
     */
    public byte[] getBkPasswd() {
        return Arrays.copyOf(passwd, passwd.length);
    }

    /**
     * Return digest type used in hedwig admin
     *
     * @return bookeeper digest type
     */
    public DigestType getBkDigestType() {
        return DigestType.CRC32;
    }

    /**
     * Dose topic exist?
     *
     * @param topic
     *            Topic name
     * @return whether topic exists or not?
     * @throws Exception
     */
    public boolean hasTopic(ByteString topic) throws Exception {
        // current persistence info is bound with a topic, so if there is persistence info
        // there is topic.
        final SyncObj<Boolean> syncObj = new SyncObj<Boolean>();
        tpm.readTopicPersistenceInfo(topic, new Callback<Versioned<LedgerRanges>>() {
            @Override
            public void operationFinished(Object ctx, Versioned<LedgerRanges> result) {
                if (null == result) {
                    syncObj.success(false);
                } else {
                    syncObj.success(true);
                }
            }
            @Override
            public void operationFailed(Object ctx, PubSubException pse) {
                syncObj.fail(pse);
            }
        }, syncObj);

        syncObj.block();

        if (!syncObj.isSuccess()) {
            throw syncObj.exception;
        }

        return syncObj.value;
    }

    /**
     * Get available hubs.
     *
     * @return available hubs and their loads
     * @throws Exception
     */
    public Map<HedwigSocketAddress, HubStats> getAvailableHubs() throws Exception {
        String zkHubsPath = serverConf.getZkHostsPrefix(new StringBuilder()).toString();
        Map<HedwigSocketAddress, HubStats> hubs =
            new HashMap<HedwigSocketAddress, HubStats>();
        List<String> hosts = zk.getChildren(zkHubsPath, false);
        for (String host : hosts) {
            String zkHubPath = serverConf.getZkHostsPrefix(new StringBuilder())
                                         .append("/").append(host).toString();
            HedwigSocketAddress addr = new HedwigSocketAddress(host);
            try {
                Stat stat = new Stat();
                byte[] data = zk.getData(zkHubPath, false, stat);
                if (data == null) {
                    continue;
                }
                HubLoad load = HubLoad.parse(new String(data));
                HubInfo info = new HubInfo(addr, stat.getCzxid());
                hubs.put(addr, new HubStats(info, load));
            } catch (KeeperException ke) {
                LOG.warn("Couldn't read hub data from ZooKeeper", ke);
            } catch (InterruptedException ie) {
                LOG.warn("Interrupted during read", ie);
            }
        }
        return hubs;
    }

    /**
     * Get list of topics
     *
     * @return list of topics
     * @throws Exception
     */
    public Iterator<ByteString> getTopics() throws Exception {
        return mmFactory.getTopics();
    }

    /**
     * Return the topic owner of a topic
     *
     * @param topic
     *            Topic name
     * @return the address of the owner of a topic
     * @throws Exception
     */
    public HubInfo getTopicOwner(ByteString topic) throws Exception {
        final SyncObj<HubInfo> syncObj = new SyncObj<HubInfo>();
        tom.readOwnerInfo(topic, new Callback<Versioned<HubInfo>>() {
            @Override
            public void operationFinished(Object ctx, Versioned<HubInfo> result) {
                if (null == result) {
                    syncObj.success(null);
                } else {
                    syncObj.success(result.getValue());
                }
            }
            @Override
            public void operationFailed(Object ctx, PubSubException pse) {
                syncObj.fail(pse);
            }
        }, syncObj);

        syncObj.block();

        if (!syncObj.isSuccess()) {
            throw syncObj.exception;
        }

        return syncObj.value;
    }

    private static LedgerRange buildLedgerRange(long ledgerId, long startOfLedger, MessageSeqId endOfLedger) {
        LedgerRange.Builder builder =
            LedgerRange.newBuilder().setLedgerId(ledgerId).setStartSeqIdIncluded(startOfLedger)
                       .setEndSeqIdIncluded(endOfLedger);
        return builder.build();
    }

    /**
     * Return the ledger range forming the topic
     *
     * @param topic
     *          Topic name
     * @return ledger ranges forming the topic
     * @throws Exception
     */
    public List<LedgerRange> getTopicLedgers(ByteString topic) throws Exception {
        final SyncObj<LedgerRanges> syncObj = new SyncObj<LedgerRanges>();
        tpm.readTopicPersistenceInfo(topic, new Callback<Versioned<LedgerRanges>>() {
            @Override
            public void operationFinished(Object ctx, Versioned<LedgerRanges> result) {
                if (null == result) {
                    syncObj.success(null);
                } else {
                    syncObj.success(result.getValue());
                }
            }
            @Override
            public void operationFailed(Object ctx, PubSubException pse) {
                syncObj.fail(pse);
            }
        }, syncObj);

        syncObj.block();

        if (!syncObj.isSuccess()) {
            throw syncObj.exception;
        }

        LedgerRanges ranges = syncObj.value;
        if (null == ranges) {
            return null;
        }
        List<LedgerRange> results = new ArrayList<LedgerRange>();
        List<LedgerRange> lrs = ranges.getRangesList();
        long startSeqId = 1L;
        if (!lrs.isEmpty()) {
            LedgerRange range = lrs.get(0);
            if (!range.hasStartSeqIdIncluded() && range.hasEndSeqIdIncluded()) {
                long ledgerId = range.getLedgerId();
                try {
                    LedgerHandle lh = bk.openLedgerNoRecovery(ledgerId, DigestType.CRC32, passwd);
                    long numEntries = lh.readLastConfirmed() + 1;
                    long endOfLedger = range.getEndSeqIdIncluded().getLocalComponent();
                    startSeqId = endOfLedger - numEntries + 1;
                } catch (BKException.BKNoSuchLedgerExistsException be) {
                    // ignore it
                }
            }
        }
        Iterator<LedgerRange> lrIter = lrs.iterator();
        while (lrIter.hasNext()) {
            LedgerRange range = lrIter.next();
            if (range.hasEndSeqIdIncluded()) {
                long endOfLedger = range.getEndSeqIdIncluded().getLocalComponent();
                if (range.hasStartSeqIdIncluded()) {
                    startSeqId = range.getStartSeqIdIncluded();
                } else {
                    range = buildLedgerRange(range.getLedgerId(), startSeqId, range.getEndSeqIdIncluded());
                }
                results.add(range);
                if (startSeqId < endOfLedger + 1) {
                    startSeqId = endOfLedger + 1;
                }
                continue;
            }
            if (lrIter.hasNext()) {
                throw new IllegalStateException("Ledger " + range.getLedgerId() + " for topic " + topic.toString()
                                                + " is not the last one but still does not have an end seq-id");
            }

            if (range.hasStartSeqIdIncluded()) {
                startSeqId = range.getStartSeqIdIncluded();
            }

            LedgerHandle lh = bk.openLedgerNoRecovery(range.getLedgerId(), DigestType.CRC32, passwd);
            long endOfLedger = startSeqId + lh.readLastConfirmed();
            MessageSeqId endSeqId = MessageSeqId.newBuilder().setLocalComponent(endOfLedger).build();
            results.add(buildLedgerRange(range.getLedgerId(), startSeqId, endSeqId));
        }
        return results;
    }

    /**
     * Return subscriptions of a topic
     *
     * @param topic
     *          Topic name
     * @return subscriptions of a topic
     * @throws Exception
     */
    public Map<ByteString, SubscriptionData> getTopicSubscriptions(ByteString topic)
        throws Exception {

        final SyncObj<Map<ByteString, SubscriptionData>> syncObj =
            new SyncObj<Map<ByteString, SubscriptionData>>();
        sdm.readSubscriptions(topic, new Callback<Map<ByteString, Versioned<SubscriptionData>>>() {
            @Override
            public void operationFinished(Object ctx, Map<ByteString, Versioned<SubscriptionData>> result) {
                // It was just used to console tool to print some information, so don't need to return version for it
                // just keep the getTopicSubscriptions interface as before
                Map<ByteString, SubscriptionData> subs = new ConcurrentHashMap<ByteString, SubscriptionData>();
                for (Map.Entry<ByteString, Versioned<SubscriptionData>> subEntry : result.entrySet()) {
                    subs.put(subEntry.getKey(), subEntry.getValue().getValue());
                }
                syncObj.success(subs);
            }
            @Override
            public void operationFailed(Object ctx, PubSubException pse) {
                syncObj.fail(pse);
            }
        }, syncObj);

        syncObj.block();

        if (!syncObj.isSuccess()) {
            throw syncObj.exception;
        }

        return syncObj.value;
    }

    /**
     * Return subscription state of a subscriber of topic
     *
     * @param topic
     *          Topic name
     * @param subscriber
     *          Subscriber name
     * @return subscription state
     * @throws Exception
     */
    public SubscriptionData getSubscription(ByteString topic, ByteString subscriber) throws Exception {
        final SyncObj<SubscriptionData> syncObj = new SyncObj<SubscriptionData>();
        sdm.readSubscriptionData(topic, subscriber, new Callback<Versioned<SubscriptionData>>() {
            @Override
            public void operationFinished(Object ctx, Versioned<SubscriptionData> result) {
                if (null == result) {
                    syncObj.success(null);
                } else {
                    syncObj.success(result.getValue());
                }
            }
            @Override
            public void operationFailed(Object ctx, PubSubException pse) {
                syncObj.fail(pse);
            }
        }, syncObj);

        syncObj.block();

        if (!syncObj.isSuccess()) {
            throw syncObj.exception;
        }

        return syncObj.value;
    }

    /**
     * Format metadata for Hedwig.
     */
    public void format() throws Exception {
        // format metadata first
        mmFactory.format(serverConf, zk);
        LOG.info("Formatted Hedwig metadata successfully.");
        // remove metadata layout
        FactoryLayout.deleteLayout(zk, serverConf);
        LOG.info("Removed old factory layout.");
        // create new metadata manager factory and write new metadata layout
        MetadataManagerFactory.createMetadataManagerFactory(serverConf, zk,
            serverConf.getMetadataManagerFactoryClass());
        LOG.info("Created new factory layout.");
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/admin/console/HedwigCommands.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hedwig.admin.console;

import java.util.Map;
import java.util.List;
import java.util.LinkedList;
import java.util.LinkedHashMap;

/**
 * List all the available commands
 */
public final class HedwigCommands {

    static final String[] EMPTY_ARRAY = new String[0];

    //
    // List all commands used to play with hedwig
    //

    /* PUB : publish a message to hedwig */
    static final String PUB = "pub";
    static final String PUB_DESC = "Publish a message to a topic in Hedwig";
    static final String[] PUB_USAGE = new String[] {
        "usage: pub {topic} {message}",
        "",
        "  {topic}   : topic name.",
        "              any printable string without spaces.",
        "  {message} : message body.",
        "              remaining arguments are used as message body to publish.",
    };

    /* SUB : subscriber a topic in hedwig for a specified subscriber */
    static final String SUB = "sub";
    static final String SUB_DESC = "Subscribe a topic for a specified subscriber";
    static final String[] SUB_USAGE = new String[] {
        "usage: sub {topic} {subscriber} [mode]",
        "",
        "  {topic}      : topic name.",
        "                 any printable string without spaces.",
        "  {subscriber} : subscriber id.",
        "                 any printable string without spaces.",
        "  [mode]       : mode to create subscription.",
        "  [receive]    : bool. whether to start delivery to receive messages.",
        "",
        "  available modes: (default value is 1)",
        "    0 = CREATE: create subscription.",
        "                if the subscription is exsited, it will fail.",
        "    1 = ATTACH: attach to exsited subscription.",
        "                if the subscription is not existed, it will faile.",
        "    2 = CREATE_OR_ATTACH:",
        "                attach to subscription, if not existed create one."
    };

    /* CLOSESUB : close the subscription of a subscriber for a topic */
    static final String CLOSESUB = "closesub";
    static final String CLOSESUB_DESC = "Close subscription of a subscriber to a specified topic";
    static final String[] CLOSESUB_USAGE = new String[] {
        "usage: closesub {topic} {subscriber}",
        "",
        "  {topic}      : topic name.",
        "                 any printable string without spaces.",
        "  {subscriber} : subscriber id.",
        "                 any printable string without spaces.",
        "",
        " NOTE: this command just cleanup subscription states on client side.",
        "       You can try UNSUB to clean subscription states on server side.",
    };

    /* UNSUB: unsubscribe of a subscriber to a topic */
    static final String UNSUB = "unsub";
    static final String UNSUB_DESC = "Unsubscribe a topic for a subscriber";
    static final String[] UNSUB_USAGE = new String[] {
        "usage: unsub {topic} {subscriber}",
        "",
        "  {topic}      : topic name.",
        "                 any printable string without spaces.",
        "  {subscriber} : subscriber id.",
        "                 any printable string without spaces.",
        "",
        " NOTE: this command will cleanup subscription states on server side.",
        "       You can try CLOSESUB to just clean subscription states on client side.",
    };

    static final String RMSUB = "rmsub";
    static final String RMSUB_DESC = "Remove subscriptions for topics";
    static final String[] RMSUB_USAGE = new String[] {
        "usage: rmsub {topic_prefix} {start_topic} {end_topic} {subscriber_prefix} {start_sub} {end_sub}",
        "",
        "  {topic_prefix}       : topic prefix.",
        "  {start_topic}        : start topic id.",
        "  {end_topic}          : end topic id.",
        "  {subscriber_prefix}  : subscriber prefix.",
        "  {start_sub}          : start subscriber id.",
        "  {end_sub}            : end subscriber id.",
    };

    /* CONSUME: move consume ptr of a subscription with specified steps */
    static final String CONSUME = "consume";
    static final String CONSUME_DESC = "Move consume ptr of a subscription with sepcified steps";
    static final String[] CONSUME_USAGE = new String[] {
        "usage: consume {topic} {subscriber} {nmsgs}",
        "",
        "  {topic}      : topic name.",
        "                 any printable string without spaces.",
        "  {subscriber} : subscriber id.",
        "                 any printable string without spaces.",
        "  {nmsgs}      : how many messages to move consume ptr.",
        "",
        "  Example:",
        "  suppose, from zk we know subscriber B consumed topic T to message 10",
        "  [hedwig: (standalone) 1] consume T B 2",
        "  after executed above command, a consume(10+2) request will be sent to hedwig.",
        "",
        "  NOTE:",
        "  since Hedwig updates subscription consume ptr lazily, so you need to know that",
        "    1) the consumption ptr read from zookeeper may be stable; ",
        "    2) after sent the consume request, hedwig may just move ptr in its memory and lazily update it to zookeeper. you may not see the ptr changed when DESCRIBE the topic.",
    };

    /* CONSUMETO: move consume ptr of a subscription to a specified pos */
    static final String CONSUMETO = "consumeto";
    static final String CONSUMETO_DESC = "Move consume ptr of a subscription to a specified message id";
    static final String[] CONSUMETO_USAGE = new String[] {
        "usage: consumeto {topic} {subscriber} {msg_id}",
        "",
        "  {topic}      : topic name.",
        "                 any printable string without spaces.",
        "  {subscriber} : subscriber id.",
        "                 any printable string without spaces.",
        "  {msg_id}     : message id that consume ptr will be moved to.",
        "                 if the message id is less than current consume ptr,",
        "                 hedwig will do nothing.",
        "",
        "  Example:",
        "  suppose, from zk we know subscriber B consumed topic T to message 10",
        "  [hedwig: (standalone) 1] consumeto T B 12",
        "  after executed above command, a consume(12) request will be sent to hedwig.",
        "",
        "  NOTE:",
        "  since Hedwig updates subscription consume ptr lazily, so you need to know that",
        "    1) the consumption ptr read from zookeeper may be stable; ",
        "    2) after sent the consume request, hedwig may just move ptr in its memory and lazily update it to zookeeper. you may not see the ptr changed when DESCRIBE the topic.",
    };

    /* PUBSUB: a healthy checking command to ensure cluster is running */
    static final String PUBSUB = "pubsub";
    static final String PUBSUB_DESC = "A healthy checking command to ensure hedwig is in running state";
    static final String[] PUBSUB_USAGE = new String[] {
        "usage: pubsub {topic} {subscriber} {timeout_secs} {message}",
        "",
        "  {topic}        : topic name.",
        "                   any printable string without spaces.",
        "  {subscriber}   : subscriber id.",
        "                   any printable string without spaces.",
        "  {timeout_secs} : how long will the subscriber wait for published message.",
        "  {message}      : message body.",
        "                   remaining arguments are used as message body to publish.",
        "",
        "  Example:",
        "  [hedwig: (standalone) 1] pubsub TOPIC SUBID 10 TEST_MESSAGS",
        "",
        "  1) hw will subscribe topic TOPIC as subscriber SUBID;",
        "  2) subscriber SUBID will wait a message until 10 seconds;",
        "  3) hw publishes TEST_MESSAGES to topic TOPIC;",
        "  4) if subscriber recevied message in 10 secs, it checked that whether the message is published message.",
        "     if true, it will return SUCCESS, otherwise return FAILED.",
    };

    //
    // List all commands used to admin hedwig
    //

    /* SHOW: list all available hub servers or topics */
    static final String SHOW = "show";
    static final String SHOW_DESC = "list all available hub servers or topics";
    static final String[] SHOW_USAGE = new String[] {
        "usage: show [topics | hubs]",
        "",
        "  show topics :",
        "    listing all available topics in hedwig.",
        "",
        "  show hubs :",
        "    listing all available hubs in hedwig.",
        "",
        "  NOTES:",
        "  'show topics' will not works when there are millions of topics in hedwig, since we have packetLen limitation fetching data from zookeeper.",
    };

    static final String SHOW_TOPICS = "topics";
    static final String SHOW_HUBS   = "hubs";

    /* DESCRIBE: show the metadata of a topic */
    static final String DESCRIBE = "describe";
    static final String DESCRIBE_DESC = "show metadata of a topic, including topic owner, persistence info, subscriptions info";
    static final String[] DESCRIBE_USAGE = new String[] {
        "usage: describe topic {topic}",
        "",
        "  {topic} : topic name.",
        "            any printable string without spaces.",
        "",
        "  Example: describe topic ttttt",
        "",
        "  Output:",
        "  ===== Topic Information : ttttt =====",
        "",
        "  Owner : 98.137.99.27:9875:9876",
        "",
        "  >>> Persistence Info <<<",
        "  Ledger 54729 [ 1 ~ 59 ]",
        "  Ledger 54731 [ 60 ~ 60 ]",
        "  Ledger 54733 [ 61 ~ 61 ]",
        "",
        "  >>> Subscription Info <<<",
        "  Subscriber mysub : consumeSeqId: local:50",
    };

    static final String DESCRIBE_TOPIC = "topic";

    /* READTOPIC: read messages of a specified topic */
    static final String READTOPIC = "readtopic";
    static final String READTOPIC_DESC = "read messages of a specified topic";
    static final String[] READTOPIC_USAGE = new String[] {
        "usage: readtopic {topic} [start_msg_id]",
        "",
        "  {topic}        : topic name.",
        "                   any printable string without spaces.",
        "  [start_msg_id] : message id that start to read from.",
        "",
        "  no start_msg_id provided:",
        "    it will start from least_consumed_message_id + 1.",
        "    least_consume_message_id is computed from all its subscribers.",
        "",
        "  start_msg_id provided:",
        "    it will start from MAX(start_msg_id, least_consumed_message_id).",
        "",
        "  MESSAGE FORMAT:",
        "",
        "  ---------- MSGID=LOCAL(51) ----------",
        "  MsgId:     LOCAL(51)",
        "  SrcRegion: standalone",
        "  Message:",
        "",
        "  hello",
    };

    /* FORMAT: format metadata for Hedwig */
    static final String FORMAT = "format";
    static final String FORMAT_DESC = "format metadata for Hedwig";
    static final String[] FORMAT_USAGE = new String[] {
        "usage: format [-force]",
        "",
        "  [-force] : Format metadata for Hedwig w/o confirmation.",
    };


    //
    // List other useful commands
    //

    /* SET: set whether printing zk watches or not */
    static final String SET = "set";
    static final String SET_DESC = "set whether printing zk watches or not";
    static final String[] SET_USAGE = EMPTY_ARRAY;

    /* HISTORY: list history commands */
    static final String HISTORY = "history";
    static final String HISTORY_DESC = "list history commands";
    static final String[] HISTORY_USAGE = EMPTY_ARRAY;

    /* REDO: redo previous command */
    static final String REDO = "redo";
    static final String REDO_DESC = "redo history command";
    static final String[] REDO_USAGE = new String[] {
        "usage: redo [{cmdno} | !]",
        "",
        "  {cmdno} : history command no.",
        "  !       : last command.",
    };

    /* HELP: print usage information of a specified command */
    static final String HELP = "help";
    static final String HELP_DESC = "print usage information of a specified command";
    static final String[] HELP_USAGE = new String[] {
        "usage: help {command}",
        "",
        "  {command} : command name",
    };

    static final String QUIT = "quit";
    static final String QUIT_DESC = "exit console";
    static final String[] QUIT_USAGE = EMPTY_ARRAY;

    static final String EXIT = "exit";
    static final String EXIT_DESC = QUIT_DESC;
    static final String[] EXIT_USAGE = EMPTY_ARRAY;

    public static enum COMMAND {

        CMD_PUB (PUB, PUB_DESC, PUB_USAGE),
        CMD_SUB (SUB, SUB_DESC, SUB_USAGE),
        CMD_CLOSESUB (CLOSESUB, CLOSESUB_DESC, CLOSESUB_USAGE),
        CMD_UNSUB (UNSUB, UNSUB_DESC, UNSUB_USAGE),
        CMD_RMSUB (RMSUB, RMSUB_DESC, RMSUB_USAGE),
        CMD_CONSUME (CONSUME, CONSUME_DESC, CONSUME_USAGE),
        CMD_CONSUMETO (CONSUMETO, CONSUMETO_DESC, CONSUMETO_USAGE),
        CMD_PUBSUB (PUBSUB, PUBSUB_DESC, PUBSUB_USAGE),
        CMD_SHOW (SHOW, SHOW_DESC, SHOW_USAGE),
        CMD_DESCRIBE (DESCRIBE, DESCRIBE_DESC, DESCRIBE_USAGE),
        CMD_READTOPIC (READTOPIC, READTOPIC_DESC, READTOPIC_USAGE),
        CMD_FORMAT (FORMAT, FORMAT_DESC, FORMAT_USAGE),
        CMD_SET (SET, SET_DESC, SET_USAGE),
        CMD_HISTORY (HISTORY, HISTORY_DESC, HISTORY_USAGE),
        CMD_REDO (REDO, REDO_DESC, REDO_USAGE),
        CMD_HELP (HELP, HELP_DESC, HELP_USAGE),
        CMD_QUIT (QUIT, QUIT_DESC, QUIT_USAGE),
        CMD_EXIT (EXIT, EXIT_DESC, EXIT_USAGE),
        // sub commands
        CMD_SHOW_TOPICS (SHOW_TOPICS, "", EMPTY_ARRAY),
        CMD_SHOW_HUBS (SHOW_HUBS, "", EMPTY_ARRAY),
        CMD_DESCRIBE_TOPIC (DESCRIBE_TOPIC, "", EMPTY_ARRAY);

        COMMAND(String name, String desc, String[] usage) {
            this.name = name;
            this.desc = desc;
            this.usage = usage;
            this.subCmds = new LinkedHashMap<String, COMMAND>();
        }

        public String getName() { return name; }

        public String getDescription() { return desc; }

        public Map<String, COMMAND> getSubCommands() { return subCmds; }

        public void addSubCommand(COMMAND c) {
            this.subCmds.put(c.name, c);
        };

        public void printUsage() {
            System.err.println(name + ": " + desc);
            for(String line : usage) {
                System.err.println(line);
            }
            System.err.println();
        }

        protected String name;
        protected String desc;
        protected String[] usage;
        protected Map<String, COMMAND> subCmds;
    }

    static Map<String, COMMAND> commands = null;

    private static void addCommand(COMMAND c) {
        commands.put(c.getName(), c);
    }

    static synchronized void init() {
        if (commands != null) {
            return;
        }
        commands = new LinkedHashMap<String, COMMAND>();

        addCommand(COMMAND.CMD_PUB);
        addCommand(COMMAND.CMD_SUB);
        addCommand(COMMAND.CMD_CLOSESUB);
        addCommand(COMMAND.CMD_UNSUB);
        addCommand(COMMAND.CMD_RMSUB);
        addCommand(COMMAND.CMD_CONSUME);
        addCommand(COMMAND.CMD_CONSUMETO);
        addCommand(COMMAND.CMD_PUBSUB);

        // show
        COMMAND.CMD_SHOW.addSubCommand(COMMAND.CMD_SHOW_TOPICS);
        COMMAND.CMD_SHOW.addSubCommand(COMMAND.CMD_SHOW_HUBS);
        addCommand(COMMAND.CMD_SHOW);

        // describe
        COMMAND.CMD_DESCRIBE.addSubCommand(COMMAND.CMD_DESCRIBE_TOPIC);
        addCommand(COMMAND.CMD_DESCRIBE);

        addCommand(COMMAND.CMD_READTOPIC);
        addCommand(COMMAND.CMD_FORMAT);
        addCommand(COMMAND.CMD_SET);
        addCommand(COMMAND.CMD_HISTORY);
        addCommand(COMMAND.CMD_REDO);
        addCommand(COMMAND.CMD_HELP);
        addCommand(COMMAND.CMD_QUIT);
        addCommand(COMMAND.CMD_EXIT);
    }

    public static Map<String, COMMAND> getHedwigCommands() {
        return commands;
    }

    /**
     * Find candidate commands by the specified token list
     *
     * @param token token list
     *
     * @return list of candidate commands
     */
    public static List<String> findCandidateCommands(String[] tokens) {
        List<String> cmds = new LinkedList<String>();

        Map<String, COMMAND> cmdMap = commands;
        for (int i=0; i<(tokens.length - 1); i++) {
            COMMAND c = cmdMap.get(tokens[i]);
            // no commands
            if (c == null || c.getSubCommands().size() <= 0) {
                return cmds;
            } else {
                cmdMap = c.getSubCommands();
            }
        }
        cmds.addAll(cmdMap.keySet());
        return cmds;
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/admin/console/HedwigConsole.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hedwig.admin.console;

import jline.ConsoleReader;
import jline.History;
import jline.Terminal;

import java.io.BufferedReader;
import java.io.File;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.Iterator;
import java.util.LinkedHashMap;
import java.util.List;
import java.util.Map;
import java.util.NoSuchElementException;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.TimeUnit;

import org.apache.bookkeeper.util.MathUtils;
import org.apache.commons.configuration.ConfigurationException;
import org.apache.hedwig.admin.HedwigAdmin;
import org.apache.hedwig.client.api.MessageHandler;
import org.apache.hedwig.client.api.Publisher;
import org.apache.hedwig.client.api.Subscriber;
import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.HedwigClient;
import org.apache.hedwig.protocol.PubSubProtocol.LedgerRange;
import org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEvent;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions;
import org.apache.hedwig.protoextensions.SubscriptionStateUtils;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.topics.HubInfo;
import org.apache.hedwig.util.Callback;
import org.apache.hedwig.util.HedwigSocketAddress;
import org.apache.hedwig.util.SubscriptionListener;
import org.apache.zookeeper.WatchedEvent;
import org.apache.zookeeper.Watcher;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.protobuf.ByteString;

import static org.apache.hedwig.admin.console.HedwigCommands.*;
import static org.apache.hedwig.admin.console.HedwigCommands.COMMAND.*;

/**
 * Console Client to Hedwig
 */
public class HedwigConsole {
    private static final Logger LOG = LoggerFactory.getLogger(HedwigConsole.class);
    // NOTE: now it is fixed passwd in bookkeeper
    static byte[] passwd = "sillysecret".getBytes();

    // history file name
    static final String HW_HISTORY_FILE = ".hw_history";

    static final char[] CONTINUE_OR_QUIT = new char[] { 'Q', 'q', '\n' };

    protected MyCommandOptions cl = new MyCommandOptions();
    protected HashMap<Integer, String> history = new LinkedHashMap<Integer, String>();
    protected int commandCount = 0;
    protected boolean printWatches = true;
    protected Map<String, MyCommand> myCommands;

    protected boolean inConsole = true;
    protected ConsoleReader console = null;

    protected HedwigAdmin admin;
    protected HedwigClient hubClient;
    protected Publisher publisher;
    protected Subscriber subscriber;
    protected ConsoleMessageHandler consoleHandler =
            new ConsoleMessageHandler();
    protected Terminal terminal;

    protected String myRegion;

    interface MyCommand {
        boolean runCmd(String[] args) throws Exception;
    }

    static class HelpCmd implements MyCommand {

        @Override
        public boolean runCmd(String[] args) throws Exception {
            boolean printUsage = true;
            if (args.length >= 2) {
                String command = args[1];
                COMMAND c = getHedwigCommands().get(command);
                if (c != null) {
                    c.printUsage();
                    printUsage = false;
                }
            }
            if (printUsage) {
                usage();
            }
            return true;
        }
    }

    class ExitCmd implements MyCommand {

        @Override
        public boolean runCmd(String[] args) throws Exception {
            printMessage("Quitting ...");
            hubClient.close();
            admin.close();
            Runtime.getRuntime().exit(0);
            return true;
        }
    }

    class RedoCmd implements MyCommand {

        @Override
        public boolean runCmd(String[] args) throws Exception {
            if (args.length < 2) {
                return false;
            }

            int index;
            if ("!".equals(args[1])) {
                index = commandCount - 1;
            } else {
                index = Integer.decode(args[1]);
                if (commandCount <= index) {
                    System.err.println("Command index out of range");
                    return false;
                }
            }
            cl.parseCommand(history.get(index));
            if (cl.getCommand().equals("redo")) {
                System.err.println("No redoing redos");
                return false;
            }
            history.put(commandCount, history.get(index));
            processCmd(cl);
            return true;
        }
        
    }

    class HistoryCmd implements MyCommand {

        @Override
        public boolean runCmd(String[] args) throws Exception {
            for (int i=commandCount - 10; i<=commandCount; ++i) {
                if (i < 0) {
                    continue;
                }
                System.out.println(i + " - " + history.get(i));
            }
            return true;
        }
        
    }

    class SetCmd implements MyCommand {

        @Override
        public boolean runCmd(String[] args) throws Exception {
            if (args.length < 3 || !"printwatches".equals(args[1])) {
                return false;
            } else if (args.length == 2) {
                System.out.println("printwatches is " + (printWatches ? "on" : "off"));
            } else {
                printWatches = args[2].equals("on");
            }
            return true;
        }
        
    }

    class PubCmd implements MyCommand {

        @Override
        public boolean runCmd(String[] args) throws Exception {
            if (args.length < 3) {
                return false;
            }
            ByteString topic = ByteString.copyFromUtf8(args[1]);

            StringBuilder sb = new StringBuilder();
            for (int i=2; i<args.length; i++) {
                sb.append(args[i]);
                if (i != args.length - 1) {
                    sb.append(' ');
                }
            }
            ByteString msgBody = ByteString.copyFromUtf8(sb.toString());
            Message msg = Message.newBuilder().setBody(msgBody).build();
            try {
                publisher.publish(topic, msg);
                System.out.println("PUB DONE");
            } catch (Exception e) {
                System.err.println("PUB FAILED");
                e.printStackTrace();
            }
            return true;
        }
        
    }

    static class ConsoleMessageHandler implements MessageHandler {

        @Override
        public void deliver(ByteString topic, ByteString subscriberId,
                Message msg, Callback<Void> callback, Object context) {
            System.out.println("Received message from topic " + topic.toStringUtf8() + 
                    " for subscriber " + subscriberId.toStringUtf8() + " : "
                    + msg.getBody().toStringUtf8());
            callback.operationFinished(context, null);
        }
        
    }

    static class ConsoleSubscriptionListener implements SubscriptionListener {

        @Override
        public void processEvent(ByteString t, ByteString s, SubscriptionEvent event) {
            System.out.println("Subscription Channel for (topic:" + t.toStringUtf8() + ", subscriber:"
                                + s.toStringUtf8() + ") received event : " + event);
        }
    }

    class SubCmd implements MyCommand {

        @Override
        public boolean runCmd(String[] args) throws Exception {
            CreateOrAttach mode;
            boolean receive = true;
            if (args.length < 3) {
                return false;
            } else if (args.length == 3) {
                mode = CreateOrAttach.ATTACH;
                receive = true;
            } else {
                try {
                    mode = CreateOrAttach.valueOf(Integer.parseInt(args[3]));
                } catch (Exception e) {
                    System.err.println("Unknow mode : " + args[3]);
                    return false;
                }
                if (args.length >= 5) {
                    try {
                        receive = Boolean.parseBoolean(args[4]);
                    } catch (Exception e) {
                        receive = false;
                    }
                }
            }
            if (mode == null) {
                System.err.println("Unknow mode : " + args[3]);
                return false;
            }
            ByteString topic = ByteString.copyFromUtf8(args[1]);
            ByteString subId = ByteString.copyFromUtf8(args[2]);
            try {
                SubscriptionOptions options =
                    SubscriptionOptions.newBuilder().setCreateOrAttach(mode)
                                       .setForceAttach(false).build();
                subscriber.subscribe(topic, subId, options);
                if (receive) {
                    subscriber.startDelivery(topic, subId, consoleHandler);
                    System.out.println("SUB DONE AND RECEIVE");
                } else {
                    System.out.println("SUB DONE BUT NOT RECEIVE");
                }
            } catch (Exception e) {
                System.err.println("SUB FAILED");
                e.printStackTrace();
            }
            return true;
        }
    }

    class UnsubCmd implements MyCommand {

        @Override
        public boolean runCmd(String[] args) throws Exception {
            if (args.length < 3) {
                return false;
            }
            ByteString topic = ByteString.copyFromUtf8(args[1]);
            ByteString subId = ByteString.copyFromUtf8(args[2]);
            try {
                subscriber.stopDelivery(topic, subId);
                subscriber.unsubscribe(topic, subId);
                System.out.println("UNSUB DONE");
            } catch (Exception e) {
                System.err.println("UNSUB FAILED");
                e.printStackTrace();
            }
            return true;
        }
        
    }

    class RmsubCmd implements MyCommand {

        @Override
        public boolean runCmd(String[] args) throws Exception {
            if (args.length < 7) {
                return false;
            }
            String topicPrefix = args[1];
            int startTopic = Integer.parseInt(args[2]);
            int endTopic = Integer.parseInt(args[3]);
            String subPrefix = args[4];
            int startSub = Integer.parseInt(args[5]);
            int endSub = Integer.parseInt(args[6]);
            if (startTopic > endTopic || endSub < startSub) {
                return false;
            }
            for (int i=startTopic; i<=endTopic; i++) {
                ByteString topic = ByteString.copyFromUtf8(topicPrefix + i);
                try {
                    for (int j=startSub; j<=endSub; j++) {
                        ByteString sub = ByteString.copyFromUtf8(subPrefix + j);
                        subscriber.subscribe(topic, sub, CreateOrAttach.CREATE_OR_ATTACH);
                        subscriber.unsubscribe(topic, sub);
                    }
                    System.out.println("RMSUB " + topic.toStringUtf8() + " DONE");
                } catch (Exception e) {
                    System.err.println("RMSUB " + topic.toStringUtf8() + " FAILED");
                    e.printStackTrace();
                }
            }
            return true;
        }

    }
    
    class CloseSubscriptionCmd implements MyCommand {

        @Override
        public boolean runCmd(String[] args) throws Exception {
            if (args.length < 3) {
                return false;
            }
            ByteString topic = ByteString.copyFromUtf8(args[1]);
            ByteString sudId = ByteString.copyFromUtf8(args[2]);
            
            try {
                subscriber.stopDelivery(topic, sudId);
                subscriber.closeSubscription(topic, sudId);
            } catch (Exception e) {
                System.err.println("CLOSESUB FAILED");
            }
            return true;
        }
        
    }
    
    class ConsumeToCmd implements MyCommand {

        @Override
        public boolean runCmd(String[] args) throws Exception {
            if (args.length < 4) {
                return false;
            }
            ByteString topic = ByteString.copyFromUtf8(args[1]);
            ByteString subId = ByteString.copyFromUtf8(args[2]);
            long msgId = Long.parseLong(args[3]);
            MessageSeqId consumeId = MessageSeqId.newBuilder().setLocalComponent(msgId).build();
            try {
                subscriber.consume(topic, subId, consumeId);
            } catch (Exception e) {
                System.err.println("CONSUMETO FAILED");
            }
            return true;
        }
        
    }
    
    class ConsumeCmd implements MyCommand {

        @Override
        public boolean runCmd(String[] args) throws Exception {
            if (args.length < 4) {
                return false;
            }
            long lastConsumedId = 0;
            SubscriptionData subData = admin.getSubscription(ByteString.copyFromUtf8(args[1]),
                                                             ByteString.copyFromUtf8(args[2]));
            if (null == subData) {
                System.err.println("Failed to read subscription for topic: " + args[1]
                                 + " subscriber: " + args[2]);
                return true;
            }
            lastConsumedId = subData.getState().getMsgId().getLocalComponent();
            long numMessagesToConsume = Long.parseLong(args[3]);
            long idToConsumed = lastConsumedId + numMessagesToConsume;
            System.out.println("Try to move subscriber(" + args[2] + ") consume ptr of topic(" + args[1]
                             + ") from " + lastConsumedId + " to " + idToConsumed);
            MessageSeqId consumeId = MessageSeqId.newBuilder().setLocalComponent(idToConsumed).build();
            ByteString topic = ByteString.copyFromUtf8(args[1]);
            ByteString subId = ByteString.copyFromUtf8(args[2]);
            try {
                subscriber.consume(topic, subId, consumeId);
            } catch (Exception e) {
                System.err.println("CONSUME FAILED");
            }
            return true;
        }
        
    }

    class PubSubCmd implements MyCommand {

        @Override
        public boolean runCmd(String[] args) throws Exception {
            if (args.length < 5) {
                return false;
            }
            final long startTime = MathUtils.now();

            final ByteString topic = ByteString.copyFromUtf8(args[1]);
            final ByteString subId = ByteString.copyFromUtf8(args[2] + "-" + startTime);
            int timeoutSecs = 60;
            try {
                timeoutSecs = Integer.parseInt(args[3]);
            } catch (NumberFormatException nfe) {
            }

            StringBuilder sb = new StringBuilder();
            for (int i=4; i<args.length; i++) {
                sb.append(args[i]);
                if (i != args.length - 1) {
                    sb.append(' ');
                }
            }
            // append a timestamp tag
            ByteString msgBody = ByteString.copyFromUtf8(sb.toString() + "-" + startTime);
            final Message msg = Message.newBuilder().setBody(msgBody).build();

            boolean subscribed = false;
            boolean success = false;
            final CountDownLatch isDone = new CountDownLatch(1);
            long elapsedTime = 0L;

            System.out.println("Starting PUBSUB test ...");
            try {
                // sub the topic
                subscriber.subscribe(topic, subId, CreateOrAttach.CREATE_OR_ATTACH);
                subscribed = true;

                System.out.println("Sub topic " + topic.toStringUtf8() + ", subscriber id " + subId.toStringUtf8());

                

                // pub topic
                publisher.publish(topic, msg);
                System.out.println("Pub topic " + topic.toStringUtf8() + " : " + msg.getBody().toStringUtf8());

                // ensure subscriber first, publish next, then we start delivery to receive message
                // if start delivery first before publish, isDone may notify before wait
                subscriber.startDelivery(topic, subId, new MessageHandler() {

                    @Override
                    public void deliver(ByteString thisTopic, ByteString subscriberId,
                            Message message, Callback<Void> callback, Object context) {
                        if (thisTopic.equals(topic) && subscriberId.equals(subId) &&
                            msg.getBody().equals(message.getBody())) {
                            System.out.println("Received message : " + message.getBody().toStringUtf8());
                            isDone.countDown();
                        }
                        callback.operationFinished(context, null);
                    }

                });

                // wait for the message
                success = isDone.await(timeoutSecs, TimeUnit.SECONDS);
                elapsedTime = MathUtils.now() - startTime;
            } finally {
                try {
                    if (subscribed) {
                        subscriber.stopDelivery(topic, subId);
                        subscriber.unsubscribe(topic, subId);
                    }
                } finally {
                    if (success) {
                        System.out.println("PUBSUB SUCCESS. TIME: " + elapsedTime + " MS");
                    } else {
                        System.out.println("PUBSUB FAILED. ");
                    }
                    return success;
                }
            }
        }

    }
    
    class ReadTopicCmd implements MyCommand {

        @Override
        public boolean runCmd(String[] args) throws Exception {
            if (args.length < 2) {
                return false;
            }
            ReadTopic rt;
            ByteString topic = ByteString.copyFromUtf8(args[1]);
            if (args.length == 2) {
                rt = new ReadTopic(admin, topic, inConsole);
            } else {
                rt = new ReadTopic(admin, topic, Long.parseLong(args[2]), inConsole);
            }
            rt.readTopic();
            return true;
        }
        
    }

    class ShowCmd implements MyCommand {

        static final int MAX_TOPICS_PER_SHOW = 100;

        @Override
        public boolean runCmd(String[] args) throws Exception {
            if (args.length < 2) {
                return false;
            }
            String errorMsg = null;
            try {
                if (HedwigCommands.SHOW_HUBS.equals(args[1])) {
                    errorMsg = "Unable to fetch the list of hub servers";
                    showHubs();
                } else if (HedwigCommands.SHOW_TOPICS.equals(args[1])) {
                    errorMsg = "Unable to fetch the list of topics";
                    showTopics();
                } else {
                    System.err.println("ERROR: Unknown show command '" + args[1] + "'");
                    return false;
                }
            } catch (Exception e) {
                if (null != errorMsg) {
                    System.err.println(errorMsg);
                }
                e.printStackTrace();
            }
            return true;
        }

        protected void showHubs() throws Exception {
            Map<HedwigSocketAddress, HedwigAdmin.HubStats> hubs = admin.getAvailableHubs();
            System.out.println("Available Hub Servers:");
            for (Map.Entry<HedwigSocketAddress, HedwigAdmin.HubStats> entry : hubs.entrySet()) {
                System.out.println("\t" + entry.getKey() + " :\t" + entry.getValue());
            }
        }

        protected void showTopics() throws Exception {
            List<String> topics = new ArrayList<String>();
            Iterator<ByteString> iter = admin.getTopics();

            System.out.println("Topic List:");
            boolean stop = false;
            while (iter.hasNext()) {
                if (topics.size() >= MAX_TOPICS_PER_SHOW) {
                    System.out.println(topics);
                    topics.clear();
                    stop = !continueOrQuit();
                    if (stop) {
                        break;
                    }
                }
                ByteString t = iter.next();
                topics.add(t.toStringUtf8());
            }
            if (!stop) {
                System.out.println(topics);
            }
        }

        
        
    }

    class DescribeCmd implements MyCommand {

        @Override
        public boolean runCmd(String[] args) throws Exception {
            if (args.length < 3) {
                return false;
            }
            if (HedwigCommands.DESCRIBE_TOPIC.equals(args[1])) {
                return describeTopic(args[2]);
            } else {
                return false;
            }
        }

        protected boolean describeTopic(String topic) throws Exception {
            ByteString btopic = ByteString.copyFromUtf8(topic);
            HubInfo owner = admin.getTopicOwner(btopic);
            List<LedgerRange> ranges = admin.getTopicLedgers(btopic);
            Map<ByteString, SubscriptionData> states = admin.getTopicSubscriptions(btopic);

            System.out.println("===== Topic Information : " + topic + " =====");
            System.out.println();
            System.out.println("Owner : " + (owner == null ? "NULL" :
                               owner.toString().trim().replaceAll("\n", ", ")));
            System.out.println();

            // print ledgers
            printTopicLedgers(ranges);
            // print subscriptions
            printTopicSubscriptions(states);

            return true;
        }

        private void printTopicLedgers(List<LedgerRange> ranges) {
            System.out.println(">>> Persistence Info <<<");
            if (null == ranges) {
                System.out.println("N/A");
                return;
            }
            if (ranges.isEmpty()) {
                System.out.println("No Ledger used.");
                return;
            }
            for (LedgerRange range : ranges) {
                System.out.println("Ledger " + range.getLedgerId() + " [ "
                                   + range.getStartSeqIdIncluded() + " ~ "
                                   + range.getEndSeqIdIncluded().getLocalComponent() + " ]");
            }
            System.out.println();
        }

        private void printTopicSubscriptions(Map<ByteString, SubscriptionData> states) {
            System.out.println(">>> Subscription Info <<<");
            if (0 == states.size()) {
                System.out.println("No subscriber.");
                return;
            }
            for (Map.Entry<ByteString, SubscriptionData> entry : states.entrySet()) {
                System.out.println("Subscriber " + entry.getKey().toStringUtf8() + " : "
                                 + SubscriptionStateUtils.toString(entry.getValue()));
            }
            System.out.println();
        }

    }

    class FormatCmd implements MyCommand {

        @Override
        public boolean runCmd(String[] args) throws Exception {
            boolean force = false;
            if (args.length >= 2 && "-force".equals(args[1])) {
                force = true;
            }
            boolean doFormat = true;
            System.out.println("You ask to format hedwig metadata stored in "
                               + admin.getMetadataManagerFactory().getClass().getName() + ".");
            if (!force) {
                doFormat = continueOrQuit();
            }
            if (doFormat) {
                admin.format();
                System.out.println("Formatted hedwig metadata successfully.");
            } else {
                System.out.println("Given up formatting hedwig metadata.");
            }
            return true;
        }

    }

    protected Map<String, MyCommand> buildMyCommands() {
        Map<String, MyCommand> cmds =
                new HashMap<String, MyCommand>();

        ExitCmd exitCmd = new ExitCmd();
        cmds.put(EXIT, exitCmd);
        cmds.put(QUIT, exitCmd);
        cmds.put(HELP, new HelpCmd());
        cmds.put(HISTORY, new HistoryCmd());
        cmds.put(REDO, new RedoCmd());
        cmds.put(SET, new SetCmd());
        cmds.put(PUB, new PubCmd());
        cmds.put(SUB, new SubCmd());
        cmds.put(PUBSUB, new PubSubCmd());
        cmds.put(CLOSESUB, new CloseSubscriptionCmd());
        cmds.put(UNSUB, new UnsubCmd());
        cmds.put(RMSUB, new RmsubCmd());
        cmds.put(CONSUME, new ConsumeCmd());
        cmds.put(CONSUMETO, new ConsumeToCmd());
        cmds.put(SHOW, new ShowCmd());
        cmds.put(DESCRIBE, new DescribeCmd());
        cmds.put(READTOPIC, new ReadTopicCmd());
        cmds.put(FORMAT, new FormatCmd());

        return cmds;
    }

    static void usage() {
        System.err.println("HedwigConsole [options] [command] [args]");
        System.err.println();
        System.err.println("Avaiable commands:");
        for (String cmd : getHedwigCommands().keySet()) {
            System.err.println("\t" + cmd);
        }
        System.err.println();
    }

    /**
     * A storage class for both command line options and shell commands.
     */
    static private class MyCommandOptions {

        private Map<String,String> options = new HashMap<String,String>();
        private List<String> cmdArgs = null;
        private String command = null;

        public MyCommandOptions() {
        }

        public String getOption(String opt) {
            return options.get(opt);
        }

        public String getCommand( ) {
            return command;
        }

        public String getCmdArgument( int index ) {
            return cmdArgs.get(index);
        }

        public int getNumArguments( ) {
            return cmdArgs.size();
        }

        public String[] getArgArray() {
            return cmdArgs.toArray(new String[0]);
        }

        /**
         * Parses a command line that may contain one or more flags
         * before an optional command string
         * @param args command line arguments
         * @return true if parsing succeeded, false otherwise.
         */
        public boolean parseOptions(String[] args) {
            List<String> argList = Arrays.asList(args);
            Iterator<String> it = argList.iterator();

            while (it.hasNext()) {
                String opt = it.next();
                if (!opt.startsWith("-")) {
                    command = opt;
                    cmdArgs = new ArrayList<String>( );
                    cmdArgs.add( command );
                    while (it.hasNext()) {
                        cmdArgs.add(it.next());
                    }
                    return true;
                } else {
                    try {
                        options.put(opt.substring(1), it.next());
                    } catch (NoSuchElementException e) {
                        System.err.println("Error: no argument found for option "
                                + opt);
                        return false;
                    }
                }
            }
            return true;
        }

        /**
         * Breaks a string into command + arguments.
         * @param cmdstring string of form "cmd arg1 arg2..etc"
         * @return true if parsing succeeded.
         */
        public boolean parseCommand( String cmdstring ) {
            String[] args = cmdstring.split(" ");
            if (args.length == 0){
                return false;
            }
            command = args[0];
            cmdArgs = Arrays.asList(args);
            return true;
        }
    }

    private class MyWatcher implements Watcher {
        public void process(WatchedEvent event) {
            if (getPrintWatches()) {
                printMessage("WATCHER::");
                printMessage(event.toString());
            }
        }
    }

    public void printMessage(String msg) {
        if (inConsole) {
            System.out.println("\n"+msg);
        }
    }

    /**
     * Hedwig Console
     *
     * @param args arguments
     * @throws IOException
     * @throws InterruptedException 
     */
    public HedwigConsole(String[] args) throws IOException, InterruptedException {
        // Setup Terminal
        terminal = Terminal.setupTerminal();
        HedwigCommands.init();
        cl.parseOptions(args);

        if (cl.getCommand() == null) {
            inConsole = true;
        } else {
            inConsole = false;
        }

        org.apache.bookkeeper.conf.ClientConfiguration bkClientConf =
            new org.apache.bookkeeper.conf.ClientConfiguration();
        ServerConfiguration hubServerConf = new ServerConfiguration();
        String serverCfgFile = cl.getOption("server-cfg");
        if (serverCfgFile != null) {
            try {
                hubServerConf.loadConf(new File(serverCfgFile).toURI().toURL());
            } catch (ConfigurationException e) {
                throw new IOException(e);
            }
            try {
                bkClientConf.loadConf(new File(serverCfgFile).toURI().toURL());
            } catch (ConfigurationException e) {
                throw new IOException(e);
            }
        }

        ClientConfiguration hubClientCfg = new ClientConfiguration();
        String clientCfgFile = cl.getOption("client-cfg");
        if (clientCfgFile != null) {
            try {
                hubClientCfg.loadConf(new File(clientCfgFile).toURI().toURL());
            } catch (ConfigurationException e) {
                throw new IOException(e);
            }
        }

        printMessage("Connecting to zookeeper/bookkeeper using HedwigAdmin");
        try {
            admin = new HedwigAdmin(bkClientConf, hubServerConf);
            admin.getZkHandle().register(new MyWatcher());
        } catch (Exception e) {
            throw new IOException(e);
        }
        
        printMessage("Connecting to default hub server " + hubClientCfg.getDefaultServerHost());
        hubClient = new HedwigClient(hubClientCfg);
        publisher = hubClient.getPublisher();
        subscriber = hubClient.getSubscriber();
        subscriber.addSubscriptionListener(new ConsoleSubscriptionListener());
        
        // other parameters
        myRegion = hubServerConf.getMyRegion();
    }

    public boolean getPrintWatches() {
        return printWatches;
    }

    protected String getPrompt() {
        StringBuilder sb = new StringBuilder();
        sb.append("[hedwig: (").append(myRegion).append(") ").append(commandCount).append("] ");
        return sb.toString();
    }

    protected boolean continueOrQuit() throws IOException {
        System.out.println("Press <Return> to continue, or Q to cancel ...");
        int ch;
        if (null != console) {
            ch = console.readCharacter(CONTINUE_OR_QUIT);
        } else {
            do {
                ch = terminal.readCharacter(System.in);
            } while (ch != 'q' && ch != 'Q' && ch != '\n');
        }
        if (ch == 'q' ||
            ch == 'Q') {
            return false;
        }
        return true;
    }

    protected void addToHistory(int i, String cmd) {
        history.put(i, cmd);
    }

    public void executeLine(String line) {
        if (!line.equals("")) {
            cl.parseCommand(line);
            addToHistory(commandCount, line);
            processCmd(cl);
            commandCount++;
        }
    }

    protected boolean processCmd(MyCommandOptions co) {
        String[] args = co.getArgArray();
        String cmd = co.getCommand();
        if (args.length < 1) {
            usage();
            return false;
        }
        if (!getHedwigCommands().containsKey(cmd)) {
            usage();
            return false;
        }

        LOG.debug("Processing {}", cmd);

        MyCommand myCommand = myCommands.get(cmd);
        if (myCommand == null) {
            System.err.println("No Command Processor found for command " + cmd);
            usage();
            return false;
        }

        long startTime = MathUtils.now();
        boolean success = false;
        try {
            success = myCommand.runCmd(args);
        } catch (Exception e) {
            e.printStackTrace();
            success = false;
        }
        long elapsedTime = MathUtils.now() - startTime;
        if (inConsole) {
            if (success) {
                System.out.println("Finished " + ((double)elapsedTime / 1000) + " s.");
            } else {
                COMMAND c = getHedwigCommands().get(cmd);
                if (c != null) {
                    c.printUsage();
                }
            }
        }
        return success;
    }

    @SuppressWarnings("unchecked")
    void run() throws IOException {
        inConsole = true;
        myCommands = buildMyCommands();
        if (cl.getCommand() == null) {
            System.out.println("Welcome to Hedwig!");
            System.out.println("JLine support is enabled");

            console = new ConsoleReader();
            JLineHedwigCompletor completor = new JLineHedwigCompletor(admin);
            console.addCompletor(completor);

            // load history file
            History history = new History();
            File file = new File(System.getProperty("hw.history",
                                 new File(System.getProperty("user.home"), HW_HISTORY_FILE).toString()));
            if (LOG.isDebugEnabled()) {
                LOG.debug("History file is " + file.toString());
            }
            history.setHistoryFile(file);
            // set history to console reader
            console.setHistory(history);
            // load history from history file
            history.moveToFirstEntry();

            while (history.next()) {
                String entry = history.current();
                if (!entry.equals("")) {
                    addToHistory(commandCount, entry);
                }
                commandCount++;
            }
            System.out.println("JLine history support is enabled");

            String line;
            while ((line = console.readLine(getPrompt())) != null) {
                executeLine(line);
                history.addToHistory(line);
            }
        }

        inConsole = false;
        processCmd(cl);
        try {
            myCommands.get(EXIT).runCmd(new String[0]);
        } catch (Exception e) {
        }
    }

    public static void main(String[] args) throws IOException, InterruptedException {
        HedwigConsole console = new HedwigConsole(args);
        console.run();
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/admin/console/JLineHedwigCompletor.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hedwig.admin.console;

import java.util.Iterator;
import java.util.List;

import org.apache.zookeeper.KeeperException;
import org.apache.hedwig.admin.HedwigAdmin;

import com.google.protobuf.ByteString;

import jline.Completor;

import static org.apache.hedwig.admin.console.HedwigCommands.*;

/**
 * A jline completor for hedwig console
 */
public class JLineHedwigCompletor implements Completor {
    // for topic completion
    static final int MAX_TOPICS_TO_SEARCH = 1000;

    private HedwigAdmin admin;

    public JLineHedwigCompletor(HedwigAdmin admin) {
        this.admin = admin;
    }

    @Override
    public int complete(String buffer, int cursor, List candidates) {
        // Guarantee that the final token is the one we're expanding
        buffer = buffer.substring(0,cursor);
        String[] tokens = buffer.split(" ");
        if (buffer.endsWith(" ")) {
            String[] newTokens = new String[tokens.length + 1];
            System.arraycopy(tokens, 0, newTokens, 0, tokens.length);
            newTokens[newTokens.length - 1] = "";
            tokens = newTokens;
        }
        
        if (tokens.length > 2 &&
            DESCRIBE.equalsIgnoreCase(tokens[0]) &&
            DESCRIBE_TOPIC.equalsIgnoreCase(tokens[1])) {
            return completeTopic(buffer, tokens[2], candidates);
        } else if (tokens.length > 1 &&
                   (SUB.equalsIgnoreCase(tokens[0]) ||
                    PUB.equalsIgnoreCase(tokens[0]) ||
                    CLOSESUB.equalsIgnoreCase(tokens[0]) ||
                    CONSUME.equalsIgnoreCase(tokens[0]) ||
                    CONSUMETO.equalsIgnoreCase(tokens[0]) ||
                    READTOPIC.equalsIgnoreCase(tokens[0]))) {
            return completeTopic(buffer, tokens[1], candidates);
        }
        List<String> cmds = HedwigCommands.findCandidateCommands(tokens);
        return completeCommand(buffer, tokens[tokens.length - 1], cmds, candidates);
    }

    private int completeCommand(String buffer, String token,
            List<String> commands, List<String> candidates) {
        for (String cmd : commands) {
            if (cmd.startsWith(token)) {
                candidates.add(cmd);
            }
        }
        return buffer.lastIndexOf(" ") + 1;
    }

    private int completeTopic(String buffer, String token, List<String> candidates) {
        try {
            Iterator<ByteString> children = admin.getTopics();
            int i = 0;
            while (children.hasNext() && i <= MAX_TOPICS_TO_SEARCH) {
                String child = children.next().toStringUtf8();
                if (child.startsWith(token)) {
                    candidates.add(child);
                }
                ++i;
            }
        } catch (Exception e) {
            return buffer.length();
        }
        return candidates.size() == 0 ? buffer.length() : buffer.lastIndexOf(" ") + 1;
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/admin/console/ReadTopic.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hedwig.admin.console;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.ArrayList;
import java.util.Enumeration;
import java.util.Iterator;
import java.util.List;
import java.util.Map;

import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.client.BookKeeper;
import org.apache.bookkeeper.client.BookKeeper.DigestType;
import org.apache.bookkeeper.client.LedgerEntry;
import org.apache.bookkeeper.client.LedgerHandle;
import org.apache.hedwig.admin.HedwigAdmin;
import org.apache.hedwig.protocol.PubSubProtocol.LedgerRange;
import org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.ZooKeeper;
import org.apache.zookeeper.data.Stat;

import com.google.protobuf.ByteString;
import com.google.protobuf.InvalidProtocolBufferException;

/**
 * A tool to read topic messages.
 *
 * This tool :
 * 1) read persistence info from zookeeper: ledger ranges
 * 2) read subscription infor from zookeeper: we can know the least message id (ledger id) 
 * 3) use bk client to read message starting from least message id
 */
public class ReadTopic {
    
    final HedwigAdmin admin;
    final ByteString topic;
    long startSeqId;
    long leastConsumedSeqId = Long.MAX_VALUE;
    final boolean inConsole;

    static final int RC_OK = 0;
    static final int RC_ERROR = -1;
    static final int RC_NOTOPIC = -2;
    static final int RC_NOLEDGERS = -3;
    static final int RC_NOSUBSCRIBERS = -4;
    
    static final int NUM_MESSAGES_TO_PRINT = 15;

    List<LedgerRange> ledgers = new ArrayList<LedgerRange>();
    
    /**
     * Constructor
     */
    public ReadTopic(HedwigAdmin admin, ByteString topic, boolean inConsole) {
        this(admin, topic, 1, inConsole);
    }

    /**
     * Constructor
     */
    public ReadTopic(HedwigAdmin admin, ByteString topic, long msgSeqId, boolean inConsole) {
        this.admin = admin;
        this.topic = topic;
        this.startSeqId = msgSeqId;
        this.inConsole = inConsole;
    }
    
    /**
     * Check whether the topic existed or not
     *
     * @return RC_OK if topic is existed; RC_NOTOPIC if not.
     * @throws Exception
     */
    protected int checkTopic() throws Exception {
        return admin.hasTopic(topic) ? RC_OK : RC_NOTOPIC;
    }
    
    /**
     * Get the ledgers used by this topic to store messages
     *
     * @return RC_OK if topic has messages; RC_NOLEDGERS if not.
     * @throws Exception
     */
    protected int getTopicLedgers() throws Exception {
        List<LedgerRange> ranges = admin.getTopicLedgers(topic); 
        if (null == ranges || ranges.isEmpty()) {
            return RC_NOLEDGERS;
        }
        ledgers.addAll(ranges);
        return RC_OK;
    }
    
    protected int getLeastSubscription() throws Exception {
        Map<ByteString, SubscriptionData> states = admin.getTopicSubscriptions(topic); 
        if (states.isEmpty()) {
            return RC_NOSUBSCRIBERS;
        }
        for (Map.Entry<ByteString, SubscriptionData> entry : states.entrySet()) {
            SubscriptionData state = entry.getValue();
            long localMsgId = state.getState().getMsgId().getLocalComponent();
            if (localMsgId < leastConsumedSeqId) {
                leastConsumedSeqId = localMsgId;
            }
        }
        if (leastConsumedSeqId == Long.MAX_VALUE) {
            leastConsumedSeqId = 0;
        }
        return RC_OK;
    }
    
    public void readTopic() {
        try {
            int rc = _readTopic();
            switch (rc) {
            case RC_NOTOPIC:
                System.err.println("No topic " + topic + " found.");
                break;
            case RC_NOLEDGERS:
                System.err.println("No message is published to topic " + topic);
                break;
            default:
                break;
            }
        } catch (Exception e) {
            System.err.println("ERROR: read messages of topic " + topic + " failed.");
            e.printStackTrace();
        }
    }
    
    protected int _readTopic() throws Exception {
        int rc;
        // check topic
        rc = checkTopic();
        if (RC_OK != rc) {
            return rc;
        }
        // get topic ledgers
        rc = getTopicLedgers();
        if (RC_OK != rc) {
            return rc;
        }
        // get topic subscription to find the least one
        rc = getLeastSubscription();
        if (RC_NOSUBSCRIBERS == rc) {
            startSeqId = 1;
        } else if (RC_OK == rc) {
            if (leastConsumedSeqId > startSeqId) {
                startSeqId = leastConsumedSeqId + 1;
            }
        } else {
            return rc;
        }

        for (LedgerRange range : ledgers) {
            long endSeqId = range.getEndSeqIdIncluded().getLocalComponent();
            if (endSeqId < startSeqId) {
                continue;
            }
            boolean toContinue = readLedger(range);
            startSeqId = endSeqId + 1;
            if (!toContinue) {
                break;
            }
        }
        
        return RC_OK;
    }
    
    /**
     * Read a specific ledger
     *
     * @param ledger in memory ledger range
     * @param endSeqId end seq id
     * @return true if continue, otherwise false
     * @throws BKException
     * @throws IOException
     * @throws InterruptedException
     */
    protected boolean readLedger(LedgerRange ledger)
    throws BKException, IOException, InterruptedException {
        long tEndSeqId = ledger.getEndSeqIdIncluded().getLocalComponent();

        if (tEndSeqId < this.startSeqId) {
            return true;
        }
        // Open Ledger Handle
        long ledgerId = ledger.getLedgerId();
        System.out.println("\n>>>>> " + ledger + " <<<<<\n");
        LedgerHandle lh = null;
        try {
            lh = admin.getBkHandle().openLedgerNoRecovery(ledgerId, admin.getBkDigestType(), admin.getBkPasswd());
        } catch (BKException e) {
            System.err.println("ERROR: No ledger " + ledgerId + " found. maybe garbage collected due to the messages are consumed.");
        }
        if (null == lh) {
            return true;
        }
        long expectedEntryId = startSeqId - ledger.getStartSeqIdIncluded();
        
        long correctedEndSeqId = tEndSeqId;
        try {
            while (startSeqId <= tEndSeqId) {
                correctedEndSeqId = Math.min(startSeqId + NUM_MESSAGES_TO_PRINT - 1, tEndSeqId);
                
                try {
                    Enumeration<LedgerEntry> seq =
                        lh.readEntries(startSeqId - ledger.getStartSeqIdIncluded(),
                                       correctedEndSeqId - ledger.getStartSeqIdIncluded());
                    LedgerEntry entry = null;
                    while (seq.hasMoreElements()) {
                        entry = seq.nextElement();
                        Message message;
                        try {
                            message = Message.parseFrom(entry.getEntryInputStream());
                        } catch (IOException e) {
                            System.out.println("WARN: Unreadable message found\n");
                            expectedEntryId++;
                            continue;
                        }
                        if (expectedEntryId != entry.getEntryId()
                            || (message.getMsgId().getLocalComponent() - ledger.getStartSeqIdIncluded()) != expectedEntryId) {
                            throw new IOException("ERROR: Message ids are out of order : expected entry id " + expectedEntryId
                                                + ", current entry id " + entry.getEntryId() + ", msg seq id " + message.getMsgId().getLocalComponent());
                        }
                        expectedEntryId++;
                        formatMessage(message);

                    }
                    startSeqId = correctedEndSeqId + 1;
                    if (inConsole) {
                        if (!pressKeyToContinue()) {
                            return false;
                        }
                    }
                } catch (BKException.BKReadException be) {
                    throw be;
                }
            }
        } catch (BKException bke) {
            if (tEndSeqId != Long.MAX_VALUE) {
                System.err.println("ERROR: ledger " + ledgerId + " may be corrupted, since read messages ["
                                 + startSeqId + " ~ " + correctedEndSeqId + " ] failed :");
                throw bke;
            }
        }
        System.out.println("\n");
        return true;
    }
    
    protected void formatMessage(Message message) {
        // print msg id
        String msgId;
        if (!message.hasMsgId()) {
            msgId = "N/A";
        } else {
            MessageSeqId seqId = message.getMsgId();
            StringBuilder idBuilder = new StringBuilder();
            if (seqId.hasLocalComponent()) {
                idBuilder.append("LOCAL(").append(seqId.getLocalComponent()).append(")");
            } else {
                List<RegionSpecificSeqId> remoteIds = seqId.getRemoteComponentsList();
                int i = 0, numRegions = remoteIds.size();
                idBuilder.append("REMOTE(");
                for (RegionSpecificSeqId rssid : remoteIds) {
                    idBuilder.append(rssid.getRegion().toStringUtf8());
                    idBuilder.append("[");
                    idBuilder.append(rssid.getSeqId());
                    idBuilder.append("]");
                    ++i;
                    if (i < numRegions) {
                        idBuilder.append(",");
                    }
                }
                idBuilder.append(")");
            }
            msgId = idBuilder.toString();
        }
        System.out.println("---------- MSGID=" + msgId + " ----------");
        System.out.println("MsgId:     " + msgId);
        // print source region
        if (message.hasSrcRegion()) {
            System.out.println("SrcRegion: " + message.getSrcRegion().toStringUtf8());
        } else {
            System.out.println("SrcRegion: N/A");
        }
        // print message body
        System.out.println("Message:");
        System.out.println();
        if (message.hasBody()) {
            System.out.println(message.getBody().toStringUtf8());
        } else {
            System.out.println("N/A");
        }
        System.out.println();
    }
    
    boolean pressKeyToContinue() throws IOException {
        System.out.println("Press Y to continue...");
        BufferedReader stdin = new BufferedReader(new InputStreamReader(System.in));
        int ch = stdin.read();
        if (ch == 'y' ||
            ch == 'Y') {
            return true;
        }
        return false;
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/data/MessageFormatter.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

package org.apache.hedwig.data;

import java.io.IOException;
import java.util.List;

import org.apache.bookkeeper.util.EntryFormatter;
import org.apache.commons.configuration.Configuration;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Format a pub sub message into a readable format.
 */
public class MessageFormatter extends EntryFormatter {
    static Logger logger = LoggerFactory.getLogger(MessageFormatter.class);

    static final String MESSAGE_PAYLOAD_FORMATTER_CLASS = "message_payload_formatter_class";

    EntryFormatter dataFormatter = EntryFormatter.STRING_FORMATTER;

    @Override
    public void setConf(Configuration conf) {
        super.setConf(conf);
        dataFormatter = EntryFormatter.newEntryFormatter(conf, MESSAGE_PAYLOAD_FORMATTER_CLASS);
    }

    @Override
    public void formatEntry(java.io.InputStream input) {
        Message message;
        try {
            message = Message.parseFrom(input);
        } catch (IOException e) {
            System.out.println("WARN: Unreadable message found\n");
            EntryFormatter.STRING_FORMATTER.formatEntry(input);
            return;
        }
        formatMessage(message);
    }

    @Override
    public void formatEntry(byte[] data) {
        Message message;
        try {
            message = Message.parseFrom(data);
        } catch (IOException e) {
            System.out.println("WARN: Unreadable message found\n");
            EntryFormatter.STRING_FORMATTER.formatEntry(data);
            return;
        }
        formatMessage(message);
    }

    void formatMessage(Message message) {
        // print msg id
        String msgId;
        if (!message.hasMsgId()) {
            msgId = "N/A";
        } else {
            MessageSeqId seqId = message.getMsgId();
            StringBuilder idBuilder = new StringBuilder();
            if (seqId.hasLocalComponent()) {
                idBuilder.append("LOCAL(").append(seqId.getLocalComponent()).append(")");
            } else {
                List<RegionSpecificSeqId> remoteIds = seqId.getRemoteComponentsList();
                int i = 0, numRegions = remoteIds.size();
                idBuilder.append("REMOTE(");
                for (RegionSpecificSeqId rssid : remoteIds) {
                    idBuilder.append(rssid.getRegion().toStringUtf8());
                    idBuilder.append("[");
                    idBuilder.append(rssid.getSeqId());
                    idBuilder.append("]");
                    ++i;
                    if (i < numRegions) {
                        idBuilder.append(",");
                    }
                }
                idBuilder.append(")");
            }
            msgId = idBuilder.toString();
        }
        System.out.println("****** MSGID=" + msgId + " ******");
        System.out.println("MessageId:      " + msgId);
        // print source region
        if (message.hasSrcRegion()) {
            System.out.println("SrcRegion:      " + message.getSrcRegion().toStringUtf8());
        } else {
            System.out.println("SrcRegion:      N/A");
        }
        // print message body
        if (message.hasBody()) {
            System.out.println("Body:");
            dataFormatter.formatEntry(message.getBody().toByteArray());
        } else {
            System.out.println("Body:           N/A");
        }
        System.out.println();
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/benchmark/AbstractBenchmark.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.benchmark;

import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.Semaphore;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicLong;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.bookkeeper.util.MathUtils;
import org.apache.hedwig.util.ConcurrencyUtils;

public abstract class AbstractBenchmark {

    static final Logger logger = LoggerFactory.getLogger(AbstractBenchmark.class);

    AtomicLong totalLatency = new AtomicLong();
    LinkedBlockingQueue<Boolean> doneSignalQueue = new LinkedBlockingQueue<Boolean>();

    abstract void doOps(int numOps) throws Exception;
    abstract void tearDown() throws Exception;

    protected class AbstractCallback {
        AtomicInteger numDone = new AtomicInteger(0);
        Semaphore outstanding;
        int numOps;
        boolean logging;

        public AbstractCallback(Semaphore outstanding, int numOps) {
            this.outstanding = outstanding;
            this.numOps = numOps;
            logging = Boolean.getBoolean("progress");
        }

        public void handle(boolean success, Object ctx) {
            outstanding.release();

            if (!success) {
                ConcurrencyUtils.put(doneSignalQueue, false);
                return;
            }

            totalLatency.addAndGet(MathUtils.now() - (Long)ctx);
            int numDoneInt = numDone.incrementAndGet();

            if (logging && numDoneInt % 10000 == 0) {
                logger.info("Finished " + numDoneInt + " ops");
            }

            if (numOps == numDoneInt) {
                ConcurrencyUtils.put(doneSignalQueue, true);
            }
        }
    }

    public void runPhase(String phase, int numOps) throws Exception {
        long startTime = MathUtils.now();

        doOps(numOps);

        if (!doneSignalQueue.take()) {
            logger.error("One or more operations failed in phase: " + phase);
            throw new RuntimeException();
        } else {
            logger.info("Phase: " + phase + " Avg latency : " + totalLatency.get() / numOps + ", tput = " + (numOps * 1000/ (MathUtils.now() - startTime)));
        }
    }





    public void run() throws Exception {

        int numWarmup = Integer.getInteger("nWarmup", 50000);
        runPhase("warmup", numWarmup);

        logger.info("Sleeping for 10 seconds");
        Thread.sleep(10000);
        //reset latency
        totalLatency.set(0);

        int numOps = Integer.getInteger("nOps", 400000);
        runPhase("real", numOps);

        tearDown();
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/benchmark/BookieBenchmark.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.benchmark;

import java.net.InetSocketAddress;
import java.nio.ByteBuffer;
import java.util.concurrent.Executors;
import java.util.concurrent.Semaphore;
import org.apache.bookkeeper.conf.ClientConfiguration;
import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.proto.BookieClient;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.apache.bookkeeper.proto.BookkeeperInternalCallbacks.WriteCallback;
import org.apache.bookkeeper.util.MathUtils;
import org.apache.bookkeeper.util.OrderedSafeExecutor;
import org.jboss.netty.buffer.ChannelBuffer;
import org.jboss.netty.buffer.ChannelBuffers;
import org.jboss.netty.channel.socket.ClientSocketChannelFactory;
import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory;

public class BookieBenchmark extends AbstractBenchmark {

    static final Logger logger = LoggerFactory.getLogger(BookkeeperBenchmark.class);

    BookieClient bkc;
    InetSocketAddress addr;
    ClientSocketChannelFactory channelFactory;
    OrderedSafeExecutor executor = new OrderedSafeExecutor(1);


    public BookieBenchmark(String bookieHostPort)  throws Exception {
        channelFactory = new NioClientSocketChannelFactory(Executors.newCachedThreadPool(), Executors.newCachedThreadPool());
        bkc = new BookieClient(new ClientConfiguration(), channelFactory, executor);
        String[] hostPort = bookieHostPort.split(":");
        addr = new InetSocketAddress(hostPort[0], Integer.parseInt(hostPort[1]));

    }


    @Override
    void doOps(final int numOps) throws Exception {
        int numOutstanding = Integer.getInteger("nPars",1000);
        final Semaphore outstanding = new Semaphore(numOutstanding);


        WriteCallback callback = new WriteCallback() {
            AbstractCallback handler = new AbstractCallback(outstanding, numOps);

            @Override
            public void writeComplete(int rc, long ledgerId, long entryId,
            InetSocketAddress addr, Object ctx) {
                handler.handle(rc == BKException.Code.OK, ctx);
            }
        };

        byte[] passwd = new byte[20];
        int size = Integer.getInteger("size", 1024);
        byte[] data = new byte[size];

        for (int i=0; i<numOps; i++) {
            outstanding.acquire();

            ByteBuffer buffer = ByteBuffer.allocate(44);
            long ledgerId = 1000;
            buffer.putLong(ledgerId);
            buffer.putLong(i);
            buffer.putLong(0);
            buffer.put(passwd);
            buffer.rewind();
            ChannelBuffer toSend = ChannelBuffers.wrappedBuffer(ChannelBuffers.wrappedBuffer(buffer.slice()), ChannelBuffers.wrappedBuffer(data));
            bkc.addEntry(addr, ledgerId, passwd, i, toSend, callback, MathUtils.now(), 0);
        }

    }

    @Override
    public void tearDown() {
        bkc.close();
        channelFactory.releaseExternalResources();
        executor.shutdown();
    }


    public static void main(String[] args) throws Exception {
        BookieBenchmark benchmark = new BookieBenchmark(args[0]);
        benchmark.run();
    }


}
"
hedwig-server/src/main/java/org/apache/hedwig/server/benchmark/BookkeeperBenchmark.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.benchmark;

import java.util.Random;
import java.util.concurrent.Semaphore;
import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.client.BookKeeper;
import org.apache.bookkeeper.client.LedgerHandle;
import org.apache.bookkeeper.client.AsyncCallback.AddCallback;
import org.apache.bookkeeper.client.BookKeeper.DigestType;
import org.apache.bookkeeper.util.MathUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class BookkeeperBenchmark extends AbstractBenchmark {

    static final Logger logger = LoggerFactory.getLogger(BookkeeperBenchmark.class);

    BookKeeper bk;
    LedgerHandle[] lh;

    public BookkeeperBenchmark(String zkHostPort) throws Exception {
        bk = new BookKeeper(zkHostPort);
        int numLedgers = Integer.getInteger("nLedgers",5);
        lh = new LedgerHandle[numLedgers];
        int quorumSize = Integer.getInteger("quorum", 2);
        int ensembleSize = Integer.getInteger("ensemble", 4);
        DigestType digestType = DigestType.valueOf(System.getProperty("digestType", "CRC32"));
        for (int i=0; i< numLedgers; i++) {
            lh[i] = bk.createLedger(ensembleSize, quorumSize, digestType, "blah".getBytes());
        }

    }


    @Override
    void doOps(final int numOps) throws Exception {
        int size = Integer.getInteger("size", 1024);
        byte[] msg = new byte[size];

        int numOutstanding = Integer.getInteger("nPars",1000);
        final Semaphore outstanding = new Semaphore(numOutstanding);

        AddCallback callback = new AddCallback() {
            AbstractCallback handler = new AbstractCallback(outstanding, numOps);


            @Override
            public void addComplete(int rc, LedgerHandle lh, long entryId, Object ctx) {
                handler.handle(rc == BKException.Code.OK, ctx);
            }

        };



        Random rand = new Random();

        for (int i=0; i<numOps; i++) {
            outstanding.acquire();
            lh[rand.nextInt(lh.length)].asyncAddEntry(msg, callback, MathUtils.now());
        }


    }

    @Override
    public void tearDown() throws Exception {
        bk.close();
    }


    public static void main(String[] args) throws Exception {
        BookkeeperBenchmark benchmark = new BookkeeperBenchmark(args[0]);
        benchmark.run();
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/benchmark/FakeBookie.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.benchmark;

import java.net.InetSocketAddress;
import java.util.concurrent.Executors;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.bootstrap.ServerBootstrap;
import org.jboss.netty.buffer.ChannelBuffer;
import org.jboss.netty.channel.ChannelHandlerContext;
import org.jboss.netty.channel.ChannelPipeline;
import org.jboss.netty.channel.ChannelPipelineCoverage;
import org.jboss.netty.channel.ChannelPipelineFactory;
import org.jboss.netty.channel.Channels;
import org.jboss.netty.channel.MessageEvent;
import org.jboss.netty.channel.SimpleChannelHandler;
import org.jboss.netty.channel.socket.ServerSocketChannelFactory;
import org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory;
import org.jboss.netty.handler.codec.frame.LengthFieldBasedFrameDecoder;
import org.jboss.netty.handler.codec.frame.LengthFieldPrepender;
import org.jboss.netty.logging.InternalLoggerFactory;
import org.jboss.netty.logging.Log4JLoggerFactory;

@ChannelPipelineCoverage("all")
public class FakeBookie extends SimpleChannelHandler implements
    ChannelPipelineFactory {
    static final Logger logger = LoggerFactory.getLogger(FakeBookie.class);
    ServerSocketChannelFactory serverChannelFactory = new NioServerSocketChannelFactory(
        Executors.newCachedThreadPool(), Executors.newCachedThreadPool());

    public FakeBookie(int port) {
        InternalLoggerFactory.setDefaultFactory(new Log4JLoggerFactory());
        ServerBootstrap bootstrap = new ServerBootstrap(serverChannelFactory);

        bootstrap.setPipelineFactory(this);
        bootstrap.setOption("child.tcpNoDelay", true);
        bootstrap.setOption("child.keepAlive", true);
        bootstrap.setOption("reuseAddress", true);

        logger.info("Going into receive loop");
        // Bind and start to accept incoming connections.
        bootstrap.bind(new InetSocketAddress(port));
    }

    @Override
    public ChannelPipeline getPipeline() throws Exception {
        ChannelPipeline pipeline = Channels.pipeline();
        pipeline.addLast("lengthbaseddecoder",
                         new LengthFieldBasedFrameDecoder(1024 * 1024, 0, 4, 0, 4));
        pipeline.addLast("lengthprepender", new LengthFieldPrepender(4));
        pipeline.addLast("main", this);
        return pipeline;
    }

    @Override
    public void messageReceived(ChannelHandlerContext ctx, MessageEvent e)
            throws Exception {
        if (!(e.getMessage() instanceof ChannelBuffer)) {
            ctx.sendUpstream(e);
            return;
        }

        ChannelBuffer buffer = (ChannelBuffer) e.getMessage();

        int type = buffer.readInt();
        buffer.readerIndex(24);
        long ledgerId = buffer.readLong();
        long entryId = buffer.readLong();

        ChannelBuffer outBuf = ctx.getChannel().getConfig().getBufferFactory()
                               .getBuffer(24);
        outBuf.writeInt(type);
        outBuf.writeInt(0); // rc
        outBuf.writeLong(ledgerId);
        outBuf.writeLong(entryId);
        e.getChannel().write(outBuf);

    }


    public static void main(String args[]) {
        new FakeBookie(Integer.parseInt(args[0]));
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/common/ByteStringInterner.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.common;

import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ConcurrentMap;

import com.google.protobuf.ByteString;

public class ByteStringInterner {
    // TODO: how to release references when strings are no longer used. weak
    // references?

    private static final ConcurrentMap<ByteString, ByteString> map = new ConcurrentHashMap<ByteString, ByteString>();

    public static ByteString intern(ByteString in) {
        ByteString presentValueInMap = map.putIfAbsent(in, in);
        if (presentValueInMap != null) {
            return presentValueInMap;
        }
        return in;
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/common/ServerConfiguration.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.common;

import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.InputStream;
import java.net.InetAddress;
import java.net.URL;
import java.net.UnknownHostException;
import java.util.Arrays;
import java.util.LinkedList;
import java.util.List;

import org.apache.commons.configuration.ConfigurationException;
import org.apache.commons.lang.StringUtils;

import com.google.protobuf.ByteString;
import org.apache.bookkeeper.util.ReflectionUtils;
import org.apache.hedwig.conf.AbstractConfiguration;
import org.apache.hedwig.server.meta.MetadataManagerFactory;
import org.apache.hedwig.util.HedwigSocketAddress;

public class ServerConfiguration extends AbstractConfiguration {
    public final static String REGION = "region";
    protected final static String MAX_MESSAGE_SIZE = "max_message_size";
    protected final static String READAHEAD_COUNT = "readahead_count";
    protected final static String READAHEAD_SIZE = "readahead_size";
    protected final static String CACHE_SIZE = "cache_size";
    protected final static String CACHE_ENTRY_TTL = "cache_entry_ttl";
    protected final static String SCAN_BACKOFF_MSEC = "scan_backoff_ms";
    protected final static String SERVER_PORT = "server_port";
    protected final static String SSL_SERVER_PORT = "ssl_server_port";
    protected final static String ZK_PREFIX = "zk_prefix";
    protected final static String ZK_HOST = "zk_host";
    protected final static String ZK_TIMEOUT = "zk_timeout";
    protected final static String READAHEAD_ENABLED = "readahead_enabled";
    protected final static String STANDALONE = "standalone";
    protected final static String REGIONS = "regions";
    protected final static String CERT_NAME = "cert_name";
    protected final static String CERT_PATH = "cert_path";
    protected final static String PASSWORD = "password";
    protected final static String SSL_ENABLED = "ssl_enabled";
    protected final static String CONSUME_INTERVAL = "consume_interval";
    protected final static String RETENTION_SECS = "retention_secs";
    protected final static String INTER_REGION_SSL_ENABLED = "inter_region_ssl_enabled";
    protected final static String MESSAGES_CONSUMED_THREAD_RUN_INTERVAL = "messages_consumed_thread_run_interval";
    protected final static String BK_ENSEMBLE_SIZE = "bk_ensemble_size";
    @Deprecated
    protected final static String BK_QUORUM_SIZE = "bk_quorum_size";
    protected final static String BK_WRITE_QUORUM_SIZE = "bk_write_quorum_size";
    protected final static String BK_ACK_QUORUM_SIZE = "bk_ack_quorum_size";
    protected final static String RETRY_REMOTE_SUBSCRIBE_THREAD_RUN_INTERVAL = "retry_remote_subscribe_thread_run_interval";
    protected final static String DEFAULT_MESSAGE_WINDOW_SIZE =
        "default_message_window_size";
    protected final static String NUM_READAHEAD_CACHE_THREADS = "num_readahead_cache_threads";

    protected final static String MAX_ENTRIES_PER_LEDGER = "max_entries_per_ledger";

    // manager related settings
    protected final static String METADATA_MANAGER_BASED_TOPIC_MANAGER_ENABLED = "metadata_manager_based_topic_manager_enabled";
    protected final static String METADATA_MANAGER_FACTORY_CLASS = "metadata_manager_factory_class";

    // metastore settings, only being used when METADATA_MANAGER_FACTORY_CLASS is MsMetadataManagerFactory
    protected final static String METASTORE_IMPL_CLASS = "metastore_impl_class";
    protected final static String METASTORE_MAX_ENTRIES_PER_SCAN = "metastoreMaxEntriesPerScan";

    private static ClassLoader defaultLoader;
    static {
        defaultLoader = Thread.currentThread().getContextClassLoader();
        if (null == defaultLoader) {
            defaultLoader = ServerConfiguration.class.getClassLoader();
        }
    }

    // these are the derived attributes
    protected ByteString myRegionByteString = null;
    protected HedwigSocketAddress myServerAddress = null;
    protected List<String> regionList = null;

    // Although this method is not currently used, currently maintaining it like
    // this so that we can support on-the-fly changes in configuration
    protected void refreshDerivedAttributes() {
        refreshMyRegionByteString();
        refreshMyServerAddress();
        refreshRegionList();
    }

    @Override
    public void loadConf(URL confURL) throws ConfigurationException {
        super.loadConf(confURL);
        refreshDerivedAttributes();
    }

    public int getMaximumMessageSize() {
        return conf.getInt(MAX_MESSAGE_SIZE, 1258291); /* 1.2M */
    }

    public String getMyRegion() {
        return conf.getString(REGION, "standalone");
    }

    protected void refreshMyRegionByteString() {
        myRegionByteString = ByteString.copyFromUtf8(getMyRegion());
    }

    protected void refreshMyServerAddress() {
        try {
            // Use the raw IP address as the hostname
            myServerAddress = new HedwigSocketAddress(InetAddress.getLocalHost().getHostAddress(), getServerPort(),
                    getSSLServerPort());
        } catch (UnknownHostException e) {
            throw new RuntimeException(e);
        }
    }

    // The expected format for the regions parameter is Hostname:Port:SSLPort
    // with spaces in between each of the regions.
    protected void refreshRegionList() {
        String regions = conf.getString(REGIONS, "");
        if (regions.isEmpty()) {
            regionList = new LinkedList<String>();
        } else {
            regionList = Arrays.asList(regions.split(" "));
        }
    }

    public ByteString getMyRegionByteString() {
        if (myRegionByteString == null) {
            refreshMyRegionByteString();
        }
        return myRegionByteString;
    }

    /**
     * Maximum number of messages to read ahead. Default is 10.
     * 
     * @return int
     */
    public int getReadAheadCount() {
        return conf.getInt(READAHEAD_COUNT, 10);
    }

    /**
     * Maximum number of bytes to read ahead. Default is 4MB.
     * 
     * @return long
     */
    public long getReadAheadSizeBytes() {
        return conf.getLong(READAHEAD_SIZE, 4 * 1024 * 1024); // 4M
    }

    /**
     * Maximum cache size. By default is the smallest of 2G or
     * half the heap size.
     * 
     * @return long
     */
    public long getMaximumCacheSize() {
        // 2G or half of the maximum amount of memory the JVM uses
        return conf.getLong(CACHE_SIZE, Math.min(2 * 1024L * 1024L * 1024L, Runtime.getRuntime().maxMemory() / 2));
    }

    /**
     * Cache Entry TTL. By default is 0, cache entry will not be evicted
     * until the cache is fullfilled or the messages are already consumed.
     * The TTL is only checked when trying adding a new entry into the cache.
     *
     * @return cache entry ttl.
     */
    public long getCacheEntryTTL() {
        return conf.getLong(CACHE_ENTRY_TTL, 0L);
    }

    /**
     * After a scan of a log fails, how long before we retry (in msec)
     * 
     * @return long
     */
    public long getScanBackoffPeriodMs() {
        return conf.getLong(SCAN_BACKOFF_MSEC, 1000);
    }
    
    /**
     * Returns server port.
     * 
     * @return int
     */
    public int getServerPort() {
        return conf.getInt(SERVER_PORT, 4080);
    }

    /**
     * Returns SSL server port.
     * 
     * @return int
     */
    public int getSSLServerPort() {
        return conf.getInt(SSL_SERVER_PORT, 9876);
    }

    /**
     * Returns ZooKeeper path prefix.
     * 
     * @return string
     */
    public String getZkPrefix() {
        return conf.getString(ZK_PREFIX, "/hedwig");
    }

    public StringBuilder getZkRegionPrefix(StringBuilder sb) {
        return sb.append(getZkPrefix()).append("/").append(getMyRegion());
    }

    /**
     * Get znode path to store manager layouts.
     *
     * @param sb
     *          StringBuilder to store znode path to store manager layouts.
     * @return znode path to store manager layouts.
     */
    public StringBuilder getZkManagersPrefix(StringBuilder sb) {
        return getZkRegionPrefix(sb).append("/managers");
    }

    public StringBuilder getZkTopicsPrefix(StringBuilder sb) {
        return getZkRegionPrefix(sb).append("/topics");
    }

    public StringBuilder getZkTopicPath(StringBuilder sb, ByteString topic) {
        return getZkTopicsPrefix(sb).append("/").append(topic.toStringUtf8());
    }

    public StringBuilder getZkHostsPrefix(StringBuilder sb) {
        return getZkRegionPrefix(sb).append("/hosts");
    }

    public HedwigSocketAddress getServerAddr() {
        if (myServerAddress == null) {
            refreshMyServerAddress();
        }
        return myServerAddress;
    }

    /**
     * Return ZooKeeper list of servers. Default is localhost.
     * 
     * @return String
     */
    public String getZkHost() {
        List<Object> servers = conf.getList(ZK_HOST, null);
        if (null == servers || 0 == servers.size()) {
            return "localhost";
        }
        return StringUtils.join(servers, ",");
    }

    /**
     * Return ZooKeeper session timeout. Default is 2s.
     * 
     * @return int
     */
    public int getZkTimeout() {
        return conf.getInt(ZK_TIMEOUT, 2000);
    }

    /** 
     * Returns true if read-ahead enabled. Default is true.
     * 
     * @return boolean
     */
    public boolean getReadAheadEnabled() {
        return conf.getBoolean(READAHEAD_ENABLED, true)
            || conf.getBoolean("readhead_enabled");
        // the key was misspelt in a previous version, so compensate here
    }

    /**
     * Returns true if standalone. Default is false.
     * 
     * @return boolean
     */
    public boolean isStandalone() {
        return conf.getBoolean(STANDALONE, false);
    }

    /**
     * Returns list of regions. 
     * 
     * @return List<String>
     */
    public List<String> getRegions() {
        if (regionList == null) {
            refreshRegionList();
        }
        return regionList;
    }

    /**
     *  Returns the name of the SSL certificate if available as a resource.
     * 
     * @return String
     */
    public String getCertName() {
        return conf.getString(CERT_NAME, "");
    }

    /**
     * This is the path to the SSL certificate if it is available as a file.
     * 
     * @return String
     */
    public String getCertPath() {
        return conf.getString(CERT_PATH, "");
    }

    // This method return the SSL certificate as an InputStream based on if it
    // is configured to be available as a resource or as a file. If nothing is
    // configured correctly, then a ConfigurationException will be thrown as
    // we do not know how to obtain the SSL certificate stream.
    public InputStream getCertStream() throws FileNotFoundException, ConfigurationException {
        String certName = getCertName();
        String certPath = getCertPath();
        if (certName != null && !certName.isEmpty()) {
            return getClass().getResourceAsStream(certName);
        } else if (certPath != null && !certPath.isEmpty()) {
            return new FileInputStream(certPath);
        } else
            throw new ConfigurationException("SSL Certificate configuration does not have resource name or path set!");
    }

    /**
     * Returns the password used for BookKeeper ledgers. Default
     * is the empty string.
     * 
     * @return
     */
    public String getPassword() {
        return conf.getString(PASSWORD, "");
    }

    /**
     * Returns true if SSL is enabled. Default is false.
     * 
     * @return boolean
     */
    public boolean isSSLEnabled() {
        return conf.getBoolean(SSL_ENABLED, false);
    }

    /**
     * Gets the number of messages consumed before persisting
     * information about consumed messages. A value greater than
     * one avoids persisting information about consumed messages
     * upon every consumed message. Default is 50.
     * 
     * @return int
     */
    public int getConsumeInterval() {
        return conf.getInt(CONSUME_INTERVAL, 50);
    }

    /**
     * Returns the interval to release a topic. If this
     * parameter is greater than zero, then schedule a
     * task to release an owned topic. Default is 0 (never released).
     * 
     * @return int
     */
    public int getRetentionSecs() {
        return conf.getInt(RETENTION_SECS, 0);
    }

    /**
     * True if SSL is enabled across regions.
     * 
     * @return boolean
     */
    public boolean isInterRegionSSLEnabled() {
        return conf.getBoolean(INTER_REGION_SSL_ENABLED, false);
    }

    /**
     * This parameter is used to determine how often we run the 
     * SubscriptionManager's Messages Consumed timer task thread 
     * (in milliseconds).
     * 
     * @return int
     */
    public int getMessagesConsumedThreadRunInterval() {
        return conf.getInt(MESSAGES_CONSUMED_THREAD_RUN_INTERVAL, 60000);
    }

    /**
     * This parameter is used to determine how often we run a thread
     * to retry those failed remote subscriptions in asynchronous mode
     * (in milliseconds).
     * 
     * @return int
     */
    public int getRetryRemoteSubscribeThreadRunInterval() {
        return conf.getInt(RETRY_REMOTE_SUBSCRIBE_THREAD_RUN_INTERVAL, 120000);
    }

    /**
     * This parameter is for setting the default maximum number of messages which
     * can be delivered to a subscriber without being consumed.
     * we pause messages delivery to a subscriber when reaching the window size
     * 
     * @return int
     */
    public int getDefaultMessageWindowSize() {
        return conf.getInt(DEFAULT_MESSAGE_WINDOW_SIZE, 0);
    }

    /**
     * This parameter is used when Bookkeeper is the persistence
     * store and indicates what the ensemble size is (i.e. how
     * many bookie servers to stripe the ledger entries across).
     * 
     * @return int
     */
    public int getBkEnsembleSize() {
        return conf.getInt(BK_ENSEMBLE_SIZE, 3);
    }


    /**
     * This parameter is used when Bookkeeper is the persistence store
     * and indicates what the quorum size is (i.e. how many redundant
     * copies of each ledger entry is written).
     * 
     * @return int
     */
    @Deprecated
    protected int getBkQuorumSize() {
        return conf.getInt(BK_QUORUM_SIZE, 2);
    }

    /**
     * Get the write quorum size for BookKeeper client, which is used to
     * indicate how many redundant copies of each ledger entry is written.
     *
     * @return write quorum size for BookKeeper client.
     */
    public int getBkWriteQuorumSize() {
        if (conf.containsKey(BK_WRITE_QUORUM_SIZE)) {
            return conf.getInt(BK_WRITE_QUORUM_SIZE, 2);
        } else {
            return getBkQuorumSize();
        }
    }

    /**
     * Get the ack quorum size for BookKeeper client.
     *
     * @return ack quorum size for BookKeeper client.
     */
    public int getBkAckQuorumSize() {
        if (conf.containsKey(BK_ACK_QUORUM_SIZE)) {
            return conf.getInt(BK_ACK_QUORUM_SIZE, 2);
        } else {
            return getBkQuorumSize();
        }
    }

    /**
     * This parameter is used when BookKeeper is the persistence storage,
     * and indicates when the number of entries stored in a ledger reach
     * the threshold, hub server will open a new ledger to write.
     *
     * @return max entries per ledger
     */
    public long getMaxEntriesPerLedger() {
        return conf.getLong(MAX_ENTRIES_PER_LEDGER, 0L);
    }

    /*
     * Is this a valid configuration that we can run with? This code might grow
     * over time.
     */
    public void validate() throws ConfigurationException {
        if (!getZkPrefix().startsWith("/")) {
            throw new ConfigurationException(ZK_PREFIX + " must start with a /");
        }
        // Validate that if Regions exist and inter-region communication is SSL
        // enabled, that the Regions correspond to valid HedwigSocketAddresses,
        // namely that SSL ports are present.
        if (isInterRegionSSLEnabled() && getRegions().size() > 0) {
            for (String hubString : getRegions()) {
                HedwigSocketAddress hub = new HedwigSocketAddress(hubString);
                if (hub.getSSLSocketAddress() == null)
                    throw new ConfigurationException("Region defined does not have required SSL port: " + hubString);
            }
        }
        // Validate that the Bookkeeper ensemble size >= quorum size.
        if (getBkEnsembleSize() < getBkWriteQuorumSize()) {
            throw new ConfigurationException("BK ensemble size (" + getBkEnsembleSize()
                                             + ") is less than the write quorum size (" + getBkWriteQuorumSize() + ")");
        }

        if (getBkWriteQuorumSize() < getBkAckQuorumSize()) {
            throw new ConfigurationException("BK write quorum size (" + getBkWriteQuorumSize()
                                             + ") is less than the ack quorum size (" + getBkAckQuorumSize() + ")");
        }

        // add other checks here
    }

    /**
     * Get number of read ahead cache threads.
     *
     * @return number of read ahead cache threads.
     */
    public int getNumReadAheadCacheThreads() {
        return conf.getInt(NUM_READAHEAD_CACHE_THREADS, Runtime.getRuntime().availableProcessors());
    }

    /**
     * Whether enable metadata manager based topic manager.
     *
     * @return true if enabled metadata manager based topic manager.
     */
    public boolean isMetadataManagerBasedTopicManagerEnabled() {
        return conf.getBoolean(METADATA_MANAGER_BASED_TOPIC_MANAGER_ENABLED, false);
    }

    /**
     * Get metadata manager factory class.
     *
     * @return manager class
     */
    public Class<? extends MetadataManagerFactory> getMetadataManagerFactoryClass()
    throws ConfigurationException {
        return ReflectionUtils.getClass(conf, METADATA_MANAGER_FACTORY_CLASS,
                                        null, MetadataManagerFactory.class,
                                        defaultLoader);
    }

    /**
     * Set metadata manager factory class name
     *
     * @param managerClsName
     *          Manager Class Name
     * @return server configuration
     */
    public ServerConfiguration setMetadataManagerFactoryName(String managerClsName) {
        conf.setProperty(METADATA_MANAGER_FACTORY_CLASS, managerClsName);
        return this;
    }

    /**
     * Get metastore implementation class.
     *
     * @return metastore implementation class name.
     */
    public String getMetastoreImplClass() {
        return conf.getString(METASTORE_IMPL_CLASS);
    }

    /**
     * Get max entries per scan in metastore.
     *
     * @return max entries per scan in metastore.
     */
    public int getMetastoreMaxEntriesPerScan() {
        return conf.getInt(METASTORE_MAX_ENTRIES_PER_SCAN, 50);
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/common/TerminateJVMExceptionHandler.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.common;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class TerminateJVMExceptionHandler implements Thread.UncaughtExceptionHandler {
    static Logger logger = LoggerFactory.getLogger(TerminateJVMExceptionHandler.class);

    @Override
    public void uncaughtException(Thread t, Throwable e) {
        logger.error("Uncaught exception in thread " + t.getName(), e);
        Runtime.getRuntime().exit(1);
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/common/TopicOpQueuer.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.common;

import java.util.HashMap;
import java.util.LinkedList;
import java.util.Queue;
import java.util.concurrent.ScheduledExecutorService;

import com.google.protobuf.ByteString;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.util.Callback;

public class TopicOpQueuer {
    /**
     * Map from topic to the queue of operations for that topic.
     */
    protected HashMap<ByteString, Queue<Runnable>> topic2ops = new HashMap<ByteString, Queue<Runnable>>();

    protected final ScheduledExecutorService scheduler;

    public TopicOpQueuer(ScheduledExecutorService scheduler) {
        this.scheduler = scheduler;
    }

    public interface Op extends Runnable {
    }

    public abstract class AsynchronousOp<T> implements Op {
        final public ByteString topic;
        final public Callback<T> cb;
        final public Object ctx;

        public AsynchronousOp(final ByteString topic, final Callback<T> cb, Object ctx) {
            this.topic = topic;
            this.cb = new Callback<T>() {
                @Override
                public void operationFailed(Object ctx, PubSubException exception) {
                    cb.operationFailed(ctx, exception);
                    popAndRunNext(topic);
                }

                @Override
                public void operationFinished(Object ctx, T resultOfOperation) {
                    cb.operationFinished(ctx, resultOfOperation);
                    popAndRunNext(topic);
                }
            };
            this.ctx = ctx;
        }
    }

    public abstract class SynchronousOp implements Op {
        final public ByteString topic;

        public SynchronousOp(ByteString topic) {
            this.topic = topic;
        }

        @Override
        public final void run() {
            runInternal();
            popAndRunNext(topic);
        }

        protected abstract void runInternal();

    }

    protected synchronized void popAndRunNext(ByteString topic) {
        Queue<Runnable> ops = topic2ops.get(topic);
        if (!ops.isEmpty())
            ops.remove();
        if (!ops.isEmpty())
            scheduler.submit(ops.peek());
    }

    public void pushAndMaybeRun(ByteString topic, Op op) {
        int size;
        synchronized (this) {
            Queue<Runnable> ops = topic2ops.get(topic);
            if (ops == null) {
                ops = new LinkedList<Runnable>();
                topic2ops.put(topic, ops);
            }
            ops.add(op);
            size = ops.size();
        }
        if (size == 1)
            op.run();
    }

    public Runnable peek(ByteString topic) {
        return topic2ops.get(topic).peek();
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/common/UnexpectedError.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.common;

public class UnexpectedError extends Error {

    /**
     *
     */
    private static final long serialVersionUID = 1L;

    public UnexpectedError(String msg) {
        super(msg);
    }

    public UnexpectedError(Throwable cause) {
        super(cause);
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/delivery/ChannelEndPoint.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.delivery;

import java.util.HashMap;
import java.util.Map;

import org.jboss.netty.channel.Channel;
import org.jboss.netty.channel.ChannelFuture;
import org.jboss.netty.channel.ChannelFutureListener;

import org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse;
import org.apache.hedwig.server.common.UnexpectedError;

public class ChannelEndPoint implements DeliveryEndPoint, ChannelFutureListener {

    Channel channel;

    public Channel getChannel() {
        return channel;
    }

    Map<ChannelFuture, DeliveryCallback> callbacks = new HashMap<ChannelFuture, DeliveryCallback>();

    public ChannelEndPoint(Channel channel) {
        this.channel = channel;
    }

    public void close() {
        channel.close();
    }

    public void send(PubSubResponse response, DeliveryCallback callback) {
        ChannelFuture future = channel.write(response);
        callbacks.put(future, callback);
        future.addListener(this);
    }

    public void operationComplete(ChannelFuture future) throws Exception {
        DeliveryCallback callback = callbacks.get(future);
        callbacks.remove(future);

        if (callback == null) {
            throw new UnexpectedError("Could not locate callback for channel future");
        }

        if (future.isSuccess()) {
            callback.sendingFinished();
        } else {
            // treat all channel errors as permanent
            callback.permanentErrorOnSend();
        }

    }

    @Override
    public boolean equals(Object obj) {
        if (obj instanceof ChannelEndPoint) {
            ChannelEndPoint channelEndPoint = (ChannelEndPoint) obj;
            return channel.equals(channelEndPoint.channel);
        } else {
            return false;
        }
    }

    @Override
    public int hashCode() {
        return channel.hashCode();
    }

    @Override
    public String toString() {
        return channel.toString();
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/delivery/DeliveryCallback.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.delivery;

public interface DeliveryCallback {

    public void sendingFinished();

    public void transientErrorOnSend();

    public void permanentErrorOnSend();
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/delivery/DeliveryEndPoint.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.delivery;

import org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse;

public interface DeliveryEndPoint {

    public void send(PubSubResponse response, DeliveryCallback callback);

    public void close();

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/delivery/DeliveryManager.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.delivery;

import com.google.protobuf.ByteString;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEvent;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences;
import org.apache.hedwig.filter.ServerMessageFilter;
import org.apache.hedwig.util.Callback;

public interface DeliveryManager {
    public void start();

    /**
     * Start serving a given subscription.
     *
     * @param topic
     *          Topic Name
     * @param subscriberId
     *          Subscriber Id
     * @param preferences
     *          Subscription Preferences
     * @param seqIdToStartFrom
     *          Message sequence id starting delivery from.
     * @param endPoint
     *          End point to deliver messages to.
     * @param filter
     *          Message filter used to filter messages before delivery.
     * @param callback
     *          Callback instance.
     * @param ctx
     *          Callback context.
     */
    public void startServingSubscription(ByteString topic, ByteString subscriberId,
                                         SubscriptionPreferences preferences,
                                         MessageSeqId seqIdToStartFrom,
                                         DeliveryEndPoint endPoint,
                                         ServerMessageFilter filter,
                                         Callback<Void> callback, Object ctx);

    /**
     * Stop serving a given subscription.
     *
     * @param topic
     *          Topic Name
     * @param subscriberId
     *          Subscriber Id
     * @param event
     *          Subscription event indicating the reason to stop the subscriber.
     * @param callback
     *          Callback instance.
     * @param ctx
     *          Callback context.
     */
    public void stopServingSubscriber(ByteString topic, ByteString subscriberId,
                                      SubscriptionEvent event,
                                      Callback<Void> callback, Object ctx);

    /**
     * Tell the delivery manager where that a subscriber has consumed
     *
     * @param topic
     *          Topic Name
     * @param subscriberId
     *          Subscriber Id
     * @param consumedSeqId
     *          Max consumed seq id.
     */
    public void messageConsumed(ByteString topic, ByteString subscriberId,
                                MessageSeqId consumedSeqId);

    /**
     * Stop delivery manager
     */
    public void stop();
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/delivery/FIFODeliveryManager.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.delivery;

import java.util.Comparator;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Map;
import java.util.Queue;
import java.util.Set;
import java.util.SortedMap;
import java.util.TreeMap;
import java.util.concurrent.BlockingQueue;
import java.util.concurrent.PriorityBlockingQueue;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.locks.ReentrantReadWriteLock;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.common.annotations.VisibleForTesting;
import com.google.protobuf.ByteString;

import org.apache.bookkeeper.util.MathUtils;
import org.apache.hedwig.client.data.TopicSubscriber;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.filter.ServerMessageFilter;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse;
import org.apache.hedwig.protocol.PubSubProtocol.StatusCode;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEvent;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences;
import org.apache.hedwig.protoextensions.PubSubResponseUtils;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.common.UnexpectedError;
import org.apache.hedwig.server.handlers.SubscriptionChannelManager.SubChannelDisconnectedListener;
import org.apache.hedwig.server.netty.ServerStats;
import org.apache.hedwig.server.persistence.CancelScanRequest;
import org.apache.hedwig.server.persistence.Factory;
import org.apache.hedwig.server.persistence.MapMethods;
import org.apache.hedwig.server.persistence.PersistenceManager;
import org.apache.hedwig.server.persistence.ReadAheadCache;
import org.apache.hedwig.server.persistence.ScanCallback;
import org.apache.hedwig.server.persistence.ScanRequest;
import org.apache.hedwig.util.Callback;
import static org.apache.hedwig.util.VarArgs.va;

public class FIFODeliveryManager implements Runnable, DeliveryManager, SubChannelDisconnectedListener {

    protected static final Logger logger = LoggerFactory.getLogger(FIFODeliveryManager.class);

    private static Callback<Void> NOP_CALLBACK = new Callback<Void>() {
        @Override
        public void operationFinished(Object ctx, Void result) {
        }
        @Override
        public void operationFailed(Object ctx, PubSubException exception) {
        }
    };

    protected interface DeliveryManagerRequest {
        public void performRequest();
    }

    /**
     * the main queue that the single-threaded delivery manager works off of
     */
    BlockingQueue<DeliveryManagerRequest> requestQueue = new LinkedBlockingQueue<DeliveryManagerRequest>();

    /**
     * The queue of all subscriptions that are facing a transient error either
     * in scanning from the persistence manager, or in sending to the consumer
     */
    Queue<ActiveSubscriberState> retryQueue =
        new PriorityBlockingQueue<ActiveSubscriberState>(32, new Comparator<ActiveSubscriberState>() {
            @Override
            public int compare(ActiveSubscriberState as1, ActiveSubscriberState as2) {
                long s = as1.lastScanErrorTime - as2.lastScanErrorTime;
                return s > 0 ? 1 : (s < 0 ? -1 : 0);
            }
        });

    /**
     * Stores a mapping from topic to the delivery pointers on the topic. The
     * delivery pointers are stored in a sorted map from seq-id to the set of
     * subscribers at that seq-id
     */
    Map<ByteString, SortedMap<Long, Set<ActiveSubscriberState>>> perTopicDeliveryPtrs;

    /**
     * Mapping from delivery end point to the subscriber state that we are
     * serving at that end point. This prevents us e.g., from serving two
     * subscriptions to the same endpoint
     */
    Map<TopicSubscriber, ActiveSubscriberState> subscriberStates;

    private final ReadAheadCache cache;
    private final PersistenceManager persistenceMgr;

    private ServerConfiguration cfg;

    // Boolean indicating if this thread should continue running. This is used
    // when we want to stop the thread during a PubSubServer shutdown.
    protected boolean keepRunning = true;
    private final Thread workerThread;

    private Object suspensionLock = new Object();
    private boolean suspended = false;


    public FIFODeliveryManager(PersistenceManager persistenceMgr, ServerConfiguration cfg) {
        this.persistenceMgr = persistenceMgr;
        if (persistenceMgr instanceof ReadAheadCache) {
            this.cache = (ReadAheadCache) persistenceMgr;
        } else {
            this.cache = null;
        }
        perTopicDeliveryPtrs = new HashMap<ByteString, SortedMap<Long, Set<ActiveSubscriberState>>>();
        subscriberStates = new HashMap<TopicSubscriber, ActiveSubscriberState>();
        workerThread = new Thread(this, "DeliveryManagerThread");
        this.cfg = cfg;
    }

    public void start() {
        workerThread.start();
    }

    /**
     * Stop FIFO delivery manager from processing requests. (for testing)
     */
    @VisibleForTesting
    public void suspendProcessing() {
        synchronized(suspensionLock) {
            suspended = true;
        }
    }

    /**
     * Resume FIFO delivery manager. (for testing)
     */
    @VisibleForTesting
    public void resumeProcessing() {
        synchronized(suspensionLock) {
            suspended = false;
            suspensionLock.notify();
        }
    }

    /**
     * ===================================================================== Our
     * usual enqueue function, stop if error because of unbounded queue, should
     * never happen
     *
     */
    protected void enqueueWithoutFailure(DeliveryManagerRequest request) {
        if (!requestQueue.offer(request)) {
            throw new UnexpectedError("Could not enqueue object: " + request + " to delivery manager request queue.");
        }
    }

    /**
     * ====================================================================
     * Public interface of the delivery manager
     */

    /**
     * Tells the delivery manager to start sending out messages for a particular
     * subscription
     *
     * @param topic
     * @param subscriberId
     * @param seqIdToStartFrom
     *            Message sequence-id from where delivery should be started
     * @param endPoint
     *            The delivery end point to which send messages to
     * @param filter
     *            Only messages passing this filter should be sent to this
     *            subscriber
     * @param callback
     *            Callback instance
     * @param ctx
     *            Callback context
     */
    @Override
    public void startServingSubscription(ByteString topic, ByteString subscriberId,
                                         SubscriptionPreferences preferences,
                                         MessageSeqId seqIdToStartFrom,
                                         DeliveryEndPoint endPoint, ServerMessageFilter filter,
                                         Callback<Void> callback, Object ctx) {
        ActiveSubscriberState subscriber = 
            new ActiveSubscriberState(topic, subscriberId,
                                      preferences,
                                      seqIdToStartFrom.getLocalComponent() - 1,
                                      endPoint, filter, callback, ctx);

        enqueueWithoutFailure(subscriber);
    }

    public void stopServingSubscriber(ByteString topic, ByteString subscriberId,
                                      SubscriptionEvent event,
                                      Callback<Void> cb, Object ctx) {
        enqueueWithoutFailure(new StopServingSubscriber(topic, subscriberId, event, cb, ctx));
    }

    /**
     * Instructs the delivery manager to backoff on the given subscriber and
     * retry sending after some time
     *
     * @param subscriber
     */
    public void retryErroredSubscriberAfterDelay(ActiveSubscriberState subscriber) {

        subscriber.setLastScanErrorTime(MathUtils.now());

        if (!retryQueue.offer(subscriber)) {
            throw new UnexpectedError("Could not enqueue to delivery manager retry queue");
        }
    }

    public void clearRetryDelayForSubscriber(ActiveSubscriberState subscriber) {
        subscriber.clearLastScanErrorTime();
        if (!retryQueue.offer(subscriber)) {
            throw new UnexpectedError("Could not enqueue to delivery manager retry queue");
        }
        // no request in request queue now
        // issue a empty delivery request to not waiting for polling requests queue
        if (requestQueue.isEmpty()) {
            enqueueWithoutFailure(new DeliveryManagerRequest() {
                    @Override
                    public void performRequest() {
                    // do nothing
                    }
                    });
        }
    }

    // TODO: for now, I don't move messageConsumed request to delivery manager thread,
    //       which is supposed to be fixed in {@link https://issues.apache.org/jira/browse/BOOKKEEPER-503}
    @Override
    public void messageConsumed(ByteString topic, ByteString subscriberId,
                                MessageSeqId consumedSeqId) {
        ActiveSubscriberState subState =
            subscriberStates.get(new TopicSubscriber(topic, subscriberId));
        if (null == subState) {
            return;
        }
        subState.messageConsumed(consumedSeqId.getLocalComponent()); 
    }

    /**
     * Instructs the delivery manager to move the delivery pointer for a given
     * subscriber
     *
     * @param subscriber
     * @param prevSeqId
     * @param newSeqId
     */
    public void moveDeliveryPtrForward(ActiveSubscriberState subscriber, long prevSeqId, long newSeqId) {
        enqueueWithoutFailure(new DeliveryPtrMove(subscriber, prevSeqId, newSeqId));
    }

    /*
     * ==========================================================================
     * == End of public interface, internal machinery begins.
     */
    public void run() {
        while (keepRunning) {
            DeliveryManagerRequest request = null;

            try {
                // We use a timeout of 1 second, so that we can wake up once in
                // a while to check if there is something in the retry queue.
                request = requestQueue.poll(1, TimeUnit.SECONDS);
                synchronized(suspensionLock) {
                    while (suspended) {
                        suspensionLock.wait();
                    }
                }
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
            }

            // First retry any subscriptions that had failed and need a retry
            retryErroredSubscribers();

            if (request == null) {
                continue;
            }

            request.performRequest();

        }
    }

    /**
     * Stop method which will enqueue a ShutdownDeliveryManagerRequest.
     */
    public void stop() {
        enqueueWithoutFailure(new ShutdownDeliveryManagerRequest());
    }

    protected void retryErroredSubscribers() {
        long lastInterestingFailureTime = MathUtils.now() - cfg.getScanBackoffPeriodMs();
        ActiveSubscriberState subscriber;

        while ((subscriber = retryQueue.peek()) != null) {
            if (subscriber.getLastScanErrorTime() > lastInterestingFailureTime) {
                // Not enough time has elapsed yet, will retry later
                // Since the queue is fifo, no need to check later items
                return;
            }

            // retry now
            subscriber.deliverNextMessage();
            retryQueue.poll();
        }
    }

    protected void removeDeliveryPtr(ActiveSubscriberState subscriber, Long seqId, boolean isAbsenceOk,
                                     boolean pruneTopic) {

        assert seqId != null;

        // remove this subscriber from the delivery pointers data structure
        ByteString topic = subscriber.getTopic();
        SortedMap<Long, Set<ActiveSubscriberState>> deliveryPtrs = perTopicDeliveryPtrs.get(topic);

        if (deliveryPtrs == null && !isAbsenceOk) {
            throw new UnexpectedError("No delivery pointers found while disconnecting " + "channel for topic:" + topic);
        }

        if(null == deliveryPtrs) {
            return;
        }

        if (!MapMethods.removeFromMultiMap(deliveryPtrs, seqId, subscriber) && !isAbsenceOk) {

            throw new UnexpectedError("Could not find subscriber:" + subscriber + " at the expected delivery pointer");
        }

        if (pruneTopic && deliveryPtrs.isEmpty()) {
            perTopicDeliveryPtrs.remove(topic);
        }

    }

    protected long getMinimumSeqId(ByteString topic) {
        SortedMap<Long, Set<ActiveSubscriberState>> deliveryPtrs = perTopicDeliveryPtrs.get(topic);

        if (deliveryPtrs == null || deliveryPtrs.isEmpty()) {
            return Long.MAX_VALUE - 1;
        }
        return deliveryPtrs.firstKey();
    }

    protected void addDeliveryPtr(ActiveSubscriberState subscriber, Long seqId) {

        // If this topic doesn't exist in the per-topic delivery pointers table,
        // create an entry for it
        SortedMap<Long, Set<ActiveSubscriberState>> deliveryPtrs = MapMethods.getAfterInsertingIfAbsent(
                    perTopicDeliveryPtrs, subscriber.getTopic(), TreeMapLongToSetSubscriberFactory.instance);

        MapMethods.addToMultiMap(deliveryPtrs, seqId, subscriber, HashMapSubscriberFactory.instance);
    }

    public class ActiveSubscriberState
        implements ScanCallback, DeliveryCallback, DeliveryManagerRequest, CancelScanRequest {

        static final int UNLIMITED = 0;

        ByteString topic;
        ByteString subscriberId;
        long lastLocalSeqIdDelivered;
        boolean connected = true;
        ReentrantReadWriteLock connectedLock = new ReentrantReadWriteLock();
        DeliveryEndPoint deliveryEndPoint;
        long lastScanErrorTime = -1;
        long localSeqIdDeliveringNow;
        long lastSeqIdCommunicatedExternally;
        long lastSeqIdConsumedUtil;
        boolean isThrottled = false;
        final int messageWindowSize;
        ServerMessageFilter filter;
        Callback<Void> cb;
        Object ctx;

        // track the outstanding scan request
        // so we could cancel it
        ScanRequest outstandingScanRequest;

        final static int SEQ_ID_SLACK = 10;

        public ActiveSubscriberState(ByteString topic, ByteString subscriberId,
                                     SubscriptionPreferences preferences,
                                     long lastLocalSeqIdDelivered,
                                     DeliveryEndPoint deliveryEndPoint,
                                     ServerMessageFilter filter,
                                     Callback<Void> cb, Object ctx) {
            this.topic = topic;
            this.subscriberId = subscriberId;
            this.lastLocalSeqIdDelivered = lastLocalSeqIdDelivered;
            this.lastSeqIdConsumedUtil = lastLocalSeqIdDelivered;
            this.deliveryEndPoint = deliveryEndPoint;
            this.filter = filter;
            if (preferences.hasMessageWindowSize()) {
                messageWindowSize = preferences.getMessageWindowSize();
            } else {
                if (FIFODeliveryManager.this.cfg.getDefaultMessageWindowSize() > 0) {
                    messageWindowSize =
                        FIFODeliveryManager.this.cfg.getDefaultMessageWindowSize();
                } else {
                    messageWindowSize = UNLIMITED;
                }
            }
            this.cb = cb;
            this.ctx = ctx;
        }

        public void setNotConnected(SubscriptionEvent event) {
            this.connectedLock.writeLock().lock();
            try {
                // have closed it.
                if (!connected) {
                    return;
                }
                this.connected = false;
                // put itself in ReadAhead queue to cancel outstanding scan request
                // if outstanding scan request callback before cancel op executed,
                // nothing it would cancel.
                if (null != cache && null != outstandingScanRequest) {
                    cache.cancelScanRequest(topic, this);
                }
            } finally {
                this.connectedLock.writeLock().unlock();
            }

            if (null != event &&
                (SubscriptionEvent.TOPIC_MOVED == event ||
                 SubscriptionEvent.SUBSCRIPTION_FORCED_CLOSED == event)) {
                // we should not close the channel now after enabling multiplexing
                PubSubResponse response = PubSubResponseUtils.getResponseForSubscriptionEvent(
                    topic, subscriberId, event
                );
                deliveryEndPoint.send(response, new DeliveryCallback() {
                    @Override
                    public void sendingFinished() {
                        // do nothing now
                    }
                    @Override
                    public void transientErrorOnSend() {
                        // do nothing now
                    }
                    @Override
                    public void permanentErrorOnSend() {
                        // if channel is broken, close the channel
                        deliveryEndPoint.close();
                    }
                });
            }
            // uninitialize filter
            this.filter.uninitialize();
        }

        public ByteString getTopic() {
            return topic;
        }

        public synchronized long getLastScanErrorTime() {
            return lastScanErrorTime;
        }

        public synchronized void setLastScanErrorTime(long lastScanErrorTime) {
            this.lastScanErrorTime = lastScanErrorTime;
        }

        /**
         * Clear the last scan error time so it could be retry immediately.
         */
        protected synchronized void clearLastScanErrorTime() {
            this.lastScanErrorTime = -1;
        }

        protected boolean isConnected() {
            connectedLock.readLock().lock();
            try {
                return connected;
            } finally {
                connectedLock.readLock().unlock();
            }
        }

        protected synchronized void messageConsumed(long newSeqIdConsumed) {
            if (newSeqIdConsumed <= lastSeqIdConsumedUtil) {
                return;
            }
            if (logger.isDebugEnabled()) {
                logger.debug("Subscriber ({}) moved consumed ptr from {} to {}.",
                             va(this, lastSeqIdConsumedUtil, newSeqIdConsumed));
            }
            lastSeqIdConsumedUtil = newSeqIdConsumed;
            // after updated seq id check whether it still exceed msg limitation
            if (msgLimitExceeded()) {
                return;
            }
            if (isThrottled) {
                isThrottled = false;
                logger.info("Try to wake up subscriber ({}) to deliver messages again : last delivered {}, last consumed {}.",
                            va(this, lastLocalSeqIdDelivered, lastSeqIdConsumedUtil));

                enqueueWithoutFailure(new DeliveryManagerRequest() {
                    @Override
                    public void performRequest() {
                        // enqueue 
                        clearRetryDelayForSubscriber(ActiveSubscriberState.this);            
                    }
                });
            }
        }

        protected boolean msgLimitExceeded() {
            if (messageWindowSize == UNLIMITED) {
                return false;
            }
            if (lastLocalSeqIdDelivered - lastSeqIdConsumedUtil >= messageWindowSize) {
                return true;
            }
            return false;
        }

        public void deliverNextMessage() {
            connectedLock.readLock().lock();
            try {
                doDeliverNextMessage();
            } finally {
                connectedLock.readLock().unlock();
            }
        }

        private void doDeliverNextMessage() {
            if (!connected) {
                return;
            }

            synchronized (this) {
                // check whether we have delivered enough messages without receiving their consumes
                if (msgLimitExceeded()) {
                    logger.info("Subscriber ({}) is throttled : last delivered {}, last consumed {}.",
                                va(this, lastLocalSeqIdDelivered, lastSeqIdConsumedUtil));
                    isThrottled = true;
                    // do nothing, since the delivery process would be throttled.
                    // After message consumed, it would be added back to retry queue.
                    return;
                }

                localSeqIdDeliveringNow = persistenceMgr.getSeqIdAfterSkipping(topic, lastLocalSeqIdDelivered, 1);

                outstandingScanRequest = new ScanRequest(topic, localSeqIdDeliveringNow,
                        /* callback= */this, /* ctx= */null);
            }

            persistenceMgr.scanSingleMessage(outstandingScanRequest);
        }

        /**
         * ===============================================================
         * {@link CancelScanRequest} methods
         *
         * This method runs ins same threads with ScanCallback. When it runs,
         * it checked whether it is outstanding scan request. if there is one,
         * cancel it.
         */
        @Override
        public ScanRequest getScanRequest() {
            // no race between cancel request and scan callback
            // the only race is between stopServing and deliverNextMessage
            // deliverNextMessage would be executed in netty callback which is in netty thread
            // stopServing is run in delivery thread. if stopServing runs before deliverNextMessage
            // deliverNextMessage would have chance to put a stub in ReadAheadCache
            // then we don't have any chance to cancel it.
            // use connectedLock to avoid such race.
            return outstandingScanRequest;
        }

        private boolean checkConnected() {
            connectedLock.readLock().lock();
            try {
                // message scanned means the outstanding request is executed
                outstandingScanRequest = null;
                return connected;
            } finally {
                connectedLock.readLock().unlock();
            }
        }

        /**
         * ===============================================================
         * {@link ScanCallback} methods
         */

        public void messageScanned(Object ctx, Message message) {
            if (!checkConnected()) {
                return;
            }

            if (!filter.testMessage(message)) {
                sendingFinished();
                return;
            }

            /**
             * The method below will invoke our sendingFinished() method when
             * done
             */
            PubSubResponse response = PubSubResponse.newBuilder()
                                      .setProtocolVersion(ProtocolVersion.VERSION_ONE)
                                      .setStatusCode(StatusCode.SUCCESS).setTxnId(0)
                                      .setMessage(message).setTopic(topic)
                                      .setSubscriberId(subscriberId).build();

            deliveryEndPoint.send(response, //
                                  // callback =
                                  this);

        }

        public void scanFailed(Object ctx, Exception exception) {
            if (!checkConnected()) {
                return;
            }

            // wait for some time and then retry
            retryErroredSubscriberAfterDelay(this);
        }

        public void scanFinished(Object ctx, ReasonForFinish reason) {
            checkConnected();
        }

        /**
         * ===============================================================
         * {@link DeliveryCallback} methods
         */
        public void sendingFinished() {
            if (!isConnected()) {
                return;
            }

            synchronized (this) {
                lastLocalSeqIdDelivered = localSeqIdDeliveringNow;

                if (lastLocalSeqIdDelivered > lastSeqIdCommunicatedExternally + SEQ_ID_SLACK) {
                    // Note: The order of the next 2 statements is important. We should
                    // submit a request to change our delivery pointer only *after* we
                    // have actually changed it. Otherwise, there is a race condition
                    // with removal of this channel, w.r.t, maintaining the deliveryPtrs
                    // tree map.
                    long prevId = lastSeqIdCommunicatedExternally;
                    lastSeqIdCommunicatedExternally = lastLocalSeqIdDelivered;
                    moveDeliveryPtrForward(this, prevId, lastLocalSeqIdDelivered);
                }
            }
            // increment deliveried message
            ServerStats.getInstance().incrementMessagesDelivered();
            deliverNextMessage();
        }

        public synchronized long getLastSeqIdCommunicatedExternally() {
            return lastSeqIdCommunicatedExternally;
        }


        public void permanentErrorOnSend() {
            // the underlying channel is broken, the channel will
            // be closed in UmbrellaHandler when exception happened.
            // so we don't need to close the channel again
            stopServingSubscriber(topic, subscriberId, null,
                                  NOP_CALLBACK, null);
        }

        public void transientErrorOnSend() {
            retryErroredSubscriberAfterDelay(this);
        }

        /**
         * ===============================================================
         * {@link DeliveryManagerRequest} methods
         */
        public void performRequest() {
            // Put this subscriber in the channel to subscriber mapping
            ActiveSubscriberState prevSubscriber =
                subscriberStates.put(new TopicSubscriber(topic, subscriberId), this);

            // after put the active subscriber in subscriber states mapping
            // trigger the callback to tell it started to deliver the message
            // should let subscriber response go first before first delivered message.
            cb.operationFinished(ctx, (Void)null);

            if (prevSubscriber != null) {
                // we already in the delivery thread, we don't need to equeue a stop request
                // just stop it now, since stop is not blocking operation.
                // and also it cleans the old state of the active subscriber immediately.
                SubscriptionEvent se;
                if (deliveryEndPoint.equals(prevSubscriber.deliveryEndPoint)) {
                    logger.debug("Subscriber {} replaced a duplicated subscriber {} at same delivery point {}.",
                                 va(this, prevSubscriber, deliveryEndPoint));
                    se = null;
                } else {
                    logger.debug("Subscriber {} from delivery point {} forcelly closed delivery point {}.",
                                 va(this, deliveryEndPoint, prevSubscriber.deliveryEndPoint));
                    se = SubscriptionEvent.SUBSCRIPTION_FORCED_CLOSED;
                }
                doStopServingSubscriber(prevSubscriber, se);
            }

            synchronized (this) {
                lastSeqIdCommunicatedExternally = lastLocalSeqIdDelivered;
                addDeliveryPtr(this, lastLocalSeqIdDelivered);
            }

            deliverNextMessage();
        };

        @Override
        public String toString() {
            StringBuilder sb = new StringBuilder();
            sb.append("Topic: ");
            sb.append(topic.toStringUtf8());
            sb.append("Subscriber: ");
            sb.append(subscriberId.toStringUtf8());
            sb.append(", DeliveryPtr: ");
            sb.append(lastLocalSeqIdDelivered);
            return sb.toString();

        }
    }

    protected class StopServingSubscriber implements DeliveryManagerRequest {
        TopicSubscriber ts;
        SubscriptionEvent event;
        final Callback<Void> cb;
        final Object ctx;

        public StopServingSubscriber(ByteString topic, ByteString subscriberId,
                                     SubscriptionEvent event,
                                     Callback<Void> callback, Object ctx) {
            this.ts = new TopicSubscriber(topic, subscriberId);
            this.event = event;
            this.cb = callback;
            this.ctx = ctx;
        }

        @Override
        public void performRequest() {
            ActiveSubscriberState subscriber = subscriberStates.remove(ts);
            if (null != subscriber) {
                doStopServingSubscriber(subscriber, event);
            }
            cb.operationFinished(ctx, null);
        }

    }

    /**
     * Stop serving a subscriber. This method should be called in a
     * {@link DeliveryManagerRequest}.
     *
     * @param subscriber
     *          Active Subscriber to stop
     * @param event
     *          Subscription Event for the stop reason
     */
    private void doStopServingSubscriber(ActiveSubscriberState subscriber, SubscriptionEvent event) {
        // This will automatically stop delivery, and disconnect the channel
        subscriber.setNotConnected(event);

        // if the subscriber has moved on, a move request for its delivery
        // pointer must be pending in the request queue. Note that the
        // subscriber first changes its delivery pointer and then submits a
        // request to move so this works.
        removeDeliveryPtr(subscriber, subscriber.getLastSeqIdCommunicatedExternally(), //
                          // isAbsenceOk=
                          true,
                          // pruneTopic=
                          true);
    }

    protected class DeliveryPtrMove implements DeliveryManagerRequest {

        ActiveSubscriberState subscriber;
        Long oldSeqId;
        Long newSeqId;

        public DeliveryPtrMove(ActiveSubscriberState subscriber, Long oldSeqId, Long newSeqId) {
            this.subscriber = subscriber;
            this.oldSeqId = oldSeqId;
            this.newSeqId = newSeqId;
        }

        @Override
        public void performRequest() {
            ByteString topic = subscriber.getTopic();
            long prevMinSeqId = getMinimumSeqId(topic);

            if (subscriber.isConnected()) {
                removeDeliveryPtr(subscriber, oldSeqId, //
                                  // isAbsenceOk=
                                  false,
                                  // pruneTopic=
                                  false);

                addDeliveryPtr(subscriber, newSeqId);
            } else {
                removeDeliveryPtr(subscriber, oldSeqId, //
                                  // isAbsenceOk=
                                  true,
                                  // pruneTopic=
                                  true);
            }

            long nowMinSeqId = getMinimumSeqId(topic);

            if (nowMinSeqId > prevMinSeqId) {
                persistenceMgr.deliveredUntil(topic, nowMinSeqId);
            }
        }
    }

    protected class ShutdownDeliveryManagerRequest implements DeliveryManagerRequest {
        // This is a simple type of Request we will enqueue when the
        // PubSubServer is shut down and we want to stop the DeliveryManager
        // thread.
        public void performRequest() {
            keepRunning = false;
        }
    }

    /**
     * ====================================================================
     *
     * Dumb factories for our map methods
     */
    protected static class TreeMapLongToSetSubscriberFactory implements
        Factory<SortedMap<Long, Set<ActiveSubscriberState>>> {
        static TreeMapLongToSetSubscriberFactory instance = new TreeMapLongToSetSubscriberFactory();

        @Override
        public SortedMap<Long, Set<ActiveSubscriberState>> newInstance() {
            return new TreeMap<Long, Set<ActiveSubscriberState>>();
        }
    }

    protected static class HashMapSubscriberFactory implements Factory<Set<ActiveSubscriberState>> {
        static HashMapSubscriberFactory instance = new HashMapSubscriberFactory();

        @Override
        public Set<ActiveSubscriberState> newInstance() {
            return new HashSet<ActiveSubscriberState>();
        }
    }

    @Override
    public void onSubChannelDisconnected(TopicSubscriber topicSubscriber) {
        stopServingSubscriber(topicSubscriber.getTopic(), topicSubscriber.getSubscriberId(),
                null, NOP_CALLBACK, null);
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/handlers/BaseHandler.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.handlers;

import org.jboss.netty.channel.Channel;

import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.ServerNotResponsibleForTopicException;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;
import org.apache.hedwig.protoextensions.PubSubResponseUtils;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.netty.ServerStats;
import org.apache.hedwig.server.topics.TopicManager;
import org.apache.hedwig.util.Callback;
import org.apache.hedwig.util.HedwigSocketAddress;

public abstract class BaseHandler implements Handler {

    protected TopicManager topicMgr;
    protected ServerConfiguration cfg;

    protected BaseHandler(TopicManager tm, ServerConfiguration cfg) {
        this.topicMgr = tm;
        this.cfg = cfg;
    }


    public void handleRequest(final PubSubRequest request, final Channel channel) {
        topicMgr.getOwner(request.getTopic(), request.getShouldClaim(),
        new Callback<HedwigSocketAddress>() {
            @Override
            public void operationFailed(Object ctx, PubSubException exception) {
                channel.write(PubSubResponseUtils.getResponseForException(exception, request.getTxnId()));
                ServerStats.getInstance().getOpStats(request.getType()).incrementFailedOps();
            }

            @Override
            public void operationFinished(Object ctx, HedwigSocketAddress owner) {
                if (!owner.equals(cfg.getServerAddr())) {
                    channel.write(PubSubResponseUtils.getResponseForException(
                                      new ServerNotResponsibleForTopicException(owner.toString()), request.getTxnId()));
                    ServerStats.getInstance().incrementRequestsRedirect();
                    return;
                }
                handleRequestAtOwner(request, channel);
            }
        }, null);
    }

    public abstract void handleRequestAtOwner(PubSubRequest request, Channel channel);

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/handlers/ChannelDisconnectListener.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.handlers;

import org.jboss.netty.channel.Channel;

public interface ChannelDisconnectListener {

    /**
     * Act on a particular channel being disconnected
     * @param channel
     */
    public void channelDisconnected(Channel channel);
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/handlers/CloseSubscriptionHandler.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.handlers;

import org.jboss.netty.channel.Channel;
import org.jboss.netty.channel.ChannelFutureListener;

import com.google.protobuf.ByteString;

import org.apache.hedwig.client.data.TopicSubscriber;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.protocol.PubSubProtocol.CloseSubscriptionRequest;
import org.apache.hedwig.protocol.PubSubProtocol.OperationType;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEvent;
import org.apache.hedwig.protoextensions.PubSubResponseUtils;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.delivery.DeliveryManager;
import org.apache.hedwig.server.netty.ServerStats;
import org.apache.hedwig.server.netty.ServerStats.OpStats;
import org.apache.hedwig.server.netty.UmbrellaHandler;
import org.apache.hedwig.server.subscriptions.SubscriptionManager;
import org.apache.hedwig.server.topics.TopicManager;
import org.apache.hedwig.util.Callback;

public class CloseSubscriptionHandler extends BaseHandler {
    SubscriptionManager subMgr;
    DeliveryManager deliveryMgr;
    SubscriptionChannelManager subChannelMgr;
    // op stats
    final OpStats closesubStats;

    public CloseSubscriptionHandler(ServerConfiguration cfg, TopicManager tm,
                                    SubscriptionManager subMgr,
                                    DeliveryManager deliveryMgr,
                                    SubscriptionChannelManager subChannelMgr) {
        super(tm, cfg);
        this.subMgr = subMgr;
        this.deliveryMgr = deliveryMgr;
        this.subChannelMgr = subChannelMgr;
        closesubStats = ServerStats.getInstance().getOpStats(OperationType.CLOSESUBSCRIPTION);
    }

    @Override
    public void handleRequestAtOwner(final PubSubRequest request, final Channel channel) {
        if (!request.hasCloseSubscriptionRequest()) {
            UmbrellaHandler.sendErrorResponseToMalformedRequest(channel, request.getTxnId(),
                    "Missing closesubscription request data");
            closesubStats.incrementFailedOps();
            return;
        }

        final CloseSubscriptionRequest closesubRequest =
                request.getCloseSubscriptionRequest();
        final ByteString topic = request.getTopic();
        final ByteString subscriberId = closesubRequest.getSubscriberId();

        final long requestTime = System.currentTimeMillis();

        subMgr.closeSubscription(topic, subscriberId, new Callback<Void>() {
            @Override
            public void operationFinished(Object ctx, Void result) {
                // we should not close the channel in delivery manager
                // since client waits the response for closeSubscription request
                // client side would close the channel
                deliveryMgr.stopServingSubscriber(topic, subscriberId, null,
                new Callback<Void>() {
                    @Override
                    public void operationFailed(Object ctx, PubSubException exception) {
                        channel.write(PubSubResponseUtils.getResponseForException(exception, request.getTxnId()));
                        closesubStats.incrementFailedOps();
                    }
                    @Override
                    public void operationFinished(Object ctx, Void resultOfOperation) {
                        // remove the topic subscription from subscription channels
                        subChannelMgr.remove(new TopicSubscriber(topic, subscriberId),
                                             channel);
                        channel.write(PubSubResponseUtils.getSuccessResponse(request.getTxnId()));
                        closesubStats.updateLatency(System.currentTimeMillis() - requestTime);
                    }
                }, null);
            }
            @Override
            public void operationFailed(Object ctx, PubSubException exception) {
                channel.write(PubSubResponseUtils.getResponseForException(exception, request.getTxnId()));
                closesubStats.incrementFailedOps();
            }
        }, null);
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/handlers/ConsumeHandler.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.handlers;

import org.jboss.netty.channel.Channel;

import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest;
import org.apache.hedwig.protocol.PubSubProtocol.OperationType;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.netty.ServerStats;
import org.apache.hedwig.server.netty.UmbrellaHandler;
import org.apache.hedwig.server.netty.ServerStats.OpStats;
import org.apache.hedwig.server.subscriptions.SubscriptionManager;
import org.apache.hedwig.server.topics.TopicManager;
import org.apache.hedwig.util.Callback;

public class ConsumeHandler extends BaseHandler {

    SubscriptionManager sm;
    Callback<Void> noopCallback = new NoopCallback<Void>();
    final OpStats consumeStats = ServerStats.getInstance().getOpStats(OperationType.CONSUME);

    class NoopCallback<T> implements Callback<T> {
        @Override
        public void operationFailed(Object ctx, PubSubException exception) {
            consumeStats.incrementFailedOps();
        }

        public void operationFinished(Object ctx, T resultOfOperation) {
            // we don't collect consume process time
            consumeStats.updateLatency(0);
        };
    }

    @Override
    public void handleRequestAtOwner(PubSubRequest request, Channel channel) {
        if (!request.hasConsumeRequest()) {
            UmbrellaHandler.sendErrorResponseToMalformedRequest(channel, request.getTxnId(),
                    "Missing consume request data");
            consumeStats.incrementFailedOps();
            return;
        }

        ConsumeRequest consumeRequest = request.getConsumeRequest();

        sm.setConsumeSeqIdForSubscriber(request.getTopic(), consumeRequest.getSubscriberId(),
                                        consumeRequest.getMsgId(), noopCallback, null);

    }

    public ConsumeHandler(TopicManager tm, SubscriptionManager sm, ServerConfiguration cfg) {
        super(tm, cfg);
        this.sm = sm;
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/handlers/Handler.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.handlers;

import org.jboss.netty.channel.Channel;

import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;

public interface Handler {

    /**
     * Handle a request synchronously or asynchronously. After handling the
     * request, the appropriate response should be written on the given channel
     *
     * @param request
     *            The request to handle
     *
     * @param channel
     *            The channel on which to write the response
     */
    public void handleRequest(final PubSubRequest request, final Channel channel);
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/handlers/NettyHandlerBean.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hedwig.server.handlers;

import org.apache.hedwig.server.handlers.SubscriptionChannelManager;
import org.apache.hedwig.server.jmx.HedwigMBeanInfo;

public class NettyHandlerBean implements NettyHandlerMXBean, HedwigMBeanInfo {

    SubscriptionChannelManager subChannelMgr;

   public NettyHandlerBean(SubscriptionChannelManager subChannelMgr) {
       this.subChannelMgr = subChannelMgr;
    }

    @Override
    public String getName() {
        return "NettyHandlers";
    }

    @Override
    public boolean isHidden() {
        return false;
    }

    @Override
    public int getNumSubscriptionChannels() {
        return subChannelMgr.getNumSubscriptionChannels();
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/handlers/NettyHandlerMXBean.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hedwig.server.handlers;

/**
 * Netty Handler MBean
 */
public interface NettyHandlerMXBean {

    /**
     * @return number of subscription channels
     */
    public int getNumSubscriptionChannels();

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/handlers/PublishHandler.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.handlers;

import org.apache.hedwig.protocol.PubSubProtocol;
import org.jboss.netty.channel.Channel;
import org.apache.bookkeeper.util.MathUtils;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.OperationType;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;
import org.apache.hedwig.protoextensions.PubSubResponseUtils;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.netty.ServerStats;
import org.apache.hedwig.server.netty.ServerStats.OpStats;
import org.apache.hedwig.server.netty.UmbrellaHandler;
import org.apache.hedwig.server.persistence.PersistRequest;
import org.apache.hedwig.server.persistence.PersistenceManager;
import org.apache.hedwig.server.topics.TopicManager;
import org.apache.hedwig.util.Callback;

public class PublishHandler extends BaseHandler {

    private PersistenceManager persistenceMgr;
    private final OpStats pubStats;

    public PublishHandler(TopicManager topicMgr, PersistenceManager persistenceMgr, ServerConfiguration cfg) {
        super(topicMgr, cfg);
        this.persistenceMgr = persistenceMgr;
        this.pubStats = ServerStats.getInstance().getOpStats(OperationType.PUBLISH);
    }

    @Override
    public void handleRequestAtOwner(final PubSubRequest request, final Channel channel) {
        if (!request.hasPublishRequest()) {
            UmbrellaHandler.sendErrorResponseToMalformedRequest(channel, request.getTxnId(),
                    "Missing publish request data");
            pubStats.incrementFailedOps();
            return;
        }

        Message msgToSerialize = Message.newBuilder(request.getPublishRequest().getMsg()).setSrcRegion(
                                     cfg.getMyRegionByteString()).build();

        final long requestTime = MathUtils.now();
        PersistRequest persistRequest = new PersistRequest(request.getTopic(), msgToSerialize,
        new Callback<PubSubProtocol.MessageSeqId>() {
            @Override
            public void operationFailed(Object ctx, PubSubException exception) {
                channel.write(PubSubResponseUtils.getResponseForException(exception, request.getTxnId()));
                pubStats.incrementFailedOps();
            }

            @Override
            public void operationFinished(Object ctx, PubSubProtocol.MessageSeqId resultOfOperation) {
                channel.write(getSuccessResponse(request.getTxnId(), resultOfOperation));
                pubStats.updateLatency(MathUtils.now() - requestTime);
            }
        }, null);

        persistenceMgr.persistMessage(persistRequest);
    }

    private static PubSubProtocol.PubSubResponse getSuccessResponse(long txnId, PubSubProtocol.MessageSeqId publishedMessageSeqId) {
        if (null == publishedMessageSeqId) {
            return PubSubResponseUtils.getSuccessResponse(txnId);
        }
        PubSubProtocol.PublishResponse publishResponse = PubSubProtocol.PublishResponse.newBuilder().setPublishedMsgId(publishedMessageSeqId).build();
        PubSubProtocol.ResponseBody responseBody = PubSubProtocol.ResponseBody.newBuilder().setPublishResponse(publishResponse).build();
        return PubSubProtocol.PubSubResponse.newBuilder().
            setProtocolVersion(PubSubResponseUtils.serverVersion).
            setStatusCode(PubSubProtocol.StatusCode.SUCCESS).setTxnId(txnId).
            setResponseBody(responseBody).build();
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/handlers/SubscribeHandler.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.handlers;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;
import org.jboss.netty.channel.ChannelFuture;
import org.jboss.netty.channel.ChannelFutureListener;

import com.google.protobuf.ByteString;

import org.apache.bookkeeper.util.MathUtils;
import org.apache.bookkeeper.util.ReflectionUtils;
import org.apache.hedwig.client.data.TopicSubscriber;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.ServerNotResponsibleForTopicException;
import org.apache.hedwig.filter.PipelineFilter;
import org.apache.hedwig.filter.ServerMessageFilter;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.OperationType;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;
import org.apache.hedwig.protocol.PubSubProtocol.ResponseBody;
import org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest;
import org.apache.hedwig.protocol.PubSubProtocol.SubscribeResponse;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData;
import org.apache.hedwig.protoextensions.PubSubResponseUtils;
import org.apache.hedwig.protoextensions.SubscriptionStateUtils;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.delivery.ChannelEndPoint;
import org.apache.hedwig.server.delivery.DeliveryManager;
import org.apache.hedwig.server.netty.ServerStats;
import org.apache.hedwig.server.netty.ServerStats.OpStats;
import org.apache.hedwig.server.netty.UmbrellaHandler;
import org.apache.hedwig.server.persistence.PersistenceManager;
import org.apache.hedwig.server.subscriptions.SubscriptionManager;
import org.apache.hedwig.server.subscriptions.AllToAllTopologyFilter;
import org.apache.hedwig.server.topics.TopicManager;
import org.apache.hedwig.util.Callback;
import static org.apache.hedwig.util.VarArgs.va;

public class SubscribeHandler extends BaseHandler {
    static Logger logger = LoggerFactory.getLogger(SubscribeHandler.class);

    private final DeliveryManager deliveryMgr;
    private final PersistenceManager persistenceMgr;
    private final SubscriptionManager subMgr;
    private final SubscriptionChannelManager subChannelMgr;

    // op stats
    private final OpStats subStats;

    public SubscribeHandler(ServerConfiguration cfg, TopicManager topicMgr,
                            DeliveryManager deliveryManager,
                            PersistenceManager persistenceMgr,
                            SubscriptionManager subMgr,
                            SubscriptionChannelManager subChannelMgr) {
        super(topicMgr, cfg);
        this.deliveryMgr = deliveryManager;
        this.persistenceMgr = persistenceMgr;
        this.subMgr = subMgr;
        this.subChannelMgr = subChannelMgr;
        subStats = ServerStats.getInstance().getOpStats(OperationType.SUBSCRIBE);
    }

    @Override
    public void handleRequestAtOwner(final PubSubRequest request, final Channel channel) {

        if (!request.hasSubscribeRequest()) {
            UmbrellaHandler.sendErrorResponseToMalformedRequest(channel, request.getTxnId(),
                    "Missing subscribe request data");
            subStats.incrementFailedOps();
            return;
        }

        final ByteString topic = request.getTopic();

        MessageSeqId seqId;
        try {
            seqId = persistenceMgr.getCurrentSeqIdForTopic(topic);
        } catch (ServerNotResponsibleForTopicException e) {
            channel.write(PubSubResponseUtils.getResponseForException(e, request.getTxnId())).addListener(
                ChannelFutureListener.CLOSE);
            logger.error("Error getting current seq id for topic " + topic.toStringUtf8()
                       + " when processing subscribe request (txnid:" + request.getTxnId() + ") :", e);
            subStats.incrementFailedOps();
            ServerStats.getInstance().incrementRequestsRedirect();
            return;
        }

        final SubscribeRequest subRequest = request.getSubscribeRequest();
        final ByteString subscriberId = subRequest.getSubscriberId();

        MessageSeqId lastSeqIdPublished = MessageSeqId.newBuilder(seqId).setLocalComponent(seqId.getLocalComponent()).build();

        final long requestTime = MathUtils.now();
        subMgr.serveSubscribeRequest(topic, subRequest, lastSeqIdPublished, new Callback<SubscriptionData>() {

            @Override
            public void operationFailed(Object ctx, PubSubException exception) {
                channel.write(PubSubResponseUtils.getResponseForException(exception, request.getTxnId())).addListener(
                    ChannelFutureListener.CLOSE);
                logger.error("Error serving subscribe request (" + request.getTxnId() + ") for (topic: "
                           + topic.toStringUtf8() + " , subscriber: " + subscriberId.toStringUtf8() + ")", exception);
                subStats.incrementFailedOps();
            }

            @Override
            public void operationFinished(Object ctx, final SubscriptionData subData) {

                TopicSubscriber topicSub = new TopicSubscriber(topic, subscriberId);
                synchronized (channel) {
                    if (!channel.isConnected()) {
                        // channel got disconnected while we were processing the
                        // subscribe request,
                        // nothing much we can do in this case
                        subStats.incrementFailedOps();
                        return;
                    }
                }
                // initialize the message filter
                PipelineFilter filter = new PipelineFilter();
                try {
                    // the filter pipeline should be
                    // 1) AllToAllTopologyFilter to filter cross-region messages
                    filter.addLast(new AllToAllTopologyFilter());
                    // 2) User-Customized MessageFilter
                    if (subData.hasPreferences() &&
                        subData.getPreferences().hasMessageFilter()) {
                        String messageFilterName = subData.getPreferences().getMessageFilter();
                        filter.addLast(ReflectionUtils.newInstance(messageFilterName, ServerMessageFilter.class));
                    }
                    // initialize the filter
                    filter.initialize(cfg.getConf());
                    filter.setSubscriptionPreferences(topic, subscriberId,
                                                      subData.getPreferences());
                } catch (RuntimeException re) {
                    String errMsg = "RuntimeException caught when instantiating message filter for (topic:"
                                  + topic.toStringUtf8() + ", subscriber:" + subscriberId.toStringUtf8() + ")."
                                  + "It might be introduced by programming error in message filter.";
                    logger.error(errMsg, re);
                    PubSubException pse = new PubSubException.InvalidMessageFilterException(errMsg, re);
                    subStats.incrementFailedOps();
                    // we should not close the subscription channel, just response error
                    // client decide to close it or not.
                    channel.write(PubSubResponseUtils.getResponseForException(pse, request.getTxnId()));
                    return;
                } catch (Throwable t) {
                    String errMsg = "Failed to instantiate message filter for (topic:" + topic.toStringUtf8()
                                  + ", subscriber:" + subscriberId.toStringUtf8() + ").";
                    logger.error(errMsg, t);
                    PubSubException pse = new PubSubException.InvalidMessageFilterException(errMsg, t);
                    subStats.incrementFailedOps();
                    channel.write(PubSubResponseUtils.getResponseForException(pse, request.getTxnId()))
                    .addListener(ChannelFutureListener.CLOSE);
                    return;
                }
                boolean forceAttach = false;
                if (subRequest.hasForceAttach()) {
                    forceAttach = subRequest.getForceAttach();
                }
                // Try to store the subscription channel for the topic subscriber
                Channel oldChannel = subChannelMgr.put(topicSub, channel, forceAttach);
                if (null != oldChannel) {
                    PubSubException pse = new PubSubException.TopicBusyException(
                        "Subscriber " + subscriberId.toStringUtf8() + " for topic " + topic.toStringUtf8()
                        + " is already being served on a different channel " + oldChannel + ".");
                    subStats.incrementFailedOps();
                    channel.write(PubSubResponseUtils.getResponseForException(pse, request.getTxnId()))
                    .addListener(ChannelFutureListener.CLOSE);
                    return;
                }

                // want to start 1 ahead of the consume ptr
                MessageSeqId lastConsumedSeqId = subData.getState().getMsgId();
                MessageSeqId seqIdToStartFrom = MessageSeqId.newBuilder(lastConsumedSeqId).setLocalComponent(
                                                    lastConsumedSeqId.getLocalComponent() + 1).build();
                deliveryMgr.startServingSubscription(topic, subscriberId,
                        subData.getPreferences(), seqIdToStartFrom, new ChannelEndPoint(channel), filter,
                        new Callback<Void>() {
                            @Override
                            public void operationFinished(Object ctx, Void result) {
                                // First write success and then tell the delivery manager,
                                // otherwise the first message might go out before the response
                                // to the subscribe
                                SubscribeResponse.Builder subRespBuilder = SubscribeResponse.newBuilder()
                                    .setPreferences(subData.getPreferences());
                                ResponseBody respBody = ResponseBody.newBuilder()
                                    .setSubscribeResponse(subRespBuilder).build();
                                channel.write(PubSubResponseUtils.getSuccessResponse(request.getTxnId(), respBody));
                                logger.info("Subscribe request (" + request.getTxnId() + ") for (topic:"
                                            + topic.toStringUtf8() + ", subscriber:" + subscriberId.toStringUtf8()
                                            + ") from channel " + channel.getRemoteAddress()
                                            + " succeed - its subscription data is "
                                            + SubscriptionStateUtils.toString(subData));
                                subStats.updateLatency(MathUtils.now() - requestTime);
                            }
                            @Override
                            public void operationFailed(Object ctx, PubSubException exception) {
                                // would not happened
                            }
                        }, null);
            }
        }, null);

    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/handlers/SubscriptionChannelManager.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.handlers;

import java.util.ArrayList;
import java.util.HashSet;
import java.util.List;
import java.util.Set;
import java.util.concurrent.ConcurrentHashMap;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;
import org.jboss.netty.channel.ChannelFuture;
import org.jboss.netty.channel.ChannelFutureListener;

import org.apache.hedwig.client.data.TopicSubscriber;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEvent;
import org.apache.hedwig.protoextensions.PubSubResponseUtils;
import org.apache.hedwig.util.Callback;
import static org.apache.hedwig.util.VarArgs.va;

public class SubscriptionChannelManager implements ChannelDisconnectListener {

    static Logger logger = LoggerFactory.getLogger(SubscriptionChannelManager.class);

    static class CloseSubscriptionListener implements ChannelFutureListener {

        final TopicSubscriber ts;

        CloseSubscriptionListener(TopicSubscriber topicSubscriber) {
            this.ts = topicSubscriber;
        }

        @Override
        public void operationComplete(ChannelFuture future) throws Exception {
            if (!future.isSuccess()) {
                logger.warn("Failed to write response to close old subscription {}.", ts);
            } else {
                logger.debug("Close old subscription {} succeed.", ts);
            }
        }
    };

    final List<SubChannelDisconnectedListener> listeners;

    public interface SubChannelDisconnectedListener {
        /**
         * Act on a particular topicSubscriber being disconnected
         * @param topicSubscriber
         */
        public void onSubChannelDisconnected(TopicSubscriber topicSubscriber);
    }

    final ConcurrentHashMap<TopicSubscriber, Channel> sub2Channel;
    final ConcurrentHashMap<Channel, Set<TopicSubscriber>> channel2sub;

    public SubscriptionChannelManager() {
        sub2Channel = new ConcurrentHashMap<TopicSubscriber, Channel>();
        channel2sub = new ConcurrentHashMap<Channel, Set<TopicSubscriber>>();
        listeners = new ArrayList<SubChannelDisconnectedListener>();
    }

    public void addSubChannelDisconnectedListener(SubChannelDisconnectedListener listener) {
        if (null != listener) {
            listeners.add(listener);
        }
    }

    @Override
    public void channelDisconnected(Channel channel) {
        // Evils of synchronized programming: there is a race between a channel
        // getting disconnected, and us adding it to the maps when a subscribe
        // succeeds
        Set<TopicSubscriber> topicSubs;
        synchronized (channel) {
            topicSubs = channel2sub.remove(channel);
        }
        if (topicSubs != null) {
            for (TopicSubscriber topicSub : topicSubs) {
                logger.info("Subscription channel {} for {} is disconnected.",
                            va(channel.getRemoteAddress(), topicSub));
                // remove entry only currently mapped to given value.
                sub2Channel.remove(topicSub, channel);
                for (SubChannelDisconnectedListener listener : listeners) {
                    listener.onSubChannelDisconnected(topicSub);
                }
            }
        }
    }

    public int getNumSubscriptionChannels() {
        return channel2sub.size();
    }

    public int getNumSubscriptions() {
        return sub2Channel.size();
    }

    /**
     * Put <code>topicSub</code> on Channel <code>channel</code>.
     *
     * @param topicSub
     *          Topic Subscription
     * @param channel
     *          Netty channel
     * @param mode
     *          Create or Attach mode
     * @return null succeed, otherwise the old existed channel.
     */
    public Channel put(TopicSubscriber topicSub, Channel channel, boolean forceAttach) {
        // race with channel getting disconnected while we are adding it
        // to the 2 maps
        synchronized (channel) {
            Channel oldChannel = sub2Channel.putIfAbsent(topicSub, channel);
            // if a subscribe request send from same channel,
            // we treated it a success action.
            if (null != oldChannel && !oldChannel.equals(channel)) {
                boolean subSuccess = false;
                if (forceAttach) {
                    // it is safe to close old subscription here since the new subscription
                    // has come from other channel succeed.
                    synchronized (oldChannel) {
                        Set<TopicSubscriber> oldTopicSubs = channel2sub.get(oldChannel);
                        if (null != oldTopicSubs) {
                            if (!oldTopicSubs.remove(topicSub)) {
                                logger.warn("Failed to remove old subscription ({}) due to it isn't on channel ({}).",
                                            va(topicSub, oldChannel));
                            } else if (oldTopicSubs.isEmpty()) {
                                channel2sub.remove(oldChannel);
                            }
                        }
                    }
                    PubSubResponse resp = PubSubResponseUtils.getResponseForSubscriptionEvent(
                        topicSub.getTopic(), topicSub.getSubscriberId(),
                        SubscriptionEvent.SUBSCRIPTION_FORCED_CLOSED
                    );
                    oldChannel.write(resp).addListener(new CloseSubscriptionListener(topicSub));
                    logger.info("Subscribe request for ({}) from channel ({}) closes old subscripiton on channel ({}).",
                                va(topicSub, channel, oldChannel));
                    // try replace the oldChannel
                    // if replace failure, it migth caused because channelDisconnect callback
                    // has removed the old channel.
                    if (!sub2Channel.replace(topicSub, oldChannel, channel)) {
                        // try to add it now.
                        // if add failure, it means other one has obtained the channel
                        oldChannel = sub2Channel.putIfAbsent(topicSub, channel);
                        if (null == oldChannel) {
                            subSuccess = true;
                        }
                    } else {
                        subSuccess = true;
                    }
                }
                if (!subSuccess) {
                    logger.error("Error serving subscribe request for ({}) from ({}) since it already served on ({}).",
                                 va(topicSub, channel, oldChannel));
                    return oldChannel;
                }
            }
            // channel2sub is just a cache, so we can add to it
            // without synchronization
            Set<TopicSubscriber> topicSubs = channel2sub.get(channel);
            if (null == topicSubs) {
                topicSubs = new HashSet<TopicSubscriber>();
                channel2sub.put(channel, topicSubs); 
            }
            topicSubs.add(topicSub);
            return null;
        }
    }

    /**
     * Remove <code>topicSub</code> from Channel <code>channel</code>
     *
     * @param topicSub
     *          Topic Subscription
     * @param channel
     *          Netty channel
     */
    public void remove(TopicSubscriber topicSub, Channel channel) {
        synchronized (channel) {
            Set<TopicSubscriber> topicSubs = channel2sub.get(channel);
            if (null != topicSubs) {
                if (!topicSubs.remove(topicSub)) {
                    logger.warn("Failed to remove subscription ({}) due to it isn't on channel ({}).",
                                va(topicSub, channel));
                } else if (topicSubs.isEmpty()) {
                    channel2sub.remove(channel);
                }
            }
            if (!sub2Channel.remove(topicSub, channel)) {
                logger.warn("Failed to remove channel ({}) due to it isn't ({})'s channel.",
                            va(channel, topicSub));
            }
        }
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/handlers/UnsubscribeHandler.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.handlers;

import org.jboss.netty.channel.Channel;
import com.google.protobuf.ByteString;

import org.apache.bookkeeper.util.MathUtils;
import org.apache.hedwig.client.data.TopicSubscriber;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.protocol.PubSubProtocol.OperationType;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEvent;
import org.apache.hedwig.protocol.PubSubProtocol.UnsubscribeRequest;
import org.apache.hedwig.protoextensions.PubSubResponseUtils;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.delivery.DeliveryManager;
import org.apache.hedwig.server.netty.ServerStats;
import org.apache.hedwig.server.netty.ServerStats.OpStats;
import org.apache.hedwig.server.netty.UmbrellaHandler;
import org.apache.hedwig.server.subscriptions.SubscriptionManager;
import org.apache.hedwig.server.topics.TopicManager;
import org.apache.hedwig.util.Callback;
import static org.apache.hedwig.util.VarArgs.va;

public class UnsubscribeHandler extends BaseHandler {
    SubscriptionManager subMgr;
    DeliveryManager deliveryMgr;
    SubscriptionChannelManager subChannelMgr;
    // op stats
    final OpStats unsubStats;

    public UnsubscribeHandler(ServerConfiguration cfg,
                              TopicManager tm,
                              SubscriptionManager subMgr,
                              DeliveryManager deliveryMgr,
                              SubscriptionChannelManager subChannelMgr) {
        super(tm, cfg);
        this.subMgr = subMgr;
        this.deliveryMgr = deliveryMgr;
        this.subChannelMgr = subChannelMgr;
        unsubStats = ServerStats.getInstance().getOpStats(OperationType.UNSUBSCRIBE);
    }

    @Override
    public void handleRequestAtOwner(final PubSubRequest request, final Channel channel) {
        if (!request.hasUnsubscribeRequest()) {
            UmbrellaHandler.sendErrorResponseToMalformedRequest(channel, request.getTxnId(),
                    "Missing unsubscribe request data");
            unsubStats.incrementFailedOps();
            return;
        }

        final UnsubscribeRequest unsubRequest = request.getUnsubscribeRequest();
        final ByteString topic = request.getTopic();
        final ByteString subscriberId = unsubRequest.getSubscriberId();

        final long requestTime = MathUtils.now();
        subMgr.unsubscribe(topic, subscriberId, new Callback<Void>() {
            @Override
            public void operationFailed(Object ctx, PubSubException exception) {
                channel.write(PubSubResponseUtils.getResponseForException(exception, request.getTxnId()));
                unsubStats.incrementFailedOps();
            }

            @Override
            public void operationFinished(Object ctx, Void resultOfOperation) {
                // we should not close the channel in delivery manager
                // since client waits the response for closeSubscription request
                // client side would close the channel
                deliveryMgr.stopServingSubscriber(topic, subscriberId, null,
                new Callback<Void>() {
                    @Override
                    public void operationFailed(Object ctx, PubSubException exception) {
                        channel.write(PubSubResponseUtils.getResponseForException(exception, request.getTxnId()));
                        unsubStats.incrementFailedOps();
                    }
                    @Override
                    public void operationFinished(Object ctx, Void resultOfOperation) {
                        // remove the topic subscription from subscription channels
                        subChannelMgr.remove(new TopicSubscriber(topic, subscriberId),
                                             channel);
                        channel.write(PubSubResponseUtils.getSuccessResponse(request.getTxnId()));
                        unsubStats.updateLatency(System.currentTimeMillis() - requestTime);
                    }
                }, ctx);
            }
        }, null);

    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/jmx/HedwigJMXService.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hedwig.server.jmx;

/**
 * An implementor of this interface is basiclly responsible for jmx beans.
 */
public interface HedwigJMXService {
    /**
     * register jmx
     *
     * @param parent
     *          Parent JMX Bean
     */
    public void registerJMX(HedwigMBeanInfo parent);

    /**
     * unregister jmx
     */
    public void unregisterJMX();
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/jmx/HedwigMBeanInfo.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hedwig.server.jmx;

import org.apache.zookeeper.jmx.ZKMBeanInfo;

/**
 * Hedwig MBean info interface.
 */
public interface HedwigMBeanInfo extends ZKMBeanInfo {
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/jmx/HedwigMBeanRegistry.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hedwig.server.jmx;

import javax.management.MalformedObjectNameException;
import javax.management.ObjectName;

import org.apache.bookkeeper.jmx.BKMBeanRegistry;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * This class provides a unified interface for registering/unregistering of
 * Hedwig MBeans with the platform MBean server.
 */
public class HedwigMBeanRegistry extends BKMBeanRegistry {

    static final String SERVICE = "org.apache.HedwigServer";

    static HedwigMBeanRegistry instance = new HedwigMBeanRegistry();

    public static HedwigMBeanRegistry getInstance(){
        return instance;
    }

    @Override
    protected String getDomainName() {
        return SERVICE;
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/meta/FactoryLayout.java,false,"package org.apache.hedwig.server.meta;

/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.BufferedReader;
import java.io.IOException;
import java.io.StringReader;

import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.ZooDefs.Ids;
import org.apache.zookeeper.ZooKeeper;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.protobuf.TextFormat;
import com.google.protobuf.InvalidProtocolBufferException;
import org.apache.hedwig.protocol.PubSubProtocol.ManagerMeta;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.zookeeper.ZkUtils;

/**
 * This class encapsulates metadata manager layout information
 * that is persistently stored in zookeeper.
 * It provides parsing and serialization methods of such information.
 *
 */
public class FactoryLayout {
    static final Logger logger = LoggerFactory.getLogger(FactoryLayout.class);

    // metadata manager name
    public static final String NAME = "METADATA";
    // Znode name to store layout information
    public static final String LAYOUT_ZNODE = "LAYOUT";
    public static final String LSEP = "\n";

    private ManagerMeta managerMeta;

    /**
     * Construct metadata manager factory layout.
     *
     * @param meta
     *          Meta describes what kind of factory used.
     */
    public FactoryLayout(ManagerMeta meta) {
        this.managerMeta = meta;
    }

    public static String getFactoryLayoutPath(StringBuilder sb, ServerConfiguration cfg) {
        return cfg.getZkManagersPrefix(sb).append("/").append(NAME)
               .append("/").append(LAYOUT_ZNODE).toString();
    }

    public ManagerMeta getManagerMeta() {
        return managerMeta;
    }

    /**
     * Store the factory layout into zookeeper
     *
     * @param zk
     *          ZooKeeper Handle
     * @param cfg
     *          Server Configuration Object
     * @throws KeeperException
     * @throws IOException
     * @throws InterruptedException
     */
    public void store(ZooKeeper zk, ServerConfiguration cfg)
    throws KeeperException, IOException, InterruptedException {
        String factoryLayoutPath = getFactoryLayoutPath(new StringBuilder(), cfg);

        byte[] layoutData = TextFormat.printToString(managerMeta).getBytes();
        ZkUtils.createFullPathOptimistic(zk, factoryLayoutPath, layoutData,
                                         Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
    }

    @Override
    public int hashCode() {
        return managerMeta.hashCode();
    }

    @Override
    public boolean equals(Object o) {
        if (null == o ||
            !(o instanceof FactoryLayout)) {
            return false;
        }
        FactoryLayout other = (FactoryLayout)o;
        return managerMeta.equals(other.managerMeta);
    }

    @Override
    public String toString() {
        return TextFormat.printToString(managerMeta);
    }

    /**
     * Read factory layout from zookeeper
     *
     * @param zk
     *          ZooKeeper Client
     * @param cfg
     *          Server configuration object
     * @return Factory layout, or null if none set in zookeeper
     */
    public static FactoryLayout readLayout(final ZooKeeper zk,
                                           final ServerConfiguration cfg)
    throws IOException, KeeperException {
        String factoryLayoutPath = getFactoryLayoutPath(new StringBuilder(), cfg);
        byte[] layoutData;
        try {
            layoutData = zk.getData(factoryLayoutPath, false, null);
        } catch (KeeperException.NoNodeException nne) {
            return null;
        } catch (InterruptedException ie) {
            throw new IOException(ie);
        }
        ManagerMeta meta;
        try {
            BufferedReader reader = new BufferedReader(
                new StringReader(new String(layoutData)));
            ManagerMeta.Builder metaBuilder = ManagerMeta.newBuilder();
            TextFormat.merge(reader, metaBuilder);
            meta = metaBuilder.build();
        } catch (InvalidProtocolBufferException ipbe) {
            throw new IOException("Corrupted factory layout : ", ipbe);
        }

        return new FactoryLayout(meta);
    }

    /**
     * Remove the factory layout from ZooKeeper.
     *
     * @param zk
     *          ZooKeeper instance
     * @param cfg
     *          Server configuration object
     * @throws KeeperException
     * @throws InterruptedException
     */
    public static void deleteLayout(ZooKeeper zk, ServerConfiguration cfg)
            throws KeeperException, InterruptedException {
        String factoryLayoutPath = getFactoryLayoutPath(new StringBuilder(), cfg);
        zk.delete(factoryLayoutPath, -1);
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/meta/MetadataManagerFactory.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.meta;

import java.io.IOException;
import java.util.Iterator;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.bookkeeper.util.ReflectionUtils;
import org.apache.hedwig.protocol.PubSubProtocol.ManagerMeta;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.ZooKeeper;

import com.google.protobuf.ByteString;

/**
 * Metadata Manager used to manage metadata used by hedwig.
 */
public abstract class MetadataManagerFactory {

    static final Logger LOG = LoggerFactory.getLogger(MetadataManagerFactory.class);

    /**
     * Return current factory version.
     *
     * @return current version used by factory.
     */
    public abstract int getCurrentVersion();

    /**
     * Initialize the metadata manager factory with given
     * configuration and version.
     *
     * @param cfg
     *          Server configuration object
     * @param zk
     *          ZooKeeper handler
     * @param version
     *          Manager version
     * @return metadata manager factory
     * @throws IOException when fail to initialize the manager.
     */
    protected abstract MetadataManagerFactory initialize(
        ServerConfiguration cfg, ZooKeeper zk, int version)
    throws IOException;

    /**
     * Uninitialize the factory.
     *
     * @throws IOException when fail to shutdown the factory.
     */
    public abstract void shutdown() throws IOException;

    /**
     * Iterate over the topics list.
     * Used by HedwigConsole to list available topics.
     *
     * @return iterator of the topics list.
     * @throws IOException
     */
    public abstract Iterator<ByteString> getTopics() throws IOException;

    /**
     * Create topic persistence manager.
     *
     * @return topic persistence manager
     */
    public abstract TopicPersistenceManager newTopicPersistenceManager();

    /**
     * Create subscription data manager.
     *
     * @return subscription data manager.
     */
    public abstract SubscriptionDataManager newSubscriptionDataManager();

    /**
     * Create topic ownership manager.
     *
     * @return topic ownership manager.
     */
    public abstract TopicOwnershipManager newTopicOwnershipManager();

    /**
     * Format the metadata for Hedwig.
     *
     * @param cfg
     *          Configuration instance
     * @param zk
     *          ZooKeeper instance
     */
    public abstract void format(ServerConfiguration cfg, ZooKeeper zk) throws IOException;

    /**
     * Create new Metadata Manager Factory.
     *
     * @param conf
     *          Configuration Object.
     * @param zk
     *          ZooKeeper Client Handle, talk to zk to know which manager factory is used.
     * @return new manager factory.
     * @throws IOException
     */
    public static MetadataManagerFactory newMetadataManagerFactory(
        final ServerConfiguration conf, final ZooKeeper zk)
    throws IOException, KeeperException, InterruptedException {
        Class<? extends MetadataManagerFactory> factoryClass;
        try {
            factoryClass = conf.getMetadataManagerFactoryClass();
        } catch (Exception e) {
            throw new IOException("Failed to get metadata manager factory class from configuration : ", e);
        }
        // check that the configured manager is
        // compatible with the existing layout
        FactoryLayout layout = FactoryLayout.readLayout(zk, conf);
        if (layout == null) { // no existing layout
            return createMetadataManagerFactory(conf, zk, factoryClass);
        }
        LOG.debug("read meta layout {}", layout);

        if (factoryClass != null &&
            !layout.getManagerMeta().getManagerImpl().equals(factoryClass.getName())) {
            throw new IOException("Configured metadata manager factory " + factoryClass.getName()
                                + " does not match existing factory "  + layout.getManagerMeta().getManagerImpl());
        }
        if (factoryClass == null) {
            // no factory specified in configuration
            String factoryClsName = layout.getManagerMeta().getManagerImpl();
            try {
                Class<?> theCls = Class.forName(factoryClsName);
                if (!MetadataManagerFactory.class.isAssignableFrom(theCls)) {
                    throw new IOException("Wrong metadata manager factory " + factoryClsName);
                }
                factoryClass = theCls.asSubclass(MetadataManagerFactory.class);
            } catch (ClassNotFoundException cnfe) {
                throw new IOException("No class found to instantiate metadata manager factory " + factoryClsName);
            }
        }
        // instantiate the metadata manager factory
        MetadataManagerFactory managerFactory;
        try {
            managerFactory = ReflectionUtils.newInstance(factoryClass);
        } catch (Throwable t) {
            throw new IOException("Failed to instantiate metadata manager factory : " + factoryClass, t);
        }
        return managerFactory.initialize(conf, zk, layout.getManagerMeta().getManagerVersion());
    }

    /**
     * Create metadata manager factory and write factory layout to ZooKeeper.
     *
     * @param cfg
     *          Server Configuration object.
     * @param zk
     *          ZooKeeper instance.
     * @param factoryClass
     *          Metadata Manager Factory Class.
     * @return metadata manager factory instance.
     * @throws IOException
     * @throws KeeperException
     * @throws InterruptedException
     */
    public static MetadataManagerFactory createMetadataManagerFactory(
            ServerConfiguration cfg, ZooKeeper zk,
            Class<? extends MetadataManagerFactory> factoryClass)
            throws IOException, KeeperException, InterruptedException {
        // use default manager if no one provided
        if (factoryClass == null) {
            factoryClass = ZkMetadataManagerFactory.class;
        }

        MetadataManagerFactory managerFactory;
        try {
            managerFactory = ReflectionUtils.newInstance(factoryClass);
        } catch (Throwable t) {
            throw new IOException("Fail to instantiate metadata manager factory : " + factoryClass, t);
        }
        ManagerMeta managerMeta = ManagerMeta.newBuilder()
                                  .setManagerImpl(factoryClass.getName())
                                  .setManagerVersion(managerFactory.getCurrentVersion())
                                  .build();
        FactoryLayout layout = new FactoryLayout(managerMeta);
        try {
            layout.store(zk, cfg);
        } catch (KeeperException.NodeExistsException nee) {
            FactoryLayout layout2 = FactoryLayout.readLayout(zk, cfg);
            if (!layout2.equals(layout)) {
                throw new IOException("Contention writing to layout to zookeeper, "
                        + " other layout " + layout2 + " is incompatible with our "
                        + "layout " + layout);
            }
        }
        return managerFactory.initialize(cfg, zk, layout.getManagerMeta().getManagerVersion());
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/meta/MsMetadataManagerFactory.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.meta;

import java.io.IOException;
import java.io.UnsupportedEncodingException;
import java.util.Iterator;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;

import com.google.protobuf.ByteString;
import com.google.protobuf.TextFormat;
import com.google.protobuf.TextFormat.ParseException;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.bookkeeper.metastore.MetaStore;
import org.apache.bookkeeper.metastore.MetastoreCallback;
import org.apache.bookkeeper.metastore.MetastoreCursor;
import org.apache.bookkeeper.metastore.MetastoreCursor.ReadEntriesCallback;
import org.apache.bookkeeper.metastore.MetastoreException;
import org.apache.bookkeeper.metastore.MetastoreFactory;
import org.apache.bookkeeper.metastore.MetastoreScannableTable;
import org.apache.bookkeeper.metastore.MetastoreScannableTable.Order;
import org.apache.bookkeeper.metastore.MetastoreTable;
import org.apache.bookkeeper.metastore.MetastoreUtils;

import static org.apache.bookkeeper.metastore.MetastoreTable.*;
import org.apache.bookkeeper.metastore.MetastoreTableItem;
import org.apache.bookkeeper.metastore.MSException;
import org.apache.bookkeeper.metastore.Value;
import org.apache.bookkeeper.versioning.Version;
import org.apache.bookkeeper.versioning.Versioned;

import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges;
import org.apache.hedwig.protocol.PubSubProtocol.StatusCode;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState;
import org.apache.hedwig.protoextensions.SubscriptionStateUtils;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.topics.HubInfo;
import org.apache.hedwig.util.Callback;

import org.apache.zookeeper.ZooKeeper;

/**
 * MetadataManagerFactory for plug-in metadata storage.
 */
public class MsMetadataManagerFactory extends MetadataManagerFactory {
    protected final static Logger logger = LoggerFactory.getLogger(MsMetadataManagerFactory.class);

    static final String UTF8 = "UTF-8";

    static final int CUR_VERSION = 1;

    static final String OWNER_TABLE_NAME = "owner";
    static final String PERSIST_TABLE_NAME = "persist";
    static final String SUB_TABLE_NAME = "sub";

    static class SyncResult<T> {
        T value;
        int rc;
        boolean finished = false;

        public synchronized void complete(int rc, T value) {
            this.rc = rc;
            this.value = value;
            finished = true;

            notify();
        }

        public synchronized void block() throws InterruptedException {
            while (!finished) {
                wait();
            }
        }

        public int getReturnCode() {
            return rc;
        }

        public T getValue() {
            return value;
        }
    }

    MetaStore metastore;
    MetastoreTable ownerTable;
    MetastoreTable persistTable;
    MetastoreScannableTable subTable;
    ServerConfiguration cfg;

    @Override
    public MetadataManagerFactory initialize(ServerConfiguration cfg, ZooKeeper zk, int version) throws IOException {
        if (CUR_VERSION != version) {
            throw new IOException("Incompatible MsMetadataManagerFactory version " + version
                    + " found, expected version " + CUR_VERSION);
        }
        this.cfg = cfg;
        try {
            metastore = MetastoreFactory.createMetaStore(cfg.getMetastoreImplClass());
            // TODO: need to store metastore class and version in some place.
            metastore.init(cfg.getConf(), metastore.getVersion());
        } catch (Exception e) {
            throw new IOException("Load metastore failed : ", e);
        }

        try {
            ownerTable = metastore.createTable(OWNER_TABLE_NAME);
            if (ownerTable == null) {
                throw new IOException("create owner table failed");
            }

            persistTable = metastore.createTable(PERSIST_TABLE_NAME);
            if (persistTable == null) {
                throw new IOException("create persistence table failed");
            }

            subTable = metastore.createScannableTable(SUB_TABLE_NAME);
            if (subTable == null) {
                throw new IOException("create subscription table failed");
            }
        } catch (MetastoreException me) {
            throw new IOException("Failed to create tables : ", me);
        }

        return this;
    }

    @Override
    public int getCurrentVersion() {
        return CUR_VERSION;
    }

    @Override
    public void shutdown() {
        if (metastore == null) {
            return;
        }

        if (ownerTable != null) {
            ownerTable.close();
            ownerTable = null;
        }

        if (persistTable != null) {
            persistTable.close();
            persistTable = null;
        }

        if (subTable != null) {
            subTable.close();
            subTable = null;
        }

        metastore.close();
        metastore = null;
    }

    @Override
    public Iterator<ByteString> getTopics() throws IOException {
        SyncResult<MetastoreCursor> syn = new SyncResult<MetastoreCursor>();
        persistTable.openCursor(NON_FIELDS, new MetastoreCallback<MetastoreCursor>() {
            public void complete(int rc, MetastoreCursor cursor, Object ctx) {
                @SuppressWarnings("unchecked")
                SyncResult<MetastoreCursor> syn = (SyncResult<MetastoreCursor>) ctx;
                syn.complete(rc, cursor);
            }
        }, syn);
        try {
            syn.block();
        } catch (Exception e) {
            throw new IOException("Interrupted on getting topics list : ", e);
        }

        if (syn.getReturnCode() != MSException.Code.OK.getCode()) {
            throw new IOException("Failed to get topics : ", MSException.create(
                    MSException.Code.get(syn.getReturnCode()), ""));
        }

        final MetastoreCursor cursor = syn.getValue();
        return new Iterator<ByteString>() {
            Iterator<MetastoreTableItem> itemIter = null;

            @Override
            public boolean hasNext() {
                while (null == itemIter || !itemIter.hasNext()) {
                    if (!cursor.hasMoreEntries()) {
                        return false;
                    }

                    try {
                        itemIter = cursor.readEntries(cfg.getMetastoreMaxEntriesPerScan());
                    } catch (MSException mse) {
                        logger.warn("Interrupted when iterating the topics list : ", mse);
                        return false;
                    }
                }
                return true;
            }

            @Override
            public ByteString next() {
                MetastoreTableItem t = itemIter.next();
                return ByteString.copyFromUtf8(t.getKey());
            }

            @Override
            public void remove() {
                throw new UnsupportedOperationException("Doesn't support remove topic from topic iterator.");
            }
        };
    }

    @Override
    public TopicOwnershipManager newTopicOwnershipManager() {
        return new MsTopicOwnershipManagerImpl(ownerTable);
    }

    static class MsTopicOwnershipManagerImpl implements TopicOwnershipManager {

        static final String OWNER_FIELD = "owner";

        final MetastoreTable ownerTable;

        MsTopicOwnershipManagerImpl(MetastoreTable ownerTable) {
            this.ownerTable = ownerTable;
        }

        @Override
        public void close() throws IOException {
            // do nothing
        }

        @Override
        public void readOwnerInfo(final ByteString topic, final Callback<Versioned<HubInfo>> callback, Object ctx) {
            ownerTable.get(topic.toStringUtf8(), new MetastoreCallback<Versioned<Value>>() {
                @Override
                public void complete(int rc, Versioned<Value> value, Object ctx) {
                    if (MSException.Code.NoKey.getCode() == rc) {
                        callback.operationFinished(ctx, null);
                        return;
                    }

                    if (MSException.Code.OK.getCode() != rc) {
                        logErrorAndFinishOperation("Could not read ownership for topic " + topic.toStringUtf8(),
                                callback, ctx, rc);
                        return;
                    }

                    HubInfo owner = null;
                    try {
                        byte[] data = value.getValue().getField(OWNER_FIELD);
                        if (data != null) {
                            owner = HubInfo.parse(new String(data));
                        }
                    } catch (HubInfo.InvalidHubInfoException ihie) {
                        logger.warn("Failed to parse hub info for topic " + topic.toStringUtf8(), ihie);
                    }
                    Version version = value.getVersion();
                    callback.operationFinished(ctx, new Versioned<HubInfo>(owner, version));
                }
            }, ctx);
        }

        @Override
        public void writeOwnerInfo(final ByteString topic, final HubInfo owner, final Version version,
                final Callback<Version> callback, Object ctx) {
            Value value = new Value();
            value.setField(OWNER_FIELD, owner.toString().getBytes());

            ownerTable.put(topic.toStringUtf8(), value, version, new MetastoreCallback<Version>() {
                @Override
                public void complete(int rc, Version ver, Object ctx) {
                    if (MSException.Code.OK.getCode() == rc) {
                        callback.operationFinished(ctx, ver);
                        return;
                    } else if (MSException.Code.NoKey.getCode() == rc) {
                        // no node
                        callback.operationFailed(
                                ctx,
                                PubSubException.create(StatusCode.NO_TOPIC_OWNER_INFO, "No owner info found for topic "
                                        + topic.toStringUtf8()));
                        return;
                    } else if (MSException.Code.KeyExists.getCode() == rc) {
                        // key exists
                        callback.operationFailed(
                                ctx,
                                PubSubException.create(StatusCode.TOPIC_OWNER_INFO_EXISTS, "Owner info of topic "
                                        + topic.toStringUtf8() + " existed."));
                        return;
                    } else if (MSException.Code.BadVersion.getCode() == rc) {
                        // bad version
                        callback.operationFailed(ctx, PubSubException.create(StatusCode.BAD_VERSION,
                                "Bad version provided to update owner info of topic " + topic.toStringUtf8()));
                        return;
                    } else {
                        logErrorAndFinishOperation("Failed to update ownership of topic " + topic.toStringUtf8()
                                + " to " + owner, callback, ctx, rc);
                        return;
                    }
                }
            }, ctx);
        }

        @Override
        public void deleteOwnerInfo(final ByteString topic, Version version, final Callback<Void> callback,
                Object ctx) {
            ownerTable.remove(topic.toStringUtf8(), version, new MetastoreCallback<Void>() {
                @Override
                public void complete(int rc, Void value, Object ctx) {
                    if (MSException.Code.OK.getCode() == rc) {
                        logger.debug("Successfully deleted owner info for topic {}", topic.toStringUtf8());
                        callback.operationFinished(ctx, null);
                        return;
                    } else if (MSException.Code.NoKey.getCode() == rc) {
                        // no node
                        callback.operationFailed(
                                ctx,
                                PubSubException.create(StatusCode.NO_TOPIC_OWNER_INFO, "No owner info found for topic "
                                        + topic.toStringUtf8()));
                        return;
                    } else if (MSException.Code.BadVersion.getCode() == rc) {
                        // bad version
                        callback.operationFailed(ctx, PubSubException.create(StatusCode.BAD_VERSION,
                                "Bad version provided to delete owner info of topic " + topic.toStringUtf8()));
                        return;
                    } else {
                        logErrorAndFinishOperation("Failed to delete owner info for topic " + topic.toStringUtf8(),
                                callback, ctx, rc);
                        return;
                    }
                }
            }, ctx);
        }
    }

    @Override
    public TopicPersistenceManager newTopicPersistenceManager() {
        return new MsTopicPersistenceManagerImpl(persistTable);
    }

    static class MsTopicPersistenceManagerImpl implements TopicPersistenceManager {

        static final String PERSIST_FIELD = "prst";

        final MetastoreTable persistTable;

        MsTopicPersistenceManagerImpl(MetastoreTable persistTable) {
            this.persistTable = persistTable;
        }

        @Override
        public void close() throws IOException {
            // do nothing
        }

        @Override
        public void readTopicPersistenceInfo(final ByteString topic, final Callback<Versioned<LedgerRanges>> callback,
                Object ctx) {
            persistTable.get(topic.toStringUtf8(), new MetastoreCallback<Versioned<Value>>() {
                @Override
                public void complete(int rc, Versioned<Value> value, Object ctx) {
                    if (MSException.Code.OK.getCode() == rc) {
                        byte[] data = value.getValue().getField(PERSIST_FIELD);
                        if (data != null) {
                            parseAndReturnTopicLedgerRanges(topic, data, value.getVersion(), callback, ctx);
                        } else { // null data is same as NoKey
                            callback.operationFinished(ctx, null);
                        }
                    } else if (MSException.Code.NoKey.getCode() == rc) {
                        callback.operationFinished(ctx, null);
                    } else {
                        logErrorAndFinishOperation("Could not read ledgers node for topic " + topic.toStringUtf8(),
                                callback, ctx, rc);
                    }
                }
            }, ctx);
        }

        /**
         * Parse ledger ranges data and return it thru callback.
         *
         * @param topic
         *            Topic name
         * @param data
         *            Topic Ledger Ranges data
         * @param version
         *            Version of the topic ledger ranges data
         * @param callback
         *            Callback to return ledger ranges
         * @param ctx
         *            Context of the callback
         */
        private void parseAndReturnTopicLedgerRanges(ByteString topic, byte[] data, Version version,
                Callback<Versioned<LedgerRanges>> callback, Object ctx) {
            try {
                LedgerRanges.Builder rangesBuilder = LedgerRanges.newBuilder();
                TextFormat.merge(new String(data, UTF8), rangesBuilder);
                LedgerRanges lr = rangesBuilder.build();
                Versioned<LedgerRanges> ranges = new Versioned<LedgerRanges>(lr, version);
                callback.operationFinished(ctx, ranges);
            } catch (ParseException e) {
                StringBuilder sb = new StringBuilder();
                sb.append("Ledger ranges for topic ").append(topic.toStringUtf8())
                        .append(" could not be deserialized.");
                String msg = sb.toString();
                logger.error(msg, e);
                callback.operationFailed(ctx, new PubSubException.UnexpectedConditionException(msg));
            } catch (UnsupportedEncodingException uee) {
                StringBuilder sb = new StringBuilder();
                sb.append("Ledger ranges for topic ").append(topic.toStringUtf8()).append(" is not UTF-8 encoded.");
                String msg = sb.toString();
                logger.error(msg, uee);
                callback.operationFailed(ctx, new PubSubException.UnexpectedConditionException(msg));
            }
        }

        @Override
        public void writeTopicPersistenceInfo(final ByteString topic, LedgerRanges ranges, final Version version,
                final Callback<Version> callback, Object ctx) {
            Value value = new Value();
            value.setField(PERSIST_FIELD, TextFormat.printToString(ranges).getBytes());

            persistTable.put(topic.toStringUtf8(), value, version, new MetastoreCallback<Version>() {
                @Override
                public void complete(int rc, Version ver, Object ctx) {
                    if (MSException.Code.OK.getCode() == rc) {
                        callback.operationFinished(ctx, ver);
                        return;
                    } else if (MSException.Code.NoKey.getCode() == rc) {
                        // no node
                        callback.operationFailed(ctx, PubSubException.create(StatusCode.NO_TOPIC_PERSISTENCE_INFO,
                                "No persistence info found for topic " + topic.toStringUtf8()));
                        return;
                    } else if (MSException.Code.KeyExists.getCode() == rc) {
                        // key exists
                        callback.operationFailed(ctx, PubSubException.create(StatusCode.TOPIC_PERSISTENCE_INFO_EXISTS,
                                "Persistence info of topic " + topic.toStringUtf8() + " existed."));
                        return;
                    } else if (MSException.Code.BadVersion.getCode() == rc) {
                        // bad version
                        callback.operationFailed(ctx, PubSubException.create(StatusCode.BAD_VERSION,
                                "Bad version provided to update persistence info of topic " + topic.toStringUtf8()));
                        return;
                    } else {
                        logErrorAndFinishOperation("Could not write ledgers node for topic " + topic.toStringUtf8(),
                                callback, ctx, rc);
                    }
                }
            }, ctx);
        }

        @Override
        public void deleteTopicPersistenceInfo(final ByteString topic, final Version version,
                final Callback<Void> callback, Object ctx) {
            persistTable.remove(topic.toStringUtf8(), version, new MetastoreCallback<Void>() {
                @Override
                public void complete(int rc, Void value, Object ctx) {
                    if (MSException.Code.OK.getCode() == rc) {
                        logger.debug("Successfully deleted persistence info for topic {}.", topic.toStringUtf8());
                        callback.operationFinished(ctx, null);
                        return;
                    } else if (MSException.Code.NoKey.getCode() == rc) {
                        // no node
                        callback.operationFailed(ctx, PubSubException.create(StatusCode.NO_TOPIC_PERSISTENCE_INFO,
                                "No persistence info found for topic " + topic.toStringUtf8()));
                        return;
                    } else if (MSException.Code.BadVersion.getCode() == rc) {
                        // bad version
                        callback.operationFailed(ctx, PubSubException.create(StatusCode.BAD_VERSION,
                                "Bad version provided to delete persistence info of topic " + topic.toStringUtf8()));
                        return;
                    } else {
                        logErrorAndFinishOperation("Failed to delete persistence info topic: " + topic.toStringUtf8()
                                + ", version: " + version, callback, ctx, rc, StatusCode.SERVICE_DOWN);
                        return;
                    }
                }
            }, ctx);
        }
    }

    @Override
    public SubscriptionDataManager newSubscriptionDataManager() {
        return new MsSubscriptionDataManagerImpl(cfg, subTable);
    }

    static class MsSubscriptionDataManagerImpl implements SubscriptionDataManager {

        static final String SUB_STATE_FIELD = "sub_state";
        static final String SUB_PREFS_FIELD = "sub_preferences";

        static final char TOPIC_SUB_FIRST_SEPARATOR = '\001';
        static final char TOPIC_SUB_LAST_SEPARATOR = '\002';

        final ServerConfiguration cfg;
        final MetastoreScannableTable subTable;

        MsSubscriptionDataManagerImpl(ServerConfiguration cfg, MetastoreScannableTable subTable) {
            this.cfg = cfg;
            this.subTable = subTable;
        }

        @Override
        public void close() throws IOException {
            // do nothing
        }

        private String getSubscriptionKey(ByteString topic, ByteString subscriberId) {
            return new StringBuilder(topic.toStringUtf8()).append(TOPIC_SUB_FIRST_SEPARATOR)
                    .append(subscriberId.toStringUtf8()).toString();
        }

        private Value subscriptionData2Value(SubscriptionData subData) {
            Value value = new Value();
            if (subData.hasState()) {
                value.setField(SUB_STATE_FIELD, TextFormat.printToString(subData.getState()).getBytes());
            }
            if (subData.hasPreferences()) {
                value.setField(SUB_PREFS_FIELD, TextFormat.printToString(subData.getPreferences()).getBytes());
            }
            return value;
        }

        @Override
        public void createSubscriptionData(final ByteString topic, final ByteString subscriberId,
                final SubscriptionData subData, final Callback<Version> callback, Object ctx) {
            String key = getSubscriptionKey(topic, subscriberId);
            Value value = subscriptionData2Value(subData);

            subTable.put(key, value, Version.NEW, new MetastoreCallback<Version>() {
                @Override
                public void complete(int rc, Version ver, Object ctx) {
                    if (rc == MSException.Code.OK.getCode()) {
                        if (logger.isDebugEnabled()) {
                            logger.debug("Successfully create subscription for topic: " + topic.toStringUtf8()
                                    + ", subscriberId: " + subscriberId.toStringUtf8() + ", data: "
                                    + SubscriptionStateUtils.toString(subData));
                        }
                        callback.operationFinished(ctx, ver);
                    } else if (rc == MSException.Code.KeyExists.getCode()) {
                        callback.operationFailed(ctx, PubSubException.create(
                                StatusCode.SUBSCRIPTION_STATE_EXISTS,
                                "Subscription data for (topic:" + topic.toStringUtf8() + ", subscriber:"
                                        + subscriberId.toStringUtf8() + ") existed."));
                        return;
                    } else {
                        logErrorAndFinishOperation("Failed to create topic: " + topic.toStringUtf8()
                                + ", subscriberId: " + subscriberId.toStringUtf8() + ", data: "
                                + SubscriptionStateUtils.toString(subData), callback, ctx, rc);
                    }
                }
            }, ctx);
        }

        @Override
        public boolean isPartialUpdateSupported() {
            // TODO: Here we assume Metastore support partial update, but this
            // maybe incorrect.
            return true;
        }

        @Override
        public void replaceSubscriptionData(final ByteString topic, final ByteString subscriberId,
                final SubscriptionData subData, final Version version, final Callback<Version> callback,
                final Object ctx) {
            updateSubscriptionData(topic, subscriberId, subData, version, callback, ctx);
        }

        @Override
        public void updateSubscriptionData(final ByteString topic, final ByteString subscriberId,
                final SubscriptionData subData, final Version version, final Callback<Version> callback,
                final Object ctx) {
            String key = getSubscriptionKey(topic, subscriberId);
            Value value = subscriptionData2Value(subData);

            subTable.put(key, value, version, new MetastoreCallback<Version>() {
                @Override
                public void complete(int rc, Version version, Object ctx) {
                    if (rc == MSException.Code.OK.getCode()) {
                        if (logger.isDebugEnabled()) {
                            logger.debug("Successfully updated subscription data for topic: " + topic.toStringUtf8()
                                    + ", subscriberId: " + subscriberId.toStringUtf8() + ", data: "
                                    + SubscriptionStateUtils.toString(subData) + ", version: " + version);
                        }
                        callback.operationFinished(ctx, version);
                    } else if (rc == MSException.Code.NoKey.getCode()) {
                        // no node
                        callback.operationFailed(ctx, PubSubException.create(StatusCode.NO_SUBSCRIPTION_STATE,
                                "No subscription data found for (topic:" + topic.toStringUtf8() + ", subscriber:"
                                        + subscriberId.toStringUtf8() + ")."));
                        return;
                    } else if (rc == MSException.Code.BadVersion.getCode()) {
                        // bad version
                        callback.operationFailed(ctx, PubSubException.create(StatusCode.BAD_VERSION,
                                "Bad version provided to update subscription data of topic " + topic.toStringUtf8()
                                        + " subscriberId " + subscriberId));
                        return;
                    } else {
                        logErrorAndFinishOperation(
                                "Failed to update subscription data for topic: " + topic.toStringUtf8()
                                        + ", subscriberId: " + subscriberId.toStringUtf8() + ", data: "
                                        + SubscriptionStateUtils.toString(subData) + ", version: " + version, callback,
                                ctx, rc);
                    }
                }
            }, ctx);
        }

        @Override
        public void deleteSubscriptionData(final ByteString topic, final ByteString subscriberId, Version version,
                final Callback<Void> callback, Object ctx) {
            String key = getSubscriptionKey(topic, subscriberId);
            subTable.remove(key, version, new MetastoreCallback<Void>() {
                @Override
                public void complete(int rc, Void value, Object ctx) {
                    if (rc == MSException.Code.OK.getCode()) {
                        logger.debug("Successfully delete subscription for topic: {}, subscriberId: {}.",
                                topic.toStringUtf8(), subscriberId.toStringUtf8());
                        callback.operationFinished(ctx, null);
                        return;
                    } else if (rc == MSException.Code.BadVersion.getCode()) {
                        // bad version
                        callback.operationFailed(ctx, PubSubException.create(StatusCode.BAD_VERSION,
                                "Bad version provided to delete subscriptoin data of topic " + topic.toStringUtf8()
                                        + " subscriberId " + subscriberId));
                        return;
                    } else if (rc == MSException.Code.NoKey.getCode()) {
                        // no node
                        callback.operationFailed(ctx, PubSubException.create(StatusCode.NO_SUBSCRIPTION_STATE,
                                "No subscription data found for (topic:" + topic.toStringUtf8() + ", subscriber:"
                                        + subscriberId.toStringUtf8() + ")."));
                        return;
                    } else {
                        logErrorAndFinishOperation("Failed to delete subscription topic: " + topic.toStringUtf8()
                                + ", subscriberId: " + subscriberId.toStringUtf8(), callback, ctx, rc,
                                StatusCode.SERVICE_DOWN);
                    }
                }
            }, ctx);
        }

        private SubscriptionData value2SubscriptionData(Value value) throws ParseException,
                UnsupportedEncodingException {
            SubscriptionData.Builder builder = SubscriptionData.newBuilder();

            byte[] stateData = value.getField(SUB_STATE_FIELD);
            if (null != stateData) {
                SubscriptionState.Builder stateBuilder = SubscriptionState.newBuilder();
                TextFormat.merge(new String(stateData, UTF8), stateBuilder);
                SubscriptionState state = stateBuilder.build();
                builder.setState(state);
            }

            byte[] prefsData = value.getField(SUB_PREFS_FIELD);
            if (null != prefsData) {
                SubscriptionPreferences.Builder preferencesBuilder = SubscriptionPreferences.newBuilder();
                TextFormat.merge(new String(prefsData, UTF8), preferencesBuilder);
                SubscriptionPreferences preferences = preferencesBuilder.build();
                builder.setPreferences(preferences);
            }

            return builder.build();
        }

        @Override
        public void readSubscriptionData(final ByteString topic, final ByteString subscriberId,
                final Callback<Versioned<SubscriptionData>> callback, Object ctx) {
            String key = getSubscriptionKey(topic, subscriberId);
            subTable.get(key, new MetastoreCallback<Versioned<Value>>() {
                @Override
                public void complete(int rc, Versioned<Value> value, Object ctx) {
                    if (rc == MSException.Code.NoKey.getCode()) {
                        callback.operationFinished(ctx, null);
                        return;
                    }

                    if (rc != MSException.Code.OK.getCode()) {
                        logErrorAndFinishOperation(
                                "Could not read subscription data for topic: " + topic.toStringUtf8()
                                        + ", subscriberId: " + subscriberId.toStringUtf8(), callback, ctx, rc);
                        return;
                    }

                    try {
                        Versioned<SubscriptionData> subData = new Versioned<SubscriptionData>(
                                value2SubscriptionData(value.getValue()), value.getVersion());
                        if (logger.isDebugEnabled()) {
                            logger.debug("Found subscription while acquiring topic: " + topic.toStringUtf8()
                                    + ", subscriberId: " + subscriberId.toStringUtf8() + ", data: "
                                    + SubscriptionStateUtils.toString(subData.getValue()) + ", version: "
                                    + subData.getVersion());
                        }
                        callback.operationFinished(ctx, subData);
                    } catch (ParseException e) {
                        StringBuilder sb = new StringBuilder();
                        sb.append("Failed to deserialize subscription data for topic:").append(topic.toStringUtf8())
                                .append(", subscriberId: ").append(subscriberId.toStringUtf8());
                        String msg = sb.toString();
                        logger.error(msg, e);
                        callback.operationFailed(ctx, new PubSubException.UnexpectedConditionException(msg));
                    } catch (UnsupportedEncodingException uee) {
                        StringBuilder sb = new StringBuilder();
                        sb.append("Subscription data for topic: ").append(topic.toStringUtf8())
                                .append(", subscriberId: ").append(subscriberId.toStringUtf8())
                                .append(" is not UFT-8 encoded");
                        String msg = sb.toString();
                        logger.error(msg, uee);
                        callback.operationFailed(ctx, new PubSubException.UnexpectedConditionException(msg));
                    }
                }
            }, ctx);
        }

        private String getSubscriptionPrefix(ByteString topic, char sep) {
            return new StringBuilder(topic.toStringUtf8()).append(sep).toString();
        }

        private void readSubscriptions(final ByteString topic, final int keyLength, final MetastoreCursor cursor,
                final Map<ByteString, Versioned<SubscriptionData>> topicSubs,
                final Callback<Map<ByteString, Versioned<SubscriptionData>>> callback, Object ctx) {
            if (!cursor.hasMoreEntries()) {
                callback.operationFinished(ctx, topicSubs);
                return;
            }
            ReadEntriesCallback readCb = new ReadEntriesCallback() {
                @Override
                public void complete(int rc, Iterator<MetastoreTableItem> items, Object ctx) {
                    if (rc != MSException.Code.OK.getCode()) {
                        logErrorAndFinishOperation("Could not read subscribers for cursor " + cursor,
                                callback, ctx, rc);
                        return;
                    }
                    while (items.hasNext()) {
                        MetastoreTableItem item = items.next();
                        final ByteString subscriberId = ByteString.copyFromUtf8(item.getKey().substring(keyLength));
                        try {
                            Versioned<Value> vv = item.getValue();
                            Versioned<SubscriptionData> subData = new Versioned<SubscriptionData>(
                                    value2SubscriptionData(vv.getValue()), vv.getVersion());
                            topicSubs.put(subscriberId, subData);
                        } catch (ParseException e) {
                            StringBuilder sb = new StringBuilder();
                            sb.append("Failed to deserialize subscription data for topic: ")
                                    .append(topic.toStringUtf8()).append(", subscriberId: ")
                                    .append(subscriberId.toStringUtf8());
                            String msg = sb.toString();
                            logger.error(msg, e);
                            callback.operationFailed(ctx, new PubSubException.UnexpectedConditionException(msg));
                            return;
                        } catch (UnsupportedEncodingException e) {
                            StringBuilder sb = new StringBuilder();
                            sb.append("Subscription data for topic: ").append(topic.toStringUtf8())
                                    .append(", subscriberId: ").append(subscriberId.toStringUtf8())
                                    .append(" is not UTF-8 encoded.");
                            String msg = sb.toString();
                            logger.error(msg, e);
                            callback.operationFailed(ctx, new PubSubException.UnexpectedConditionException(msg));
                            return;
                        }
                    }
                    readSubscriptions(topic, keyLength, cursor, topicSubs, callback, ctx);
                }
            };
            cursor.asyncReadEntries(cfg.getMetastoreMaxEntriesPerScan(), readCb, ctx);
        }

        @Override
        public void readSubscriptions(final ByteString topic,
                final Callback<Map<ByteString, Versioned<SubscriptionData>>> callback, Object ctx) {
            final String firstKey = getSubscriptionPrefix(topic, TOPIC_SUB_FIRST_SEPARATOR);
            String lastKey = getSubscriptionPrefix(topic, TOPIC_SUB_LAST_SEPARATOR);
            subTable.openCursor(firstKey, true, lastKey, true, Order.ASC, ALL_FIELDS,
                    new MetastoreCallback<MetastoreCursor>() {
                        @Override
                        public void complete(int rc, MetastoreCursor cursor, Object ctx) {
                            if (rc != MSException.Code.OK.getCode()) {
                                logErrorAndFinishOperation(
                                        "Could not read subscribers for topic " + topic.toStringUtf8(), callback, ctx,
                                        rc);
                                return;
                            }

                            final Map<ByteString, Versioned<SubscriptionData>> topicSubs =
                                    new ConcurrentHashMap<ByteString, Versioned<SubscriptionData>>();
                            readSubscriptions(topic, firstKey.length(), cursor, topicSubs, callback, ctx);
                        }
                    }, ctx);
        }
    }

    /**
     * callback finish operation with exception specify by code, regardless of
     * the value of return code rc.
     */
    private static <T> void logErrorAndFinishOperation(String msg, Callback<T> callback, Object ctx, int rc,
            StatusCode code) {
        logger.error(msg, MSException.create(MSException.Code.get(rc), ""));
        callback.operationFailed(ctx, PubSubException.create(code, msg));
    }

    /**
     * callback finish operation with corresponding PubSubException converted
     * from return code rc.
     */
    private static <T> void logErrorAndFinishOperation(String msg, Callback<T> callback, Object ctx, int rc) {
        StatusCode code;

        if (rc == MSException.Code.NoKey.getCode()) {
            code = StatusCode.NO_SUCH_TOPIC;
        } else if (rc == MSException.Code.ServiceDown.getCode()) {
            code = StatusCode.SERVICE_DOWN;
        } else {
            code = StatusCode.UNEXPECTED_CONDITION;
        }

        logErrorAndFinishOperation(msg, callback, ctx, rc, code);
    }

    @Override
    public void format(ServerConfiguration cfg, ZooKeeper zk) throws IOException {
        try {
            int maxEntriesPerScan = cfg.getMetastoreMaxEntriesPerScan();

            // clean topic ownership table.
            logger.info("Cleaning topic ownership table ...");
            MetastoreUtils.cleanTable(ownerTable, maxEntriesPerScan);
            logger.info("Cleaned topic ownership table successfully.");

            // clean topic subscription table.
            logger.info("Cleaning topic subscription table ...");
            MetastoreUtils.cleanTable(subTable, maxEntriesPerScan);
            logger.info("Cleaned topic subscription table successfully.");

            // clean topic persistence info table.
            logger.info("Cleaning topic persistence info table ...");
            MetastoreUtils.cleanTable(persistTable, maxEntriesPerScan);
            logger.info("Cleaned topic persistence info table successfully.");
        } catch (MSException mse) {
            throw new IOException("Exception when formatting hedwig metastore : ", mse);
        } catch (InterruptedException ie) {
            throw new IOException("Interrupted when formatting hedwig metastore : ", ie);
        }
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/meta/SubscriptionDataManager.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.meta;

import java.io.Closeable;
import java.util.Map;

import com.google.protobuf.ByteString;

import org.apache.bookkeeper.versioning.Version;
import org.apache.bookkeeper.versioning.Versioned;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData;
import org.apache.hedwig.server.subscriptions.InMemorySubscriptionState;
import org.apache.hedwig.util.Callback;

/**
 * Manage subscription data.
 */
public interface SubscriptionDataManager extends Closeable {

    /**
     * Create subscription data.
     *
     * @param topic
     *          Topic name
     * @param subscriberId
     *          Subscriber id
     * @param data 
     *          Subscription data
     * @param callback
     *          Callback when subscription state created. New version would be returned.
     *          {@link PubSubException.SubscriptionStateExistsException} is returned when subscription state
     *          existed before.
     * @param ctx
     *          Context of the callback
     */
    public void createSubscriptionData(ByteString topic, ByteString subscriberId, SubscriptionData data,
                                       Callback<Version> callback, Object ctx);

    /**
     * Whether the metadata manager supports partial update.
     *
     * @return true if the metadata manager supports partial update.
     *         otherwise, return false.
     */
    public boolean isPartialUpdateSupported();

    /**
     * Update subscription data.
     *
     * @param topic
     *          Topic name
     * @param subscriberId
     *          Subscriber id
     * @param dataToUpdate
     *          Subscription data to update. So it is a partial data, which contains
     *          the part of data to update. The implementation should not replace
     *          existing subscription data with <i>dataToUpdate</i> directly.
     *          E.g. if there is only state in it, you should update state only.
     * @param version
     *          Current version of subscription data.
     * @param callback
     *          Callback when subscription state updated. New version would be returned.
     *          {@link PubSubException.BadVersionException} is returned when version doesn't match,
     *          {@link PubSubException.NoSubscriptionStateException} is returned when no subscription state
     *          is found.
     * @param ctx
     *          Context of the callback
     */
    public void updateSubscriptionData(ByteString topic, ByteString subscriberId, SubscriptionData dataToUpdate, 
                                       Version version, Callback<Version> callback, Object ctx);

    /**
     * Replace subscription data.
     *
     * @param topic
     *          Topic name
     * @param subscriberId
     *          Subscriber id
     * @param dataToReplace
     *          Subscription data to replace.
     * @param version
     *          Current version of subscription data.
     * @param callback
     *          Callback when subscription state updated. New version would be returned.
     *          {@link PubSubException.BadVersionException} is returned when version doesn't match,
     *          {@link PubSubException.NoSubscriptionStateException} is returned when no subscription state
     *          is found.
     * @param ctx
     *          Context of the callback
     */
    public void replaceSubscriptionData(ByteString topic, ByteString subscriberId, SubscriptionData dataToReplace,
                                        Version version, Callback<Version> callback, Object ctx);

    /**
     * Remove subscription data.
     *
     * @param topic
     *          Topic name
     * @param subscriberId
     *          Subscriber id
     * @param version
     *          Current version of subscription data.
     * @param callback
     *          Callback when subscription state deleted
     *          {@link PubSubException.BadVersionException} is returned when version doesn't match,
     *          {@link PubSubException.NoSubscriptionStateException} is returned when no subscription state
     *          is found.
     * @param ctx
     *          Context of the callback
     */
    public void deleteSubscriptionData(ByteString topic, ByteString subscriberId, Version version,
                                       Callback<Void> callback, Object ctx);

    /**
     * Read subscription data with version.
     *
     * @param topic
     *          Topic Name
     * @param subscriberId
     *          Subscriber id
     * @param callback
     *          Callback when subscription data read.
     *          Null is returned when no subscription data is found.
     * @param ctx
     *          Context of the callback
     */
    public void readSubscriptionData(ByteString topic, ByteString subscriberId,
                                     Callback<Versioned<SubscriptionData>> callback, Object ctx);

    /**
     * Read all subscriptions of a topic.
     *
     * @param topic
     *          Topic name
     * @param callback
     *          Callback to return subscriptions with version information
     * @param ctx
     *          Contxt of the callback
     */
    public void readSubscriptions(ByteString topic, Callback<Map<ByteString, Versioned<SubscriptionData>>> cb,
                                  Object ctx);
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/meta/TopicOwnershipManager.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.meta;

import java.io.Closeable;
import java.io.IOException;
import java.util.Map;

import com.google.protobuf.ByteString;

import org.apache.bookkeeper.versioning.Version;
import org.apache.bookkeeper.versioning.Versioned;
import org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.subscriptions.InMemorySubscriptionState;
import org.apache.hedwig.server.topics.HubInfo;
import org.apache.hedwig.util.Callback;
import org.apache.zookeeper.ZooKeeper;

/**
 * Manage topic ownership
 */
public interface TopicOwnershipManager extends Closeable {

    /**
     * Read owner information of a topic.
     *
     * @param topic
     *          Topic Name
     * @param callback
     *          Callback to return hub info. If there is no owner info, return null;
     *          If there is data but not valid owner info, return a Versioned object with null hub info;
     *          If there is valid owner info, return versioned hub info.
     * @param ctx
     *          Context of the callback
     */
    public void readOwnerInfo(ByteString topic, Callback<Versioned<HubInfo>> callback, Object ctx);

    /**
     * Write owner info for a specified topic.
     * A new owner info would be created if there is no one existed before.
     *
     * @param topic
     *          Topic Name
     * @param owner
     *          Owner hub info
     * @param version
     *          Current version of owner info
     *          If <code>version</code> is {@link Version.NEW}, create owner info.
     *          {@link PubSubException.TopicOwnerInfoExistsException} is returned when
     *          owner info existed before.
     *          Otherwise, the owner info is updated only when
     *          provided version equals to its current version.
     *          {@link PubSubException.BadVersionException} is returned when version doesn't match,
     *          {@link PubSubException.NoTopicOwnerInfoException} is returned when no owner info
     *          found to update.
     * @param callback
     *          Callback when owner info updated. New version would be returned if succeed to write.
     * @param ctx
     *          Context of the callback
     */
    public void writeOwnerInfo(ByteString topic, HubInfo owner, Version version,
                               Callback<Version> callback, Object ctx);

    /**
     * Delete owner info for a specified topic.
     *
     * @param topic
     *          Topic Name
     * @param version
     *          Current version of owner info
     *          If <code>version</code> is {@link Version.ANY}, delete owner info no matter its current version.
     *          Otherwise, the owner info is deleted only when
     *          provided version equals to its current version.
     * @param callback
     *          Callback when owner info deleted.
     *          {@link PubSubException.NoTopicOwnerInfoException} is returned when no owner info.
     *          {@link PubSubException.BadVersionException} is returned when version doesn't match.
     * @param ctx
     *          Context of the callback.
     */
    public void deleteOwnerInfo(ByteString topic, Version version,
                                Callback<Void> callback, Object ctx);
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/meta/TopicPersistenceManager.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.meta;

import java.io.Closeable;

import com.google.protobuf.ByteString;

import org.apache.bookkeeper.versioning.Version;
import org.apache.bookkeeper.versioning.Versioned;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges;
import org.apache.hedwig.util.Callback;

/**
 * Manage topic persistence metadata.
 */
public interface TopicPersistenceManager extends Closeable {

    /**
     * Read persistence info of a specified topic.
     *
     * @param topic
     *          Topic Name
     * @param callback
     *          Callback when read persistence info.
     *          If no persistence info found, return null.
     * @param ctx
     *          Context of the callback
     */
    public void readTopicPersistenceInfo(ByteString topic,
                                         Callback<Versioned<LedgerRanges>> callback, Object ctx);

    /**
     * Update persistence info of a specified topic.
     *
     * @param topic
     *          Topic name
     * @param ranges
     *          Persistence info
     * @param version
     *          Current version of persistence info.
     *          If <code>version</code> is {@link Version.NEW}, create persistence info;
     *          {@link PubSubException.TopicPersistenceInfoExistsException} is returned when
     *          persistence info existed before.
     *          Otherwise, the persitence info is updated only when
     *          provided version equals to its current version.
     *          {@link PubSubException.BadVersionException} is returned when version doesn't match,
     *          {@link PubSubException.NoTopicPersistenceInfoException} is returned when no
     *          persistence info found to update.
     * @param callback
     *          Callback when persistence info updated. New version would be returned.
     * @param ctx
     *          Context of the callback
     */
    public void writeTopicPersistenceInfo(ByteString topic, LedgerRanges ranges, Version version,
                                          Callback<Version> callback, Object ctx);

    /**
     * Delete persistence info of a specified topic.
     * Currently used in test cases.
     *
     * @param topic
     *          Topic name
     * @param version
     *          Current version of persistence info
     *          If <code>version</code> is {@link Version.ANY}, delete persistence info no matter its current version.
     *          Otherwise, the persitence info is deleted only when
     *          provided version equals to its current version.
     * @param callback
     *          Callback return whether the deletion succeed.
     *          {@link PubSubException.NoTopicPersistenceInfoException} is returned when no persistence.
     *          {@link PubSubException.BadVersionException} is returned when version doesn't match.
     *          info found to delete.
     * @param ctx
     *          Context of the callback
     */
    public void deleteTopicPersistenceInfo(ByteString topic, Version version,
                                           Callback<Void> callback, Object ctx);

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/meta/ZkMetadataManagerFactory.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.meta;

import java.io.IOException;
import java.util.List;
import java.util.Map;
import java.util.Iterator;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicInteger;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.KeeperException.Code;
import org.apache.zookeeper.ZKUtil;
import org.apache.zookeeper.ZooKeeper;
import org.apache.zookeeper.ZooDefs.Ids;
import org.apache.zookeeper.data.Stat;
import com.google.protobuf.ByteString;
import com.google.protobuf.InvalidProtocolBufferException;
import org.apache.bookkeeper.versioning.Version;
import org.apache.bookkeeper.versioning.Versioned;
import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.meta.ZkVersion;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges;
import org.apache.hedwig.protocol.PubSubProtocol.StatusCode;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState;
import org.apache.hedwig.protoextensions.SubscriptionStateUtils;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.topics.HubInfo;
import org.apache.hedwig.util.Callback;
import org.apache.hedwig.zookeeper.SafeAsyncZKCallback;
import org.apache.hedwig.zookeeper.ZkUtils;

/**
 * ZooKeeper-based Metadata Manager.
 */
public class ZkMetadataManagerFactory extends MetadataManagerFactory {
    protected final static Logger logger = LoggerFactory.getLogger(ZkMetadataManagerFactory.class);

    static final int CUR_VERSION = 1;

    ZooKeeper zk;
    ServerConfiguration cfg;

    @Override
    public int getCurrentVersion() {
        return CUR_VERSION;
    }

    @Override
    public MetadataManagerFactory initialize(ServerConfiguration cfg,
                                             ZooKeeper zk,
                                             int version)
    throws IOException {
        if (CUR_VERSION != version) {
            throw new IOException("Incompatible ZkMetadataManagerFactory version " + version
                                + " found, expected version " + CUR_VERSION);
        }
        this.cfg = cfg;
        this.zk = zk;
        return this;
    }

    @Override
    public void shutdown() {
        // do nothing here, because zookeeper handle is passed from outside
        // we don't need to stop it.
    }

    @Override
    public Iterator<ByteString> getTopics() throws IOException {
        List<String> topics;
        try {
            topics = zk.getChildren(cfg.getZkTopicsPrefix(new StringBuilder()).toString(), false);
        } catch (KeeperException ke) {
            throw new IOException("Failed to get topics list : ", ke);
        } catch (InterruptedException ie) {
            throw new IOException("Interrupted on getting topics list : ", ie);
        }
        final Iterator<String> iter = topics.iterator();
        return new Iterator<ByteString>() {
            @Override
            public boolean hasNext() {
                return iter.hasNext();
            }
            @Override
            public ByteString next() {
                String t = iter.next();
                return ByteString.copyFromUtf8(t);
            }
            @Override
            public void remove() {
                iter.remove();
            }
        };
    }

    @Override
    public TopicPersistenceManager newTopicPersistenceManager() {
        return new ZkTopicPersistenceManagerImpl(cfg, zk);
    }

    @Override
    public SubscriptionDataManager newSubscriptionDataManager() {
        return new ZkSubscriptionDataManagerImpl(cfg, zk);
    }

    @Override
    public TopicOwnershipManager newTopicOwnershipManager() {
        return new ZkTopicOwnershipManagerImpl(cfg, zk);
    }

    /**
     * ZooKeeper based topic persistence manager.
     */
    static class ZkTopicPersistenceManagerImpl implements TopicPersistenceManager {

        ZooKeeper zk;
        ServerConfiguration cfg;

        ZkTopicPersistenceManagerImpl(ServerConfiguration conf, ZooKeeper zk) {
            this.cfg = conf;
            this.zk = zk;
        }

        @Override
        public void close() throws IOException {
            // do nothing in zookeeper based impl
        }

        /**
         * Get znode path to store persistence info of a topic.
         *
         * @param topic
         *          Topic name
         * @return znode path to store persistence info.
         */
        private String ledgersPath(ByteString topic) {
            return cfg.getZkTopicPath(new StringBuilder(), topic).append("/ledgers").toString();
        }

        /**
         * Parse ledger ranges data and return it thru callback.
         *
         * @param topic
         *          Topic name
         * @param data
         *          Topic Ledger Ranges data
         * @param version
         *          Version of the topic ledger ranges data
         * @param callback
         *          Callback to return ledger ranges
         * @param ctx
         *          Context of the callback
         */
        private void parseAndReturnTopicLedgerRanges(ByteString topic, byte[] data, int version,
                                                     Callback<Versioned<LedgerRanges>> callback, Object ctx) {
            try {
                Versioned<LedgerRanges> ranges = new Versioned<LedgerRanges>(LedgerRanges.parseFrom(data),
                                                                             new ZkVersion(version));
                callback.operationFinished(ctx, ranges);
                return;
            } catch (InvalidProtocolBufferException e) {
                String msg = "Ledger ranges for topic:" + topic.toStringUtf8() + " could not be deserialized";
                logger.error(msg, e);
                callback.operationFailed(ctx, new PubSubException.UnexpectedConditionException(msg));
                return;
            }
        }

        @Override
        public void readTopicPersistenceInfo(final ByteString topic,
                                             final Callback<Versioned<LedgerRanges>> callback,
                                             Object ctx) {
            // read topic ledgers node data
            final String zNodePath = ledgersPath(topic);

            zk.getData(zNodePath, false, new SafeAsyncZKCallback.DataCallback() {
                @Override
                public void safeProcessResult(int rc, String path, Object ctx, byte[] data, Stat stat) {
                    if (rc == Code.OK.intValue()) {
                        parseAndReturnTopicLedgerRanges(topic, data, stat.getVersion(), callback, ctx);
                        return;
                    }

                    if (rc == Code.NONODE.intValue()) {
                        // we don't create the znode until we first write it.
                        callback.operationFinished(ctx, null);
                        return;
                    }

                    // otherwise some other error
                    KeeperException ke =
                        ZkUtils.logErrorAndCreateZKException("Could not read ledgers node for topic: "
                                                             + topic.toStringUtf8(), path, rc);
                    callback.operationFailed(ctx, new PubSubException.ServiceDownException(ke));
                }
            }, ctx);
        }

        private void createTopicPersistenceInfo(final ByteString topic, LedgerRanges ranges,
                                                final Callback<Version> callback, Object ctx) {
            final String zNodePath = ledgersPath(topic);
            final byte[] data = ranges.toByteArray();
            // create it
            ZkUtils.createFullPathOptimistic(zk, zNodePath, data, Ids.OPEN_ACL_UNSAFE,
            CreateMode.PERSISTENT, new SafeAsyncZKCallback.StringCallback() {
                @Override
                public void safeProcessResult(int rc, String path, Object ctx, String name) {
                    if (rc == Code.NODEEXISTS.intValue()) {
                        callback.operationFailed(ctx, PubSubException.create(StatusCode.TOPIC_PERSISTENCE_INFO_EXISTS,
                                                      "Persistence info of topic " + topic.toStringUtf8() + " existed."));
                        return;
                    }
                    if (rc != Code.OK.intValue()) {
                        KeeperException ke = ZkUtils.logErrorAndCreateZKException(
                                             "Could not create ledgers node for topic: " + topic.toStringUtf8(),
                                             path, rc);
                        callback.operationFailed(ctx, new PubSubException.ServiceDownException(ke));
                        return;
                    }
                    // initial version is version 0
                    callback.operationFinished(ctx, new ZkVersion(0));
                }
            }, ctx);
            return;
        }

        @Override
        public void writeTopicPersistenceInfo(final ByteString topic, LedgerRanges ranges, final Version version,
                                              final Callback<Version> callback, Object ctx) {
            if (Version.NEW == version) {
                createTopicPersistenceInfo(topic, ranges, callback, ctx);
                return;
            }

            final String zNodePath = ledgersPath(topic);
            final byte[] data = ranges.toByteArray();

            if (!(version instanceof ZkVersion)) {
                callback.operationFailed(ctx, new PubSubException.UnexpectedConditionException(
                                              "Invalid version provided to update persistence info for topic " + topic.toStringUtf8()));
                return;
            }

            int znodeVersion = ((ZkVersion)version).getZnodeVersion();
            zk.setData(zNodePath, data, znodeVersion, new SafeAsyncZKCallback.StatCallback() {
                    @Override
                    public void safeProcessResult(int rc, String path, Object ctx, Stat stat) {
                        if (rc == Code.NONODE.intValue()) {
                            // no node
                            callback.operationFailed(ctx, PubSubException.create(StatusCode.NO_TOPIC_PERSISTENCE_INFO,
                                                          "No persistence info found for topic " + topic.toStringUtf8()));
                            return;
                        } else if (rc == Code.BadVersion) {
                            // bad version
                            callback.operationFailed(ctx, PubSubException.create(StatusCode.BAD_VERSION,
                                                          "Bad version provided to update persistence info of topic " + topic.toStringUtf8()));
                            return;
                        } else if (rc == Code.OK.intValue()) {
                            callback.operationFinished(ctx, new ZkVersion(stat.getVersion()));
                            return;
                        } else {
                            KeeperException ke = ZkUtils.logErrorAndCreateZKException(
                                    "Could not write ledgers node for topic: " + topic.toStringUtf8(), path, rc);
                            callback.operationFailed(ctx, new PubSubException.ServiceDownException(ke));
                            return;
                        }
                    }
            }, ctx);
        }

        @Override
        public void deleteTopicPersistenceInfo(final ByteString topic, final Version version,
                                               final Callback<Void> callback, Object ctx) {
            final String zNodePath = ledgersPath(topic);

            int znodeVersion = -1;
            if (Version.ANY != version) {
                if (!(version instanceof ZkVersion)) {
                    callback.operationFailed(ctx, new PubSubException.UnexpectedConditionException(
                                                  "Invalid version provided to delete persistence info for topic " + topic.toStringUtf8()));
                    return;
                } else {
                    znodeVersion = ((ZkVersion)version).getZnodeVersion();
                }
            }
            zk.delete(zNodePath, znodeVersion, new SafeAsyncZKCallback.VoidCallback() {
                @Override
                public void safeProcessResult(int rc, String path, Object ctx) {
                    if (rc == Code.OK.intValue()) {
                        callback.operationFinished(ctx, null);
                        return;
                    } else if (rc == Code.NONODE.intValue()) {
                        // no node
                        callback.operationFailed(ctx, PubSubException.create(StatusCode.NO_TOPIC_PERSISTENCE_INFO,
                                                      "No persistence info found for topic " + topic.toStringUtf8()));
                        return;
                    } else if (rc == Code.BadVersion) {
                        // bad version
                        callback.operationFailed(ctx, PubSubException.create(StatusCode.BAD_VERSION,
                                                      "Bad version provided to delete persistence info of topic " + topic.toStringUtf8()));
                        return;
                    }

                    KeeperException e = ZkUtils.logErrorAndCreateZKException("Topic: " + topic.toStringUtf8()
                                        + " failed to delete persistence info @version " + version + " : ", path, rc);
                    callback.operationFailed(ctx, new PubSubException.ServiceDownException(e));
                }
            }, ctx);
        }
    }

    /**
     * ZooKeeper based subscription data manager.
     */
    static class ZkSubscriptionDataManagerImpl implements SubscriptionDataManager {

        ZooKeeper zk;
        ServerConfiguration cfg;

        ZkSubscriptionDataManagerImpl(ServerConfiguration conf, ZooKeeper zk) {
            this.cfg = conf;
            this.zk = zk;
        }

        @Override
        public void close() throws IOException {
            // do nothing in zookeeper based impl
        }

        /**
         * Get znode path to store subscription states.
         *
         * @param sb
         *          String builder to store the znode path.
         * @param topic
         *          Topic name.
         *
         * @return string builder to store znode path.
         */
        private StringBuilder topicSubscribersPath(StringBuilder sb, ByteString topic) {
            return cfg.getZkTopicPath(sb, topic).append("/subscribers");
        }

        /**
         * Get znode path to store subscription state for a specified subscriber.
         *
         * @param topic
         *          Topic name.
         * @param subscriber
         *          Subscriber id.
         * @return znode path to store subscription state.
         */
        private String topicSubscriberPath(ByteString topic, ByteString subscriber) {
            return topicSubscribersPath(new StringBuilder(), topic).append("/").append(subscriber.toStringUtf8())
                   .toString();
        }

        @Override
        public boolean isPartialUpdateSupported() {
            return false;
        }

        @Override
        public void createSubscriptionData(final ByteString topic, final ByteString subscriberId, final SubscriptionData data,
                                           final Callback<Version> callback, final Object ctx) {
            ZkUtils.createFullPathOptimistic(zk, topicSubscriberPath(topic, subscriberId), data.toByteArray(),
            Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT, new SafeAsyncZKCallback.StringCallback() {

                @Override
                public void safeProcessResult(int rc, String path, Object ctx, String name) {

                    if (rc == Code.NODEEXISTS.intValue()) {
                        callback.operationFailed(ctx, PubSubException.create(StatusCode.SUBSCRIPTION_STATE_EXISTS,
                                                      "Subscription state for (topic:" + topic.toStringUtf8() + ", subscriber:"
                                                      + subscriberId.toStringUtf8() + ") existed."));
                        return;
                    } else if (rc == Code.OK.intValue()) {
                        if (logger.isDebugEnabled()) {
                            logger.debug("Successfully recorded subscription for topic: " + topic.toStringUtf8()
                                         + " subscriberId: " + subscriberId.toStringUtf8() + " data: "
                                         + SubscriptionStateUtils.toString(data));
                        }
                        callback.operationFinished(ctx, new ZkVersion(0));
                    } else {
                        KeeperException ke = ZkUtils.logErrorAndCreateZKException(
                                                 "Could not record new subscription for topic: " + topic.toStringUtf8()
                                                 + " subscriberId: " + subscriberId.toStringUtf8(), path, rc);
                        callback.operationFailed(ctx, new PubSubException.ServiceDownException(ke));
                    }
                }
            }, ctx);
        }

        @Override
        public void updateSubscriptionData(final ByteString topic, final ByteString subscriberId, final SubscriptionData data,
                                           final Version version, final Callback<Version> callback, final Object ctx) {
            throw new UnsupportedOperationException("ZooKeeper based metadata manager doesn't support partial update!");
        }

        @Override
        public void replaceSubscriptionData(final ByteString topic, final ByteString subscriberId, final SubscriptionData data,
                                            final Version version, final Callback<Version> callback, final Object ctx) {
            int znodeVersion = -1;
            if (Version.NEW == version) {
                callback.operationFailed(ctx, 
                        new PubSubException.BadVersionException("Can not replace Version.New subscription data"));
                return;
            } else if (Version.ANY != version) {
                if (!(version instanceof ZkVersion)) {
                    callback.operationFailed(ctx, new PubSubException.UnexpectedConditionException(
                                                  "Invalid version provided to replace subscription data for topic  " 
                                                  + topic.toStringUtf8() + " subscribe id: " + subscriberId));
                    return;
                } else {
                    znodeVersion = ((ZkVersion)version).getZnodeVersion();
                }
            }
            zk.setData(topicSubscriberPath(topic, subscriberId), data.toByteArray(), 
                    znodeVersion, new SafeAsyncZKCallback.StatCallback() {
                @Override
                public void safeProcessResult(int rc, String path, Object ctx, Stat stat) {
                    if (rc == Code.NONODE.intValue()) {
                        // no node
                        callback.operationFailed(ctx, PubSubException.create(StatusCode.NO_SUBSCRIPTION_STATE,
                                                      "No subscription state found for (topic:" + topic.toStringUtf8() + ", subscriber:"
                                                      + subscriberId.toStringUtf8() + ")."));
                        return;
                    } else if (rc == Code.BadVersion) {
                        // bad version
                        callback.operationFailed(ctx, PubSubException.create(StatusCode.BAD_VERSION,
                                                      "Bad version provided to replace subscription data of topic " 
                                                      + topic.toStringUtf8() + " subscriberId " + subscriberId));
                        return;
                    } else if (rc != Code.OK.intValue()) {
                        KeeperException e = ZkUtils.logErrorAndCreateZKException("Topic: " + topic.toStringUtf8()
                                            + " subscriberId: " + subscriberId.toStringUtf8()
                                            + " could not set subscription data: " + SubscriptionStateUtils.toString(data),
                                            path, rc);
                        callback.operationFailed(ctx, new PubSubException.ServiceDownException(e));
                    } else {
                        if (logger.isDebugEnabled()) {
                            logger.debug("Successfully updated subscription for topic: " + topic.toStringUtf8()
                                         + " subscriberId: " + subscriberId.toStringUtf8() + " data: "
                                         + SubscriptionStateUtils.toString(data));
                        }

                        callback.operationFinished(ctx, new ZkVersion(stat.getVersion()));
                    }
                }
            }, ctx);
        }

        @Override
        public void deleteSubscriptionData(final ByteString topic, final ByteString subscriberId, Version version,
                                           final Callback<Void> callback, Object ctx) {
            
            int znodeVersion = -1;
            if (Version.NEW == version) {
                callback.operationFailed(ctx, 
                        new PubSubException.BadVersionException("Can not delete Version.New subscription data"));
                return;
            } else if (Version.ANY != version) {
                if (!(version instanceof ZkVersion)) {
                    callback.operationFailed(ctx, new PubSubException.UnexpectedConditionException(
                                                  "Invalid version provided to delete subscription data for topic  " 
                                                  + topic.toStringUtf8() + " subscribe id: " + subscriberId));
                    return;
                } else {
                    znodeVersion = ((ZkVersion)version).getZnodeVersion();
                }
            }
            
            zk.delete(topicSubscriberPath(topic, subscriberId), znodeVersion, new SafeAsyncZKCallback.VoidCallback() {
                @Override
                public void safeProcessResult(int rc, String path, Object ctx) {
                    if (rc == Code.NONODE.intValue()) {
                        // no node
                        callback.operationFailed(ctx, PubSubException.create(StatusCode.NO_SUBSCRIPTION_STATE,
                                                      "No subscription state found for (topic:" + topic.toStringUtf8() + ", subscriber:"
                                                      + subscriberId.toStringUtf8() + ")."));
                        return;
                    } else if (rc == Code.BadVersion) {
                        // bad version
                        callback.operationFailed(ctx, PubSubException.create(StatusCode.BAD_VERSION,
                                                      "Bad version provided to delete subscription data of topic " 
                                                      + topic.toStringUtf8() + " subscriberId " + subscriberId));
                        return;
                    } else if (rc == Code.OK.intValue()) {
                        if (logger.isDebugEnabled()) {
                            logger.debug("Successfully deleted subscription for topic: " + topic.toStringUtf8()
                                         + " subscriberId: " + subscriberId.toStringUtf8());
                        }

                        callback.operationFinished(ctx, null);
                        return;
                    }

                    KeeperException e = ZkUtils.logErrorAndCreateZKException("Topic: " + topic.toStringUtf8()
                                        + " subscriberId: " + subscriberId.toStringUtf8() + " failed to delete subscription", path, rc);
                    callback.operationFailed(ctx, new PubSubException.ServiceDownException(e));
                }
            }, ctx);
        }

        @Override
        public void readSubscriptionData(final ByteString topic, final ByteString subscriberId,
                                         final Callback<Versioned<SubscriptionData>> callback, final Object ctx) {
            zk.getData(topicSubscriberPath(topic, subscriberId), false, new SafeAsyncZKCallback.DataCallback() {
                @Override
                public void safeProcessResult(int rc, String path, Object ctx, byte[] data, Stat stat) {
                    if (rc == Code.NONODE.intValue()) {
                        callback.operationFinished(ctx, null);
                        return;
                    }
                    if (rc != Code.OK.intValue()) {
                        KeeperException e = ZkUtils.logErrorAndCreateZKException(
                                                "Could not read subscription data for topic: " + topic.toStringUtf8()
                                                + ", subscriberId: " + subscriberId.toStringUtf8(), path, rc);
                        callback.operationFailed(ctx, new PubSubException.ServiceDownException(e));
                        return;
                    }
                    
                    Versioned<SubscriptionData> subData;
                    try {
                        subData = new Versioned<SubscriptionData>(
                                        SubscriptionStateUtils.parseSubscriptionData(data), 
                                        new ZkVersion(stat.getVersion()));
                    } catch (InvalidProtocolBufferException ex) {
                        String msg = "Failed to deserialize subscription data for topic: " + topic.toStringUtf8()
                                     + " subscriberId: " + subscriberId.toStringUtf8();
                        logger.error(msg, ex);
                        callback.operationFailed(ctx, new PubSubException.UnexpectedConditionException(msg));
                        return;
                    }

                    if (logger.isDebugEnabled()) {
                        logger.debug("Found subscription while acquiring topic: " + topic.toStringUtf8()
                                     + " subscriberId: " + subscriberId.toStringUtf8()
                                     + " data: " + SubscriptionStateUtils.toString(subData.getValue()));
                    }
                    callback.operationFinished(ctx, subData);
                }
            }, ctx);
        }

        @Override
        public void readSubscriptions(final ByteString topic,
                                      final Callback<Map<ByteString, Versioned<SubscriptionData>>> cb, final Object ctx) {
            String topicSubscribersPath = topicSubscribersPath(new StringBuilder(), topic).toString();
            zk.getChildren(topicSubscribersPath, false, new SafeAsyncZKCallback.ChildrenCallback() {
                @Override
                public void safeProcessResult(int rc, String path, final Object ctx, final List<String> children) {

                    if (rc != Code.OK.intValue() && rc != Code.NONODE.intValue()) {
                        KeeperException e = ZkUtils.logErrorAndCreateZKException("Could not read subscribers for topic "
                                            + topic.toStringUtf8(), path, rc);
                        cb.operationFailed(ctx, new PubSubException.ServiceDownException(e));
                        return;
                    }

                    final Map<ByteString, Versioned<SubscriptionData>> topicSubs = 
                            new ConcurrentHashMap<ByteString, Versioned<SubscriptionData>>();

                    if (rc == Code.NONODE.intValue() || children.size() == 0) {
                        if (logger.isDebugEnabled()) {
                            logger.debug("No subscriptions found while acquiring topic: " + topic.toStringUtf8());
                        }
                        cb.operationFinished(ctx, topicSubs);
                        return;
                    }

                    final AtomicBoolean failed = new AtomicBoolean();
                    final AtomicInteger count = new AtomicInteger();

                    for (final String child : children) {

                        final ByteString subscriberId = ByteString.copyFromUtf8(child);
                        final String childPath = path + "/" + child;

                        zk.getData(childPath, false, new SafeAsyncZKCallback.DataCallback() {
                            @Override
                            public void safeProcessResult(int rc, String path, Object ctx, byte[] data, Stat stat) {

                                if (rc != Code.OK.intValue()) {
                                    KeeperException e = ZkUtils.logErrorAndCreateZKException(
                                                            "Could not read subscription data for topic: " + topic.toStringUtf8()
                                                            + ", subscriberId: " + subscriberId.toStringUtf8(), path, rc);
                                    reportFailure(new PubSubException.ServiceDownException(e));
                                    return;
                                }

                                if (failed.get()) {
                                    return;
                                }

                                Versioned<SubscriptionData> subData;
                                try {
                                    subData = new Versioned<SubscriptionData>(
                                            SubscriptionStateUtils.parseSubscriptionData(data), 
                                            new ZkVersion(stat.getVersion()));
                                } catch (InvalidProtocolBufferException ex) {
                                    String msg = "Failed to deserialize subscription data for topic: " + topic.toStringUtf8()
                                                 + " subscriberId: " + subscriberId.toStringUtf8();
                                    logger.error(msg, ex);
                                    reportFailure(new PubSubException.UnexpectedConditionException(msg));
                                    return;
                                }

                                if (logger.isDebugEnabled()) {
                                    logger.debug("Found subscription while acquiring topic: " + topic.toStringUtf8()
                                                 + " subscriberId: " + child + "state: "
                                                 + SubscriptionStateUtils.toString(subData.getValue()));
                                }

                                topicSubs.put(subscriberId, subData);
                                if (count.incrementAndGet() == children.size()) {
                                    assert topicSubs.size() == count.get();
                                    cb.operationFinished(ctx, topicSubs);
                                }
                            }

                            private void reportFailure(PubSubException e) {
                                if (failed.compareAndSet(false, true))
                                    cb.operationFailed(ctx, e);
                            }
                        }, ctx);
                    }
                }
            }, ctx);
        }
    }

    /**
     * ZooKeeper base topic ownership manager.
     */
    static class ZkTopicOwnershipManagerImpl implements TopicOwnershipManager {

        ZooKeeper zk;
        ServerConfiguration cfg;

        ZkTopicOwnershipManagerImpl(ServerConfiguration conf, ZooKeeper zk) {
            this.cfg = conf;
            this.zk = zk;
        }

        @Override
        public void close() throws IOException {
            // do nothing in zookeeper based impl
        }

        /**
         * Return znode path to store topic owner.
         *
         * @param topic
         *          Topic Name
         * @return znode path to store topic owner.
         */
        String hubPath(ByteString topic) {
            return cfg.getZkTopicPath(new StringBuilder(), topic).append("/hub").toString();
        }

        @Override
        public void readOwnerInfo(final ByteString topic, final Callback<Versioned<HubInfo>> callback, Object ctx) {
            String ownerPath = hubPath(topic);
            zk.getData(ownerPath, false, new SafeAsyncZKCallback.DataCallback() {
                @Override
                public void safeProcessResult(int rc, String path, Object ctx, byte[] data, Stat stat) {
                    if (Code.NONODE.intValue() == rc) {
                        callback.operationFinished(ctx, null);
                        return;
                    }

                    if (Code.OK.intValue() != rc) {
                        KeeperException e = ZkUtils.logErrorAndCreateZKException("Could not read ownership for topic: "
                                            + topic.toStringUtf8(), path, rc);
                        callback.operationFailed(ctx, new PubSubException.ServiceDownException(e));
                        return;
                    }
                    HubInfo owner = null;
                    try {
                        owner = HubInfo.parse(new String(data));
                    } catch (HubInfo.InvalidHubInfoException ihie) {
                        logger.warn("Failed to parse hub info for topic " + topic.toStringUtf8() + " : ", ihie);
                    }
                    int version = stat.getVersion();
                    callback.operationFinished(ctx, new Versioned<HubInfo>(owner, new ZkVersion(version)));
                    return;
                }
            }, ctx);
        }

        @Override
        public void writeOwnerInfo(final ByteString topic, final HubInfo owner, final Version version,
                                   final Callback<Version> callback, Object ctx) {
            if (Version.NEW == version) {
                createOwnerInfo(topic, owner, callback, ctx);
                return;
            }

            if (!(version instanceof ZkVersion)) {
                callback.operationFailed(ctx, new PubSubException.UnexpectedConditionException(
                                              "Invalid version provided to update owner info for topic " + topic.toStringUtf8()));
                return;
            }

            int znodeVersion = ((ZkVersion)version).getZnodeVersion();
            zk.setData(hubPath(topic), owner.toString().getBytes(), znodeVersion,
                       new SafeAsyncZKCallback.StatCallback() {
                @Override
                public void safeProcessResult(int rc, String path, Object ctx, Stat stat) {
                    if (rc == Code.NONODE.intValue()) {
                        // no node
                        callback.operationFailed(ctx, PubSubException.create(StatusCode.NO_TOPIC_OWNER_INFO,
                                                      "No owner info found for topic " + topic.toStringUtf8()));
                        return;
                    } else if (rc == Code.BadVersion) {
                        // bad version
                        callback.operationFailed(ctx, PubSubException.create(StatusCode.BAD_VERSION,
                                                      "Bad version provided to update owner info of topic " + topic.toStringUtf8()));
                        return;
                    } else if (Code.OK.intValue() == rc) {
                        callback.operationFinished(ctx, new ZkVersion(stat.getVersion()));
                        return;
                    } else {
                        KeeperException e = ZkUtils.logErrorAndCreateZKException(
                            "Failed to update ownership of topic " + topic.toStringUtf8() +
                            " to " + owner, path, rc);
                        callback.operationFailed(ctx, new PubSubException.ServiceDownException(e));
                        return;
                    }
                }
            }, ctx);
        }

        protected void createOwnerInfo(final ByteString topic, final HubInfo owner,
                                       final Callback<Version> callback, Object ctx) {
            String ownerPath = hubPath(topic);
            ZkUtils.createFullPathOptimistic(zk, ownerPath, owner.toString().getBytes(), Ids.OPEN_ACL_UNSAFE,
                                             CreateMode.PERSISTENT, new SafeAsyncZKCallback.StringCallback() {
                @Override
                public void safeProcessResult(int rc, String path, Object ctx, String name) {
                    if (Code.OK.intValue() == rc) {
                        // assume the initial version is 0
                        callback.operationFinished(ctx, new ZkVersion(0));
                        return;
                    } else if (Code.NODEEXISTS.intValue() == rc) {
                        // node existed
                        callback.operationFailed(ctx, PubSubException.create(StatusCode.TOPIC_OWNER_INFO_EXISTS,
                                                      "Owner info of topic " + topic.toStringUtf8() + " existed."));
                        return;
                    } else {
                        KeeperException e = ZkUtils.logErrorAndCreateZKException(
                                                "Failed to create znode for ownership of topic: "
                                                + topic.toStringUtf8(), path, rc);
                        callback.operationFailed(ctx, new PubSubException.ServiceDownException(e));
                        return;
                    }
                }
            }, ctx);
        }

        @Override
        public void deleteOwnerInfo(final ByteString topic, final Version version,
                                    final Callback<Void> callback, Object ctx) {
            int znodeVersion = -1;
            if (Version.ANY != version) {
                if (!(version instanceof ZkVersion)) {
                    callback.operationFailed(ctx, new PubSubException.UnexpectedConditionException(
                                                  "Invalid version provided to delete owner info for topic " + topic.toStringUtf8()));
                    return;
                } else {
                    znodeVersion = ((ZkVersion)version).getZnodeVersion();
                }
            }

            zk.delete(hubPath(topic), znodeVersion, new SafeAsyncZKCallback.VoidCallback() {
                @Override
                public void safeProcessResult(int rc, String path, Object ctx) {
                    if (Code.OK.intValue() == rc) {
                        if (logger.isDebugEnabled()) {
                            logger.debug("Successfully deleted owner info for topic " + topic.toStringUtf8() + ".");
                        }
                        callback.operationFinished(ctx, null);
                        return;
                    } else if (Code.NONODE.intValue() == rc) {
                        // no node
                        callback.operationFailed(ctx, PubSubException.create(StatusCode.NO_TOPIC_OWNER_INFO,
                                                      "No owner info found for topic " + topic.toStringUtf8()));
                        return;
                    } else if (Code.BadVersion == rc) {
                        // bad version
                        callback.operationFailed(ctx, PubSubException.create(StatusCode.BAD_VERSION,
                                                      "Bad version provided to delete owner info of topic " + topic.toStringUtf8()));
                        return;
                    } else {
                        KeeperException e = ZkUtils.logErrorAndCreateZKException(
                                                "Failed to delete owner info for topic "
                                                + topic.toStringUtf8(), path, rc);
                        callback.operationFailed(ctx, new PubSubException.ServiceDownException(e));
                    }
                }
            }, ctx);
        }
    }

    @Override
    public void format(ServerConfiguration cfg, ZooKeeper zk) throws IOException {
        try {
            ZKUtil.deleteRecursive(zk, cfg.getZkTopicsPrefix(new StringBuilder()).toString());
        } catch (KeeperException.NoNodeException e) {
            logger.debug("Hedwig root node doesn't exist in zookeeper to delete");
        } catch (KeeperException ke) {
            throw new IOException(ke);
        } catch (InterruptedException ie) {
            throw new IOException(ie);
        }
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/netty/PubSubServer.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.netty;

import java.io.File;
import java.io.IOException;
import java.net.InetSocketAddress;
import java.net.MalformedURLException;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.SynchronousQueue;
import java.util.concurrent.TimeUnit;

import com.google.common.annotations.VisibleForTesting;
import org.apache.bookkeeper.conf.ClientConfiguration;
import org.apache.bookkeeper.client.BookKeeper;
import org.apache.bookkeeper.client.BKException;
import org.apache.commons.configuration.ConfigurationException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.WatchedEvent;
import org.apache.zookeeper.Watcher;
import org.apache.zookeeper.ZooKeeper;
import org.jboss.netty.bootstrap.ServerBootstrap;
import org.jboss.netty.channel.group.ChannelGroup;
import org.jboss.netty.channel.group.DefaultChannelGroup;
import org.jboss.netty.channel.socket.ClientSocketChannelFactory;
import org.jboss.netty.channel.socket.ServerSocketChannelFactory;
import org.jboss.netty.channel.socket.nio.NioClientSocketChannelFactory;
import org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory;
import org.jboss.netty.logging.InternalLoggerFactory;
import org.jboss.netty.logging.Log4JLoggerFactory;

import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.protocol.PubSubProtocol.OperationType;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.common.TerminateJVMExceptionHandler;
import org.apache.hedwig.server.delivery.DeliveryManager;
import org.apache.hedwig.server.delivery.FIFODeliveryManager;
import org.apache.hedwig.server.handlers.CloseSubscriptionHandler;
import org.apache.hedwig.server.handlers.ConsumeHandler;
import org.apache.hedwig.server.handlers.Handler;
import org.apache.hedwig.server.handlers.NettyHandlerBean;
import org.apache.hedwig.server.handlers.PublishHandler;
import org.apache.hedwig.server.handlers.SubscribeHandler;
import org.apache.hedwig.server.handlers.SubscriptionChannelManager;
import org.apache.hedwig.server.handlers.SubscriptionChannelManager.SubChannelDisconnectedListener;
import org.apache.hedwig.server.handlers.UnsubscribeHandler;
import org.apache.hedwig.server.jmx.HedwigMBeanRegistry;
import org.apache.hedwig.server.meta.MetadataManagerFactory;
import org.apache.hedwig.server.meta.ZkMetadataManagerFactory;
import org.apache.hedwig.server.persistence.BookkeeperPersistenceManager;
import org.apache.hedwig.server.persistence.LocalDBPersistenceManager;
import org.apache.hedwig.server.persistence.PersistenceManager;
import org.apache.hedwig.server.persistence.PersistenceManagerWithRangeScan;
import org.apache.hedwig.server.persistence.ReadAheadCache;
import org.apache.hedwig.server.regions.HedwigHubClientFactory;
import org.apache.hedwig.server.regions.RegionManager;
import org.apache.hedwig.server.ssl.SslServerContextFactory;
import org.apache.hedwig.server.subscriptions.InMemorySubscriptionManager;
import org.apache.hedwig.server.subscriptions.SubscriptionManager;
import org.apache.hedwig.server.subscriptions.MMSubscriptionManager;
import org.apache.hedwig.server.topics.MMTopicManager;
import org.apache.hedwig.server.topics.TopicManager;
import org.apache.hedwig.server.topics.TrivialOwnAllTopicManager;
import org.apache.hedwig.server.topics.ZkTopicManager;
import org.apache.hedwig.util.ConcurrencyUtils;
import org.apache.hedwig.util.Either;
import org.apache.hedwig.zookeeper.SafeAsyncCallback;

public class PubSubServer {

    static Logger logger = LoggerFactory.getLogger(PubSubServer.class);

    private static final String JMXNAME_PREFIX = "PubSubServer_";

    // Netty related variables
    ServerSocketChannelFactory serverChannelFactory;
    ClientSocketChannelFactory clientChannelFactory;
    ServerConfiguration conf;
    org.apache.hedwig.client.conf.ClientConfiguration clientConfiguration;
    ChannelGroup allChannels;

    // Manager components that make up the PubSubServer
    PersistenceManager pm;
    DeliveryManager dm;
    TopicManager tm;
    SubscriptionManager sm;
    RegionManager rm;

    // Metadata Manager Factory
    MetadataManagerFactory mm;

    ZooKeeper zk; // null if we are in standalone mode
    BookKeeper bk; // null if we are in standalone mode

    // we use this to prevent long stack chains from building up in callbacks
    ScheduledExecutorService scheduler;

    // JMX Beans
    NettyHandlerBean jmxNettyBean;
    PubSubServerBean jmxServerBean;
    final ThreadGroup tg;

    protected PersistenceManager instantiatePersistenceManager(TopicManager topicMgr) throws IOException,
        InterruptedException {

        PersistenceManagerWithRangeScan underlyingPM;

        if (conf.isStandalone()) {

            underlyingPM = LocalDBPersistenceManager.instance();

        } else {
            try {
                ClientConfiguration bkConf = new ClientConfiguration();
                bkConf.addConfiguration(conf.getConf());
                bk = new BookKeeper(bkConf, zk, clientChannelFactory);
            } catch (KeeperException e) {
                logger.error("Could not instantiate bookkeeper client", e);
                throw new IOException(e);
            }
            underlyingPM = new BookkeeperPersistenceManager(bk, mm, topicMgr, conf, scheduler);

        }

        PersistenceManager pm = underlyingPM;

        if (conf.getReadAheadEnabled()) {
            pm = new ReadAheadCache(underlyingPM, conf).start();
        }

        return pm;
    }

    protected SubscriptionManager instantiateSubscriptionManager(TopicManager tm, PersistenceManager pm,
                                                                 DeliveryManager dm) {
        if (conf.isStandalone()) {
            return new InMemorySubscriptionManager(conf, tm, pm, dm, scheduler);
        } else {
            return new MMSubscriptionManager(conf, mm, tm, pm, dm, scheduler);
        }

    }

    protected RegionManager instantiateRegionManager(PersistenceManager pm, ScheduledExecutorService scheduler) {
        return new RegionManager(pm, conf, zk, scheduler, new HedwigHubClientFactory(conf, clientConfiguration,
                clientChannelFactory));
    }

    protected void instantiateZookeeperClient() throws Exception {
        if (!conf.isStandalone()) {
            final CountDownLatch signalZkReady = new CountDownLatch(1);

            zk = new ZooKeeper(conf.getZkHost(), conf.getZkTimeout(), new Watcher() {
                @Override
                public void process(WatchedEvent event) {
                    if(Event.KeeperState.SyncConnected.equals(event.getState())) {
                        signalZkReady.countDown();
                    }
                }
            });
            // wait until connection is effective
            if (!signalZkReady.await(conf.getZkTimeout()*2, TimeUnit.MILLISECONDS)) {
                logger.error("Could not establish connection with ZooKeeper after zk_timeout*2 = " +
                             conf.getZkTimeout()*2 + " ms. (Default value for zk_timeout is 2000).");
                throw new Exception("Could not establish connection with ZooKeeper after zk_timeout*2 = " +
                                    conf.getZkTimeout()*2 + " ms. (Default value for zk_timeout is 2000).");
            }
        }
    }

    protected void instantiateMetadataManagerFactory() throws Exception {
        if (conf.isStandalone()) {
            return;
        }
        mm = MetadataManagerFactory.newMetadataManagerFactory(conf, zk);
    }

    protected TopicManager instantiateTopicManager() throws IOException {
        TopicManager tm;

        if (conf.isStandalone()) {
            tm = new TrivialOwnAllTopicManager(conf, scheduler);
        } else {
            try {
                if (conf.isMetadataManagerBasedTopicManagerEnabled()) {
                    tm = new MMTopicManager(conf, zk, mm, scheduler);
                } else {
                    if (!(mm instanceof ZkMetadataManagerFactory)) {
                        throw new IOException("Uses " + mm.getClass().getName() + " to store hedwig metadata, "
                                            + "but uses zookeeper ephemeral znodes to store topic ownership. "
                                            + "Check your configuration as this could lead to scalability issues.");
                    }
                    tm = new ZkTopicManager(zk, conf, scheduler);
                }
            } catch (PubSubException e) {
                logger.error("Could not instantiate TopicOwnershipManager based topic manager", e);
                throw new IOException(e);
            }
        }
        return tm;
    }

   protected Map<OperationType, Handler> initializeNettyHandlers(
           TopicManager tm, DeliveryManager dm,
           PersistenceManager pm, SubscriptionManager sm,
           SubscriptionChannelManager subChannelMgr) {
        Map<OperationType, Handler> handlers = new HashMap<OperationType, Handler>();
        handlers.put(OperationType.PUBLISH, new PublishHandler(tm, pm, conf));
        handlers.put(OperationType.SUBSCRIBE,
                     new SubscribeHandler(conf, tm, dm, pm, sm, subChannelMgr));
        handlers.put(OperationType.UNSUBSCRIBE,
                     new UnsubscribeHandler(conf, tm, sm, dm, subChannelMgr));
        handlers.put(OperationType.CONSUME, new ConsumeHandler(tm, sm, conf));
        handlers.put(OperationType.CLOSESUBSCRIPTION,
                     new CloseSubscriptionHandler(conf, tm, sm, dm, subChannelMgr));
        handlers = Collections.unmodifiableMap(handlers);
        return handlers;
    }

    protected void initializeNetty(SslServerContextFactory sslFactory,
                                   Map<OperationType, Handler> handlers,
                                   SubscriptionChannelManager subChannelMgr) {
        boolean isSSLEnabled = (sslFactory != null) ? true : false;
        InternalLoggerFactory.setDefaultFactory(new Log4JLoggerFactory());
        ServerBootstrap bootstrap = new ServerBootstrap(serverChannelFactory);
        UmbrellaHandler umbrellaHandler =
            new UmbrellaHandler(allChannels, handlers, subChannelMgr, isSSLEnabled);
        PubSubServerPipelineFactory pipeline =
            new PubSubServerPipelineFactory(umbrellaHandler, sslFactory,
                                            conf.getMaximumMessageSize());

        bootstrap.setPipelineFactory(pipeline);
        bootstrap.setOption("child.tcpNoDelay", true);
        bootstrap.setOption("child.keepAlive", true);
        bootstrap.setOption("reuseAddress", true);

        // Bind and start to accept incoming connections.
        allChannels.add(bootstrap.bind(isSSLEnabled ? new InetSocketAddress(conf.getSSLServerPort())
                                       : new InetSocketAddress(conf.getServerPort())));
        logger.info("Going into receive loop");
    }

    public void shutdown() {
        // TODO: tell bk to close logs

        // Stop topic manager first since it is core of Hub server
        tm.stop();

        // Stop the RegionManager.
        rm.stop();

        // Stop the DeliveryManager and ReadAheadCache threads (if
        // applicable).
        dm.stop();
        pm.stop();

        // Stop the SubscriptionManager if needed.
        sm.stop();

        // Shutdown metadata manager if needed
        if (null != mm) {
            try {
                mm.shutdown();
            } catch (IOException ie) {
                logger.error("Error while shutdown metadata manager factory!", ie);
            }
        }

        // Shutdown the ZooKeeper and BookKeeper clients only if we are
        // not in stand-alone mode.
        try {
            if (bk != null)
                bk.close();
            if (zk != null)
                zk.close();
        } catch (InterruptedException e) {
            logger.error("Error while closing ZooKeeper client : ", e);
        } catch (BKException bke) {
            logger.error("Error while closing BookKeeper client : ", bke);
        }

        // Close and release the Netty channels and resources
        allChannels.close().awaitUninterruptibly();
        serverChannelFactory.releaseExternalResources();
        clientChannelFactory.releaseExternalResources();
        scheduler.shutdown();

        // unregister jmx
        unregisterJMX();
    }

    protected void registerJMX(SubscriptionChannelManager subChannelMgr) {
        try {
            String jmxName = JMXNAME_PREFIX + conf.getServerPort() + "_"
                                            + conf.getSSLServerPort();
            jmxServerBean = new PubSubServerBean(jmxName);
            HedwigMBeanRegistry.getInstance().register(jmxServerBean, null);
            try {
                jmxNettyBean = new NettyHandlerBean(subChannelMgr);
                HedwigMBeanRegistry.getInstance().register(jmxNettyBean, jmxServerBean);
            } catch (Exception e) {
                logger.warn("Failed to register with JMX", e);
                jmxNettyBean = null;
            }
        } catch (Exception e) {
            logger.warn("Failed to register with JMX", e);
            jmxServerBean = null;
        }
        if (pm instanceof ReadAheadCache) {
            ((ReadAheadCache)pm).registerJMX(jmxServerBean);
        }
    }

    protected void unregisterJMX() {
        if (pm != null && pm instanceof ReadAheadCache) {
            ((ReadAheadCache)pm).unregisterJMX();
        }
        try {
            if (jmxNettyBean != null) {
                HedwigMBeanRegistry.getInstance().unregister(jmxNettyBean);
            }
        } catch (Exception e) {
            logger.warn("Failed to unregister with JMX", e);
        }
        try {
            if (jmxServerBean != null) {
                HedwigMBeanRegistry.getInstance().unregister(jmxServerBean);
            }
        } catch (Exception e) {
            logger.warn("Failed to unregister with JMX", e);
        }
        jmxNettyBean = null;
        jmxServerBean = null;
    }

    /**
     * Starts the hedwig server on the given port
     *
     * @param port
     * @throws ConfigurationException
     *             if there is something wrong with the given configuration
     * @throws IOException
     * @throws InterruptedException
     * @throws ConfigurationException
     */
    public PubSubServer(final ServerConfiguration serverConfiguration,
                        final org.apache.hedwig.client.conf.ClientConfiguration clientConfiguration,
                        final Thread.UncaughtExceptionHandler exceptionHandler)
            throws ConfigurationException {

        // First validate the serverConfiguration
        this.conf = serverConfiguration;
        serverConfiguration.validate();

        // Validate the client configuration
        this.clientConfiguration = clientConfiguration;
        clientConfiguration.validate();

        // We need a custom thread group, so that we can override the uncaught
        // exception method
        tg = new ThreadGroup("hedwig") {
            @Override
            public void uncaughtException(Thread t, Throwable e) {
                exceptionHandler.uncaughtException(t, e);
            }
        };
        // ZooKeeper threads register their own handler. But if some work that
        // we do in ZK threads throws an exception, we want our handler to be
        // called, not theirs.
        SafeAsyncCallback.setUncaughtExceptionHandler(exceptionHandler);
    }

    public void start() throws Exception {
        final SynchronousQueue<Either<Object, Exception>> queue = new SynchronousQueue<Either<Object, Exception>>();

        new Thread(tg, new Runnable() {
            @Override
            public void run() {
                try {
                    // Since zk is needed by almost everyone,try to see if we
                    // need that first
                    scheduler = Executors.newSingleThreadScheduledExecutor();
                    serverChannelFactory = new NioServerSocketChannelFactory(Executors.newCachedThreadPool(), Executors
                            .newCachedThreadPool());
                    clientChannelFactory = new NioClientSocketChannelFactory(Executors.newCachedThreadPool(), Executors
                            .newCachedThreadPool());

                    instantiateZookeeperClient();
                    instantiateMetadataManagerFactory();
                    tm = instantiateTopicManager();
                    pm = instantiatePersistenceManager(tm);
                    dm = new FIFODeliveryManager(pm, conf);
                    dm.start();

                    sm = instantiateSubscriptionManager(tm, pm, dm);
                    rm = instantiateRegionManager(pm, scheduler);
                    sm.addListener(rm);

                    allChannels = new DefaultChannelGroup("hedwig");
                    // Initialize the Netty Handlers (used by the
                    // UmbrellaHandler) once so they can be shared by
                    // both the SSL and non-SSL channels.
                    SubscriptionChannelManager subChannelMgr = new SubscriptionChannelManager();
                    subChannelMgr.addSubChannelDisconnectedListener((SubChannelDisconnectedListener) dm);
                    Map<OperationType, Handler> handlers =
                        initializeNettyHandlers(tm, dm, pm, sm, subChannelMgr);
                    // Initialize Netty for the regular non-SSL channels
                    initializeNetty(null, handlers, subChannelMgr);
                    if (conf.isSSLEnabled()) {
                        initializeNetty(new SslServerContextFactory(conf),
                                        handlers, subChannelMgr);
                    }
                    // register jmx
                    registerJMX(subChannelMgr);
                } catch (Exception e) {
                    ConcurrencyUtils.put(queue, Either.right(e));
                    return;
                }

                ConcurrencyUtils.put(queue, Either.of(new Object(), (Exception) null));
            }

        }).start();

        Either<Object, Exception> either = ConcurrencyUtils.take(queue);
        if (either.left() == null) {
            throw either.right();
        }
    }

    public PubSubServer(ServerConfiguration serverConfiguration,
                        org.apache.hedwig.client.conf.ClientConfiguration clientConfiguration) throws Exception {
        this(serverConfiguration, clientConfiguration, new TerminateJVMExceptionHandler());
    }

    public PubSubServer(ServerConfiguration serverConfiguration) throws Exception {
        this(serverConfiguration, new org.apache.hedwig.client.conf.ClientConfiguration());
    }

    @VisibleForTesting
    public DeliveryManager getDeliveryManager() {
        return dm;
    }

    /**
     *
     * @param msg
     * @param rc
     *            : code to exit with
     */
    public static void errorMsgAndExit(String msg, Throwable t, int rc) {
        logger.error(msg, t);
        System.err.println(msg);
        System.exit(rc);
    }

    public final static int RC_INVALID_CONF_FILE = 1;
    public final static int RC_MISCONFIGURED = 2;
    public final static int RC_OTHER = 3;

    /**
     * @param args
     */
    public static void main(String[] args) {

        logger.info("Attempting to start Hedwig");
        ServerConfiguration serverConfiguration = new ServerConfiguration();
        // The client configuration for the hedwig client in the region manager.
        org.apache.hedwig.client.conf.ClientConfiguration regionMgrClientConfiguration
                = new org.apache.hedwig.client.conf.ClientConfiguration();
        if (args.length > 0) {
            String confFile = args[0];
            try {
                serverConfiguration.loadConf(new File(confFile).toURI().toURL());
            } catch (MalformedURLException e) {
                String msg = "Could not open server configuration file: " + confFile;
                errorMsgAndExit(msg, e, RC_INVALID_CONF_FILE);
            } catch (ConfigurationException e) {
                String msg = "Malformed server configuration file: " + confFile;
                errorMsgAndExit(msg, e, RC_MISCONFIGURED);
            }
            logger.info("Using configuration file " + confFile);
        }
        if (args.length > 1) {
            // args[1] is the client configuration file.
            String confFile = args[1];
            try {
                regionMgrClientConfiguration.loadConf(new File(confFile).toURI().toURL());
            } catch (MalformedURLException e) {
                String msg = "Could not open client configuration file: " + confFile;
                errorMsgAndExit(msg, e, RC_INVALID_CONF_FILE);
            } catch (ConfigurationException e) {
                String msg = "Malformed client configuration file: " + confFile;
                errorMsgAndExit(msg, e, RC_MISCONFIGURED);
            }
        }
        try {
            new PubSubServer(serverConfiguration, regionMgrClientConfiguration).start();
        } catch (Throwable t) {
            errorMsgAndExit("Error during startup", t, RC_OTHER);
        }
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/netty/PubSubServerBean.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hedwig.server.netty;

import org.apache.hedwig.server.jmx.HedwigMBeanInfo;
import org.apache.hedwig.server.netty.ServerStats.OpStatData;

import org.apache.hedwig.protocol.PubSubProtocol.OperationType;

/**
 * PubSub Server Bean
 */
public class PubSubServerBean implements PubSubServerMXBean, HedwigMBeanInfo {

    private final String name;

    public PubSubServerBean(String jmxName) {
        this.name = jmxName;
    }

    @Override
    public String getName() {
        return name;
    }

    @Override
    public boolean isHidden() {
        return false;
    }

    @Override
    public OpStatData getPubStats() {
        return ServerStats.getInstance().getOpStats(OperationType.PUBLISH).toOpStatData();
    }

    @Override
    public OpStatData getSubStats() {
        return ServerStats.getInstance().getOpStats(OperationType.SUBSCRIBE).toOpStatData();
    }

    @Override
    public OpStatData getUnsubStats() {
        return ServerStats.getInstance().getOpStats(OperationType.UNSUBSCRIBE).toOpStatData();
    }

    @Override
    public OpStatData getConsumeStats() {
        return ServerStats.getInstance().getOpStats(OperationType.CONSUME).toOpStatData();
    }

    @Override
    public long getNumRequestsReceived() {
        return ServerStats.getInstance().getNumRequestsReceived();
    }

    @Override
    public long getNumRequestsRedirect() {
        return ServerStats.getInstance().getNumRequestsRedirect();
    }

    @Override
    public long getNumMessagesDelivered() {
        return ServerStats.getInstance().getNumMessagesDelivered();
    }


}
"
hedwig-server/src/main/java/org/apache/hedwig/server/netty/PubSubServerMXBean.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hedwig.server.netty;

import org.apache.hedwig.server.netty.ServerStats.OpStatData;

/**
 * PubSub Server MBean
 */
public interface PubSubServerMXBean {

    /**
     * @return publish stats
     */
    public OpStatData getPubStats();

    /**
     * @return subscription stats
     */
    public OpStatData getSubStats();

    /**
     * @return unsub stats
     */
    public OpStatData getUnsubStats();

    /**
     * @return consume stats
     */
    public OpStatData getConsumeStats();

    /**
     * @return number of requests received
     */
    public long getNumRequestsReceived();

    /**
     * @return number of requests redirect
     */
    public long getNumRequestsRedirect();

    /**
     * @return number of messages delivered
     */
    public long getNumMessagesDelivered();

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/netty/PubSubServerPipelineFactory.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.netty;

import org.jboss.netty.channel.ChannelPipeline;
import org.jboss.netty.channel.ChannelPipelineFactory;
import org.jboss.netty.channel.Channels;
import org.jboss.netty.handler.codec.frame.LengthFieldBasedFrameDecoder;
import org.jboss.netty.handler.codec.frame.LengthFieldPrepender;
import org.jboss.netty.handler.codec.protobuf.ProtobufDecoder;
import org.jboss.netty.handler.codec.protobuf.ProtobufEncoder;
import org.jboss.netty.handler.ssl.SslHandler;

import org.apache.hedwig.protocol.PubSubProtocol;
import org.apache.hedwig.server.ssl.SslServerContextFactory;

public class PubSubServerPipelineFactory implements ChannelPipelineFactory {

    // TODO: make these conf settings
    final static int MAX_WORKER_THREADS = 32;
    final static int MAX_CHANNEL_MEMORY_SIZE = 10 * 1024 * 1024;
    final static int MAX_TOTAL_MEMORY_SIZE = 100 * 1024 * 1024;

    private UmbrellaHandler uh;
    private SslServerContextFactory sslFactory;
    private int maxMessageSize;

    /**
     *
     * @param uh
     * @param sslFactory
     *            may be null if ssl is disabled
     * @param cfg
     */
    public PubSubServerPipelineFactory(UmbrellaHandler uh, SslServerContextFactory sslFactory, int maxMessageSize) {
        this.uh = uh;
        this.sslFactory = sslFactory;
        this.maxMessageSize = maxMessageSize;
    }

    public ChannelPipeline getPipeline() throws Exception {
        ChannelPipeline pipeline = Channels.pipeline();
        if (sslFactory != null) {
            pipeline.addLast("ssl", new SslHandler(sslFactory.getEngine()));
        }
        pipeline.addLast("lengthbaseddecoder",
                         new LengthFieldBasedFrameDecoder(maxMessageSize, 0, 4, 0, 4));
        pipeline.addLast("lengthprepender", new LengthFieldPrepender(4));

        pipeline.addLast("protobufdecoder", new ProtobufDecoder(PubSubProtocol.PubSubRequest.getDefaultInstance()));
        pipeline.addLast("protobufencoder", new ProtobufEncoder());

        // pipeline.addLast("executor", new ExecutionHandler(
        // new OrderedMemoryAwareThreadPoolExecutor(MAX_WORKER_THREADS,
        // MAX_CHANNEL_MEMORY_SIZE, MAX_TOTAL_MEMORY_SIZE)));
        //
        // Dependency injection.
        pipeline.addLast("umbrellahandler", uh);
        return pipeline;
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/netty/ServerStats.java,false,"/*
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hedwig.server.netty;

import java.util.HashMap;
import java.util.Map;

import java.beans.ConstructorProperties;
import java.util.concurrent.atomic.AtomicLong;

import org.apache.hedwig.protocol.PubSubProtocol.OperationType;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Server Stats
 */
public class ServerStats {
    private static final Logger LOG = LoggerFactory.getLogger(ServerStats.class);
    static ServerStats instance = new ServerStats();

    /**
     * A read view of stats, also used in CompositeViewData to expose to JMX
     */
    public static class OpStatData {
        private final long maxLatency, minLatency;
        private final double avgLatency;
        private final long numSuccessOps, numFailedOps;
        private final String latencyHist;

        @ConstructorProperties({"maxLatency", "minLatency", "avgLatency",
                                "numSuccessOps", "numFailedOps", "latencyHist"})
        public OpStatData(long maxLatency, long minLatency, double avgLatency,
                          long numSuccessOps, long numFailedOps, String latencyHist) {
            this.maxLatency = maxLatency;
            this.minLatency = minLatency == Long.MAX_VALUE ? 0 : minLatency;
            this.avgLatency = avgLatency;
            this.numSuccessOps = numSuccessOps;
            this.numFailedOps = numFailedOps;
            this.latencyHist = latencyHist;
        }

        public long getMaxLatency() {
            return maxLatency;
        }

        public long getMinLatency() {
            return minLatency;
        }

        public double getAvgLatency() {
            return avgLatency;
        }

        public long getNumSuccessOps() {
            return numSuccessOps;
        }

        public long getNumFailedOps() {
            return numFailedOps;
        }

        public String getLatencyHist() {
            return latencyHist;
        }
    }

    /**
     * Operation Statistics
     */
    public static class OpStats {
        static final int NUM_BUCKETS = 3*9 + 2;

        long maxLatency = 0;
        long minLatency = Long.MAX_VALUE;
        double totalLatency = 0.0f;
        long numSuccessOps = 0;
        long numFailedOps = 0;
        long[] latencyBuckets = new long[NUM_BUCKETS];

        OpStats() {}

        /**
         * Increment number of failed operations
         */
        synchronized public void incrementFailedOps() {
            ++numFailedOps;
        }

        /**
         * Update Latency
         */
        synchronized public void updateLatency(long latency) {
            if (latency < 0) {
                // less than 0ms . Ideally this should not happen.
                // We have seen this latency negative in some cases due to the
                // behaviors of JVM. Ignoring the statistics updation for such
                // cases.
                LOG.warn("Latency time coming negative");
                return;
            }
            totalLatency += latency;
            ++numSuccessOps;
            if (latency < minLatency) {
                minLatency = latency;
            }
            if (latency > maxLatency) {
                maxLatency = latency;
            }
            int bucket;
            if (latency <= 100) { // less than 100ms
                bucket = (int)(latency / 10);
            } else if (latency <= 1000) { // 100ms ~ 1000ms
                bucket = 1 * 9 + (int)(latency / 100);
            } else if (latency <= 10000) { // 1s ~ 10s
                bucket = 2 * 9 + (int)(latency / 1000);
            } else { // more than 10s
                bucket = 3 * 9 + 1;
            }
            ++latencyBuckets[bucket];
        }

        synchronized public OpStatData toOpStatData() {
            double avgLatency = numSuccessOps > 0 ? totalLatency / numSuccessOps : 0.0f;
            StringBuilder sb = new StringBuilder();
            for (int i=0; i<NUM_BUCKETS; i++) {
                sb.append(latencyBuckets[i]);
                if (i != NUM_BUCKETS - 1) {
                    sb.append(',');
                }
            }

            return new OpStatData(maxLatency, minLatency, avgLatency,
                                  numSuccessOps, numFailedOps, sb.toString());
        }

    }

    public static ServerStats getInstance() {
        return instance;
    }

    protected ServerStats() {
        stats = new HashMap<OperationType, OpStats>();
        for (OperationType type : OperationType.values()) {
            stats.put(type, new OpStats());
        }
    }
    Map<OperationType, OpStats> stats;


    AtomicLong numRequestsReceived = new AtomicLong(0);
    AtomicLong numRequestsRedirect = new AtomicLong(0);
    AtomicLong numMessagesDelivered = new AtomicLong(0);

    /**
     * Stats of operations
     *
     * @param type
     *          Operation Type
     * @return op stats
     */
    public OpStats getOpStats(OperationType type) {
        return stats.get(type);
    }

    public void incrementRequestsReceived() {
        numRequestsReceived.incrementAndGet();
    }

    public void incrementRequestsRedirect() {
        numRequestsRedirect.incrementAndGet();
    }

    public void incrementMessagesDelivered() {
        numMessagesDelivered.incrementAndGet();
    }

    public long getNumRequestsReceived() {
        return numRequestsReceived.get();
    }

    public long getNumRequestsRedirect() {
        return numRequestsRedirect.get();
    }

    public long getNumMessagesDelivered() {
        return numMessagesDelivered.get();
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/netty/UmbrellaHandler.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.netty;

import java.io.IOException;
import java.util.Map;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;
import org.jboss.netty.channel.ChannelFuture;
import org.jboss.netty.channel.ChannelFutureListener;
import org.jboss.netty.channel.ChannelHandlerContext;
import org.jboss.netty.channel.ChannelPipelineCoverage;
import org.jboss.netty.channel.ChannelStateEvent;
import org.jboss.netty.channel.ExceptionEvent;
import org.jboss.netty.channel.MessageEvent;
import org.jboss.netty.channel.SimpleChannelHandler;
import org.jboss.netty.channel.group.ChannelGroup;
import org.jboss.netty.handler.codec.frame.CorruptedFrameException;
import org.jboss.netty.handler.codec.frame.TooLongFrameException;
import org.jboss.netty.handler.ssl.SslHandler;

import org.apache.hedwig.exceptions.PubSubException.MalformedRequestException;
import org.apache.hedwig.protocol.PubSubProtocol;
import org.apache.hedwig.protocol.PubSubProtocol.OperationType;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse;
import org.apache.hedwig.protoextensions.PubSubResponseUtils;
import org.apache.hedwig.server.handlers.ChannelDisconnectListener;
import org.apache.hedwig.server.handlers.Handler;

@ChannelPipelineCoverage("all")
public class UmbrellaHandler extends SimpleChannelHandler {
    static Logger logger = LoggerFactory.getLogger(UmbrellaHandler.class);

    private final Map<OperationType, Handler> handlers;
    private final ChannelGroup allChannels;
    private final ChannelDisconnectListener channelDisconnectListener;
    private final boolean isSSLEnabled; 

    public UmbrellaHandler(ChannelGroup allChannels, Map<OperationType, Handler> handlers,
                           ChannelDisconnectListener channelDisconnectListener,
                           boolean isSSLEnabled) {
        this.allChannels = allChannels;
        this.isSSLEnabled = isSSLEnabled;
        this.handlers = handlers;
        this.channelDisconnectListener = channelDisconnectListener;
    }

    @Override
    public void exceptionCaught(ChannelHandlerContext ctx, ExceptionEvent e) throws Exception {
        Throwable throwable = e.getCause();

        // Add here if there are more exceptions we need to be able to tolerate.
        // 1. IOException may be thrown when a channel is forcefully closed by
        // the other end, or by the ProtobufDecoder when an invalid protobuf is
        // received
        // 2. TooLongFrameException is thrown by the LengthBasedDecoder if it
        // receives a packet that is too big
        // 3. CorruptedFramException is thrown by the LengthBasedDecoder when
        // the length is negative etc.
        if (throwable instanceof IOException || throwable instanceof TooLongFrameException
                || throwable instanceof CorruptedFrameException) {
            e.getChannel().close();
            logger.debug("Uncaught exception", throwable);
        } else {
            // call our uncaught exception handler, which might decide to
            // shutdown the system
            Thread thread = Thread.currentThread();
            thread.getUncaughtExceptionHandler().uncaughtException(thread, throwable);
        }

    }

    @Override
    public void channelOpen(ChannelHandlerContext ctx, ChannelStateEvent e) throws Exception {
        // If SSL is NOT enabled, then we can add this channel to the
        // ChannelGroup. Otherwise, that is done when the channel is connected
        // and the SSL handshake has completed successfully.
        if (!isSSLEnabled) {
            allChannels.add(ctx.getChannel());
        }
    }

    @Override
    public void channelConnected(ChannelHandlerContext ctx, ChannelStateEvent e) throws Exception {
        if (isSSLEnabled) {
            ctx.getPipeline().get(SslHandler.class).handshake(e.getChannel()).addListener(new ChannelFutureListener() {
                public void operationComplete(ChannelFuture future) throws Exception {
                    if (future.isSuccess()) {
                        logger.debug("SSL handshake has completed successfully!");
                        allChannels.add(future.getChannel());
                    } else {
                        future.getChannel().close();
                    }
                }
            });
        }
    }

    @Override
    public void channelDisconnected(ChannelHandlerContext ctx, ChannelStateEvent e) throws Exception {
        Channel channel = ctx.getChannel();
        // subscribe handler needs to know about channel disconnects
        channelDisconnectListener.channelDisconnected(channel);
        channel.close();
    }

    public static void sendErrorResponseToMalformedRequest(Channel channel, long txnId, String msg) {
        logger.debug("Malformed request from {}, msg = {}", channel.getRemoteAddress(), msg);
        MalformedRequestException mre = new MalformedRequestException(msg);
        PubSubResponse response = PubSubResponseUtils.getResponseForException(mre, txnId);
        channel.write(response);
    }

    @Override
    public void messageReceived(ChannelHandlerContext ctx, MessageEvent e) throws Exception {

        if (!(e.getMessage() instanceof PubSubProtocol.PubSubRequest)) {
            ctx.sendUpstream(e);
            return;
        }

        PubSubProtocol.PubSubRequest request = (PubSubProtocol.PubSubRequest) e.getMessage();

        Handler handler = handlers.get(request.getType());
        Channel channel = ctx.getChannel();
        long txnId = request.getTxnId();

        if (handler == null) {
            sendErrorResponseToMalformedRequest(channel, txnId, "Request type " + request.getType().getNumber()
                                                + " unknown");
            return;
        }

        handler.handleRequest(request, channel);
        ServerStats.getInstance().incrementRequestsReceived();
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/persistence/BookkeeperPersistenceManager.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.persistence;

import java.io.IOException;
import java.util.Enumeration;
import java.util.Iterator;
import java.util.HashSet;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.TreeMap;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.atomic.AtomicBoolean;

import org.apache.bookkeeper.client.AsyncCallback.CloseCallback;
import org.apache.bookkeeper.client.AsyncCallback.DeleteCallback;
import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.client.BookKeeper;
import org.apache.bookkeeper.client.LedgerEntry;
import org.apache.bookkeeper.client.LedgerHandle;
import org.apache.bookkeeper.client.BookKeeper.DigestType;
import org.apache.bookkeeper.versioning.Version;
import org.apache.bookkeeper.versioning.Versioned;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.protobuf.ByteString;
import com.google.protobuf.InvalidProtocolBufferException;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.ServerNotResponsibleForTopicException;
import org.apache.hedwig.protocol.PubSubProtocol.LedgerRange;
import org.apache.hedwig.protocol.PubSubProtocol.LedgerRanges;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protoextensions.MessageIdUtils;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.common.TopicOpQueuer;
import org.apache.hedwig.server.common.UnexpectedError;
import org.apache.hedwig.server.meta.MetadataManagerFactory;
import org.apache.hedwig.server.meta.TopicPersistenceManager;
import org.apache.hedwig.server.persistence.ScanCallback.ReasonForFinish;
import org.apache.hedwig.server.topics.TopicManager;
import org.apache.hedwig.server.topics.TopicOwnershipChangeListener;
import org.apache.hedwig.util.Callback;
import org.apache.hedwig.zookeeper.SafeAsynBKCallback;
import static org.apache.hedwig.util.VarArgs.va;

/**
 * This persistence manager uses zookeeper and bookkeeper to store messages.
 *
 * Information about topics are stored in zookeeper with a znode named after the
 * topic that contains an ASCII encoded list with records of the following form:
 *
 * <pre>
 * startSeqId(included)\tledgerId\n
 * </pre>
 *
 */

public class BookkeeperPersistenceManager implements PersistenceManagerWithRangeScan, TopicOwnershipChangeListener {
    static Logger logger = LoggerFactory.getLogger(BookkeeperPersistenceManager.class);
    static byte[] passwd = "sillysecret".getBytes();
    private BookKeeper bk;
    private TopicPersistenceManager tpManager;
    private ServerConfiguration cfg;
    private TopicManager tm;

    private static final long START_SEQ_ID = 1L;
    // max number of entries allowed in a ledger
    private static final long UNLIMITED_ENTRIES = 0L;
    private final long maxEntriesPerLedger;

    static class InMemoryLedgerRange {
        LedgerRange range;
        LedgerHandle handle;

        public InMemoryLedgerRange(LedgerRange range, LedgerHandle handle) {
            this.range = range;
            this.handle = handle;
        }

        public InMemoryLedgerRange(LedgerRange range) {
            this(range, null);
        }

        public long getStartSeqIdIncluded() {
            assert range.hasStartSeqIdIncluded();
            return range.getStartSeqIdIncluded();
        }
    }

    static class TopicInfo {
        /**
         * stores the last message-seq-id vector that has been pushed to BK for
         * persistence (but not necessarily acked yet by BK)
         *
         */
        MessageSeqId lastSeqIdPushed;

        /**
         * stores the last message-id that has been acked by BK. This number is
         * basically used for limiting scans to not read past what has been
         * persisted by BK
         */
        long lastEntryIdAckedInCurrentLedger = -1; // because BK ledgers starts
        // at 0

        /**
         * stores a sorted structure of the ledgers for a topic, mapping from
         * the endSeqIdIncluded to the ledger info. This structure does not
         * include the current ledger
         */
        TreeMap<Long, InMemoryLedgerRange> ledgerRanges = new TreeMap<Long, InMemoryLedgerRange>();
        Version ledgerRangesVersion = Version.NEW;

        /**
         * This is the handle of the current ledger that is being used to write
         * messages
         */
        InMemoryLedgerRange currentLedgerRange;

        /**
         * Flag to release topic when encountering unrecoverable exceptions
         */
        AtomicBoolean doRelease = new AtomicBoolean(false);

        /**
         * Flag indicats the topic is changing ledger
         */
        AtomicBoolean doChangeLedger = new AtomicBoolean(false);
        /**
         * Last seq id to change ledger.
         */
        long lastSeqIdBeforeLedgerChange = -1;
        /**
         * List to buffer all persist requests during changing ledger.
         */
        LinkedList<PersistRequest> deferredRequests = null;

        final static int UNLIMITED = 0;
        int messageBound = UNLIMITED;
    }

    Map<ByteString, TopicInfo> topicInfos = new ConcurrentHashMap<ByteString, TopicInfo>();

    TopicOpQueuer queuer;

    /**
     * Instantiates a BookKeeperPersistence manager.
     *
     * @param bk
     *            a reference to bookkeeper to use.
     * @param metaManagerFactory
     *            a metadata manager factory handle to use.
     * @param tm
     *            a reference to topic manager.
     * @param cfg
     *            Server configuration object
     * @param executor
     *            A executor
     */
    public BookkeeperPersistenceManager(BookKeeper bk, MetadataManagerFactory metaManagerFactory,
                                        TopicManager tm, ServerConfiguration cfg,
                                        ScheduledExecutorService executor) {
        this.bk = bk;
        this.tpManager = metaManagerFactory.newTopicPersistenceManager();
        this.cfg = cfg;
        this.tm = tm;
        this.maxEntriesPerLedger = cfg.getMaxEntriesPerLedger();
        queuer = new TopicOpQueuer(executor);
        tm.addTopicOwnershipChangeListener(this);
    }

    private static LedgerRange buildLedgerRange(long ledgerId, long startOfLedger,
                                                MessageSeqId endOfLedger) {
        LedgerRange.Builder builder =
            LedgerRange.newBuilder().setLedgerId(ledgerId).setStartSeqIdIncluded(startOfLedger)
                       .setEndSeqIdIncluded(endOfLedger);
        return builder.build();
    }

    class RangeScanOp extends TopicOpQueuer.SynchronousOp {
        RangeScanRequest request;
        int numMessagesRead = 0;
        long totalSizeRead = 0;
        TopicInfo topicInfo;
        long startSeqIdToScan;

        public RangeScanOp(RangeScanRequest request) {
            this(request, -1L, 0, 0L);
        }

        public RangeScanOp(RangeScanRequest request, long startSeqId, int numMessagesRead, long totalSizeRead) {
            queuer.super(request.topic);
            this.request = request;
            this.startSeqIdToScan = startSeqId;
            this.numMessagesRead = numMessagesRead;
            this.totalSizeRead = totalSizeRead;
        }

        @Override
        protected void runInternal() {
            topicInfo = topicInfos.get(topic);

            if (topicInfo == null) {
                request.callback.scanFailed(request.ctx, new PubSubException.ServerNotResponsibleForTopicException(""));
                return;
            }

            // if startSeqIdToScan is less than zero, which means it is an unfinished scan request
            // we continue the scan from the provided position
            startReadingFrom(startSeqIdToScan < 0 ? request.startSeqId : startSeqIdToScan);
        }

        protected void read(final InMemoryLedgerRange imlr, final long startSeqId, final long endSeqId) {
            // Verify whether startSeqId falls in ledger range.
            // Only the left endpoint of range needs to be checked.
            if (imlr.getStartSeqIdIncluded() > startSeqId) {
                logger.error(
                        "Invalid RangeScan read, startSeqId {} doesn't fall in ledger range [{} ~ {}]",
                        va(startSeqId, imlr.getStartSeqIdIncluded(), imlr.range.hasEndSeqIdIncluded() ? imlr.range
                                .getEndSeqIdIncluded().getLocalComponent() : ""));
                request.callback.scanFailed(request.ctx, new PubSubException.UnexpectedConditionException("Scan request is out of range"));

                // try release topic to reset the state
                lostTopic(topic);
                return;
            }

            if (imlr.handle == null) {

                bk.asyncOpenLedger(imlr.range.getLedgerId(), DigestType.CRC32, passwd,
                new SafeAsynBKCallback.OpenCallback() {
                    @Override
                    public void safeOpenComplete(int rc, LedgerHandle ledgerHandle, Object ctx) {
                        if (rc == BKException.Code.OK) {
                            imlr.handle = ledgerHandle;
                            read(imlr, startSeqId, endSeqId);
                            return;
                        }
                        BKException bke = BKException.create(rc);
                        logger.error("Could not open ledger: " + imlr.range.getLedgerId() + " for topic: "
                                     + topic);
                        request.callback.scanFailed(ctx, new PubSubException.ServiceDownException(bke));
                        return;
                    }
                }, request.ctx);
                return;
            }

            // ledger handle is not null, we can read from it
            long correctedEndSeqId = Math.min(startSeqId + request.messageLimit - numMessagesRead - 1, endSeqId);

            if (logger.isDebugEnabled()) {
                logger.debug("Issuing a bk read for ledger: " + imlr.handle.getId() + " from entry-id: "
                             + (startSeqId - imlr.getStartSeqIdIncluded()) + " to entry-id: "
                             + (correctedEndSeqId - imlr.getStartSeqIdIncluded()));
            }

            imlr.handle.asyncReadEntries(startSeqId - imlr.getStartSeqIdIncluded(), correctedEndSeqId
            - imlr.getStartSeqIdIncluded(), new SafeAsynBKCallback.ReadCallback() {

                long expectedEntryId = startSeqId - imlr.getStartSeqIdIncluded();

                @Override
                public void safeReadComplete(int rc, LedgerHandle lh, Enumeration<LedgerEntry> seq, Object ctx) {
                    if (rc != BKException.Code.OK || !seq.hasMoreElements()) {
                        if (rc == BKException.Code.OK) {
                            // means that there is no entries read, provide a meaningful exception
                            rc = BKException.Code.NoSuchEntryException;
                        }
                        BKException bke = BKException.create(rc);
                        logger.error("Error while reading from ledger: " + imlr.range.getLedgerId() + " for topic: "
                                     + topic.toStringUtf8(), bke);
                        request.callback.scanFailed(request.ctx, new PubSubException.ServiceDownException(bke));
                        return;
                    }

                    LedgerEntry entry = null;
                    while (seq.hasMoreElements()) {
                        entry = seq.nextElement();
                        Message message;
                        try {
                            message = Message.parseFrom(entry.getEntryInputStream());
                        } catch (IOException e) {
                            String msg = "Unreadable message found in ledger: " + imlr.range.getLedgerId()
                                         + " for topic: " + topic.toStringUtf8();
                            logger.error(msg, e);
                            request.callback.scanFailed(ctx, new PubSubException.UnexpectedConditionException(msg));
                            return;
                        }

                        logger.debug("Read response from ledger: {} entry-id: {}",
                                     lh.getId(), entry.getEntryId());

                        assert expectedEntryId == entry.getEntryId() : "expectedEntryId (" + expectedEntryId
                        + ") != entry.getEntryId() (" + entry.getEntryId() + ")";
                        assert (message.getMsgId().getLocalComponent() - imlr.getStartSeqIdIncluded()) == expectedEntryId;

                        expectedEntryId++;
                        request.callback.messageScanned(ctx, message);
                        numMessagesRead++;
                        totalSizeRead += message.getBody().size();

                        if (numMessagesRead >= request.messageLimit) {
                            request.callback.scanFinished(ctx, ReasonForFinish.NUM_MESSAGES_LIMIT_EXCEEDED);
                            return;
                        }

                        if (totalSizeRead >= request.sizeLimit) {
                            request.callback.scanFinished(ctx, ReasonForFinish.SIZE_LIMIT_EXCEEDED);
                            return;
                        }
                    }

                    // continue scanning messages
                    scanMessages(request, imlr.getStartSeqIdIncluded() + entry.getEntryId() + 1, numMessagesRead, totalSizeRead);
                }
            }, request.ctx);
        }

        protected void startReadingFrom(long startSeqId) {

            Map.Entry<Long, InMemoryLedgerRange> entry = topicInfo.ledgerRanges.ceilingEntry(startSeqId);

            if (entry == null) {
                // None of the old ledgers have this seq-id, we must use the
                // current ledger
                long endSeqId = topicInfo.currentLedgerRange.getStartSeqIdIncluded()
                                + topicInfo.lastEntryIdAckedInCurrentLedger;

                if (endSeqId < startSeqId) {
                    request.callback.scanFinished(request.ctx, ReasonForFinish.NO_MORE_MESSAGES);
                    return;
                }

                read(topicInfo.currentLedgerRange, startSeqId, endSeqId);
            } else {
                read(entry.getValue(), startSeqId, entry.getValue().range.getEndSeqIdIncluded().getLocalComponent());
            }

        }

    }

    @Override
    public void scanMessages(RangeScanRequest request) {
        queuer.pushAndMaybeRun(request.topic, new RangeScanOp(request));
    }

    protected void scanMessages(RangeScanRequest request, long scanSeqId, int numMsgsRead, long totalSizeRead) {
        queuer.pushAndMaybeRun(request.topic, new RangeScanOp(request, scanSeqId, numMsgsRead, totalSizeRead));
    }

    public void deliveredUntil(ByteString topic, Long seqId) {
        // Nothing to do here. this is just a hint that we cannot use.
    }

    class UpdateLedgerOp extends TopicOpQueuer.AsynchronousOp<Void> {
        private Set<Long> ledgersDeleted;

        public UpdateLedgerOp(ByteString topic, final Callback<Void> cb, final Object ctx,
                              Set<Long> ledgersDeleted) {
            queuer.super(topic, cb, ctx);
            this.ledgersDeleted = ledgersDeleted;
        }

        @Override
        public void run() {
            final TopicInfo topicInfo = topicInfos.get(topic);
            if (topicInfo == null) {
                logger.error("Server is not responsible for topic!");
                cb.operationFailed(ctx, new PubSubException.ServerNotResponsibleForTopicException(""));
                return;
            }
            LedgerRanges.Builder builder = LedgerRanges.newBuilder();
            final Set<Long> keysToRemove = new HashSet<Long>();
            boolean foundUnconsumedLedger = false;
            for (Map.Entry<Long, InMemoryLedgerRange> e : topicInfo.ledgerRanges.entrySet()) {
                LedgerRange lr = e.getValue().range;
                long ledgerId = lr.getLedgerId();
                if (!foundUnconsumedLedger && ledgersDeleted.contains(ledgerId)) {
                    keysToRemove.add(e.getKey());
                    if (!lr.hasEndSeqIdIncluded()) {
                        String msg = "Should not remove unclosed ledger " + ledgerId + " for topic " + topic.toStringUtf8();
                        logger.error(msg);
                        cb.operationFailed(ctx, new PubSubException.UnexpectedConditionException(msg));
                        return;
                    }
                } else {
                    foundUnconsumedLedger = true;
                    builder.addRanges(lr);
                }
            }
            builder.addRanges(topicInfo.currentLedgerRange.range);

            if (!keysToRemove.isEmpty()) {
                final LedgerRanges newRanges = builder.build();
                tpManager.writeTopicPersistenceInfo(
                topic, newRanges, topicInfo.ledgerRangesVersion, new Callback<Version>() {
                    public void operationFinished(Object ctx, Version newVersion) {
                        // Finally, all done
                        for (Long k : keysToRemove) {
                            topicInfo.ledgerRanges.remove(k);
                        }
                        topicInfo.ledgerRangesVersion = newVersion;
                        cb.operationFinished(ctx, null);
                    }
                    public void operationFailed(Object ctx, PubSubException exception) {
                        cb.operationFailed(ctx, exception);
                    }
                }, ctx);
            } else {
                cb.operationFinished(ctx, null);
            }
        }
    }

    class ConsumeUntilOp extends TopicOpQueuer.SynchronousOp {
        private final long seqId;

        public ConsumeUntilOp(ByteString topic, long seqId) {
            queuer.super(topic);
            this.seqId = seqId;
        }

        @Override
        public void runInternal() {
            TopicInfo topicInfo = topicInfos.get(topic);
            if (topicInfo == null) {
                logger.error("Server is not responsible for topic!");
                return;
            }

            final LinkedList<Long> ledgersToDelete = new LinkedList<Long>();
            for (Long endSeqIdIncluded : topicInfo.ledgerRanges.keySet()) {
                if (endSeqIdIncluded <= seqId) {
                    // This ledger's message entries have all been consumed already
                    // so it is safe to delete it from BookKeeper.
                    long ledgerId = topicInfo.ledgerRanges.get(endSeqIdIncluded).range.getLedgerId();
                    ledgersToDelete.add(ledgerId);
                } else {
                    break;
                }
            }

            // no ledgers need to delete
            if (ledgersToDelete.isEmpty()) {
                return;
            }

            Set<Long> ledgersDeleted = new HashSet<Long>();
            deleteLedgersAndUpdateLedgersRange(topic, ledgersToDelete, ledgersDeleted);
        }
    }

    private void deleteLedgersAndUpdateLedgersRange(final ByteString topic,
                                                    final LinkedList<Long> ledgersToDelete,
                                                    final Set<Long> ledgersDeleted) {
        if (ledgersToDelete.isEmpty()) {
            Callback<Void> cb = new Callback<Void>() {
                public void operationFinished(Object ctx, Void result) {
                    // do nothing, op is async to stop other ops
                    // occurring on the topic during the update
                }
                public void operationFailed(Object ctx, PubSubException exception) {
                    logger.error("Failed to update ledger znode for topic {} deleting ledgers {} : {}",
                                 va(topic.toStringUtf8(), ledgersDeleted, exception.getMessage()));
                }
            };
            queuer.pushAndMaybeRun(topic, new UpdateLedgerOp(topic, cb, null, ledgersDeleted));
            return;
        }

        final Long ledger = ledgersToDelete.poll();
        if (null == ledger) {
            deleteLedgersAndUpdateLedgersRange(topic, ledgersToDelete, ledgersDeleted);
            return;
        }

        bk.asyncDeleteLedger(ledger, new DeleteCallback() {
            @Override
            public void deleteComplete(int rc, Object ctx) {
                if (BKException.Code.NoSuchLedgerExistsException == rc ||
                    BKException.Code.OK == rc) {
                    ledgersDeleted.add(ledger);
                    deleteLedgersAndUpdateLedgersRange(topic, ledgersToDelete, ledgersDeleted);
                    return;
                } else {
                    logger.warn("Exception while deleting consumed ledger {}, stop deleting other ledgers {} "
                                + "and update ledger ranges with deleted ledgers {} : {}",
                                va(ledger, ledgersToDelete, ledgersDeleted, BKException.create(rc)));
                    // We should not continue when failed to delete ledger
                    Callback<Void> cb = new Callback<Void>() {
                        public void operationFinished(Object ctx, Void result) {
                            // do nothing, op is async to stop other ops
                            // occurring on the topic during the update
                        }
                        public void operationFailed(Object ctx, PubSubException exception) {
                            logger.error("Failed to update ledger znode for topic {} deleting ledgers {} : {}",
                                         va(topic, ledgersDeleted, exception.getMessage()));
                        }
                    };
                    queuer.pushAndMaybeRun(topic, new UpdateLedgerOp(topic, cb, null, ledgersDeleted));
                    return;
                }
            }
        }, null);
    }

    public void consumedUntil(ByteString topic, Long seqId) {
        queuer.pushAndMaybeRun(topic, new ConsumeUntilOp(topic, Math.max(seqId, getMinSeqIdForTopic(topic))));
    }

    public void consumeToBound(ByteString topic) {
        TopicInfo topicInfo = topicInfos.get(topic);

        if (topicInfo == null || topicInfo.messageBound == topicInfo.UNLIMITED) {
            return;
        }
        queuer.pushAndMaybeRun(topic, new ConsumeUntilOp(topic, getMinSeqIdForTopic(topic)));
    }

    public long getMinSeqIdForTopic(ByteString topic) {
        TopicInfo topicInfo = topicInfos.get(topic);

        if (topicInfo == null || topicInfo.messageBound == topicInfo.UNLIMITED) {
            return Long.MIN_VALUE;
        } else {
            return (topicInfo.lastSeqIdPushed.getLocalComponent() - topicInfo.messageBound) + 1;
        }
    }

    public MessageSeqId getCurrentSeqIdForTopic(ByteString topic) throws ServerNotResponsibleForTopicException {
        TopicInfo topicInfo = topicInfos.get(topic);

        if (topicInfo == null) {
            throw new PubSubException.ServerNotResponsibleForTopicException("");
        }

        return topicInfo.lastSeqIdPushed;
    }

    public long getSeqIdAfterSkipping(ByteString topic, long seqId, int skipAmount) {
        return Math.max(seqId + skipAmount, getMinSeqIdForTopic(topic));
    }

    /**
     * Release topic on failure
     *
     * @param topic
     *          Topic Name
     * @param e
     *          Failure Exception
     * @param ctx
     *          Callback context
     */
    protected void releaseTopicIfRequested(final ByteString topic, Exception e, Object ctx) {
        TopicInfo topicInfo = topicInfos.get(topic);
        if (topicInfo == null) {
            logger.warn("No topic found when trying to release ownership of topic " + topic.toStringUtf8()
                      + " on failure.");
            return;
        }
        // do release owner ship of topic
        if (topicInfo.doRelease.compareAndSet(false, true)) {
            logger.info("Release topic " + topic.toStringUtf8() + " when bookkeeper persistence mananger encounters failure :",
                        e);
            tm.releaseTopic(topic, new Callback<Void>() {
                @Override
                public void operationFailed(Object ctx, PubSubException exception) {
                    logger.error("Exception found on releasing topic " + topic.toStringUtf8()
                               + " when encountering exception from bookkeeper:", exception);
                }
                @Override
                public void operationFinished(Object ctx, Void resultOfOperation) {
                    logger.info("successfully releasing topic {} when encountering"
                              + " exception from bookkeeper", topic.toStringUtf8());
                }
            }, null);
        }
        // if release happens when the topic is changing ledger
        // we need to fail all queued persist requests
        if (topicInfo.doChangeLedger.get()) {
            for (PersistRequest pr : topicInfo.deferredRequests) {
                pr.getCallback().operationFailed(ctx, new PubSubException.ServiceDownException(e));
            }
            topicInfo.deferredRequests.clear();
            topicInfo.lastSeqIdBeforeLedgerChange = -1;
        }
    }

    public class PersistOp extends TopicOpQueuer.SynchronousOp {
        PersistRequest request;

        public PersistOp(PersistRequest request) {
            queuer.super(request.topic);
            this.request = request;
        }

        @Override
        public void runInternal() {
            doPersistMessage(request);
        }
    }

    /**
     * Persist a message by executing a persist request.
     */
    protected void doPersistMessage(final PersistRequest request) {
        final ByteString topic = request.topic;
        final TopicInfo topicInfo = topicInfos.get(topic);

        if (topicInfo == null) {
            request.getCallback().operationFailed(request.ctx,
                                             new PubSubException.ServerNotResponsibleForTopicException(""));
            return;
        }

        if (topicInfo.doRelease.get()) {
            request.getCallback().operationFailed(request.ctx, new PubSubException.ServiceDownException(
                "The ownership of the topic is releasing due to unrecoverable issue."));
            return;
        }

        // if the topic is changing ledger, queue following persist requests until ledger is changed
        if (topicInfo.doChangeLedger.get()) {
            logger.info("Topic {} is changing ledger, so queue persist request for message.",
                        topic.toStringUtf8());
            topicInfo.deferredRequests.add(request);
            return;
        }

        final long localSeqId = topicInfo.lastSeqIdPushed.getLocalComponent() + 1;
        MessageSeqId.Builder builder = MessageSeqId.newBuilder();
        if (request.message.hasMsgId()) {
            MessageIdUtils.takeRegionMaximum(builder, topicInfo.lastSeqIdPushed, request.message.getMsgId());
        } else {
            builder.addAllRemoteComponents(topicInfo.lastSeqIdPushed.getRemoteComponentsList());
        }
        builder.setLocalComponent(localSeqId);

        // check whether reach the threshold of a ledger, if it does,
        // open a ledger to write
        long entriesInThisLedger = localSeqId - topicInfo.currentLedgerRange.getStartSeqIdIncluded() + 1;
        if (UNLIMITED_ENTRIES != maxEntriesPerLedger &&
            entriesInThisLedger >= maxEntriesPerLedger) {
            if (topicInfo.doChangeLedger.compareAndSet(false, true)) {
                // for order guarantees, we should wait until all the adding operations for current ledger
                // are succeed. so we just mark it as lastSeqIdBeforeLedgerChange
                // when the lastSeqIdBeforeLedgerChange acked, we do changing the ledger
                if (null == topicInfo.deferredRequests) {
                    topicInfo.deferredRequests = new LinkedList<PersistRequest>();
                }
                topicInfo.lastSeqIdBeforeLedgerChange = localSeqId;
            }
        }

        topicInfo.lastSeqIdPushed = builder.build();
        Message msgToSerialize = Message.newBuilder(request.message).setMsgId(topicInfo.lastSeqIdPushed).build();

        final MessageSeqId responseSeqId = msgToSerialize.getMsgId();
        topicInfo.currentLedgerRange.handle.asyncAddEntry(msgToSerialize.toByteArray(),
        new SafeAsynBKCallback.AddCallback() {
            AtomicBoolean processed = new AtomicBoolean(false);
            @Override
            public void safeAddComplete(int rc, LedgerHandle lh, long entryId, Object ctx) {

                // avoid double callback by mistake, since we may do change ledger in this callback.
                if (!processed.compareAndSet(false, true)) {
                    return;
                }
                if (rc != BKException.Code.OK) {
                    BKException bke = BKException.create(rc);
                    logger.error("Error while persisting entry to ledger: " + lh.getId() + " for topic: "
                                 + topic.toStringUtf8(), bke);
                    request.getCallback().operationFailed(ctx, new PubSubException.ServiceDownException(bke));

                    // To preserve ordering guarantees, we
                    // should give up the topic and not let
                    // other operations through
                    releaseTopicIfRequested(request.topic, bke, ctx);
                    return;
                }

                if (entryId + topicInfo.currentLedgerRange.getStartSeqIdIncluded() != localSeqId) {
                    String msg = "Expected BK to assign entry-id: "
                                 + (localSeqId - topicInfo.currentLedgerRange.getStartSeqIdIncluded())
                                 + " but it instead assigned entry-id: " + entryId + " topic: "
                                 + topic.toStringUtf8() + "ledger: " + lh.getId();
                    logger.error(msg);
                    throw new UnexpectedError(msg);
                }

                topicInfo.lastEntryIdAckedInCurrentLedger = entryId;
                request.getCallback().operationFinished(ctx, responseSeqId);
                // if this acked entry is the last entry of current ledger
                // we can add a ChangeLedgerOp to execute to change ledger
                if (topicInfo.doChangeLedger.get() &&
                    entryId + topicInfo.currentLedgerRange.getStartSeqIdIncluded() == topicInfo.lastSeqIdBeforeLedgerChange) {
                    // change ledger
                    changeLedger(topic, new Callback<Void>() {
                        @Override
                        public void operationFailed(Object ctx, PubSubException exception) {
                            logger.error("Failed to change ledger for topic " + topic.toStringUtf8(), exception);
                            // change ledger failed, we should give up topic
                            releaseTopicIfRequested(request.topic, exception, ctx);
                        }
                        @Override
                        public void operationFinished(Object ctx, Void resultOfOperation) {
                            topicInfo.doChangeLedger.set(false);
                            topicInfo.lastSeqIdBeforeLedgerChange = -1;
                            // the ledger is changed, persist queued requests
                            // if the number of queued persist requests is more than maxEntriesPerLedger
                            // we just persist maxEntriesPerLedger requests, other requests are still queued
                            // until next ledger changed.
                            int numRequests = 0;
                            while (!topicInfo.deferredRequests.isEmpty() &&
                                   numRequests < maxEntriesPerLedger) {
                                PersistRequest pr = topicInfo.deferredRequests.removeFirst();
                                doPersistMessage(pr);
                                ++numRequests;
                            }
                            logger.debug("Finished persisting {} queued requests, but there are still {} requests in queue.",
                                         numRequests, topicInfo.deferredRequests.size());
                        }
                    }, ctx);
                }
            }
        }, request.ctx);
    }

    public void persistMessage(PersistRequest request) {
        queuer.pushAndMaybeRun(request.topic, new PersistOp(request));
    }

    public void scanSingleMessage(ScanRequest request) {
        throw new RuntimeException("Not implemented");
    }

    static SafeAsynBKCallback.CloseCallback noOpCloseCallback = new SafeAsynBKCallback.CloseCallback() {
        @Override
        public void safeCloseComplete(int rc, LedgerHandle ledgerHandle, Object ctx) {
        };
    };

    class AcquireOp extends TopicOpQueuer.AsynchronousOp<Void> {
        public AcquireOp(ByteString topic, Callback<Void> cb, Object ctx) {
            queuer.super(topic, cb, ctx);
        }

        @Override
        public void run() {
            if (topicInfos.containsKey(topic)) {
                // Already acquired, do nothing
                cb.operationFinished(ctx, null);
                return;
            }

            // read persistence info
            tpManager.readTopicPersistenceInfo(topic, new Callback<Versioned<LedgerRanges>>() {
                @Override
                public void operationFinished(Object ctx, Versioned<LedgerRanges> ranges) {
                    if (null != ranges) {
                        processTopicLedgerRanges(ranges.getValue(), ranges.getVersion());
                    } else {
                        processTopicLedgerRanges(LedgerRanges.getDefaultInstance(), Version.NEW);
                    }
                }
                @Override
                public void operationFailed(Object ctx, PubSubException exception) {
                    cb.operationFailed(ctx, exception);
                }
            }, ctx);
        }

        void processTopicLedgerRanges(final LedgerRanges ranges, final Version version) {
            final List<LedgerRange> rangesList = ranges.getRangesList();
            if (!rangesList.isEmpty()) {
                LedgerRange range = rangesList.get(0);
                if (range.hasStartSeqIdIncluded()) {
                    // we already have start seq id
                    processTopicLedgerRanges(rangesList, version, range.getStartSeqIdIncluded());
                    return;
                }
                getStartSeqIdToProcessTopicLedgerRanges(rangesList, version);
                return;
            }
            // process topic ledger ranges directly
            processTopicLedgerRanges(rangesList, version, START_SEQ_ID);
        }

        /**
         * Process old version ledger ranges to fetch start seq id.
         */
        void getStartSeqIdToProcessTopicLedgerRanges(
            final List<LedgerRange> rangesList, final Version version) {

            final LedgerRange range = rangesList.get(0);

            if (!range.hasEndSeqIdIncluded()) {
                // process topic ledger ranges directly
                processTopicLedgerRanges(rangesList, version, START_SEQ_ID);
                return;
            }

            final long ledgerId = range.getLedgerId();
            // open the first ledger to compute right start seq id
            bk.asyncOpenLedger(ledgerId, DigestType.CRC32, passwd,
            new SafeAsynBKCallback.OpenCallback() {

                @Override
                public void safeOpenComplete(int rc, LedgerHandle ledgerHandle, Object ctx) {

                    if (rc == BKException.Code.NoSuchLedgerExistsException) {
                        // process next ledger 
                        processTopicLedgerRanges(rangesList, version, START_SEQ_ID);
                        return;
                    } else if (rc != BKException.Code.OK) {
                        BKException bke = BKException.create(rc);
                        logger.error("Could not open ledger {} to get start seq id while acquiring topic {} : {}",
                                     va(ledgerId, topic.toStringUtf8(), bke));
                        cb.operationFailed(ctx, new PubSubException.ServiceDownException(bke));
                        return;
                    }

                    final long numEntriesInLastLedger = ledgerHandle.getLastAddConfirmed() + 1;

                    // the ledger is closed before, calling close is just a nop operation.
                    try {
                        ledgerHandle.close();
                    } catch (InterruptedException ie) {
                        // the exception would never be thrown for a read only ledger handle.
                    } catch (BKException bke) {
                        // the exception would never be thrown for a read only ledger handle.
                    }

                    if (numEntriesInLastLedger <= 0) {
                        String msg = "No entries found in a have-end-seq-id ledger " + ledgerId
                                     + " when acquiring topic " + topic.toStringUtf8() + ".";
                        logger.error(msg);
                        cb.operationFailed(ctx, new PubSubException.UnexpectedConditionException(msg));
                        return;
                    }
                    long endOfLedger = range.getEndSeqIdIncluded().getLocalComponent();
                    long startOfLedger = endOfLedger - numEntriesInLastLedger + 1;

                    processTopicLedgerRanges(rangesList, version, startOfLedger);
                }

            }, ctx);
        }

        void processTopicLedgerRanges(final List<LedgerRange> rangesList, final Version version,
                                      long startOfLedger) {
            logger.info("Process {} ledgers for topic {} starting from seq id {}.",
                        va(rangesList.size(), topic.toStringUtf8(), startOfLedger));

            Iterator<LedgerRange> lrIterator = rangesList.iterator();

            TopicInfo topicInfo = new TopicInfo();
            while (lrIterator.hasNext()) {
                LedgerRange range = lrIterator.next();

                if (range.hasEndSeqIdIncluded()) {
                    // this means it was a valid and completely closed ledger
                    long endOfLedger = range.getEndSeqIdIncluded().getLocalComponent();
                    if (range.hasStartSeqIdIncluded()) {
                        startOfLedger = range.getStartSeqIdIncluded();
                    } else {
                        range = buildLedgerRange(range.getLedgerId(), startOfLedger,
                                                 range.getEndSeqIdIncluded());
                    }
                    topicInfo.ledgerRanges.put(endOfLedger, new InMemoryLedgerRange(range));
                    if (startOfLedger < endOfLedger + 1) {
                        startOfLedger = endOfLedger + 1;
                    }
                    continue;
                }

                // If it doesn't have a valid end, it must be the last ledger
                if (lrIterator.hasNext()) {
                    String msg = "Ledger-id: " + range.getLedgerId() + " for topic: " + topic.toStringUtf8()
                                 + " is not the last one but still does not have an end seq-id";
                    logger.error(msg);
                    cb.operationFailed(ctx, new PubSubException.UnexpectedConditionException(msg));
                    return;
                }

                if (range.hasStartSeqIdIncluded()) {
                    startOfLedger = range.getStartSeqIdIncluded();
                }

                // The last ledger does not have a valid seq-id, lets try to
                // find it out
                recoverLastTopicLedgerAndOpenNewOne(range.getLedgerId(), startOfLedger,
                                                    version, topicInfo);
                return;
            }

            // All ledgers were found properly closed, just start a new one
            openNewTopicLedger(topic, version, topicInfo, startOfLedger, false, cb, ctx);
        }

        /**
         * Recovers the last ledger, opens a new one, and persists the new
         * information to ZK
         *
         * @param ledgerId
         *            Ledger to be recovered
         * @param expectedStartSeqId 
         *            Start seq id of the ledger to recover
         * @param expectedVersionOfLedgerNode
         *            Expected version to update ledgers range
         * @param topicInfo
         *            Topic info
         */
        private void recoverLastTopicLedgerAndOpenNewOne(final long ledgerId, final long expectedStartSeqId,
                final Version expectedVersionOfLedgerNode, final TopicInfo topicInfo) {

            bk.asyncOpenLedger(ledgerId, DigestType.CRC32, passwd, new SafeAsynBKCallback.OpenCallback() {
                @Override
                public void safeOpenComplete(int rc, LedgerHandle ledgerHandle, Object ctx) {

                    if (rc != BKException.Code.OK) {
                        BKException bke = BKException.create(rc);
                        logger.error("While acquiring topic: " + topic.toStringUtf8()
                                     + ", could not open unrecovered ledger: " + ledgerId, bke);
                        cb.operationFailed(ctx, new PubSubException.ServiceDownException(bke));
                        return;
                    }

                    final long numEntriesInLastLedger = ledgerHandle.getLastAddConfirmed() + 1;

                    if (numEntriesInLastLedger <= 0) {
                        // this was an empty ledger that someone created but
                        // couldn't write to, so just ignore it
                        logger.info("Pruning empty ledger: " + ledgerId + " for topic: " + topic.toStringUtf8());
                        closeLedger(ledgerHandle);
                        openNewTopicLedger(topic, expectedVersionOfLedgerNode, topicInfo,
                                           expectedStartSeqId, false, cb, ctx);
                        return;
                    }

                    // we have to read the last entry of the ledger to find
                    // out the last seq-id

                    ledgerHandle.asyncReadEntries(numEntriesInLastLedger - 1, numEntriesInLastLedger - 1,
                    new SafeAsynBKCallback.ReadCallback() {
                        @Override
                        public void safeReadComplete(int rc, LedgerHandle lh, Enumeration<LedgerEntry> seq,
                        Object ctx) {
                            if (rc != BKException.Code.OK || !seq.hasMoreElements()) {
                                if (rc == BKException.Code.OK) {
                                    // means that there is no entries read, provide a meaningful exception
                                    rc = BKException.Code.NoSuchEntryException;
                                }
                                logger.info("Received error code {}", rc);
                                BKException bke = BKException.create(rc);
                                logger.error("While recovering ledger: " + ledgerId + " for topic: "
                                             + topic.toStringUtf8() + ", could not read last entry", bke);
                                cb.operationFailed(ctx, new PubSubException.ServiceDownException(bke));
                                return;
                            }

                            Message lastMessage;
                            try {
                                lastMessage = Message.parseFrom(seq.nextElement().getEntry());
                            } catch (InvalidProtocolBufferException e) {
                                String msg = "While recovering ledger: " + ledgerId + " for topic: "
                                             + topic.toStringUtf8() + ", could not deserialize last message";
                                logger.error(msg, e);
                                cb.operationFailed(ctx, new PubSubException.UnexpectedConditionException(msg));
                                return;
                            }

                            long endOfLedger  = lastMessage.getMsgId().getLocalComponent();
                            long startOfLedger = endOfLedger - numEntriesInLastLedger + 1;

                            if (startOfLedger != expectedStartSeqId) {
                                // gap would be introduced by old version when gc consumed ledgers
                                String msg = "Expected start seq id of recovered ledger " + ledgerId
                                             + " to be " + expectedStartSeqId + " but it was "
                                             + startOfLedger + ".";
                                logger.warn(msg);
                            }

                            LedgerRange lr = buildLedgerRange(ledgerId, startOfLedger, lastMessage.getMsgId());
                            topicInfo.ledgerRanges.put(endOfLedger,
                                    new InMemoryLedgerRange(lr, lh));

                            logger.info("Recovered unclosed ledger: {} for topic: {} with {} entries starting from seq id {}",
                                        va(ledgerId, topic.toStringUtf8(), numEntriesInLastLedger, startOfLedger));

                            openNewTopicLedger(topic, expectedVersionOfLedgerNode, topicInfo, endOfLedger + 1, false, cb, ctx);
                        }
                    }, ctx);

                }

            }, ctx);
        }
    }

    /**
     * Open New Ledger to write for a topic.
     *
     * @param topic
     *          Topic Name
     * @param expectedVersionOfLedgersNode
     *          Expected Version to Update Ledgers Node.
     * @param topicInfo
     *          Topic Information
     * @param startSeqId
     *          Start of sequence id for new ledger
     * @param changeLedger
     *          Whether is it called when changing ledger
     * @param cb
     *          Callback to trigger after opening new ledger.
     * @param ctx
     *          Callback context.
     */
    void openNewTopicLedger(final ByteString topic,
                            final Version expectedVersionOfLedgersNode, final TopicInfo topicInfo,
                            final long startSeqId, final boolean changeLedger,
                            final Callback<Void> cb, final Object ctx) {
        bk.asyncCreateLedger(cfg.getBkEnsembleSize(), cfg.getBkWriteQuorumSize(),
                             cfg.getBkAckQuorumSize(), DigestType.CRC32, passwd,
        new SafeAsynBKCallback.CreateCallback() {
            AtomicBoolean processed = new AtomicBoolean(false);

            @Override
            public void safeCreateComplete(int rc, LedgerHandle lh, Object ctx) {
                if (!processed.compareAndSet(false, true)) {
                    return;
                }

                if (rc != BKException.Code.OK) {
                    BKException bke = BKException.create(rc);
                    logger.error("Could not create new ledger while acquiring topic: "
                                 + topic.toStringUtf8(), bke);
                    cb.operationFailed(ctx, new PubSubException.ServiceDownException(bke));
                    return;
                }

                // compute last seq id
                if (!changeLedger) {
                    topicInfo.lastSeqIdPushed = topicInfo.ledgerRanges.isEmpty() ? MessageSeqId.newBuilder()
                                                .setLocalComponent(startSeqId - 1).build() : topicInfo.ledgerRanges.lastEntry().getValue().range
                                                .getEndSeqIdIncluded();
                }

                LedgerRange lastRange = LedgerRange.newBuilder().setLedgerId(lh.getId())
                                        .setStartSeqIdIncluded(startSeqId).build();
                topicInfo.currentLedgerRange = new InMemoryLedgerRange(lastRange, lh);
                topicInfo.lastEntryIdAckedInCurrentLedger = -1;

                // Persist the fact that we started this new
                // ledger to ZK

                LedgerRanges.Builder builder = LedgerRanges.newBuilder();
                for (InMemoryLedgerRange imlr : topicInfo.ledgerRanges.values()) {
                    builder.addRanges(imlr.range);
                }
                builder.addRanges(lastRange);

                tpManager.writeTopicPersistenceInfo(
                topic, builder.build(), expectedVersionOfLedgersNode, new Callback<Version>() {
                    @Override
                    public void operationFinished(Object ctx, Version newVersion) {
                        // Finally, all done
                        topicInfo.ledgerRangesVersion = newVersion;
                        topicInfos.put(topic, topicInfo);
                        cb.operationFinished(ctx, null);
                    }
                    @Override
                    public void operationFailed(Object ctx, PubSubException exception) {
                        cb.operationFailed(ctx, exception);
                    }
                }, ctx);
                return;
            }
        }, ctx);
    }

    /**
     * acquire ownership of a topic, doing whatever is needed to be able to
     * perform reads and writes on that topic from here on
     *
     * @param topic
     * @param callback
     * @param ctx
     */
    @Override
    public void acquiredTopic(ByteString topic, Callback<Void> callback, Object ctx) {
        queuer.pushAndMaybeRun(topic, new AcquireOp(topic, callback, ctx));
    }

    /**
     * Change ledger to write for a topic.
     */
    class ChangeLedgerOp extends TopicOpQueuer.AsynchronousOp<Void> {

        public ChangeLedgerOp(ByteString topic, Callback<Void> cb, Object ctx) {
            queuer.super(topic, cb, ctx);
        }

        @Override
        public void run() {
            TopicInfo topicInfo = topicInfos.get(topic);
            if (null == topicInfo) {
                logger.error("Weired! hub server doesn't own topic " + topic.toStringUtf8()
                           + " when changing ledger to write.");
                cb.operationFailed(ctx, new PubSubException.ServerNotResponsibleForTopicException(""));
                return;
            }
            closeLastTopicLedgerAndOpenNewOne(topicInfo);
        }

        private void closeLastTopicLedgerAndOpenNewOne(final TopicInfo topicInfo) {
            final long ledgerId = topicInfo.currentLedgerRange.handle.getId();
            topicInfo.currentLedgerRange.handle.asyncClose(new CloseCallback() {
                AtomicBoolean processed = new AtomicBoolean(false);
                @Override
                public void closeComplete(int rc, LedgerHandle lh, Object ctx) {
                    if (!processed.compareAndSet(false, true)) {
                        return;
                    }
                    if (BKException.Code.OK != rc) {
                        BKException bke = BKException.create(rc);
                        logger.error("Could not close ledger " + ledgerId
                                   + " while changing ledger of topic " + topic.toStringUtf8(), bke);
                        cb.operationFailed(ctx, new PubSubException.ServiceDownException(bke));
                        return;
                    }
                    long endSeqId = topicInfo.lastSeqIdPushed.getLocalComponent();
                    // update last range
                    LedgerRange lastRange =
                        buildLedgerRange(ledgerId, topicInfo.currentLedgerRange.getStartSeqIdIncluded(),
                                         topicInfo.lastSeqIdPushed);

                    topicInfo.currentLedgerRange.range = lastRange;
                    // put current ledger to ledger ranges
                    topicInfo.ledgerRanges.put(endSeqId, topicInfo.currentLedgerRange);
                    logger.info("Closed written ledger " + ledgerId + " for topic "
                              + topic.toStringUtf8() + " to change ledger.");
                    openNewTopicLedger(topic, topicInfo.ledgerRangesVersion,
                                       topicInfo, endSeqId + 1, true, cb, ctx);
                }
            }, ctx);
        }

    }

    /**
     * Change ledger to write for a topic.
     *
     * @param topic
     *          Topic Name
     */
    protected void changeLedger(ByteString topic, Callback<Void> cb, Object ctx) {
        queuer.pushAndMaybeRun(topic, new ChangeLedgerOp(topic, cb, ctx));
    }

    public void closeLedger(LedgerHandle lh) {
        // try {
        // lh.asyncClose(noOpCloseCallback, null);
        // } catch (InterruptedException e) {
        // logger.error(e);
        // Thread.currentThread().interrupt();
        // }
    }

    class ReleaseOp extends TopicOpQueuer.SynchronousOp {

        public ReleaseOp(ByteString topic) {
            queuer.super(topic);
        }

        @Override
        public void runInternal() {
            TopicInfo topicInfo = topicInfos.remove(topic);

            if (topicInfo == null) {
                return;
            }

            for (InMemoryLedgerRange imlr : topicInfo.ledgerRanges.values()) {
                if (imlr.handle != null) {
                    closeLedger(imlr.handle);
                }
            }

            if (topicInfo.currentLedgerRange != null && topicInfo.currentLedgerRange.handle != null) {
                closeLedger(topicInfo.currentLedgerRange.handle);
            }
        }
    }

    /**
     * Release any resources for the topic that might be currently held. There
     * wont be any subsequent reads or writes on that topic coming
     *
     * @param topic
     */
    @Override
    public void lostTopic(ByteString topic) {
        queuer.pushAndMaybeRun(topic, new ReleaseOp(topic));
    }

    class SetMessageBoundOp extends TopicOpQueuer.SynchronousOp {
        final int bound;

        public SetMessageBoundOp(ByteString topic, int bound) {
            queuer.super(topic);
            this.bound = bound;
        }

        @Override
        public void runInternal() {
            TopicInfo topicInfo = topicInfos.get(topic);
            if (topicInfo != null) {
                topicInfo.messageBound = bound;
            }
        }
    }

    public void setMessageBound(ByteString topic, Integer bound) {
        queuer.pushAndMaybeRun(topic, new SetMessageBoundOp(topic, bound));
    }

    public void clearMessageBound(ByteString topic) {
        queuer.pushAndMaybeRun(topic, new SetMessageBoundOp(topic, TopicInfo.UNLIMITED));
    }

    @Override
    public void stop() {
        try {
            tpManager.close();
        } catch (IOException ioe) {
            logger.warn("Exception closing topic persistence manager : ", ioe);
        }
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/persistence/CacheKey.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.persistence;

import com.google.protobuf.ByteString;
import org.apache.hedwig.server.common.ByteStringInterner;

public class CacheKey {

    ByteString topic;
    long seqId;

    public CacheKey(ByteString topic, long seqId) {
        this.topic = ByteStringInterner.intern(topic);
        this.seqId = seqId;
    }

    public ByteString getTopic() {
        return topic;
    }

    public long getSeqId() {
        return seqId;
    }

    @Override
    public int hashCode() {
        final int prime = 31;
        int result = 1;
        result = prime * result + (int) (seqId ^ (seqId >>> 32));
        result = prime * result + ((topic == null) ? 0 : topic.hashCode());
        return result;
    }

    @Override
    public boolean equals(Object obj) {
        if (this == obj)
            return true;
        if (obj == null)
            return false;
        if (getClass() != obj.getClass())
            return false;
        CacheKey other = (CacheKey) obj;
        if (seqId != other.seqId)
            return false;
        if (topic == null) {
            if (other.topic != null)
                return false;
        } else if (!topic.equals(other.topic))
            return false;
        return true;
    }

    @Override
    public String toString() {
        return "(" + topic.toStringUtf8() + "," + seqId + ")";
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/persistence/CacheValue.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.persistence;

import java.util.HashSet;
import java.util.Set;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.server.common.UnexpectedError;

/**
 * This class is NOT thread safe. It need not be thread-safe because our
 * read-ahead cache will operate with only 1 thread
 *
 */
public class CacheValue {

    static Logger logger = LoggerFactory.getLogger(ReadAheadCache.class);

    // Actually we don't care the order of callbacks
    // when a scan callback, it should be delivered to both callbacks
    Set<ScanCallbackWithContext> callbacks = new HashSet<ScanCallbackWithContext>();
    Message message;
    long timeOfAddition = 0;

    public CacheValue() {
    }

    public boolean isStub() {
        return message == null;
    }

    public long getTimeOfAddition() {
        if (message == null) {
            throw new UnexpectedError("Time of add requested from a stub");
        }
        return timeOfAddition;
    }

    public void setMessageAndInvokeCallbacks(Message message, long currTime) {
        if (this.message != null) {
            // Duplicate read for the same message coming back
            return;
        }

        this.message = message;
        this.timeOfAddition = currTime;

        logger.debug("Invoking {} callbacks for {} message added to cache", callbacks.size(), message);
        for (ScanCallbackWithContext callbackWithCtx : callbacks) {
            if (null != callbackWithCtx) {
                callbackWithCtx.getScanCallback().messageScanned(callbackWithCtx.getCtx(), message);
            }
        }
    }

    public boolean removeCallback(ScanCallback callback, Object ctx) {
        return callbacks.remove(new ScanCallbackWithContext(callback, ctx));
    }

    public void addCallback(ScanCallback callback, Object ctx) {
        if (!isStub()) {
            // call the callback right away
            callback.messageScanned(ctx, message);
            return;
        }

        callbacks.add(new ScanCallbackWithContext(callback, ctx));
    }

    public Message getMessage() {
        return message;
    }

    public void setErrorAndInvokeCallbacks(Exception exception) {
        for (ScanCallbackWithContext callbackWithCtx : callbacks) {
            if (null != callbackWithCtx) {
                callbackWithCtx.getScanCallback().scanFailed(callbackWithCtx.getCtx(), exception);
            }
        }
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/persistence/CancelScanRequest.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.persistence;

public interface CancelScanRequest {

    /**
     * @return the scan request to cancel
     */
    public ScanRequest getScanRequest();

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/persistence/Factory.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.persistence;

public interface Factory<T> {
    public T newInstance();
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/persistence/LocalDBPersistenceManager.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.persistence;

import java.io.File;
import java.io.IOException;
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.sql.Statement;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ConcurrentMap;

import java.math.BigInteger;
import java.security.MessageDigest;
import java.security.NoSuchAlgorithmException;

import javax.sql.rowset.serial.SerialBlob;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.protobuf.ByteString;
import org.apache.hedwig.exceptions.PubSubException.ServiceDownException;
import org.apache.hedwig.exceptions.PubSubException.UnexpectedConditionException;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protoextensions.MessageIdUtils;
import org.apache.hedwig.server.persistence.ScanCallback.ReasonForFinish;
import org.apache.hedwig.util.Callback;
import org.apache.hedwig.util.FileUtils;

public class LocalDBPersistenceManager implements PersistenceManagerWithRangeScan {
    static Logger logger = LoggerFactory.getLogger(LocalDBPersistenceManager.class);

    static String connectionURL;

    static {
        try {
            File tempDir = FileUtils.createTempDirectory("derby", null);

            // Since derby needs to create it, I will have to delete it first
            if (!tempDir.delete()) {
                throw new IOException("Could not delete dir: " + tempDir.getAbsolutePath());
            }
            connectionURL = "jdbc:derby:" + tempDir.getAbsolutePath() + ";create=true";
        } catch (IOException e) {
            throw new RuntimeException(e);
        }

    }

    private static final ThreadLocal<Connection> threadLocalConnection = new ThreadLocal<Connection>() {
        @Override
        protected Connection initialValue() {
            try {
                return DriverManager.getConnection(connectionURL);
            } catch (SQLException e) {
                logger.error("Could not connect to derby", e);
                return null;
            }
        }
    };

    private static final ThreadLocal<MessageDigest> threadLocalDigest = new ThreadLocal<MessageDigest>() {
        @Override
        protected MessageDigest initialValue() {
            try {
                return MessageDigest.getInstance("MD5");
            } catch (NoSuchAlgorithmException e) {
                logger.error("Could not find MD5 hash", e);
                return null;
            }
        }
    };
    static final String ID_FIELD_NAME = "id";
    static final String MSG_FIELD_NAME = "msg";
    static final String driver = "org.apache.derby.jdbc.EmbeddedDriver";

    static final int SCAN_CHUNK = 1000;

    /**
     * Having trouble restarting the database multiple times from within the
     * same jvm. Hence to facilitate units tests, we are just going to have a
     * version number that we will append to every table name. This version
     * number will be incremented in lieu of shutting down the database and
     * restarting it, so that we get different table names, and it behaves like
     * a brand new database
     */
    private int version = 0;

    ConcurrentMap<ByteString, MessageSeqId> currTopicSeqIds = new ConcurrentHashMap<ByteString, MessageSeqId>();

    static LocalDBPersistenceManager instance = new LocalDBPersistenceManager();

    public static LocalDBPersistenceManager instance() {
        return instance;
    }

    private LocalDBPersistenceManager() {

        try {
            Class.forName(driver).newInstance();
            logger.info("Derby Driver loaded");
        } catch (java.lang.ClassNotFoundException e) {
            logger.error("Derby driver not found", e);
        } catch (InstantiationException e) {
            logger.error("Could not instantiate derby driver", e);
        } catch (IllegalAccessException e) {
            logger.error("Could not instantiate derby driver", e);
        }
    }

    @Override
    public void stop() {
        // do nothing
    }

    /**
     * Ensures that at least the default seq-id exists in the map for the given
     * topic. Checks for race conditions (.e.g, another thread inserts the
     * default id before us), and returns the latest seq-id value in the map
     *
     * @param topic
     * @return
     */
    private MessageSeqId ensureSeqIdExistsForTopic(ByteString topic) {
        MessageSeqId presentSeqIdInMap = currTopicSeqIds.get(topic);

        if (presentSeqIdInMap != null) {
            return presentSeqIdInMap;
        }

        presentSeqIdInMap = MessageSeqId.newBuilder().setLocalComponent(0).build();
        MessageSeqId oldSeqIdInMap = currTopicSeqIds.putIfAbsent(topic, presentSeqIdInMap);

        if (oldSeqIdInMap != null) {
            return oldSeqIdInMap;
        }
        return presentSeqIdInMap;

    }

    /**
     * Adjust the current seq id of the topic based on the message we are about
     * to publish. The local component of the current seq-id is always
     * incremented by 1. For the other components, there are two cases:
     *
     * 1. If the message to be published doesn't have a seq-id (locally
     * published messages), the other components are left as is.
     *
     * 2. If the message to be published has a seq-id, we take the max of the
     * current one we have, and that in the message to be published.
     *
     * @param topic
     * @param messageToPublish
     * @return The value of the local seq-id obtained after incrementing the
     *         local component. This value should be used as an id while
     *         persisting to Derby
     * @throws UnexpectedConditionException
     */
    private long adjustTopicSeqIdForPublish(ByteString topic, Message messageToPublish)
            throws UnexpectedConditionException {
        long retValue = 0;
        MessageSeqId oldId;
        MessageSeqId.Builder newIdBuilder = MessageSeqId.newBuilder();

        do {
            oldId = ensureSeqIdExistsForTopic(topic);

            // Increment our own component by 1
            retValue = oldId.getLocalComponent() + 1;
            newIdBuilder.setLocalComponent(retValue);

            if (messageToPublish.hasMsgId()) {
                // take a region-wise max
                MessageIdUtils.takeRegionMaximum(newIdBuilder, messageToPublish.getMsgId(), oldId);

            } else {
                newIdBuilder.addAllRemoteComponents(oldId.getRemoteComponentsList());
            }
        } while (!currTopicSeqIds.replace(topic, oldId, newIdBuilder.build()));

        return retValue;

    }

    public long getSeqIdAfterSkipping(ByteString topic, long seqId, int skipAmount) {
        return seqId + skipAmount;
    }

    public void persistMessage(PersistRequest request) {

        Connection conn = threadLocalConnection.get();

        Callback<MessageSeqId> callback = request.getCallback();
        Object ctx = request.getCtx();
        ByteString topic = request.getTopic();
        Message message = request.getMessage();

        if (conn == null) {
            callback.operationFailed(ctx, new ServiceDownException("Not connected to derby"));
            return;
        }

        long seqId;

        try {
            seqId = adjustTopicSeqIdForPublish(topic, message);
        } catch (UnexpectedConditionException e) {
            callback.operationFailed(ctx, e);
            return;
        }
        PreparedStatement stmt;

        boolean triedCreatingTable = false;
        while (true) {
            try {
                message.getBody();
                stmt = conn.prepareStatement("INSERT INTO " + getTableNameForTopic(topic) + " VALUES(?,?)");
                stmt.setLong(1, seqId);
                stmt.setBlob(2, new SerialBlob(message.toByteArray()));

                int rowCount = stmt.executeUpdate();
                stmt.close();
                if (rowCount != 1) {
                    logger.error("Unexpected number of affected rows from derby");
                    callback.operationFailed(ctx, new ServiceDownException("Unexpected response from derby"));
                    return;
                }
                break;
            } catch (SQLException sqle) {
                String theError = (sqle).getSQLState();
                if (theError.equals("42X05") && !triedCreatingTable) {
                    createTable(conn, topic);
                    triedCreatingTable = true;
                    continue;
                }

                logger.error("Error while executing derby insert", sqle);
                callback.operationFailed(ctx, new ServiceDownException(sqle));
                return;
            }
        }
        callback.operationFinished(ctx, MessageIdUtils.mergeLocalSeqId(message, seqId).getMsgId());
    }

    /*
     * This method does not throw an exception because another thread might
     * sneak in and create the table before us
     */
    private void createTable(Connection conn, ByteString topic) {
        Statement stmt = null;
        try {
            stmt = conn.createStatement();
            String tableName = getTableNameForTopic(topic);
            stmt.execute("CREATE TABLE " + tableName + " (" + ID_FIELD_NAME + " BIGINT NOT NULL CONSTRAINT ID_PK_"
                    + tableName + " PRIMARY KEY," + MSG_FIELD_NAME + " BLOB(2M) NOT NULL)");
        } catch (SQLException e) {
            logger.debug("Could not create table", e);
        } finally {
            try {
                if (stmt != null) {
                    stmt.close();
                }
            } catch (SQLException e) {
                logger.error("Error closing statement", e);
            }
        }
    }

    public MessageSeqId getCurrentSeqIdForTopic(ByteString topic) {
        return ensureSeqIdExistsForTopic(topic);
    }

    public void scanSingleMessage(ScanRequest request) {
        scanMessagesInternal(request.getTopic(), request.getStartSeqId(), 1, Long.MAX_VALUE, request.getCallback(),
                             request.getCtx(), 1);
        return;
    }

    public void scanMessages(RangeScanRequest request) {
        scanMessagesInternal(request.getTopic(), request.getStartSeqId(), request.getMessageLimit(), request
                             .getSizeLimit(), request.getCallback(), request.getCtx(), SCAN_CHUNK);
        return;
    }

    private String getTableNameForTopic(ByteString topic) {
        String src = (topic.toStringUtf8() + "_" + version);
        threadLocalDigest.get().reset();
        byte[] digest = threadLocalDigest.get().digest(src.getBytes());
        BigInteger bigInt = new BigInteger(1,digest);
        return String.format("TABLE_%032X", bigInt);
    }

    private void scanMessagesInternal(ByteString topic, long startSeqId, int messageLimit, long sizeLimit,
                                      ScanCallback callback, Object ctx, int scanChunk) {

        Connection conn = threadLocalConnection.get();

        if (conn == null) {
            callback.scanFailed(ctx, new ServiceDownException("Not connected to derby"));
            return;
        }

        long currentSeqId;
        currentSeqId = startSeqId;

        PreparedStatement stmt = null;
        try {
            try {
                stmt = conn.prepareStatement("SELECT * FROM " + getTableNameForTopic(topic) + " WHERE " + ID_FIELD_NAME
                                             + " >= ?  AND " + ID_FIELD_NAME + " <= ?");

            } catch (SQLException sqle) {
                String theError = (sqle).getSQLState();
                if (theError.equals("42X05")) {
                    // No table, scan is over
                    callback.scanFinished(ctx, ReasonForFinish.NO_MORE_MESSAGES);
                    return;
                } else {
                    throw sqle;
                }
            }

            int numMessages = 0;
            long totalSize = 0;

            while (true) {

                stmt.setLong(1, currentSeqId);
                stmt.setLong(2, currentSeqId + scanChunk);

                if (!stmt.execute()) {
                    String errorMsg = "Select query did not return a result set";
                    logger.error(errorMsg);
                    stmt.close();
                    callback.scanFailed(ctx, new ServiceDownException(errorMsg));
                    return;
                }

                ResultSet resultSet = stmt.getResultSet();

                if (!resultSet.next()) {
                    stmt.close();
                    callback.scanFinished(ctx, ReasonForFinish.NO_MORE_MESSAGES);
                    return;
                }

                do {

                    long localSeqId = resultSet.getLong(1);

                    Message.Builder messageBuilder = Message.newBuilder().mergeFrom(resultSet.getBinaryStream(2));

                    // Merge in the local seq-id since that is not stored with
                    // the message
                    Message message = MessageIdUtils.mergeLocalSeqId(messageBuilder, localSeqId);

                    callback.messageScanned(ctx, message);
                    numMessages++;
                    totalSize += message.getBody().size();

                    if (numMessages > messageLimit) {
                        stmt.close();
                        callback.scanFinished(ctx, ReasonForFinish.NUM_MESSAGES_LIMIT_EXCEEDED);
                        return;
                    } else if (totalSize > sizeLimit) {
                        stmt.close();
                        callback.scanFinished(ctx, ReasonForFinish.SIZE_LIMIT_EXCEEDED);
                        return;
                    }

                } while (resultSet.next());

                currentSeqId += SCAN_CHUNK;
            }
        } catch (SQLException e) {
            logger.error("SQL Exception", e);
            callback.scanFailed(ctx, new ServiceDownException(e));
            return;
        } catch (IOException e) {
            logger.error("Message stored in derby is not parseable", e);
            callback.scanFailed(ctx, new ServiceDownException(e));
            return;
        } finally {
            try {
                if (stmt != null) {
                    stmt.close();
                }
            } catch (SQLException e) {
                logger.error("Error closing statement", e);
            }
        }
    }

    public void deliveredUntil(ByteString topic, Long seqId) {
        // noop
    }

    public void consumedUntil(ByteString topic, Long seqId) {
        Connection conn = threadLocalConnection.get();
        if (conn == null) {
            logger.error("Not connected to derby");
            return;
        }
        PreparedStatement stmt = null;
        try {
            stmt = conn.prepareStatement("DELETE FROM " + getTableNameForTopic(topic) + " WHERE " + ID_FIELD_NAME
                                         + " <= ?");
            stmt.setLong(1, seqId);
            int rowCount = stmt.executeUpdate();
            if (logger.isDebugEnabled()) {
              logger.debug("Deleted " + rowCount + " records for topic: " + topic.toStringUtf8()
                  + ", seqId: " + seqId);
            }
        } catch (SQLException sqle) {
            String theError = (sqle).getSQLState();
            if (theError.equals("42X05")) {
                logger.warn("Table for topic (" + topic + ") does not exist so no consumed messages to delete!");
            } else
                logger.error("Error while executing derby delete for consumed messages", sqle);
        } finally {
            try {
                if (stmt != null) {
                    stmt.close();
                }
            } catch (SQLException e) {
                logger.error("Error closing statement", e);
            }
        }
    }

    public void setMessageBound(ByteString topic, Integer bound) {
        // noop; Maybe implement later
    }

    public void clearMessageBound(ByteString topic) {
        // noop; Maybe implement later
    }

    public void consumeToBound(ByteString topic) {
        // noop; Maybe implement later
    }

    @Override
    protected void finalize() throws Throwable {
        if (driver.equals("org.apache.derby.jdbc.EmbeddedDriver")) {
            boolean gotSQLExc = false;
            // This is weird: on normal shutdown, it throws an exception
            try {
                DriverManager.getConnection("jdbc:derby:;shutdown=true").close();
            } catch (SQLException se) {
                if (se.getSQLState().equals("XJ015")) {
                    gotSQLExc = true;
                }
            }
            if (!gotSQLExc) {
                logger.error("Database did not shut down normally");
            } else {
                logger.info("Database shut down normally");
            }
        }
        super.finalize();
    }

    public void reset() {
        // just move the namespace over to the next one
        version++;
        currTopicSeqIds.clear();
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/persistence/MapMethods.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.persistence;

import java.util.Collection;
import java.util.Map;

public class MapMethods {

    public static <K, V> V getAfterInsertingIfAbsent(Map<K, V> map, K key, Factory<V> valueFactory) {
        V value = map.get(key);

        if (value == null) {
            value = valueFactory.newInstance();
            map.put(key, value);
        }

        return value;
    }

    public static <K, V, Z extends Collection<V>> void addToMultiMap(Map<K, Z> map, K key, V value,
            Factory<Z> valueFactory) {
        Collection<V> collection = getAfterInsertingIfAbsent(map, key, valueFactory);

        collection.add(value);

    }

    public static <K, V, Z extends Collection<V>> boolean removeFromMultiMap(Map<K, Z> map, K key, V value) {
        Collection<V> collection = map.get(key);

        if (collection == null) {
            return false;
        }

        if (!collection.remove(value)) {
            return false;
        } else {
            if (collection.isEmpty()) {
                map.remove(key);
            }
            return true;
        }

    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/persistence/PersistenceManager.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.persistence;

import com.google.protobuf.ByteString;
import org.apache.hedwig.exceptions.PubSubException.ServerNotResponsibleForTopicException;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;

/**
 * An implementation of this interface will persist messages in order and assign
 * a seqId to each persisted message. SeqId need not be a single number in
 * general. SeqId is opaque to all layers above {@link PersistenceManager}. Only
 * the {@link PersistenceManager} needs to understand the format of the seqId
 * and maintain it in such a way that there is a total order on the seqIds of a
 * topic.
 *
 */
public interface PersistenceManager {

    /**
     * Executes the given persist request asynchronously. When done, the
     * callback specified in the request object is called with the result of the
     * operation set to the {@link LocalMessageSeqId} assigned to the persisted
     * message.
     */
    public void persistMessage(PersistRequest request);

    /**
     * Get the seqId of the last message that has been persisted to the given
     * topic. The returned seqId will be set as the consume position of any
     * brand new subscription on this topic.
     *
     * Note that the return value may quickly become invalid because a
     * {@link #persistMessage(String, PublishedMessage)} call from another
     * thread succeeds. For us, the typical use case is choosing the consume
     * position of a new subscriber. Since the subscriber need not receive all
     * messages that are published while the subscribe call is in progress, such
     * loose semantics from this method is acceptable.
     *
     * @param topic
     * @return the seqId of the last persisted message.
     * @throws ServerNotResponsibleForTopicException
     */
    public MessageSeqId getCurrentSeqIdForTopic(ByteString topic) throws ServerNotResponsibleForTopicException;

    /**
     * Executes the given scan request
     *
     */
    public void scanSingleMessage(ScanRequest request);

    /**
     * Gets the next seq-id. This method should never block.
     */
    public long getSeqIdAfterSkipping(ByteString topic, long seqId, int skipAmount);

    /**
     * Hint that the messages until the given seqId have been delivered and wont
     * be needed unless there is a failure of some kind
     */
    public void deliveredUntil(ByteString topic, Long seqId);

    /**
     * Hint that the messages until the given seqId have been consumed by all
     * subscribers to the topic and no longer need to be stored. The
     * implementation classes can decide how and if they want to garbage collect
     * and delete these older topic messages that are no longer needed.
     *
     * @param topic
     *            Topic
     * @param seqId
     *            Message local sequence ID
     */
    public void consumedUntil(ByteString topic, Long seqId);

    public void setMessageBound(ByteString topic, Integer bound);
    public void clearMessageBound(ByteString topic);
    public void consumeToBound(ByteString topic);

    /**
     * Stop persistence manager.
     */
    public void stop();
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/persistence/PersistenceManagerWithRangeScan.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.persistence;

public interface PersistenceManagerWithRangeScan extends PersistenceManager {
    /**
     * Executes the given range scan request
     *
     * @param request
     */
    public void scanMessages(RangeScanRequest request);
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/persistence/PersistRequest.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.persistence;

import com.google.protobuf.ByteString;
import org.apache.hedwig.protocol.PubSubProtocol;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.util.Callback;

/**
 * Encapsulates a request to persist a given message on a given topic. The
 * request is completed asynchronously, callback and context are provided
 *
 */
public class PersistRequest {
    ByteString topic;
    Message message;
    private Callback<PubSubProtocol.MessageSeqId> callback;
    Object ctx;

    public PersistRequest(ByteString topic, Message message, Callback<PubSubProtocol.MessageSeqId> callback, Object ctx) {
        this.topic = topic;
        this.message = message;
        this.callback = callback;
        this.ctx = ctx;
    }

    public ByteString getTopic() {
        return topic;
    }

    public Message getMessage() {
        return message;
    }

    public Callback<PubSubProtocol.MessageSeqId> getCallback() {
        return callback;
    }

    public Object getCtx() {
        return ctx;
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/persistence/RangeScanRequest.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.persistence;

import com.google.protobuf.ByteString;

/**
 * Encapsulates a request to scan messages on the given topic starting from the
 * given seqId (included). A call-back {@link ScanCallback} is provided. As
 * messages are scanned, the relevant methods of the {@link ScanCallback} are
 * called. Two hints are provided as to when scanning should stop: in terms of
 * number of messages scanned, or in terms of the total size of messages
 * scanned. Scanning stops whenever one of these limits is exceeded. These
 * checks, especially the one about message size, are only approximate. The
 * {@link ScanCallback} used should be prepared to deal with more or less
 * messages scanned. If an error occurs during scanning, the
 * {@link ScanCallback} is notified of the error.
 *
 */
public class RangeScanRequest {
    ByteString topic;
    long startSeqId;
    int messageLimit;
    long sizeLimit;
    ScanCallback callback;
    Object ctx;

    public RangeScanRequest(ByteString topic, long startSeqId, int messageLimit, long sizeLimit, ScanCallback callback,
                            Object ctx) {
        this.topic = topic;
        this.startSeqId = startSeqId;
        this.messageLimit = messageLimit;
        this.sizeLimit = sizeLimit;
        this.callback = callback;
        this.ctx = ctx;
    }

    public ByteString getTopic() {
        return topic;
    }

    public long getStartSeqId() {
        return startSeqId;
    }

    public int getMessageLimit() {
        return messageLimit;
    }

    public long getSizeLimit() {
        return sizeLimit;
    }

    public ScanCallback getCallback() {
        return callback;
    }

    public Object getCtx() {
        return ctx;
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/persistence/ReadAheadCache.java,true,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.persistence;

import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.LinkedList;
import java.util.Map;
import java.util.Queue;
import java.util.Set;
import java.util.SortedMap;
import java.util.SortedSet;
import java.util.TreeMap;
import java.util.TreeSet;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ConcurrentMap;
import java.util.concurrent.BlockingQueue;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.RejectedExecutionException;
import java.util.concurrent.atomic.AtomicLong;
import java.util.concurrent.atomic.AtomicInteger;

import org.apache.hedwig.protocol.PubSubProtocol;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.protobuf.ByteString;

import org.apache.bookkeeper.util.MathUtils;
import org.apache.bookkeeper.util.OrderedSafeExecutor;
import org.apache.bookkeeper.util.SafeRunnable;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.ServerNotResponsibleForTopicException;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protoextensions.MessageIdUtils;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.common.UnexpectedError;
import org.apache.hedwig.server.jmx.HedwigJMXService;
import org.apache.hedwig.server.jmx.HedwigMBeanInfo;
import org.apache.hedwig.server.jmx.HedwigMBeanRegistry;
import org.apache.hedwig.server.persistence.ReadAheadCacheBean;
import org.apache.hedwig.util.Callback;

public class ReadAheadCache implements PersistenceManager, HedwigJMXService {

    static Logger logger = LoggerFactory.getLogger(ReadAheadCache.class);

    protected interface CacheRequest {
        public void performRequest();
    }

    /**
     * The underlying persistence manager that will be used for persistence and
     * scanning below the cache
     */
    protected PersistenceManagerWithRangeScan realPersistenceManager;

    /**
     * The structure for the cache
     */
    protected ConcurrentMap<CacheKey, CacheValue> cache =
        new ConcurrentHashMap<CacheKey, CacheValue>();

    /**
     * We also want to track the entries in seq-id order so that we can clean up
     * entries after the last subscriber
     */
    protected ConcurrentMap<ByteString, SortedSet<Long>> orderedIndexOnSeqId =
        new ConcurrentHashMap<ByteString, SortedSet<Long>>();

    /**
     * Partition Cache into Serveral Segments for simplify synchronization.
     * Each segment maintains its time index and segment size.
     */
    static class CacheSegment {

        /**
         * We want to keep track of when entries were added in the cache, so that we
         * can remove them in a FIFO fashion
         */
        protected SortedMap<Long, Set<CacheKey>> timeIndexOfAddition = new TreeMap<Long, Set<CacheKey>>();

        /**
         * We maintain an estimate of the current size of each cache segment,
         * so that the thread know when to evict entries from cache segment.
         */
        protected AtomicLong presentSegmentSize = new AtomicLong(0);

    }

    /**
     * We maintain an estimate of the current size of the cache, so that we know
     * when to evict entries.
     */
    protected AtomicLong presentCacheSize = new AtomicLong(0);

    /**
     * Num pending requests.
     */
    protected AtomicInteger numPendingRequests = new AtomicInteger(0);

    /**
     * Cache segment for different threads
     */
    protected final ThreadLocal<CacheSegment> cacheSegment =
        new ThreadLocal<CacheSegment>() {
            @Override
            protected CacheSegment initialValue() {
                return new CacheSegment();
            }
        };

    /**
     * One instance of a callback that we will pass to the underlying
     * persistence manager when asking it to persist messages
     */
    protected PersistCallback persistCallbackInstance = new PersistCallback();

    /**
     * 2 kinds of exceptions that we will use to signal error from readahead
     */
    protected NoSuchSeqIdException noSuchSeqIdExceptionInstance = new NoSuchSeqIdException();
    protected ReadAheadException readAheadExceptionInstance = new ReadAheadException();

    protected ServerConfiguration cfg;
    // Boolean indicating if this thread should continue running. This is used
    // when we want to stop the thread during a PubSubServer shutdown.
    protected volatile boolean keepRunning = true;

    protected final OrderedSafeExecutor cacheWorkers;
    protected final int numCacheWorkers;
    protected volatile long maxSegmentSize;
    protected volatile long cacheEntryTTL;

    // JMX Beans
    ReadAheadCacheBean jmxCacheBean = null;

    /**
     * Constructor. Starts the cache maintainer thread
     *
     * @param realPersistenceManager
     */
    public ReadAheadCache(PersistenceManagerWithRangeScan realPersistenceManager, ServerConfiguration cfg) {
        this.realPersistenceManager = realPersistenceManager;
        this.cfg = cfg;
        numCacheWorkers = cfg.getNumReadAheadCacheThreads();
        cacheWorkers = new OrderedSafeExecutor(numCacheWorkers);
        reloadConf(cfg);
    }

    /**
     * Reload configuration
     *
     * @param conf
     *          Server configuration object
     */
    protected void reloadConf(ServerConfiguration cfg) {
        maxSegmentSize = cfg.getMaximumCacheSize() / numCacheWorkers;
        cacheEntryTTL = cfg.getCacheEntryTTL();
    }

    public ReadAheadCache start() {
        return this;
    }

    /**
     * ========================================================================
     * Methods of {@link PersistenceManager} that we will pass straight down to
     * the real persistence manager.
     */

    public long getSeqIdAfterSkipping(ByteString topic, long seqId, int skipAmount) {
        return realPersistenceManager.getSeqIdAfterSkipping(topic, seqId, skipAmount);
    }

    public MessageSeqId getCurrentSeqIdForTopic(ByteString topic) throws ServerNotResponsibleForTopicException {
        return realPersistenceManager.getCurrentSeqIdForTopic(topic);
    }

    /**
     * ========================================================================
     * Other methods of {@link PersistenceManager} that the cache needs to take
     * some action on.
     *
     * 1. Persist: We pass it through to the real persistence manager but insert
     * our callback on the return path
     *
     */
    public void persistMessage(PersistRequest request) {
        // make a new PersistRequest object so that we can insert our own
        // callback in the middle. Assign the original request as the context
        // for the callback.

        PersistRequest newRequest = new PersistRequest(request.getTopic(), request.getMessage(),
                persistCallbackInstance, request);
        realPersistenceManager.persistMessage(newRequest);
    }

    /**
     * The callback that we insert on the persist request return path. The
     * callback simply forms a {@link PersistResponse} object and inserts it in
     * the request queue to be handled serially by the cache maintainer thread.
     *
     */
    public class PersistCallback implements Callback<PubSubProtocol.MessageSeqId> {

        /**
         * In case there is a failure in persisting, just pass it to the
         * original callback
         */
        public void operationFailed(Object ctx, PubSubException exception) {
            PersistRequest originalRequest = (PersistRequest) ctx;
            Callback<PubSubProtocol.MessageSeqId> originalCallback = originalRequest.getCallback();
            Object originalContext = originalRequest.getCtx();
            originalCallback.operationFailed(originalContext, exception);
        }

        /**
         * When the persist finishes, we first notify the original callback of
         * success, and then opportunistically treat the message as if it just
         * came in through a scan
         */
        public void operationFinished(Object ctx, PubSubProtocol.MessageSeqId resultOfOperation) {
            PersistRequest originalRequest = (PersistRequest) ctx;

            // Lets call the original callback first so that the publisher can
            // hear success
            originalRequest.getCallback().operationFinished(originalRequest.getCtx(), resultOfOperation);

            // Original message that was persisted didn't have the local seq-id.
            // Lets add that in
            Message messageWithLocalSeqId = MessageIdUtils.mergeLocalSeqId(originalRequest.getMessage(),
                                            resultOfOperation.getLocalComponent());

            // Now enqueue a request to add this newly persisted message to our
            // cache
            CacheKey cacheKey = new CacheKey(originalRequest.getTopic(), resultOfOperation.getLocalComponent());

            enqueueWithoutFailureByTopic(cacheKey.getTopic(),
                    new ScanResponse(cacheKey, messageWithLocalSeqId));
        }

    }

    protected void enqueueWithoutFailureByTopic(ByteString topic, final CacheRequest obj) {
        if (!keepRunning) {
            return;
        }
        try {
            numPendingRequests.incrementAndGet();
            cacheWorkers.submitOrdered(topic, new SafeRunnable() {
                @Override
                public void safeRun() {
                    numPendingRequests.decrementAndGet();
                    obj.performRequest();
                }
            });
        } catch (RejectedExecutionException ree) {
            logger.error("Failed to submit cache request for topic " + topic.toStringUtf8() + " : ", ree);
        }
    }

    /**
     * Another method from {@link PersistenceManager}.
     *
     * 2. Scan - Since the scan needs to touch the cache, we will just enqueue
     * the scan request and let the cache maintainer thread handle it.
     */
    public void scanSingleMessage(ScanRequest request) {
        // Let the scan requests be serialized through the queue
        enqueueWithoutFailureByTopic(request.getTopic(),
                new ScanRequestWrapper(request));
    }

    /**
     * Another method from {@link PersistenceManager}.
     *
     * 3. Enqueue the request so that the cache maintainer thread can delete all
     * message-ids older than the one specified
     */
    public void deliveredUntil(ByteString topic, Long seqId) {
        enqueueWithoutFailureByTopic(topic, new DeliveredUntil(topic, seqId));
    }

    /**
     * Another method from {@link PersistenceManager}.
     *
     * Since this is a cache layer on top of an underlying persistence manager,
     * we can just call the consumedUntil method there. The messages older than
     * the latest one passed here won't be accessed anymore so they should just
     * get aged out of the cache eventually. For now, there is no need to
     * proactively remove those entries from the cache.
     */
    public void consumedUntil(ByteString topic, Long seqId) {
        realPersistenceManager.consumedUntil(topic, seqId);
    }

    public void setMessageBound(ByteString topic, Integer bound) {
        realPersistenceManager.setMessageBound(topic, bound);
    }

    public void clearMessageBound(ByteString topic) {
        realPersistenceManager.clearMessageBound(topic);
    }

    public void consumeToBound(ByteString topic) {
        realPersistenceManager.consumeToBound(topic);
    }

    /**
     * Stop the readahead cache.
     */
    public void stop() {
        try {
            keepRunning = false;
            cacheWorkers.shutdown();
        } catch (Exception e) {
            logger.warn("Failed to shut down cache workers : ", e);
        }
    }

    /**
     * The readahead policy is simple: We check if an entry already exists for
     * the message being requested. If an entry exists, it means that either
     * that message is already in the cache, or a read for that message is
     * outstanding. In that case, we look a little ahead (by readAheadCount/2)
     * and issue a range read of readAheadCount/2 messages. The idea is to
     * ensure that the next readAheadCount messages are always available.
     *
     * @return the range scan that should be issued for read ahead
     */
    protected RangeScanRequest doReadAhead(ScanRequest request) {
        ByteString topic = request.getTopic();
        Long seqId = request.getStartSeqId();

        int readAheadCount = cfg.getReadAheadCount();
        // To prevent us from getting screwed by bad configuration
        readAheadCount = Math.max(1, readAheadCount);

        RangeScanRequest readAheadRequest = doReadAheadStartingFrom(topic, seqId, readAheadCount);

        if (readAheadRequest != null) {
            return readAheadRequest;
        }

        // start key was already there in the cache so no readahead happened,
        // lets look a little beyond
        seqId = realPersistenceManager.getSeqIdAfterSkipping(topic, seqId, readAheadCount / 2);

        readAheadRequest = doReadAheadStartingFrom(topic, seqId, readAheadCount / 2);

        return readAheadRequest;
    }

    /**
     * This method just checks if the provided seq-id already exists in the
     * cache. If not, a range read of the specified amount is issued.
     *
     * @param topic
     * @param seqId
     * @param readAheadCount
     * @return The range read that should be issued
     */
    protected RangeScanRequest doReadAheadStartingFrom(ByteString topic, long seqId, int readAheadCount) {

        long startSeqId = seqId;
        Queue<CacheKey> installedStubs = new LinkedList<CacheKey>();

        int i = 0;

        for (; i < readAheadCount; i++) {
            CacheKey cacheKey = new CacheKey(topic, seqId);

            // Even if a stub exists, it means that a scan for that is
            // outstanding
            if (cache.containsKey(cacheKey)) {
                break;
            }
            CacheValue cacheValue = new CacheValue();
            if (null != cache.putIfAbsent(cacheKey, cacheValue)) {
                logger.warn("It is unexpected that more than one threads are adding message to cache key {}"
                            +" at the same time.", cacheKey);
            }

            logger.debug("Adding cache stub for: {}", cacheKey);
            installedStubs.add(cacheKey);

            seqId = realPersistenceManager.getSeqIdAfterSkipping(topic, seqId, 1);
        }

        // so how many did we decide to readahead
        if (i == 0) {
            // no readahead, hence return false
            return null;
        }

        long readAheadSizeLimit = cfg.getReadAheadSizeBytes();
        ReadAheadScanCallback callback = new ReadAheadScanCallback(installedStubs, topic);
        RangeScanRequest rangeScanRequest = new RangeScanRequest(topic, startSeqId, i, readAheadSizeLimit, callback,
                null);

        return rangeScanRequest;

    }

    /**
     * This is the callback that is used for the range scans.
     */
    protected class ReadAheadScanCallback implements ScanCallback {
        Queue<CacheKey> installedStubs;
        ByteString topic;

        /**
         * Constructor
         *
         * @param installedStubs
         *            The list of stubs that were installed for this range scan
         * @param topic
         */
        public ReadAheadScanCallback(Queue<CacheKey> installedStubs, ByteString topic) {
            this.installedStubs = installedStubs;
            this.topic = topic;
        }

        public void messageScanned(Object ctx, Message message) {

            // Any message we read is potentially useful for us, so lets first
            // enqueue it
            CacheKey cacheKey = new CacheKey(topic, message.getMsgId().getLocalComponent());
            enqueueWithoutFailureByTopic(topic, new ScanResponse(cacheKey, message));

            // Now lets see if this message is the one we were expecting
            CacheKey expectedKey = installedStubs.peek();

            if (expectedKey == null) {
                // Was not expecting any more messages to come in, but they came
                // in so we will keep them
                return;
            }

            if (expectedKey.equals(cacheKey)) {
                // what we got is what we expected, dequeue it so we get the
                // next expected one
                installedStubs.poll();
                return;
            }

            // If reached here, what we scanned was not what we were expecting.
            // This means that we have wrong stubs installed in the cache. We
            // should remove them, so that whoever is waiting on them can retry.
            // This shouldn't be happening usually
            logger.warn("Unexpected message seq-id: " + message.getMsgId().getLocalComponent() + " on topic: "
                        + topic.toStringUtf8() + " from readahead scan, was expecting seq-id: " + expectedKey.seqId
                        + " topic: " + expectedKey.topic.toStringUtf8() + " installedStubs: " + installedStubs);
            enqueueDeleteOfRemainingStubs(noSuchSeqIdExceptionInstance);

        }

        public void scanFailed(Object ctx, Exception exception) {
            enqueueDeleteOfRemainingStubs(exception);
        }

        public void scanFinished(Object ctx, ReasonForFinish reason) {
            // If the scan finished because no more messages are present, its ok
            // to leave the stubs in place because they will get filled in as
            // new publishes happen. However, if the scan finished due to some
            // other reason, e.g., read ahead size limit was reached, we want to
            // delete the stubs, so that when the time comes, we can schedule
            // another readahead request.
            if (reason != ReasonForFinish.NO_MORE_MESSAGES) {
                enqueueDeleteOfRemainingStubs(readAheadExceptionInstance);
            }
        }

        private void enqueueDeleteOfRemainingStubs(Exception reason) {
            CacheKey installedStub;
            while ((installedStub = installedStubs.poll()) != null) {
                enqueueWithoutFailureByTopic(installedStub.getTopic(),
                        new ExceptionOnCacheKey(installedStub, reason));
            }
        }
    }

    protected static class HashSetCacheKeyFactory implements Factory<Set<CacheKey>> {
        protected final static HashSetCacheKeyFactory instance = new HashSetCacheKeyFactory();

        public Set<CacheKey> newInstance() {
            return new HashSet<CacheKey>();
        }
    }

    protected static class TreeSetLongFactory implements Factory<SortedSet<Long>> {
        protected final static TreeSetLongFactory instance = new TreeSetLongFactory();

        public SortedSet<Long> newInstance() {
            return new TreeSet<Long>();
        }
    }

    /**
     * For adding the message to the cache, we do some bookeeping such as the
     * total size of cache, order in which entries were added etc. If the size
     * of the cache has exceeded our budget, old entries are collected.
     *
     * @param cacheKey
     * @param message
     */
    protected void addMessageToCache(final CacheKey cacheKey,
                                     final Message message, final long currTime) {
        logger.debug("Adding msg {} to readahead cache", cacheKey);

        CacheValue cacheValue;

        if ((cacheValue = cache.get(cacheKey)) == null) {
            cacheValue = new CacheValue();
            CacheValue oldValue = cache.putIfAbsent(cacheKey, cacheValue);
            if (null != oldValue) {
                logger.warn("Weird! Should not have two threads adding message to cache key {} at the same time.",
                            cacheKey);
                cacheValue = oldValue;
            }
        }

        CacheSegment segment = cacheSegment.get();
        int size = message.getBody().size();

        // update the cache size
        segment.presentSegmentSize.addAndGet(size);
        presentCacheSize.addAndGet(size);

        synchronized (cacheValue) {
            // finally add the message to the cache
            cacheValue.setMessageAndInvokeCallbacks(message, currTime);
        }

        // maintain the index of seq-id
        // no lock since threads are partitioned by topics
        MapMethods.addToMultiMap(orderedIndexOnSeqId, cacheKey.getTopic(),
                                 cacheKey.getSeqId(), TreeSetLongFactory.instance);

        // maintain the time index of addition
        MapMethods.addToMultiMap(segment.timeIndexOfAddition, currTime,
                                 cacheKey, HashSetCacheKeyFactory.instance);

        collectOldOrExpiredCacheEntries(segment);
    }

    protected void removeMessageFromCache(final CacheKey cacheKey, Exception exception,
                                          final boolean maintainTimeIndex,
                                          final boolean maintainSeqIdIndex) {
        CacheValue cacheValue = cache.remove(cacheKey);

        if (cacheValue == null) {
            return;
        }

        CacheSegment segment = cacheSegment.get();

        long timeOfAddition = 0;
        synchronized (cacheValue) {
            if (cacheValue.isStub()) {
                cacheValue.setErrorAndInvokeCallbacks(exception);
                // Stubs are not present in the indexes, so don't need to maintain
                // indexes here
                return;
            }

            int size = 0 - cacheValue.getMessage().getBody().size();
            presentCacheSize.addAndGet(size);
            segment.presentSegmentSize.addAndGet(size);
            timeOfAddition = cacheValue.getTimeOfAddition();
        }

        if (maintainSeqIdIndex) {
            MapMethods.removeFromMultiMap(orderedIndexOnSeqId, cacheKey.getTopic(),
                                          cacheKey.getSeqId());
        }
        if (maintainTimeIndex) {
            MapMethods.removeFromMultiMap(segment.timeIndexOfAddition,
                                          timeOfAddition, cacheKey);
        }
    }

    /**
     * Collection of old entries is simple. Just collect in insert-time order,
     * oldest to newest.
     */
    protected void collectOldOrExpiredCacheEntries(CacheSegment segment) {
        if (cacheEntryTTL > 0) {
            // clear expired entries
            while (!segment.timeIndexOfAddition.isEmpty()) {
                Long earliestTime = segment.timeIndexOfAddition.firstKey();
                if (MathUtils.now() - earliestTime < cacheEntryTTL) {
                    break;
                }
                collectCacheEntriesAtTimestamp(segment, earliestTime);
            }
        }

        while (segment.presentSegmentSize.get() > maxSegmentSize &&
               !segment.timeIndexOfAddition.isEmpty()) {
            Long earliestTime = segment.timeIndexOfAddition.firstKey();
            collectCacheEntriesAtTimestamp(segment, earliestTime);
        }
    }

    private void collectCacheEntriesAtTimestamp(CacheSegment segment, long timestamp) {
        Set<CacheKey> oldCacheEntries = segment.timeIndexOfAddition.get(timestamp);

        // Note: only concrete cache entries, and not stubs are in the time
        // index. Hence there can be no callbacks pending on these cache
        // entries. Hence safe to remove them directly.
        for (Iterator<CacheKey> iter = oldCacheEntries.iterator(); iter.hasNext();) {
            final CacheKey cacheKey = iter.next();

            logger.debug("Removing {} from cache because it's the oldest.", cacheKey);
            removeMessageFromCache(cacheKey, readAheadExceptionInstance, //
                                   // maintainTimeIndex=
                                   false,
                                   // maintainSeqIdIndex=
                                   true);
        }

        segment.timeIndexOfAddition.remove(timestamp);
    }

    /**
     * ========================================================================
     * The rest is just simple wrapper classes.
     *
     */

    protected class ExceptionOnCacheKey implements CacheRequest {
        CacheKey cacheKey;
        Exception exception;

        public ExceptionOnCacheKey(CacheKey cacheKey, Exception exception) {
            this.cacheKey = cacheKey;
            this.exception = exception;
        }

        /**
         * If for some reason, an outstanding read on a cache stub fails,
         * exception for that key is enqueued by the
         * {@link ReadAheadScanCallback}. To handle this, we simply send error
         * on the callbacks registered for that stub, and delete the entry from
         * the cache
         */
        public void performRequest() {
            removeMessageFromCache(cacheKey, exception,
                                   // maintainTimeIndex=
                                   true,
                                   // maintainSeqIdIndex=
                                   true);
        }

    }

    @SuppressWarnings("serial")
    protected static class NoSuchSeqIdException extends Exception {

        public NoSuchSeqIdException() {
            super("No such seq-id");
        }
    }

    @SuppressWarnings("serial")
    protected static class ReadAheadException extends Exception {
        public ReadAheadException() {
            super("Readahead failed");
        }
    }

    public class CancelScanRequestOp implements CacheRequest {

        final CancelScanRequest request;

        public CancelScanRequestOp(CancelScanRequest request) {
            this.request = request;
        }

        public void performRequest() {
            // cancel scan request
            cancelScanRequest(request.getScanRequest());
        }

        void cancelScanRequest(ScanRequest request) {
            if (null == request) {
                // nothing to cancel
                return;
            }

            CacheKey cacheKey = new CacheKey(request.getTopic(), request.getStartSeqId());
            CacheValue cacheValue = cache.get(cacheKey);
            if (null == cacheValue) {
                // cache value is evicted
                // so it's callback would be called, we don't need to worry about
                // cancel it. since it was treated as executed.
                return;
            }
            cacheValue.removeCallback(request.getCallback(), request.getCtx());
        }
    }

    public void cancelScanRequest(ByteString topic, CancelScanRequest request) {
        enqueueWithoutFailureByTopic(topic, new CancelScanRequestOp(request));
    }

    protected class ScanResponse implements CacheRequest {
        CacheKey cacheKey;
        Message message;

        public ScanResponse(CacheKey cacheKey, Message message) {
            this.cacheKey = cacheKey;
            this.message = message;
        }

        public void performRequest() {
            addMessageToCache(cacheKey, message, MathUtils.now());
        }

    }

    protected class DeliveredUntil implements CacheRequest {
        ByteString topic;
        Long seqId;

        public DeliveredUntil(ByteString topic, Long seqId) {
            this.topic = topic;
            this.seqId = seqId;
        }

        public void performRequest() {
            SortedSet<Long> orderedSeqIds = orderedIndexOnSeqId.get(topic);
            if (orderedSeqIds == null) {
                return;
            }

            // focus on the set of messages with seq-ids <= the one that
            // has been delivered until
            SortedSet<Long> headSet = orderedSeqIds.headSet(seqId + 1);

            for (Iterator<Long> iter = headSet.iterator(); iter.hasNext();) {
                Long seqId = iter.next();
                CacheKey cacheKey = new CacheKey(topic, seqId);

                logger.debug("Removing {} from cache because every subscriber has moved past",
                    cacheKey);

                removeMessageFromCache(cacheKey, readAheadExceptionInstance, //
                                       // maintainTimeIndex=
                                       true,
                                       // maintainSeqIdIndex=
                                       false);
                iter.remove();
            }

            if (orderedSeqIds.isEmpty()) {
                orderedIndexOnSeqId.remove(topic);
            }
        }
    }

    protected class ScanRequestWrapper implements CacheRequest {
        ScanRequest request;

        public ScanRequestWrapper(ScanRequest request) {
            this.request = request;
        }

        /**
         * To handle a scan request, we first try to do readahead (which might
         * cause a range read to be issued to the underlying persistence
         * manager). The readahead will put a stub in the cache, if the message
         * is not already present in the cache. The scan callback that is part
         * of the scan request is added to this stub, and will be called later
         * when the message arrives as a result of the range scan issued to the
         * underlying persistence manager.
         */

        public void performRequest() {

            RangeScanRequest readAheadRequest = doReadAhead(request);

            // Read ahead must have installed at least a stub for us, so this
            // can't be null
            CacheKey cacheKey = new CacheKey(request.getTopic(), request.getStartSeqId());
            CacheValue cacheValue = cache.get(cacheKey);
            if (null == cacheValue) {
                logger.error("Cache key {} is removed after installing stub when scanning.", cacheKey);
                // reissue the request 
                scanSingleMessage(request);
                return;
            }

            synchronized (cacheValue) {
                // Add our callback to the stub. If the cache value was already a
                // concrete message, the callback will be called right away
                cacheValue.addCallback(request.getCallback(), request.getCtx());
            }

            if (readAheadRequest != null) {
                realPersistenceManager.scanMessages(readAheadRequest);
            }
        }
    }

    @Override
    public void registerJMX(HedwigMBeanInfo parent) {
        try {
            jmxCacheBean = new ReadAheadCacheBean(this);
            HedwigMBeanRegistry.getInstance().register(jmxCacheBean, parent);
        } catch (Exception e) {
            logger.warn("Failed to register readahead cache with JMX", e);
            jmxCacheBean = null;
        }
    }

    @Override
    public void unregisterJMX() {
        try {
            if (jmxCacheBean != null) {
                HedwigMBeanRegistry.getInstance().unregister(jmxCacheBean);
            }
        } catch (Exception e) {
            logger.warn("Failed to unregister readahead cache with JMX", e);
        }
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/persistence/ReadAheadCacheBean.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hedwig.server.persistence;

import org.apache.hedwig.server.jmx.HedwigMBeanInfo;

/**
 * Read Ahead Cache Bean
 */
public class ReadAheadCacheBean implements ReadAheadCacheMXBean,
        HedwigMBeanInfo {

    ReadAheadCache cache;
    public ReadAheadCacheBean(ReadAheadCache cache) {
        this.cache = cache;
    }

    @Override
    public String getName() {
        return "ReadAheadCache";
    }

    @Override
    public boolean isHidden() {
        return false;
    }

    @Override
    public long getMaxCacheSize() {
        return cache.cfg.getMaximumCacheSize();
    }

    @Override
    public long getPresentCacheSize() {
        return cache.presentCacheSize.get();
    }

    @Override
    public int getNumCachedEntries() {
        return cache.cache.size();
    }

    @Override
    public int getNumPendingCacheRequests() {
        return cache.numPendingRequests.get();
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/persistence/ReadAheadCacheMXBean.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.hedwig.server.persistence;

/**
 * Read Ahead Cache MBean
 */
public interface ReadAheadCacheMXBean {

    /**
     * @return max cache size
     */
    public long getMaxCacheSize();

    /**
     * @return present cache size
     */
    public long getPresentCacheSize();

    /**
     * @return number of cached entries
     */
    public int getNumCachedEntries();

    /**
     * @return number of pending cache requests
     */
    public int getNumPendingCacheRequests();
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/persistence/ScanCallback.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.persistence;

import org.apache.hedwig.protocol.PubSubProtocol.Message;

public interface ScanCallback {

    enum ReasonForFinish {
        NO_MORE_MESSAGES, SIZE_LIMIT_EXCEEDED, NUM_MESSAGES_LIMIT_EXCEEDED
    };

    /**
     * This method is called when a message is read from the persistence layer
     * as part of a scan. The message just read is handed to this listener which
     * can then take the desired action on it. The return value from the method
     * indicates whether the scan should continue or not.
     *
     * @param ctx
     *            The context for the callback
     * @param message
     *            The message just scanned from the log
     * @return true if the scan should continue, false otherwise
     */
    public void messageScanned(Object ctx, Message message);

    /**
     * This method is called when the scan finishes
     *
     *
     * @param ctx
     * @param reason
     */

    public abstract void scanFinished(Object ctx, ReasonForFinish reason);

    /**
     * This method is called when the operation failed due to some reason. The
     * reason for failure is passed in.
     *
     * @param ctx
     *            The context for the callback
     * @param exception
     *            The reason for the failure of the scan
     */
    public abstract void scanFailed(Object ctx, Exception exception);

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/persistence/ScanCallbackWithContext.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.persistence;

public class ScanCallbackWithContext {
    ScanCallback scanCallback;
    Object ctx;

    public ScanCallbackWithContext(ScanCallback callback, Object ctx) {
        this.scanCallback = callback;
        this.ctx = ctx;
    }

    public ScanCallback getScanCallback() {
        return scanCallback;
    }

    public Object getCtx() {
        return ctx;
    }

    @Override
    public boolean equals(Object other) {
        if (!(other instanceof ScanCallbackWithContext)) {
            return false;
        }
        ScanCallbackWithContext otherCb =
            (ScanCallbackWithContext) other;
        // Ensure that it was same callback & same ctx
        return scanCallback == otherCb.scanCallback &&
               ctx == otherCb.ctx;
    }

    @Override
    public int hashCode() {
        return scanCallback.hashCode();
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/persistence/ScanRequest.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.persistence;

import com.google.protobuf.ByteString;
import org.apache.hedwig.protocol.PubSubProtocol.Message;

/**
 * Encapsulates a request for reading a single message. The message on the given
 * topic <b>at</b> the given seqId is scanned. A call-back {@link ScanCallback}
 * is provided. When the message is scanned, the
 * {@link ScanCallback#messageScanned(Object, Message)} method is called. Since
 * there is only 1 record to be scanned the
 * {@link ScanCallback#operationFinished(Object)} method may not be called since
 * its redundant.
 * {@link ScanCallback#scanFailed(Object, org.apache.hedwig.exceptions.PubSubException)}
 * method is called in case of error.
 *
 */
public class ScanRequest {
    ByteString topic;
    long startSeqId;
    ScanCallback callback;
    Object ctx;

    public ScanRequest(ByteString topic, long startSeqId, ScanCallback callback, Object ctx) {
        this.topic = topic;
        this.startSeqId = startSeqId;
        this.callback = callback;
        this.ctx = ctx;
    }

    public ByteString getTopic() {
        return topic;
    }

    public long getStartSeqId() {
        return startSeqId;
    }

    public ScanCallback getCallback() {
        return callback;
    }

    public Object getCtx() {
        return ctx;
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/proxy/ChannelTracker.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.proxy;

import java.util.HashMap;
import java.util.LinkedList;
import java.util.List;

import org.jboss.netty.channel.Channel;

import com.google.protobuf.ByteString;
import org.apache.hedwig.client.api.Subscriber;
import org.apache.hedwig.client.data.TopicSubscriber;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.TopicBusyException;
import org.apache.hedwig.server.handlers.ChannelDisconnectListener;
import org.apache.hedwig.util.Callback;

public class ChannelTracker implements ChannelDisconnectListener {
    HashMap<TopicSubscriber, Channel> topicSub2Channel = new HashMap<TopicSubscriber, Channel>();
    HashMap<Channel, List<TopicSubscriber>> channel2TopicSubs = new HashMap<Channel, List<TopicSubscriber>>();
    Subscriber subscriber;

    public ChannelTracker(Subscriber subscriber) {
        this.subscriber = subscriber;
    }

    static Callback<Void> noOpCallback = new Callback<Void>() {
        public void operationFailed(Object ctx, PubSubException exception) {
        };

        public void operationFinished(Object ctx, Void resultOfOperation) {
        };
    };

    public synchronized void channelDisconnected(Channel channel) {
        List<TopicSubscriber> topicSubs = channel2TopicSubs.remove(channel);

        if (topicSubs == null) {
            return;
        }

        for (TopicSubscriber topicSub : topicSubs) {
            topicSub2Channel.remove(topicSub);
            subscriber.asyncCloseSubscription(topicSub.getTopic(), topicSub.getSubscriberId(), noOpCallback, null);
        }
    }

    public synchronized void subscribeSucceeded(TopicSubscriber topicSubscriber, Channel channel)
            throws TopicBusyException {

        if (!channel.isConnected()) {
            // channel got disconnected while we were processing the
            // subscribe request, nothing much we can do in this case
            return;
        }

        if (topicSub2Channel.containsKey(topicSubscriber)) {
            TopicBusyException pse = new PubSubException.TopicBusyException(
                "subscription for this topic, subscriberId is already being served on a different channel");
            throw pse;
        }

        topicSub2Channel.put(topicSubscriber, channel);

        List<TopicSubscriber> topicSubs = channel2TopicSubs.get(channel);

        if (topicSubs == null) {
            topicSubs = new LinkedList<TopicSubscriber>();
            channel2TopicSubs.put(channel, topicSubs);
        }
        topicSubs.add(topicSubscriber);

    }

    public void aboutToCloseSubscription(ByteString topic, ByteString subscriberId) {
        removeSubscriber(topic, subscriberId);
    } 

    public void aboutToUnsubscribe(ByteString topic, ByteString subscriberId) {
        removeSubscriber(topic, subscriberId);
    }

    private synchronized void removeSubscriber(ByteString topic, ByteString subscriberId) {
        TopicSubscriber topicSub = new TopicSubscriber(topic, subscriberId);

        Channel channel = topicSub2Channel.remove(topicSub);

        if (channel != null) {
            List<TopicSubscriber> topicSubs = channel2TopicSubs.get(channel);
            if (topicSubs != null) {
                topicSubs.remove(topicSub);
            }
        }
    }

    public synchronized void checkChannelMatches(ByteString topic, ByteString subscriberId, Channel channel)
            throws PubSubException {
        Channel subscribedChannel = getChannel(topic, subscriberId);

        if (subscribedChannel == null) {
            throw new PubSubException.ClientNotSubscribedException(
                "Can't start delivery since client is not subscribed");
        }

        if (subscribedChannel != channel) {
            throw new PubSubException.TopicBusyException(
                "Can't start delivery since client is subscribed on a different channel");
        }

    }

    public synchronized Channel getChannel(ByteString topic, ByteString subscriberId) {
        return topicSub2Channel.get(new TopicSubscriber(topic, subscriberId));
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/proxy/HedwigProxy.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.proxy;

import java.io.File;
import java.lang.Thread.UncaughtExceptionHandler;
import java.net.InetSocketAddress;
import java.net.MalformedURLException;
import java.util.HashMap;
import java.util.Map;
import java.util.concurrent.Executors;
import java.util.concurrent.LinkedBlockingQueue;
import org.apache.commons.configuration.ConfigurationException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.bootstrap.ServerBootstrap;
import org.jboss.netty.channel.group.ChannelGroup;
import org.jboss.netty.channel.group.DefaultChannelGroup;
import org.jboss.netty.channel.socket.ServerSocketChannelFactory;
import org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory;
import org.jboss.netty.logging.InternalLoggerFactory;
import org.jboss.netty.logging.Log4JLoggerFactory;

import org.apache.hedwig.client.HedwigClient;
import org.apache.hedwig.protocol.PubSubProtocol.OperationType;
import org.apache.hedwig.server.common.TerminateJVMExceptionHandler;
import org.apache.hedwig.server.handlers.ChannelDisconnectListener;
import org.apache.hedwig.server.handlers.Handler;
import org.apache.hedwig.server.netty.PubSubServer;
import org.apache.hedwig.server.netty.PubSubServerPipelineFactory;
import org.apache.hedwig.server.netty.UmbrellaHandler;

public class HedwigProxy {
    static final Logger logger = LoggerFactory.getLogger(HedwigProxy.class);

    HedwigClient client;
    ServerSocketChannelFactory serverSocketChannelFactory;
    ChannelGroup allChannels;
    Map<OperationType, Handler> handlers;
    ProxyConfiguration cfg;
    ChannelTracker tracker;
    ThreadGroup tg;

    public HedwigProxy(final ProxyConfiguration cfg, final UncaughtExceptionHandler exceptionHandler) {
        this.cfg = cfg;

        tg = new ThreadGroup("hedwigproxy") {
            @Override
            public void uncaughtException(Thread t, Throwable e) {
                exceptionHandler.uncaughtException(t, e);
            }
        };
    }

    public HedwigProxy(ProxyConfiguration conf) throws InterruptedException {
        this(conf, new TerminateJVMExceptionHandler());
    }

    public void start() throws InterruptedException {
        final LinkedBlockingQueue<Boolean> queue = new LinkedBlockingQueue<Boolean>();

        new Thread(tg, new Runnable() {
            @Override
            public void run() {
                client = new HedwigClient(cfg);

                serverSocketChannelFactory = new NioServerSocketChannelFactory(Executors.newCachedThreadPool(),
                        Executors.newCachedThreadPool());
                initializeHandlers();
                initializeNetty();

                queue.offer(true);
            }
        }).start();

        queue.take();
    }

    // used for testing
    public ChannelTracker getChannelTracker() {
        return tracker;
    }

    protected void initializeHandlers() {
        handlers = new HashMap<OperationType, Handler>();
        tracker = new ChannelTracker(client.getSubscriber());

        handlers.put(OperationType.PUBLISH, new ProxyPublishHander(client.getPublisher()));
        handlers.put(OperationType.SUBSCRIBE, new ProxySubscribeHandler(client.getSubscriber(), tracker));
        handlers.put(OperationType.UNSUBSCRIBE, new ProxyUnsubscribeHandler(client.getSubscriber(), tracker));
        handlers.put(OperationType.CONSUME, new ProxyConsumeHandler(client.getSubscriber()));
        handlers.put(OperationType.STOP_DELIVERY, new ProxyStopDeliveryHandler(client.getSubscriber(), tracker));
        handlers.put(OperationType.START_DELIVERY, new ProxyStartDeliveryHandler(client.getSubscriber(), tracker));
        handlers.put(OperationType.CLOSESUBSCRIPTION,
                     new ProxyCloseSubscriptionHandler(client.getSubscriber(), tracker));
    }

    protected void initializeNetty() {
        InternalLoggerFactory.setDefaultFactory(new Log4JLoggerFactory());
        allChannels = new DefaultChannelGroup("hedwigproxy");
        ServerBootstrap bootstrap = new ServerBootstrap(serverSocketChannelFactory);
        ChannelDisconnectListener disconnectListener =
            (ChannelDisconnectListener) handlers.get(OperationType.SUBSCRIBE);
        UmbrellaHandler umbrellaHandler =
            new UmbrellaHandler(allChannels, handlers, disconnectListener, false);
        PubSubServerPipelineFactory pipeline = new PubSubServerPipelineFactory(umbrellaHandler, null, cfg
                .getMaximumMessageSize());

        bootstrap.setPipelineFactory(pipeline);
        bootstrap.setOption("child.tcpNoDelay", true);
        bootstrap.setOption("child.keepAlive", true);
        bootstrap.setOption("reuseAddress", true);

        // Bind and start to accept incoming connections.
        allChannels.add(bootstrap.bind(new InetSocketAddress(cfg.getProxyPort())));
        logger.info("Going into receive loop");
    }

    public void shutdown() {
        allChannels.close().awaitUninterruptibly();
        client.close();
        serverSocketChannelFactory.releaseExternalResources();
    }

    // the following method only exists for unit-testing purposes, should go
    // away once we make start delivery totally server-side
    public Handler getStartDeliveryHandler() {
        return handlers.get(OperationType.START_DELIVERY);
    }

    public Handler getStopDeliveryHandler() {
        return handlers.get(OperationType.STOP_DELIVERY);
    }

    /**
     * @param args
     */
    public static void main(String[] args) {

        logger.info("Attempting to start Hedwig Proxy");
        ProxyConfiguration conf = new ProxyConfiguration();
        if (args.length > 0) {
            String confFile = args[0];
            try {
                conf.loadConf(new File(confFile).toURI().toURL());
            } catch (MalformedURLException e) {
                String msg = "Could not open configuration file: " + confFile;
                PubSubServer.errorMsgAndExit(msg, e, PubSubServer.RC_INVALID_CONF_FILE);
            } catch (ConfigurationException e) {
                String msg = "Malformed configuration file: " + confFile;
                PubSubServer.errorMsgAndExit(msg, e, PubSubServer.RC_MISCONFIGURED);
            }
            logger.info("Using configuration file " + confFile);
        }
        try {
            new HedwigProxy(conf).start();
        } catch (Throwable t) {
            PubSubServer.errorMsgAndExit("Error during startup", t, PubSubServer.RC_OTHER);
        }
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/proxy/ProxyCloseSubscriptionHandler.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.proxy;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;

import com.google.protobuf.ByteString;
import org.apache.hedwig.client.api.Subscriber;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;
import org.apache.hedwig.protoextensions.PubSubResponseUtils;
import org.apache.hedwig.server.handlers.Handler;
import org.apache.hedwig.server.netty.UmbrellaHandler;
import org.apache.hedwig.util.Callback;

public class ProxyCloseSubscriptionHandler implements Handler {

    static final Logger logger = LoggerFactory.getLogger(ProxyCloseSubscriptionHandler.class);

    Subscriber subscriber;
    ChannelTracker tracker;

    public ProxyCloseSubscriptionHandler(Subscriber subscriber, ChannelTracker tracker) {
        this.subscriber = subscriber;
        this.tracker = tracker;
    }

    @Override
    public void handleRequest(final PubSubRequest request, final Channel channel) {

        if (!request.hasCloseSubscriptionRequest()) {
            UmbrellaHandler.sendErrorResponseToMalformedRequest(channel, request.getTxnId(),
                    "Missing close subscription request data");
            return;
        }

        final ByteString topic = request.getTopic();
        final ByteString subscriberId = request.getCloseSubscriptionRequest().getSubscriberId();

        subscriber.asyncCloseSubscription(topic, subscriberId, new Callback<Void>() {
            @Override
            public void operationFailed(Object ctx, PubSubException exception) {
                channel.write(PubSubResponseUtils.getResponseForException(exception, request.getTxnId()));
            }

            @Override
            public void operationFinished(Object ctx, Void result) {
                tracker.aboutToCloseSubscription(topic, subscriberId);         
                channel.write(PubSubResponseUtils.getSuccessResponse(request.getTxnId()));
            }
        }, null);
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/proxy/ProxyConfiguration.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.proxy;

import org.apache.hedwig.client.conf.ClientConfiguration;

public class ProxyConfiguration extends ClientConfiguration {

    protected final static String PROXY_PORT = "proxy_port";
    protected final static String MAX_MESSAGE_SIZE = "max_message_size";

    public int getProxyPort() {
        return conf.getInt(PROXY_PORT, 9099);
    }

    @Override
    public int getMaximumMessageSize() {
        return conf.getInt(MAX_MESSAGE_SIZE, 1258291); /* 1.2M */
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/proxy/ProxyConsumeHandler.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.proxy;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;

import org.apache.hedwig.client.api.Subscriber;
import org.apache.hedwig.exceptions.PubSubException.ClientNotSubscribedException;
import org.apache.hedwig.protocol.PubSubProtocol.ConsumeRequest;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;
import org.apache.hedwig.server.handlers.Handler;
import org.apache.hedwig.server.netty.UmbrellaHandler;

public class ProxyConsumeHandler implements Handler {

    static final Logger logger = LoggerFactory.getLogger(ProxyConsumeHandler.class);

    Subscriber subscriber;

    public ProxyConsumeHandler(Subscriber subscriber) {
        this.subscriber = subscriber;
    }

    @Override
    public void handleRequest(PubSubRequest request, Channel channel) {
        if (!request.hasConsumeRequest()) {
            UmbrellaHandler.sendErrorResponseToMalformedRequest(channel, request.getTxnId(),
                    "Missing consume request data");
            return;
        }

        ConsumeRequest consumeRequest = request.getConsumeRequest();
        try {
            subscriber.consume(request.getTopic(), consumeRequest.getSubscriberId(), consumeRequest.getMsgId());
        } catch (ClientNotSubscribedException e) {
            // ignore
            logger.warn("Unexpected consume request", e);
        }

    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/proxy/ProxyPublishHander.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.proxy;

import org.jboss.netty.channel.Channel;

import org.apache.hedwig.client.api.Publisher;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;
import org.apache.hedwig.protocol.PubSubProtocol.PublishRequest;
import org.apache.hedwig.protoextensions.PubSubResponseUtils;
import org.apache.hedwig.server.handlers.Handler;
import org.apache.hedwig.server.netty.UmbrellaHandler;
import org.apache.hedwig.util.Callback;

public class ProxyPublishHander implements Handler {
    Publisher publisher;

    public ProxyPublishHander(Publisher publisher) {
        this.publisher = publisher;
    }

    @Override
    public void handleRequest(final PubSubRequest request, final Channel channel) {
        if (!request.hasPublishRequest()) {
            UmbrellaHandler.sendErrorResponseToMalformedRequest(channel, request.getTxnId(),
                    "Missing publish request data");
            return;
        }

        final PublishRequest publishRequest = request.getPublishRequest();

        publisher.asyncPublish(request.getTopic(), publishRequest.getMsg(), new Callback<Void>() {
            @Override
            public void operationFailed(Object ctx, PubSubException exception) {
                channel.write(PubSubResponseUtils.getResponseForException(exception, request.getTxnId()));
            }

            @Override
            public void operationFinished(Object ctx, Void resultOfOperation) {
                channel.write(PubSubResponseUtils.getSuccessResponse(request.getTxnId()));
            }
        }, null);

    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/proxy/ProxyStartDeliveryHandler.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.proxy;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;
import org.jboss.netty.channel.ChannelFuture;
import org.jboss.netty.channel.ChannelFutureListener;

import com.google.protobuf.ByteString;
import org.apache.hedwig.client.api.MessageHandler;
import org.apache.hedwig.client.api.Subscriber;
import org.apache.hedwig.client.exceptions.AlreadyStartDeliveryException;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.ClientNotSubscribedException;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.ProtocolVersion;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubResponse;
import org.apache.hedwig.protocol.PubSubProtocol.StatusCode;
import org.apache.hedwig.protoextensions.PubSubResponseUtils;
import org.apache.hedwig.server.handlers.Handler;
import org.apache.hedwig.server.netty.UmbrellaHandler;
import org.apache.hedwig.util.Callback;

public class ProxyStartDeliveryHandler implements Handler {

    static final Logger logger = LoggerFactory.getLogger(ProxyStartDeliveryHandler.class);

    Subscriber subscriber;
    ChannelTracker tracker;

    public ProxyStartDeliveryHandler(Subscriber subscriber, ChannelTracker tracker) {
        this.subscriber = subscriber;
        this.tracker = tracker;
    }

    @Override
    public void handleRequest(PubSubRequest request, Channel channel) {

        if (!request.hasStartDeliveryRequest()) {
            UmbrellaHandler.sendErrorResponseToMalformedRequest(channel, request.getTxnId(),
                    "Missing start delivery request data");
            return;
        }

        final ByteString topic = request.getTopic();
        final ByteString subscriberId = request.getStartDeliveryRequest().getSubscriberId();

        synchronized (tracker) {
            // try {
            // tracker.checkChannelMatches(topic, subscriberId, channel);
            // } catch (PubSubException e) {
            // channel.write(PubSubResponseUtils.getResponseForException(e,
            // request.getTxnId()));
            // return;
            // }

            final Channel subscribedChannel = tracker.getChannel(topic, subscriberId);

            if (subscribedChannel == null) {
                channel.write(PubSubResponseUtils.getResponseForException(
                                  new PubSubException.ClientNotSubscribedException("no subscription to start delivery on"),
                                  request.getTxnId()));
                return;
            }

            MessageHandler handler = new MessageHandler() {
                @Override
                public void deliver(ByteString topic, ByteString subscriberId, Message msg,
                final Callback<Void> callback, final Object context) {

                    PubSubResponse response = PubSubResponse.newBuilder().setProtocolVersion(
                                                  ProtocolVersion.VERSION_ONE).setStatusCode(StatusCode.SUCCESS).setTxnId(0).setMessage(msg)
                                              .setTopic(topic).setSubscriberId(subscriberId).build();

                    ChannelFuture future = subscribedChannel.write(response);

                    future.addListener(new ChannelFutureListener() {
                        @Override
                        public void operationComplete(ChannelFuture future) throws Exception {
                            if (!future.isSuccess()) {
                                // ignoring this failure, because this will
                                // only happen due to channel disconnect.
                                // Channel disconnect will in turn stop
                                // delivery, and stop these errors
                                return;
                            }

                            // Tell the hedwig client, that it can send me
                            // more messages
                            callback.operationFinished(context, null);
                        }
                    });
                }
            };

            channel.write(PubSubResponseUtils.getSuccessResponse(request.getTxnId()));

            try {
                subscriber.startDelivery(topic, subscriberId, handler);
            } catch (ClientNotSubscribedException e) {
                // This should not happen, since we already checked the correct
                // channel and so on
                logger.error("Unexpected: No subscription when attempting to start delivery", e);
                throw new RuntimeException(e);
            } catch (AlreadyStartDeliveryException e) {
                logger.error("Unexpected: Already start delivery when attempting to start delivery", e);
                throw new RuntimeException(e);
            }

        }

    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/proxy/ProxyStopDeliveryHandler.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.proxy;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;

import com.google.protobuf.ByteString;
import org.apache.hedwig.client.api.Subscriber;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.ClientNotSubscribedException;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;
import org.apache.hedwig.server.handlers.Handler;
import org.apache.hedwig.server.netty.UmbrellaHandler;

public class ProxyStopDeliveryHandler implements Handler {

    static final Logger logger = LoggerFactory.getLogger(ProxyStopDeliveryHandler.class);

    Subscriber subscriber;
    ChannelTracker tracker;

    public ProxyStopDeliveryHandler(Subscriber subscriber, ChannelTracker tracker) {
        this.subscriber = subscriber;
        this.tracker = tracker;
    }

    @Override
    public void handleRequest(PubSubRequest request, Channel channel) {
        if (!request.hasStopDeliveryRequest()) {
            UmbrellaHandler.sendErrorResponseToMalformedRequest(channel, request.getTxnId(),
                    "Missing stop delivery request data");
            return;
        }

        final ByteString topic = request.getTopic();
        final ByteString subscriberId = request.getStopDeliveryRequest().getSubscriberId();

        synchronized (tracker) {
            try {
                tracker.checkChannelMatches(topic, subscriberId, channel);
            } catch (PubSubException e) {
                // intentionally ignore this error, since stop delivery doesn't
                // send back a response
                return;
            }

            try {
                subscriber.stopDelivery(topic, subscriberId);
            } catch (ClientNotSubscribedException e) {
                // This should not happen, since we already checked the correct
                // channel and so on
                logger.warn("Unexpected: No subscription when attempting to stop delivery", e);
            }
        }

    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/proxy/ProxySubscribeHandler.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.proxy;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.jboss.netty.channel.Channel;
import org.apache.hedwig.client.api.Subscriber;
import org.apache.hedwig.client.data.TopicSubscriber;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.exceptions.PubSubException.TopicBusyException;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;
import org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest;
import org.apache.hedwig.protoextensions.PubSubResponseUtils;
import org.apache.hedwig.server.handlers.ChannelDisconnectListener;
import org.apache.hedwig.server.handlers.Handler;
import org.apache.hedwig.server.netty.UmbrellaHandler;
import org.apache.hedwig.util.Callback;

public class ProxySubscribeHandler implements Handler, ChannelDisconnectListener {

    static final Logger logger = LoggerFactory.getLogger(ProxySubscribeHandler.class);

    Subscriber subscriber;
    ChannelTracker tracker;

    public ProxySubscribeHandler(Subscriber subscriber, ChannelTracker tracker) {
        this.subscriber = subscriber;
        this.tracker = tracker;
    }

    @Override
    public void channelDisconnected(Channel channel) {
        tracker.channelDisconnected(channel);
    }

    @Override
    public void handleRequest(final PubSubRequest request, final Channel channel) {
        if (!request.hasSubscribeRequest()) {
            UmbrellaHandler.sendErrorResponseToMalformedRequest(channel, request.getTxnId(),
                    "Missing subscribe request data");
            return;
        }

        SubscribeRequest subRequest = request.getSubscribeRequest();
        final TopicSubscriber topicSubscriber = new TopicSubscriber(request.getTopic(), subRequest.getSubscriberId());

        subscriber.asyncSubscribe(topicSubscriber.getTopic(), subRequest.getSubscriberId(), subRequest
        .getCreateOrAttach(), new Callback<Void>() {
            @Override
            public void operationFailed(Object ctx, PubSubException exception) {
                channel.write(PubSubResponseUtils.getResponseForException(exception, request.getTxnId()));
            }

            @Override
            public void operationFinished(Object ctx, Void resultOfOperation) {
                try {
                    tracker.subscribeSucceeded(topicSubscriber, channel);
                } catch (TopicBusyException e) {
                    channel.write(PubSubResponseUtils.getResponseForException(e, request.getTxnId()));
                    return;
                }
                channel.write(PubSubResponseUtils.getSuccessResponse(request.getTxnId()));
            }
        }, null);
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/proxy/ProxyUnsubscribeHandler.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.proxy;

import org.jboss.netty.channel.Channel;

import com.google.protobuf.ByteString;
import org.apache.hedwig.client.api.Subscriber;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.protocol.PubSubProtocol.PubSubRequest;
import org.apache.hedwig.protoextensions.PubSubResponseUtils;
import org.apache.hedwig.server.handlers.Handler;
import org.apache.hedwig.server.netty.UmbrellaHandler;
import org.apache.hedwig.util.Callback;

public class ProxyUnsubscribeHandler implements Handler {

    Subscriber subscriber;
    ChannelTracker tracker;

    public ProxyUnsubscribeHandler(Subscriber subscriber, ChannelTracker tracker) {
        this.subscriber = subscriber;
        this.tracker = tracker;
    }

    @Override
    public void handleRequest(final PubSubRequest request, final Channel channel) {
        if (!request.hasUnsubscribeRequest()) {
            UmbrellaHandler.sendErrorResponseToMalformedRequest(channel, request.getTxnId(),
                    "Missing unsubscribe request data");
            return;
        }

        ByteString topic = request.getTopic();
        ByteString subscriberId = request.getUnsubscribeRequest().getSubscriberId();

        synchronized (tracker) {

            // Even if unsubscribe fails, the hedwig client closes the channel
            // on which the subscription is being served. Hence better to tell
            // the tracker beforehand that this subscription is no longer served
            tracker.aboutToUnsubscribe(topic, subscriberId);

            subscriber.asyncUnsubscribe(topic, subscriberId, new Callback<Void>() {
                @Override
                public void operationFailed(Object ctx, PubSubException exception) {
                    channel.write(PubSubResponseUtils.getResponseForException(exception, request.getTxnId()));
                }

                @Override
                public void operationFinished(Object ctx, Void resultOfOperation) {
                    channel.write(PubSubResponseUtils.getSuccessResponse(request.getTxnId()));
                }
            }, null);
        }

    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/regions/HedwigHubClient.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.regions;

import org.jboss.netty.channel.socket.ClientSocketChannelFactory;

import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.client.netty.HedwigClientImpl;

/**
 * This is a hub specific implementation of the HedwigClient. All this does
 * though is to override the HedwigSubscriber with the hub specific child class.
 * Creating this class so we can call the protected method in the parent to set
 * the subscriber since we don't want to expose that API to the public.
 */
public class HedwigHubClient extends HedwigClientImpl {

    // Constructor when we already have a ChannelFactory instantiated.
    public HedwigHubClient(ClientConfiguration cfg, ClientSocketChannelFactory channelFactory) {
        super(cfg, channelFactory);
        // Override the type of HedwigSubscriber with the hub specific one.
        setSubscriber(new HedwigHubSubscriber(this));
    }

    // Constructor when we don't have a ChannelFactory. The super constructor
    // will create one for us.
    public HedwigHubClient(ClientConfiguration cfg) {
        super(cfg);
        // Override the type of HedwigSubscriber with the hub specific one.
        setSubscriber(new HedwigHubSubscriber(this));
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/regions/HedwigHubClientFactory.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.regions;

import org.apache.commons.configuration.ConfigurationException;
import org.jboss.netty.channel.socket.ClientSocketChannelFactory;

import org.apache.hedwig.client.conf.ClientConfiguration;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.util.HedwigSocketAddress;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class HedwigHubClientFactory {

    private final ServerConfiguration cfg;
    private final ClientConfiguration clientConfiguration;
    private final ClientSocketChannelFactory channelFactory;
    private static final Logger logger = LoggerFactory.getLogger(HedwigHubClientFactory.class);

    // Constructor that takes in a ServerConfiguration, ClientConfiguration and a ChannelFactory
    // so we can reuse it for all Clients created here.
    public HedwigHubClientFactory(ServerConfiguration cfg, ClientConfiguration clientConfiguration,
                                  ClientSocketChannelFactory channelFactory) {
        this.cfg = cfg;
        this.clientConfiguration = clientConfiguration;
        this.channelFactory = channelFactory;
    }

    /**
     * Manufacture a hub client whose default server to connect to is the input
     * HedwigSocketAddress hub.
     *
     * @param hub
     *            The hub in another region to connect to.
     */
    HedwigHubClient create(final HedwigSocketAddress hub) {
        // Create a hub specific version of the client to use
        ClientConfiguration hubClientConfiguration = new ClientConfiguration() {
            @Override
            protected HedwigSocketAddress getDefaultServerHedwigSocketAddress() {
                return hub;
            }

            @Override
            public boolean isSSLEnabled() {
                return cfg.isInterRegionSSLEnabled() || clientConfiguration.isSSLEnabled();
            }
        };
        try {
            hubClientConfiguration.addConf(this.clientConfiguration.getConf());
        } catch (ConfigurationException e) {
            String msg = "Configuration exception while loading the client configuration for the region manager.";
            logger.error(msg);
            throw new RuntimeException(msg);
        }
        return new HedwigHubClient(hubClientConfiguration, channelFactory);
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/regions/HedwigHubSubscriber.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.regions;

import com.google.protobuf.ByteString;
import org.apache.hedwig.client.exceptions.InvalidSubscriberIdException;
import org.apache.hedwig.client.netty.HedwigClientImpl;
import org.apache.hedwig.client.netty.HedwigSubscriber;
import org.apache.hedwig.exceptions.PubSubException.ClientAlreadySubscribedException;
import org.apache.hedwig.exceptions.PubSubException.ClientNotSubscribedException;
import org.apache.hedwig.exceptions.PubSubException.CouldNotConnectException;
import org.apache.hedwig.exceptions.PubSubException.ServiceDownException;
import org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionOptions;
import org.apache.hedwig.util.Callback;

/**
 * This is a hub specific child class of the HedwigSubscriber. The main thing is
 * does is wrap the public subscribe/unsubscribe methods by calling the
 * overloaded protected ones passing in a true value for the input boolean
 * parameter isHub. That will just make sure we validate the subscriberId
 * passed, ensuring it is of the right format either for a local or hub
 * subscriber.
 */
public class HedwigHubSubscriber extends HedwigSubscriber {

    public HedwigHubSubscriber(HedwigClientImpl client) {
        super(client);
    }

    @Override
    public void subscribe(ByteString topic, ByteString subscriberId, CreateOrAttach mode)
            throws CouldNotConnectException, ClientAlreadySubscribedException, ServiceDownException,
        InvalidSubscriberIdException {
        SubscriptionOptions options = SubscriptionOptions.newBuilder().setCreateOrAttach(mode).build();
        subscribe(topic, subscriberId, options);
    }

    @Override
    public void asyncSubscribe(ByteString topic, ByteString subscriberId, CreateOrAttach mode, Callback<Void> callback,
                               Object context) {
        SubscriptionOptions options = SubscriptionOptions.newBuilder().setCreateOrAttach(mode).build();
        asyncSubscribe(topic, subscriberId, options, callback, context);
    }

    @Override
    public void subscribe(ByteString topic, ByteString subscriberId, SubscriptionOptions options)
            throws CouldNotConnectException, ClientAlreadySubscribedException, ServiceDownException,
        InvalidSubscriberIdException {
        subscribe(topic, subscriberId, options, true);
    }

    @Override
    public void asyncSubscribe(ByteString topic, ByteString subscriberId,
                               SubscriptionOptions options, Callback<Void> callback, Object context) {
        asyncSubscribe(topic, subscriberId, options, callback, context, true);
    }

    @Override
    public void unsubscribe(ByteString topic, ByteString subscriberId) throws CouldNotConnectException,
        ClientNotSubscribedException, ServiceDownException, InvalidSubscriberIdException {
        unsubscribe(topic, subscriberId, true);
    }

    @Override
    public void asyncUnsubscribe(final ByteString topic, final ByteString subscriberId, final Callback<Void> callback,
                                 final Object context) {
        asyncUnsubscribe(topic, subscriberId, callback, context, true);
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/regions/RegionManager.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.regions;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Set;
import java.util.Timer;
import java.util.TimerTask;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ConcurrentMap;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.CountDownLatch;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.apache.zookeeper.ZooKeeper;

import com.google.protobuf.ByteString;
import org.apache.hedwig.client.api.MessageHandler;
import org.apache.hedwig.client.exceptions.AlreadyStartDeliveryException;
import org.apache.hedwig.client.netty.HedwigSubscriber;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.RegionSpecificSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach;
import org.apache.hedwig.protoextensions.SubscriptionStateUtils;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.common.TopicOpQueuer;
import org.apache.hedwig.server.persistence.PersistRequest;
import org.apache.hedwig.server.persistence.PersistenceManager;
import org.apache.hedwig.server.subscriptions.SubscriptionEventListener;
import org.apache.hedwig.util.Callback;
import org.apache.hedwig.util.CallbackUtils;
import org.apache.hedwig.util.HedwigSocketAddress;

public class RegionManager implements SubscriptionEventListener {

    protected static final Logger LOGGER = LoggerFactory.getLogger(RegionManager.class);

    private final ByteString mySubId;
    private final PersistenceManager pm;
    private final ArrayList<HedwigHubClient> clients = new ArrayList<HedwigHubClient>();
    private final TopicOpQueuer queue;
    private final String myRegion;
    // Timer for running a retry thread task to retry remote-subscription in asynchronous mode.
    private final Timer timer = new Timer(true);
    private final HashMap<HedwigHubClient, Set<ByteString>> retryMap =
            new HashMap<HedwigHubClient, Set<ByteString>>();
    // map used to track whether a topic is remote subscribed or not
    private final ConcurrentMap<ByteString, Boolean> topicStatuses =
            new ConcurrentHashMap<ByteString, Boolean>();

    /**
     * This is the Timer Task for retrying subscribing to remote regions
     */
    class RetrySubscribeTask extends TimerTask {

        @Override
        public void run() {
            Set<HedwigHubClient> hubClients = new HashSet<HedwigHubClient>();
            synchronized (retryMap) {
                hubClients.addAll(retryMap.keySet());
            }
            if (hubClients.isEmpty()) {
                if (LOGGER.isDebugEnabled()) {
                    LOGGER.debug("[" + myRegion + "] There is no hub client needs to retry subscriptions.");
                }
                return;
            }
            for (HedwigHubClient client : hubClients) {
                Set<ByteString> topics = null;
                synchronized (retryMap) {
                    topics = retryMap.remove(client);
                }
                if (null == topics || topics.isEmpty()) {
                    continue;
                }
                final CountDownLatch done = new CountDownLatch(1);
                Callback<Void> postCb = new Callback<Void>() {
                    @Override
                    public void operationFinished(Object ctx,
                            Void resultOfOperation) {
                        finish();
                    }
                    @Override
                    public void operationFailed(Object ctx,
                            PubSubException exception) {
                        finish();
                    }
                    void finish() {
                        done.countDown();
                    }
                };
                Callback<Void> mcb = CallbackUtils.multiCallback(topics.size(), postCb, null);
                for (ByteString topic : topics) {
                    Boolean doRemoteSubscribe = topicStatuses.get(topic);
                    // topic has been removed, no retry again
                    if (null == doRemoteSubscribe) {
                        mcb.operationFinished(null, null);
                        continue;
                    }
                    retrySubscribe(client, topic, mcb);
                }
                try {
                    done.await();
                } catch (InterruptedException e) {
                    LOGGER.warn("Exception during retrying remote subscriptions : ", e);
                }
            }
        }

    }

    public RegionManager(final PersistenceManager pm, final ServerConfiguration cfg, final ZooKeeper zk,
                         ScheduledExecutorService scheduler, HedwigHubClientFactory hubClientFactory) {
        this.pm = pm;
        mySubId = ByteString.copyFromUtf8(SubscriptionStateUtils.HUB_SUBSCRIBER_PREFIX + cfg.getMyRegion());
        queue = new TopicOpQueuer(scheduler);
        for (final String hub : cfg.getRegions()) {
            clients.add(hubClientFactory.create(new HedwigSocketAddress(hub)));
        }
        myRegion = cfg.getMyRegionByteString().toStringUtf8();
        if (cfg.getRetryRemoteSubscribeThreadRunInterval() > 0) {
            timer.schedule(new RetrySubscribeTask(), 0, cfg.getRetryRemoteSubscribeThreadRunInterval());
        }
    }

    private void putTopicInRetryMap(HedwigHubClient client, ByteString topic) {
        if (LOGGER.isDebugEnabled()) {
            LOGGER.debug("[" + myRegion + "] Put topic in retry map : " + topic.toStringUtf8());
        }
        synchronized (retryMap) {
            Set<ByteString> topics = retryMap.get(client);
            if (null == topics) {
                topics = new HashSet<ByteString>();
                retryMap.put(client, topics);
            }
            topics.add(topic);
        }
    }
    
    /**
     * Do remote subscribe for a specified topic.
     *
     * @param client
     *          Hedwig Hub Client to subscribe remote topic.
     * @param topic
     *          Topic to subscribe.
     * @param synchronous
     *          Whether to wait for the callback of subscription.
     * @param mcb
     *          Callback to trigger after subscription is done.
     * @param contex
     *          Callback context
     */
    private void doRemoteSubscribe(final HedwigHubClient client, final ByteString topic, final boolean synchronous,
                                   final Callback<Void> mcb, final Object context) {
        final HedwigSubscriber sub = client.getSubscriber();
        try {
            if (sub.hasSubscription(topic, mySubId)) {
                if (LOGGER.isDebugEnabled()) {
                    LOGGER.debug("[" + myRegion + "] cross-region subscription for topic "
                                 + topic.toStringUtf8() + " has existed before.");
                }
                mcb.operationFinished(null, null);
                return;
            }
        } catch (PubSubException e) {
            LOGGER.error("[" + myRegion + "] checking cross-region subscription for topic "
                         + topic.toStringUtf8() + " failed (this is should not happen): ", e);
            mcb.operationFailed(context, e);
            return;
        }
        sub.asyncSubscribe(topic, mySubId, CreateOrAttach.CREATE_OR_ATTACH, new Callback<Void>() {
            @Override
            public void operationFinished(Object ctx, Void resultOfOperation) {
                if (LOGGER.isDebugEnabled())
                    LOGGER.debug("[" + myRegion + "] cross-region subscription done for topic " + topic.toStringUtf8());
                try {
                    sub.startDelivery(topic, mySubId, new MessageHandler() {
                        @Override
                        public void deliver(final ByteString topic, ByteString subscriberId, Message msg,
                        final Callback<Void> callback, final Object context) {
                            // When messages are first published
                            // locally, the PublishHandler sets the
                            // source region in the Message.
                            if (msg.hasSrcRegion()) {
                                Message.newBuilder(msg).setMsgId(
                                    MessageSeqId.newBuilder(msg.getMsgId()).addRemoteComponents(
                                        RegionSpecificSeqId.newBuilder().setRegion(
                                            msg.getSrcRegion()).setSeqId(
                                            msg.getMsgId().getLocalComponent())));
                            }
                            pm.persistMessage(new PersistRequest(topic, msg, new Callback<MessageSeqId>() {
                                @Override
                                public void operationFinished(Object ctx, MessageSeqId resultOfOperation) {
                                    if (LOGGER.isDebugEnabled())
                                        LOGGER.debug("[" + myRegion + "] cross-region recv-fwd succeeded for topic "
                                                     + topic.toStringUtf8());
                                    callback.operationFinished(context, null);
                                }

                                @Override
                                public void operationFailed(Object ctx, PubSubException exception) {
                                    if (LOGGER.isDebugEnabled())
                                        LOGGER.error("[" + myRegion + "] cross-region recv-fwd failed for topic "
                                                     + topic.toStringUtf8(), exception);
                                    callback.operationFailed(context, exception);
                                }
                            }, null));
                        }
                    });
                    if (LOGGER.isDebugEnabled())
                        LOGGER.debug("[" + myRegion + "] cross-region start-delivery succeeded for topic "
                                     + topic.toStringUtf8());
                    mcb.operationFinished(ctx, null);
                } catch (PubSubException ex) {
                    if (LOGGER.isDebugEnabled())
                        LOGGER.error(
                                "[" + myRegion + "] cross-region start-delivery failed for topic " + topic.toStringUtf8(), ex);
                    mcb.operationFailed(ctx, ex);
                } catch (AlreadyStartDeliveryException ex) {
                    LOGGER.error("[" + myRegion + "] cross-region start-delivery failed for topic "
                               + topic.toStringUtf8(), ex);
                    mcb.operationFailed(ctx, new PubSubException.UnexpectedConditionException("cross-region start-delivery failed : " + ex.getMessage()));
                }
            }

            @Override
            public void operationFailed(Object ctx, PubSubException exception) {
                if (LOGGER.isDebugEnabled())
                    LOGGER.error("[" + myRegion + "] cross-region subscribe failed for topic " + topic.toStringUtf8(),
                                 exception);
                if (!synchronous) {
                    putTopicInRetryMap(client, topic);
                }
                mcb.operationFailed(ctx, exception);
            }
        }, null);
    }

    private void retrySubscribe(final HedwigHubClient client, final ByteString topic, final Callback<Void> cb) {
        if (LOGGER.isDebugEnabled()) {
            LOGGER.debug("[" + myRegion + "] Retry remote subscribe topic : " + topic.toStringUtf8());
        }
        queue.pushAndMaybeRun(topic, queue.new AsynchronousOp<Void>(topic, cb, null) {
            @Override
            public void run() {
                Boolean doRemoteSubscribe = topicStatuses.get(topic);
                // topic has been removed, no retry again
                if (null == doRemoteSubscribe) {
                    cb.operationFinished(ctx, null);
                    return;
                }
                doRemoteSubscribe(client, topic, false, cb, ctx);
            }
        });
    }

    @Override
    public void onFirstLocalSubscribe(final ByteString topic, final boolean synchronous, final Callback<Void> cb) {
        topicStatuses.put(topic, true);
        // Whenever we acquire a topic due to a (local) subscribe, subscribe on
        // it to all the other regions (currently using simple all-to-all
        // topology).
        queue.pushAndMaybeRun(topic, queue.new AsynchronousOp<Void>(topic, cb, null) {
            @Override
            public void run() {
                Callback<Void> postCb = synchronous ? cb : CallbackUtils.logger(LOGGER, 
                        "[" + myRegion + "] all cross-region subscriptions succeeded", 
                        "[" + myRegion + "] at least one cross-region subscription failed");
                final Callback<Void> mcb = CallbackUtils.multiCallback(clients.size(), postCb, ctx);
                for (final HedwigHubClient client : clients) {
                    doRemoteSubscribe(client, topic, synchronous, mcb, ctx);
                }
                if (!synchronous)
                    cb.operationFinished(null, null);
            }
        });

    }

    @Override
    public void onLastLocalUnsubscribe(final ByteString topic) {
        topicStatuses.remove(topic);
        // TODO may want to ease up on the eager unsubscribe; this is dropping
        // cross-region subscriptions ASAP
        queue.pushAndMaybeRun(topic, queue.new AsynchronousOp<Void>(topic, new Callback<Void>() {

            @Override
            public void operationFinished(Object ctx, Void result) {
                if (LOGGER.isDebugEnabled())
                    LOGGER.debug("[" + myRegion + "] cross-region unsubscribes succeeded for topic " + topic.toStringUtf8());
            }

            @Override
            public void operationFailed(Object ctx, PubSubException exception) {
                if (LOGGER.isDebugEnabled())
                    LOGGER.error("[" + myRegion + "] cross-region unsubscribes failed for topic " + topic.toStringUtf8(), exception);
            }

        }, null) {
            @Override
            public void run() {
                Callback<Void> mcb = CallbackUtils.multiCallback(clients.size(), cb, ctx);
                for (final HedwigHubClient client : clients) {
                    final HedwigSubscriber sub = client.getSubscriber();
                    try {
                        if (!sub.hasSubscription(topic, mySubId)) {
                            if (LOGGER.isDebugEnabled()) {
                                LOGGER.debug("[" + myRegion + "] cross-region subscription for topic "
                                             + topic.toStringUtf8() + " has existed before.");
                            }
                            mcb.operationFinished(null, null);
                            continue;
                        }
                    } catch (PubSubException e) {
                        LOGGER.error("[" + myRegion + "] checking cross-region subscription for topic "
                                     + topic.toStringUtf8() + " failed (this is should not happen): ", e);
                        mcb.operationFailed(ctx, e);
                        continue;
                    }
                    sub.asyncUnsubscribe(topic, mySubId, mcb, null);
                }
            }
        });
    }

    // Method to shutdown and stop all of the cross-region Hedwig clients.
    public void stop() {
        timer.cancel();
        for (HedwigHubClient client : clients) {
            client.close();
        }
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/ssl/SslServerContextFactory.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.ssl;

import java.security.KeyStore;

import javax.net.ssl.KeyManagerFactory;
import javax.net.ssl.SSLContext;

import org.apache.hedwig.client.ssl.SslContextFactory;
import org.apache.hedwig.server.common.ServerConfiguration;

public class SslServerContextFactory extends SslContextFactory {

    public SslServerContextFactory(ServerConfiguration cfg) {
        try {
            // Load our Java key store.
            KeyStore ks = KeyStore.getInstance("pkcs12");
            ks.load(cfg.getCertStream(), cfg.getPassword().toCharArray());

            // Like ssh-agent.
            KeyManagerFactory kmf = KeyManagerFactory.getInstance("SunX509");
            kmf.init(ks, cfg.getPassword().toCharArray());

            // Create the SSL context.
            ctx = SSLContext.getInstance("TLS");
            ctx.init(kmf.getKeyManagers(), getTrustManagers(), null);
        } catch (Exception ex) {
            throw new RuntimeException(ex);
        }
    }

    @Override
    protected boolean isClient() {
        return false;
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/subscriptions/AbstractSubscriptionManager.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.subscriptions;

import java.util.ArrayList;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Timer;
import java.util.TimerTask;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.atomic.AtomicInteger;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.protobuf.ByteString;

import org.apache.bookkeeper.client.BKException;
import org.apache.bookkeeper.versioning.Version;
import org.apache.bookkeeper.versioning.Versioned;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState;
import org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest.CreateOrAttach;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionEvent;
import org.apache.hedwig.protoextensions.MessageIdUtils;
import org.apache.hedwig.protoextensions.SubscriptionStateUtils;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.common.TopicOpQueuer;
import org.apache.hedwig.server.delivery.DeliveryManager;
import org.apache.hedwig.server.persistence.PersistenceManager;
import org.apache.hedwig.server.topics.TopicManager;
import org.apache.hedwig.server.topics.TopicOwnershipChangeListener;
import org.apache.hedwig.util.Callback;
import org.apache.hedwig.util.CallbackUtils;
import org.apache.hedwig.util.ConcurrencyUtils;

public abstract class AbstractSubscriptionManager implements SubscriptionManager, TopicOwnershipChangeListener {

    static Logger logger = LoggerFactory.getLogger(AbstractSubscriptionManager.class);

    protected final ServerConfiguration cfg;
    protected final ConcurrentHashMap<ByteString, Map<ByteString, InMemorySubscriptionState>> top2sub2seq =
      new ConcurrentHashMap<ByteString, Map<ByteString, InMemorySubscriptionState>>();
    protected final TopicOpQueuer queuer;
    private final ArrayList<SubscriptionEventListener> listeners = new ArrayList<SubscriptionEventListener>();

    // Handle to the DeliveryManager for the server so we can stop serving subscribers
    // when losing topics
    private final DeliveryManager dm;
    // Handle to the PersistenceManager for the server so we can pass along the
    // message consume pointers for each topic.
    private final PersistenceManager pm;
    // Timer for running a recurring thread task to get the minimum message
    // sequence ID for each topic that all subscribers for it have consumed
    // already. With that information, we can call the PersistenceManager to
    // update it on the messages that are safe to be garbage collected.
    private final Timer timer = new Timer(true);
    // In memory mapping of topics to the minimum consumed message sequence ID
    // for all subscribers to the topic.
    private final ConcurrentHashMap<ByteString, Long> topic2MinConsumedMessagesMap = new ConcurrentHashMap<ByteString, Long>();

    protected final Callback<Void> noopCallback = new NoopCallback<Void>();

    static class NoopCallback<T> implements Callback<T> {
        @Override
        public void operationFailed(Object ctx, PubSubException exception) {
            logger.warn("Exception found in AbstractSubscriptionManager : ", exception);
        }

        public void operationFinished(Object ctx, T resultOfOperation) {
        };
    }

    public AbstractSubscriptionManager(ServerConfiguration cfg, TopicManager tm,
                                       PersistenceManager pm, DeliveryManager dm,
                                       ScheduledExecutorService scheduler) {
        this.cfg = cfg;
        queuer = new TopicOpQueuer(scheduler);
        tm.addTopicOwnershipChangeListener(this);
        this.pm = pm;
        this.dm = dm;
        // Schedule the recurring MessagesConsumedTask only if a
        // PersistenceManager is passed.
        if (pm != null) {
            timer.schedule(new MessagesConsumedTask(), 0, cfg.getMessagesConsumedThreadRunInterval());
        }
    }

    /**
     * This is the Timer Task for finding out for each topic, what the minimum
     * consumed message by the subscribers are. This information is used to pass
     * along to the server's PersistenceManager so it can garbage collect older
     * topic messages that are no longer needed by the subscribers.
     */
    class MessagesConsumedTask extends TimerTask {
        /**
         * Implement the TimerTask's abstract run method.
         */
        @Override
        public void run() {
            // We are looping through relatively small in memory data structures
            // so it should be safe to run this fairly often.
            for (ByteString topic : top2sub2seq.keySet()) {
                final Map<ByteString, InMemorySubscriptionState> topicSubscriptions = top2sub2seq.get(topic);
                if (topicSubscriptions == null) {
                    continue;
                }

                long minConsumedMessage = Long.MAX_VALUE;
                boolean hasBound = true;
                // Loop through all subscribers on the current topic to find the
                // minimum persisted message id. The reason not using in-memory
                // consumed message id is LedgerRangs and InMemorySubscriptionState
                // may be inconsistent in case of a server crash.
                for (InMemorySubscriptionState curSubscription : topicSubscriptions.values()) {
                    if (curSubscription.getLastPersistedSeqId() < minConsumedMessage) {
                        minConsumedMessage = curSubscription.getLastPersistedSeqId();
                    }
                    hasBound = hasBound && curSubscription.getSubscriptionPreferences().hasMessageBound();
                }
                boolean callPersistenceManager = true;
                // Call the PersistenceManager if nobody subscribes to the topic
                // yet, or the consume pointer has moved ahead since the last
                // time, or if this is the initial subscription.
                Long minConsumedFromMap = topic2MinConsumedMessagesMap.get(topic);
                if (topicSubscriptions.isEmpty()
                    || (minConsumedFromMap != null && minConsumedFromMap < minConsumedMessage)
                    || (minConsumedFromMap == null && minConsumedMessage != 0)) {
                    topic2MinConsumedMessagesMap.put(topic, minConsumedMessage);
                    pm.consumedUntil(topic, minConsumedMessage);
                } else if (hasBound) {
                    pm.consumeToBound(topic);
                }
            }
        }
    }

    private class AcquireOp extends TopicOpQueuer.AsynchronousOp<Void> {
        public AcquireOp(ByteString topic, Callback<Void> callback, Object ctx) {
            queuer.super(topic, callback, ctx);
        }

        @Override
        public void run() {
            if (top2sub2seq.containsKey(topic)) {
                cb.operationFinished(ctx, null);
                return;
            }

            readSubscriptions(topic, new Callback<Map<ByteString, InMemorySubscriptionState>>() {
                @Override
                public void operationFailed(Object ctx, PubSubException exception) {
                    cb.operationFailed(ctx, exception);
                }

                @Override
                public void operationFinished(final Object ctx,
                final Map<ByteString, InMemorySubscriptionState> resultOfOperation) {
                    // We've just inherited a bunch of subscriber for this
                    // topic, some of which may be local. If they are, then we
                    // need to (1) notify listeners of this and (2) record the
                    // number for bookkeeping so that future
                    // subscribes/unsubscribes can efficiently notify listeners.

                    // The final "commit" (and "abort") operations.
                    final Callback<Void> cb2 = new Callback<Void>() {

                        @Override
                        public void operationFailed(Object ctx, PubSubException exception) {
                            logger.error("Subscription manager failed to acquired topic " + topic.toStringUtf8(),
                                         exception);
                            cb.operationFailed(ctx, null);
                        }

                        @Override
                        public void operationFinished(Object ctx, Void voidObj) {
                            top2sub2seq.put(topic, resultOfOperation);
                            logger.info("Subscription manager successfully acquired topic: " + topic.toStringUtf8());
                            cb.operationFinished(ctx, null);
                        }

                    };

                    // Notify listeners if necessary.
                    if (hasLocalSubscriptions(resultOfOperation)) {
                        notifyFirstLocalSubscribe(topic, false, cb2, ctx);
                    } else {
                        cb2.operationFinished(ctx, null);
                    }

                    updateMessageBound(topic);
                }

            }, ctx);

        }

    }

    private void notifyFirstLocalSubscribe(ByteString topic, boolean synchronous, final Callback<Void> cb, final Object ctx) {
        Callback<Void> mcb = CallbackUtils.multiCallback(listeners.size(), cb, ctx);
        for (SubscriptionEventListener listener : listeners) {
            listener.onFirstLocalSubscribe(topic, synchronous, mcb);
        }
    }

    /**
     * Figure out who is subscribed. Do nothing if already acquired. If there's
     * an error reading the subscribers' sequence IDs, then the topic is not
     * acquired.
     *
     * @param topic
     * @param callback
     * @param ctx
     */
    @Override
    public void acquiredTopic(final ByteString topic, final Callback<Void> callback, Object ctx) {
        queuer.pushAndMaybeRun(topic, new AcquireOp(topic, callback, ctx));
    }

    class ReleaseOp extends TopicOpQueuer.AsynchronousOp<Void> {

        public ReleaseOp(final ByteString topic, final Callback<Void> cb, Object ctx) {
            queuer.super(topic, cb, ctx);
        }

        @Override
        public void run() {
            Callback<Void> finalCb = new Callback<Void>() {
                @Override
                public void operationFinished(Object ctx, Void resultOfOperation) {
                    logger.info("Finished update subscription states when losting topic "
                              + topic.toStringUtf8());
                    finish();
                }

                @Override
                public void operationFailed(Object ctx,
                        PubSubException exception) {
                    logger.warn("Error when releasing topic : " + topic.toStringUtf8(), exception);
                    finish();
                }

                private void finish() {
                    // tell delivery manager to stop delivery for subscriptions of this topic
                    final Map<ByteString, InMemorySubscriptionState> topicSubscriptions = top2sub2seq.remove(topic);
                    // no subscriptions now, it may be removed by other release ops
                    if (null != topicSubscriptions) {
                        for (ByteString subId : topicSubscriptions.keySet()) {
                            if (logger.isDebugEnabled()) {
                                logger.debug("Stop serving subscriber (" + topic.toStringUtf8() + ", "
                                           + subId.toStringUtf8() + ") when losing topic");
                            }
                            if (null != dm) {
                                dm.stopServingSubscriber(topic, subId, SubscriptionEvent.TOPIC_MOVED,
                                                         noopCallback, null);
                            }
                        }
                    }
                    if (logger.isDebugEnabled()) {
                        logger.debug("Stop serving topic " + topic.toStringUtf8());
                    }
                    // Since we decrement local count when some of remote subscriptions failed,
                    // while we don't unsubscribe those succeed subscriptions. so we can't depends
                    // on local count, just try to notify unsubscribe.
                    notifyLastLocalUnsubscribe(topic);
                    cb.operationFinished(ctx, null);
                }
            };
            if (logger.isDebugEnabled()) {
                logger.debug("Try to update subscription states when losing topic " + topic.toStringUtf8());
            }
            updateSubscriptionStates(topic, finalCb, ctx);
        }
    }

    void updateSubscriptionStates(ByteString topic, Callback<Void> finalCb, Object ctx) {
        // Try to update subscription states of a specified topic
        Map<ByteString, InMemorySubscriptionState> states = top2sub2seq.get(topic);
        if (null == states) {
            finalCb.operationFinished(ctx, null);
        } else {
            Callback<Void> mcb = CallbackUtils.multiCallback(states.size(), finalCb, ctx);
            for (Entry<ByteString, InMemorySubscriptionState> entry : states.entrySet()) {
                InMemorySubscriptionState memState = entry.getValue();
                if (memState.setLastConsumeSeqIdImmediately()) {
                    updateSubscriptionState(topic, entry.getKey(), memState, mcb, ctx);
                } else {
                    mcb.operationFinished(ctx, null);
                }
            }
        }
    }

    /**
     * Remove the local mapping.
     */
    @Override
    public void lostTopic(ByteString topic) {
        queuer.pushAndMaybeRun(topic, new ReleaseOp(topic, noopCallback, null));
    }

    private void notifyLastLocalUnsubscribe(ByteString topic) {
        for (SubscriptionEventListener listener : listeners)
            listener.onLastLocalUnsubscribe(topic);
    }

    protected abstract void readSubscriptions(final ByteString topic,
            final Callback<Map<ByteString, InMemorySubscriptionState>> cb, final Object ctx);
    
    protected abstract void readSubscriptionData(final ByteString topic, final ByteString subscriberId, 
            final Callback<InMemorySubscriptionState> cb, Object ctx);
    
    private class SubscribeOp extends TopicOpQueuer.AsynchronousOp<SubscriptionData> {
        SubscribeRequest subRequest;
        MessageSeqId consumeSeqId;

        public SubscribeOp(ByteString topic, SubscribeRequest subRequest, MessageSeqId consumeSeqId,
                           Callback<SubscriptionData> callback, Object ctx) {
            queuer.super(topic, callback, ctx);
            this.subRequest = subRequest;
            this.consumeSeqId = consumeSeqId;
        }

        @Override
        public void run() {

            final Map<ByteString, InMemorySubscriptionState> topicSubscriptions = top2sub2seq.get(topic);
            if (topicSubscriptions == null) {
                cb.operationFailed(ctx, new PubSubException.ServerNotResponsibleForTopicException(""));
                return;
            }

            final ByteString subscriberId = subRequest.getSubscriberId();
            final InMemorySubscriptionState subscriptionState = topicSubscriptions.get(subscriberId);
            CreateOrAttach createOrAttach = subRequest.getCreateOrAttach();

            if (subscriptionState != null) {

                if (createOrAttach.equals(CreateOrAttach.CREATE)) {
                    String msg = "Topic: " + topic.toStringUtf8() + " subscriberId: " + subscriberId.toStringUtf8()
                                 + " requested creating a subscription but it is already subscribed with state: "
                                 + SubscriptionStateUtils.toString(subscriptionState.getSubscriptionState());
                    logger.error(msg);
                    cb.operationFailed(ctx, new PubSubException.ClientAlreadySubscribedException(msg));
                    return;
                }

                // Subscription existed before, check whether new preferences provided
                // if new preferences provided, merged the subscription data and updated them
                // TODO: needs ACL mechanism when changing preferences
                if (subRequest.hasPreferences() &&
                    subscriptionState.updatePreferences(subRequest.getPreferences())) {
                    updateSubscriptionPreferences(topic, subscriberId, subscriptionState, new Callback<Void>() {
                        @Override
                        public void operationFailed(Object ctx, PubSubException exception) {
                            cb.operationFailed(ctx, exception);
                        }

                        @Override
                        public void operationFinished(Object ctx, Void resultOfOperation) {
                            if (logger.isDebugEnabled()) {
                                logger.debug("Topic: " + topic.toStringUtf8() + " subscriberId: " + subscriberId.toStringUtf8()
                                             + " attaching to subscription with state: "
                                             + SubscriptionStateUtils.toString(subscriptionState.getSubscriptionState())
                                             + ", with preferences: "
                                             + SubscriptionStateUtils.toString(subscriptionState.getSubscriptionPreferences()));
                            }
                            // update message bound if necessary
                            updateMessageBound(topic);
                            cb.operationFinished(ctx, subscriptionState.toSubscriptionData());
                        }
                    }, ctx);
                    return;
                }

                // otherwise just attach
                if (logger.isDebugEnabled()) {
                    logger.debug("Topic: " + topic.toStringUtf8() + " subscriberId: " + subscriberId.toStringUtf8()
                                 + " attaching to subscription with state: "
                                 + SubscriptionStateUtils.toString(subscriptionState.getSubscriptionState())
                                 + ", with preferences: "
                                 + SubscriptionStateUtils.toString(subscriptionState.getSubscriptionPreferences()));
                }

                cb.operationFinished(ctx, subscriptionState.toSubscriptionData());
                return;
            }

            // we don't have a mapping for this subscriber
            if (createOrAttach.equals(CreateOrAttach.ATTACH)) {
                String msg = "Topic: " + topic.toStringUtf8() + " subscriberId: " + subscriberId.toStringUtf8()
                             + " requested attaching to an existing subscription but it is not subscribed";
                logger.error(msg);
                cb.operationFailed(ctx, new PubSubException.ClientNotSubscribedException(msg));
                return;
            }

            // now the hard case, this is a brand new subscription, must record
            SubscriptionState.Builder stateBuilder = SubscriptionState.newBuilder().setMsgId(consumeSeqId);

            SubscriptionPreferences.Builder preferencesBuilder;
            if (subRequest.hasPreferences()) {
                preferencesBuilder = SubscriptionPreferences.newBuilder(subRequest.getPreferences());
            } else {
                preferencesBuilder = SubscriptionPreferences.newBuilder();
            }

            // backward compability
            if (subRequest.hasMessageBound()) {
                preferencesBuilder = preferencesBuilder.setMessageBound(subRequest.getMessageBound());
            }

            SubscriptionData.Builder subDataBuilder =
                SubscriptionData.newBuilder().setState(stateBuilder).setPreferences(preferencesBuilder);
            final SubscriptionData subData = subDataBuilder.build();

            createSubscriptionData(topic, subscriberId, subData, new Callback<Version>() {
                @Override
                public void operationFailed(Object ctx, PubSubException exception) {
                    cb.operationFailed(ctx, exception);
                }

                @Override
                public void operationFinished(Object ctx, final Version version) {
                    Callback<Void> cb2 = new Callback<Void>() {
                        @Override
                        public void operationFailed(final Object ctx, final PubSubException exception) {
                            logger.error("subscription for subscriber " + subscriberId.toStringUtf8() + " to topic "
                                         + topic.toStringUtf8() + " failed due to failed listener callback", exception);
                            // should remove subscription when synchronized cross-region subscription failed
                            deleteSubscriptionData(topic, subscriberId, version, new Callback<Void>() {
                                @Override
                                public void operationFinished(Object context,
                                        Void resultOfOperation) {
                                    finish();
                                }
                                @Override
                                public void operationFailed(Object context,
                                        PubSubException ex) {
                                    logger.error("Remove subscription for subscriber " + subscriberId.toStringUtf8() + " to topic "
                                                 + topic.toStringUtf8() + " failed : ", ex);
                                    finish();
                                }
                                private void finish() {
                                    cb.operationFailed(ctx, exception);
                                }
                            }, ctx);
                        }

                        @Override
                        public void operationFinished(Object ctx, Void resultOfOperation) {
                            topicSubscriptions.put(subscriberId, new InMemorySubscriptionState(subData, version));

                            updateMessageBound(topic);

                            cb.operationFinished(ctx, subData);
                        }

                    };

                    // if this will be the first local subscription, notifyFirstLocalSubscribe
                    if (!SubscriptionStateUtils.isHubSubscriber(subRequest.getSubscriberId())
                        && !hasLocalSubscriptions(topicSubscriptions))
                        notifyFirstLocalSubscribe(topic, subRequest.getSynchronous(), cb2, ctx);
                    else
                        cb2.operationFinished(ctx, null);
                }
            }, ctx);
        }
    }

    /**
     * @return True if the given subscriberId-to-subscriberState map contains a local subscription:
     * the vast majority of subscriptions are local, so we will quickly encounter one if it exists.
     */
    private static boolean hasLocalSubscriptions(Map<ByteString, InMemorySubscriptionState> topicSubscriptions) {
      for (ByteString subId : topicSubscriptions.keySet())
        if (!SubscriptionStateUtils.isHubSubscriber(subId))
          return true;
      return false;
    }

    public void updateMessageBound(ByteString topic) {
        final Map<ByteString, InMemorySubscriptionState> topicSubscriptions = top2sub2seq.get(topic);
        if (topicSubscriptions == null) {
            return;
        }
        int maxBound = Integer.MIN_VALUE;
        for (Map.Entry<ByteString, InMemorySubscriptionState> e : topicSubscriptions.entrySet()) {
            if (!e.getValue().getSubscriptionPreferences().hasMessageBound()) {
                maxBound = Integer.MIN_VALUE;
                break;
            } else {
                maxBound = Math.max(maxBound, e.getValue().getSubscriptionPreferences().getMessageBound());
            }
        }
        if (maxBound == Integer.MIN_VALUE) {
            pm.clearMessageBound(topic);
        } else {
            pm.setMessageBound(topic, maxBound);
        }
    }

    @Override
    public void serveSubscribeRequest(ByteString topic, SubscribeRequest subRequest, MessageSeqId consumeSeqId,
                                      Callback<SubscriptionData> callback, Object ctx) {
        queuer.pushAndMaybeRun(topic, new SubscribeOp(topic, subRequest, consumeSeqId, callback, ctx));
    }

    private class ConsumeOp extends TopicOpQueuer.AsynchronousOp<Void> {
        ByteString subscriberId;
        MessageSeqId consumeSeqId;

        public ConsumeOp(ByteString topic, ByteString subscriberId, MessageSeqId consumeSeqId, Callback<Void> callback,
                         Object ctx) {
            queuer.super(topic, callback, ctx);
            this.subscriberId = subscriberId;
            this.consumeSeqId = consumeSeqId;
        }

        @Override
        public void run() {
            Map<ByteString, InMemorySubscriptionState> topicSubs = top2sub2seq.get(topic);
            if (topicSubs == null) {
                cb.operationFinished(ctx, null);
                return;
            }

            final InMemorySubscriptionState subState = topicSubs.get(subscriberId);
            if (subState == null) {
                cb.operationFinished(ctx, null);
                return;
            }

            if (subState.setLastConsumeSeqId(consumeSeqId, cfg.getConsumeInterval())) {
                updateSubscriptionState(topic, subscriberId, subState, new Callback<Void>() {
                    @Override
                    public void operationFinished(Object ctx, Void resultOfOperation) {
                        subState.setLastPersistedSeqId(consumeSeqId.getLocalComponent());
                        cb.operationFinished(ctx, resultOfOperation);
                    }

                    @Override
                    public void operationFailed(Object ctx, PubSubException exception) {
                        cb.operationFailed(ctx, exception);
                    }
                }, ctx);
            } else {
                if (logger.isDebugEnabled()) {
                    logger.debug("Only advanced consume pointer in memory, will persist later, topic: "
                                 + topic.toStringUtf8() + " subscriberId: " + subscriberId.toStringUtf8()
                                 + " persistentState: " + SubscriptionStateUtils.toString(subState.getSubscriptionState())
                                 + " in-memory consume-id: "
                                 + MessageIdUtils.msgIdToReadableString(subState.getLastConsumeSeqId()));
                }
                cb.operationFinished(ctx, null);
            }
            // tell delivery manage about the consume event
            if (null != dm) {
                dm.messageConsumed(topic, subscriberId, consumeSeqId);
            }
        }
    }

    @Override
    public void setConsumeSeqIdForSubscriber(ByteString topic, ByteString subscriberId, MessageSeqId consumeSeqId,
            Callback<Void> callback, Object ctx) {
        queuer.pushAndMaybeRun(topic, new ConsumeOp(topic, subscriberId, consumeSeqId, callback, ctx));
    }

    private class CloseSubscriptionOp extends TopicOpQueuer.AsynchronousOp<Void> {

        public CloseSubscriptionOp(ByteString topic, ByteString subscriberId,
                                   Callback<Void> callback, Object ctx) {
            queuer.super(topic, callback, ctx);
        }

        @Override
        public void run() {
            // TODO: BOOKKEEPER-412: we might need to move the loaded subscription
            //                       to reclaim memory
            // But for now we do nothing
            cb.operationFinished(ctx, null);
        }
    }

    @Override
    public void closeSubscription(ByteString topic, ByteString subscriberId,
                                  Callback<Void> callback, Object ctx) {
        queuer.pushAndMaybeRun(topic, new CloseSubscriptionOp(topic, subscriberId, callback, ctx));
    }

    private class UnsubscribeOp extends TopicOpQueuer.AsynchronousOp<Void> {
        ByteString subscriberId;

        public UnsubscribeOp(ByteString topic, ByteString subscriberId, Callback<Void> callback, Object ctx) {
            queuer.super(topic, callback, ctx);
            this.subscriberId = subscriberId;
        }

        @Override
        public void run() {
            final Map<ByteString, InMemorySubscriptionState> topicSubscriptions = top2sub2seq.get(topic);
            if (topicSubscriptions == null) {
                cb.operationFailed(ctx, new PubSubException.ServerNotResponsibleForTopicException(""));
                return;
            }

            if (!topicSubscriptions.containsKey(subscriberId)) {
                cb.operationFailed(ctx, new PubSubException.ClientNotSubscribedException(""));
                return;
            }
            
            deleteSubscriptionData(topic, subscriberId, topicSubscriptions.get(subscriberId).getVersion(),
                    new Callback<Void>() {
                @Override
                public void operationFailed(Object ctx, PubSubException exception) {
                    cb.operationFailed(ctx, exception);
                }

                @Override
                public void operationFinished(Object ctx, Void resultOfOperation) {
                    topicSubscriptions.remove(subscriberId);
                    // Notify listeners if necessary.
                    if (!SubscriptionStateUtils.isHubSubscriber(subscriberId)
                        && !hasLocalSubscriptions(topicSubscriptions))
                        notifyLastLocalUnsubscribe(topic);

                    updateMessageBound(topic);
                    cb.operationFinished(ctx, null);
                }
            }, ctx);

        }

    }

    @Override
    public void unsubscribe(ByteString topic, ByteString subscriberId, Callback<Void> callback, Object ctx) {
        queuer.pushAndMaybeRun(topic, new UnsubscribeOp(topic, subscriberId, callback, ctx));
    }

    /**
     * Not thread-safe.
     */
    @Override
    public void addListener(SubscriptionEventListener listener) {
        listeners.add(listener);
    }

    /**
     * Method to stop this class gracefully including releasing any resources
     * used and stopping all threads spawned.
     */
    public void stop() {
        timer.cancel();
        try {
            final LinkedBlockingQueue<Boolean> queue = new LinkedBlockingQueue<Boolean>();
            // update dirty subscriptions
            for (ByteString topic : top2sub2seq.keySet()) {
                Callback<Void> finalCb = new Callback<Void>() {
                    @Override
                    public void operationFinished(Object ctx, Void resultOfOperation) {
                        ConcurrencyUtils.put(queue, true);
                    }
                    @Override
                    public void operationFailed(Object ctx,
                            PubSubException exception) {
                        ConcurrencyUtils.put(queue, false);
                    }
                };
                updateSubscriptionStates(topic, finalCb, null);
                queue.take();
            }
        } catch (InterruptedException ie) {
            logger.warn("Error during updating subscription states : ", ie);
        }
    }

    private void updateSubscriptionState(final ByteString topic, final ByteString subscriberId,
                                         final InMemorySubscriptionState state,
                                         final Callback<Void> callback, Object ctx) {
        SubscriptionData subData;
        Callback<Version> cb = new Callback<Version>() {
            @Override
            public void operationFinished(Object ctx, Version version) {
                state.setVersion(version);
                callback.operationFinished(ctx, null);
            }
            @Override
            public void operationFailed(Object ctx, PubSubException exception) {
                if (exception instanceof PubSubException.BadVersionException) {
                    readSubscriptionData(topic, subscriberId, new Callback<InMemorySubscriptionState>() {
                        @Override
                        public void operationFinished(Object ctx,
                                InMemorySubscriptionState resultOfOperation) {
                            state.setVersion(resultOfOperation.getVersion());
                            updateSubscriptionState(topic, subscriberId, state, callback, ctx);
                        }
                        @Override
                        public void operationFailed(Object ctx,
                                PubSubException exception) {
                            callback.operationFailed(ctx, exception);
                        }
                    }, ctx);
                    
                    return;
                } 
                callback.operationFailed(ctx, exception);
            }
        };
        if (isPartialUpdateSupported()) {
            subData = SubscriptionData.newBuilder().setState(state.getSubscriptionState()).build();
            updateSubscriptionData(topic, subscriberId, subData, state.getVersion(), cb, ctx);
        } else {
            subData = state.toSubscriptionData();
            replaceSubscriptionData(topic, subscriberId, subData, state.getVersion(), cb, ctx);
        }
    }

    private void updateSubscriptionPreferences(final ByteString topic, final ByteString subscriberId,
                                               final InMemorySubscriptionState state,
                                               final Callback<Void> callback, Object ctx) {
        SubscriptionData subData;
        Callback<Version> cb = new Callback<Version>() {
            @Override
            public void operationFinished(Object ctx, Version version) {
                state.setVersion(version);
                callback.operationFinished(ctx, null);
            }
            @Override
            public void operationFailed(Object ctx, PubSubException exception) {
                if (exception instanceof PubSubException.BadVersionException) {
                    readSubscriptionData(topic, subscriberId, new Callback<InMemorySubscriptionState>() {
                        @Override
                        public void operationFinished(Object ctx,
                                InMemorySubscriptionState resultOfOperation) {
                            state.setVersion(resultOfOperation.getVersion());
                            updateSubscriptionPreferences(topic, subscriberId, state, callback, ctx);
                        }
                        @Override
                        public void operationFailed(Object ctx,
                                PubSubException exception) {
                            callback.operationFailed(ctx, exception);
                        }
                    }, ctx);
                    
                    return;
                } 
                callback.operationFailed(ctx, exception);
            }
        };
        if (isPartialUpdateSupported()) {
            subData = SubscriptionData.newBuilder().setPreferences(state.getSubscriptionPreferences()).build();
            updateSubscriptionData(topic, subscriberId, subData, state.getVersion(), cb, ctx);
        } else {
            subData = state.toSubscriptionData();
            replaceSubscriptionData(topic, subscriberId, subData, state.getVersion(), cb, ctx);
        }
    }

    protected abstract boolean isPartialUpdateSupported();

    protected abstract void createSubscriptionData(final ByteString topic, ByteString subscriberId,
            SubscriptionData data, Callback<Version> callback, Object ctx);

    protected abstract void updateSubscriptionData(ByteString topic, ByteString subscriberId, SubscriptionData data,
            Version version, Callback<Version> callback, Object ctx);

    protected abstract void replaceSubscriptionData(ByteString topic, ByteString subscriberId, SubscriptionData data,
            Version version, Callback<Version> callback, Object ctx);

    protected abstract void deleteSubscriptionData(ByteString topic, ByteString subscriberId, Version version, Callback<Void> callback,
            Object ctx);

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/subscriptions/AllToAllTopologyFilter.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.subscriptions;

import java.io.IOException;

import com.google.protobuf.ByteString;
import org.apache.commons.configuration.Configuration;
import org.apache.commons.configuration.ConfigurationException;
import org.apache.hedwig.filter.MessageFilterBase;
import org.apache.hedwig.filter.ServerMessageFilter;
import org.apache.hedwig.protocol.PubSubProtocol.Message;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences;
import org.apache.hedwig.protoextensions.SubscriptionStateUtils;
import org.apache.hedwig.server.common.ServerConfiguration;

public class AllToAllTopologyFilter implements ServerMessageFilter {

    ByteString subscriberRegion;
    boolean isHubSubscriber;

    @Override
    public ServerMessageFilter initialize(Configuration conf)
    throws ConfigurationException, IOException {
        String region = conf.getString(ServerConfiguration.REGION, "standalone");
        if (null == region) {
            throw new IOException("No region found to run " + getClass().getName());
        }
        subscriberRegion = ByteString.copyFromUtf8(region);
        return this;
    }

    @Override
    public void uninitialize() {
        // do nothing now
    }

    @Override
    public MessageFilterBase setSubscriptionPreferences(ByteString topic, ByteString subscriberId,
                                                        SubscriptionPreferences preferences) {
        isHubSubscriber = SubscriptionStateUtils.isHubSubscriber(subscriberId);
        return this;
    }

    @Override
    public boolean testMessage(Message message) {
        // We're using a simple all-to-all network topology, so no region
        // should ever need to forward messages to any other region.
        // Otherwise, with the current logic, messages will end up
        // ping-pong-ing back and forth between regions with subscriptions
        // to each other without termination (or in any other cyclic
        // configuration).
        if (isHubSubscriber && !message.getSrcRegion().equals(subscriberRegion)) {
            return false;
        } else {
            return true;
        }
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/subscriptions/InMemorySubscriptionManager.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.subscriptions;

import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ScheduledExecutorService;

import com.google.protobuf.ByteString;

import org.apache.bookkeeper.versioning.Version;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.delivery.DeliveryManager;
import org.apache.hedwig.server.persistence.PersistenceManager;
import org.apache.hedwig.server.topics.TopicManager;
import org.apache.hedwig.util.Callback;

public class InMemorySubscriptionManager extends AbstractSubscriptionManager {
    // Backup for top2sub2seq
    final ConcurrentHashMap<ByteString, Map<ByteString, InMemorySubscriptionState>> top2sub2seqBackup =
        new ConcurrentHashMap<ByteString, Map<ByteString, InMemorySubscriptionState>>();

    public InMemorySubscriptionManager(ServerConfiguration conf,
                                       TopicManager tm, PersistenceManager pm,
                                       DeliveryManager dm,
                                       ScheduledExecutorService scheduler) {
        super(conf, tm, pm, dm, scheduler);
    }

    @Override
    protected void createSubscriptionData(ByteString topic, ByteString subscriberId, SubscriptionData subData,
                                           Callback<Version> callback, Object ctx) {
        // nothing to do, in-memory info is already recorded by base class
        callback.operationFinished(ctx, null);
    }

    @Override
    protected void deleteSubscriptionData(ByteString topic, ByteString subscriberId, Version version, Callback<Void> callback,
                                          Object ctx) {
        // nothing to do, in-memory info is already deleted by base class
        callback.operationFinished(ctx, null);
    }

    @Override
    protected boolean isPartialUpdateSupported() {
        return false;
    }

    @Override
    protected void updateSubscriptionData(ByteString topic, ByteString subscriberId, SubscriptionData data,
                                          Version version, Callback<Version> callback, Object ctx) {
        throw new UnsupportedOperationException("Doesn't support partial update");
    }

    @Override
    protected void replaceSubscriptionData(ByteString topic, ByteString subscriberId, SubscriptionData data,
                                           Version version, Callback<Version> callback, Object ctx) {
        // nothing to do, in-memory info is already updated by base class
        callback.operationFinished(ctx, null);
    }

    @Override
    public void lostTopic(ByteString topic) {
        // Backup topic-sub2seq map for readSubscriptions
        final Map<ByteString, InMemorySubscriptionState> sub2seq = top2sub2seq.get(topic);
        if (null != sub2seq)
            top2sub2seqBackup.put(topic, sub2seq);

        if (logger.isDebugEnabled()) {
            logger.debug("InMemorySubscriptionManager is losing topic " + topic.toStringUtf8());
        }
        queuer.pushAndMaybeRun(topic, new ReleaseOp(topic, noopCallback, null));
    }

    @Override
    protected void readSubscriptions(ByteString topic,
                                     Callback<Map<ByteString, InMemorySubscriptionState>> cb, Object ctx) {
        // Since we backed up in-memory information on lostTopic, we can just return that back
        Map<ByteString, InMemorySubscriptionState> topicSubs = top2sub2seqBackup.remove(topic);

        if (topicSubs != null) {
            cb.operationFinished(ctx, topicSubs);
        } else {
            cb.operationFinished(ctx, new ConcurrentHashMap<ByteString, InMemorySubscriptionState>());
        }

    }

    @Override
    protected void readSubscriptionData(ByteString topic,
            ByteString subscriberId, Callback<InMemorySubscriptionState> cb, Object ctx) {
        // Since we backed up in-memory information on lostTopic, we can just return that back
        Map<ByteString, InMemorySubscriptionState> sub2seqBackup = top2sub2seqBackup.get(topic);
        if (sub2seqBackup == null) {
            cb.operationFinished(ctx, new InMemorySubscriptionState(
                    SubscriptionData.getDefaultInstance(), Version.NEW));
            return;
        }
        InMemorySubscriptionState subState = sub2seqBackup.remove(subscriberId);
        
        if (subState != null) {
            cb.operationFinished(ctx, subState);
        } else {
            cb.operationFinished(ctx, new InMemorySubscriptionState(
                    SubscriptionData.getDefaultInstance(), Version.NEW));
        }
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/subscriptions/InMemorySubscriptionState.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.subscriptions;

import java.util.HashMap;
import java.util.Map;

import com.google.protobuf.ByteString;

import org.apache.bookkeeper.versioning.Version;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionPreferences;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionState;
import org.apache.hedwig.protoextensions.MapUtils;
import org.apache.hedwig.protoextensions.SubscriptionStateUtils;

public class InMemorySubscriptionState {
    SubscriptionState subscriptionState;
    SubscriptionPreferences subscriptionPreferences;
    MessageSeqId lastConsumeSeqId;
    Version version;
    long lastPersistedSeqId;

    public InMemorySubscriptionState(SubscriptionData subscriptionData, Version version, MessageSeqId lastConsumeSeqId) {
        this.subscriptionState = subscriptionData.getState();
        if (subscriptionData.hasPreferences()) {
            this.subscriptionPreferences = subscriptionData.getPreferences();
        } else {
            // set initial subscription preferences
            SubscriptionPreferences.Builder prefsBuilder = SubscriptionPreferences.newBuilder();
            // progate the old system preferences from subscription state to preferences
            prefsBuilder.setMessageBound(subscriptionState.getMessageBound());
            this.subscriptionPreferences = prefsBuilder.build();

        }
        this.lastConsumeSeqId = lastConsumeSeqId;
        this.version = version;
        this.lastPersistedSeqId = subscriptionState.getMsgId().getLocalComponent();
    }

    public InMemorySubscriptionState(SubscriptionData subscriptionData, Version version) {
        this(subscriptionData, version, subscriptionData.getState().getMsgId());
    }

    public SubscriptionData toSubscriptionData() {
        SubscriptionState.Builder stateBuilder =
            SubscriptionState.newBuilder(subscriptionState).setMsgId(lastConsumeSeqId);
        return SubscriptionData.newBuilder().setState(stateBuilder)
                                            .setPreferences(subscriptionPreferences)
                                            .build();
    }

    public SubscriptionState getSubscriptionState() {
        return subscriptionState;
    }

    public SubscriptionPreferences getSubscriptionPreferences() {
        return subscriptionPreferences;
    }

    public MessageSeqId getLastConsumeSeqId() {
        return lastConsumeSeqId;
    }
     
    public Version getVersion() {
        return version;
    }
    
    public void setVersion(Version version) {
        this.version = version;
    }

    /**
     *
     * @param lastConsumeSeqId
     * @param consumeInterval
     *            The amount of laziness we want in persisting the consume
     *            pointers
     * @return true if the resulting structure needs to be persisted, false
     *         otherwise
     */
    public boolean setLastConsumeSeqId(MessageSeqId lastConsumeSeqId, int consumeInterval) {
        long interval = lastConsumeSeqId.getLocalComponent() - subscriptionState.getMsgId().getLocalComponent();
        if (interval <= 0) {
            return false;
        }

        // set consume seq id when it is larger
        this.lastConsumeSeqId = lastConsumeSeqId;
        if (interval < consumeInterval) {
            return false;
        }

        // subscription state will be updated, marked it as clean
        subscriptionState = SubscriptionState.newBuilder(subscriptionState).setMsgId(lastConsumeSeqId).build();
        return true;
    }

    /**
     * Set lastConsumeSeqId Immediately
     *
     * @return true if the resulting structure needs to be persisted, false otherwise
     */
    public boolean setLastConsumeSeqIdImmediately() {
        long interval = lastConsumeSeqId.getLocalComponent() - subscriptionState.getMsgId().getLocalComponent();
        // no need to set
        if (interval <= 0) {
            return false;
        }
        subscriptionState = SubscriptionState.newBuilder(subscriptionState).setMsgId(lastConsumeSeqId).build();
        return true;
    }

    public long getLastPersistedSeqId() {
        return lastPersistedSeqId;
    }

    public void setLastPersistedSeqId(long lastPersistedSeqId) {
        this.lastPersistedSeqId = lastPersistedSeqId;
    }

    /**
     * Update preferences.
     *
     * @return true if preferences is updated, which needs to be persisted, false otherwise.
     */
    public boolean updatePreferences(SubscriptionPreferences preferences) {
        boolean changed = false;
        SubscriptionPreferences.Builder newPreferencesBuilder = SubscriptionPreferences.newBuilder(subscriptionPreferences);
        if (preferences.hasMessageBound()) {
            if (!subscriptionPreferences.hasMessageBound() ||
                subscriptionPreferences.getMessageBound() != preferences.getMessageBound()) {
                newPreferencesBuilder.setMessageBound(preferences.getMessageBound());
                changed = true;
            }
        }
        if (preferences.hasMessageFilter()) {
            if (!subscriptionPreferences.hasMessageFilter() ||
                !subscriptionPreferences.getMessageFilter().equals(preferences.getMessageFilter())) {
                newPreferencesBuilder.setMessageFilter(preferences.getMessageFilter());
                changed = true;
            }
        }
        if (preferences.hasMessageWindowSize()) {
            if (!subscriptionPreferences.hasMessageWindowSize() ||
                subscriptionPreferences.getMessageWindowSize() !=
                preferences.getMessageWindowSize()) {
                newPreferencesBuilder.setMessageWindowSize(preferences.getMessageWindowSize());
                changed = true;
            }
        }
        if (preferences.hasOptions()) {
            Map<String, ByteString> userOptions = SubscriptionStateUtils.buildUserOptions(subscriptionPreferences);
            Map<String, ByteString> optUpdates = SubscriptionStateUtils.buildUserOptions(preferences);
            boolean optChanged = false;
            for (Map.Entry<String, ByteString> entry : optUpdates.entrySet()) {
                String key = entry.getKey();
                if (userOptions.containsKey(key)) {
                    if (null == entry.getValue()) {
                        userOptions.remove(key);
                        optChanged = true;
                    } else {
                        if (!entry.getValue().equals(userOptions.get(key))) {
                            userOptions.put(key, entry.getValue());
                            optChanged = true;
                        }
                    }
                } else {
                    userOptions.put(key, entry.getValue());
                    optChanged = true;
                }
            }
            if (optChanged) {
                changed = true;
                newPreferencesBuilder.setOptions(MapUtils.buildMapBuilder(userOptions));
            }
        }
        if (changed) {
            subscriptionPreferences = newPreferencesBuilder.build();
        }
        return changed;
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/subscriptions/MMSubscriptionManager.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.subscriptions;

import java.io.IOException;
import java.util.Map;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ScheduledExecutorService;

import com.google.protobuf.ByteString;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.delivery.DeliveryManager;
import org.apache.hedwig.server.meta.MetadataManagerFactory;
import org.apache.hedwig.server.meta.SubscriptionDataManager;
import org.apache.hedwig.server.persistence.PersistenceManager;
import org.apache.hedwig.server.topics.TopicManager;
import org.apache.hedwig.util.Callback;
import org.apache.bookkeeper.versioning.Version;
import org.apache.bookkeeper.versioning.Versioned;

/**
 * MetaManager-based subscription manager.
 */
public class MMSubscriptionManager extends AbstractSubscriptionManager {

    SubscriptionDataManager subManager;

    public MMSubscriptionManager(ServerConfiguration cfg,
                                 MetadataManagerFactory metaManagerFactory,
                                 TopicManager topicMgr, PersistenceManager pm,
                                 DeliveryManager dm,
                                 ScheduledExecutorService scheduler) {
        super(cfg, topicMgr, pm, dm, scheduler);
        this.subManager = metaManagerFactory.newSubscriptionDataManager();
    }

    @Override
    protected void readSubscriptions(final ByteString topic,
                                     final Callback<Map<ByteString, InMemorySubscriptionState>> cb, final Object ctx) {
        subManager.readSubscriptions(topic, new Callback<Map<ByteString, Versioned<SubscriptionData>>>() {
            @Override
            public void operationFailed(Object ctx, PubSubException pse) {
                cb.operationFailed(ctx, pse);
            }
            @Override
            public void operationFinished(Object ctx, Map<ByteString, Versioned<SubscriptionData>> subs) {
                Map<ByteString, InMemorySubscriptionState> results = new ConcurrentHashMap<ByteString, InMemorySubscriptionState>();
                for (Map.Entry<ByteString, Versioned<SubscriptionData>> subEntry : subs.entrySet()) {
                    Versioned<SubscriptionData> vv = subEntry.getValue();
                    results.put(subEntry.getKey(), new InMemorySubscriptionState(vv.getValue(), vv.getVersion()));
                }
                cb.operationFinished(ctx, results);
            }
        }, ctx);
    }

    @Override
    protected void readSubscriptionData(final ByteString topic, final ByteString subscriberId,
                                        final Callback<InMemorySubscriptionState> cb, final Object ctx) {
        subManager.readSubscriptionData(topic, subscriberId, new Callback<Versioned<SubscriptionData>>() {
            @Override
            public void operationFinished(Object ctx,
                    Versioned<SubscriptionData> subData) {
                if (null != subData) {
                    cb.operationFinished(ctx, 
                            new InMemorySubscriptionState(subData.getValue(), subData.getVersion()));
                } else {
                    cb.operationFinished(ctx, new InMemorySubscriptionState(
                            SubscriptionData.getDefaultInstance(), Version.NEW));
                }
            }
            @Override
            public void operationFailed(Object ctx, PubSubException exception) {
                cb.operationFailed(ctx, exception);
            }
        }, ctx);
    }

    @Override
    protected boolean isPartialUpdateSupported() {
        return subManager.isPartialUpdateSupported();
    }

    @Override
    protected void createSubscriptionData(final ByteString topic, final ByteString subscriberId,
                                          final SubscriptionData subData, final Callback<Version> callback, final Object ctx) {
        subManager.createSubscriptionData(topic, subscriberId, subData, callback, ctx);
    }

    @Override
    protected void replaceSubscriptionData(final ByteString topic, final ByteString subscriberId, final SubscriptionData subData, 
                                           final Version version, final Callback<Version> callback, final Object ctx) {
        subManager.replaceSubscriptionData(topic, subscriberId, subData, version, callback, ctx);
    }

    @Override
    protected void updateSubscriptionData(final ByteString topic, final ByteString subscriberId, final SubscriptionData subData, 
                                          final Version version, final Callback<Version> callback, final Object ctx) {
        subManager.updateSubscriptionData(topic, subscriberId, subData, version, callback, ctx);
    }

    @Override
    protected void deleteSubscriptionData(final ByteString topic, final ByteString subscriberId, Version version,
                                          final Callback<Void> callback, final Object ctx) {
        subManager.deleteSubscriptionData(topic, subscriberId, version, callback, ctx);
    }

    @Override
    public void stop() {
        super.stop();
        try {
            subManager.close();
        } catch (IOException ioe) {
            logger.warn("Exception closing subscription data manager : ", ioe);
        }
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/subscriptions/SubscriptionEventListener.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.subscriptions;

import com.google.protobuf.ByteString;
import org.apache.hedwig.util.Callback;

/**
 * For listening to events that are issued by a SubscriptionManager.
 *
 */
public interface SubscriptionEventListener {

    /**
     * Called by the subscription manager when it previously had zero local
     * subscribers for a topic and is currently accepting its first local
     * subscriber.
     *
     * @param topic
     *            The topic of interest.
     * @param synchronous
     *            Whether this request was actually initiated by a new local
     *            subscriber, or whether it was an existing subscription
     *            inherited by the hub (e.g. when recovering the state from ZK).
     * @param cb
     *            The subscription will not complete until success is called on
     *            this callback. An error on cb will result in a subscription
     *            error.
     */
    public void onFirstLocalSubscribe(ByteString topic, boolean synchronous, Callback<Void> cb);

    /**
     * Called by the SubscriptionManager when it previously had non-zero local
     * subscribers for a topic and is currently dropping its last local
     * subscriber. This is fully asynchronous so there is no callback.
     *
     * @param topic
     *            The topic of interest.
     */
    public void onLastLocalUnsubscribe(ByteString topic);

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/subscriptions/SubscriptionManager.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.subscriptions;

import com.google.protobuf.ByteString;
import org.apache.hedwig.protocol.PubSubProtocol.MessageSeqId;
import org.apache.hedwig.protocol.PubSubProtocol.SubscribeRequest;
import org.apache.hedwig.protocol.PubSubProtocol.SubscriptionData;
import org.apache.hedwig.util.Callback;

/**
 * All methods are thread-safe.
 */
public interface SubscriptionManager {

    /**
     *
     * Register a new subscription for the given subscriber for the given topic.
     * This method should reliably persist the existence of the subscription in
     * a way that it can't be lost. If the subscription already exists,
     * depending on the create or attach flag in the subscribe request, an
     * exception may be returned.
     *
     * This is an asynchronous method.
     *
     * @param topic
     * @param subRequest
     * @param consumeSeqId
     *            The seqId to start serving the subscription from, if this is a
     *            brand new subscription
     * @param callback
     *            The subscription data returned by the callback.
     * @param ctx
     */
    public void serveSubscribeRequest(ByteString topic, SubscribeRequest subRequest, MessageSeqId consumeSeqId,
                                      Callback<SubscriptionData> callback, Object ctx);

    /**
     * Set the consume position of a given subscriber on a given topic. Note
     * that this method need not persist the consume position immediately but
     * can be lazy and persist it later asynchronously, if that is more
     * efficient.
     *
     * @param topic
     * @param subscriberId
     * @param consumeSeqId
     */
    public void setConsumeSeqIdForSubscriber(ByteString topic, ByteString subscriberId, MessageSeqId consumeSeqId,
            Callback<Void> callback, Object ctx);

    /**
     * Close a particular subscription
     *
     * @param topic
     *          Topic Name
     * @param subscriberId
     *          Subscriber Id
     * @param callback
     *          Callback
     * @param ctx
     *          Callback context
     */
    public void closeSubscription(ByteString topic, ByteString subscriberId,
                                  Callback<Void> callback, Object ctx);

    /**
     * Delete a particular subscription
     *
     * @param topic
     * @param subscriberId
     */
    public void unsubscribe(ByteString topic, ByteString subscriberId, Callback<Void> callback, Object ctx);

    // Management API methods that we will fill in later
    // /**
    // * Get the ids of all subscribers for a given topic
    // *
    // * @param topic
    // * @return A list of subscriber ids that are currently subscribed to the
    // * given topic
    // */
    // public List<ByteString> getSubscriptionsForTopic(ByteString topic);
    //
    // /**
    // * Get the topics to which a given subscriber is subscribed to
    // *
    // * @param subscriberId
    // * @return A list of the topics to which the given subscriber is
    // subscribed
    // * to
    // * @throws ServiceDownException
    // * If there is an error in looking up the subscription
    // * information
    // */
    // public List<ByteString> getTopicsForSubscriber(ByteString subscriberId)
    // throws ServiceDownException;

    /**
     * Add a listener that is notified when topic-subscription pairs are added
     * or removed.
     */
    public void addListener(SubscriptionEventListener listener);

    /**
     * Stop Subscription Manager
     */
    public void stop();
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/topics/AbstractTopicManager.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.topics;

import java.net.UnknownHostException;
import java.util.ArrayList;
import java.util.Collections;
import java.util.HashSet;
import java.util.Set;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.protobuf.ByteString;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.common.TopicOpQueuer;
import org.apache.hedwig.util.Callback;
import org.apache.hedwig.util.CallbackUtils;
import org.apache.hedwig.util.HedwigSocketAddress;

public abstract class AbstractTopicManager implements TopicManager {
    /**
     * My name.
     */
    protected HedwigSocketAddress addr;

    /**
     * Topic change listeners.
     */
    protected ArrayList<TopicOwnershipChangeListener> listeners = new ArrayList<TopicOwnershipChangeListener>();

    /**
     * List of topics I believe I am responsible for.
     */
    protected Set<ByteString> topics = Collections.synchronizedSet(new HashSet<ByteString>());

    protected TopicOpQueuer queuer;
    protected ServerConfiguration cfg;
    protected ScheduledExecutorService scheduler;

    private static final Logger logger = LoggerFactory.getLogger(AbstractTopicManager.class);

    private class GetOwnerOp extends TopicOpQueuer.AsynchronousOp<HedwigSocketAddress> {
        public boolean shouldClaim;

        public GetOwnerOp(final ByteString topic, boolean shouldClaim,
                          final Callback<HedwigSocketAddress> cb, Object ctx) {
            queuer.super(topic, cb, ctx);
            this.shouldClaim = shouldClaim;
        }

        @Override
        public void run() {
            realGetOwner(topic, shouldClaim, cb, ctx);
        }
    }

    private class ReleaseOp extends TopicOpQueuer.AsynchronousOp<Void> {
        public ReleaseOp(ByteString topic, Callback<Void> cb, Object ctx) {
            queuer.super(topic, cb, ctx);
        }

        @Override
        public void run() {
            if (!topics.contains(topic)) {
                cb.operationFinished(ctx, null);
                return;
            }
            realReleaseTopic(topic, cb, ctx);
        }
    }

    public AbstractTopicManager(ServerConfiguration cfg, ScheduledExecutorService scheduler)
            throws UnknownHostException {
        this.cfg = cfg;
        this.queuer = new TopicOpQueuer(scheduler);
        this.scheduler = scheduler;
        addr = cfg.getServerAddr();
    }

    @Override
    public synchronized void addTopicOwnershipChangeListener(TopicOwnershipChangeListener listener) {
        listeners.add(listener);
    }

    protected final synchronized void notifyListenersAndAddToOwnedTopics(final ByteString topic,
            final Callback<HedwigSocketAddress> originalCallback, final Object originalContext) {

        Callback<Void> postCb = new Callback<Void>() {

            @Override
            public void operationFinished(Object ctx, Void resultOfOperation) {
                topics.add(topic);
                if (cfg.getRetentionSecs() > 0) {
                    scheduler.schedule(new Runnable() {
                        @Override
                        public void run() {
                            // Enqueue a release operation. (Recall that release
                            // doesn't "fail" even if the topic is missing.)
                            releaseTopic(topic, new Callback<Void>() {

                                @Override
                                public void operationFailed(Object ctx, PubSubException exception) {
                                    logger.error("failure that should never happen when periodically releasing topic "
                                                 + topic, exception);
                                }

                                @Override
                                public void operationFinished(Object ctx, Void resultOfOperation) {
                                    if (logger.isDebugEnabled()) {
                                        logger.debug("successful periodic release of topic "
                                            + topic.toStringUtf8());
                                    }
                                }

                            }, null);
                        }
                    }, cfg.getRetentionSecs(), TimeUnit.SECONDS);
                }
                originalCallback.operationFinished(originalContext, addr);
            }

            @Override
            public void operationFailed(final Object ctx, final PubSubException exception) {
                // TODO: optimization: we can release this as soon as we experience the first error.
                Callback<Void> cb = new Callback<Void>() {
                    public void operationFinished(Object _ctx, Void _resultOfOperation) {
                        originalCallback.operationFailed(ctx, exception);
                    }
                    public void operationFailed(Object _ctx, PubSubException _exception) {
                        logger.error("Exception releasing topic", _exception);
                        originalCallback.operationFailed(ctx, exception);
                    }
                };
                
                realReleaseTopic(topic, cb, originalContext);
            }
        };

        Callback<Void> mcb = CallbackUtils.multiCallback(listeners.size(), postCb, null);
        for (TopicOwnershipChangeListener listener : listeners) {
            listener.acquiredTopic(topic, mcb, null);
        }
    }

    private void realReleaseTopic(ByteString topic, Callback<Void> callback, Object ctx) {
        for (TopicOwnershipChangeListener listener : listeners)
            listener.lostTopic(topic);
        topics.remove(topic);
        postReleaseCleanup(topic, callback, ctx);
    }

    @Override
    public final void getOwner(ByteString topic, boolean shouldClaim,
                               Callback<HedwigSocketAddress> cb, Object ctx) {
        queuer.pushAndMaybeRun(topic, new GetOwnerOp(topic, shouldClaim, cb, ctx));
    }

    @Override
    public final void releaseTopic(ByteString topic, Callback<Void> cb, Object ctx) {
        queuer.pushAndMaybeRun(topic, new ReleaseOp(topic, cb, ctx));
    }

    /**
     * This method should "return" the owner of the topic if one has been chosen
     * already. If there is no pre-chosen owner, either this hub or some other
     * should be chosen based on the shouldClaim parameter. If its ends up
     * choosing this hub as the owner, the {@code
     * AbstractTopicManager#notifyListenersAndAddToOwnedTopics(ByteString,
     * OperationCallback, Object)} method must be called.
     *
     */
    protected abstract void realGetOwner(ByteString topic, boolean shouldClaim,
                                         Callback<HedwigSocketAddress> cb, Object ctx);

    /**
     * The method should do any cleanup necessary to indicate to other hubs that
     * this topic has been released
     */
    protected abstract void postReleaseCleanup(ByteString topic, Callback<Void> cb, Object ctx);

    @Override
    public void stop() {
        // do nothing now
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/topics/HubInfo.java,false,"package org.apache.hedwig.server.topics;

/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

import java.io.BufferedReader;
import java.io.IOException;
import java.io.StringReader;

import org.apache.hedwig.protocol.PubSubProtocol.HubInfoData;
import org.apache.hedwig.util.HedwigSocketAddress;

import com.google.protobuf.InvalidProtocolBufferException;
import com.google.protobuf.TextFormat;

/**
 * Info identifies a hub server.
 */
public class HubInfo {

    public static class InvalidHubInfoException extends Exception {
        public InvalidHubInfoException(String msg) {
            super(msg);
        }

        public InvalidHubInfoException(String msg, Throwable t) {
            super(msg, t);
        }
    }

    // address identify a hub server
    final HedwigSocketAddress addr;
    // its znode czxid
    final long czxid;
    // protobuf encoded hub info data to be serialized
    HubInfoData hubInfoData;

    public HubInfo(HedwigSocketAddress addr, long czxid) {
        this(addr, czxid, null);
    }

    protected HubInfo(HedwigSocketAddress addr, long czxid,
                      HubInfoData data) {
        this.addr = addr;
        this.czxid = czxid;
        this.hubInfoData = data;
    }

    public HedwigSocketAddress getAddress() {
        return addr;
    }

    public long getZxid() {
        return czxid;
    }

    private synchronized HubInfoData getHubInfoData() {
        if (null == hubInfoData) {
            hubInfoData = HubInfoData.newBuilder().setHostname(addr.toString())
                                     .setCzxid(czxid).build();
        }
        return hubInfoData;
    }

    @Override
    public String toString() {
        return TextFormat.printToString(getHubInfoData());
    }

    @Override
    public boolean equals(Object o) {
        if (null == o) {
            return false;
        }
        if (!(o instanceof HubInfo)) {
            return false;
        }
        HubInfo other = (HubInfo)o;
        if (null == addr) {
            if (null == other.addr) {
                return true;
            } else {
                return czxid == other.czxid;
            }
        } else {
            if (addr.equals(other.addr)) {
                return czxid == other.czxid;
            } else {
                return false;
            }
        }
    }

    @Override
    public int hashCode() {
        return addr.hashCode();
    }

    /**
     * Parse hub info from a string.
     *
     * @param hubInfoStr
     *          String representation of hub info
     * @return hub info
     * @throws InvalidHubInfoException when <code>hubInfoStr</code> is not a valid
     *         string representation of hub info.
     */
    public static HubInfo parse(String hubInfoStr) throws InvalidHubInfoException {
        // it is not protobuf encoded hub info, it might be generated by ZkTopicManager
        if (!hubInfoStr.startsWith("hostname")) {
            final HedwigSocketAddress owner;
            try {
                owner = new HedwigSocketAddress(hubInfoStr);
            } catch (Exception e) {
                throw new InvalidHubInfoException("Corrupted hub server address : " + hubInfoStr, e);
            }
            return new HubInfo(owner, 0L);
        }

        // it is a protobuf encoded hub info.
        HubInfoData hubInfoData;

        try {
            BufferedReader reader = new BufferedReader(
                new StringReader(hubInfoStr));
            HubInfoData.Builder dataBuilder = HubInfoData.newBuilder();
            TextFormat.merge(reader, dataBuilder);
            hubInfoData = dataBuilder.build();
        } catch (InvalidProtocolBufferException ipbe) {
            throw new InvalidHubInfoException("Corrupted hub info : " + hubInfoStr, ipbe);
        } catch (IOException ie) {
            throw new InvalidHubInfoException("Corrupted hub info : " + hubInfoStr, ie);
        }

        final HedwigSocketAddress owner;
        try {
            owner = new HedwigSocketAddress(hubInfoData.getHostname().trim());
        } catch (Exception e) {
            throw new InvalidHubInfoException("Corrupted hub server address : " + hubInfoData.getHostname(), e);
        }
        long ownerZxid = hubInfoData.getCzxid();
        return new HubInfo(owner, ownerZxid, hubInfoData);
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/topics/HubLoad.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */
package org.apache.hedwig.server.topics;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.StringReader;

import org.apache.hedwig.protocol.PubSubProtocol.HubLoadData;

import com.google.protobuf.InvalidProtocolBufferException;
import com.google.protobuf.TextFormat;

/**
 * This class encapsulates metrics for determining the load on a hub server.
 */
public class HubLoad implements Comparable<HubLoad> {

    public static final HubLoad MAX_LOAD = new HubLoad(Long.MAX_VALUE);
    public static final HubLoad MIN_LOAD = new HubLoad(0);

    public static class InvalidHubLoadException extends Exception {
        public InvalidHubLoadException(String msg) {
            super(msg);
        }

        public InvalidHubLoadException(String msg, Throwable t) {
            super(msg, t);
        }
    }

    // how many topics that a hub server serves
    long numTopics; 

    public HubLoad(long num) {
        this.numTopics = num;
    }

    public HubLoad(HubLoadData data) {
        this.numTopics = data.getNumTopics();
    }

    public HubLoad setNumTopics(long numTopics) {
        this.numTopics = numTopics;
        return this;
    }

    public HubLoadData toHubLoadData() {
        return HubLoadData.newBuilder().setNumTopics(numTopics).build();
    }

    @Override
    public String toString() {
        return TextFormat.printToString(toHubLoadData());
    }

    @Override
    public boolean equals(Object o) {
        if (null == o ||
            !(o instanceof HubLoad)) {
            return false;
        }
        return 0 == compareTo((HubLoad)o);
    }

    @Override
    public int compareTo(HubLoad other) {
        return numTopics > other.numTopics ?
               1 : (numTopics < other.numTopics ? -1 : 0);
    }

    @Override
    public int hashCode() {
        return (int)numTopics;
    }

    /**
     * Parse hub load from a string.
     *
     * @param hubLoadStr
     *          String representation of hub load
     * @return hub load
     * @throws InvalidHubLoadException when <code>hubLoadStr</code> is not a valid
     *         string representation of hub load.
     */
    public static HubLoad parse(String hubLoadStr) throws InvalidHubLoadException {
        // it is no protobuf encoded hub info, it might be generated by ZkTopicManager
        if (!hubLoadStr.startsWith("numTopics")) {
            try {
                long numTopics = Long.parseLong(hubLoadStr, 10);
                return new HubLoad(numTopics);
            } catch (NumberFormatException nfe) {
                throw new InvalidHubLoadException("Corrupted hub load data : " + hubLoadStr, nfe);
            }
        }
        // it it a protobuf encoded hub load data.
        HubLoadData hubLoadData;
        try {
            BufferedReader reader = new BufferedReader(
                new StringReader(hubLoadStr));
            HubLoadData.Builder dataBuilder = HubLoadData.newBuilder();
            TextFormat.merge(reader, dataBuilder);
            hubLoadData = dataBuilder.build();
        } catch (InvalidProtocolBufferException ipbe) {
            throw new InvalidHubLoadException("Corrupted hub load data : " + hubLoadStr, ipbe);
        } catch (IOException ie) {
            throw new InvalidHubLoadException("Corrupted hub load data : " + hubLoadStr, ie);
        }

        return new HubLoad(hubLoadData);
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/topics/HubServerManager.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.topics;

import java.io.IOException;

import org.apache.hedwig.util.Callback;

/**
 * The HubServerManager class manages info about hub servers.
 */
interface HubServerManager {

    static interface ManagerListener {

        /**
         * Server manager is suspended if encountering some transient errors.
         * {@link #onResume()} would be called if those errors could be fixed.
         * {@link #onShutdown()} would be called if those errors could not be fixed.
         */
        public void onSuspend();

        /**
         * Server manager is resumed after fixing some transient errors.
         */
        public void onResume();

        /**
         * Server manager had to shutdown due to unrecoverable errors.
         */
        public void onShutdown();
    }

    /**
     * Register a listener to listen events of server manager
     *
     * @param listener
     *          Server Manager Listener
     */
    public void registerListener(ManagerListener listener);

    /**
     * Register itself to the cluster.
     *
     * @param selfLoad
     *          Self load data
     * @param callback
     *          Callback when itself registered.
     * @param ctx
     *          Callback context.
     */
    public void registerSelf(HubLoad selfLoad, Callback<HubInfo> callback, Object ctx);

    /**
     * Unregister itself from the cluster.
     */
    public void unregisterSelf() throws IOException;

    /**
     * Uploading self server load data.
     *
     * It is an asynchrounous call which should not block other operations.
     * Currently we don't need to care about whether it succeed or not.
     *
     * @param selfLoad
     *          Hub server load data.
     */
    public void uploadSelfLoadData(HubLoad selfLoad);

    /**
     * Check whether a hub server is alive as the id
     *
     * @param hub
     *          Hub id to identify a lifecycle of a hub server
     * @param callback
     *          Callback of check result. If the hub server is still
     *          alive as the provided id <code>hub</code>, return true.
     *          Otherwise return false.
     * @param ctx
     *          Callback context
     */
    public void isHubAlive(HubInfo hub, Callback<Boolean> callback, Object ctx);

    /**
     * Choose a least loaded hub server from available hub servers.
     *
     * @param callback
     *          Callback to return least loaded hub server.
     * @param ctx
     *          Callback context.
     */
    public void chooseLeastLoadedHub(Callback<HubInfo> callback, Object ctx);
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/topics/MMTopicManager.java,false,"/*
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */
package org.apache.hedwig.server.topics;

import java.net.UnknownHostException;
import java.io.IOException;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.SynchronousQueue;

import org.apache.bookkeeper.versioning.Version;
import org.apache.bookkeeper.versioning.Versioned;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.server.meta.MetadataManagerFactory;
import org.apache.hedwig.server.meta.TopicOwnershipManager;
import org.apache.hedwig.util.Callback;
import org.apache.hedwig.util.ConcurrencyUtils;
import org.apache.hedwig.util.Either;
import org.apache.hedwig.util.HedwigSocketAddress;
import org.apache.zookeeper.ZooKeeper;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.google.protobuf.ByteString;
/**
 * TopicOwnershipManager based topic manager
 */
public class MMTopicManager extends AbstractTopicManager implements TopicManager {

    static Logger logger = LoggerFactory.getLogger(MMTopicManager.class);

    // topic ownership manager
    private final TopicOwnershipManager mm;
    // hub server manager
    private final HubServerManager hubManager;

    private final HubInfo myHubInfo;
    private final HubLoad myHubLoad;

    // Boolean flag indicating if we should suspend activity. If this is true,
    // all of the Ops put into the queuer will fail automatically.
    protected volatile boolean isSuspended = false;

    public MMTopicManager(ServerConfiguration cfg, ZooKeeper zk, 
                          MetadataManagerFactory mmFactory,
                          ScheduledExecutorService scheduler)
            throws UnknownHostException, PubSubException {
        super(cfg, scheduler);
        // initialize topic ownership manager
        this.mm = mmFactory.newTopicOwnershipManager();
        this.hubManager = new ZkHubServerManager(cfg, zk, addr);

        final SynchronousQueue<Either<HubInfo, PubSubException>> queue =
            new SynchronousQueue<Either<HubInfo, PubSubException>>();

        myHubLoad = new HubLoad(topics.size());
        this.hubManager.registerListener(new HubServerManager.ManagerListener() {
            @Override
            public void onSuspend() {
                isSuspended = true;
            }
            @Override
            public void onResume() {
                isSuspended = false;
            }
            @Override
            public void onShutdown() {
                // if hub server manager can't work, we had to quit
                Runtime.getRuntime().exit(1);
            }
        });
        this.hubManager.registerSelf(myHubLoad, new Callback<HubInfo>() {
            @Override
            public void operationFinished(final Object ctx, final HubInfo resultOfOperation) {
                logger.info("Successfully registered hub {} with zookeeper", resultOfOperation);
                ConcurrencyUtils.put(queue, Either.of(resultOfOperation, (PubSubException) null));
            }
            @Override
            public void operationFailed(Object ctx, PubSubException exception) {
                logger.error("Failed to register hub with zookeeper", exception);
                ConcurrencyUtils.put(queue, Either.of((HubInfo)null, exception));
            }
        }, null);
        Either<HubInfo, PubSubException> result = ConcurrencyUtils.take(queue);
        PubSubException pse = result.right();
        if (pse != null) {
            throw pse;
        }
        myHubInfo = result.left();
        logger.info("Start metadata manager based topic manager with hub id : " + myHubInfo);
    }

    @Override
    protected void realGetOwner(final ByteString topic, final boolean shouldClaim,
                                final Callback<HedwigSocketAddress> cb, final Object ctx) {
        // If operations are suspended due to a ZK client disconnect, just error
        // out this call and return.
        if (isSuspended) {
            cb.operationFailed(ctx, new PubSubException.ServiceDownException(
                                    "MMTopicManager service is temporarily suspended!"));
            return;
        }

        if (topics.contains(topic)) {
            cb.operationFinished(ctx, addr);
            return;
        }

        new MMGetOwnerOp(topic, cb, ctx).read();
    }

    /**
     * MetadataManager do topic ledger election using versioned writes.
     */
    class MMGetOwnerOp {
        ByteString topic;
        Callback<HedwigSocketAddress> cb;
        Object ctx;

        public MMGetOwnerOp(ByteString topic,
                            Callback<HedwigSocketAddress> cb, Object ctx) {
            this.topic = topic;
            this.cb = cb;
            this.ctx = ctx;
        }

        protected void read() {
            mm.readOwnerInfo(topic, new Callback<Versioned<HubInfo>>() {
                @Override
                public void operationFinished(final Object ctx, final Versioned<HubInfo> owner) {
                    if (null == owner) {
                        logger.info("{} : No owner found for topic {}",
                                    new Object[] { addr, topic.toStringUtf8() });
                        // no data found
                        choose(Version.NEW);
                        return;
                    }
                    final Version ownerVersion = owner.getVersion();
                    if (null == owner.getValue()) {
                        logger.info("{} : Invalid owner found for topic {}",
                                    new Object[] { addr, topic.toStringUtf8() });
                        choose(ownerVersion);
                        return;
                    }
                    final HubInfo hub = owner.getValue();
                    logger.info("{} : Read owner of topic {} : {}",
                                new Object[] { addr, topic.toStringUtf8(), hub });

                    logger.info("{}, {}", new Object[] { hub, myHubInfo });

                    if (hub.getAddress().equals(addr)) {
                        if (myHubInfo.getZxid() == hub.getZxid()) {
                            claimTopic(ctx);
                            return;
                        } else {
                            choose(ownerVersion);
                            return;
                        }
                    }

                    logger.info("{} : Check whether owner {} for topic {} is still alive.",
                                new Object[] { addr, hub, topic.toStringUtf8() });
                    hubManager.isHubAlive(hub, new Callback<Boolean>() {
                        @Override
                        public void operationFinished(Object ctx, Boolean isAlive) {
                            if (isAlive) {
                                cb.operationFinished(ctx, hub.getAddress());
                            } else {
                                choose(ownerVersion);
                            }
                        }
                        @Override
                        public void operationFailed(Object ctx, PubSubException pse) {
                            cb.operationFailed(ctx, pse);
                        }
                    }, ctx);
                }

                @Override
                public void operationFailed(Object ctx, PubSubException exception) {
                    cb.operationFailed(ctx, new PubSubException.ServiceDownException(
                                       "Could not read ownership for topic " + topic.toStringUtf8() + " : "
                                       + exception.getMessage()));
                }
            }, ctx);
        }

        public void claim(final Version prevOwnerVersion) {
            logger.info("{} : claiming topic {} 's owner to be {}",
                        new Object[] { addr, topic.toStringUtf8(), myHubInfo });
            mm.writeOwnerInfo(topic, myHubInfo, prevOwnerVersion, new Callback<Version>() {
                @Override
                public void operationFinished(Object ctx, Version newVersion) {
                    claimTopic(ctx);
                }
                @Override
                public void operationFailed(Object ctx, PubSubException exception) {
                    if (exception instanceof PubSubException.NoTopicOwnerInfoException ||
                        exception instanceof PubSubException.BadVersionException) {
                        // some one has updated the owner
                        logger.info("{} : Some one has claimed topic {} 's owner. Try to read the owner again.",
                                    new Object[] { addr, topic.toStringUtf8() });
                        read();
                        return;
                    }
                    cb.operationFailed(ctx, new PubSubException.ServiceDownException(
                                       "Exception when writing owner info to claim ownership of topic "
                                       + topic.toStringUtf8() + " : " + exception.getMessage()));
                }
            }, ctx);
        }

        protected void claimTopic(Object ctx) {
            logger.info("{} : claimed topic {} 's owner to be {}",
                        new Object[] { addr, topic.toStringUtf8(), myHubInfo });
            notifyListenersAndAddToOwnedTopics(topic, cb, ctx);
            hubManager.uploadSelfLoadData(myHubLoad.setNumTopics(topics.size()));
        }

        public void choose(final Version prevOwnerVersion) {
            hubManager.chooseLeastLoadedHub(new Callback<HubInfo>() {
                @Override
                public void operationFinished(Object ctx, HubInfo owner) {
                    logger.info("{} : Least loaded owner {} is chosen for topic {}",
                                new Object[] { addr, owner, topic.toStringUtf8() });
                    if (owner.getAddress().equals(addr)) {
                        claim(prevOwnerVersion);
                    } else {
                        setOwner(owner, prevOwnerVersion);
                    }
                }
                @Override
                public void operationFailed(Object ctx, PubSubException pse) {
                    logger.error("Failed to choose least loaded hub server for topic "
                               + topic.toStringUtf8() + " : ", pse);
                    cb.operationFailed(ctx, pse);
                }
            }, null);
        }

        public void setOwner(final HubInfo ownerHubInfo, final Version prevOwnerVersion) {
            logger.info("{} : setting topic {} 's owner to be {}",
                        new Object[] { addr, topic.toStringUtf8(), ownerHubInfo });
            mm.writeOwnerInfo(topic, ownerHubInfo, prevOwnerVersion, new Callback<Version>() {
                @Override
                public void operationFinished(Object ctx, Version newVersion) {
                    logger.info("{} : Set topic {} 's owner to be {}",
                                new Object[] { addr, topic.toStringUtf8(), ownerHubInfo });
                    cb.operationFinished(ctx, ownerHubInfo.getAddress());
                }
                @Override
                public void operationFailed(Object ctx, PubSubException exception) {
                    if (exception instanceof PubSubException.NoTopicOwnerInfoException ||
                        exception instanceof PubSubException.BadVersionException) {
                        // some one has updated the owner
                        logger.info("{} : Some one has set topic {} 's owner. Try to read the owner again.",
                                    new Object[] { addr, topic.toStringUtf8() });
                        read();
                        return;
                    }
                    cb.operationFailed(ctx, new PubSubException.ServiceDownException(
                                       "Exception when writing owner info to claim ownership of topic "
                                       + topic.toStringUtf8() + " : " + exception.getMessage()));
                }
            }, ctx);
        }
    }

    @Override
    protected void postReleaseCleanup(final ByteString topic,
                                      final Callback<Void> cb, final Object ctx) {
        mm.readOwnerInfo(topic, new Callback<Versioned<HubInfo>>() {
            @Override
            public void operationFinished(Object ctx, Versioned<HubInfo> owner) {
                if (null == owner) {
                    // Node has somehow disappeared from under us, live with it
                    logger.warn("No owner info found when cleaning up topic " + topic.toStringUtf8());
                    cb.operationFinished(ctx, null);
                    return;
                }
                // no valid hub info found, just return
                if (null == owner.getValue()) {
                    logger.warn("No valid owner info found when cleaning up topic " + topic.toStringUtf8());
                    cb.operationFinished(ctx, null);
                    return;
                }
                HedwigSocketAddress ownerAddr = owner.getValue().getAddress();
                if (!ownerAddr.equals(addr)) {
                    logger.warn("Wanted to clean up self owner info for topic " + topic.toStringUtf8()
                                + " but owner " + owner + " found, leaving untouched");
                    // Not our node, someone else's, leave it alone
                    cb.operationFinished(ctx, null);
                    return;
                }

                mm.deleteOwnerInfo(topic, owner.getVersion(), new Callback<Void>() {
                    @Override
                    public void operationFinished(Object ctx, Void result) {
                        cb.operationFinished(ctx, null);
                    }
                    @Override
                    public void operationFailed(Object ctx, PubSubException exception) {
                        if (exception instanceof PubSubException.NoTopicOwnerInfoException) {
                            logger.warn("Wanted to clean up self owner info for topic " + topic.toStringUtf8()
                                      + " but it has been removed.");
                            cb.operationFinished(ctx, null);
                            return;
                        }
                        logger.error("Exception when deleting self-ownership metadata for topic "
                                     + topic.toStringUtf8() + " : ", exception);
                        cb.operationFailed(ctx, new PubSubException.ServiceDownException(exception));
                    }
                }, ctx);
            }
            @Override
            public void operationFailed(Object ctx, PubSubException exception) {
                logger.error("Exception when cleaning up owner info of topic " + topic.toStringUtf8() + " : ", exception);
                cb.operationFailed(ctx, new PubSubException.ServiceDownException(exception));
            }
        }, ctx);
    }

    @Override
    public void stop() {
        // we just unregister it with zookeeper to make it unavailable from hub servers list
        try {
            hubManager.unregisterSelf();
        } catch (IOException e) {
            logger.error("Error unregistering hub server " + myHubInfo + " : ", e);
        }
        super.stop();
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/topics/TopicManager.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.topics;

import com.google.protobuf.ByteString;
import org.apache.hedwig.exceptions.PubSubException.ServiceDownException;
import org.apache.hedwig.server.persistence.PersistenceManager;
import org.apache.hedwig.util.Callback;
import org.apache.hedwig.util.HedwigSocketAddress;

/**
 * An implementor of this interface is basically responsible for ensuring that
 * there is at most a single host responsible for a given topic at a given time.
 * Also, it is desirable that on a host failure, some other hosts in the cluster
 * claim responsibilities for the topics that were at the failed host. On
 * claiming responsibility for a topic, a host should call its
 * {@link TopicOwnershipChangeListener}.
 *
 */

public interface TopicManager {
    /**
     * Get the name of the host responsible for the given topic.
     *
     * @param topic
     *            The topic whose owner to get.
     * @param cb
     *            Callback.
     * @return The name of host responsible for the given topic
     * @throws ServiceDownException
     *             If there is an error looking up the information
     */
    public void getOwner(ByteString topic, boolean shouldClaim,
                         Callback<HedwigSocketAddress> cb, Object ctx);

    /**
     * Whenever the topic manager finds out that the set of topics owned by this
     * node has changed, it can notify a set of
     * {@link TopicOwnershipChangeListener} objects. Any component of the system
     * (e.g., the {@link PersistenceManager}) can listen for such changes by
     * implementing the {@link TopicOwnershipChangeListener} interface and
     * registering themselves with the {@link TopicManager} using this method.
     * It is important that the {@link TopicOwnershipChangeListener} reacts
     * immediately to such notifications, and with no blocking (because multiple
     * listeners might need to be informed and they are all informed by the same
     * thread).
     *
     * @param listener
     */
    public void addTopicOwnershipChangeListener(TopicOwnershipChangeListener listener);

    /**
     * Give up ownership of a topic. If I don't own it, do nothing.
     *
     * @throws ServiceDownException
     *             If there is an error in claiming responsibility for the topic
     */
    public void releaseTopic(ByteString topic, Callback<Void> cb, Object ctx);

    /**
     * Stop topic manager
     */
    public void stop();

}
"
hedwig-server/src/main/java/org/apache/hedwig/server/topics/TopicOwnershipChangeListener.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.topics;

import com.google.protobuf.ByteString;
import org.apache.hedwig.util.Callback;

public interface TopicOwnershipChangeListener {

    public void acquiredTopic(ByteString topic, Callback<Void> callback, Object ctx);

    public void lostTopic(ByteString topic);
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/topics/TrivialOwnAllTopicManager.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.topics;

import java.net.UnknownHostException;
import java.util.concurrent.ScheduledExecutorService;

import com.google.protobuf.ByteString;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.util.Callback;
import org.apache.hedwig.util.HedwigSocketAddress;

public class TrivialOwnAllTopicManager extends AbstractTopicManager {

    public TrivialOwnAllTopicManager(ServerConfiguration cfg, ScheduledExecutorService scheduler)
            throws UnknownHostException {
        super(cfg, scheduler);
    }

    @Override
    protected void realGetOwner(ByteString topic, boolean shouldClaim,
                                Callback<HedwigSocketAddress> cb, Object ctx) {

        if (topics.contains(topic)) {
            cb.operationFinished(ctx, addr);
            return;
        }

        notifyListenersAndAddToOwnedTopics(topic, cb, ctx);
    }

    @Override
    protected void postReleaseCleanup(ByteString topic, Callback<Void> cb, Object ctx) {
        // No cleanup to do
        cb.operationFinished(ctx, null);
    }

    @Override
    public void stop() {
        // do nothing
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/topics/ZkHubServerManager.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.topics;

import java.io.IOException;
import java.util.List;
import java.util.Random;

import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.util.Callback;
import org.apache.hedwig.util.HedwigSocketAddress;
import org.apache.hedwig.zookeeper.SafeAsyncZKCallback;
import org.apache.hedwig.zookeeper.SafeAsyncZKCallback.StatCallback;
import org.apache.hedwig.zookeeper.ZkUtils;
import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.KeeperException.Code;
import org.apache.zookeeper.WatchedEvent;
import org.apache.zookeeper.Watcher;
import org.apache.zookeeper.ZooDefs.Ids;
import org.apache.zookeeper.ZooKeeper;
import org.apache.zookeeper.data.Stat;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * ZooKeeper based hub server manager.
 */
class ZkHubServerManager implements HubServerManager {

    static Logger logger = LoggerFactory.getLogger(ZkHubServerManager.class);

    final Random rand = new Random();

    private final ServerConfiguration conf;
    private final ZooKeeper zk;
    private final HedwigSocketAddress addr;
    private final String ephemeralNodePath;
    private final String hubNodesPath;

    // hub info structure represent itself
    protected HubInfo myHubInfo;
    protected volatile boolean isSuspended = false;
    protected ManagerListener listener = null;

    // upload hub server load to zookeeper
    StatCallback loadReportingStatCallback = new StatCallback() {
        @Override
        public void safeProcessResult(int rc, String path, Object ctx, Stat stat) {
            if (rc != KeeperException.Code.OK.intValue()) {
                logger.warn("Failed to update load information of hub {} in zk", myHubInfo);
            }
        }
    };

    /**
     * Watcher to monitor available hub server list.
     */
    class ZkHubsWatcher implements Watcher {
        @Override
        public void process(WatchedEvent event) {
            if (event.getType().equals(Watcher.Event.EventType.None)) {
                if (event.getState().equals(
                        Watcher.Event.KeeperState.Disconnected)) {
                    logger.warn("ZK client has been disconnected to the ZK server!");
                    isSuspended = true;
                    if (null != listener) {
                        listener.onSuspend();
                    }
                } else if (event.getState().equals(
                        Watcher.Event.KeeperState.SyncConnected)) {
                    if (isSuspended) {
                        logger.info("ZK client has been reconnected to the ZK server!");
                    }
                    isSuspended = false;
                    if (null != listener) {
                        listener.onResume();
                    }
                }
            }
            if (event.getState().equals(Watcher.Event.KeeperState.Expired)) {
                logger.error("ZK client connection to the ZK server has expired.!");
                if (null != listener) {
                    listener.onShutdown();
                }
            }
        }
    }

    public ZkHubServerManager(ServerConfiguration conf,
                              ZooKeeper zk,
                              HedwigSocketAddress addr) {
        this.conf = conf;
        this.zk = zk;
        this.addr = addr;

        // znode path to store all available hub servers
        this.hubNodesPath = this.conf.getZkHostsPrefix(new StringBuilder()).toString();
        // the node's ephemeral node path
        this.ephemeralNodePath = getHubZkNodePath(addr);
        // register available hub servers list watcher
        zk.register(new ZkHubsWatcher());
    }

    @Override
    public void registerListener(ManagerListener listener) {
        this.listener = listener;
    }

    /**
     * Get the znode path identifying a hub server.
     *
     * @param node
     *          Hub Server Address
     * @return znode path identifying the hub server.
     */
    private String getHubZkNodePath(HedwigSocketAddress node) {
        String nodePath = this.conf.getZkHostsPrefix(new StringBuilder())
                          .append("/").append(node).toString();
        return nodePath;
    }

    @Override
    public void registerSelf(final HubLoad selfData, final Callback<HubInfo> callback, Object ctx) {
        byte[] loadDataBytes = selfData.toString().getBytes();
        ZkUtils.createFullPathOptimistic(zk, ephemeralNodePath, loadDataBytes, Ids.OPEN_ACL_UNSAFE,
                                         CreateMode.EPHEMERAL, new SafeAsyncZKCallback.StringCallback() {
            @Override
            public void safeProcessResult(int rc, String path, Object ctx, String name) {
                if (rc == Code.OK.intValue()) {
                    // now we are here
                    zk.exists(ephemeralNodePath, false, new SafeAsyncZKCallback.StatCallback() {
                        @Override
                        public void safeProcessResult(int rc, String path, Object ctx, Stat stat) {
                            if (rc == Code.OK.intValue()) {
                                myHubInfo = new HubInfo(addr, stat.getCzxid());
                                callback.operationFinished(ctx, myHubInfo);
                                return;
                            } else {
                                callback.operationFailed(ctx,
                                    new PubSubException.ServiceDownException(
                                        "I can't state my hub node after I created it : "
                                        + ephemeralNodePath));
                                return;
                            }
                        }
                    }, ctx);
                    return;
                }
                if (rc != Code.NODEEXISTS.intValue()) {
                    KeeperException ke = ZkUtils .logErrorAndCreateZKException(
                            "Could not create ephemeral node to register hub", ephemeralNodePath, rc);
                    callback.operationFailed(ctx, new PubSubException.ServiceDownException(ke));
                    return;
                }

                logger.info("Found stale ephemeral node while registering hub with ZK, deleting it");

                // Node exists, lets try to delete it and retry
                zk.delete(ephemeralNodePath, -1, new SafeAsyncZKCallback.VoidCallback() {
                    @Override
                    public void safeProcessResult(int rc, String path, Object ctx) {
                        if (rc == Code.OK.intValue() || rc == Code.NONODE.intValue()) {
                            registerSelf(selfData, callback, ctx);
                            return;
                        }
                        KeeperException ke = ZkUtils.logErrorAndCreateZKException(
                                "Could not delete stale ephemeral node to register hub", ephemeralNodePath, rc);
                        callback.operationFailed(ctx, new PubSubException.ServiceDownException(ke));
                        return;
                    }
                }, ctx);
            }
        }, ctx);
    }

    @Override
    public void unregisterSelf() throws IOException {
        try {
            zk.delete(ephemeralNodePath, -1);
        } catch (InterruptedException e) {
            throw new IOException(e);
        } catch (KeeperException e) {
            throw new IOException(e);
        }
    }


    @Override
    public void uploadSelfLoadData(HubLoad selfLoad) {
        logger.debug("Reporting hub load of {} : {}", myHubInfo, selfLoad);
        byte[] loadDataBytes = selfLoad.toString().getBytes();
        zk.setData(ephemeralNodePath, loadDataBytes, -1,
                   loadReportingStatCallback, null);
    }

    @Override
    public void isHubAlive(final HubInfo hub, final Callback<Boolean> callback, Object ctx) {
        zk.exists(getHubZkNodePath(hub.getAddress()), false, new SafeAsyncZKCallback.StatCallback() {
            @Override
            public void safeProcessResult(int rc, String path, Object ctx, Stat stat) {
                if (rc == Code.NONODE.intValue()) {
                    callback.operationFinished(ctx, false);
                } else if (rc == Code.OK.intValue()) {
                    if (hub.getZxid() == stat.getCzxid()) {
                        callback.operationFinished(ctx, true);
                    } else {
                        callback.operationFinished(ctx, false);
                    }
                } else {
                    callback.operationFailed(ctx, new PubSubException.ServiceDownException(
                        "Failed to check whether hub server " + hub + " is alive!"));
                }
            }
        }, ctx);
    }

    @Override
    public void chooseLeastLoadedHub(final Callback<HubInfo> callback, Object ctx) {
        // Get the list of existing hosts
        zk.getChildren(hubNodesPath, false, new SafeAsyncZKCallback.ChildrenCallback() {
            @Override
            public void safeProcessResult(int rc, String path, Object ctx,
                                          List<String> children) {
                if (rc != Code.OK.intValue()) {
                    KeeperException e = ZkUtils.logErrorAndCreateZKException(
                        "Could not get list of available hubs", path, rc);
                    callback.operationFailed(ctx, new PubSubException.ServiceDownException(e));
                    return;
                }
                chooseLeastLoadedNode(children, callback, ctx);
            }
        }, ctx);
    }

    private void chooseLeastLoadedNode(final List<String> children,
                                       final Callback<HubInfo> callback, Object ctx) {
        SafeAsyncZKCallback.DataCallback dataCallback = new SafeAsyncZKCallback.DataCallback() {
            int numResponses = 0;
            HubLoad minLoad = HubLoad.MAX_LOAD;
            String leastLoaded = null;
            long leastLoadedCzxid = 0;

            @Override
            public void safeProcessResult(int rc, String path, Object ctx,
                                          byte[] data, Stat stat) {
                synchronized (this) {
                    if (rc == KeeperException.Code.OK.intValue()) {
                        try {
                            HubLoad load = HubLoad.parse(new String(data));
                            logger.debug("Found server {} with load: {}", ctx, load);
                            int compareRes = load.compareTo(minLoad);
                            if (compareRes < 0 || (compareRes == 0 && rand.nextBoolean())) {
                                minLoad = load;
                                leastLoaded = (String) ctx;
                                leastLoadedCzxid = stat.getCzxid();
                            }
                        } catch (HubLoad.InvalidHubLoadException e) {
                            logger.warn("Corrupted load information from hub : " + ctx);
                            // some corrupted data, we'll just ignore this hub
                        }
                    }
                    numResponses++;

                    if (numResponses == children.size()) {
                        if (leastLoaded == null) {
                            callback.operationFailed(ctx, 
                                new PubSubException.ServiceDownException("No hub available"));
                            return;
                        }
                        try {
                            HedwigSocketAddress owner = new HedwigSocketAddress(leastLoaded);
                            callback.operationFinished(ctx, new HubInfo(owner, leastLoadedCzxid));
                        } catch (Throwable t) {
                            callback.operationFailed(ctx,
                                new PubSubException.ServiceDownException("Least loaded hub server "
                                                                       + leastLoaded + " is invalid."));
                        }
                    }
                }
            }
        };

        for (String child : children) {
            zk.getData(conf.getZkHostsPrefix(new StringBuilder()).append("/").append(child).toString(), false,
                       dataCallback, child);
        }
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/server/topics/ZkTopicManager.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.server.topics;

import java.net.UnknownHostException;
import java.io.IOException;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.SynchronousQueue;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.KeeperException.Code;
import org.apache.zookeeper.ZooKeeper;
import org.apache.zookeeper.ZooDefs.Ids;
import org.apache.zookeeper.data.Stat;

import com.google.protobuf.ByteString;
import org.apache.hedwig.exceptions.PubSubException;
import org.apache.hedwig.server.common.ServerConfiguration;
import org.apache.hedwig.util.Callback;
import org.apache.hedwig.util.ConcurrencyUtils;
import org.apache.hedwig.util.Either;
import org.apache.hedwig.util.HedwigSocketAddress;
import org.apache.hedwig.zookeeper.SafeAsyncZKCallback;
import org.apache.hedwig.zookeeper.ZkUtils;
import org.apache.hedwig.zookeeper.SafeAsyncZKCallback.DataCallback;
import org.apache.hedwig.zookeeper.SafeAsyncZKCallback.StatCallback;

/**
 * Topics are operated on in parallel as they are independent.
 *
 */
public class ZkTopicManager extends AbstractTopicManager implements TopicManager {

    static Logger logger = LoggerFactory.getLogger(ZkTopicManager.class);

    /**
     * Persistent storage for topic metadata.
     */
    private ZooKeeper zk;

    // hub server manager
    private final HubServerManager hubManager;

    private final HubInfo myHubInfo;
    private final HubLoad myHubLoad;

    // Boolean flag indicating if we should suspend activity. If this is true,
    // all of the Ops put into the queuer will fail automatically.
    protected volatile boolean isSuspended = false;

    /**
     * Create a new topic manager. Pass in an active ZooKeeper client object.
     *
     * @param zk
     */
    public ZkTopicManager(final ZooKeeper zk, final ServerConfiguration cfg, ScheduledExecutorService scheduler)
            throws UnknownHostException, PubSubException {

        super(cfg, scheduler);
        this.zk = zk;
        this.hubManager = new ZkHubServerManager(cfg, zk, addr);

        myHubLoad = new HubLoad(topics.size());
        this.hubManager.registerListener(new HubServerManager.ManagerListener() {
            @Override
            public void onSuspend() {
                isSuspended = true;
            }
            @Override
            public void onResume() {
                isSuspended = false;
            }
            @Override
            public void onShutdown() {
                // if hub server manager can't work, we had to quit
                Runtime.getRuntime().exit(1);
            }
        });

        final SynchronousQueue<Either<HubInfo, PubSubException>> queue =
            new SynchronousQueue<Either<HubInfo, PubSubException>>();
        this.hubManager.registerSelf(myHubLoad, new Callback<HubInfo>() {
            @Override
            public void operationFinished(final Object ctx, final HubInfo resultOfOperation) {
                logger.info("Successfully registered hub {} with zookeeper", resultOfOperation);
                ConcurrencyUtils.put(queue, Either.of(resultOfOperation, (PubSubException) null));
            }
            @Override
            public void operationFailed(Object ctx, PubSubException exception) {
                logger.error("Failed to register hub with zookeeper", exception);
                ConcurrencyUtils.put(queue, Either.of((HubInfo)null, exception));
            }
        }, null);

        Either<HubInfo, PubSubException> result = ConcurrencyUtils.take(queue);
        PubSubException pse = result.right();
        if (pse != null) {
            throw pse;
        }
        myHubInfo = result.left();
    }

    String hubPath(ByteString topic) {
        return cfg.getZkTopicPath(new StringBuilder(), topic).append("/hub").toString();
    }

    @Override
    protected void realGetOwner(final ByteString topic, final boolean shouldClaim,
                                final Callback<HedwigSocketAddress> cb, final Object ctx) {
        // If operations are suspended due to a ZK client disconnect, just error
        // out this call and return.
        if (isSuspended) {
            cb.operationFailed(ctx, new PubSubException.ServiceDownException(
                                   "ZKTopicManager service is temporarily suspended!"));
            return;
        }

        if (topics.contains(topic)) {
            cb.operationFinished(ctx, addr);
            return;
        }

        new ZkGetOwnerOp(topic, shouldClaim, cb, ctx).read();
    }

    // Recursively call each other.
    class ZkGetOwnerOp {
        ByteString topic;
        boolean shouldClaim;
        Callback<HedwigSocketAddress> cb;
        Object ctx;
        String hubPath;

        public ZkGetOwnerOp(ByteString topic, boolean shouldClaim, Callback<HedwigSocketAddress> cb, Object ctx) {
            this.topic = topic;
            this.shouldClaim = shouldClaim;
            this.cb = cb;
            this.ctx = ctx;
            hubPath = hubPath(topic);

        }

        public void choose() {
            hubManager.chooseLeastLoadedHub(new Callback<HubInfo>() {
                @Override
                public void operationFinished(Object ctx, HubInfo owner) {
                    logger.info("{} : Least loaded owner {} is chosen for topic {}",
                                new Object[] { addr, owner, topic.toStringUtf8() });
                    if (owner.getAddress().equals(addr)) {
                        claim();
                    } else {
                        cb.operationFinished(ZkGetOwnerOp.this.ctx, owner.getAddress());
                    }
                }
                @Override
                public void operationFailed(Object ctx, PubSubException pse) {
                    logger.error("Failed to choose least loaded hub server for topic "
                               + topic.toStringUtf8() + " : ", pse);
                    cb.operationFailed(ctx, pse);
                }
            }, null);
        }

        public void claimOrChoose() {
            if (shouldClaim)
                claim();
            else
                choose();
        }

        public void read() {
            zk.getData(hubPath, false, new SafeAsyncZKCallback.DataCallback() {
                @Override
                public void safeProcessResult(int rc, String path, Object ctx, byte[] data, Stat stat) {

                    if (rc == Code.NONODE.intValue()) {
                        claimOrChoose();
                        return;
                    }

                    if (rc != Code.OK.intValue()) {
                        KeeperException e = ZkUtils.logErrorAndCreateZKException("Could not read ownership for topic: "
                                            + topic.toStringUtf8(), path, rc);
                        cb.operationFailed(ctx, new PubSubException.ServiceDownException(e));
                        return;
                    }

                    // successfully did a read
                    try {
                        HubInfo ownerHubInfo = HubInfo.parse(new String(data));
                        HedwigSocketAddress owner = ownerHubInfo.getAddress();
                        if (!owner.equals(addr)) {
                            if (logger.isDebugEnabled()) {
                                logger.debug("topic: " + topic.toStringUtf8() + " belongs to someone else: " + owner);
                            }
                            cb.operationFinished(ctx, owner);
                            return;
                        }
                        logger.info("Discovered stale self-node for topic: " + topic.toStringUtf8() + ", will delete it");
                    } catch (HubInfo.InvalidHubInfoException ihie) {
                        logger.info("Discovered invalid hub info for topic: " + topic.toStringUtf8() + ", will delete it : ", ihie);
                    }

                    // we must have previously failed and left a
                    // residual ephemeral node here, so we must
                    // delete it (clean it up) and then
                    // re-create/re-acquire the topic.
                    zk.delete(hubPath, stat.getVersion(), new VoidCallback() {
                        @Override
                        public void processResult(int rc, String path, Object ctx) {
                            if (Code.OK.intValue() == rc || Code.NONODE.intValue() == rc) {
                                claimOrChoose();
                            } else {
                                KeeperException e = ZkUtils.logErrorAndCreateZKException(
                                                        "Could not delete self node for topic: " + topic.toStringUtf8(), path, rc);
                                cb.operationFailed(ctx, new PubSubException.ServiceDownException(e));
                            }
                        }
                    }, ctx);
                }
            }, ctx);
        }

        public void claim() {
            if (logger.isDebugEnabled()) {
                logger.debug("claiming topic: " + topic.toStringUtf8());
            }

            ZkUtils.createFullPathOptimistic(zk, hubPath, myHubInfo.toString().getBytes(), Ids.OPEN_ACL_UNSAFE,
            CreateMode.EPHEMERAL, new SafeAsyncZKCallback.StringCallback() {

                @Override
                public void safeProcessResult(int rc, String path, Object ctx, String name) {
                    if (rc == Code.OK.intValue()) {
                        if (logger.isDebugEnabled()) {
                            logger.debug("claimed topic: " + topic.toStringUtf8());
                        }
                        notifyListenersAndAddToOwnedTopics(topic, cb, ctx);
                        hubManager.uploadSelfLoadData(myHubLoad.setNumTopics(topics.size()));
                    } else if (rc == Code.NODEEXISTS.intValue()) {
                        read();
                    } else {
                        KeeperException e = ZkUtils.logErrorAndCreateZKException(
                                                "Failed to create ephemeral node to claim ownership of topic: "
                                                + topic.toStringUtf8(), path, rc);
                        cb.operationFailed(ctx, new PubSubException.ServiceDownException(e));
                    }
                }
            }, ctx);
        }

    }

    @Override
    protected void postReleaseCleanup(final ByteString topic, final Callback<Void> cb, Object ctx) {

        zk.getData(hubPath(topic), false, new SafeAsyncZKCallback.DataCallback() {
            @Override
            public void safeProcessResult(int rc, String path, Object ctx, byte[] data, Stat stat) {
                if (rc == Code.NONODE.intValue()) {
                    // Node has somehow disappeared from under us, live with it
                    // since its a transient node
                    logger.warn("While deleting self-node for topic: " + topic.toStringUtf8() + ", node not found");
                    cb.operationFinished(ctx, null);
                    return;
                }

                if (rc != Code.OK.intValue()) {
                    KeeperException e = ZkUtils.logErrorAndCreateZKException(
                                            "Failed to delete self-ownership node for topic: " + topic.toStringUtf8(), path, rc);
                    cb.operationFailed(ctx, new PubSubException.ServiceDownException(e));
                    return;
                }

                String hubInfoStr = new String(data);
                try {
                    HubInfo ownerHubInfo = HubInfo.parse(hubInfoStr);
                    HedwigSocketAddress owner = ownerHubInfo.getAddress();
                    if (!owner.equals(addr)) {
                        logger.warn("Wanted to delete self-node for topic: " + topic.toStringUtf8() + " but node for "
                                    + owner + " found, leaving untouched");
                        // Not our node, someone else's, leave it alone
                        cb.operationFinished(ctx, null);
                        return;
                    }
                } catch (HubInfo.InvalidHubInfoException ihie) {
                    logger.info("Invalid hub info " + hubInfoStr + " found when release topic "
                              + topic.toStringUtf8() + ". Leaving untouched until next acquire action.");
                    cb.operationFinished(ctx, null);
                    return;
                }

                zk.delete(path, stat.getVersion(), new SafeAsyncZKCallback.VoidCallback() {
                    @Override
                    public void safeProcessResult(int rc, String path, Object ctx) {
                        if (rc != Code.OK.intValue() && rc != Code.NONODE.intValue()) {
                            KeeperException e = ZkUtils
                                                .logErrorAndCreateZKException("Failed to delete self-ownership node for topic: "
                                                        + topic.toStringUtf8(), path, rc);
                            cb.operationFailed(ctx, new PubSubException.ServiceDownException(e));
                            return;
                        }

                        cb.operationFinished(ctx, null);
                    }
                }, ctx);
            }
        }, ctx);
    }

    @Override
    public void stop() {
        // we just unregister it with zookeeper to make it unavailable from hub servers list
        try {
            hubManager.unregisterSelf();
        } catch (IOException e) {
            logger.error("Error unregistering hub server :", e);
        }
        super.stop();
    }

}
"
hedwig-server/src/main/java/org/apache/hedwig/zookeeper/SafeAsynBKCallback.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.zookeeper;

import java.util.Enumeration;

import org.apache.bookkeeper.client.AsyncCallback;
import org.apache.bookkeeper.client.LedgerEntry;
import org.apache.bookkeeper.client.LedgerHandle;


public class SafeAsynBKCallback extends SafeAsyncCallback {

    public static abstract class OpenCallback implements AsyncCallback.OpenCallback {
        @Override
        public void openComplete(int rc, LedgerHandle ledgerHandle, Object ctx) {
            try {
                safeOpenComplete(rc, ledgerHandle, ctx);
            } catch(Throwable t) {
                invokeUncaughtExceptionHandler(t);
            }
        }

        public abstract void safeOpenComplete(int rc, LedgerHandle ledgerHandle, Object ctx);

    }

    public static abstract class CloseCallback implements AsyncCallback.CloseCallback {
        @Override
        public void closeComplete(int rc, LedgerHandle ledgerHandle, Object ctx) {
            try {
                safeCloseComplete(rc, ledgerHandle, ctx);
            } catch(Throwable t) {
                invokeUncaughtExceptionHandler(t);
            }
        }

        public abstract void safeCloseComplete(int rc, LedgerHandle ledgerHandle, Object ctx) ;
    }

    public static abstract class ReadCallback implements AsyncCallback.ReadCallback {

        @Override
        public void readComplete(int rc, LedgerHandle lh, Enumeration<LedgerEntry> seq, Object ctx) {
            try {
                safeReadComplete(rc, lh, seq, ctx);
            } catch(Throwable t) {
                invokeUncaughtExceptionHandler(t);
            }

        }

        public abstract void safeReadComplete(int rc, LedgerHandle lh, Enumeration<LedgerEntry> seq, Object ctx);
    }

    public static abstract class CreateCallback implements AsyncCallback.CreateCallback {

        @Override
        public void createComplete(int rc, LedgerHandle lh, Object ctx) {
            try {
                safeCreateComplete(rc, lh, ctx);
            } catch(Throwable t) {
                invokeUncaughtExceptionHandler(t);
            }

        }

        public abstract void safeCreateComplete(int rc, LedgerHandle lh, Object ctx);


    }

    public static abstract class AddCallback implements AsyncCallback.AddCallback {

        @Override
        public void addComplete(int rc, LedgerHandle lh, long entryId, Object ctx) {
            try {
                safeAddComplete(rc, lh, entryId, ctx);
            } catch(Throwable t) {
                invokeUncaughtExceptionHandler(t);
            }
        }

        public abstract void safeAddComplete(int rc, LedgerHandle lh, long entryId, Object ctx);

    }

}

"
hedwig-server/src/main/java/org/apache/hedwig/zookeeper/SafeAsyncCallback.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.zookeeper;

import java.lang.Thread.UncaughtExceptionHandler;

import org.apache.hedwig.server.common.TerminateJVMExceptionHandler;

public class SafeAsyncCallback {
    static UncaughtExceptionHandler uncaughtExceptionHandler = new TerminateJVMExceptionHandler();

    public static void setUncaughtExceptionHandler(UncaughtExceptionHandler uncaughtExceptionHandler) {
        SafeAsyncCallback.uncaughtExceptionHandler = uncaughtExceptionHandler;
    }

    static void invokeUncaughtExceptionHandler(Throwable t) {
        Thread thread = Thread.currentThread();
        uncaughtExceptionHandler.uncaughtException(thread, t);
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/zookeeper/SafeAsyncZKCallback.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.zookeeper;

import java.util.List;

import org.apache.zookeeper.AsyncCallback;
import org.apache.zookeeper.data.ACL;
import org.apache.zookeeper.data.Stat;

public class SafeAsyncZKCallback extends SafeAsyncCallback {
    public static abstract class StatCallback implements AsyncCallback.StatCallback {
        public void processResult(int rc, String path, Object ctx, Stat stat) {
            try {
                safeProcessResult(rc, path, ctx, stat);
            } catch (Throwable t) {
                invokeUncaughtExceptionHandler(t);
            }
        }

        public abstract void safeProcessResult(int rc, String path, Object ctx, Stat stat);
    }

    public static abstract class DataCallback implements AsyncCallback.DataCallback {
        public void processResult(int rc, String path, Object ctx, byte data[], Stat stat) {
            try {
                safeProcessResult(rc, path, ctx, data, stat);
            } catch (Throwable t) {
                invokeUncaughtExceptionHandler(t);
            }
        }

        public abstract void safeProcessResult(int rc, String path, Object ctx, byte data[], Stat stat);
    }

    public static abstract class ACLCallback implements AsyncCallback.ACLCallback {
        public void processResult(int rc, String path, Object ctx, List<ACL> acl, Stat stat) {
            try {
                safeProcessResult(rc, path, ctx, acl, stat);
            } catch (Throwable t) {
                invokeUncaughtExceptionHandler(t);
            }
        }

        public abstract void safeProcessResult(int rc, String path, Object ctx, List<ACL> acl, Stat stat);
    }

    public static abstract class ChildrenCallback implements AsyncCallback.ChildrenCallback {
        public void processResult(int rc, String path, Object ctx, List<String> children) {
            try {
                safeProcessResult(rc, path, ctx, children);
            } catch (Throwable t) {
                invokeUncaughtExceptionHandler(t);
            }
        }

        public abstract void safeProcessResult(int rc, String path, Object ctx, List<String> children);
    }

    public static abstract class StringCallback implements AsyncCallback.StringCallback {
        public void processResult(int rc, String path, Object ctx, String name) {
            try {
                safeProcessResult(rc, path, ctx, name);
            } catch (Throwable t) {
                invokeUncaughtExceptionHandler(t);
            }
        }

        public abstract void safeProcessResult(int rc, String path, Object ctx, String name);
    }

    public static abstract class VoidCallback implements AsyncCallback.VoidCallback {
        public void processResult(int rc, String path, Object ctx) {
            try {
                safeProcessResult(rc, path, ctx);
            } catch (Throwable t) {
                invokeUncaughtExceptionHandler(t);
            }
        }

        public abstract void safeProcessResult(int rc, String path, Object ctx);
    }
}
"
hedwig-server/src/main/java/org/apache/hedwig/zookeeper/ZkUtils.java,false,"/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.hedwig.zookeeper;

import java.io.IOException;
import java.util.List;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.apache.zookeeper.AsyncCallback;
import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.ZooKeeper;
import org.apache.zookeeper.KeeperException.Code;
import org.apache.zookeeper.data.ACL;

import org.apache.hedwig.util.PathUtils;

public class ZkUtils {

    static Logger logger = LoggerFactory.getLogger(ZkUtils.class);

    static class SyncObject {
        int rc;
        String path;
        boolean called = false;
    }

    public static void createFullPathOptimistic(final ZooKeeper zk, final String originalPath, final byte[] data,
            final List<ACL> acl, final CreateMode createMode)
    throws KeeperException, IOException, InterruptedException {
        final SyncObject syncObj = new SyncObject();

        createFullPathOptimistic(
            zk, originalPath, data, acl, createMode,
            new SafeAsyncZKCallback.StringCallback() {
                @Override
                public void safeProcessResult(final int rc, String path, Object ctx, String name) {
                    synchronized (syncObj) {
                        syncObj.rc = rc;
                        syncObj.path = path;
                        syncObj.called = true;
                        syncObj.notify();
                    }
                }
            }, syncObj
        );

        synchronized (syncObj) {
            while (!syncObj.called) {
                syncObj.wait();
            }
        }

        if (Code.OK.intValue() != syncObj.rc) {
            throw KeeperException.create(syncObj.rc, syncObj.path);
        }
    }

    public static void createFullPathOptimistic(final ZooKeeper zk, final String originalPath, final byte[] data,
            final List<ACL> acl, final CreateMode createMode, final AsyncCallback.StringCallback callback,
            final Object ctx) {

        zk.create(originalPath, data, acl, createMode, new SafeAsyncZKCallback.StringCallback() {
            @Override
            public void safeProcessResult(int rc, String path, Object ctx, String name) {

                if (rc != Code.NONODE.intValue()) {
                    callback.processResult(rc, path, ctx, name);
                    return;
                }

                // Since I got a nonode, it means that my parents don't exist
                // create mode is persistent since ephemeral nodes can't be
                // parents
                ZkUtils.createFullPathOptimistic(zk, PathUtils.parent(originalPath), new byte[0], acl,
                CreateMode.PERSISTENT, new SafeAsyncZKCallback.StringCallback() {

                    @Override
                    public void safeProcessResult(int rc, String path, Object ctx, String name) {
                        if (rc == Code.OK.intValue() || rc == Code.NODEEXISTS.intValue()) {
                            // succeeded in creating the parent, now
                            // create the original path
                            ZkUtils.createFullPathOptimistic(zk, originalPath, data, acl, createMode, callback,
                                                             ctx);
                        } else {
                            callback.processResult(rc, path, ctx, name);
                        }
                    }
                }, ctx);
            }
        }, ctx);

    }

    public static KeeperException logErrorAndCreateZKException(String msg, String path, int rc) {
        KeeperException ke = KeeperException.create(Code.get(rc), path);
        logger.error(msg + ",zkPath: " + path, ke);
        return ke;
    }

}
"
