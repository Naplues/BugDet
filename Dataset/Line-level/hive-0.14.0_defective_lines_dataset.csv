File,Line_number,SRC
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/HiveOptiqUtil.java,213,rexBuilder.makeInputRef(newLeftFields.get(i).getType(), newLeftOffset + i),
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/HiveOptiqUtil.java,214,rexBuilder.makeInputRef(newLeftFields.get(i).getType(), newRightOffset + i));
ql/src/java/org/apache/hadoop/hive/ql/metadata/SessionHiveMetaStoreClient.java,364,wh.getFileStatusesForSD(newtCopy.getSd()), false, true);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,918,if (dbProduct == null) {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5321,try {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5322,return getTxnHandler().getOpenTxns();
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5324,throw new TException(e);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5331,try {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5332,return getTxnHandler().getOpenTxnsInfo();
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5334,throw new TException(e);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5340,try {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5341,return getTxnHandler().openTxns(rqst);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5343,throw new TException(e);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5349,try {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5350,getTxnHandler().abortTxn(rqst);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5352,throw new TException(e);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5359,try {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5360,getTxnHandler().commitTxn(rqst);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5362,throw new TException(e);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5369,try {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5370,return getTxnHandler().lock(rqst);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5372,throw new TException(e);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5379,try {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5380,return getTxnHandler().checkLock(rqst);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5382,throw new TException(e);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5389,try {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5390,getTxnHandler().unlock(rqst);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5392,throw new TException(e);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5398,try {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5399,return getTxnHandler().showLocks(rqst);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5401,throw new TException(e);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5408,try {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5409,getTxnHandler().heartbeat(ids);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5411,throw new TException(e);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5418,try {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5419,return getTxnHandler().heartbeatTxnRange(rqst);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5421,throw new TException(e);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5427,try {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5428,getTxnHandler().compact(rqst);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5430,throw new TException(e);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5436,try {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5437,return getTxnHandler().showCompact(rqst);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5439,throw new TException(e);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,55,Connection dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,59,stmt = dbConn.createStatement();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,61,String s = "select distinct ctc_database, ctc_table, " +
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,63,LOG.debug("Going to execute query <" + s + ">");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,64,ResultSet rs = stmt.executeQuery(s);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,65,while (rs.next()) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,66,CompactionInfo info = new CompactionInfo();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,67,info.dbname = rs.getString(1);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,68,info.tableName = rs.getString(2);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,69,info.partName = rs.getString(3);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,70,response.add(info);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,74,s = "select tc_database, tc_table, tc_partition " +
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,80,LOG.debug("Going to execute query <" + s + ">");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,81,rs = stmt.executeQuery(s);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,82,while (rs.next()) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,83,CompactionInfo info = new CompactionInfo();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,84,info.dbname = rs.getString(1);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,85,info.tableName = rs.getString(2);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,86,info.partName = rs.getString(3);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,87,info.tooManyAborts = true;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,88,response.add(info);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,91,LOG.debug("Going to rollback");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,92,dbConn.rollback();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,94,LOG.error("Unable to connect to transaction database " + e.getMessage());
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,96,closeDbConn(dbConn);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,97,closeStmt(stmt);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,99,return response;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,110,Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,113,stmt = dbConn.createStatement();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,114,String s = "update COMPACTION_QUEUE set cq_run_as = '" + user + "' where cq_id = " + cq_id;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,115,LOG.debug("Going to execute update <" + s + ">");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,116,if (stmt.executeUpdate(s) != 1) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,117,LOG.error("Unable to update compaction record");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,118,LOG.debug("Going to rollback");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,119,dbConn.rollback();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,121,LOG.debug("Going to commit");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,122,dbConn.commit();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,124,LOG.error("Unable to update compaction queue, " + e.getMessage());
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,125,try {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,126,LOG.debug("Going to rollback");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,127,dbConn.rollback();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,130,detectDeadlock(dbConn, e, "setRunAs");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,132,closeDbConn(dbConn);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,133,closeStmt(stmt);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,138,deadlockCnt = 0;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,150,Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,190,try {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,191,LOG.debug("Going to rollback");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,192,dbConn.rollback();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,195,detectDeadlock(dbConn, e, "findNextToCompact");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,197,StringUtils.stringifyException(e));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,205,deadlockCnt = 0;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,216,Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,231,try {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,232,LOG.error("Unable to update compaction queue " + e.getMessage());
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,233,LOG.debug("Going to rollback");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,234,dbConn.rollback();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,237,detectDeadlock(dbConn, e, "markCompacted");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,239,StringUtils.stringifyException(e));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,247,deadlockCnt = 0;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,257,Connection dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,262,stmt = dbConn.createStatement();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,263,String s = "select cq_id, cq_database, cq_table, cq_partition, " +
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,265,LOG.debug("Going to execute query <" + s + ">");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,266,ResultSet rs = stmt.executeQuery(s);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,267,while (rs.next()) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,268,CompactionInfo info = new CompactionInfo();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,269,info.id = rs.getLong(1);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,270,info.dbname = rs.getString(2);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,271,info.tableName = rs.getString(3);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,272,info.partName = rs.getString(4);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,273,switch (rs.getString(5).charAt(0)) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,274,case MAJOR_TYPE: info.type = CompactionType.MAJOR; break;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,275,case MINOR_TYPE: info.type = CompactionType.MINOR; break;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,276,default: throw new MetaException("Unexpected compaction type " + rs.getString(5));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,278,info.runAs = rs.getString(6);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,279,rc.add(info);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,281,LOG.debug("Going to rollback");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,282,dbConn.rollback();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,283,return rc;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,285,LOG.error("Unable to select next element for cleaning, " + e.getMessage());
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,286,try {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,291,throw new MetaException("Unable to connect to transaction database " +
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,294,closeDbConn(dbConn);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,295,closeStmt(stmt);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,306,Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,333,TXN_ABORTED + "' and tc_database = '" + info.dbname + "' and tc_table = '" +
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,334,info.tableName + "'";
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,374,try {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,375,LOG.error("Unable to delete from compaction queue " + e.getMessage());
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,376,LOG.debug("Going to rollback");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,377,dbConn.rollback();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,380,detectDeadlock(dbConn, e, "markCleaned");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,382,StringUtils.stringifyException(e));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,390,deadlockCnt = 0;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,399,Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,428,try {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,429,dbConn.rollback();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,432,detectDeadlock(dbConn, e, "cleanEmptyAbortedTxns");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,434,StringUtils.stringifyException(e));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,442,deadlockCnt = 0;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,457,Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,462,+ INITIATED_STATE+ "' where cq_state = '" + WORKING_STATE + "' and cq_worker_id like '"
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,463,+  hostname + "%'";
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,471,try {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,472,LOG.error("Unable to change dead worker's records back to initiated state " +
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,473,e.getMessage());
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,474,LOG.debug("Going to rollback");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,475,dbConn.rollback();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,478,detectDeadlock(dbConn, e, "revokeFromLocalWorkers");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,480,StringUtils.stringifyException(e));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,488,deadlockCnt = 0;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,503,Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,504,long latestValidStart = getDbTime(dbConn) - timeout;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,509,+ INITIATED_STATE+ "' where cq_state = '" + WORKING_STATE + "' and cq_start < "
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,510,+  latestValidStart;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,518,try {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,519,LOG.error("Unable to change dead worker's records back to initiated state " +
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,520,e.getMessage());
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,521,LOG.debug("Going to rollback");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,522,dbConn.rollback();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,525,detectDeadlock(dbConn, e, "revokeTimedoutWorkers");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,527,StringUtils.stringifyException(e));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,535,deadlockCnt = 0;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,546,Connection dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,550,String quote = getIdentifierQuoteString(dbConn);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,551,stmt = dbConn.createStatement();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,552,StringBuilder bldr = new StringBuilder();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,560,.append(" = '").append(ci.tableName).append("'");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,561,if (ci.partName != null) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,565,String s = bldr.toString();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,571,LOG.debug("Going to execute <" + s + ">");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,572,rs = stmt.executeQuery(s);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,573,List<String> columns = new ArrayList<String>();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,574,while(rs.next()) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,575,columns.add(rs.getString(1));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,577,LOG.debug("Found columns to update stats: " + columns + " on " + ci.tableName +
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,578,(ci.partName == null ? "" : "/" + ci.partName));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,579,dbConn.commit();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,580,return columns;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,582,try {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,584,(ci.partName == null ? "" : "/" + ci.partName), e);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,585,dbConn.rollback();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,589,throw new MetaException("Unable to connect to transaction database " +
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,590,StringUtils.stringifyException(e));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,592,close(rs, stmt, dbConn);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,80,private static Boolean lockLock = new Boolean("true"); // Random object to lock on for the lock
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,135,Connection dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,136,Statement stmt = null;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,138,stmt = dbConn.createStatement();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,139,String s = "select ntxn_next - 1 from NEXT_TXN_ID";
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,140,LOG.debug("Going to execute query <" + s + ">");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,141,ResultSet rs = stmt.executeQuery(s);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,142,if (!rs.next()) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,143,throw new MetaException("Transaction tables not properly " +
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,146,long hwm = rs.getLong(1);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,147,if (rs.wasNull()) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,148,throw new MetaException("Transaction tables not properly " +
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,152,List<TxnInfo> txnInfo = new ArrayList<TxnInfo>();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,153,s = "select txn_id, txn_state, txn_user, txn_host from TXNS";
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,154,LOG.debug("Going to execute query<" + s + ">");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,155,rs = stmt.executeQuery(s);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,156,while (rs.next()) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,157,char c = rs.getString(2).charAt(0);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,158,TxnState state;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,159,switch (c) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,160,case TXN_ABORTED:
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,161,state = TxnState.ABORTED;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,162,break;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,164,case TXN_OPEN:
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,165,state = TxnState.OPEN;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,166,break;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,168,default:
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,169,throw new MetaException("Unexpected transaction state " + c +
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,172,txnInfo.add(new TxnInfo(rs.getLong(1), state, rs.getString(3), rs.getString(4)));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,174,LOG.debug("Going to rollback");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,175,dbConn.rollback();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,176,return new GetOpenTxnsInfoResponse(hwm, txnInfo);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,178,try {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,183,throw new MetaException("Unable to select from transaction database, "
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,186,closeStmt(stmt);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,187,closeDbConn(dbConn);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,196,Connection dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,197,Statement stmt = null;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,199,timeOutTxns(dbConn);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,200,stmt = dbConn.createStatement();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,201,String s = "select ntxn_next - 1 from NEXT_TXN_ID";
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,202,LOG.debug("Going to execute query <" + s + ">");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,203,ResultSet rs = stmt.executeQuery(s);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,204,if (!rs.next()) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,205,throw new MetaException("Transaction tables not properly " +
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,208,long hwm = rs.getLong(1);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,209,if (rs.wasNull()) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,210,throw new MetaException("Transaction tables not properly " +
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,214,Set<Long> openList = new HashSet<Long>();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,215,s = "select txn_id from TXNS";
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,216,LOG.debug("Going to execute query<" + s + ">");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,217,rs = stmt.executeQuery(s);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,218,while (rs.next()) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,219,openList.add(rs.getLong(1));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,221,LOG.debug("Going to rollback");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,222,dbConn.rollback();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,223,return new GetOpenTxnsResponse(hwm, openList);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,225,try {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,230,throw new MetaException("Unable to select from transaction database, "
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,233,closeStmt(stmt);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,234,closeDbConn(dbConn);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,262,Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,267,HiveConf.ConfVars.HIVE_TXN_MAX_OPEN_BATCH);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,285,now + ", '" + rqst.getUser() + "', '" + rqst.getHostname() + "')";
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,299,try {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,300,LOG.debug("Going to rollback");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,301,dbConn.rollback();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,304,detectDeadlock(dbConn, e, "openTxns");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,314,deadlockCnt = 0;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,321,Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,334,try {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,335,LOG.debug("Going to rollback");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,336,dbConn.rollback();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,339,detectDeadlock(dbConn, e, "abortTxn");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,348,deadlockCnt = 0;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,353,throws NoSuchTxnException, TxnAbortedException,  MetaException {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,356,Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,391,try {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,392,LOG.debug("Going to rollback");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,393,dbConn.rollback();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,396,detectDeadlock(dbConn, e, "commitTxn");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,406,deadlockCnt = 0;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,411,throws NoSuchTxnException, TxnAbortedException, MetaException {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,413,Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,417,try {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,418,LOG.debug("Going to rollback");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,419,dbConn.rollback();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,422,detectDeadlock(dbConn, e, "lock");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,424,StringUtils.stringifyException(e));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,431,deadlockCnt = 0;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,436,throws NoSuchTxnException,  TxnAbortedException, MetaException {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,438,Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,442,try {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,443,LOG.debug("Going to rollback");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,444,dbConn.rollback();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,447,detectDeadlock(dbConn, e, "lockNoWait");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,449,StringUtils.stringifyException(e));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,456,deadlockCnt = 0;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,461,throws NoSuchTxnException, NoSuchLockException, TxnAbortedException, MetaException {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,463,Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,477,try {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,478,LOG.debug("Going to rollback");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,479,dbConn.rollback();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,482,detectDeadlock(dbConn, e, "checkLock");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,484,StringUtils.stringifyException(e));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,491,deadlockCnt = 0;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,497,throws NoSuchLockException, TxnOpenException, MetaException {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,499,Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,532,try {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,533,LOG.debug("Going to rollback");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,534,dbConn.rollback();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,537,detectDeadlock(dbConn, e, "unlock");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,539,StringUtils.stringifyException(e));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,547,deadlockCnt = 0;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,552,Connection dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,553,ShowLocksResponse rsp = new ShowLocksResponse();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,554,List<ShowLocksResponseElement> elems = new ArrayList<ShowLocksResponseElement>();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,555,Statement stmt = null;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,557,stmt = dbConn.createStatement();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,559,String s = "select hl_lock_ext_id, hl_txnid, hl_db, hl_table, hl_partition, hl_lock_state, " +
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,561,LOG.debug("Doing to execute query <" + s + ">");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,562,ResultSet rs = stmt.executeQuery(s);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,563,while (rs.next()) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,564,ShowLocksResponseElement e = new ShowLocksResponseElement();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,565,e.setLockid(rs.getLong(1));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,566,long txnid = rs.getLong(2);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,567,if (!rs.wasNull()) e.setTxnid(txnid);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,568,e.setDbname(rs.getString(3));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,569,e.setTablename(rs.getString(4));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,570,String partition = rs.getString(5);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,571,if (partition != null) e.setPartname(partition);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,572,switch (rs.getString(6).charAt(0)) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,573,case LOCK_ACQUIRED: e.setState(LockState.ACQUIRED); break;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,574,case LOCK_WAITING: e.setState(LockState.WAITING); break;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,575,default: throw new MetaException("Unknown lock state " + rs.getString(6).charAt(0));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,577,switch (rs.getString(7).charAt(0)) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,578,case LOCK_SEMI_SHARED: e.setType(LockType.SHARED_WRITE); break;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,579,case LOCK_EXCLUSIVE: e.setType(LockType.EXCLUSIVE); break;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,580,case LOCK_SHARED: e.setType(LockType.SHARED_READ); break;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,581,default: throw new MetaException("Unknown lock type " + rs.getString(6).charAt(0));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,583,e.setLastheartbeat(rs.getLong(8));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,584,long acquiredAt = rs.getLong(9);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,585,if (!rs.wasNull()) e.setAcquiredat(acquiredAt);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,586,e.setUser(rs.getString(10));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,587,e.setHostname(rs.getString(11));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,588,elems.add(e);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,590,LOG.debug("Going to rollback");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,591,dbConn.rollback();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,593,throw new MetaException("Unable to select from transaction database " +
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,596,closeStmt(stmt);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,597,closeDbConn(dbConn);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,599,rsp.setLocks(elems);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,600,return rsp;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,604,throws NoSuchTxnException,  NoSuchLockException, TxnAbortedException, MetaException {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,606,Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,611,try {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,612,LOG.debug("Going to rollback");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,613,dbConn.rollback();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,616,detectDeadlock(dbConn, e, "heartbeat");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,618,StringUtils.stringifyException(e));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,630,throws MetaException {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,632,Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,650,try {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,651,LOG.debug("Going to rollback");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,652,dbConn.rollback();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,655,detectDeadlock(dbConn, e, "heartbeatTxnRange");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,657,StringUtils.stringifyException(e));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,669,Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,733,try {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,734,LOG.debug("Going to rollback");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,735,dbConn.rollback();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,738,detectDeadlock(dbConn, e, "compact");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,740,StringUtils.stringifyException(e));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,748,deadlockCnt = 0;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,754,Connection dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,757,stmt = dbConn.createStatement();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,758,String s = "select cq_database, cq_table, cq_partition, cq_state, cq_type, cq_worker_id, " +
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,760,LOG.debug("Going to execute query <" + s + ">");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,761,ResultSet rs = stmt.executeQuery(s);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,762,while (rs.next()) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,763,ShowCompactResponseElement e = new ShowCompactResponseElement();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,764,e.setDbname(rs.getString(1));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,765,e.setTablename(rs.getString(2));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,766,e.setPartitionname(rs.getString(3));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,767,switch (rs.getString(4).charAt(0)) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,768,case INITIATED_STATE: e.setState(INITIATED_RESPONSE); break;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,769,case WORKING_STATE: e.setState(WORKING_RESPONSE); break;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,770,case READY_FOR_CLEANING: e.setState(CLEANING_RESPONSE); break;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,771,default: throw new MetaException("Unexpected compaction state " + rs.getString(4));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,773,switch (rs.getString(5).charAt(0)) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,774,case MAJOR_TYPE: e.setType(CompactionType.MAJOR); break;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,775,case MINOR_TYPE: e.setType(CompactionType.MINOR); break;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,776,default: throw new MetaException("Unexpected compaction type " + rs.getString(5));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,778,e.setWorkerid(rs.getString(6));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,779,e.setStart(rs.getLong(7));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,780,e.setRunAs(rs.getString(8));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,781,response.addToCompacts(e);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,783,LOG.debug("Going to rollback");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,784,dbConn.rollback();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,786,LOG.debug("Going to rollback");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,787,try {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,791,throw new MetaException("Unable to select from transaction database " +
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,794,closeStmt(stmt);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,795,closeDbConn(dbConn);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,797,return response;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,831,protected class DeadlockException extends Exception {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,842,protected Connection getDbConn(int isolationLevel) throws MetaException {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,844,Connection dbConn = connPool.getConnection();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,845,dbConn.setAutoCommit(false);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,846,dbConn.setTransactionIsolation(isolationLevel);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,847,return dbConn;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,849,String msg = "Unable to get jdbc connection from pool, " + e.getMessage();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,850,throw new MetaException(msg);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,858,LOG.warn("Failed to close db connection " + e.getMessage());
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,870,LOG.warn("Failed to close statement " + e.getMessage());
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,885,LOG.warn("Failed to close statement " + ex.getMessage());
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,907,protected void detectDeadlock(Connection conn,
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,909,String caller) throws DeadlockException, MetaException {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,922,((dbProduct == DatabaseProduct.MYSQL || dbProduct == DatabaseProduct.POSTGRES ||
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,923,dbProduct == DatabaseProduct.SQLSERVER) && e.getSQLState().equals("40001")) ||
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,924,(dbProduct == DatabaseProduct.POSTGRES && e.getSQLState().equals("40P01")) ||
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,926,|| e.getMessage().contains("can't serialize access for this transaction")))) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,929,throw new DeadlockException();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1076,intLockId + " txnId:" + Long.toString
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1077,(txnId) + " db:" + db + " table:" + table + " partition:" +
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1079,+ " type:" + (type == null ? "null" : type.toString());
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1091,info2.state != LockState .ACQUIRED) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1095,info2.state == LockState .ACQUIRED) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1127,HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_IN_TEZ_TEST);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1205,throws NoSuchTxnException,  TxnAbortedException, MetaException, SQLException {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1255,(tblName == null ? "null" : "'" + tblName + "'") + ", " +
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1256,(partName == null ? "null" : "'" +  partName + "'") + ")";
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1280,+ intLockId + "," + (txnid >= 0 ? txnid : "null") + ", '" +
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1282,+ ", " + (partName == null ? "null" : "'" + partName + "'") +
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1284,rqst.getUser() + "', '" + rqst.getHostname() + "')";
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1308,throws NoSuchLockException, NoSuchTxnException, TxnAbortedException, MetaException, SQLException {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1433,&& !locks[index].table.equals(locks[i].table)) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1441,&& !locks[index].partition.equals(locks[i].partition)) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1448,(locks[i].state)) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1449,case ACQUIRE:
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1450,acquire(dbConn, stmt, extLockId, info.intLockId);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1451,acquired = true;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1452,break;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1453,case WAIT:
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1454,wait(dbConn, save);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1455,if (alwaysCommit) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1458,LOG.debug("Going to commit");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1459,dbConn.commit();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1461,response.setState(LockState.WAITING);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1462,return response;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1463,case KEEP_LOOKING:
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1464,continue;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1497,throws SQLException, NoSuchLockException, MetaException {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1501,extLockId + " and hl_lock_int_id = " + intLockId;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1508,+ intLockId + ")");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1517,throws NoSuchLockException, SQLException, MetaException {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1526,now + " where hl_lock_ext_id = " + extLockId;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1543,throws NoSuchTxnException, TxnAbortedException, SQLException, MetaException {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1578,throws NoSuchLockException, MetaException, SQLException {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1583,extLockId;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1600,throws NoSuchLockException, MetaException, SQLException {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1635,(now - timeout);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1678,HiveConf.ConfVars.METASTOREPWD.varname);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1683,HiveConf.ConfVars.METASTORE_CONNECTION_POOLING_TYPE).toLowerCase();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1699,new PoolableConnectionFactory(connFactory, objectPool, null, null, false, true);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1706,private static synchronized void buildJumpTable() {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1710,new HashMap<LockType, Map<LockType, Map<LockState,  LockAction>>>(3);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1714,new HashMap<LockType, Map<LockState, LockAction>>(3);
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1982,String tbl_row_format = "";
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1985,tbl_row_format += "ROW FORMAT";
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1987,if (serdeInfo.getParametersSize() > 1) {
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1991,tbl_row_format += " DELIMITED \n";
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1992,Map<String, String> delims = serdeInfo.getParameters();
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1997,if (delims.containsKey(serdeConstants.FIELD_DELIM)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1998,tbl_row_format += "  FIELDS TERMINATED BY '" +
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1999,escapeHiveCommand(StringEscapeUtils.escapeJava(delims.get(
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2000,serdeConstants.FIELD_DELIM))) + "' \n";
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2002,if (delims.containsKey(serdeConstants.COLLECTION_DELIM)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2003,tbl_row_format += "  COLLECTION ITEMS TERMINATED BY '" +
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2004,escapeHiveCommand(StringEscapeUtils.escapeJava(delims.get(
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2005,serdeConstants.COLLECTION_DELIM))) + "' \n";
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2007,if (delims.containsKey(serdeConstants.MAPKEY_DELIM)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2008,tbl_row_format += "  MAP KEYS TERMINATED BY '" +
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2009,escapeHiveCommand(StringEscapeUtils.escapeJava(delims.get(
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2010,serdeConstants.MAPKEY_DELIM))) + "' \n";
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2012,if (delims.containsKey(serdeConstants.LINE_DELIM)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2013,tbl_row_format += "  LINES TERMINATED BY '" +
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2014,escapeHiveCommand(StringEscapeUtils.escapeJava(delims.get(
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2015,serdeConstants.LINE_DELIM))) + "' \n";
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2017,if (delims.containsKey(serdeConstants.SERIALIZATION_NULL_FORMAT)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2018,tbl_row_format += "  NULL DEFINED AS '" +
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2019,escapeHiveCommand(StringEscapeUtils.escapeJava(delims.get(
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2020,serdeConstants.SERIALIZATION_NULL_FORMAT))) + "' \n";
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2023,else {
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2024,tbl_row_format += " SERDE \n  '" +
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2025,escapeHiveCommand(serdeInfo.getSerializationLib()) + "' \n";
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2027,tbl_row_format += "STORED AS INPUTFORMAT \n  '" +
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2028,escapeHiveCommand(sd.getInputFormat()) + "' \n";
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2029,tbl_row_format += "OUTPUTFORMAT \n  '" +
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2030,escapeHiveCommand(sd.getOutputFormat()) + "'";
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2032,else {
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2034,tbl_row_format += " SERDE \n  '" +
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2035,escapeHiveCommand(serdeInfo.getSerializationLib()) + "' \n";
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2036,tbl_row_format += "STORED BY \n  '" + escapeHiveCommand(tbl.getParameters().get(
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2037,org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_STORAGE)) + "' \n";
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2040,tbl_row_format += "WITH SERDEPROPERTIES ( \n";
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2041,List<String> serdeCols = new ArrayList<String>();
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2042,for (Map.Entry<String, String> entry : serdeInfo.getParameters().entrySet()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2043,serdeCols.add("  '" + entry.getKey() + "'='"
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2044,+ escapeHiveCommand(StringEscapeUtils.escapeJava(entry.getValue())) + "'");
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2046,tbl_row_format += StringUtils.join(serdeCols, ", \n");
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2047,tbl_row_format += ")";
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2054,Map<String, String> properties = new TreeMap<String, String>(tbl.getParameters());
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2055,if (properties.size() > 0) {
ql/src/java/org/apache/hadoop/hive/ql/io/parquet/MapredParquetInputFormat.java,35,public class MapredParquetInputFormat extends FileInputFormat<Void, ArrayWritable>
ql/src/java/org/apache/hadoop/hive/ql/io/parquet/MapredParquetInputFormat.java,36,implements VectorizedInputFormatInterface {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java,139,if (limit > 0 && data.hasOnlyPruningFilter()) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java,140,return true;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java,298,public boolean hasOnlyPruningFilter() {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java,865,if (skipFolding(joinOp.getConf(), rsDesc.getTag())) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java,911,private boolean skipFolding(JoinDesc joinDesc, int tag) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java,912,JoinCondDesc[] conds = joinDesc.getConds();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java,913,int i;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java,914,for (i = conds.length - 1; i >= 0; i--) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java,915,if (conds[i].getType() == JoinDesc.INNER_JOIN) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java,917,return false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java,919,return true;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java,922,return false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java,923,return true;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java,926,return true;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java,929,if (tag == 0) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java,930,return false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java,932,return true;
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,283,boolean removeParents = false;
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,284,for (Operator<?> parent: new ArrayList<Operator<?>>(root.getParentOperators())) {
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,285,removeParents = true;
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,286,context.leafOperatorToFollowingWork.put(parent, work);
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,287,LOG.debug("Removing " + parent + " as parent from " + root);
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,289,if (removeParents) {
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,290,for (Operator<?> parent : new ArrayList<Operator<?>>(root.getParentOperators())) {
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,291,root.removeParent(parent);
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,411,return -1;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,506,int buckets) {
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,100,acquire(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,101,ShimLoader.getHadoopShims().closeAllForUGI(sessionUgi);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,102,cancelDelegationToken();
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,104,release(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,105,super.close();
ql/src/java/org/apache/hadoop/hive/ql/exec/ExprNodeEvaluatorFactory.java,103,private final Map<String, ExprNodeEvaluator> cached = new HashMap<String, ExprNodeEvaluator>();
ql/src/java/org/apache/hadoop/hive/ql/exec/ExprNodeEvaluatorFactory.java,108,String key = eval.getExpr().toString();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorColumnAssignFactory.java,417,HiveVarchar hiveVarchar = (HiveVarchar) val;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorColumnAssignFactory.java,432,HiveChar hiveChar = (HiveChar) val;
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java,774,FileStatus fstatus = fs.getFileStatus(hdfsDirPath);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java,775,if (!fstatus.isDir()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java,776,throw new IOException(ErrorMsg.INVALID_DIR.format(hdfsDirPath.toString()));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/BytesColumnVector.java,317,setVal(outElementNum, in.vector[inputElementNum], in.start[inputElementNum], in.length[outElementNum]);
ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java,21,import java.math.BigDecimal;
ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java,22,import java.sql.Timestamp;
ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java,23,import java.util.ArrayDeque;
ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java,24,import java.util.ArrayList;
ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java,25,import java.util.Collections;
ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java,26,import java.util.Deque;
ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java,27,import java.util.HashMap;
ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java,28,import java.util.List;
ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java,29,import java.util.Map;
ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java,59,import com.esotericsoftware.kryo.Kryo;
ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java,60,import com.esotericsoftware.kryo.io.Input;
ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java,61,import com.esotericsoftware.kryo.io.Output;
ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java,728,root = new ExpressionTree(ExpressionTree.Operator.AND);
ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java,729,generateAllCombinations(root.children, andList, nonAndList);
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,1587,HIVE_SSL_PROTOCOL_BLACKLIST("hive.ssl.protocol.blacklist", "SSLv2,SSLv2Hello,SSLv3",
ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsAggregator.java,137,fileID = JDBCStatsUtils.truncateRowId(fileID);
ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsAggregator.java,221,rowID = JDBCStatsUtils.truncateRowId(rowID);
ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsPublisher.java,142,String rowId = JDBCStatsUtils.truncateRowId(fileID);
ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsPublisher.java,144,String truncateSuffix = (rowId != fileID) ? " (from " + fileID + ")" : ""; // object equality
ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsPublisher.java,145,LOG.info("Stats publishing for key " + rowId + truncateSuffix);
ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsPublisher.java,160,insStmt.setString(1, rowId);
ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsPublisher.java,179,updStmt.setString(supportedStatistics.size() + 1, rowId);
ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsPublisher.java,182,updStmt.setString(supportedStatistics.size() + 3, rowId);
ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsPublisher.java,286,rs = dbm.getTables(null, null, JDBCStatsUtils.getStatTableName(), null);
ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsPublisher.java,290,stmt.executeUpdate(createTable);
ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsSetupConstants.java,39,public static final int ID_COLUMN_VARCHAR_SIZE = 255;
ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsUtils.java,27,import org.apache.hadoop.util.hash.MurmurHash;
ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsUtils.java,201,public static String truncateRowId(String rowId) {
ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsUtils.java,203,? rowId : Integer.toHexString(MurmurHash.getInstance().hash(rowId.getBytes()));
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,341,int index = getMergeIndex(tezWork, unionWork, rs);
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,343,BaseWork baseWork = tezWork.getChildren(unionWork).get(index);
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,344,if (baseWork instanceof MergeJoinWork) {
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,345,MergeJoinWork mergeJoinWork = (MergeJoinWork) baseWork;
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,347,followingWork = mergeJoinWork;
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,348,rWork = (ReduceWork) mergeJoinWork.getMainWork();
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,350,throw new SemanticException("Unknown work type found: "
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,351,+ baseWork.getClass().getCanonicalName());
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,395,private int getMergeIndex(TezWork tezWork, UnionWork unionWork, ReduceSinkOperator rs) {
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,398,if (baseWork instanceof MergeJoinWork) {
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,399,MergeJoinWork mergeJoinWork = (MergeJoinWork) baseWork;
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,400,int tag = mergeJoinWork.getMergeJoinOperator().getTagForOperator(rs);
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,401,if (tag != -1) {
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,402,return index;
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,404,index++;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java,20,import java.io.DataInput;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java,21,import java.io.DataOutput;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java,22,import java.io.IOException;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java,23,import java.util.ArrayList;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java,24,import java.util.HashMap;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java,25,import java.util.List;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java,26,import java.util.Map;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,118,import org.apache.hadoop.hive.serde2.Deserializer;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,119,import org.apache.hadoop.hive.serde2.SerDe;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,120,import org.apache.hadoop.hive.serde2.SerDeUtils;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,125,import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,127,import org.apache.hadoop.util.ReflectionUtils;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,348,MapWorkValidationNodeProcessor vnp = new MapWorkValidationNodeProcessor(isTez);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,520,public MapWorkValidationNodeProcessor(boolean isTez) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,532,boolean ret = validateMapWorkOperator(op, isTez);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,848,boolean validateMapWorkOperator(Operator<? extends OperatorDesc> op, boolean isTez) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,871,ret = validateTableScanOperator((TableScanOperator) op);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,953,private boolean validateTableScanOperator(TableScanOperator op) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,955,return !desc.isGatherStats();
ql/src/java/org/apache/hadoop/hive/ql/exec/OrcFileMergeOperator.java,36,import java.util.List;
ql/src/java/org/apache/hadoop/hive/ql/exec/OrcFileMergeOperator.java,50,List<Integer> version;
ql/src/java/org/apache/hadoop/hive/ql/exec/OrcFileMergeOperator.java,93,version = k.getVersionList();
ql/src/java/org/apache/hadoop/hive/ql/exec/OrcFileMergeOperator.java,170,if (!k.getVersionList().equals(version)) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFileKeyWrapper.java,26,import org.apache.hadoop.fs.Path;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFileKeyWrapper.java,27,import org.apache.hadoop.hive.ql.io.orc.OrcProto;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFileKeyWrapper.java,28,import org.apache.hadoop.io.WritableComparable;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFileKeyWrapper.java,41,protected List<Integer> versionList;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFileKeyWrapper.java,43,public List<Integer> getVersionList() {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFileKeyWrapper.java,44,return versionList;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFileKeyWrapper.java,47,public void setVersionList(List<Integer> versionList) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFileKeyWrapper.java,48,this.versionList = versionList;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFileStripeMergeRecordReader.java,21,import java.io.IOException;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFileStripeMergeRecordReader.java,22,import java.util.Iterator;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFileStripeMergeRecordReader.java,23,import java.util.List;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFileStripeMergeRecordReader.java,91,keyWrapper.setVersionList(((ReaderImpl) reader).getFileMetaInfo().versionList);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java,21,import java.io.IOException;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java,22,import java.io.OutputStream;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java,23,import java.lang.management.ManagementFactory;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java,24,import java.nio.ByteBuffer;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java,25,import java.sql.Timestamp;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java,26,import java.util.ArrayList;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java,27,import java.util.EnumSet;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java,28,import java.util.List;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java,29,import java.util.Map;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java,30,import java.util.TreeMap;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java,91,import static com.google.common.base.Preconditions.checkArgument;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java,2392,treeWriter.fileStatistics.merge(ColumnStatisticsImpl.deserialize(cs.get(0)));
ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java,2393,TreeWriter[] childWriters = treeWriter.getChildrenWriters();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java,2394,for (int i = 0; i < childWriters.length; i++) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java,2395,childWriters[i].fileStatistics.merge(
ql/src/java/org/apache/hadoop/hive/ql/io/orc/WriterImpl.java,2396,ColumnStatisticsImpl.deserialize(cs.get(i + 1)));
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5776,transFactory = saslServer.createTransportFactory(
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5777,MetaStoreUtils.getMetaStoreSaslProperties(conf));
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,21,import java.net.InetAddress;
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,31,import javax.security.auth.login.LoginException;
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,62,NOSASL("NOSASL"),
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,63,NONE("NONE"),
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,64,LDAP("LDAP"),
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,65,KERBEROS("KERBEROS"),
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,66,CUSTOM("CUSTOM"),
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,67,PAM("PAM");
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,69,private final String authType;
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,71,AuthTypes(String authType) {
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,72,this.authType = authType;
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,75,public String getAuthName() {
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,76,return authType;
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,81,private HadoopThriftAuthBridge.Server saslServer;
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,82,private String authTypeStr;
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,83,private final String transportMode;
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,91,transportMode = conf.getVar(HiveConf.ConfVars.HIVE_SERVER2_TRANSPORT_MODE);
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,92,authTypeStr = conf.getVar(HiveConf.ConfVars.HIVE_SERVER2_AUTHENTICATION);
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,95,if ("http".equalsIgnoreCase(transportMode)) {
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,96,if (authTypeStr == null) {
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,97,authTypeStr = AuthTypes.NOSASL.getAuthName();
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,100,if (authTypeStr == null) {
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,101,authTypeStr = AuthTypes.NONE.getAuthName();
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,104,&& ShimLoader.getHadoopShims().isSecureShimImpl()) {
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,106,.createServer(conf.getVar(ConfVars.HIVE_SERVER2_KERBEROS_KEYTAB),
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,107,conf.getVar(ConfVars.HIVE_SERVER2_KERBEROS_PRINCIPAL));
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,109,try {
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,110,saslServer.startDelegationTokenSecretManager(conf, null, ServerMode.HIVESERVER2);
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,112,throw new TTransportException("Failed to start token manager", e);
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,126,public TTransportFactory getAuthTransFactory() throws LoginException {
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,127,TTransportFactory transportFactory;
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,128,if (authTypeStr.equalsIgnoreCase(AuthTypes.KERBEROS.getAuthName())) {
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,129,try {
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,130,transportFactory = saslServer.createTransportFactory(getSaslProperties());
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,132,throw new LoginException(e.getMessage());
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,135,transportFactory = PlainSaslHelper.getPlainTransportFactory(authTypeStr);
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,137,transportFactory = PlainSaslHelper.getPlainTransportFactory(authTypeStr);
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,139,transportFactory = PlainSaslHelper.getPlainTransportFactory(authTypeStr);
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,141,transportFactory = new TTransportFactory();
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,143,transportFactory = PlainSaslHelper.getPlainTransportFactory(authTypeStr);
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,145,throw new LoginException("Unsupported authentication type " + authTypeStr);
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,147,return transportFactory;
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,156,public TProcessorFactory getAuthProcFactory(ThriftCLIService service) throws LoginException {
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,157,if (authTypeStr.equalsIgnoreCase(AuthTypes.KERBEROS.getAuthName())) {
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,160,return PlainSaslHelper.getPlainProcessorFactory(service);
service/src/java/org/apache/hive/service/auth/PlainSaslHelper.java,56,throws LoginException {
service/src/java/org/apache/hive/service/auth/PlainSaslHelper.java,57,TSaslServerTransport.Factory saslFactory = new TSaslServerTransport.Factory();
service/src/java/org/apache/hive/service/auth/PlainSaslHelper.java,58,try {
service/src/java/org/apache/hive/service/auth/PlainSaslHelper.java,59,saslFactory.addServerDefinition("PLAIN", authTypeStr, null, new HashMap<String, String>(),
service/src/java/org/apache/hive/service/auth/PlainSaslHelper.java,60,new PlainServerCallbackHandler(authTypeStr));
service/src/java/org/apache/hive/service/auth/PlainSaslHelper.java,62,throw new LoginException("Error setting callback handler" + e);
service/src/java/org/apache/hive/service/auth/PlainSaslHelper.java,64,return saslFactory;
shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge20S.java,103,public Server createServer(String keytabFile, String principalConf) throws TTransportException {
shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge20S.java,367,throws TTransportException {
shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge20S.java,375,TSaslServerTransport.Factory transFactory = new TSaslServerTransport.Factory();
shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge20S.java,376,transFactory.addServerDefinition(
shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge20S.java,377,AuthMethod.KERBEROS.getMechanismName(),
shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge20S.java,378,names[0], names[1],  // two parts of kerberos principal
shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge20S.java,379,saslProps,
shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge20S.java,380,new SaslRpcServer.SaslGssCallbackHandler());
shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge20S.java,381,transFactory.addServerDefinition(AuthMethod.DIGEST.getMechanismName(),
shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge20S.java,382,null, SaslRpcServer.SASL_DEFAULT_REALM,
shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge20S.java,383,saslProps, new SaslDigestCallbackHandler(secretManager));
shims/common-secure/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge20S.java,385,return new TUGIAssumingTransportFactory(transFactory, realUgi);
shims/common/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java,62,throws TTransportException {
shims/common/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java,105,public abstract TTransportFactory createTransportFactory(Map<String, String> saslProps) throws TTransportException;
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezJobMonitor.java,705,String result = s + " ";
ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java,1286,cplan.setName("Tez Merge File Work");
ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java,1295,cplan.setName("Tez Merge File Work");
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java,891,ArrayList<ExprNodeDesc> newPartExprs = new ArrayList<ExprNodeDesc>();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java,892,for (ExprNodeDesc desc : rsDesc.getPartitionCols()) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java,893,ExprNodeDesc expr = foldExpr(desc, constants, cppCtx, op, 0, false);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java,894,if (expr instanceof ExprNodeConstantDesc || expr instanceof ExprNodeNullDesc) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java,895,continue;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java,897,newPartExprs.add(expr);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConstantPropagateProcFactory.java,899,rsDesc.setPartitionCols(newPartExprs);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezJobMonitor.java,313,console.logInfo(getReport(progressMap));
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezJobMonitor.java,322,console.logInfo(getReport(progressMap));
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezJobMonitor.java,347,console.logInfo(getReport(progressMap));
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezJobMonitor.java,359,console.logInfo(getReport(progressMap));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprStringScalarStringScalar.java,91,outputColVector.setRef(i, arg3Scalar, 0, arg2Scalar.length);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprStringScalarStringScalar.java,99,outputColVector.setRef(i, arg3Scalar, 0, arg2Scalar.length);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprStringScalarStringScalar.java,110,outputColVector.setRef(i, arg3Scalar, 0, arg2Scalar.length);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprStringScalarStringScalar.java,118,outputColVector.setRef(i, arg3Scalar, 0, arg2Scalar.length);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1753,return resultType.equalsIgnoreCase("string") || charVarcharTypePattern.matcher(resultType).matches();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/StringGroupColConcatStringScalar.java,128,return "StringGroup";
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/StringGroupConcatColCol.java,419,return "StringGroup";
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/StringScalarConcatStringGroupCol.java,128,return "StringGroup";
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,562,context.splits.add(new OrcSplit(dir, b, 0, new String[0], null,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,563,false, false, deltas));
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java,219,if (field.getFieldName().equals(s)) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java,307,if (!(left.getFieldName().equals(right.getFieldName()) &&
beeline/src/java/org/apache/hive/beeline/HiveSchemaHelper.java,275,return new DerbyCommandParser();
beeline/src/java/org/apache/hive/beeline/HiveSchemaHelper.java,277,return new MSSQLCommandParser();
beeline/src/java/org/apache/hive/beeline/HiveSchemaHelper.java,279,return new MySqlCommandParser();
beeline/src/java/org/apache/hive/beeline/HiveSchemaHelper.java,281,return new PostgresCommandParser();
beeline/src/java/org/apache/hive/beeline/HiveSchemaHelper.java,283,return new OracleCommandParser();
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,20,import java.io.BufferedReader;
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,23,import java.io.FileReader;
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,28,import java.sql.DriverManager;
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,33,import java.util.IllegalFormatException;
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,68,throws HiveMetaException {
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,124,throws HiveMetaException {
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,126,if (HiveSchemaHelper.getDbCommandParser(dbType).needsQuotedIdentifier()) {
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,163,throws HiveMetaException {
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,164,try {
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,165,String connectionURL = getValidConfVar(ConfVars.METASTORECONNECTURLKEY);
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,166,String driver = getValidConfVar(ConfVars.METASTORE_CONNECTION_DRIVER);
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,167,if (printInfo) {
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,168,System.out.println("Metastore connection URL:\t " + connectionURL);
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,169,System.out.println("Metastore Connection Driver :\t " + driver);
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,170,System.out.println("Metastore connection User:\t " + userName);
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,172,if ((userName == null) || userName.isEmpty()) {
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,173,throw new HiveMetaException("UserName empty ");
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,177,Class.forName(driver);
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,180,return DriverManager.getConnection(connectionURL, userName, passWord);
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,182,throw new HiveMetaException("Failed to get schema version.", e);
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,184,throw new HiveMetaException("Failed to get schema version.", e);
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,186,throw new HiveMetaException("Failed to load driver", e);
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,199,String newSchemaVersion =
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,200,getMetaStoreSchemaVersion(getConnectionToMetastore(false));
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,212,String fromVersion = getMetaStoreSchemaVersion(getConnectionToMetastore(false));
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,295,public static String buildCommand(NestedScriptParser dbCommandParser,
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,296,String scriptDir, String scriptFile) throws IllegalFormatException, IOException {
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,298,BufferedReader bfReader =
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,299,new BufferedReader(new FileReader(scriptDir + File.separatorChar + scriptFile));
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,300,String currLine;
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,301,StringBuilder sb = new StringBuilder();
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,302,String currentCommand = null;
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,303,while ((currLine = bfReader.readLine()) != null) {
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,304,currLine = currLine.trim();
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,305,if (currLine.isEmpty()) {
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,306,continue; // skip empty lines
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,309,if (currentCommand == null) {
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,310,currentCommand = currLine;
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,312,currentCommand = currentCommand + " " + currLine;
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,314,if (dbCommandParser.isPartialCommand(currLine)) {
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,316,continue;
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,320,if (!dbCommandParser.isNonExecCommand(currentCommand)) {
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,321,currentCommand = dbCommandParser.cleanseCommand(currentCommand);
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,323,if (dbCommandParser.isNestedScript(currentCommand)) {
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,325,String currScript = dbCommandParser.getScriptName(currentCommand);
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,326,sb.append(buildCommand(dbCommandParser, scriptDir, currScript));
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,330,sb.append(currentCommand);
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,331,sb.append(System.getProperty("line.separator"));
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,334,currentCommand = null;
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,336,bfReader.close();
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,337,return sb.toString();
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,341,private void runBeeLine(String scriptDir, String scriptFile) throws IOException {
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,342,NestedScriptParser dbCommandParser =
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,343,HiveSchemaHelper.getDbCommandParser(dbType);
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,345,String sqlCommands = buildCommand(dbCommandParser, scriptDir, scriptFile);
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,363,argList.add(getValidConfVar(ConfVars.METASTORECONNECTURLKEY));
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,365,argList.add(getValidConfVar(ConfVars.METASTORE_CONNECTION_DRIVER));
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,387,private String getValidConfVar(ConfVars confVar) throws IOException {
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,388,String confVarStr = hiveConf.get(confVar.varname);
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,389,if (confVarStr == null || confVarStr.isEmpty()) {
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,390,throw new IOException("Empty " + confVar.varname);
beeline/src/java/org/apache/hive/beeline/HiveSchemaTool.java,392,return confVarStr;
metastore/src/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java,371,String keyEqual = FileUtils.escapePathName(keyName) + "=";
metastore/src/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java,372,String valString = "partitionName.substring(";
metastore/src/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java,373,String indexOfKeyStr = "";
metastore/src/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java,374,if (partitionColumnIndex != 0) {
metastore/src/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java,375,keyEqual = "/" + keyEqual;
metastore/src/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java,376,indexOfKeyStr = "partitionName.indexOf(\"" + keyEqual + "\") + ";
metastore/src/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java,377,valString += indexOfKeyStr;
metastore/src/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java,379,valString += keyEqual.length();
metastore/src/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java,380,if (partitionColumnIndex != (partitionColumnCount - 1)) {
metastore/src/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java,381,valString += ", partitionName.concat(\"/\").indexOf(\"/\", " + indexOfKeyStr + keyEqual.length() + ")";
metastore/src/java/org/apache/hadoop/hive/metastore/parser/ExpressionTree.java,383,valString += ")";
ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java,1138,prefix = conf.getTableInfo().getTableName();
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,123,LOG.error("Database initialization failed; direct SQL is disabled", ex);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,201,List<Object[]> sqlResult = (List<Object[]>)queryDbSelector.executeWithArray(params);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,226,List<Object[]> sqlResult2 = ensureList(queryDbParams.executeWithArray(params));
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,229,dbParams.put(extractSqlString(line[0]),extractSqlString(line[1]));
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,287,String sqlFilter = PartitionFilterGenerator.generateSqlFilter(table, tree, params, joins);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,292,isViewTable(table), sqlFilter, params, joins, max);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,320,Object result = query.executeWithArray(params);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,378,List<Object> sqlResult = (List<Object>)query.executeWithArray(params);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,772,Table table, List<Object> params, List<String> joins) {
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,786,public static String generateSqlFilter(Table table,
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,787,ExpressionTree tree, List<Object> params, List<String> joins) throws MetaException {
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,792,PartitionFilterGenerator visitor = new PartitionFilterGenerator(table, params, joins);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,932,tableValue = "(case when \"TBLS\".\"TBL_NAME\" = ? and \"DBS\".\"NAME\" = ? then "
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,933,+ tableValue + " else null end)";
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,934,params.add(table.getTableName().toLowerCase());
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,935,params.add(table.getDbName().toLowerCase());
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,964,Object qResult = query.executeWithArray(params);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,992,String qText = "select count(\"COLUMN_NAME\") from \"PART_COL_STATS\""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,998,Query query = pm.newQuery("javax.jdo.query.SQL", qText);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,999,Object qResult = query.executeWithArray(prepareParams(dbName, tableName,
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1000,partNames, colNames));
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1002,timingTrace(doTrace, qText, start, end);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1021,String qText = null;
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1031,qText = commonPrefix
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1036,query = pm.newQuery("javax.jdo.query.SQL", qText);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1037,qResult = query.executeWithArray(prepareParams(dbName, tableName,
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1038,partNames, colNames));
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1044,timingTrace(doTrace, qText, start, end);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1059,qText = "select \"COLUMN_NAME\", \"COLUMN_TYPE\", count(\"PARTITION_NAME\") "
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1066,query = pm.newQuery("javax.jdo.query.SQL", qText);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1067,qResult = query.executeWithArray(prepareParams(dbName, tableName,
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1068,partNames, colNames));
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1070,timingTrace(doTrace, qText, start, end);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1095,qText = commonPrefix
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1100,query = pm.newQuery("javax.jdo.query.SQL", qText);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1101,qResult = query.executeWithArray(prepareParams(dbName, tableName,
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1102,partNames, noExtraColumnNames));
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1112,timingTrace(doTrace, qText, start, end);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1124,qText = "select \"COLUMN_NAME\", sum(\"NUM_NULLS\"), sum(\"NUM_TRUES\"), sum(\"NUM_FALSES\")"
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1131,query = pm.newQuery("javax.jdo.query.SQL", qText);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1134,qResult = query.executeWithArray(prepareParams(dbName, tableName,
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1135,partNames, extraColumnNames));
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1151,timingTrace(doTrace, qText, start, end);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1186,qText = "select \""
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1194,query = pm.newQuery("javax.jdo.query.SQL", qText);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1195,qResult = query.executeWithArray(prepareParams(dbName,
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1196,tableName, partNames, Arrays.asList(colName)));
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1205,timingTrace(doTrace, qText, start, end);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1263,Object qResult = query.executeWithArray(prepareParams(dbName, tableName, partNames, colNames));
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,751,org.apache.hadoop.hive.metastore.api.StorageDescriptor storageDescriptor = baseTbl.getSd().deepCopy();
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,752,SerDeInfo serdeInfo = storageDescriptor.getSerdeInfo();
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,791,storageDescriptor.setLocation(null);
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,792,if (location != null) {
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,793,storageDescriptor.setLocation(location);
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,795,storageDescriptor.setInputFormat(inputFormat);
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,796,storageDescriptor.setOutputFormat(outputFormat);
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,798,Map<String, String> params = new HashMap<String,String>();
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,802,storageDescriptor.setBucketCols(null);
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,818,storageDescriptor.setCols(indexTblCols);
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,819,storageDescriptor.setSortCols(sortCols);
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,855,storageDescriptor, params, deferredRebuild);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java,670,dropTable(name, table, deleteData, false);
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2148,columnNames, expr, defaultPartName, result);
metastore/src/java/org/apache/hadoop/hive/metastore/PartitionExpressionProxy.java,46,public boolean filterPartitionsByExpr(List<String> columnNames, byte[] expr,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartExprEvalUtils.java,106,ExprNodeGenericFuncDesc expr, List<String> partNames) throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartExprEvalUtils.java,110,partObjectInspectors.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionExpressionForMetastore.java,43,public boolean filterPartitionsByExpr(List<String> columnNames, byte[] exprBytes,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionExpressionForMetastore.java,49,columnNames, expr, defaultPartitionName, partitionNames);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,425,partCols, prunerExpr, defaultPartitionName, partNames);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,453,public static boolean prunePartitionNames(List<String> columnNames, ExprNodeGenericFuncDesc prunerExpr,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,457,PartExprEvalUtils.prepareExpr(prunerExpr, columnNames);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,465,ArrayList<String> values = new ArrayList<String>(columnNames.size());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,466,for (int i = 0; i < columnNames.size(); ++i) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,477,Boolean isNeeded = (Boolean)PartExprEvalUtils.evaluateExprOnPart(handle, values);
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,21,import java.util.ArrayList;
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,22,import java.util.LinkedList;
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,23,import java.util.List;
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,24,import java.util.Map;
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,25,import java.util.Map.Entry;
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,26,import java.util.Stack;
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,115,work = context.rootToWorkMap.get(root);
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,62,private ScheduledFuture<?> cleanupHandle; // used to cleanup cache
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,137,cleanupHandle = Executors.newScheduledThreadPool(1).scheduleWithFixedDelay(
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,288,private AtomicInteger users = new AtomicInteger(0);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,83,private static final Log LOG = LogFactory.getLog(MetaStoreDirectSql.class);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,93,private final boolean isMySql;
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,100,public MetaStoreDirectSql(PersistenceManager pm) {
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,102,Transaction tx = pm.currentTransaction();
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,103,tx.begin();
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,104,boolean isMySql = false;
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,105,try {
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,106,trySetAnsiQuotesForMysql();
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,107,isMySql = true;
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,109,LOG.info("MySQL check failed, assuming we are not on mysql: " + sqlEx.getMessage());
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,110,tx.rollback();
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,111,tx = pm.currentTransaction();
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,112,tx.begin();
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,115,boolean isCompatibleDatastore = true;
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,122,isCompatibleDatastore = false;
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,126,if (isCompatibleDatastore) {
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,128,String selfTestQuery = "select \"DB_ID\" from \"DBS\"";
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,129,try {
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,130,pm.newQuery("javax.jdo.query.SQL", selfTestQuery).execute();
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,131,tx.commit();
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,133,isCompatibleDatastore = false;
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,134,LOG.error("Self-test query [" + selfTestQuery + "] failed; direct SQL is disabled", ex);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,135,tx.rollback();
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,139,this.isCompatibleDatastore = isCompatibleDatastore;
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,140,this.isMySql = isMySql;
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,153,if (!isMySql) return;
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,156,trySetAnsiQuotesForMysql();
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,167,private void trySetAnsiQuotesForMysql() throws SQLException {
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,168,final String queryText = "SET @@session.sql_mode=ANSI_QUOTES";
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,212,Long dbid = StatObjectConverter.extractSqlLong(dbline[0]);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,266,String dbName, String tblName, List<String> partNames, Integer max) throws MetaException {
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,272,partNames, new ArrayList<String>(), max);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,386,int sbCapacity = sqlResult.size() * 7; // if there are 100k things => 6 chars, plus comma
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,389,for (Object partitionId : sqlResult) {
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,390,partSb.append(StatObjectConverter.extractSqlLong(partitionId)).append(",");
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,393,timingTrace(doTrace, queryText, start, queryTime);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,396,queryText =
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,406,start = doTrace ? System.nanoTime() : 0;
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,407,query = pm.newQuery("javax.jdo.query.SQL", queryText);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,409,List<Object[]> sqlResult2 = (List<Object[]>)query.executeWithArray(params);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,410,queryTime = doTrace ? System.nanoTime() : 0;
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,418,ArrayList<Partition> orderedResult = new ArrayList<Partition>(sqlResult.size());
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,425,for (Object[] fields : sqlResult2) {
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,427,long partitionId = StatObjectConverter.extractSqlLong(fields[0]);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,428,Long sdId = StatObjectConverter.extractSqlLong(fields[1]);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,429,Long colId = StatObjectConverter.extractSqlLong(fields[2]);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,430,Long serdeId = StatObjectConverter.extractSqlLong(fields[3]);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,599,long fieldsListId = StatObjectConverter.extractSqlLong(fields[1]);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,641,long fieldsListId = StatObjectConverter.extractSqlLong(fields[1]);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,752,long nestedId = StatObjectConverter.extractSqlLong(fields[keyIndex]);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,981,long partsFound = partsFoundForPartitions(dbName, tableName, partNames,
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,982,colNames);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1006,if (StatObjectConverter.extractSqlLong(iter.next()) == colNames.size()) {
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1085,Long count = StatObjectConverter.extractSqlLong(row[2]);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1180,Long val = StatObjectConverter.extractSqlLong(o);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1190,+ " and \"COLUMN_NAME\" in (" +makeParams(1)+ ")"
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1309,if (laObj != null && (!csd.isSetLastAnalyzed() || csd.getLastAnalyzed() > StatObjectConverter.extractSqlLong(laObj))) {
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,1310,csd.setLastAnalyzed(StatObjectConverter.extractSqlLong(laObj));
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,270,directSql = new MetaStoreDirectSql(pm);
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2004,return directSql.getPartitionsViaSqlFilter(dbName, tblName, partNames, null);
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2057,result = directSql.getPartitionsViaSqlFilter(dbName, tblName, partNames, null);
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,422,boolStats.setNumFalses(extractSqlLong(falses));
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,423,boolStats.setNumTrues(extractSqlLong(trues));
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,424,boolStats.setNumNulls(extractSqlLong(nulls));
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,429,stringStats.setNumNulls(extractSqlLong(nulls));
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,431,stringStats.setMaxColLen(extractSqlLong(maxlen));
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,432,stringStats.setNumDVs(extractSqlLong(dist));
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,436,binaryStats.setNumNulls(extractSqlLong(nulls));
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,438,binaryStats.setMaxColLen(extractSqlLong(maxlen));
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,444,longStats.setNumNulls(extractSqlLong(nulls));
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,446,longStats.setHighValue(extractSqlLong(lhigh));
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,449,longStats.setLowValue(extractSqlLong(llow));
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,451,longStats.setNumDVs(extractSqlLong(dist));
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,455,doubleStats.setNumNulls(extractSqlLong(nulls));
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,462,doubleStats.setNumDVs(extractSqlLong(dist));
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,466,decimalStats.setNumNulls(extractSqlLong(nulls));
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,473,decimalStats.setNumDVs(extractSqlLong(dist));
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,487,static Long extractSqlLong(Object obj) throws MetaException {
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,488,if (obj == null) return null;
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,489,if (!(obj instanceof Number)) {
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,490,throw new MetaException("Expected numeric type but got " + obj.getClass().getName());
metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java,492,return ((Number)obj).longValue();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,41,public static final HivePushFilterPastJoinRule FILTER_ON_JOIN = new HivePushFilterIntoJoinRule();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,43,public static final HivePushFilterPastJoinRule JOIN = new HivePushDownJoinConditionRule();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,48,protected HivePushFilterPastJoinRule(RelOptRuleOperand operand, String id,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,49,boolean smart, RelFactories.FilterFactory filterFactory,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,50,RelFactories.ProjectFactory projectFactory) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,51,super(operand, id, smart, filterFactory, projectFactory);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,58,public static class HivePushFilterIntoJoinRule extends
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,59,HivePushFilterPastJoinRule {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,60,public HivePushFilterIntoJoinRule() {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,61,super(RelOptRule.operand(FilterRelBase.class,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,62,RelOptRule.operand(JoinRelBase.class, RelOptRule.any())),
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,64,HiveFilterRel.DEFAULT_FILTER_FACTORY,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,65,HiveProjectRel.DEFAULT_PROJECT_FACTORY);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,69,public void onMatch(RelOptRuleCall call) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,70,FilterRelBase filter = call.rel(0);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,71,JoinRelBase join = call.rel(1);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,72,super.perform(call, filter, join);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,76,public static class HivePushDownJoinConditionRule extends
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,77,HivePushFilterPastJoinRule {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,78,public HivePushDownJoinConditionRule() {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,79,super(RelOptRule.operand(JoinRelBase.class, RelOptRule.any()),
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,81,HiveFilterRel.DEFAULT_FILTER_FACTORY,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,82,HiveProjectRel.DEFAULT_PROJECT_FACTORY);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,86,public void onMatch(RelOptRuleCall call) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,87,JoinRelBase join = call.rel(0);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,88,super.perform(call, null, join);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,110,|| (c.getOperator().getKind() == SqlKind.GREATER_THAN_OR_EQUAL)) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,111,boolean validHiveJoinFilter = true;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,124,continue;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,133,private boolean filterRefersToBothSidesOfJoin(RexNode filter, JoinRelBase j) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,134,boolean refersToBothSides = false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,136,int joinNoOfProjects = j.getRowType().getFieldCount();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,137,BitSet filterProjs = new BitSet(joinNoOfProjects);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,138,BitSet allLeftProjs = new BitSet(joinNoOfProjects);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,139,BitSet allRightProjs = new BitSet(joinNoOfProjects);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,140,allLeftProjs.set(0, j.getInput(0).getRowType().getFieldCount(), true);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,141,allRightProjs.set(j.getInput(0).getRowType().getFieldCount(),
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,142,joinNoOfProjects, true);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,144,InputFinder inputFinder = new InputFinder(filterProjs);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,145,filter.accept(inputFinder);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,149,refersToBothSides = true;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/rules/HivePushFilterPastJoinRule.java,151,return refersToBothSides;
ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java,230,lowLinks.put(child, Math.min(lowLinks.get(o), lowLinks.get(child)));
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,504,HashSet<ReadEntity> inputs = sem.getInputs();
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,505,HashSet<WriteEntity> outputs = sem.getOutputs();
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,708,private static void doAuthorizationV2(SessionState ss, HiveOperation op, HashSet<ReadEntity> inputs,
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,709,HashSet<WriteEntity> outputs, String command, Map<String, List<String>> tab2cols,
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,729,HashSet<? extends Entity> privObjects, Map<String, List<String>> tableName2Cols) {
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,156,executeStatementInternal(cmd_trimed, null, false);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,158,rc = -1;
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,159,LOG.warn("Failed to execute HQL command in global .hiverc file.", e);
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,586,Decimal val = csd.getDecimalStats().getHighValue();
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,587,BigDecimal maxVal = HiveDecimal.
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,588,create(new BigInteger(val.getUnscaled()), val.getScale()).bigDecimalValue();
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,589,val = csd.getDecimalStats().getLowValue();
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,590,BigDecimal minVal = HiveDecimal.
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,591,create(new BigInteger(val.getUnscaled()), val.getScale()).bigDecimalValue();
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,592,cs.setRange(minVal, maxVal);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFMapKeys.java,64,retArray.addAll(mapOI.getMap(mapObj).keySet());
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,246,AggrStats aggrStats = Hive.get().getAggrColStatsFor(table.getDbName(), table.getTableName(),
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,247,neededColumns, partNames);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorSMBMapJoinOperator.java,125,VectorizedRowBatch inBatch = (VectorizedRowBatch) row;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorSMBMapJoinOperator.java,126,return keyEvaluator.evaluate(keyValues[batchIndex]);
serde/src/java/org/apache/hadoop/hive/serde2/io/DateWritable.java,28,import org.apache.commons.logging.Log;
serde/src/java/org/apache/hadoop/hive/serde2/io/DateWritable.java,29,import org.apache.commons.logging.LogFactory;
serde/src/java/org/apache/hadoop/hive/serde2/io/DateWritable.java,30,import org.apache.hadoop.hive.serde2.ByteStream.Output;
serde/src/java/org/apache/hadoop/hive/serde2/io/DateWritable.java,47,private static final Log LOG = LogFactory.getLog(DateWritable.class);
serde/src/java/org/apache/hadoop/hive/serde2/io/DateWritable.java,140,long millisUtc = millisLocal + LOCAL_TIMEZONE.get().getOffset(millisLocal);
serde/src/java/org/apache/hadoop/hive/serde2/io/DateWritable.java,141,return (int)(millisUtc / MILLIS_PER_DAY);
ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java,312,ObjectInspector oi = TypeInfoUtils.getStandardJavaObjectInspectorFromTypeInfo(
ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java,313,TypeInfoFactory.getPrimitiveTypeInfo(partKeyTypes[i]));
service/src/java/org/apache/hive/service/server/HiveServer2.java,270,if (hiveConf.getBoolVar(ConfVars.HIVE_SERVER2_SUPPORT_DYNAMIC_SERVICE_DISCOVERY)) {
service/src/java/org/apache/hive/service/server/HiveServer2.java,279,if (hiveConf.getBoolVar(ConfVars.HIVE_SERVER2_TEZ_INITIALIZE_DEFAULT_SESSIONS)) {
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,248,if (null == aggrStats) {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java,433,LOG.error("Unable to shutdown local metastore client", e);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractBucketJoinProc.java,482,if (bigTblBucketNum >= smallTblBucketNum) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractBucketJoinProc.java,487,int toAddSmallIndex = bindex % smallTblBucketNum;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractBucketJoinProc.java,488,resultFileNames.add(smallTblFileNames.get(toAddSmallIndex));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractBucketJoinProc.java,490,int jump = smallTblBucketNum / bigTblBucketNum;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractBucketJoinProc.java,491,for (int i = bindex; i < smallTblFileNames.size(); i = i + jump) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/AbstractBucketJoinProc.java,492,resultFileNames.add(smallTblFileNames.get(i));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,391,.getOpTraits().getSortCols(), rsOp.getColumnExprMap(), tezBucketJoinProcCtx) == false) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,448,tezBucketJoinProcCtx) == false) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,469,TezBucketJoinProcCtx tezBucketJoinProcCtx) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,496,return true;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ReduceSinkMapJoinProc.java,173,parentRS.getConf().setReducerTraits(EnumSet.of(FIXED));
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,650,if (tbl.getCols().size() == 0) {
ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java,913,if (triedDouble ||
contrib/src/java/org/apache/hadoop/hive/contrib/serde2/RegexSerDe.java,93,inputRegex = tbl.getProperty("input.regex");
contrib/src/java/org/apache/hadoop/hive/contrib/serde2/RegexSerDe.java,94,outputFormatString = tbl.getProperty("output.format.string");
contrib/src/java/org/apache/hadoop/hive/contrib/serde2/RegexSerDe.java,98,.getProperty("input.regex.case.insensitive"));
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,3975,private int createTableLike(Hive db, CreateTableLikeDesc crtTbl) throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,4045,List<String> paramsList = Arrays.asList(paramsStr.split(","));
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,4046,params.keySet().retainAll(paramsList);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSerde.java,78,String columnNameProperty = table.getProperty("columns");
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSerde.java,80,String columnTypeProperty = table.getProperty("columns.types");
ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveSerDe.java,26,import org.apache.hadoop.hive.ql.io.IOConstants;
ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveSerDe.java,108,final String columnNameProperty = tbl.getProperty(IOConstants.COLUMNS);
ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveSerDe.java,109,final String columnTypeProperty = tbl.getProperty(IOConstants.COLUMNS_TYPES);
ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveSerDe.java,245,return new ByteWritable((byte) ((ByteObjectInspector) inspector).get(obj));
ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveSerDe.java,255,return new ShortWritable((short) ((ShortObjectInspector) inspector).get(obj));
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,37,import org.apache.hadoop.hive.common.JavaUtils;
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,195,if (null == getDeserializerFromMetaStore()) {
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,256,deserializer = getDeserializerFromMetaStore();
ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java,261,private Deserializer getDeserializerFromMetaStore() {
serde/src/java/org/apache/hadoop/hive/serde2/OpenCSVSerde.java,72,.split(","));
serde/src/java/org/apache/hadoop/hive/serde2/RegexSerDe.java,42,import org.apache.hadoop.hive.serde2.typeinfo.DecimalTypeInfo;
serde/src/java/org/apache/hadoop/hive/serde2/RegexSerDe.java,98,inputRegex = tbl.getProperty("input.regex");
serde/src/java/org/apache/hadoop/hive/serde2/RegexSerDe.java,102,.getProperty("input.regex.case.insensitive"));
serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerDe.java,60,private static String TABLE_NAME = "name";
serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerDe.java,61,private static String TABLE_COMMENT = "comment";
serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerDe.java,82,final String columnNameProperty = properties.getProperty("columns");
serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerDe.java,83,final String columnTypeProperty = properties.getProperty("columns.types");
serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerDe.java,84,final String columnCommentProperty = properties.getProperty("columns.comments","");
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/BinarySortableSerDe.java,81,import org.apache.hadoop.hive.serde2.typeinfo.VarcharTypeInfo;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazySimpleSerDe.java,22,import java.nio.charset.Charset;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazySimpleSerDe.java,34,import org.apache.hadoop.hive.serde2.AbstractSerDe;
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinarySerDe.java,30,import org.apache.hadoop.hive.common.type.Decimal128;
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinarySerDe.java,31,import org.apache.hadoop.hive.common.type.HiveDecimal;
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinarySerDe.java,35,import org.apache.hadoop.hive.serde2.ByteStream.Output;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java,262,WindowTableFunctionDef def = (WindowTableFunctionDef) conf.getFuncDef();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java,263,ArrayList<ColumnInfo> sig = new ArrayList<ColumnInfo>();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java,267,prunedCols = new ArrayList<String>(prunedCols);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java,268,prunedColumnsList(prunedCols, def);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java,269,RowResolver oldRR = cppCtx.getOpToParseCtxMap().get(op).getRowResolver();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java,270,RowResolver newRR = buildPrunedRR(prunedCols, oldRR, sig);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java,271,cppCtx.getPrunedColLists().put(op, prunedInputList(prunedCols, def));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java,272,cppCtx.getOpToParseCtxMap().get(op).setRowResolver(newRR);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java,277,private static RowResolver buildPrunedRR(List<String> prunedCols,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java,278,RowResolver oldRR, ArrayList<ColumnInfo> sig) throws SemanticException{
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java,279,RowResolver newRR = new RowResolver();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java,281,for(ColumnInfo cInfo : oldRR.getRowSchema().getSignature()) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java,283,String[] nm = oldRR.reverseLookup(cInfo.getInternalName());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java,284,newRR.put(nm[0], nm[1], cInfo);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java,288,return newRR;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java,294,private void prunedColumnsList(List<String> prunedCols, WindowTableFunctionDef tDef) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java,302,Utilities.mergeUniqElems(prunedCols, exprNode.getCols());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java,309,Utilities.mergeUniqElems(prunedCols, exprNode.getCols());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java,315,Utilities.mergeUniqElems(prunedCols, exprNode.getCols());
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5141,Operator select = insertSelectAllPlanForGroupBy(selectInput);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8390,throws SemanticException {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8405,new SelectDesc(colList, columnNames, true), new RowSchema(inputRR
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8406,.getColumnInfos()), input), inputRR);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8853,curr = insertSelectAllPlanForGroupBy(curr);
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezProcContext.java,127,public final List<UnionOperator> currentUnionOperators;
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezUtils.java,90,public UnionWork createUnionWork(GenTezProcContext context, Operator<?> operator, TezWork tezWork) {
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezUtils.java,92,context.unionWorkMap.put(operator, unionWork);
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,267,unionWork = utils.createUnionWork(context, operator, tezWork);
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,271,LOG.debug("Connecting union work ("+unionWork+") with work ("+work+")");
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,272,TezEdgeProperty edgeProp = new TezEdgeProperty(EdgeType.CONTAINS);
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,273,tezWork.connect(unionWork, work, edgeProp);
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,274,unionWork.addUnionOperators(context.currentUnionOperators);
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java,276,context.workWithUnionOperators.add(work);
ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java,296,ObjectInspector oi = TypeInfoUtils.getStandardJavaObjectInspectorFromTypeInfo(
ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java,297,TypeInfoFactory.getPrimitiveTypeInfo(partKeyTypes[i]));
ql/src/java/org/apache/hadoop/hive/ql/exec/StatsNoJobTask.java,226,List<Partition> partitions = getPartitionsList();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java,93,boolean partialScan = parseInfo.isPartialScanAnalyzeCommand();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java,94,boolean noScan = parseInfo.isNoScanAnalyzeCommand();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java,95,if (inputFormat.equals(OrcInputFormat.class) && (noScan || partialScan)) {
ql/src/java/org/apache/hadoop/hive/ql/parse/ProcessAnalyzeTable.java,98,boolean partialScan = parseInfo.isPartialScanAnalyzeCommand();
ql/src/java/org/apache/hadoop/hive/ql/parse/ProcessAnalyzeTable.java,99,boolean noScan = parseInfo.isNoScanAnalyzeCommand();
ql/src/java/org/apache/hadoop/hive/ql/parse/ProcessAnalyzeTable.java,100,if (inputFormat.equals(OrcInputFormat.class) && (noScan || partialScan)) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,195,if (isBooleanExpr(compactExpr)) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,197,if (!isFalseExpr(compactExpr)) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,199,LOG.debug("Filter " + oldFilter + " was null after compacting");
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,200,return getAllPartsFromCacheOrServer(tab, key, true, prunedPartitionsMap);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,202,return new PrunedPartitionList(tab, new LinkedHashSet<Partition>(new ArrayList<Partition>()),
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,203,new ArrayList<String>(), false);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,238,return  expr != null && expr instanceof ExprNodeConstantDesc &&
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,241,getTypeName().equals(serdeConstants.BOOLEAN_TYPE_NAME);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,244,return  isBooleanExpr(expr) &&
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,245,((ExprNodeConstantDesc)expr).getValue() != null &&
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,246,((ExprNodeConstantDesc)expr).getValue().equals(Boolean.TRUE);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,249,return  isBooleanExpr(expr) &&
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,251,((ExprNodeConstantDesc)expr).getValue().equals(Boolean.FALSE);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,263,if (expr == null) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,264,return null;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,266,if (expr instanceof ExprNodeConstantDesc) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,267,if (isBooleanExpr(expr)) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,268,return expr;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,270,throw new IllegalStateException("Unexpected non-null ExprNodeConstantDesc: "
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,271,+ expr.getExprString());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,280,ExprNodeDesc left = children.get(0);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,281,children.set(0, compactExpr(left));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,282,ExprNodeDesc right = children.get(1);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,283,children.set(1, compactExpr(right));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,285,if (isTrueExpr(children.get(0)) && isTrueExpr(children.get(1))) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,286,return new ExprNodeConstantDesc(Boolean.TRUE);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,288,return isAnd ? children.get(1) :  new ExprNodeConstantDesc(Boolean.TRUE);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,290,return isAnd ? children.get(0) : new ExprNodeConstantDesc(Boolean.TRUE);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,292,return new ExprNodeConstantDesc(Boolean.FALSE);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,294,return isAnd ? new ExprNodeConstantDesc(Boolean.FALSE) : children.get(1);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,296,return isAnd ? new ExprNodeConstantDesc(Boolean.FALSE) : children.get(0);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/translator/PlanModifierForASTConv.java,108,throw new RuntimeException("Found MultiJoinRel");
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/translator/PlanModifierForASTConv.java,110,throw new RuntimeException("Found OneRowRelBase");
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFInline.java,60,for (Object row : new ArrayList<Object>(li.getList(os[0]))) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDTFInline.java,61,forward(row);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,37,import org.apache.hadoop.hive.ql.exec.Description;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,76,import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,77,import org.apache.hive.common.util.AnnotationUtils;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,190,if(tsOp.getParentOperators() != null && tsOp.getParentOperators().size() > 0) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,194,SelectOperator selOp = (SelectOperator)tsOp.getChildren().get(0);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,195,for(ExprNodeDesc desc : selOp.getConf().getColList()) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,201,Map<String, ExprNodeDesc> exprMap = selOp.getColumnExprMap();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,204,GroupByOperator gbyOp = (GroupByOperator)selOp.getChildren().get(0);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,205,ReduceSinkOperator rsOp = (ReduceSinkOperator)gbyOp.getChildren().get(0);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,211,selOp = (SelectOperator)rsOp.getChildOperators().get(0).getChildOperators().get(0);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,212,List<AggregationDesc> aggrs = gbyOp.getConf().getAggregators();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,214,if (!(selOp.getConf().getColList().size() == aggrs.size())) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,219,for(ExprNodeDesc desc : selOp.getConf().getColList()) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,220,if (!(desc instanceof ExprNodeColumnDesc)) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,222,return null;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,225,FileSinkOperator fsOp = (FileSinkOperator)(selOp.getChildren().get(0));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,226,if (fsOp.getChildOperators() != null && fsOp.getChildOperators().size() > 0) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,228,return null;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,233,List<ObjectInspector> ois = new ArrayList<ObjectInspector>();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,237,for (AggregationDesc aggr : aggrs) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,259,oneRow.add(HiveDecimal.create(constant).multiply(HiveDecimal.create(rowCnt)));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,260,ois.add(PrimitiveObjectInspectorFactory.getPrimitiveJavaObjectInspector(
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,261,PrimitiveCategory.DECIMAL));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,338,ois.add(PrimitiveObjectInspectorFactory.
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,339,getPrimitiveJavaObjectInspector(PrimitiveCategory.LONG));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,357,case Integeral:
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,359,oneRow.add(lstats.isSetHighValue() ? lstats.getHighValue() : null);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,360,ois.add(PrimitiveObjectInspectorFactory.
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,361,getPrimitiveJavaObjectInspector(PrimitiveCategory.LONG));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,363,case Double:
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,365,oneRow.add(dstats.isSetHighValue() ? dstats.getHighValue() : null);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,366,ois.add(PrimitiveObjectInspectorFactory.
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,367,getPrimitiveJavaObjectInspector(PrimitiveCategory.DOUBLE));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,396,oneRow.add(maxVal);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,397,ois.add(PrimitiveObjectInspectorFactory.
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,398,getPrimitiveJavaObjectInspector(PrimitiveCategory.LONG));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,418,oneRow.add(maxVal);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,419,ois.add(PrimitiveObjectInspectorFactory.
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,420,getPrimitiveJavaObjectInspector(PrimitiveCategory.DOUBLE));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,442,case Integeral:
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,444,oneRow.add(lstats.isSetLowValue() ? lstats.getLowValue() : null);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,445,ois.add(PrimitiveObjectInspectorFactory.
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,446,getPrimitiveJavaObjectInspector(PrimitiveCategory.LONG));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,448,case Double:
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,450,oneRow.add(dstats.isSetLowValue() ? dstats.getLowValue() : null);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,451,ois.add(PrimitiveObjectInspectorFactory.
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,452,getPrimitiveJavaObjectInspector(PrimitiveCategory.DOUBLE));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,479,oneRow.add(minVal);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,480,ois.add(PrimitiveObjectInspectorFactory.
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,481,getPrimitiveJavaObjectInspector(PrimitiveCategory.LONG));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,501,oneRow.add(minVal);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,502,ois.add(PrimitiveObjectInspectorFactory.
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,503,getPrimitiveJavaObjectInspector(PrimitiveCategory.DOUBLE));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,525,for (ColumnInfo colInfo: gbyOp.getSchema().getSignature()) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/unionproc/UnionProcFactory.java,272,if (union.getChildOperators().size() > 1) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/unionproc/UnionProcFactory.java,273,return null;
ql/src/java/org/apache/hadoop/hive/ql/exec/SelectOperator.java,142,if(this.getConf().isSelStarNoCompute() ||
ql/src/java/org/apache/hadoop/hive/ql/exec/SelectOperator.java,143,this.getConf().isSelectStar()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/SelectOperator.java,148,if(!OperatorUtils.sameRowSchema(this, this.getParentOperators().get(0))) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcCtx.java,131,List<ExprNodeDesc> exprList = conf.getColList();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcCtx.java,132,for (ExprNodeDesc expr : exprList) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcCtx.java,133,cols = Utilities.mergeUniqElems(cols, expr.getCols());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java,877,ArrayList<ColumnInfo> inputCols = inputSchema.getSignature();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java,878,for (ColumnInfo i: inputCols) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java,879,if (cols.contains(i.getInternalName())) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateAdd.java,92,+ arguments[2].getTypeName() + " is passed. as second arguments");
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateSub.java,92,+ arguments[2].getTypeName() + " is passed. as second arguments");
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,801,unparseTranslator.addTableNameTranslation(tableTree, SessionState.get().getCurrentDatabase());
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,802,if (aliasIndex != 0) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,803,unparseTranslator.addIdentifierTranslation((ASTNode) tabref
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,804,.getChild(aliasIndex));
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1028,String currDB = SessionState.get().getCurrentDatabase();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1029,if ( currDB != null && cteName.startsWith(currDB) &&
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1030,cteName.length() > currDB.length() &&
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1031,cteName.charAt(currDB.length()) == '.'   ) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1032,cteName = cteName.substring(currDB.length() + 1);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1068,String cteText = ctx.getTokenRewriteStream().toString(
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1069,cteQryNode.getTokenStartIndex(), cteQryNode.getTokenStopIndex());
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1070,final ASTNodeOrigin cteOrigin = new ASTNodeOrigin("CTE", cteName,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1071,cteText, cteAlias, cteQryNode);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1072,cteQryNode = (ASTNode) ParseDriver.adaptor.dupTree(cteQryNode);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1073,SubQueryUtils.setOriginDeep(cteQryNode, cteOrigin);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1582,ASTNode cteNode = findCTEFromName(qb, tab_name.toLowerCase());
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1583,if ( cteNode != null ) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1584,String cte_name = tab_name.toLowerCase();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1585,if (ctesExpanded.contains(cte_name)) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1586,throw new SemanticException("Recursive cte " + tab_name +
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1590,addCTEAsSubQuery(qb, cte_name, alias);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1591,sqAliasToCTEName.put(alias, cte_name);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1592,continue;
ql/src/java/org/apache/hadoop/hive/ql/parse/WindowingSpec.java,617,return amt - rb.amt;
ql/src/java/org/apache/hadoop/hive/ql/parse/WindowingSpec.java,715,return amt - vb.amt;
ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsAggregator.java,84,stmt.setQueryTimeout(timeout);
ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsPublisher.java,84,stmt.setQueryTimeout(timeout);
ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsPublisher.java,282,stmt.setQueryTimeout(timeout);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,38,private static final int bitVectorSize = 31;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,39,private int numBitVectors;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,42,private final double phi =  0.77351;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,44,private int[] a;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,45,private int[] b;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,46,private  FastBitSet[] bitVector = new FastBitSet[numBitVectors];
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,48,private Random aValue;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,49,private Random bValue;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,57,bitVector[i] = new FastBitSet(bitVectorSize);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,101,a[i] = a[i] + (1 << bitVectorSize - 1);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,105,b[i] = b[i] + (1 << bitVectorSize - 1);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,111,FastBitSet b[] = deserialize(s, numBitVectors);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,114,bitVector[i] = new FastBitSet(bitVectorSize);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,116,bitVector[i].or(b[i]);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,138,return bitVectorSize;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,148,LOG.debug(bitVectorSize);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,176,b[j] = new FastBitSet(bitVectorSize);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,222,int mod = (1<<bitVectorSize) - 1;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,237,int mod = 1 << (bitVectorSize - 1) - 1;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,262,for (index=0; index<bitVectorSize; index++) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,263,if (hash % 2 == 1) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,280,for (index=0; index<bitVectorSize; index++) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,281,if (rho % 2 == 1) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,324,while (bitVector[i].get(index) && index < bitVectorSize) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,330,numDistinctValues = ((numBitVectors/phi) * Math.pow(2.0, S/numBitVectors));
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/NumDistinctValueEstimator.java,348,(double)(sumLeastSigZero/(numBitVectors * 1.0)) - (Math.log(phi)/Math.log(2.0));
ql/src/java/org/apache/hadoop/hive/ql/parse/QBSubQuery.java,516,ASTNode selectClause = (ASTNode) subQueryAST.getChild(1).getChild(1);
ql/src/java/org/apache/hadoop/hive/ql/parse/QBSubQuery.java,540,List<String> sqAliases = SubQueryUtils.getTableAliasesInSubQuery(this);
ql/src/java/org/apache/hadoop/hive/ql/parse/QBSubQuery.java,548,ASTNode whereClause = SubQueryUtils.subQueryWhere(subQueryAST);
ql/src/java/org/apache/hadoop/hive/ql/parse/QBSubQuery.java,584,rewrite(outerQueryRR, forHavingClause, outerQueryAlias);
ql/src/java/org/apache/hadoop/hive/ql/parse/QBSubQuery.java,756,String outerQueryAlias) throws SemanticException {
ql/src/java/org/apache/hadoop/hive/ql/parse/QBSubQuery.java,757,ASTNode selectClause = (ASTNode) subQueryAST.getChild(1).getChild(1);
ql/src/java/org/apache/hadoop/hive/ql/parse/QBSubQuery.java,758,ASTNode whereClause = SubQueryUtils.subQueryWhere(subQueryAST);
ql/src/java/org/apache/hadoop/hive/ql/parse/SubQueryUtils.java,274,static List<String> getTableAliasesInSubQuery(QBSubQuery sq) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SubQueryUtils.java,276,ASTNode joinAST = (ASTNode) sq.getSubQueryAST().getChild(0);
ql/src/java/org/apache/hadoop/hive/ql/parse/SubQueryUtils.java,277,getTableAliasesInSubQuery((ASTNode) joinAST.getChild(0), aliases);
ql/src/java/org/apache/hadoop/hive/ql/parse/SubQueryUtils.java,321,static ASTNode subQueryWhere(ASTNode subQueryAST) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SubQueryUtils.java,322,if ( subQueryAST.getChild(1).getChildCount() > 2 &&
ql/src/java/org/apache/hadoop/hive/ql/parse/SubQueryUtils.java,323,subQueryAST.getChild(1).getChild(2).getType() == HiveParser.TOK_WHERE ) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SubQueryUtils.java,324,return (ASTNode) subQueryAST.getChild(1).getChild(2);
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,506,name = name.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,618,dbname = dbname.toLowerCase();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,915,dbName = dbName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,961,db = db.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,962,table = table.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,985,db = db.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,997,lowered_tbl_names.add(t.toLowerCase().trim());
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,1077,return new MTable(tbl.getTableName().toLowerCase(), mdb,
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,1090,mkeys.add(new MFieldSchema(part.getName().toLowerCase(),
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,1114,mkeys.add(new MOrder(part.getCol().toLowerCase(), part.getOrder()));
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,1489,dbName = dbName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,1490,tableName = tableName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,1823,dbName = dbName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,1824,tableName = tableName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,1860,dbName = dbName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,1861,tableName = tableName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,1970,dbName = dbName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,1971,tableName = tableName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2290,params.put("t1", tblName.trim().toLowerCase());
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2291,params.put("t2", dbName.trim().toLowerCase());
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2317,this.dbName = dbName.toLowerCase();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2319,this.tblName = tblName.toLowerCase();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2620,dbName = dbName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2665,dbName = dbName.toLowerCase();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2666,tableName = tableName.toLowerCase();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2717,name = name.toLowerCase();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2718,dbname = dbname.toLowerCase();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2731,oldt.setTableName(newt.getTableName().toLowerCase());
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2759,name = name.toLowerCase();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2760,baseTblName = baseTblName.toLowerCase();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2761,dbname = dbname.toLowerCase();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2786,name = name.toLowerCase();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2787,dbname = dbname.toLowerCase();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,3020,return new MIndex(index.getIndexName().toLowerCase(), origTable, index.getCreateTime(),
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,3049,dbName = dbName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,3050,originalTblName = originalTblName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,3061,midx = (MIndex) query.execute(originalTblName, dbName, indexName.toLowerCase());
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,3128,dbName = dbName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,3129,origTableName = origTableName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,3154,dbName = dbName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,3155,origTableName = origTableName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,3576,dbName = dbName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,3602,dbName = dbName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,3645,tableName = tableName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,3646,dbName = dbName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,3688,tableName = tableName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,3689,dbName = dbName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,3730,tableName = tableName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,3731,dbName = dbName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,3732,columnName = columnName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,3774,tableName = tableName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,3775,dbName = dbName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,3803,tableName = tableName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,3804,dbName = dbName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,3829,tableName = tableName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,3830,dbName = dbName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,3831,columnName = columnName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,4415,dbName = dbName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,4499,tableName = tableName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,4500,dbName = dbName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,4502,tableName = tableName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,4503,dbName = dbName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,4530,tableName = tableName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,4531,dbName = dbName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,4562,tableName = tableName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,4563,dbName = dbName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,4589,tableName = tableName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,4590,dbName = dbName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,4617,tableName = tableName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,4618,dbName = dbName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,4649,dbName = dbName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,4675,tableName = tableName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,4676,dbName = dbName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,4719,params[0] = tableName;
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,4720,params[1] = dbName;
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,4739,tableName = tableName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,4740,dbName = dbName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,4773,tableName = tableName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,4774,dbName = dbName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,4808,tableName = tableName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,4809,dbName = dbName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,4810,columnName = columnName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,4842,tableName = tableName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,4843,dbName = dbName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,4844,columnName = columnName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,5995,return new GetStatHelper(dbName.toLowerCase(), tableName.toLowerCase(), allowSql, allowJdo) {
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6212,dbName.trim(), tableName.trim(), colName.trim());
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6223,dbName.trim(), tableName.trim());
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6289,mStatsObj = (MTableColumnStatistics)query.execute(tableName.trim(),
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6290,dbName.trim(), colName.trim());
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6300,mStatsObjColl= (List<MTableColumnStatistics>)query.execute(tableName.trim(), dbName.trim());
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6679,dbName = dbName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6680,tableName = tableName.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6812,funcName = funcName.toLowerCase();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6813,dbName = dbName.toLowerCase();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6825,oldf.setFunctionName(newf.getFunctionName().toLowerCase());
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6866,db = db.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6867,function = function.toLowerCase().trim();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,6905,dbName = dbName.toLowerCase().trim();
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/SettableConfigUpdater.java,47,if(whiteListParamsStr == null && whiteListParamsStr.trim().isEmpty()) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUtils.java,140,TypeInfo commonTypeInfo = FunctionRegistry.getCommonClass(oiTypeInfo,
hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/JsonSerDe.java,515,case VARCHAR:
hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/JsonSerDe.java,516,appendWithQuotes(sb,
hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/JsonSerDe.java,517,((HiveVarcharObjectInspector)poi).getPrimitiveJavaObject(o).toString());
hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/JsonSerDe.java,519,case CHAR:
hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/JsonSerDe.java,522,appendWithQuotes(sb, ((HiveCharObjectInspector)poi).getPrimitiveJavaObject(o).toString());
jdbc/src/java/org/apache/hive/jdbc/HiveBaseResultSet.java,88,if (columnIndex==-1) {
jdbc/src/java/org/apache/hive/jdbc/HiveBaseResultSet.java,89,throw new SQLException();
jdbc/src/java/org/apache/hive/jdbc/HiveBaseResultSet.java,91,return ++columnIndex;
jdbc/src/java/org/apache/hive/jdbc/HiveBaseResultSet.java,87,int columnIndex = columnNames.indexOf(columnName);
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseStorageHandler.java,453,User.getCurrent().obtainAuthTokenForJob(conf,new Job(conf));
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorUtils.java,956,if (childFieldsList1.size() != childFieldsList2.size()) {
service/src/java/org/apache/hive/service/cli/thrift/ThriftBinaryCLIService.java,82,.protocolFactory(new TBinaryProtocol.Factory()).executorService(executorService);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/SkewJoinOptimizer.java,178,List<TableScanOperator> tableScanCloneOpsForJoin =
ql/src/java/org/apache/hadoop/hive/ql/optimizer/SkewJoinOptimizer.java,179,new ArrayList<TableScanOperator>();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/SkewJoinOptimizer.java,180,assert
ql/src/java/org/apache/hadoop/hive/ql/optimizer/SkewJoinOptimizer.java,181,getTableScanOpsForJoin(joinOpClone, tableScanCloneOpsForJoin);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,318,for(int i = 0; i < props.length; i++) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,319,props[i] = TempletonUtils.unEscapeString(props[i]);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,321,return Arrays.asList(props);
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,385,in = new FileInputStream(localPath.toUri().getPath());
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,135,return (conf.get(AcidUtils.CONF_ACID_KEY) != null) || AcidUtils.isAcid(path, conf);
ql/src/java/org/apache/hadoop/hive/ql/exec/OrcFileMergeOperator.java,35,import java.io.IOException;
ql/src/java/org/apache/hadoop/hive/ql/exec/OrcFileMergeOperator.java,85,LOG.info("ORC merge file input path: " + k.getInputPath());
ql/src/java/org/apache/hadoop/hive/ql/exec/OrcFileMergeOperator.java,101,LOG.info("ORC merge file output path: " + outPath);
ql/src/java/org/apache/hadoop/hive/ql/exec/OrcFileMergeOperator.java,124,LOG.info("Merged stripe from file " + k.getInputPath() + " [ offset : "
ql/src/java/org/apache/hadoop/hive/ql/exec/OrcFileMergeOperator.java,125,+ v.getStripeInformation().getOffset() + " length: "
ql/src/java/org/apache/hadoop/hive/ql/exec/OrcFileMergeOperator.java,126,+ v.getStripeInformation().getLength() + " ]");
ql/src/java/org/apache/hadoop/hive/ql/exec/OrcFileMergeOperator.java,134,closeOp(true);
ql/src/java/org/apache/hadoop/hive/ql/exec/OrcFileMergeOperator.java,152,LOG.info("Incompatible ORC file merge! Column counts does not match for "
ql/src/java/org/apache/hadoop/hive/ql/exec/OrcFileMergeOperator.java,158,LOG.info("Incompatible ORC file merge! Compression codec does not match" +
ql/src/java/org/apache/hadoop/hive/ql/exec/OrcFileMergeOperator.java,164,LOG.info("Incompatible ORC file merge! Compression buffer size does not" +
ql/src/java/org/apache/hadoop/hive/ql/exec/OrcFileMergeOperator.java,171,LOG.info("Incompatible ORC file merge! Version does not match for "
ql/src/java/org/apache/hadoop/hive/ql/exec/OrcFileMergeOperator.java,177,LOG.info("Incompatible ORC file merge! Row index stride does not match" +
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,60,BooleanStatisticsImpl bkt = (BooleanStatisticsImpl) other;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,61,trueCount += bkt.trueCount;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,151,IntegerStatisticsImpl otherInt = (IntegerStatisticsImpl) other;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,152,if (!hasMinimum) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,153,hasMinimum = otherInt.hasMinimum;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,154,minimum = otherInt.minimum;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,155,maximum = otherInt.maximum;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,157,if (otherInt.minimum < minimum) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,160,if (otherInt.maximum > maximum) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,164,super.merge(other);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,165,overflow |= otherInt.overflow;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,166,if (!overflow) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,167,boolean wasPositive = sum >= 0;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,168,sum += otherInt.sum;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,169,if ((otherInt.sum >= 0) == wasPositive) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,170,overflow = (sum >= 0) != wasPositive;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,278,super.merge(other);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,279,DoubleStatisticsImpl dbl = (DoubleStatisticsImpl) other;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,280,if (!hasMinimum) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,281,hasMinimum = dbl.hasMinimum;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,282,minimum = dbl.minimum;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,283,maximum = dbl.maximum;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,285,if (dbl.minimum < minimum) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,288,if (dbl.maximum > maximum) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,292,sum += dbl.sum;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,384,super.merge(other);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,385,StringStatisticsImpl str = (StringStatisticsImpl) other;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,386,if (minimum == null) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,387,if(str.minimum != null) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,388,maximum = new Text(str.getMaximum());
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,389,minimum = new Text(str.getMinimum());
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,392,maximum = minimum = null;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,395,if (minimum.compareTo(str.minimum) > 0) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,396,minimum = new Text(str.getMinimum());
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,398,if (maximum.compareTo(str.maximum) < 0) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,399,maximum = new Text(str.getMaximum());
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,402,sum += str.sum;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,479,BinaryStatisticsImpl bin = (BinaryStatisticsImpl) other;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,480,sum += bin.sum;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,558,super.merge(other);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,559,DecimalStatisticsImpl dec = (DecimalStatisticsImpl) other;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,560,if (minimum == null) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,561,minimum = dec.minimum;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,562,maximum = dec.maximum;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,563,sum = dec.sum;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,565,if (minimum.compareTo(dec.minimum) > 0) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,568,if (maximum.compareTo(dec.maximum) < 0) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,571,if (sum == null || dec.sum == null) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,572,sum = null;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,574,sum = sum.add(dec.sum);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,584,if (getNumberOfValues() != 0) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,668,super.merge(other);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,669,DateStatisticsImpl dateStats = (DateStatisticsImpl) other;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,670,if (minimum == null) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,671,minimum = dateStats.minimum;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,672,maximum = dateStats.maximum;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,674,if (minimum > dateStats.minimum) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,677,if (maximum < dateStats.maximum) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,688,if (getNumberOfValues() != 0) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,765,super.merge(other);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,766,TimestampStatisticsImpl timestampStats = (TimestampStatisticsImpl) other;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,767,if (minimum == null) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,768,minimum = timestampStats.minimum;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,769,maximum = timestampStats.maximum;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,771,if (minimum > timestampStats.minimum) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,774,if (maximum < timestampStats.maximum) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ColumnStatisticsImpl.java,785,if (getNumberOfValues() != 0) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFileKeyWrapper.java,36,protected Path inputPath;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFileKeyWrapper.java,37,protected CompressionKind compression;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFileKeyWrapper.java,38,protected long compressBufferSize;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFileKeyWrapper.java,39,protected List<OrcProto.Type> types;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFileKeyWrapper.java,40,protected int rowIndexStride;
serde/src/java/org/apache/hadoop/hive/serde2/ColumnProjectionUtils.java,109,appendReadColumns(readColumnsBuffer, ids);
serde/src/java/org/apache/hadoop/hive/serde2/ColumnProjectionUtils.java,110,appendReadColumnNames(readColumnNamesBuffer, names);
serde/src/java/org/apache/hadoop/hive/serde2/ColumnProjectionUtils.java,113,public static void appendReadColumns(StringBuilder readColumnsBuffer, List<Integer> ids) {
serde/src/java/org/apache/hadoop/hive/serde2/ColumnProjectionUtils.java,114,String id = toReadColumnIDString(ids);
serde/src/java/org/apache/hadoop/hive/serde2/ColumnProjectionUtils.java,115,String newConfStr = id;
serde/src/java/org/apache/hadoop/hive/serde2/ColumnProjectionUtils.java,116,if (readColumnsBuffer.length() > 0) {
serde/src/java/org/apache/hadoop/hive/serde2/ColumnProjectionUtils.java,117,readColumnsBuffer.append(StringUtils.COMMA_STR).append(newConfStr);
serde/src/java/org/apache/hadoop/hive/serde2/ColumnProjectionUtils.java,119,if (readColumnsBuffer.length() == 0) {
serde/src/java/org/apache/hadoop/hive/serde2/ColumnProjectionUtils.java,120,readColumnsBuffer.append(READ_COLUMN_IDS_CONF_STR_DEFAULT);
serde/src/java/org/apache/hadoop/hive/serde2/ColumnProjectionUtils.java,124,private static void appendReadColumnNames(StringBuilder readColumnNamesBuffer, List<String> cols) {
serde/src/java/org/apache/hadoop/hive/serde2/ColumnProjectionUtils.java,125,boolean first = readColumnNamesBuffer.length() > 0;
serde/src/java/org/apache/hadoop/hive/serde2/ColumnProjectionUtils.java,126,for(String col: cols) {
serde/src/java/org/apache/hadoop/hive/serde2/ColumnProjectionUtils.java,127,if (first) {
serde/src/java/org/apache/hadoop/hive/serde2/ColumnProjectionUtils.java,128,first = false;
serde/src/java/org/apache/hadoop/hive/serde2/ColumnProjectionUtils.java,130,readColumnNamesBuffer.append(',');
serde/src/java/org/apache/hadoop/hive/serde2/ColumnProjectionUtils.java,132,readColumnNamesBuffer.append(col);
ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java,421,if (buckNum < 0) {
ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java,422,buckNum = -1 * buckNum;
ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java,425,return buckNum % numBuckets;
ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java,353,Map<String, ArrayList<String>> pa = getPathToAliases();
ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java,354,if (pa != null) {
ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java,355,for (List<String> ls : pa.values()) {
ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java,356,for (String a : ls) {
ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java,357,Operator<?> op = getAliasToWork().get(a);
ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java,358,if (op != null ) {
ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java,359,opSet.add(op);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFileStripeMergeRecordReader.java,82,valueWrapper.setStripeStatistics(stripeStatistics.get(stripeIdx++));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,115,ReduceSinkOperator rs = (ReduceSinkOperator)parentOp;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,139,convertJoinSMBJoin(joinOp, context, 0, 0, false, false);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,140,return null;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,165,mapJoinOp.setOpTraits(new OpTraits(null, -1, null));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,179,if (context.conf.getBoolVar(HiveConf.ConfVars.HIVE_AUTO_SORTMERGE_JOIN) == false) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,252,OpTraits opTraits =
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,254,.getSortCols());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,290,List<Operator<? extends OperatorDesc>> newParentOpList =
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,291,new ArrayList<Operator<? extends OperatorDesc>>();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,329,currentOp.setOpTraits(new OpTraits(null, -1, null));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,352,tezBucketJoinProcCtx.getNumBuckets(), null);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,379,.getNumBuckets();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,432,throws SemanticException {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,488,if (((ExprNodeColumnDesc)exprNodeDesc).getColumn().equals(listBucketCols.get(colCount))) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,527,LOG.warn("Couldn't get statistics from: "+parentOp);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,532,if ((bigInputStat == null) ||
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,533,((bigInputStat != null) &&
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,534,(inputSize > bigInputStat.getDataSize()))) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,632,for (Operator<? extends OperatorDesc> op : mapJoinOp.getParentOperators()) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,646,for (Operator<?> op: parent.getChildOperators()) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,113,OpTraits opTraits = new OpTraits(listBucketCols, numBuckets, listBucketCols);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,125,public boolean checkBucketedTable(Table tbl,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,126,ParseContext pGraphContext,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,134,List<String> fileNames =
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,135,AbstractBucketJoinProc.getBucketFilePathsOfPartition(p.getDataLocation(), pGraphContext);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,146,List<String> fileNames =
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,147,AbstractBucketJoinProc.getBucketFilePathsOfPartition(tbl.getDataLocation(), pGraphContext);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,186,OpTraits opTraits = new OpTraits(bucketColsList, numBuckets, sortedColsList);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,212,OpTraits opTraits = new OpTraits(listBucketCols, -1, listBucketCols);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,220,public List<List<String>> getConvertedColNames(List<List<String>> parentColNames,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,221,SelectOperator selOp) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,247,SelectOperator selOp = (SelectOperator)nd;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,248,List<List<String>> parentBucketColNames =
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,258,.getSortCols();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,265,if (selOp.getParentOperators().get(0).getOpTraits() != null) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,266,numBuckets = selOp.getParentOperators().get(0).getOpTraits().getNumBuckets();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,268,OpTraits opTraits = new OpTraits(listBucketCols, numBuckets, listSortCols);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,279,JoinOperator joinOp = (JoinOperator)nd;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,288,ReduceSinkOperator rsOp = (ReduceSinkOperator)parentOp;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,293,bucketColsList.add(getOutputColNames(joinOp, rsOp.getOpTraits().getBucketColNames(), pos));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,294,sortColsList.add(getOutputColNames(joinOp, rsOp.getOpTraits().getSortCols(), pos));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,298,joinOp.setOpTraits(new OpTraits(bucketColsList, -1, bucketColsList));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,314,if(((ExprNodeColumnDesc)(exprNode)).getColumn().equals(colName)) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,349,OpTraits opTraits = new OpTraits(null, -1, null);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,351,Operator<? extends OperatorDesc> operator = (Operator<? extends OperatorDesc>)nd;
ql/src/java/org/apache/hadoop/hive/ql/plan/OpTraits.java,29,public OpTraits(List<List<String>> bucketColNames, int numBuckets, List<List<String>> sortColNames) {
cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java,129,this.processFile(cmd_1);
ql/src/java/org/apache/hadoop/hive/ql/plan/MergeJoinWork.java,22,import java.util.HashMap;
service/src/java/org/apache/hive/service/server/ThreadFactoryWithGarbageCleanup.java,22,import java.util.HashMap;
service/src/java/org/apache/hive/service/server/ThreadFactoryWithGarbageCleanup.java,46,private static Map<Long, RawStore> threadRawStoreMap = new HashMap<Long, RawStore>();
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,75,import com.google.common.annotations.VisibleForTesting;
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,1247,normalizeColSpec(partSpec, astKeyName, colType, colSpec, convertedValue);
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,1252,static void normalizeColSpec(Map<String, String> partSpec, String colName,
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,1253,String colType, String originalColSpec, Object colValue) throws SemanticException {
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,1254,if (colValue == null) return; // nothing to do with nulls
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,1255,String normalizedColSpec = originalColSpec;
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,1256,if (colType.equals(serdeConstants.DATE_TYPE_NAME)) {
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,1257,normalizedColSpec = normalizeDateCol(colValue, originalColSpec);
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,1259,if (!normalizedColSpec.equals(originalColSpec)) {
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,1260,STATIC_LOG.warn("Normalizing partition spec - " + colName + " from "
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,1261,+ originalColSpec + " to " + normalizedColSpec);
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,1262,partSpec.put(colName, normalizedColSpec);
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,1266,private static String normalizeDateCol(
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,1267,Object colValue, String originalColSpec) throws SemanticException {
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,1268,Date value;
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,1269,if (colValue instanceof DateWritable) {
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,1270,value = ((DateWritable) colValue).get();
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,1272,value = (Date) colValue;
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,1274,throw new SemanticException("Unexpected date type " + colValue.getClass());
ql/src/java/org/apache/hadoop/hive/ql/parse/BaseSemanticAnalyzer.java,1276,return HiveMetaStore.PARTITION_DATE_FORMAT.format(value);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1473,if (childCount != partition.size()) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1477,Table table = null;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1478,try {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1479,table = db.getTable(tableName);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1481,throw new SemanticException(ex);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1483,try {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1484,Partition parMetaData = db.getPartition(table, partition, false);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1486,if (parMetaData != null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1487,phase1Result = false;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1488,skipRecursion = true;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1489,LOG.info("Partition already exists so insert into overwrite " +
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1491,break;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1494,LOG.info("Error while getting metadata : ", e);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1496,validatePartSpec(table, partition, (ASTNode)tab, conf, false);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java,321,transport = new TSocket(store.getHost(), store.getPort(), clientSocketTimeout);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/FileDump.java,20,import java.util.ArrayList;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/FileDump.java,21,import java.util.List;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/FileDump.java,32,import org.apache.hadoop.hive.ql.io.sarg.PredicateLeaf;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/FileDump.java,33,import org.apache.hadoop.hive.ql.io.sarg.SearchArgument.TruthValue;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,2838,if (stream.getKind() == OrcProto.Stream.Kind.PRESENT) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,2846,if (StreamName.getArea(streamKind) == StreamName.Area.DATA &&
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,2984,StreamName.getArea(streamDesc.getKind()) == StreamName.Area.DATA) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java,3220,if (stream.getKind() == OrcProto.Stream.Kind.ROW_INDEX) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,224,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_IntegerStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,229,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_IntegerStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,431,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_IntegerStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,436,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_IntegerStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,476,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_IntegerStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,782,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DoubleStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,787,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DoubleStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,989,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DoubleStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,994,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DoubleStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,1034,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DoubleStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,1358,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StringStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,1363,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StringStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,1627,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StringStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,1632,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StringStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,1672,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StringStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,2073,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BucketStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,2078,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BucketStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,2253,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BucketStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,2258,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BucketStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,2294,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BucketStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,2574,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DecimalStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,2579,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DecimalStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,2862,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DecimalStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,2867,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DecimalStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,2907,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DecimalStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,3335,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DateStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,3340,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DateStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,3526,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DateStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,3531,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DateStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,3569,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_DateStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,3844,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_TimestampStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,3849,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_TimestampStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4035,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_TimestampStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4040,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_TimestampStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4078,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_TimestampStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4338,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BinaryStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4343,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BinaryStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4505,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BinaryStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4510,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BinaryStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4546,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_BinaryStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4974,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,4979,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5373,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5378,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,5470,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6740,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndexEntry_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6745,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndexEntry_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6951,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndexEntry_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6956,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndexEntry_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,6999,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndexEntry_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7385,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndex_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7390,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndex_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7563,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndex_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7568,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndex_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,7609,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndex_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8074,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Stream_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8079,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Stream_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8290,if (!hasKind()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8291,memoizedIsInitialized = 0;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8292,return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8417,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Stream_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8422,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Stream_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8462,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Stream_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8523,if (!hasKind()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8525,return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8766,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnEncoding_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8771,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnEncoding_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8933,if (!hasKind()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8934,memoizedIsInitialized = 0;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,8935,return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9053,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnEncoding_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9058,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnEncoding_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9096,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnEncoding_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9150,if (!hasKind()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9152,return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9396,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeFooter_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9401,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeFooter_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9502,for (int i = 0; i < getStreamsCount(); i++) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9503,if (!getStreams(i).isInitialized()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9504,memoizedIsInitialized = 0;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9505,return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9508,for (int i = 0; i < getColumnsCount(); i++) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9509,if (!getColumns(i).isInitialized()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9510,memoizedIsInitialized = 0;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9511,return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9630,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeFooter_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9635,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeFooter_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9683,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeFooter_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9791,for (int i = 0; i < getStreamsCount(); i++) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9792,if (!getStreams(i).isInitialized()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9794,return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9797,for (int i = 0; i < getColumnsCount(); i++) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9798,if (!getColumns(i).isInitialized()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,9800,return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10519,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Type_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10524,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Type_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10902,if (!hasKind()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10903,memoizedIsInitialized = 0;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,10904,return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11069,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Type_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11074,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Type_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11120,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Type_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11219,if (!hasKind()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11221,return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11693,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11698,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11948,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11953,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,11997,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12373,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_UserMetadataItem_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12378,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_UserMetadataItem_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12467,if (!hasName()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12468,memoizedIsInitialized = 0;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12469,return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12471,if (!hasValue()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12472,memoizedIsInitialized = 0;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12473,return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12591,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_UserMetadataItem_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12596,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_UserMetadataItem_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12634,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_UserMetadataItem_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12690,if (!hasName()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12692,return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12694,if (!hasValue()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12696,return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12945,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,12950,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13123,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13128,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeStatistics_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13169,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_StripeStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13619,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Metadata_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13624,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Metadata_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13797,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Metadata_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13802,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Metadata_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,13843,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Metadata_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14461,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Footer_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14466,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Footer_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14710,for (int i = 0; i < getTypesCount(); i++) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14711,if (!getTypes(i).isInitialized()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14712,memoizedIsInitialized = 0;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14713,return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14716,for (int i = 0; i < getMetadataCount(); i++) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14717,if (!getMetadata(i).isInitialized()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14718,memoizedIsInitialized = 0;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14719,return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14880,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Footer_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14885,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Footer_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,14955,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_Footer_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15163,for (int i = 0; i < getTypesCount(); i++) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15164,if (!getTypes(i).isInitialized()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15166,return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15169,for (int i = 0; i < getMetadataCount(); i++) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15170,if (!getMetadata(i).isInitialized()) {
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,15172,return false;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16553,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_PostScript_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16558,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_PostScript_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16951,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_PostScript_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,16956,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_PostScript_fieldAccessorTable
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17004,return org.apache.hadoop.hive.ql.io.orc.OrcProto.internal_static_org_apache_hadoop_hive_ql_io_orc_PostScript_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17535,internal_static_org_apache_hadoop_hive_ql_io_orc_IntegerStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17538,internal_static_org_apache_hadoop_hive_ql_io_orc_IntegerStatistics_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17540,internal_static_org_apache_hadoop_hive_ql_io_orc_DoubleStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17543,internal_static_org_apache_hadoop_hive_ql_io_orc_DoubleStatistics_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17545,internal_static_org_apache_hadoop_hive_ql_io_orc_StringStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17548,internal_static_org_apache_hadoop_hive_ql_io_orc_StringStatistics_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17550,internal_static_org_apache_hadoop_hive_ql_io_orc_BucketStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17553,internal_static_org_apache_hadoop_hive_ql_io_orc_BucketStatistics_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17555,internal_static_org_apache_hadoop_hive_ql_io_orc_DecimalStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17558,internal_static_org_apache_hadoop_hive_ql_io_orc_DecimalStatistics_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17560,internal_static_org_apache_hadoop_hive_ql_io_orc_DateStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17563,internal_static_org_apache_hadoop_hive_ql_io_orc_DateStatistics_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17565,internal_static_org_apache_hadoop_hive_ql_io_orc_TimestampStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17568,internal_static_org_apache_hadoop_hive_ql_io_orc_TimestampStatistics_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17570,internal_static_org_apache_hadoop_hive_ql_io_orc_BinaryStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17573,internal_static_org_apache_hadoop_hive_ql_io_orc_BinaryStatistics_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17575,internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17578,internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnStatistics_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17580,internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndexEntry_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17583,internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndexEntry_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17585,internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndex_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17588,internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndex_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17590,internal_static_org_apache_hadoop_hive_ql_io_orc_Stream_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17593,internal_static_org_apache_hadoop_hive_ql_io_orc_Stream_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17595,internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnEncoding_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17598,internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnEncoding_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17600,internal_static_org_apache_hadoop_hive_ql_io_orc_StripeFooter_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17603,internal_static_org_apache_hadoop_hive_ql_io_orc_StripeFooter_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17605,internal_static_org_apache_hadoop_hive_ql_io_orc_Type_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17608,internal_static_org_apache_hadoop_hive_ql_io_orc_Type_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17610,internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17613,internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17615,internal_static_org_apache_hadoop_hive_ql_io_orc_UserMetadataItem_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17618,internal_static_org_apache_hadoop_hive_ql_io_orc_UserMetadataItem_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17620,internal_static_org_apache_hadoop_hive_ql_io_orc_StripeStatistics_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17623,internal_static_org_apache_hadoop_hive_ql_io_orc_StripeStatistics_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17625,internal_static_org_apache_hadoop_hive_ql_io_orc_Metadata_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17628,internal_static_org_apache_hadoop_hive_ql_io_orc_Metadata_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17630,internal_static_org_apache_hadoop_hive_ql_io_orc_Footer_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17633,internal_static_org_apache_hadoop_hive_ql_io_orc_Footer_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17635,internal_static_org_apache_hadoop_hive_ql_io_orc_PostScript_descriptor;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17638,internal_static_org_apache_hadoop_hive_ql_io_orc_PostScript_fieldAccessorTable;
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17737,internal_static_org_apache_hadoop_hive_ql_io_orc_IntegerStatistics_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17739,internal_static_org_apache_hadoop_hive_ql_io_orc_IntegerStatistics_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17741,internal_static_org_apache_hadoop_hive_ql_io_orc_IntegerStatistics_descriptor,
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17743,internal_static_org_apache_hadoop_hive_ql_io_orc_DoubleStatistics_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17745,internal_static_org_apache_hadoop_hive_ql_io_orc_DoubleStatistics_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17747,internal_static_org_apache_hadoop_hive_ql_io_orc_DoubleStatistics_descriptor,
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17749,internal_static_org_apache_hadoop_hive_ql_io_orc_StringStatistics_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17751,internal_static_org_apache_hadoop_hive_ql_io_orc_StringStatistics_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17753,internal_static_org_apache_hadoop_hive_ql_io_orc_StringStatistics_descriptor,
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17755,internal_static_org_apache_hadoop_hive_ql_io_orc_BucketStatistics_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17757,internal_static_org_apache_hadoop_hive_ql_io_orc_BucketStatistics_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17759,internal_static_org_apache_hadoop_hive_ql_io_orc_BucketStatistics_descriptor,
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17761,internal_static_org_apache_hadoop_hive_ql_io_orc_DecimalStatistics_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17763,internal_static_org_apache_hadoop_hive_ql_io_orc_DecimalStatistics_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17765,internal_static_org_apache_hadoop_hive_ql_io_orc_DecimalStatistics_descriptor,
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17767,internal_static_org_apache_hadoop_hive_ql_io_orc_DateStatistics_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17769,internal_static_org_apache_hadoop_hive_ql_io_orc_DateStatistics_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17771,internal_static_org_apache_hadoop_hive_ql_io_orc_DateStatistics_descriptor,
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17773,internal_static_org_apache_hadoop_hive_ql_io_orc_TimestampStatistics_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17775,internal_static_org_apache_hadoop_hive_ql_io_orc_TimestampStatistics_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17777,internal_static_org_apache_hadoop_hive_ql_io_orc_TimestampStatistics_descriptor,
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17779,internal_static_org_apache_hadoop_hive_ql_io_orc_BinaryStatistics_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17781,internal_static_org_apache_hadoop_hive_ql_io_orc_BinaryStatistics_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17783,internal_static_org_apache_hadoop_hive_ql_io_orc_BinaryStatistics_descriptor,
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17785,internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnStatistics_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17787,internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnStatistics_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17789,internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnStatistics_descriptor,
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17791,internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndexEntry_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17793,internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndexEntry_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17795,internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndexEntry_descriptor,
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17797,internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndex_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17799,internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndex_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17801,internal_static_org_apache_hadoop_hive_ql_io_orc_RowIndex_descriptor,
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17803,internal_static_org_apache_hadoop_hive_ql_io_orc_Stream_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17805,internal_static_org_apache_hadoop_hive_ql_io_orc_Stream_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17807,internal_static_org_apache_hadoop_hive_ql_io_orc_Stream_descriptor,
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17809,internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnEncoding_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17811,internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnEncoding_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17813,internal_static_org_apache_hadoop_hive_ql_io_orc_ColumnEncoding_descriptor,
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17815,internal_static_org_apache_hadoop_hive_ql_io_orc_StripeFooter_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17817,internal_static_org_apache_hadoop_hive_ql_io_orc_StripeFooter_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17819,internal_static_org_apache_hadoop_hive_ql_io_orc_StripeFooter_descriptor,
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17821,internal_static_org_apache_hadoop_hive_ql_io_orc_Type_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17823,internal_static_org_apache_hadoop_hive_ql_io_orc_Type_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17825,internal_static_org_apache_hadoop_hive_ql_io_orc_Type_descriptor,
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17827,internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17829,internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17831,internal_static_org_apache_hadoop_hive_ql_io_orc_StripeInformation_descriptor,
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17833,internal_static_org_apache_hadoop_hive_ql_io_orc_UserMetadataItem_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17835,internal_static_org_apache_hadoop_hive_ql_io_orc_UserMetadataItem_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17837,internal_static_org_apache_hadoop_hive_ql_io_orc_UserMetadataItem_descriptor,
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17839,internal_static_org_apache_hadoop_hive_ql_io_orc_StripeStatistics_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17841,internal_static_org_apache_hadoop_hive_ql_io_orc_StripeStatistics_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17843,internal_static_org_apache_hadoop_hive_ql_io_orc_StripeStatistics_descriptor,
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17845,internal_static_org_apache_hadoop_hive_ql_io_orc_Metadata_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17847,internal_static_org_apache_hadoop_hive_ql_io_orc_Metadata_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17849,internal_static_org_apache_hadoop_hive_ql_io_orc_Metadata_descriptor,
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17851,internal_static_org_apache_hadoop_hive_ql_io_orc_Footer_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17853,internal_static_org_apache_hadoop_hive_ql_io_orc_Footer_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17855,internal_static_org_apache_hadoop_hive_ql_io_orc_Footer_descriptor,
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17857,internal_static_org_apache_hadoop_hive_ql_io_orc_PostScript_descriptor =
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17859,internal_static_org_apache_hadoop_hive_ql_io_orc_PostScript_fieldAccessorTable = new
ql/src/gen/protobuf/gen-java/org/apache/hadoop/hive/ql/io/orc/OrcProto.java,17861,internal_static_org_apache_hadoop_hive_ql_io_orc_PostScript_descriptor,
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreSchemaInfo.java,48,ImmutableMap.of("0.13.1", "0.13.0");
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,39,import org.apache.hadoop.hive.ql.metadata.HiveException;
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,40,import org.apache.hadoop.hive.ql.session.SessionState;
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,79,try {
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,80,applyAuthorizationConfigPolicy(hiveConf);
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,82,throw new RuntimeException("Error applying authorization policy on hive configuration", e);
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,119,private void applyAuthorizationConfigPolicy(HiveConf newHiveConf) throws HiveException {
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,122,SessionState ss = new SessionState(newHiveConf);
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,123,ss.setIsHiveServerQuery(true);
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,124,SessionState.start(ss);
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,125,ss.applyAuthorizationPolicy();
ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java,671,public static FunctionInfo getFunctionInfo(String functionName) {
ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java,672,return getFunctionInfo(mFunctions, functionName);
ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java,746,FunctionInfo funcInfo = getFunctionInfo(funcName);
ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java,1221,public static GenericUDAFResolver getGenericUDAFResolver(String functionName) {
ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java,1520,return FunctionRegistry.getFunctionInfo("index").getGenericUDF();
ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java,1527,return FunctionRegistry.getFunctionInfo("and").getGenericUDF();
ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java,1900,public static boolean impliesOrder(String functionName) {
ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java,1969,public static boolean isRankingFunction(String name) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,759,FunctionInfo funcInfo = FunctionRegistry.getFunctionInfo(udfName);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/translator/SqlFunctionConverter.java,114,FunctionInfo hFn = name != null ? FunctionRegistry.getFunctionInfo(name) : null;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/translator/SqlFunctionConverter.java,116,hFn = handleExplicitCast(op, dt);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/translator/SqlFunctionConverter.java,120,private static FunctionInfo handleExplicitCast(SqlOperator op, RelDataType dt) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/translator/SqlFunctionConverter.java,286,FunctionInfo hFn = FunctionRegistry.getFunctionInfo(name);
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,3002,String fn, ExprNodeDesc left, ExprNodeDesc right) {
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,3003,return new ExprNodeGenericFuncDesc(TypeInfoFactory.booleanTypeInfo,
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,3004,FunctionRegistry.getFunctionInfo(fn).getGenericUDF(), Lists.newArrayList(left, right));
ql/src/java/org/apache/hadoop/hive/ql/parse/FunctionSemanticAnalyzer.java,60,analyzeDropFunction(ast);
ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java,680,FunctionInfo fi = FunctionRegistry.getFunctionInfo(udfName);
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingMetaStoreClient.java,100,e.getCause().getMessage().matches("JDO[a-zA-Z]*Exception")) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/MetadataOnlyOptimizer.java,137,|| (walkerCtx.getMayBeMetadataOnlyTableScans().size() > 1)) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java,24,import java.util.Iterator;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java,104,private void processAlias(MapWork work, String alias) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java,105,List<String> paths = getPathsForAlias(work, alias);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java,106,if (paths.isEmpty()) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java,109,return;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java,111,work.setUseOneNullRowInputFormat(true);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java,114,PartitionDesc aliasPartn = work.getAliasToPartnInfo().get(alias);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java,115,changePartitionToMetadataOnly(aliasPartn);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java,118,for (String path : paths) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java,119,PartitionDesc partDesc = work.getPathToPartitionInfo().get(path);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java,123,+ encode(newPartition.getPartSpec()));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java,124,work.getPathToPartitionInfo().remove(path);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java,126,ArrayList<String> aliases = work.getPathToAliases().remove(path);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java,127,work.getPathToAliases().put(fakePath.getName(), aliases);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java,180,Iterator<TableScanOperator> iterator
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java,181,= walkerCtx.getMetadataOnlyTableScans().iterator();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java,183,while (iterator.hasNext()) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java,184,TableScanOperator tso = iterator.next();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java,185,tso.getConf().setIsMetadataOnly(true);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java,186,String alias = getAliasForTableScanOperator(mapWork, tso);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java,187,LOG.info("Null table scan for " + alias);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java,188,processAlias(mapWork, alias);
common/src/java/org/apache/hadoop/hive/common/ValidTxnListImpl.java,23,public class ValidTxnListImpl implements ValidTxnList {
common/src/java/org/apache/hadoop/hive/common/ValidTxnListImpl.java,25,private long[] exceptions;
common/src/java/org/apache/hadoop/hive/common/ValidTxnListImpl.java,26,private long highWatermark;
common/src/java/org/apache/hadoop/hive/common/ValidTxnListImpl.java,28,public ValidTxnListImpl() {
common/src/java/org/apache/hadoop/hive/common/ValidTxnListImpl.java,32,public ValidTxnListImpl(long[] exceptions, long highWatermark) {
common/src/java/org/apache/hadoop/hive/common/ValidTxnListImpl.java,42,public ValidTxnListImpl(String value) {
common/src/java/org/apache/hadoop/hive/common/ValidTxnListImpl.java,47,public boolean isTxnCommitted(long txnid) {
common/src/java/org/apache/hadoop/hive/common/ValidTxnListImpl.java,55,public RangeResponse isTxnRangeCommitted(long minTxnId, long maxTxnId) {
common/src/java/org/apache/hadoop/hive/common/ValidTxnListImpl.java,122,public long[] getOpenTransactions() {
common/src/java/org/apache/hadoop/hive/common/ValidTxnList.java,46,public boolean isTxnCommitted(long txnid);
common/src/java/org/apache/hadoop/hive/common/ValidTxnList.java,55,public RangeResponse isTxnRangeCommitted(long minTxnId, long maxTxnId);
common/src/java/org/apache/hadoop/hive/common/ValidTxnList.java,81,public long[] getOpenTransactions();
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java,1691,return TxnHandler.createValidTxnList(client.get_open_txns(), 0);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java,1696,return TxnHandler.createValidTxnList(client.get_open_txns(), currentTxn);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,32,import org.apache.hadoop.hive.common.ValidTxnListImpl;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,247,public static ValidTxnList createValidTxnList(GetOpenTxnsResponse txns, long currentTxn) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,256,return new ValidTxnListImpl(exceptions, highWater);
ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java,57,return path.getName().startsWith(BUCKET_PREFIX);
ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java,372,if (txnList.isTxnRangeCommitted(delta.minTransaction,
ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java,405,if (txnList.isTxnRangeCommitted(current+1, next.maxTransaction) !=
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,42,import org.apache.hadoop.hive.common.ValidTxnListImpl;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,411,transactionList = new ValidTxnListImpl(value);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1138,ValidTxnList validTxnList = new ValidTxnListImpl(txnString);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java,578,if (!validTxnList.isTxnCommitted(
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRecordUpdater.java,132,return new Path(main + "_flush_length");
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DummyTxnManager.java,23,import org.apache.hadoop.hive.common.ValidTxnListImpl;
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DummyTxnManager.java,199,return new ValidTxnListImpl();
ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java,26,import org.apache.hadoop.hive.common.ValidTxnListImpl;
ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java,186,final ValidTxnList txnList = new ValidTxnListImpl();
ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java,28,import org.apache.hadoop.hive.common.ValidTxnListImpl;
ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java,161,LOG.error("No delta files found to compact in " + sd.getLocation());
ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java,507,new ValidTxnListImpl(jobConf.get(ValidTxnList.VALID_TXNS_KEY));
ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java,79,ValidTxnList txns = TxnHandler.createValidTxnList(txnHandler.getOpenTxns(), 0);
ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java,123,TxnHandler.createValidTxnList(txnHandler.getOpenTxns(), 0);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,240,db.setParameters(dbParams);
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,144,import com.google.common.collect.Maps;
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,574,db.setParameters(mdb.getParameters());
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,1021,private <K, V> Map<K, V> convertMap(Map<K, V> dnMap) {
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,1022,return (dnMap == null) ? null : Maps.newHashMap(dnMap);
ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java,171,List<org.apache.hadoop.hive.ql.metadata.Partition> partitions) throws SemanticException, IOException {
ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java,173,JSONObject jsonContainer = new JSONObject();
ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java,174,jsonContainer.put("version", METADATA_FORMAT_VERSION);
ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java,175,if (METADATA_FORMAT_FORWARD_COMPATIBLE_VERSION != null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java,176,jsonContainer.put("fcversion", METADATA_FORMAT_FORWARD_COMPATIBLE_VERSION);
ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java,178,TSerializer serializer = new TSerializer(new TJSONProtocol.Factory());
ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java,179,try {
ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java,180,String tableDesc = serializer.toString(tableHandle.getTTable(), "UTF-8");
ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java,181,jsonContainer.put("table", tableDesc);
ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java,182,JSONArray jsonPartitions = new JSONArray();
ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java,183,if (partitions != null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java,184,for (org.apache.hadoop.hive.ql.metadata.Partition partition : partitions) {
ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java,185,String partDesc = serializer.toString(partition.getTPartition(), "UTF-8");
ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java,186,jsonPartitions.put(partDesc);
ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java,189,jsonContainer.put("partitions", jsonPartitions);
ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java,191,throw new SemanticException(
ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java,192,ErrorMsg.GENERIC_ERROR
ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java,193,.getMsg("Exception while serializing the metastore objects"), e);
ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java,195,OutputStream out = fs.create(metadataPath);
ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java,196,out.write(jsonContainer.toString().getBytes("UTF-8"));
ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java,197,out.close();
ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java,200,throw new SemanticException(ErrorMsg.GENERIC_ERROR.getMsg("Error in serializing metadata"), e);
ql/src/java/org/apache/hadoop/hive/ql/parse/ExportSemanticAnalyzer.java,85,List<Partition> partitions = null;
ql/src/java/org/apache/hadoop/hive/ql/parse/ExportSemanticAnalyzer.java,87,partitions = null;
ql/src/java/org/apache/hadoop/hive/ql/parse/ExportSemanticAnalyzer.java,89,partitions = (ts.partitions != null) ? ts.partitions : db.getPartitions(ts.tableHandle);
