File,Line_number,SRC
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,421,connections.remove(remoteId, this);
src/main/java/org/apache/hadoop/hbase/client/HTablePool.java,257,this.tables.remove(tableName, table);
src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java,711,connections.remove(remoteId, this);
src/main/java/org/apache/hadoop/hbase/mapreduce/hadoopbackport/InputSampler.java,344,K[] samples = sampler.getSample(inf, job);
src/main/java/org/apache/hadoop/hbase/util/PoolMap.java,89,remove((K) key, pool.get());
src/main/java/org/apache/hadoop/hbase/util/PoolMap.java,94,public boolean remove(K key, V value) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3574,KeyValueScanner scanner = store.getScanner(scan, entry.getValue());
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3693,do {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3694,heap.next(results, limit - results.size(), metric);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3695,if (limit > 0 && results.size() == limit) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3696,return KV_LIMIT;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3698,nextKv = heap.peek();
src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationAdmin.java,88,throw new IOException("Unable setup the ZooKeeper connection", e);
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureServer.java,620,SecureCall call = new SecureCall(id, param, this, responder, buf.length);
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,207,private int maxQueueSize;
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,1597,private void setupResponse(ByteArrayOutputStream response,
src/main/java/org/apache/hadoop/hbase/util/HBaseFsckRepair.java,150,rs.closeRegion(region, false);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java,150,this.identifier = descriptor;
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java,347,this.identifier = this.identifier + "-0x" +
src/main/java/org/apache/hadoop/hbase/thrift/generated/AlreadyExists.java,229,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/BatchMutation.java,317,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/BatchMutation.java,440,Mutation _elem2; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/BatchMutation.java,537,Mutation _elem7; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/ColumnDescriptor.java,733,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,4736,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,5089,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,5466,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,5819,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,6196,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,6611,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,6692,__isset_bit_vector = new BitSet(1);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,7013,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,7366,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,7731,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,8084,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,8374,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,8763,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,8874,org.apache.thrift.protocol.TList _list26 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,8875,struct.success = new ArrayList<ByteBuffer>(_list26.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,8878,ByteBuffer _elem28; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,8879,_elem28 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,8880,struct.success.add(_elem28);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,8919,oprot.writeBinary(_iter29);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,8960,oprot.writeBinary(_iter30);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,8975,org.apache.thrift.protocol.TList _list31 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,8976,struct.success = new ArrayList<ByteBuffer>(_list31.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,8979,ByteBuffer _elem33; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,8980,_elem33 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,8981,struct.success.add(_elem33);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9213,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9650,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9761,org.apache.thrift.protocol.TMap _map34 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9762,struct.success = new HashMap<ByteBuffer,ColumnDescriptor>(2*_map34.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9765,ByteBuffer _key36; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9766,ColumnDescriptor _val37; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9767,_key36 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9768,_val37 = new ColumnDescriptor();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9769,_val37.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9770,struct.success.put(_key36, _val37);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9809,oprot.writeBinary(_iter38.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9810,_iter38.getValue().write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9851,oprot.writeBinary(_iter39.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9852,_iter39.getValue().write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9867,org.apache.thrift.protocol.TMap _map40 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9868,struct.success = new HashMap<ByteBuffer,ColumnDescriptor>(2*_map40.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9871,ByteBuffer _key42; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9872,ColumnDescriptor _val43; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9873,_key42 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9874,_val43 = new ColumnDescriptor();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9875,_val43.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9876,struct.success.put(_key42, _val43);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10108,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10540,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10651,org.apache.thrift.protocol.TList _list44 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10652,struct.success = new ArrayList<TRegionInfo>(_list44.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10655,TRegionInfo _elem46; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10656,_elem46 = new TRegionInfo();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10657,_elem46.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10658,struct.success.add(_elem46);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10697,_iter47.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10738,_iter48.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10753,org.apache.thrift.protocol.TList _list49 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10754,struct.success = new ArrayList<TRegionInfo>(_list49.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10757,TRegionInfo _elem51; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10758,_elem51 = new TRegionInfo();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10759,_elem51.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10760,struct.success.add(_elem51);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11083,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11202,org.apache.thrift.protocol.TList _list52 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11203,struct.columnFamilies = new ArrayList<ColumnDescriptor>(_list52.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11206,ColumnDescriptor _elem54; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11207,_elem54 = new ColumnDescriptor();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11208,_elem54.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11209,struct.columnFamilies.add(_elem54);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11244,_iter55.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11283,_iter56.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11299,org.apache.thrift.protocol.TList _list57 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11300,struct.columnFamilies = new ArrayList<ColumnDescriptor>(_list57.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11303,ColumnDescriptor _elem59; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11304,_elem59 = new ColumnDescriptor();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11305,_elem59.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11306,struct.columnFamilies.add(_elem59);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11629,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,12092,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,12445,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13080,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13251,org.apache.thrift.protocol.TMap _map60 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13252,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map60.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13255,ByteBuffer _key62; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13256,ByteBuffer _val63; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13257,_key62 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13258,_val63 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13259,struct.attributes.put(_key62, _val63);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13304,oprot.writeBinary(_iter64.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13305,oprot.writeBinary(_iter64.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13356,oprot.writeBinary(_iter65.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13357,oprot.writeBinary(_iter65.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13381,org.apache.thrift.protocol.TMap _map66 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13382,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map66.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13385,ByteBuffer _key68; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13386,ByteBuffer _val69; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13387,_key68 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13388,_val69 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13389,struct.attributes.put(_key68, _val69);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13673,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13784,org.apache.thrift.protocol.TList _list70 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13785,struct.success = new ArrayList<TCell>(_list70.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13788,TCell _elem72; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13789,_elem72 = new TCell();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13790,_elem72.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13791,struct.success.add(_elem72);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13830,_iter73.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13871,_iter74.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13886,org.apache.thrift.protocol.TList _list75 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13887,struct.success = new ArrayList<TCell>(_list75.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13890,TCell _elem77; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13891,_elem77 = new TCell();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13892,_elem77.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13893,struct.success.add(_elem77);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14457,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14652,org.apache.thrift.protocol.TMap _map78 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14653,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map78.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14656,ByteBuffer _key80; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14657,ByteBuffer _val81; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14658,_key80 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14659,_val81 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14660,struct.attributes.put(_key80, _val81);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14708,oprot.writeBinary(_iter82.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14709,oprot.writeBinary(_iter82.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14766,oprot.writeBinary(_iter83.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14767,oprot.writeBinary(_iter83.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14795,org.apache.thrift.protocol.TMap _map84 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14796,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map84.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14799,ByteBuffer _key86; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14800,ByteBuffer _val87; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14801,_key86 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14802,_val87 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14803,struct.attributes.put(_key86, _val87);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15087,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15198,org.apache.thrift.protocol.TList _list88 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15199,struct.success = new ArrayList<TCell>(_list88.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15202,TCell _elem90; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15203,_elem90 = new TCell();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15204,_elem90.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15205,struct.success.add(_elem90);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15244,_iter91.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15285,_iter92.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15300,org.apache.thrift.protocol.TList _list93 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15301,struct.success = new ArrayList<TCell>(_list93.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15304,TCell _elem95; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15305,_elem95 = new TCell();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15306,_elem95.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15307,struct.success.add(_elem95);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15942,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16159,org.apache.thrift.protocol.TMap _map96 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16160,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map96.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16163,ByteBuffer _key98; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16164,ByteBuffer _val99; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16165,_key98 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16166,_val99 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16167,struct.attributes.put(_key98, _val99);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16218,oprot.writeBinary(_iter100.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16219,oprot.writeBinary(_iter100.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16282,oprot.writeBinary(_iter101.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16283,oprot.writeBinary(_iter101.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16315,org.apache.thrift.protocol.TMap _map102 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16316,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map102.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16319,ByteBuffer _key104; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16320,ByteBuffer _val105; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16321,_key104 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16322,_val105 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16323,struct.attributes.put(_key104, _val105);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16607,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16718,org.apache.thrift.protocol.TList _list106 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16719,struct.success = new ArrayList<TCell>(_list106.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16722,TCell _elem108; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16723,_elem108 = new TCell();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16724,_elem108.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16725,struct.success.add(_elem108);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16764,_iter109.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16805,_iter110.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16820,org.apache.thrift.protocol.TList _list111 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16821,struct.success = new ArrayList<TCell>(_list111.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16824,TCell _elem113; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16825,_elem113 = new TCell();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16826,_elem113.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16827,struct.success.add(_elem113);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17236,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17381,org.apache.thrift.protocol.TMap _map114 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17382,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map114.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17385,ByteBuffer _key116; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17386,ByteBuffer _val117; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17387,_key116 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17388,_val117 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17389,struct.attributes.put(_key116, _val117);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17429,oprot.writeBinary(_iter118.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17430,oprot.writeBinary(_iter118.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17475,oprot.writeBinary(_iter119.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17476,oprot.writeBinary(_iter119.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17496,org.apache.thrift.protocol.TMap _map120 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17497,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map120.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17500,ByteBuffer _key122; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17501,ByteBuffer _val123; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17502,_key122 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17503,_val123 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17504,struct.attributes.put(_key122, _val123);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17788,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17899,org.apache.thrift.protocol.TList _list124 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17900,struct.success = new ArrayList<TRowResult>(_list124.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17903,TRowResult _elem126; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17904,_elem126 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17905,_elem126.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17906,struct.success.add(_elem126);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17945,_iter127.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17986,_iter128.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18001,org.apache.thrift.protocol.TList _list129 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18002,struct.success = new ArrayList<TRowResult>(_list129.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18005,TRowResult _elem131; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18006,_elem131 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18007,_elem131.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18008,struct.success.add(_elem131);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18508,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18671,org.apache.thrift.protocol.TList _list132 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18672,struct.columns = new ArrayList<ByteBuffer>(_list132.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18675,ByteBuffer _elem134; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18676,_elem134 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18677,struct.columns.add(_elem134);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18689,org.apache.thrift.protocol.TMap _map135 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18690,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map135.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18693,ByteBuffer _key137; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18694,ByteBuffer _val138; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18695,_key137 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18696,_val138 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18697,struct.attributes.put(_key137, _val138);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18737,oprot.writeBinary(_iter139);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18749,oprot.writeBinary(_iter140.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18750,oprot.writeBinary(_iter140.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18798,oprot.writeBinary(_iter141);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18807,oprot.writeBinary(_iter142.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18808,oprot.writeBinary(_iter142.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18828,org.apache.thrift.protocol.TList _list143 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18829,struct.columns = new ArrayList<ByteBuffer>(_list143.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18832,ByteBuffer _elem145; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18833,_elem145 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18834,struct.columns.add(_elem145);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18841,org.apache.thrift.protocol.TMap _map146 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18842,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map146.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18845,ByteBuffer _key148; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18846,ByteBuffer _val149; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18847,_key148 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18848,_val149 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18849,struct.attributes.put(_key148, _val149);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19133,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19244,org.apache.thrift.protocol.TList _list150 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19245,struct.success = new ArrayList<TRowResult>(_list150.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19248,TRowResult _elem152; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19249,_elem152 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19250,_elem152.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19251,struct.success.add(_elem152);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19290,_iter153.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19331,_iter154.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19346,org.apache.thrift.protocol.TList _list155 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19347,struct.success = new ArrayList<TRowResult>(_list155.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19350,TRowResult _elem157; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19351,_elem157 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19352,_elem157.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19353,struct.success.add(_elem157);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19836,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20005,org.apache.thrift.protocol.TMap _map158 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20006,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map158.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20009,ByteBuffer _key160; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20010,ByteBuffer _val161; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20011,_key160 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20012,_val161 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20013,struct.attributes.put(_key160, _val161);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20056,oprot.writeBinary(_iter162.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20057,oprot.writeBinary(_iter162.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20108,oprot.writeBinary(_iter163.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20109,oprot.writeBinary(_iter163.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20133,org.apache.thrift.protocol.TMap _map164 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20134,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map164.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20137,ByteBuffer _key166; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20138,ByteBuffer _val167; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20139,_key166 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20140,_val167 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20141,struct.attributes.put(_key166, _val167);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20425,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20536,org.apache.thrift.protocol.TList _list168 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20537,struct.success = new ArrayList<TRowResult>(_list168.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20540,TRowResult _elem170; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20541,_elem170 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20542,_elem170.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20543,struct.success.add(_elem170);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20582,_iter171.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20623,_iter172.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20638,org.apache.thrift.protocol.TList _list173 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20639,struct.success = new ArrayList<TRowResult>(_list173.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20642,TRowResult _elem175; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20643,_elem175 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20644,_elem175.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20645,struct.success.add(_elem175);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21207,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21386,org.apache.thrift.protocol.TList _list176 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21387,struct.columns = new ArrayList<ByteBuffer>(_list176.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21390,ByteBuffer _elem178; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21391,_elem178 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21392,struct.columns.add(_elem178);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21412,org.apache.thrift.protocol.TMap _map179 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21413,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map179.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21416,ByteBuffer _key181; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21417,ByteBuffer _val182; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21418,_key181 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21419,_val182 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21420,struct.attributes.put(_key181, _val182);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21460,oprot.writeBinary(_iter183);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21475,oprot.writeBinary(_iter184.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21476,oprot.writeBinary(_iter184.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21527,oprot.writeBinary(_iter185);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21539,oprot.writeBinary(_iter186.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21540,oprot.writeBinary(_iter186.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21560,org.apache.thrift.protocol.TList _list187 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21561,struct.columns = new ArrayList<ByteBuffer>(_list187.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21564,ByteBuffer _elem189; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21565,_elem189 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21566,struct.columns.add(_elem189);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21577,org.apache.thrift.protocol.TMap _map190 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21578,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map190.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21581,ByteBuffer _key192; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21582,ByteBuffer _val193; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21583,_key192 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21584,_val193 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21585,struct.attributes.put(_key192, _val193);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21869,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21980,org.apache.thrift.protocol.TList _list194 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21981,struct.success = new ArrayList<TRowResult>(_list194.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21984,TRowResult _elem196; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21985,_elem196 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21986,_elem196.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21987,struct.success.add(_elem196);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22026,_iter197.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22067,_iter198.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22082,org.apache.thrift.protocol.TList _list199 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22083,struct.success = new ArrayList<TRowResult>(_list199.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22086,TRowResult _elem201; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22087,_elem201 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22088,_elem201.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22089,struct.success.add(_elem201);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22508,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22645,org.apache.thrift.protocol.TList _list202 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22646,struct.rows = new ArrayList<ByteBuffer>(_list202.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22649,ByteBuffer _elem204; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22650,_elem204 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22651,struct.rows.add(_elem204);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22663,org.apache.thrift.protocol.TMap _map205 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22664,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map205.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22667,ByteBuffer _key207; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22668,ByteBuffer _val208; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22669,_key207 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22670,_val208 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22671,struct.attributes.put(_key207, _val208);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22706,oprot.writeBinary(_iter209);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22718,oprot.writeBinary(_iter210.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22719,oprot.writeBinary(_iter210.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22761,oprot.writeBinary(_iter211);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22770,oprot.writeBinary(_iter212.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22771,oprot.writeBinary(_iter212.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22787,org.apache.thrift.protocol.TList _list213 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22788,struct.rows = new ArrayList<ByteBuffer>(_list213.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22791,ByteBuffer _elem215; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22792,_elem215 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22793,struct.rows.add(_elem215);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22800,org.apache.thrift.protocol.TMap _map216 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22801,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map216.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22804,ByteBuffer _key218; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22805,ByteBuffer _val219; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22806,_key218 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22807,_val219 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22808,struct.attributes.put(_key218, _val219);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23092,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23203,org.apache.thrift.protocol.TList _list220 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23204,struct.success = new ArrayList<TRowResult>(_list220.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23207,TRowResult _elem222; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23208,_elem222 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23209,_elem222.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23210,struct.success.add(_elem222);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23249,_iter223.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23290,_iter224.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23305,org.apache.thrift.protocol.TList _list225 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23306,struct.success = new ArrayList<TRowResult>(_list225.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23309,TRowResult _elem227; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23310,_elem227 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23311,_elem227.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23312,struct.success.add(_elem227);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23822,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23977,org.apache.thrift.protocol.TList _list228 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23978,struct.rows = new ArrayList<ByteBuffer>(_list228.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23981,ByteBuffer _elem230; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23982,_elem230 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23983,struct.rows.add(_elem230);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23995,org.apache.thrift.protocol.TList _list231 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23996,struct.columns = new ArrayList<ByteBuffer>(_list231.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23999,ByteBuffer _elem233; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24000,_elem233 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24001,struct.columns.add(_elem233);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24013,org.apache.thrift.protocol.TMap _map234 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24014,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map234.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24017,ByteBuffer _key236; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24018,ByteBuffer _val237; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24019,_key236 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24020,_val237 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24021,struct.attributes.put(_key236, _val237);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24056,oprot.writeBinary(_iter238);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24068,oprot.writeBinary(_iter239);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24080,oprot.writeBinary(_iter240.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24081,oprot.writeBinary(_iter240.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24126,oprot.writeBinary(_iter241);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24135,oprot.writeBinary(_iter242);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24144,oprot.writeBinary(_iter243.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24145,oprot.writeBinary(_iter243.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24161,org.apache.thrift.protocol.TList _list244 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24162,struct.rows = new ArrayList<ByteBuffer>(_list244.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24165,ByteBuffer _elem246; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24166,_elem246 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24167,struct.rows.add(_elem246);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24174,org.apache.thrift.protocol.TList _list247 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24175,struct.columns = new ArrayList<ByteBuffer>(_list247.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24178,ByteBuffer _elem249; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24179,_elem249 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24180,struct.columns.add(_elem249);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24187,org.apache.thrift.protocol.TMap _map250 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24188,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map250.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24191,ByteBuffer _key252; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24192,ByteBuffer _val253; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24193,_key252 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24194,_val253 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24195,struct.attributes.put(_key252, _val253);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24479,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24590,org.apache.thrift.protocol.TList _list254 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24591,struct.success = new ArrayList<TRowResult>(_list254.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24594,TRowResult _elem256; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24595,_elem256 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24596,_elem256.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24597,struct.success.add(_elem256);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24636,_iter257.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24677,_iter258.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24692,org.apache.thrift.protocol.TList _list259 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24693,struct.success = new ArrayList<TRowResult>(_list259.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24696,TRowResult _elem261; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24697,_elem261 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24698,_elem261.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24699,struct.success.add(_elem261);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25192,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25345,org.apache.thrift.protocol.TList _list262 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25346,struct.rows = new ArrayList<ByteBuffer>(_list262.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25349,ByteBuffer _elem264; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25350,_elem264 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25351,struct.rows.add(_elem264);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25371,org.apache.thrift.protocol.TMap _map265 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25372,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map265.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25375,ByteBuffer _key267; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25376,ByteBuffer _val268; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25377,_key267 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25378,_val268 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25379,struct.attributes.put(_key267, _val268);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25414,oprot.writeBinary(_iter269);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25429,oprot.writeBinary(_iter270.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25430,oprot.writeBinary(_iter270.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25475,oprot.writeBinary(_iter271);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25487,oprot.writeBinary(_iter272.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25488,oprot.writeBinary(_iter272.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25504,org.apache.thrift.protocol.TList _list273 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25505,struct.rows = new ArrayList<ByteBuffer>(_list273.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25508,ByteBuffer _elem275; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25509,_elem275 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25510,struct.rows.add(_elem275);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25521,org.apache.thrift.protocol.TMap _map276 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25522,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map276.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25525,ByteBuffer _key278; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25526,ByteBuffer _val279; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25527,_key278 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25528,_val279 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25529,struct.attributes.put(_key278, _val279);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25813,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25924,org.apache.thrift.protocol.TList _list280 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25925,struct.success = new ArrayList<TRowResult>(_list280.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25928,TRowResult _elem282; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25929,_elem282 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25930,_elem282.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25931,struct.success.add(_elem282);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25970,_iter283.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26011,_iter284.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26026,org.apache.thrift.protocol.TList _list285 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26027,struct.success = new ArrayList<TRowResult>(_list285.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26030,TRowResult _elem287; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26031,_elem287 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26032,_elem287.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26033,struct.success.add(_elem287);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26605,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26776,org.apache.thrift.protocol.TList _list288 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26777,struct.rows = new ArrayList<ByteBuffer>(_list288.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26780,ByteBuffer _elem290; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26781,_elem290 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26782,struct.rows.add(_elem290);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26794,org.apache.thrift.protocol.TList _list291 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26795,struct.columns = new ArrayList<ByteBuffer>(_list291.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26798,ByteBuffer _elem293; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26799,_elem293 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26800,struct.columns.add(_elem293);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26820,org.apache.thrift.protocol.TMap _map294 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26821,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map294.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26824,ByteBuffer _key296; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26825,ByteBuffer _val297; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26826,_key296 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26827,_val297 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26828,struct.attributes.put(_key296, _val297);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26863,oprot.writeBinary(_iter298);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26875,oprot.writeBinary(_iter299);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26890,oprot.writeBinary(_iter300.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26891,oprot.writeBinary(_iter300.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26939,oprot.writeBinary(_iter301);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26948,oprot.writeBinary(_iter302);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26960,oprot.writeBinary(_iter303.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26961,oprot.writeBinary(_iter303.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26977,org.apache.thrift.protocol.TList _list304 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26978,struct.rows = new ArrayList<ByteBuffer>(_list304.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26981,ByteBuffer _elem306; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26982,_elem306 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26983,struct.rows.add(_elem306);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26990,org.apache.thrift.protocol.TList _list307 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26991,struct.columns = new ArrayList<ByteBuffer>(_list307.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26994,ByteBuffer _elem309; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26995,_elem309 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26996,struct.columns.add(_elem309);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27007,org.apache.thrift.protocol.TMap _map310 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27008,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map310.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27011,ByteBuffer _key312; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27012,ByteBuffer _val313; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27013,_key312 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27014,_val313 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27015,struct.attributes.put(_key312, _val313);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27299,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27410,org.apache.thrift.protocol.TList _list314 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27411,struct.success = new ArrayList<TRowResult>(_list314.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27414,TRowResult _elem316; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27415,_elem316 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27416,_elem316.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27417,struct.success.add(_elem316);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27456,_iter317.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27497,_iter318.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27512,org.apache.thrift.protocol.TList _list319 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27513,struct.success = new ArrayList<TRowResult>(_list319.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27516,TRowResult _elem321; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27517,_elem321 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27518,_elem321.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27519,struct.success.add(_elem321);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28019,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28182,org.apache.thrift.protocol.TList _list322 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28183,struct.mutations = new ArrayList<Mutation>(_list322.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28186,Mutation _elem324; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28187,_elem324 = new Mutation();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28188,_elem324.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28189,struct.mutations.add(_elem324);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28201,org.apache.thrift.protocol.TMap _map325 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28202,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map325.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28205,ByteBuffer _key327; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28206,ByteBuffer _val328; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28207,_key327 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28208,_val328 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28209,struct.attributes.put(_key327, _val328);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28249,_iter329.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28261,oprot.writeBinary(_iter330.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28262,oprot.writeBinary(_iter330.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28310,_iter331.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28319,oprot.writeBinary(_iter332.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28320,oprot.writeBinary(_iter332.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28340,org.apache.thrift.protocol.TList _list333 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28341,struct.mutations = new ArrayList<Mutation>(_list333.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28344,Mutation _elem335; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28345,_elem335 = new Mutation();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28346,_elem335.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28347,struct.mutations.add(_elem335);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28354,org.apache.thrift.protocol.TMap _map336 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28355,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map336.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28358,ByteBuffer _key338; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28359,ByteBuffer _val339; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28360,_key338 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28361,_val339 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28362,struct.attributes.put(_key338, _val339);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28626,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29388,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29523,__isset_bit_vector = new BitSet(1);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29567,org.apache.thrift.protocol.TList _list340 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29568,struct.mutations = new ArrayList<Mutation>(_list340.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29571,Mutation _elem342; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29572,_elem342 = new Mutation();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29573,_elem342.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29574,struct.mutations.add(_elem342);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29594,org.apache.thrift.protocol.TMap _map343 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29595,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map343.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29598,ByteBuffer _key345; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29599,ByteBuffer _val346; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29600,_key345 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29601,_val346 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29602,struct.attributes.put(_key345, _val346);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29642,_iter347.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29657,oprot.writeBinary(_iter348.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29658,oprot.writeBinary(_iter348.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29709,_iter349.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29721,oprot.writeBinary(_iter350.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29722,oprot.writeBinary(_iter350.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29742,org.apache.thrift.protocol.TList _list351 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29743,struct.mutations = new ArrayList<Mutation>(_list351.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29746,Mutation _elem353; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29747,_elem353 = new Mutation();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29748,_elem353.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29749,struct.mutations.add(_elem353);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29760,org.apache.thrift.protocol.TMap _map354 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29761,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map354.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29764,ByteBuffer _key356; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29765,ByteBuffer _val357; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29766,_key356 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29767,_val357 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29768,struct.attributes.put(_key356, _val357);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30032,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30639,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30776,org.apache.thrift.protocol.TList _list358 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30777,struct.rowBatches = new ArrayList<BatchMutation>(_list358.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30780,BatchMutation _elem360; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30781,_elem360 = new BatchMutation();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30782,_elem360.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30783,struct.rowBatches.add(_elem360);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30795,org.apache.thrift.protocol.TMap _map361 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30796,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map361.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30799,ByteBuffer _key363; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30800,ByteBuffer _val364; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30801,_key363 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30802,_val364 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30803,struct.attributes.put(_key363, _val364);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30838,_iter365.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30850,oprot.writeBinary(_iter366.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30851,oprot.writeBinary(_iter366.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30893,_iter367.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30902,oprot.writeBinary(_iter368.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30903,oprot.writeBinary(_iter368.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30919,org.apache.thrift.protocol.TList _list369 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30920,struct.rowBatches = new ArrayList<BatchMutation>(_list369.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30923,BatchMutation _elem371; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30924,_elem371 = new BatchMutation();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30925,_elem371.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30926,struct.rowBatches.add(_elem371);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30933,org.apache.thrift.protocol.TMap _map372 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30934,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map372.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30937,ByteBuffer _key374; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30938,ByteBuffer _val375; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30939,_key374 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30940,_val375 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30941,struct.attributes.put(_key374, _val375);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,31205,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,31886,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32037,org.apache.thrift.protocol.TList _list376 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32038,struct.rowBatches = new ArrayList<BatchMutation>(_list376.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32041,BatchMutation _elem378; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32042,_elem378 = new BatchMutation();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32043,_elem378.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32044,struct.rowBatches.add(_elem378);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32064,org.apache.thrift.protocol.TMap _map379 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32065,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map379.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32068,ByteBuffer _key381; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32069,ByteBuffer _val382; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32070,_key381 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32071,_val382 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32072,struct.attributes.put(_key381, _val382);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32107,_iter383.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32122,oprot.writeBinary(_iter384.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32123,oprot.writeBinary(_iter384.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32168,_iter385.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32180,oprot.writeBinary(_iter386.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32181,oprot.writeBinary(_iter386.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32197,org.apache.thrift.protocol.TList _list387 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32198,struct.rowBatches = new ArrayList<BatchMutation>(_list387.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32201,BatchMutation _elem389; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32202,_elem389 = new BatchMutation();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32203,_elem389.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32204,struct.rowBatches.add(_elem389);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32215,org.apache.thrift.protocol.TMap _map390 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32216,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map390.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32219,ByteBuffer _key392; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32220,ByteBuffer _val393; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32221,_key392 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32222,_val393 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32223,struct.attributes.put(_key392, _val393);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32487,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,33143,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,33736,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,33835,__isset_bit_vector = new BitSet(1);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34451,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34622,org.apache.thrift.protocol.TMap _map394 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34623,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map394.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34626,ByteBuffer _key396; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34627,ByteBuffer _val397; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34628,_key396 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34629,_val397 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34630,struct.attributes.put(_key396, _val397);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34675,oprot.writeBinary(_iter398.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34676,oprot.writeBinary(_iter398.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34727,oprot.writeBinary(_iter399.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34728,oprot.writeBinary(_iter399.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34752,org.apache.thrift.protocol.TMap _map400 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34753,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map400.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34756,ByteBuffer _key402; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34757,ByteBuffer _val403; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34758,_key402 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34759,_val403 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34760,struct.attributes.put(_key402, _val403);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34965,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,35674,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,35869,org.apache.thrift.protocol.TMap _map404 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,35870,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map404.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,35873,ByteBuffer _key406; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,35874,ByteBuffer _val407; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,35875,_key406 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,35876,_val407 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,35877,struct.attributes.put(_key406, _val407);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,35925,oprot.writeBinary(_iter408.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,35926,oprot.writeBinary(_iter408.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,35983,oprot.writeBinary(_iter409.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,35984,oprot.writeBinary(_iter409.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36012,org.apache.thrift.protocol.TMap _map410 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36013,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map410.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36016,ByteBuffer _key412; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36017,ByteBuffer _val413; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36018,_key412 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36019,_val413 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36020,struct.attributes.put(_key412, _val413);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36225,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36779,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36924,org.apache.thrift.protocol.TMap _map414 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36925,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map414.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36928,ByteBuffer _key416; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36929,ByteBuffer _val417; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36930,_key416 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36931,_val417 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36932,struct.attributes.put(_key416, _val417);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36972,oprot.writeBinary(_iter418.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36973,oprot.writeBinary(_iter418.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,37018,oprot.writeBinary(_iter419.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,37019,oprot.writeBinary(_iter419.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,37039,org.apache.thrift.protocol.TMap _map420 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,37040,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map420.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,37043,ByteBuffer _key422; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,37044,ByteBuffer _val423; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,37045,_key422 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,37046,_val423 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,37047,struct.attributes.put(_key422, _val423);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,37252,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,37619,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,37974,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38361,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38454,org.apache.thrift.protocol.TList _list424 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38455,struct.increments = new ArrayList<TIncrement>(_list424.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38458,TIncrement _elem426; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38459,_elem426 = new TIncrement();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38460,_elem426.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38461,struct.increments.add(_elem426);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38491,_iter427.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38524,_iter428.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38536,org.apache.thrift.protocol.TList _list429 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38537,struct.increments = new ArrayList<TIncrement>(_list429.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38540,TIncrement _elem431; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38541,_elem431 = new TIncrement();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38542,_elem431.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38543,struct.increments.add(_elem431);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38748,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39376,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39545,org.apache.thrift.protocol.TMap _map432 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39546,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map432.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39549,ByteBuffer _key434; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39550,ByteBuffer _val435; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39551,_key434 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39552,_val435 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39553,struct.attributes.put(_key434, _val435);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39596,oprot.writeBinary(_iter436.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39597,oprot.writeBinary(_iter436.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39648,oprot.writeBinary(_iter437.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39649,oprot.writeBinary(_iter437.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39673,org.apache.thrift.protocol.TMap _map438 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39674,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map438.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39677,ByteBuffer _key440; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39678,ByteBuffer _val441; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39679,_key440 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39680,_val441 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39681,struct.attributes.put(_key440, _val441);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39886,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40430,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40576,org.apache.thrift.protocol.TMap _map442 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40577,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map442.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40580,ByteBuffer _key444; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40581,ByteBuffer _val445; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40582,_key444 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40583,_val445 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40584,struct.attributes.put(_key444, _val445);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40624,oprot.writeBinary(_iter446.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40625,oprot.writeBinary(_iter446.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40670,oprot.writeBinary(_iter447.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40671,oprot.writeBinary(_iter447.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40692,org.apache.thrift.protocol.TMap _map448 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40693,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map448.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40696,ByteBuffer _key450; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40697,ByteBuffer _val451; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40698,_key450 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40699,_val451 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40700,struct.attributes.put(_key450, _val451);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40967,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41048,__isset_bit_vector = new BitSet(1);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41661,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41824,org.apache.thrift.protocol.TList _list452 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41825,struct.columns = new ArrayList<ByteBuffer>(_list452.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41828,ByteBuffer _elem454; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41829,_elem454 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41830,struct.columns.add(_elem454);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41842,org.apache.thrift.protocol.TMap _map455 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41843,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map455.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41846,ByteBuffer _key457; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41847,ByteBuffer _val458; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41848,_key457 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41849,_val458 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41850,struct.attributes.put(_key457, _val458);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41890,oprot.writeBinary(_iter459);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41902,oprot.writeBinary(_iter460.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41903,oprot.writeBinary(_iter460.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41951,oprot.writeBinary(_iter461);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41960,oprot.writeBinary(_iter462.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41961,oprot.writeBinary(_iter462.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41981,org.apache.thrift.protocol.TList _list463 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41982,struct.columns = new ArrayList<ByteBuffer>(_list463.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41985,ByteBuffer _elem465; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41986,_elem465 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41987,struct.columns.add(_elem465);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41994,org.apache.thrift.protocol.TMap _map466 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41995,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map466.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41998,ByteBuffer _key468; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41999,ByteBuffer _val469; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,42000,_key468 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,42001,_val469 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,42002,struct.attributes.put(_key468, _val469);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,42269,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,42350,__isset_bit_vector = new BitSet(1);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43048,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43237,org.apache.thrift.protocol.TList _list470 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43238,struct.columns = new ArrayList<ByteBuffer>(_list470.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43241,ByteBuffer _elem472; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43242,_elem472 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43243,struct.columns.add(_elem472);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43255,org.apache.thrift.protocol.TMap _map473 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43256,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map473.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43259,ByteBuffer _key475; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43260,ByteBuffer _val476; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43261,_key475 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43262,_val476 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43263,struct.attributes.put(_key475, _val476);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43308,oprot.writeBinary(_iter477);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43320,oprot.writeBinary(_iter478.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43321,oprot.writeBinary(_iter478.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43375,oprot.writeBinary(_iter479);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43384,oprot.writeBinary(_iter480.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43385,oprot.writeBinary(_iter480.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43409,org.apache.thrift.protocol.TList _list481 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43410,struct.columns = new ArrayList<ByteBuffer>(_list481.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43413,ByteBuffer _elem483; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43414,_elem483 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43415,struct.columns.add(_elem483);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43422,org.apache.thrift.protocol.TMap _map484 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43423,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map484.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43426,ByteBuffer _key486; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43427,ByteBuffer _val487; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43428,_key486 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43429,_val487 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43430,struct.attributes.put(_key486, _val487);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43697,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43778,__isset_bit_vector = new BitSet(1);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44379,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44542,org.apache.thrift.protocol.TList _list488 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44543,struct.columns = new ArrayList<ByteBuffer>(_list488.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44546,ByteBuffer _elem490; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44547,_elem490 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44548,struct.columns.add(_elem490);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44560,org.apache.thrift.protocol.TMap _map491 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44561,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map491.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44564,ByteBuffer _key493; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44565,ByteBuffer _val494; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44566,_key493 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44567,_val494 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44568,struct.attributes.put(_key493, _val494);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44608,oprot.writeBinary(_iter495);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44620,oprot.writeBinary(_iter496.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44621,oprot.writeBinary(_iter496.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44669,oprot.writeBinary(_iter497);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44678,oprot.writeBinary(_iter498.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44679,oprot.writeBinary(_iter498.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44699,org.apache.thrift.protocol.TList _list499 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44700,struct.columns = new ArrayList<ByteBuffer>(_list499.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44703,ByteBuffer _elem501; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44704,_elem501 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44705,struct.columns.add(_elem501);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44712,org.apache.thrift.protocol.TMap _map502 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44713,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map502.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44716,ByteBuffer _key504; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44717,ByteBuffer _val505; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44718,_key504 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44719,_val505 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44720,struct.attributes.put(_key504, _val505);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44987,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,45068,__isset_bit_vector = new BitSet(1);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,45755,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,45934,org.apache.thrift.protocol.TList _list506 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,45935,struct.columns = new ArrayList<ByteBuffer>(_list506.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,45938,ByteBuffer _elem508; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,45939,_elem508 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,45940,struct.columns.add(_elem508);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,45960,org.apache.thrift.protocol.TMap _map509 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,45961,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map509.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,45964,ByteBuffer _key511; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,45965,ByteBuffer _val512; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,45966,_key511 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,45967,_val512 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,45968,struct.attributes.put(_key511, _val512);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46008,oprot.writeBinary(_iter513);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46023,oprot.writeBinary(_iter514.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46024,oprot.writeBinary(_iter514.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46075,oprot.writeBinary(_iter515);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46087,oprot.writeBinary(_iter516.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46088,oprot.writeBinary(_iter516.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46108,org.apache.thrift.protocol.TList _list517 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46109,struct.columns = new ArrayList<ByteBuffer>(_list517.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46112,ByteBuffer _elem519; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46113,_elem519 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46114,struct.columns.add(_elem519);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46125,org.apache.thrift.protocol.TMap _map520 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46126,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map520.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46129,ByteBuffer _key522; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46130,ByteBuffer _val523; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46131,_key522 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46132,_val523 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46133,struct.attributes.put(_key522, _val523);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46400,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46481,__isset_bit_vector = new BitSet(1);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47253,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47456,org.apache.thrift.protocol.TList _list524 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47457,struct.columns = new ArrayList<ByteBuffer>(_list524.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47460,ByteBuffer _elem526; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47461,_elem526 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47462,struct.columns.add(_elem526);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47482,org.apache.thrift.protocol.TMap _map527 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47483,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map527.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47486,ByteBuffer _key529; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47487,ByteBuffer _val530; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47488,_key529 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47489,_val530 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47490,struct.attributes.put(_key529, _val530);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47535,oprot.writeBinary(_iter531);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47550,oprot.writeBinary(_iter532.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47551,oprot.writeBinary(_iter532.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47608,oprot.writeBinary(_iter533);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47620,oprot.writeBinary(_iter534.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47621,oprot.writeBinary(_iter534.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47645,org.apache.thrift.protocol.TList _list535 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47646,struct.columns = new ArrayList<ByteBuffer>(_list535.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47649,ByteBuffer _elem537; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47650,_elem537 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47651,struct.columns.add(_elem537);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47662,org.apache.thrift.protocol.TMap _map538 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47663,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map538.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47666,ByteBuffer _key540; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47667,ByteBuffer _val541; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47668,_key540 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47669,_val541 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47670,struct.attributes.put(_key540, _val541);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47937,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,48018,__isset_bit_vector = new BitSet(1);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,48344,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,48831,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,48960,org.apache.thrift.protocol.TList _list542 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,48961,struct.success = new ArrayList<TRowResult>(_list542.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,48964,TRowResult _elem544; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,48965,_elem544 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,48966,_elem544.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,48967,struct.success.add(_elem544);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,49015,_iter545.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,49064,_iter546.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,49082,org.apache.thrift.protocol.TList _list547 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,49083,struct.success = new ArrayList<TRowResult>(_list547.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,49086,TRowResult _elem549; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,49087,_elem549 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,49088,_elem549.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,49089,struct.success.add(_elem549);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,49390,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,49910,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50039,org.apache.thrift.protocol.TList _list550 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50040,struct.success = new ArrayList<TRowResult>(_list550.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50043,TRowResult _elem552; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50044,_elem552 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50045,_elem552.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50046,struct.success.add(_elem552);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50094,_iter553.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50143,_iter554.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50161,org.apache.thrift.protocol.TList _list555 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50162,struct.success = new ArrayList<TRowResult>(_list555.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50165,TRowResult _elem557; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50166,_elem557 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50167,_elem557.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50168,struct.success.add(_elem557);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50398,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50806,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,51388,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,51902,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52013,org.apache.thrift.protocol.TList _list558 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52014,struct.success = new ArrayList<TCell>(_list558.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52017,TCell _elem560; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52018,_elem560 = new TCell();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52019,_elem560.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52020,struct.success.add(_elem560);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52059,_iter561.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52100,_iter562.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52115,org.apache.thrift.protocol.TList _list563 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52116,struct.success = new ArrayList<TCell>(_list563.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52119,TCell _elem565; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52120,_elem565 = new TCell();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52121,_elem565.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52122,struct.success.add(_elem565);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52354,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52766,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/IOError.java,230,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/IllegalArgument.java,229,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Mutation.java,432,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/TCell.java,303,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/TIncrement.java,439,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/TRegionInfo.java,625,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/TRowResult.java,322,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/TScan.java,558,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TColumn.java,369,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TColumnIncrement.java,371,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TColumnValue.java,439,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TDelete.java,530,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TGet.java,501,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,1683,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,2130,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,2614,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,3058,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,3578,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,4074,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,4598,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,4983,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,5768,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,6370,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,6874,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,7291,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,7740,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,8125,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,8594,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,9090,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,9950,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,10552,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,11036,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,11480,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,11972,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,12419,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,12897,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,13428,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,13916,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,13980,__isset_bit_vector = new BitSet(1);
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,14330,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIOError.java,224,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIllegalArgument.java,223,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIncrement.java,383,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIncrement.java,526,org.apache.thrift.protocol.TList _list32 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIncrement.java,527,struct.columns = new ArrayList<TColumnIncrement>(_list32.size);
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIncrement.java,531,_elem34 = new TColumnIncrement();
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIncrement.java,532,_elem34.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIncrement.java,533,struct.columns.add(_elem34);
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIncrement.java,576,_iter35.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIncrement.java,609,_iter36.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIncrement.java,628,org.apache.thrift.protocol.TList _list37 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIncrement.java,629,struct.columns = new ArrayList<TColumnIncrement>(_list37.size);
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIncrement.java,633,_elem39 = new TColumnIncrement();
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIncrement.java,634,_elem39.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIncrement.java,635,struct.columns.add(_elem39);
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TPut.java,444,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TResult.java,317,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TScan.java,553,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TTimeRange.java,287,return 0;
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,178,ct.start();
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,187,private void cleanupCatalogTracker(final CatalogTracker ct) {
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,950,LOG.warn("Encountered problems when prefetch META table: ", e);
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,550,private final Map<Integer, SoftValueSortedMap<byte [], HRegionLocation>>
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,552,new HashMap<Integer, SoftValueSortedMap<byte [], HRegionLocation>>();
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,563,private final Set<Integer> regionCachePrefetchDisabledTables =
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,564,new CopyOnWriteArraySet<Integer>();
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1223,Integer key = Bytes.mapKey(tableName);
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1248,this.cachedRegionLocations.remove(Bytes.mapKey(tableName));
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1653,Integer key = Bytes.mapKey(tableName);
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1656,this.cachedRegionLocations.get(key);
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1680,regionCachePrefetchDisabledTables.add(Bytes.mapKey(tableName));
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1683,regionCachePrefetchDisabledTables.remove(Bytes.mapKey(tableName));
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1688,return !regionCachePrefetchDisabledTables.contains(Bytes.mapKey(tableName));
src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java,292,&& !(cause instanceof RegionServerStoppedException))) {
src/main/java/org/apache/hadoop/hbase/client/ScannerCallable.java,126,rrs = server.next(scannerId, caching);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,298,final Map<String, RegionScanner> scanners =
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,299,new ConcurrentHashMap<String, RegionScanner>();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,536,RegionScanner scanner = scanners.get(scannerIdString);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,537,if (scanner != null && scanner.getRegionInfo().isMetaTable()) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,983,for (Map.Entry<String, RegionScanner> e : this.scanners.entrySet()) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,985,e.getValue().close();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2418,scanners.put(scannerName, s);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2431,public Result[] next(final long scannerId, int nbRows) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2433,RegionScanner s = this.scanners.get(scannerName);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2569,RegionScanner s = scanners.remove(this.scannerName);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2570,if (s != null) {
src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java,84,server = hrsc.getConstructor(Configuration.class).newInstance(c);
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,1429,LOG.debug(getName()+", call "+call+": error: " + e, e);
src/main/java/org/apache/hadoop/hbase/ipc/ExecRPCInvoker.java,81,LOG.debug("Result is region="+ Bytes.toStringBinary(regionName) +
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,98,this.zk = new ZooKeeper(quorumServers, sessionTimeout, watcher);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,112,throws IOException, InterruptedException {
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,113,LOG.info("Closing dead ZooKeeper connection, session" +
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,115,zk.close();
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,116,this.zk = new ZooKeeper(this.quorumServers,
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,117,this.sessionTimeout, this.watcher);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,133,zk.delete(path, version);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,172,return zk.exists(path, watcher);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,199,return zk.exists(path, watch);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,236,return zk.getChildren(path, watcher);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,263,return zk.getChildren(path, watch);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,290,byte[] revData = zk.getData(path, watcher, stat);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,318,byte[] revData = zk.getData(path, watch, stat);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,349,return zk.setData(path, newData, version);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,361,byte[] revData = zk.getData(path, false, stat);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,421,return zk.create(path, data, acl, createMode);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,429,byte[] currentData = zk.getData(path, false, null);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,476,return zk.create(newPath, data, acl, createMode);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,531,return zk.multi(multiOps);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,556,List<String> nodes = zk.getChildren(parent, false);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,560,Stat stat = zk.exists(nodePath, false);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,605,return zk.getSessionId();
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,609,zk.close();
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,613,return zk.getState();
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,621,return zk.getSessionPasswd();
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,624,public void sync(String path, AsyncCallback.VoidCallback cb, Object ctx) {
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,625,this.zk.sync(path, null, null);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java,199,try {
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java,201,InetAddress.getByName(host);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java,202,anyValid = true;
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java,204,LOG.warn(StringUtils.stringifyException(e));
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java,253,public void reconnectAfterExpiration() throws IOException, InterruptedException {
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java,381,public void sync(String path) {
src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java,328,p.add(HConstants.CATALOG_FAMILY, HConstants.SERVER_QUALIFIER,
src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java,330,p.add(HConstants.CATALOG_FAMILY, HConstants.STARTCODE_QUALIFIER,
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2471,: results.toArray(new Result[0]);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2510,: results.toArray(new Result[0]);
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,433,RegionMetricsStorage.incrNumericMetric(this.metricNamePrefix + metric,
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,434,cumulativeMetric);
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,395,FSDataOutputStream s = fs.create(versionFile);
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,399,s.close();
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,492,Path filePath = new Path(rootdir, HConstants.CLUSTER_ID_FILE_NAME);
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,493,FSDataOutputStream s = fs.create(filePath);
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,497,LOG.debug("Created cluster ID file at " + filePath.toString() +
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,109,private long lastNodeCreateTime = Long.MAX_VALUE;
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,283,LOG.warn("returning success without actually splitting and " +
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,436,lastNodeCreateTime = EnvironmentEdgeManager.currentTimeMillis();
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,451,new GetDataAsyncCallback(), retry_count);
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,459,new GetDataAsyncCallback(), new Long(-1) /* retry count */);
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,682,lastNodeCreateTime = EnvironmentEdgeManager.currentTimeMillis();
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,985,((EnvironmentEdgeManager.currentTimeMillis() - lastNodeCreateTime) >
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,1062,getDataSetWatchSuccess(path, null, Integer.MIN_VALUE);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1384,int readRequestsCount = 0;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1385,int writeRequestsCount = 0;
src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java,623,markClosed(e);
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,291,handleSaslConnectionFailure(numRetries++, MAX_RETRIES, ex, rand,
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,292,ticket);
src/main/java/org/apache/hadoop/hbase/thrift/TBoundedThreadPoolServer.java,287,while (!stopped && processor.process(inputProtocol, outputProtocol)) {}
src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java,275,protocolFactory = new TBinaryProtocol.Factory();
src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java,285,transportFactory = new TFramedTransport.Factory();
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftServer.java,121,private static TTransportFactory getTTransportFactory(boolean framed) {
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftServer.java,124,return new TFramedTransport.Factory();
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftServer.java,250,TTransportFactory transportFactory = getTTransportFactory(framed);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,85,private static final char ZNODE_PATH_SEPARATOR = '/';
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1678,try {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1680,this.stopped = true;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1681,LOG.info("STOPPED: " + msg);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1683,sleeper.skipSleepCycle();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1685,LOG.warn("The region server did not stop", exp);
src/main/java/org/apache/hadoop/hbase/io/encoding/FastDiffDeltaEncoder.java,392,return ByteBuffer.wrap(block.array(), pos, keyLength).slice();
src/main/java/org/apache/hadoop/hbase/io/encoding/PrefixKeyDeltaEncoder.java,157,return ByteBuffer.wrap(block.array(), pos, keyLength).slice();
src/main/java/org/apache/hadoop/hbase/mapreduce/KeyValueSortReducer.java,47,if (index > 0 && index % 100 == 0) context.setStatus("Wrote " + index);
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,53,private static final Configuration conf = HBaseConfiguration.create();
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,286,HbaseObjectWritable.writeObject(out, filter, Writable.class, conf);
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/SchemaMetrics.java,589,conf.getBoolean(SHOW_TABLE_NAME_CONF_KEY, false);
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/SchemaMetrics.java,592,setUseTableName(false);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,649,this.metrics.shippedOpsRate.inc(
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,650,this.currentNbOperations);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,651,this.metrics.setAgeOfLastShippedOp(
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1336,public boolean flushcache() throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1339,LOG.debug("Skipping flush on " + this + " because closing");
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1340,return false;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1348,LOG.debug("Skipping flush on " + this + " because closed");
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1349,status.abort("Skipped: closed");
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1350,return false;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1369,status.abort("Not flushing since "
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1371,: "writes not enabled"));
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1372,return false;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1376,boolean result = internalFlushcache(status);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1384,return result;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1447,protected boolean internalFlushcache(
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1456,return false;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1596,return compactionRequested;
src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java,413,boolean shouldCompact = region.flushcache();
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,399,max = Math.max(max, sf.getMaxSequenceId());
src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java,53,private List<TaskAndWeakRefPair> tasks =
src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java,54,Lists.newArrayList();
src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java,96,int size = 0;
src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java,115,size++;
src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java,119,if (size > MAX_TASKS) {
src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java,120,LOG.warn("Too many actions in action monitor! Purging some.");
src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java,121,tasks = tasks.subList(size - MAX_TASKS, size);
src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java,133,for (TaskAndWeakRefPair pair : tasks) {
src/main/java/org/apache/hadoop/hbase/mapreduce/RowCounter.java,122,String [] fields = columnName.split(":");
src/main/java/org/apache/hadoop/hbase/mapreduce/RowCounter.java,123,if(fields.length == 1) {
src/main/java/org/apache/hadoop/hbase/mapreduce/RowCounter.java,124,scan.addFamily(Bytes.toBytes(fields[0]));
src/main/java/org/apache/hadoop/hbase/mapreduce/RowCounter.java,126,scan.addColumn(Bytes.toBytes(fields[0]), Bytes.toBytes(fields[1]));
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,294,if (retainDeletesInOutput
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,296,|| kv.getMemstoreTS() > maxReadPointToTrackVersions) {
src/main/java/org/apache/hadoop/hbase/filter/NullComparator.java,41,throw new UnsupportedOperationException();
src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java,102,if (topScanner == null ||
src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java,103,this.comparator.compare(kvNext, topScanner.peek()) >= 0) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,937,boolean wasFlushing = false;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,942,wasFlushing = writestate.flushing;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,949,if (!abort && !wasFlushing && worthPreFlushing()) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,952,internalFlushcache(status);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,968,internalFlushcache(status);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1482,long flushsize = this.memstoreSize.get();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1546,this.addAndGetGlobalMemstoreSize(-flushsize);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1586,StringUtils.humanReadableInt(flushsize) + "/" + flushsize +
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1594,this.recentFlushes.add(new Pair<Long,Long>(time/1000, flushsize));
src/main/java/org/apache/hadoop/hbase/regionserver/ShutdownHook.java,194,if (!fsShutdownHooks.containsKey(hdfsClientFinalizer) &&
src/main/java/org/apache/hadoop/hbase/regionserver/ShutdownHook.java,195,!ShutdownHookManager.deleteShutdownHook(hdfsClientFinalizer)) {
src/main/java/org/apache/hadoop/hbase/regionserver/ShutdownHook.java,196,throw new RuntimeException("Failed suppression of fs shutdown hook: " +
src/main/java/org/apache/hadoop/hbase/regionserver/ShutdownHook.java,197,hdfsClientFinalizer);
src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java,229,NavigableSet<HostAndWeight> orderedHosts = new TreeSet<HostAndWeight>(
src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java,230,new HostAndWeight.WeightComparator());
src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java,231,orderedHosts.addAll(this.hostAndWeights.values());
src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java,232,List<String> topHosts = new ArrayList<String>(orderedHosts.size());
src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java,233,for(HostAndWeight haw : orderedHosts.descendingSet()) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,701,HDFSBlocksDistribution hdfsBlocksDistribution =
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,702,new HDFSBlocksDistribution();
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,79,int maxVersions, long oldestUnexpiredTS) {
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,167,return ScanQueryMatcher.MatchCode.SEEK_NEXT_COL;
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,176,this.columns = new ExplicitColumnTracker(columns,
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,177,scanInfo.getMinVersions(), maxVersions, oldestUnexpiredTS);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,72,import org.apache.hadoop.hbase.util.Bytes;
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,73,import org.apache.hadoop.hbase.util.ChecksumType;
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,74,import org.apache.hadoop.hbase.util.ClassSize;
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,75,import org.apache.hadoop.hbase.util.CollectionBackedScanner;
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,76,import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,77,import org.apache.hadoop.hbase.util.FSUtils;
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,584,if (!srcFs.getUri().equals(desFs.getUri())) {
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3417,out.println("   -sidelineCorruptHfiles  Quarantine corrupted HFiles.  implies -checkCorruptHfiles");
src/main/java/org/apache/hadoop/hbase/regionserver/TimeRangeTracker.java,136,public void write(final DataOutput out) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/TimeRangeTracker.java,141,public void readFields(final DataInput in) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/TimeRangeTracker.java,147,public String toString() {
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,369,public static byte [] isLegalTableName(final byte [] tableName) {
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,379,if (Character.isLetterOrDigit(tableName[i]) || tableName[i] == '_' ||
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,380,tableName[i] == '-' || tableName[i] == '.') {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,248,if (!this.isActive()) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,249,return;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,253,while (this.peerClusterId == null) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,255,if (this.peerClusterId == null) {
src/main/java/org/apache/hadoop/hbase/regionserver/TimeRangeTracker.java,98,private void includeTimestamp(final long timestamp) {
src/main/java/org/apache/hadoop/hbase/regionserver/TimeRangeTracker.java,117,public boolean includesTimeRange(final TimeRange tr) {
src/main/java/org/apache/hadoop/hbase/regionserver/TimeRangeTracker.java,125,public long getMinimumTimestamp() {
src/main/java/org/apache/hadoop/hbase/regionserver/TimeRangeTracker.java,132,public long getMaximumTimestamp() {
src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java,200,return new HTable(catalogTracker.getConnection().getConfiguration(), tableName);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,273,hris = MetaReader.getServerUserRegions(this.server.getCatalogTracker(),
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,274,this.serverName);
src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java,238,if (this.scanMetrics == null) {
src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java,371,try {
src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java,372,writeScanMetrics();
src/main/java/org/apache/hadoop/hbase/regionserver/IncreasingToUpperBoundRegionSplitPolicy.java,44,private long flushSize;
src/main/java/org/apache/hadoop/hbase/regionserver/IncreasingToUpperBoundRegionSplitPolicy.java,52,this.flushSize = desc.getMemStoreFlushSize();
src/main/java/org/apache/hadoop/hbase/regionserver/IncreasingToUpperBoundRegionSplitPolicy.java,54,if (this.flushSize <= 0) {
src/main/java/org/apache/hadoop/hbase/regionserver/IncreasingToUpperBoundRegionSplitPolicy.java,55,this.flushSize = conf.getLong(HConstants.HREGION_MEMSTORE_FLUSH_SIZE,
src/main/java/org/apache/hadoop/hbase/regionserver/IncreasingToUpperBoundRegionSplitPolicy.java,94,long getSizeToCheck(final int tableRegionsCount) {
src/main/java/org/apache/hadoop/hbase/regionserver/IncreasingToUpperBoundRegionSplitPolicy.java,95,return tableRegionsCount == 0? getDesiredMaxFileSize():
src/main/java/org/apache/hadoop/hbase/regionserver/IncreasingToUpperBoundRegionSplitPolicy.java,97,this.flushSize * (tableRegionsCount * tableRegionsCount));
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,739,return this.memstoreSize.getAndAdd(memStoreSize);
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,130,boolean blockUntilBecomingActiveMaster(MonitoredTask startupStatus,
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,131,ClusterStatusTracker clusterStatusTracker) {
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,204,if (!clusterStatusTracker.isClusterUp()) {
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,433,return this.activeMasterManager.blockUntilBecomingActiveMaster(startupStatus,
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,434,this.clusterStatusTracker);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,736,if (oldRequestCount == this.requestCount.get()) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,740,oldRequestCount = this.requestCount.get();
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3614,if (checkCorruptHFiles || sidelineCorruptHFiles) {
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3615,LOG.info("Checking all hfiles for corruption");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3616,HFileCorruptionChecker hfcc = createHFileCorruptionChecker(sidelineCorruptHFiles);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3617,setHFileCorruptionChecker(hfcc); // so we can get result
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3618,Collection<String> tables = getIncludedTables();
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3619,Collection<Path> tableDirs = new ArrayList<Path>();
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3620,Path rootdir = FSUtils.getRootDir(getConf());
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3621,if (tables.size() > 0) {
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3622,for (String t : tables) {
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3623,tableDirs.add(FSUtils.getTablePath(rootdir, t));
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3626,tableDirs = FSUtils.getTableDirs(FSUtils.getCurrentFileSystem(getConf()), rootdir);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3628,hfcc.checkTables(tableDirs);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3629,hfcc.report(errors);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3633,int code = onlineHbck();
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3634,setRetCode(code);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3639,if (shouldRerun()) {
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3640,try {
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3641,LOG.info("Sleeping " + sleepBeforeRerun + "ms before re-checking after fix...");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3642,Thread.sleep(sleepBeforeRerun);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3644,return this;
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3647,setFixAssignments(false);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3648,setFixMeta(false);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3649,setFixHdfsHoles(false);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3650,setFixHdfsOverlaps(false);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3651,setFixVersionFile(false);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3652,setFixTableOrphans(false);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3653,errors.resetErrors();
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3654,code = onlineHbck();
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,91,for (KeyValue kv : value.raw()) {
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,92,kv = filterKv(kv);
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,94,if (kv == null) continue;
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,96,context.write(row, convertKv(kv, cfRenameMap));
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,141,for (KeyValue kv : result.raw()) {
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,142,kv = filterKv(kv);
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,144,if (kv == null) continue;
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,146,kv = convertKv(kv, cfRenameMap);
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,148,if (kv.isDelete()) {
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,149,if (delete == null) {
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,150,delete = new Delete(key.get());
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,152,delete.addDeleteMarker(kv);
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,154,if (put == null) {
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,155,put = new Put(key.get());
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,157,put.add(kv);
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,160,if (put != null) {
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,161,put.setClusterId(clusterId);
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,162,context.write(key, put);
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,164,if (delete != null) {
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,165,delete.setClusterId(clusterId);
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,166,context.write(key, delete);
src/main/java/org/apache/hadoop/hbase/master/HMasterCommandLine.java,146,LocalHBaseCluster cluster = new LocalHBaseCluster(conf, 1, 1,
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,345,private int webuiport = -1;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1064,private void createMyEphemeralNode() throws KeeperException {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1065,ZKUtil.createEphemeralNodeAndWatch(this.zooKeeper, getMyEphemeralNodePath(),
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1066,HConstants.EMPTY_BYTE_ARRAY);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1591,this.webuiport = putUpWebUI();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1645,return port;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,3491,this.startcode, this.webuiport);
src/main/java/org/apache/hadoop/hbase/zookeeper/RegionServerTracker.java,25,import java.util.NavigableSet;
src/main/java/org/apache/hadoop/hbase/zookeeper/RegionServerTracker.java,26,import java.util.TreeSet;
src/main/java/org/apache/hadoop/hbase/zookeeper/RegionServerTracker.java,47,private NavigableSet<ServerName> regionServers = new TreeSet<ServerName>();
src/main/java/org/apache/hadoop/hbase/zookeeper/RegionServerTracker.java,78,this.regionServers.add(sn);
src/main/java/org/apache/hadoop/hbase/zookeeper/RegionServerTracker.java,127,return new ArrayList<ServerName>(this.regionServers);
src/main/java/org/apache/hadoop/hbase/mapreduce/hadoopbackport/InputSampler.java,167,Constructor<TaskAttemptContext> c;
src/main/java/org/apache/hadoop/hbase/mapreduce/hadoopbackport/InputSampler.java,169,c = TaskAttemptContext.class.getConstructor(Configuration.class, TaskAttemptID.class);
src/main/java/org/apache/hadoop/hbase/mapreduce/hadoopbackport/InputSampler.java,174,return c.newInstance(job.getConfiguration(), new TaskAttemptID());
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,250,System.out.println("Filter returned:" + code);
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,254,if (LOG.isDebugEnabled()) {
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,255,System.out.println("Skipping key: " + kv + " from filter decision: " + code);
src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java,113,try {
src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java,114,ReplicationZookeeper zk = new ReplicationZookeeper(conn, conf,
src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java,115,conn.getZooKeeperWatcher());
src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java,116,ReplicationPeer peer = zk.getPeer(conf.get(NAME+".peerId"));
src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java,117,HTable replicatedTable = new HTable(peer.getConfiguration(),
src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java,118,conf.get(NAME+".tableName"));
src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java,119,scan.setStartRow(value.getRow());
src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java,120,replicatedScanner = replicatedTable.getScanner(scan);
src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java,122,throw new IOException("Got a ZK exception", e);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,965,if(!watchAndCheckExists(zkw, znode)) {
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,967,return createEphemeralNodeAndWatch(zkw, znode, data);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,969,return false;
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,974,return true;
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,1004,try {
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,1005,zkw.getRecoverableZooKeeper().exists(znode, zkw);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,1007,zkw.interruptedException(e);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,1008,return false;
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,1015,return true;
security/src/main/java/org/apache/hadoop/hbase/security/access/SecureBulkLoadEndpoint.java,150,final Token<?> userToken, final String bulkToken) throws IOException {
security/src/main/java/org/apache/hadoop/hbase/security/access/SecureBulkLoadProtocol.java,65,Token<?> userToken, String bulkToken) throws IOException;
src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java,561,boolean success = svrCallable.withRetries();
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,172,protected ExecutorService executor; // threads to retrieve data from regionservers
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,1815,LOG.debug("Contained region dir after close and pause");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,1824,LOG.warn("HDFS region dir " + contained.getHdfsRegionDir() + " already sidelined.");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,1845,LOG.info("Moving files from " + src + " into containing region " + dst);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,1856,LOG.debug("Sideline directory contents:");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,1862,LOG.info("Sidelined region dir "+ contained.getHdfsRegionDir() + " into " +
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2121,LOG.info("== Merging regions into one region: "
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2139,LOG.debug("Closing region before moving data around: " +  hi);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2140,LOG.debug("Contained region dir before close");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2143,LOG.info("Closing region: " + hi);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2146,LOG.warn("Was unable to close region " + hi
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2149,LOG.warn("Was unable to close region " + hi
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2154,LOG.info("Offlining region: " + hi);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2157,LOG.warn("Unable to offline region from master: " + hi
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2168,LOG.info("Created new empty container region: " +
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2176,LOG.info("Merging " + contained  + " into " + target );
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2326,for (Collection<HbckInfo> overlap : overlapGroups.asMap().values()) {
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2327,handler.handleOverlapGroup(overlap);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,1820,FileStatus[] dirs = fs.listStatus(contained.getHdfsRegionDir());
src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java,638,if (parent.equals(rsServerNameZnode)) {
src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java,639,LOG.warn("Won't lock because this is us, we're dead!");
src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java,640,return false;
src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java,877,if (ke instanceof ConnectionLossException
src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java,878,|| ke instanceof SessionExpiredException) {
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,22,import java.util.ArrayList;
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,23,import java.util.List;
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,60,private final List<ColumnCount> columns;
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,83,this.columns = new ArrayList<ColumnCount>(columns.size());
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,180,this.column = this.columns.get(this.index);
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,202,this.column = this.columns.get(this.index);
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,250,this.column = this.columns.get(this.index);
src/main/java/org/apache/hadoop/hbase/zookeeper/MiniZooKeeperCluster.java,200,FileUtil.fullyDelete(dir);
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,276,Filter filter = (Filter)HbaseObjectWritable.readObject(in, conf);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2165,this.regions.remove(region);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1221,long now = System.currentTimeMillis();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2494,matches = compareResult <= 0;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2497,matches = compareResult < 0;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2506,matches = compareResult > 0;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2509,matches = compareResult >= 0;
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,266,OutputStream outStream = NetUtils.getOutputStream(socket);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2741,new EnableTableHandler(this.master, tableName.getBytes(),
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2742,catalogTracker, this, true).process();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,146,import org.apache.hadoop.hbase.security.User;
src/main/java/org/apache/hadoop/hbase/coprocessor/BaseRegionObserver.java,295,final InternalScanner s, final byte[] currentRow, final boolean hasMore) throws IOException {
src/main/java/org/apache/hadoop/hbase/coprocessor/RegionObserver.java,718,final InternalScanner s, final byte[] currentRow, final boolean hasMore) throws IOException;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3854,return this.region.getCoprocessorHost().postScannerFilterRow(this, currentRow);
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,1341,throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,1349,hasMore);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2854,WRONG_USAGE
src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java,439,this.out = new DataOutputStream
src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java,440,(new BufferedOutputStream(NetUtils.getOutputStream(socket)));
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,441,try {
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,442,for (StoreFile file : results) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2481,&& currentScanResultSize < maxScannerResultSize; i++) {
src/main/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java,666,throw new IOException("Can't find class " + className, e);
src/main/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java,677,throw new IOException("Can't find class " + className, e);
src/main/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java,707,throw new IOException("Class not found when attempting to " +
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/SchemaConfigured.java,127,cfName = cfName.intern();
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/SchemaConfigured.java,129,tableName = splits[splits.length - 4].intern();
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/SchemaConfigured.java,156,this.tableName = tableName != null ? tableName.intern() : tableName;
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/SchemaConfigured.java,157,this.cfName = cfName != null ? cfName.intern() : cfName;
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/SchemaConfigured.java,161,tableName = that.getTableName().intern();
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/SchemaConfigured.java,162,cfName = that.getColumnFamilyName().intern();
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/SchemaConfigured.java,213,target.tableName = tableName.intern();
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/SchemaConfigured.java,214,target.cfName = cfName.intern();
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/SchemaMetrics.java,298,blockMetricNames[i] = sb.toString().intern();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3522,private Filter filter;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3647,if (isFilterDone()) {
src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java,248,c = query.charAt(i);
src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java,260,c = query.charAt(i);
src/main/java/org/apache/hadoop/hbase/client/Scan.java,275,this.startRow = startRow;
src/main/java/org/apache/hadoop/hbase/client/Scan.java,286,this.stopRow = stopRow;
src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java,75,synchronized (this) {
src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java,76,tasks.add(pair);
src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java,89,synchronized (this) {
src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java,90,tasks.add(pair);
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,265,public synchronized KeyValue peek() {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,279,public synchronized void close() {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,292,public synchronized boolean seek(KeyValue key) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,310,public synchronized boolean next(List<KeyValue> outResult, int limit) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,321,public synchronized boolean next(List<KeyValue> outResult, int limit,
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,448,public synchronized boolean next(List<KeyValue> outResult) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,460,public synchronized void updateReaders() throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,537,public synchronized boolean reseek(KeyValue kv) throws IOException {
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,443,if (actualRegCount.get() != numRegs) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2125,boolean walSyncSuccessful = false;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2333,walSyncSuccessful = true;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2366,if (!walSyncSuccessful) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1827,this.replicationSourceHandler.stopReplicationService();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1829,this.replicationSinkHandler.stopReplicationService();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1597,this.replicationSourceHandler.startReplicationService();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1599,this.replicationSinkHandler.startReplicationService();
src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java,304,if (TsvParser.ROWKEY_COLUMN_SPEC.equals(aColumn)) continue;
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,72,import org.apache.hadoop.hbase.security.User;
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,997,synchronized (regionLockObject) {
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1009,if (useCache) {
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1010,location = getCachedLocation(tableName, row);
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1011,if (location != null) {
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1012,return location;
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1015,deleteCachedLocation(tableName, row);
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1019,regionInfoRow = server.getClosestRowBefore(
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1020,metaLocation.getRegionInfo().getRegionName(), metaKey,
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1021,HConstants.CATALOG_FAMILY);
src/main/java/org/apache/hadoop/hbase/client/Append.java,26,import java.util.Map;
src/main/java/org/apache/hadoop/hbase/client/Append.java,30,import org.apache.hadoop.io.Writable;
src/main/java/org/apache/hadoop/hbase/client/Append.java,108,int numFamilies = in.readInt();
src/main/java/org/apache/hadoop/hbase/client/Append.java,110,for(int i=0;i<numFamilies;i++) {
src/main/java/org/apache/hadoop/hbase/client/Append.java,111,byte [] family = Bytes.readByteArray(in);
src/main/java/org/apache/hadoop/hbase/client/Append.java,112,int numKeys = in.readInt();
src/main/java/org/apache/hadoop/hbase/client/Append.java,113,List<KeyValue> keys = new ArrayList<KeyValue>(numKeys);
src/main/java/org/apache/hadoop/hbase/client/Append.java,114,int totalLen = in.readInt();
src/main/java/org/apache/hadoop/hbase/client/Append.java,115,byte [] buf = new byte[totalLen];
src/main/java/org/apache/hadoop/hbase/client/Append.java,116,int offset = 0;
src/main/java/org/apache/hadoop/hbase/client/Append.java,117,for (int j = 0; j < numKeys; j++) {
src/main/java/org/apache/hadoop/hbase/client/Append.java,118,int keyLength = in.readInt();
src/main/java/org/apache/hadoop/hbase/client/Append.java,119,in.readFully(buf, offset, keyLength);
src/main/java/org/apache/hadoop/hbase/client/Append.java,120,keys.add(new KeyValue(buf, offset, keyLength));
src/main/java/org/apache/hadoop/hbase/client/Append.java,121,offset += keyLength;
src/main/java/org/apache/hadoop/hbase/client/Append.java,123,this.familyMap.put(family, keys);
src/main/java/org/apache/hadoop/hbase/client/Append.java,136,out.writeInt(familyMap.size());
src/main/java/org/apache/hadoop/hbase/client/Append.java,137,for (Map.Entry<byte [], List<KeyValue>> entry : familyMap.entrySet()) {
src/main/java/org/apache/hadoop/hbase/client/Append.java,138,Bytes.writeByteArray(out, entry.getKey());
src/main/java/org/apache/hadoop/hbase/client/Append.java,139,List<KeyValue> keys = entry.getValue();
src/main/java/org/apache/hadoop/hbase/client/Append.java,140,out.writeInt(keys.size());
src/main/java/org/apache/hadoop/hbase/client/Append.java,141,int totalLen = 0;
src/main/java/org/apache/hadoop/hbase/client/Append.java,142,for(KeyValue kv : keys) {
src/main/java/org/apache/hadoop/hbase/client/Append.java,143,totalLen += kv.getLength();
src/main/java/org/apache/hadoop/hbase/client/Append.java,145,out.writeInt(totalLen);
src/main/java/org/apache/hadoop/hbase/client/Append.java,146,for(KeyValue kv : keys) {
src/main/java/org/apache/hadoop/hbase/client/Append.java,147,out.writeInt(kv.getLength());
src/main/java/org/apache/hadoop/hbase/client/Append.java,148,out.write(kv.getBuffer(), kv.getOffset(), kv.getLength());
src/main/java/org/apache/hadoop/hbase/client/Delete.java,149,if (!kv.isDelete()) {
src/main/java/org/apache/hadoop/hbase/client/Delete.java,153,if (Bytes.compareTo(this.row, 0, row.length, kv.getBuffer(),
src/main/java/org/apache/hadoop/hbase/client/Delete.java,154,kv.getRowOffset(), kv.getRowLength()) != 0) {
src/main/java/org/apache/hadoop/hbase/client/Delete.java,156,+ Bytes.toStringBinary(kv.getBuffer(), kv.getRowOffset(),
src/main/java/org/apache/hadoop/hbase/client/Delete.java,157,kv.getRowLength()) + " doesn't match the original one "
src/main/java/org/apache/hadoop/hbase/client/Delete.java,298,int numFamilies = in.readInt();
src/main/java/org/apache/hadoop/hbase/client/Delete.java,299,for(int i=0;i<numFamilies;i++) {
src/main/java/org/apache/hadoop/hbase/client/Delete.java,300,byte [] family = Bytes.readByteArray(in);
src/main/java/org/apache/hadoop/hbase/client/Delete.java,301,int numColumns = in.readInt();
src/main/java/org/apache/hadoop/hbase/client/Delete.java,302,List<KeyValue> list = new ArrayList<KeyValue>(numColumns);
src/main/java/org/apache/hadoop/hbase/client/Delete.java,303,for(int j=0;j<numColumns;j++) {
src/main/java/org/apache/hadoop/hbase/client/Delete.java,304,KeyValue kv = new KeyValue();
src/main/java/org/apache/hadoop/hbase/client/Delete.java,305,kv.readFields(in);
src/main/java/org/apache/hadoop/hbase/client/Delete.java,306,list.add(kv);
src/main/java/org/apache/hadoop/hbase/client/Delete.java,308,this.familyMap.put(family, list);
src/main/java/org/apache/hadoop/hbase/client/Delete.java,321,out.writeInt(familyMap.size());
src/main/java/org/apache/hadoop/hbase/client/Delete.java,322,for(Map.Entry<byte [], List<KeyValue>> entry : familyMap.entrySet()) {
src/main/java/org/apache/hadoop/hbase/client/Delete.java,323,Bytes.writeByteArray(out, entry.getKey());
src/main/java/org/apache/hadoop/hbase/client/Delete.java,324,List<KeyValue> list = entry.getValue();
src/main/java/org/apache/hadoop/hbase/client/Delete.java,325,out.writeInt(list.size());
src/main/java/org/apache/hadoop/hbase/client/Delete.java,326,for(KeyValue kv : list) {
src/main/java/org/apache/hadoop/hbase/client/Delete.java,327,kv.write(out);
src/main/java/org/apache/hadoop/hbase/client/Put.java,160,int res = Bytes.compareTo(this.row, 0, row.length,
src/main/java/org/apache/hadoop/hbase/client/Put.java,161,kv.getBuffer(), kv.getRowOffset(), kv.getRowLength());
src/main/java/org/apache/hadoop/hbase/client/Put.java,162,if(res != 0) {
src/main/java/org/apache/hadoop/hbase/client/Put.java,163,throw new IOException("The row in the recently added KeyValue " +
src/main/java/org/apache/hadoop/hbase/client/Put.java,164,Bytes.toStringBinary(kv.getBuffer(), kv.getRowOffset(),
src/main/java/org/apache/hadoop/hbase/client/Put.java,165,kv.getRowLength()) + " doesn't match the original one " +
src/main/java/org/apache/hadoop/hbase/client/Put.java,166,Bytes.toStringBinary(this.row));
src/main/java/org/apache/hadoop/hbase/client/Put.java,180,return  new KeyValue(this.row, family, qualifier, ts, KeyValue.Type.Put,
src/main/java/org/apache/hadoop/hbase/client/Put.java,378,int numFamilies = in.readInt();
src/main/java/org/apache/hadoop/hbase/client/Put.java,380,for(int i=0;i<numFamilies;i++) {
src/main/java/org/apache/hadoop/hbase/client/Put.java,381,byte [] family = Bytes.readByteArray(in);
src/main/java/org/apache/hadoop/hbase/client/Put.java,382,int numKeys = in.readInt();
src/main/java/org/apache/hadoop/hbase/client/Put.java,383,List<KeyValue> keys = new ArrayList<KeyValue>(numKeys);
src/main/java/org/apache/hadoop/hbase/client/Put.java,384,int totalLen = in.readInt();
src/main/java/org/apache/hadoop/hbase/client/Put.java,385,byte [] buf = new byte[totalLen];
src/main/java/org/apache/hadoop/hbase/client/Put.java,386,int offset = 0;
src/main/java/org/apache/hadoop/hbase/client/Put.java,387,for (int j = 0; j < numKeys; j++) {
src/main/java/org/apache/hadoop/hbase/client/Put.java,388,int keyLength = in.readInt();
src/main/java/org/apache/hadoop/hbase/client/Put.java,389,in.readFully(buf, offset, keyLength);
src/main/java/org/apache/hadoop/hbase/client/Put.java,390,keys.add(new KeyValue(buf, offset, keyLength));
src/main/java/org/apache/hadoop/hbase/client/Put.java,391,offset += keyLength;
src/main/java/org/apache/hadoop/hbase/client/Put.java,393,this.familyMap.put(family, keys);
src/main/java/org/apache/hadoop/hbase/client/Put.java,407,out.writeInt(familyMap.size());
src/main/java/org/apache/hadoop/hbase/client/Put.java,408,for (Map.Entry<byte [], List<KeyValue>> entry : familyMap.entrySet()) {
src/main/java/org/apache/hadoop/hbase/client/Put.java,409,Bytes.writeByteArray(out, entry.getKey());
src/main/java/org/apache/hadoop/hbase/client/Put.java,410,List<KeyValue> keys = entry.getValue();
src/main/java/org/apache/hadoop/hbase/client/Put.java,411,out.writeInt(keys.size());
src/main/java/org/apache/hadoop/hbase/client/Put.java,412,int totalLen = 0;
src/main/java/org/apache/hadoop/hbase/client/Put.java,413,for(KeyValue kv : keys) {
src/main/java/org/apache/hadoop/hbase/client/Put.java,414,totalLen += kv.getLength();
src/main/java/org/apache/hadoop/hbase/client/Put.java,416,out.writeInt(totalLen);
src/main/java/org/apache/hadoop/hbase/client/Put.java,417,for(KeyValue kv : keys) {
src/main/java/org/apache/hadoop/hbase/client/Put.java,418,out.writeInt(kv.getLength());
src/main/java/org/apache/hadoop/hbase/client/Put.java,419,out.write(kv.getBuffer(), kv.getOffset(), kv.getLength());
src/main/java/org/apache/hadoop/hbase/KeyValue.java,67,public class KeyValue implements Writable, HeapSize {
src/main/java/org/apache/hadoop/hbase/KeyValue.java,289,public KeyValue(final byte [] bytes, final int offset, final int length, final int keyLength) {
src/main/java/org/apache/hadoop/hbase/KeyValue.java,290,this.bytes = bytes;
src/main/java/org/apache/hadoop/hbase/KeyValue.java,291,this.offset = offset;
src/main/java/org/apache/hadoop/hbase/KeyValue.java,292,this.length = length;
src/main/java/org/apache/hadoop/hbase/KeyValue.java,293,this.keyLength = keyLength;
src/main/java/org/apache/hadoop/hbase/KeyValue.java,834,private int keyLength = 0;
src/main/java/org/apache/hadoop/hbase/KeyValue.java,837,if (keyLength == 0) {
src/main/java/org/apache/hadoop/hbase/KeyValue.java,838,keyLength = Bytes.toInt(this.bytes, this.offset);
src/main/java/org/apache/hadoop/hbase/KeyValue.java,840,return keyLength;
src/main/java/org/apache/hadoop/hbase/KeyValue.java,2265,+ (3 * Bytes.SIZEOF_INT) + Bytes.SIZEOF_LONG);
src/main/java/org/apache/hadoop/hbase/KeyValue.java,2274,this.keyLength = 0;
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,623,KEY_VALUE_LEN_SIZE + currKeyLen + currValueLen,
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,624,currKeyLen);
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,219,int initialOffset = offset;
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,254,int qualLength = keyLength + KeyValue.ROW_OFFSET -
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,257,long timestamp = kv.getTimestamp();
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,276,byte type = kv.getType();
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,74,private final ArrayList<KeyValue> kvs = new ArrayList<KeyValue>();
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,98,public List<KeyValue> getKeyValues() {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,175,long ret = 0;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationHLogReaderManager.java,81,public HLog.Entry readNextAndSetPosition(HLog.Entry[] entriesArray,
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationHLogReaderManager.java,82,int currentNbEntries) throws IOException {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationHLogReaderManager.java,83,HLog.Entry entry = this.reader.next(entriesArray[currentNbEntries]);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,28,import java.util.Arrays;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,83,private HLog.Entry[] entriesArray;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,131,private int currentNbEntries = 0;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,168,this.entriesArray = new HLog.Entry[this.replicationQueueNbCapacity];
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,169,for (int i = 0; i < this.replicationQueueNbCapacity; i++) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,170,this.entriesArray[i] = new HLog.Entry();
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,328,currentNbEntries = 0;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,331,if (readAllEntriesToReplicateOrNextFile(currentWALisBeingWrittenTo)) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,351,LOG.warn(peerClusterZnode + " Got EOF while reading, " +
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,353,considerDumping = true;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,354,currentNbEntries = 0;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,375,if (this.isActive() && (gotIOE || currentNbEntries == 0)) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,387,shipEdits(currentWALisBeingWrittenTo);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,413,this.repLogReader.readNextAndSetPosition(this.entriesArray, this.currentNbEntries);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,436,currentNbEntries++;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,444,currentNbEntries >= this.replicationQueueNbCapacity) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,448,entry = this.repLogReader.readNextAndSetPosition(this.entriesArray, this.currentNbEntries);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,591,List<KeyValue> kvs = edit.getKeyValues();
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,592,for (int i = edit.size()-1; i >= 0; i--) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,625,protected void shipEdits(boolean currentWALisBeingWrittenTo) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,627,if (this.currentNbEntries == 0) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,640,LOG.debug("Replicating " + currentNbEntries);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,641,rrs.replicateLogEntries(Arrays.copyOf(this.entriesArray, currentNbEntries));
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,647,this.totalReplicatedEdits += currentNbEntries;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,652,this.entriesArray[currentNbEntries-1].getKey().getWriteTime());
src/main/java/org/apache/hadoop/hbase/master/ServerManager.java,223,long skew = System.currentTimeMillis() - serverCurrentTime;
src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java,236,userToken = fs.getDelegationToken("renewer");
src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java,270,if(userToken != null) {
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,281,public static void initCredentials(Job job) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1026,createMyEphemeralNode();
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1827,if (this.master == null) {
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1828,this.master = getMaster();
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1830,HTableDescriptor[] htd = master.getHTableDescriptors();
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1837,if (this.master == null) {
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1838,this.master = getMaster();
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1840,return master.getHTableDescriptors(tableNames);
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1852,if (this.master == null) {
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1853,this.master = getMaster();
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,508,ByteBuffer bb = getKey();
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,509,compared = reader.getComparator().compare(key, offset,
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,510,length, bb.array(), bb.arrayOffset(), bb.limit());
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,173,if (isEnablingTable(tableName)) {
src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java,545,throw new CorruptHFileException("Problem reading HFile Trailer from file " + path, iae);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java,547,switch (trailer.getMajorVersion()) {
src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java,548,case 1:
src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java,549,return new HFileReaderV1(path, trailer, fsdis, size, closeIStream,
src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java,550,cacheConf);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java,551,case 2:
src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java,552,return new HFileReaderV2(path, trailer, fsdis, fsdisNoFsChecksum,
src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java,553,size, closeIStream,
src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java,554,cacheConf, preferredEncodingInCache, hfs);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java,555,default:
src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java,556,throw new CorruptHFileException("Invalid HFile version " + trailer.getMajorVersion());
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4736,if (!walSyncSuccessful) {
src/main/java/org/apache/hadoop/hbase/filter/PrefixFilter.java,23,import org.apache.hadoop.hbase.KeyValue;
src/main/java/org/apache/hadoop/hbase/filter/PrefixFilter.java,24,import org.apache.hadoop.hbase.util.Bytes;
src/main/java/org/apache/hadoop/hbase/filter/PrefixFilter.java,28,import java.io.DataInput;
src/main/java/org/apache/hadoop/hbase/filter/PrefixFilter.java,29,import java.util.List;
src/main/java/org/apache/hadoop/hbase/filter/PrefixFilter.java,66,return cmp != 0;
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,1816,if (LOG.isDebugEnabled()) {
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,1817,LOG.debug("Received dynamic protocol exec call with protocolName " + protocolName);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,5312,if (LOG.isDebugEnabled()) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,5313,LOG.debug("Received dynamic protocol exec call with protocolName " + protocolName);
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,641,new MemStoreScanner());
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,711,MemStoreScanner() {
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,719,long readPoint = MultiVersionConsistencyControl.getThreadReadPoint();
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,1303,isCompaction), !isCompaction);
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java,68,public StoreFileScanner(StoreFile.Reader reader, HFileScanner hfs, boolean useMVCC) {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java,132,skipKVsNewerThanReadpoint();
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java,153,return skipKVsNewerThanReadpoint();
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java,173,return skipKVsNewerThanReadpoint();
src/main/java/org/apache/hadoop/hbase/coprocessor/SecureBulkLoadClient.java,73,public boolean bulkLoadHFiles(List<Pair<byte[], String>> familyPaths,
src/main/java/org/apache/hadoop/hbase/coprocessor/SecureBulkLoadClient.java,74,Token<?> userToken, String bulkToken) throws IOException {
src/main/java/org/apache/hadoop/hbase/coprocessor/SecureBulkLoadClient.java,76,return (Boolean)Methods.call(protocolClazz, proxy, "bulkLoadHFiles",
src/main/java/org/apache/hadoop/hbase/coprocessor/SecureBulkLoadClient.java,77,new Class[]{List.class, Token.class, String.class},new Object[]{familyPaths, userToken, bulkToken});
src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java,96,private static final int  TABLE_CREATE_MAX_RETRIES = 20;
src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java,97,private static final long TABLE_CREATE_SLEEP = 60000;
src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java,522,success = server.bulkLoadHFiles(famPaths, regionName);
src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java,526,success = secureClient.bulkLoadHFiles(famPaths, userToken, bulkToken);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,582,long storeSeqId = store.getMaxSequenceId();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,586,if (maxSeqId == -1 || storeSeqId > maxSeqId) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,587,maxSeqId = storeSeqId;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3377,return bulkLoadHFiles(familyPaths, null);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3455,store.bulkLoadHFile(finalPath);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2762,loaded = region.bulkLoadHFiles(familyPaths);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,320,long getMaxSequenceId() {
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,321,return StoreFile.getMaxSequenceIdInList(this.getStorefiles());
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,574,void bulkLoadHFile(String srcPathStr) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,593,Path dstPath = StoreFile.getRandomFilename(fs, homedir);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1049,long maxId = StoreFile.getMaxSequenceIdInList(filesToCompact);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1118,maxId = StoreFile.getMaxSequenceIdInList(filesToCompact);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1121,Collections.sort(filesCompacting, StoreFile.Comparators.FLUSH_TIME);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1339,Collections.sort(filesCompacting, StoreFile.Comparators.FLUSH_TIME);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1680,Collections.sort(storeFiles, StoreFile.Comparators.FLUSH_TIME);
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,395,public static long getMaxSequenceIdInList(Collection<StoreFile> sfs) {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,398,if (!sf.isBulkLoadResult()) {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,1737,static final Comparator<StoreFile> FLUSH_TIME =
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,1739,Ordering.natural().onResultOf(new GetBulkTime()),
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,1744,private static class GetBulkTime implements Function<StoreFile, Long> {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,1747,if (!sf.isBulkLoadResult()) return Long.MAX_VALUE;
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,1748,return sf.getBulkLoadTimestamp();
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,1751,private static class GetSeqId implements Function<StoreFile, Long> {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,1754,if (sf.isBulkLoadResult()) return -1L;
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,1755,return sf.getMaxSequenceId();
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,1510,private long obtainSeqNum() {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4415,if (srcFiles.size() == 2) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4416,long seqA = srcFiles.get(0).getMaxSequenceId();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4417,long seqB = srcFiles.get(1).getMaxSequenceId();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4418,if (seqA == seqB) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4422,throw new IOException("Files have same sequenceid: " + seqA);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java,340,System.out.println("Mid-key: " + Bytes.toStringBinary(reader.midkey()));
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,845,TaskBatch batch;
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,1112,LOG.debug(path +
src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueSkipListSet.java,46,class KeyValueSkipListSet implements NavigableSet<KeyValue> {
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,904,ClassSize.COPYONWRITE_ARRAYSET + ClassSize.COPYONWRITE_ARRAYLIST +
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,905,(2 * ClassSize.CONCURRENT_SKIPLISTMAP));
src/main/java/org/apache/hadoop/hbase/client/HTable.java,168,this.pool = new ThreadPoolExecutor(1, maxThreads,
src/main/java/org/apache/hadoop/hbase/client/HTable.java,172,((ThreadPoolExecutor)this.pool).allowCoreThreadTimeOut(true);
src/main/java/org/apache/hadoop/hbase/client/HTable.java,174,this.finishSetup();
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,431,public HTableWrapper(byte[] tableName) throws IOException {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,433,this.table = new HTable(conf, tableName);
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,438,table.close();
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,731,return new HTableWrapper(tableName);
src/main/java/org/apache/hadoop/hbase/security/User.java,226,return "kerberos".equalsIgnoreCase(conf.get(HBASE_SECURITY_CONF_KEY)) &&
src/main/java/org/apache/hadoop/hbase/security/User.java,228,conf.get(CommonConfigurationKeys.HADOOP_SECURITY_AUTHENTICATION));
src/main/java/org/apache/hadoop/hbase/regionserver/ColumnTracker.java,58,public ScanQueryMatcher.MatchCode checkColumn(byte[] bytes, int offset,
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,107,int length, long timestamp, byte type, boolean ignoreCount) {
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,129,if (ignoreCount) return ScanQueryMatcher.MatchCode.INCLUDE;
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,132,if (sameAsPreviousTS(timestamp)) {
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,134,return ScanQueryMatcher.MatchCode.SKIP;
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,136,int count = this.column.increment();
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,137,if(count >= maxVersions || (count >= minVersions && isExpired(timestamp))) {
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,145,resetTS();
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,148,this.column = null;
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,149,return ScanQueryMatcher.MatchCode.INCLUDE_AND_SEEK_NEXT_ROW;
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,153,this.column = this.columns.get(this.index);
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,154,return ScanQueryMatcher.MatchCode.INCLUDE_AND_SEEK_NEXT_COL;
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,157,setTS(timestamp);
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,342,ReturnCode filterResponse = ReturnCode.SKIP;
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,343,if (filter != null) {
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,344,filterResponse = filter.filterKeyValue(kv);
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,345,if (filterResponse == ReturnCode.SKIP) {
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,346,return MatchCode.SKIP;
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,348,return columns.getNextRowOrNextColumn(bytes, offset, qualLength);
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,350,stickyNextRow = true;
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,351,return MatchCode.SEEK_NEXT_ROW;
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,353,return MatchCode.SEEK_NEXT_USING_HINT;
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,357,MatchCode colChecker = columns.checkColumn(bytes, offset, qualLength,
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,358,timestamp, type, kv.getMemstoreTS() > maxReadPointToTrackVersions);
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,364,if (colChecker == MatchCode.SEEK_NEXT_ROW) {
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,365,stickyNextRow = true;
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,367,filterResponse == ReturnCode.INCLUDE_AND_NEXT_COL) {
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,368,return MatchCode.INCLUDE_AND_SEEK_NEXT_COL;
src/main/java/org/apache/hadoop/hbase/regionserver/ScanWildcardColumnTracker.java,68,public MatchCode checkColumn(byte[] bytes, int offset, int length,
src/main/java/org/apache/hadoop/hbase/regionserver/ScanWildcardColumnTracker.java,69,long timestamp, byte type, boolean ignoreCount) throws IOException {
src/main/java/org/apache/hadoop/hbase/io/ImmutableBytesWritable.java,69,this(ibw.get(), 0, ibw.getSize());
src/main/java/org/apache/hadoop/hbase/filter/RegexStringComparator.java,96,return pattern.matcher(new String(value, offset, length, charset)).find() ? 0
src/main/java/org/apache/hadoop/hbase/filter/RegexStringComparator.java,97,: 1;
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,1556,this.replicationQueue = new LinkedBlockingQueue<Call>(maxQueueSize);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,75,private NavigableMap<byte[], Integer> scopes;
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,102,public NavigableMap<byte[], Integer> getScopes() {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,103,return scopes;
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,107,public void setScopes (NavigableMap<byte[], Integer> scopes) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,110,this.scopes = scopes;
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,115,if (scopes != null) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,116,scopes.clear();
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,131,int numFamilies = in.readInt();
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,132,if (numFamilies > 0) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,133,if (scopes == null) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,134,scopes = new TreeMap<byte[], Integer>(Bytes.BYTES_COMPARATOR);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,136,for (int i = 0; i < numFamilies; i++) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,137,byte[] fam = Bytes.readByteArray(in);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,139,scopes.put(fam, scope);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,163,if (scopes == null) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,164,out.writeInt(0);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,166,out.writeInt(scopes.size());
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,167,for (byte[] key : scopes.keySet()) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,168,Bytes.writeByteArray(out, key);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,169,out.writeInt(scopes.get(key));
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,179,if (scopes != null) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,180,ret += ClassSize.TREEMAP;
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,181,ret += ClassSize.align(scopes.size() * ClassSize.MAP_ENTRY);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,195,if (scopes != null) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,196,sb.append(" scopes: " + scopes.toString());
src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java,173,NavigableMap<byte[], Integer> scopes =
src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java,174,new TreeMap<byte[], Integer>(Bytes.BYTES_COMPARATOR);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java,180,!scopes.containsKey(family)) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java,181,scopes.put(family, scope);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java,184,if (!scopes.isEmpty()) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java,185,logEdit.setScopes(scopes);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,32,import java.util.NavigableMap;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,60,import org.apache.hadoop.hbase.zookeeper.ClusterId;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,421,if (!logKey.getClusterId().equals(peerClusterId)) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,590,NavigableMap<byte[], Integer> scopes = edit.getScopes();
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,596,if (scopes == null || !scopes.containsKey(kv.getFamily())) {
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureServer.java,89,public static final Set<Byte> INSECURE_VERSIONS = ImmutableSet.of((byte) 3);
src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java,163,MetaEditor.addRegionsToMeta(this.catalogTracker, regionInfos);
src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java,111,HBaseRPC.setRpcTimeout(this.callTimeout);
src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java,112,this.startTime = System.currentTimeMillis();
src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java,117,this.endTime = System.currentTimeMillis();
src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java,120,public void shouldRetry(Throwable throwable) throws IOException {
src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java,122,if (throwable instanceof SocketTimeoutException
src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java,123,|| (this.endTime - this.startTime > this.callTimeout)) {
src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java,124,throw (SocketTimeoutException) (SocketTimeoutException) new SocketTimeoutException(
src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java,128,.initCause(throwable);
src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java,130,this.callTimeout = ((int) (this.endTime - this.startTime));
src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java,165,shouldRetry(t);
src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java,180,System.currentTimeMillis(), toString());
src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java,189,Thread.sleep(ConnectionUtils.getPauseTime(pause, tries));
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,790,if (info != null) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,25,import java.lang.Thread.UncaughtExceptionHandler;
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2455,static class GeneralBulkAssigner extends StartupBulkAssigner {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2456,GeneralBulkAssigner(final Server server,
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2457,final Map<ServerName, List<HRegionInfo>> bulkPlan,
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2458,final AssignmentManager am) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2459,super(server, bulkPlan, am);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2463,protected UncaughtExceptionHandler getUncaughtExceptionHandler() {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2464,return new UncaughtExceptionHandler() {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2466,public void uncaughtException(Thread t, Throwable e) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2467,LOG.warn("Assigning regions in " + t.getName(), e);
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,732,void fixupDaughters(final MonitoredTask status) throws IOException {
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,756,fixups += ServerShutdownHandler.fixupDaughters(
src/main/java/org/apache/hadoop/hbase/regionserver/IncreasingToUpperBoundRegionSplitPolicy.java,83,break;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,31,import org.apache.hadoop.hbase.client.HTable;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,38,import org.apache.hadoop.hbase.util.Threads;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,46,import java.util.concurrent.ExecutorService;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,47,import java.util.concurrent.SynchronousQueue;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,48,import java.util.concurrent.ThreadPoolExecutor;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,49,import java.util.concurrent.TimeUnit;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,71,private final ExecutorService sharedThreadPool;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,87,this.sharedThreadPool = new ThreadPoolExecutor(1,
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,88,conf.getInt("hbase.htable.threads.max", Integer.MAX_VALUE),
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,89,conf.getLong("hbase.htable.threads.keepalivetime", 60), TimeUnit.SECONDS,
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,90,new SynchronousQueue<Runnable>(), Threads.newDaemonThreadFactory("hbase-repl"));
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,91,((ThreadPoolExecutor)this.sharedThreadPool).allowCoreThreadTimeOut(true);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,124,Map<byte[], List<Row>> rows = new TreeMap<byte[], List<Row>>(Bytes.BYTES_COMPARATOR);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,136,del.setClusterId(entry.getKey().getClusterId());
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,137,addToMultiMap(rows, table, del);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,140,put.setClusterId(entry.getKey().getClusterId());
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,141,addToMultiMap(rows, table, put);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,153,for(byte [] table : rows.keySet()) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,154,batch(table, rows.get(table));
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,174,private <K, V> List<V> addToMultiMap(Map<K, List<V>> map, K key, V value) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,175,List<V> values = map.get(key);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,178,map.put(key, values);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,188,try {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,189,this.sharedThreadPool.shutdown();
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,190,if (!this.sharedThreadPool.awaitTermination(60000, TimeUnit.MILLISECONDS)) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,191,this.sharedThreadPool.shutdownNow();
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,194,LOG.warn("Interrupted while closing the table pool", e); // ignoring it as we are closing.
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,195,Thread.currentThread().interrupt();
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,210,private void batch(byte[] tableName, List<Row> rows) throws IOException {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,211,if (rows.isEmpty()) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,216,table = new HTable(tableName, this.sharedHtableCon, this.sharedThreadPool);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,217,table.batch(rows);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,218,this.metrics.appliedOpsRate.inc(rows.size());
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,217,switch (filter.filterKeyValue(v)) {
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,295,if (curKeyHint == null && operator == Operator.MUST_PASS_ONE) {
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,306,if (operator == Operator.MUST_PASS_ALL &&
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,307,KeyValue.COMPARATOR.compare(keyHint, curKeyHint) < 0) {
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,309,keyHint = curKeyHint;
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,311,KeyValue.COMPARATOR.compare(keyHint, curKeyHint) > 0) {
src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat.java,96,final int blocksize = conf.getInt("hbase.mapreduce.hfileoutputformat.blocksize",
src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat.java,97,HFile.DEFAULT_BLOCKSIZE);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,154,private final CopyOnWriteArraySet<ChangedReadersObserver> changedReaderObservers =
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,155,new CopyOnWriteArraySet<ChangedReadersObserver>();
src/main/java/org/apache/hadoop/hbase/client/Append.java,91,list.add(new KeyValue(
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,24,import java.lang.reflect.InvocationTargetException;
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,29,import org.apache.hadoop.fs.FSDataOutputStream;
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,32,import org.apache.hadoop.hbase.RemoteExceptionHandler;
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,34,import org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException;
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,52,public static final long LEASE_SOFTLIMIT_PERIOD = 60 * 1000;
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,56,throws IOException{
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,63,if (!(fs instanceof DistributedFileSystem)) {
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,64,return;
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,66,LOG.info("Recovering file " + p);
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,67,long startWaiting = System.currentTimeMillis();
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,71,while (!recovered) {
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,73,try {
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,87,FSDataOutputStream out = fs.append(p);
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,88,out.close();
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,92,e = RemoteExceptionHandler.checkIOException(e);
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,93,if (e instanceof AlreadyBeingCreatedException) {
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,98,long waitedFor = System.currentTimeMillis() - startWaiting;
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,99,if (waitedFor > LEASE_SOFTLIMIT_PERIOD) {
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,100,LOG.warn("Waited " + waitedFor + "ms for lease recovery on " + p +
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,104,e.getMessage().contains("File does not exist")) {
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,106,throw new FileNotFoundException(
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,109,throw new IOException("Failed to open " + p + " for append", e);
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,112,try {
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,55,private boolean retainAssignment = false;
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,66,this.retainAssignment = skipTableStateCheck;
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,134,BulkEnabler bd = new BulkEnabler(this.server, regions, countOfRegionsInTable,
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,135,this.retainAssignment);
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,166,if (this.retainAssignment) {
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,190,private final boolean retainAssignment;
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,197,this.retainAssignment = retainAssignment;
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,202,boolean roundRobinAssignment = this.server.getConfiguration().getBoolean(
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,205,if (retainAssignment || !roundRobinAssignment) {
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,206,for (HRegionInfo region : regions) {
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,207,if (assignmentManager.isRegionInTransition(region) != null) {
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,208,continue;
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,210,final HRegionInfo hri = region;
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,211,pool.execute(new Runnable() {
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,212,public void run() {
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,213,if (retainAssignment) {
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,214,assignmentManager.assign(hri, true, false, false);
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,216,assignmentManager.assign(hri, true);
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,222,try {
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,223,assignmentManager.assignUserRegionsToOnlineServers(regions);
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,225,LOG.warn("Assignment was interrupted");
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,226,Thread.currentThread().interrupt();
src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java,981,call.wait();                           // wait for the result
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,442,MetaScanner.metaScan(conf, visitor, desc.getName());
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,1651,MetaScanner.metaScan(conf, visitor);
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,1778,if (this.connection != null) {
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,804,MetaScanner.metaScan(conf, visitor);
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,947,MetaScanner.metaScan(conf, visitor, tableName, row,
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,948,this.prefetchRegionLimit);
src/main/java/org/apache/hadoop/hbase/client/HTable.java,28,import java.util.Arrays;
src/main/java/org/apache/hadoop/hbase/client/HTable.java,36,import java.util.concurrent.ThreadFactory;
src/main/java/org/apache/hadoop/hbase/client/HTable.java,39,import java.util.concurrent.atomic.AtomicInteger;
src/main/java/org/apache/hadoop/hbase/client/HTable.java,214,if (pool == null || pool.isShutdown()) {
src/main/java/org/apache/hadoop/hbase/client/HTable.java,215,throw new IllegalArgumentException("Pool is null or shut down.");
src/main/java/org/apache/hadoop/hbase/client/HTable.java,485,return MetaScanner.allTableRegions(getConfiguration(), getTableName(), false);
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,38,import org.apache.hadoop.hbase.client.HConnectionManager.HConnectable;
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,67,metaScan(configuration, visitor, null);
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,80,public static void metaScan(Configuration configuration,
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,83,metaScan(configuration, visitor, userTableName, null, Integer.MAX_VALUE);
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,105,metaScan(configuration, visitor, userTableName, row, rowLimit,
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,125,public static void metaScan(Configuration configuration,
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,126,final MetaScannerVisitor visitor, final byte[] tableName,
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,128,throws IOException {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,129,try {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,130,HConnectionManager.execute(new HConnectable<Void>(configuration) {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,132,public Void connect(HConnection connection) throws IOException {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,133,metaScan(conf, connection, visitor, tableName, row, rowLimit,
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,134,metaTableName);
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,135,return null;
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,139,visitor.close();
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,143,private static void metaScan(Configuration configuration, HConnection connection,
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,147,int rowUpperLimit = rowLimit > 0 ? rowLimit: Integer.MAX_VALUE;
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,151,byte[] startRow;
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,152,if (row != null) {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,154,assert tableName != null;
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,155,byte[] searchRow =
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,156,HRegionInfo.createRegionName(tableName, row, HConstants.NINES,
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,157,false);
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,158,HTable metaTable = null;
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,159,try {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,160,metaTable = new HTable(configuration, HConstants.META_TABLE_NAME);
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,179,if (metaTable != null) {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,180,metaTable.close();
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,185,startRow = HConstants.EMPTY_START_ROW;
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,188,startRow = HRegionInfo.createRegionName(
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,189,tableName, HConstants.EMPTY_START_ROW, HConstants.ZEROES, false);
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,193,ScannerCallable callable;
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,194,int rows = Math.min(rowLimit, configuration.getInt(
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,195,HConstants.HBASE_META_SCANNER_CACHING,
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,196,HConstants.DEFAULT_HBASE_META_SCANNER_CACHING));
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,197,do {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,198,final Scan scan = new Scan(startRow).addFamily(HConstants.CATALOG_FAMILY);
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,199,if (LOG.isDebugEnabled()) {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,200,LOG.debug("Scanning " + Bytes.toString(metaTableName) +
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,202,rowUpperLimit + " rows using " + connection.toString());
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,204,callable = new ScannerCallable(connection, metaTableName, scan, null);
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,206,callable.withRetries();
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,208,int processedRows = 0;
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,209,try {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,210,callable.setCaching(rows);
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,211,done: do {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,212,if (processedRows >= rowUpperLimit) {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,213,break;
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,216,Result [] rrs = callable.withRetries();
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,217,if (rrs == null || rrs.length == 0 || rrs[0].size() == 0) {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,218,break; //exit completely
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,220,for (Result rr : rrs) {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,222,break done;
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,225,break done; //exit completely
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,226,processedRows++;
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,231,startRow = callable.getHRegionInfo().getEndKey();
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,234,callable.setClose();
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,235,callable.withRetries();
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,247,throws IOException {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,248,return listAllRegions(conf, true);
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,294,final byte [] tablename, final boolean offlined) throws IOException {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,323,metaScan(conf, visitor, tablename);
src/main/java/org/apache/hadoop/hbase/rest/RegionsResource.java,79,servlet.getConfiguration(), Bytes.toBytes(tableName), false);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2579,MetaScanner.metaScan(getConf(), visitor, null, null,
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1203,if (e.getValue().getHostnamePort().equals(server)) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1880,LOG.info("Attempting connect to Master server at " +
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1881,this.masterAddressManager.getMasterAddress());
src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReaderImpl.java,49,static final Log LOG = LogFactory.getLog(TableRecordReader.class);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2434,if (s == null) throw new UnknownScannerException("Name: " + scannerName);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2452,lease = this.leases.removeLease(scannerName);
src/main/java/org/apache/hadoop/hbase/thrift/ThriftServer.java,186,conf.setBoolean(
src/main/java/org/apache/hadoop/hbase/thrift/ThriftServer.java,187,ThriftServerRunner.COMPACT_CONF_KEY, cmd.hasOption(COMPACT_OPTION));
src/main/java/org/apache/hadoop/hbase/thrift/ThriftServer.java,188,conf.setBoolean(
src/main/java/org/apache/hadoop/hbase/thrift/ThriftServer.java,189,ThriftServerRunner.FRAMED_CONF_KEY, cmd.hasOption(FRAMED_OPTION));
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,1567,if (!this.isActiveMaster) {
src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java,89,if (i.hasNext()) {
src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java,90,sb.append('/');
src/main/java/org/apache/hadoop/hbase/filter/DependentColumnFilter.java,248,Bytes.toStringBinary(this.comparator.getValue()));
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,184,LOG.warn("Unknown check value: " + check + ", ignored");
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,77,private final Map<Integer, ResultScanner> scannerMap = new ConcurrentHashMap<Integer, ResultScanner>();
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,79,public static THBaseService.Iface newInstance(
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,80,Configuration conf, ThriftMetrics metrics) {
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,82,return (THBaseService.Iface) Proxy.newProxyInstance(
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,83,handler.getClass().getClassLoader(),
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,84,new Class[]{THBaseService.Iface.class},
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,85,new THBaseServiceMetricsProxy(handler, metrics));
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,92,private THBaseServiceMetricsProxy(
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,93,THBaseService.Iface handler, ThriftMetrics metrics) {
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,100,throws Throwable {
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,105,int processTime = (int)(now() - start);
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,110,throw new RuntimeException(
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,125,private HTableInterface getTable(byte[] tableName) {
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,126,return htablePool.getTable(tableName);
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,177,HTableInterface htable = getTable(table.array());
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,189,HTableInterface htable = getTable(table.array());
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,201,HTableInterface htable = getTable(table.array());
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,213,HTableInterface htable = getTable(table.array());
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,225,throws TIOError, TException {
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,226,HTableInterface htable = getTable(table.array());
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,228,return htable.checkAndPut(row.array(), family.array(), qualifier.array(), (value == null) ? null : value.array(), putFromThrift(put));
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,238,HTableInterface htable = getTable(table.array());
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,250,HTableInterface htable = getTable(table.array());
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,261,public List<TDelete> deleteMultiple(ByteBuffer table, List<TDelete> deletes) throws TIOError, TException {
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,262,HTableInterface htable = getTable(table.array());
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,275,public boolean checkAndDelete(ByteBuffer table, ByteBuffer row, ByteBuffer family, ByteBuffer qualifier, ByteBuffer value,
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,276,TDelete deleteSingle) throws TIOError, TException {
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,277,HTableInterface htable = getTable(table.array());
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,281,return htable.checkAndDelete(row.array(), family.array(), qualifier.array(), null, deleteFromThrift(deleteSingle));
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,283,return htable.checkAndDelete(row.array(), family.array(), qualifier.array(), value.array(), deleteFromThrift(deleteSingle));
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,294,HTableInterface htable = getTable(table.array());
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,306,HTableInterface htable = getTable(table.array());
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,319,public List<TResult> getScannerRows(int scannerId, int numRows) throws TIOError, TIllegalArgument, TException {
src/main/java/org/apache/hadoop/hbase/HConstants.java,422,public static int RETRY_BACKOFF[] = { 1, 1, 1, 2, 2, 4, 4, 8, 16, 32 };
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,366,Call call = calls.remove(id);
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,390,markClosed(new RemoteException(WritableUtils.readString(in),
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,391,WritableUtils.readString(in)));
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,456,private void setValue(final ImmutableBytesWritable key,
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,183,Configuration newConf = new Configuration(conf);
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,439,throws IOException {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,459,return null;
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,262,String hostname = Strings.domainNamePointerToHostName(DNS.getDefaultHost(
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,263,conf.get("hbase.master.dns.interface", "default"),
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,264,conf.get("hbase.master.dns.nameserver", "default")));
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1000,if (Bytes.equals(parentTable, HConstants.META_TABLE_NAME) &&
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1001,(getRegionCachePrefetch(tableName)) )  {
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1002,prefetchRegionCache(tableName, row);
src/main/java/org/apache/hadoop/hbase/client/ScannerCallable.java,104,private void checkIfRegionServerIsRemote() throws UnknownHostException {
src/main/java/org/apache/hadoop/hbase/client/ScannerCallable.java,105,String myAddress = DNS.getDefaultHost("default", "default");
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,156,return true;
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,161,return false;
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,165,return this.operator == Operator.MUST_PASS_ONE;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,172,this.maxRetriesMultiplier =
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,173,this.conf.getInt("replication.source.maxretriesmultiplier", 10);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,174,this.socketTimeoutMultiplier = maxRetriesMultiplier * maxRetriesMultiplier;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,469,try {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,470,chooseSinks();
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,471,Thread.sleep(this.sleepForRetries);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,473,LOG.error("Interrupted while trying to connect to sinks", e);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2300,region.flushcache();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2313,if (region.getLastFlushTime() < ifOlderThanTS) region.flushcache();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,3024,region.flushcache();
src/main/java/org/apache/hadoop/hbase/regionserver/Compactor.java,161,if (writer == null && !kvs.isEmpty()) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,553,long minSeqId = -1;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,583,if (minSeqId == -1 || storeSeqId < minSeqId) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,584,minSeqId = storeSeqId;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,605,this.regiondir, minSeqId, reporter, status));
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2895,final long minSeqId, final CancelableProgressable reporter,
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2898,long seqid = minSeqId;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2911,if (maxSeqId <= minSeqId) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2913,+ " and minimum sequenceid for the region is " + minSeqId
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2920,seqid = replayRecoveredEdits(edits, seqid, reporter);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2937,if (seqid > minSeqId) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2964,String msg = "Replaying edits from " + edits + "; minSequenceid=" +
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2965,minSeqId + "; path=" + edits;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2973,long currentEditSeqId = minSeqId;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3033,if (key.getLogSeqNum() <= currentEditSeqId) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3034,skippedEdits++;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3035,continue;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3037,currentEditSeqId = key.getLogSeqNum();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1696,this.readRequestsCount.increment();
src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java,286,public static void deleteDaughtersReferencesInParent(CatalogTracker catalogTracker,
src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java,288,throws NotAllMetaRegionsOnlineException, IOException {
src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java,289,Delete delete = new Delete(parent.getRegionName());
src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java,290,delete.deleteColumns(HConstants.CATALOG_FAMILY, HConstants.SPLITA_QUALIFIER);
src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java,291,delete.deleteColumns(HConstants.CATALOG_FAMILY, HConstants.SPLITB_QUALIFIER);
src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java,293,LOG.info("Deleted daughters references, qualifier=" + Bytes.toStringBinary(HConstants.SPLITA_QUALIFIER) +
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,406,HRegionInfo splitA = Writables.getHRegionInfo(rowResult.getValue(HConstants.CATALOG_FAMILY,
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,408,HRegionInfo splitB = Writables.getHRegionInfo(rowResult.getValue(HConstants.CATALOG_FAMILY,
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,413,Result resultA = getRegionResultBlocking(metaTable, blockingTimeout,
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,414,splitA.getRegionName());
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,415,if (resultA != null) {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,416,processRow(resultA);
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,417,daughterRegions.add(splitA.getRegionName());
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,419,throw new RegionOfflineException("Split daughter region " +
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,420,splitA.getRegionNameAsString() + " cannot be found in META.");
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,424,Result resultB = getRegionResultBlocking(metaTable, rem,
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,425,splitB.getRegionName());
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,426,if (resultB != null) {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,427,processRow(resultB);
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,428,daughterRegions.add(splitB.getRegionName());
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,430,throw new RegionOfflineException("Split daughter region " +
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,431,splitB.getRegionNameAsString() + " cannot be found in META.");
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,440,if (LOG.isDebugEnabled()) {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,441,LOG.debug("blocking until region is in META: " + Bytes.toStringBinary(regionName));
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,44,import org.apache.hadoop.hbase.catalog.MetaReader;
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,110,MetaReader.Visitor visitor = new MetaReader.Visitor() {
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,112,public boolean visit(Result r) throws IOException {
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,123,MetaReader.fullScan(this.server.getCatalogTracker(), visitor);
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,181,result = Bytes.compareTo(left.getEndKey(), right.getEndKey());
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,182,if (result != 0) {
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,183,if (left.getStartKey().length != 0
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,184,&& left.getEndKey().length == 0) {
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,185,return -1;  // left is last region
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,187,if (right.getStartKey().length != 0
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,188,&& right.getEndKey().length == 0) {
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,189,return 1;  // right is the last region
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,191,return -result; // Flip the result so parent comes first.
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,193,return result;
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,239,removeDaughtersFromParent(parent);
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,287,throws IOException {
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,288,MetaEditor.deleteDaughtersReferencesInParent(this.server.getCatalogTracker(), parent);
src/main/java/org/apache/hadoop/hbase/util/Bytes.java,36,import org.apache.commons.lang.StringUtils;
src/main/java/org/apache/hadoop/hbase/coprocessor/BaseRegionObserver.java,78,return null;
src/main/java/org/apache/hadoop/hbase/coprocessor/BaseRegionObserver.java,127,return null;
src/main/java/org/apache/hadoop/hbase/coprocessor/BaseRegionObserver.java,270,return null;
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2243,while(!regions.containsKey(regionInfo)) {
src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java,196,DEFAULT_LOAD_FACTOR,
src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java,198,conf.getFloat(LRU_MIN_FACTOR_CONFIG_NAME, DEFAULT_MIN_FACTOR),
src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java,199,conf.getFloat(LRU_ACCEPTABLE_FACTOR_CONFIG_NAME, DEFAULT_ACCEPTABLE_FACTOR),
src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java,200,DEFAULT_SINGLE_FACTOR,
src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java,279,throw new RuntimeException("Cached an already cached block");
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,399,getDefaultBlockSize());
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,420,this.fs.getDefaultReplication());
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,449,private long getDefaultBlockSize() throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,450,Method m = null;
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,451,Class<? extends FileSystem> cls = this.fs.getClass();
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,452,try {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,453,m = cls.getMethod("getDefaultBlockSize",
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,454,new Class<?>[] { Path.class });
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,456,LOG.info("FileSystem doesn't support getDefaultBlockSize");
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,458,LOG.info("Doesn't have access to getDefaultBlockSize on "
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,459,+ "FileSystems", e);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,462,if (null == m) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,463,return this.fs.getDefaultBlockSize();
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,465,try {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,466,Object ret = m.invoke(this.fs, this.dir);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,467,return ((Long)ret).longValue();
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,469,throw new IOException(e);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java,164,fs.getDefaultReplication())),
src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java,166,fs.getDefaultBlockSize())),
src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java,185,fs.getDefaultReplication()),
src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java,187,fs.getDefaultBlockSize()),
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,19,package org.apache.hadoop.hbase.client.coprocessor;
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,21,import java.io.DataInput;
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,22,import java.io.DataOutput;
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,23,import java.io.IOException;
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,24,import java.math.BigDecimal;
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,25,import java.math.RoundingMode;
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,26,import org.apache.commons.logging.Log;
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,27,import org.apache.commons.logging.LogFactory;
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,28,import org.apache.hadoop.hbase.KeyValue;
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,29,import org.apache.hadoop.hbase.coprocessor.ColumnInterpreter;
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,30,import org.apache.hadoop.hbase.util.Bytes;
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,37,public class BigDecimalColumnInterpreter implements ColumnInterpreter<BigDecimal, BigDecimal> {
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,38,private static final Log log = LogFactory.getLog(BigDecimalColumnInterpreter.class);
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,41,public void readFields(DataInput arg0) throws IOException {
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,45,public void write(DataOutput arg0) throws IOException {
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,50,throws IOException {
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,51,if ((kv == null || kv.getValue() == null)) return null;
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,52,return Bytes.toBigDecimal(kv.getValue()).setScale(2, RoundingMode.HALF_EVEN);
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,56,public BigDecimal add(BigDecimal val1, BigDecimal val2) {
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,57,if ((((val1 == null) ? 1 : 0) ^ ((val2 == null) ? 1 : 0)) != 0) {
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,58,return ((val1 == null) ? val2 : val1);
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,60,if (val1 == null) return null;
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,61,return val1.add(val2).setScale(2, RoundingMode.HALF_EVEN);
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,65,public BigDecimal getMaxValue() {
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,66,return BigDecimal.valueOf(Double.MAX_VALUE);
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,70,public BigDecimal getMinValue() {
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,71,return BigDecimal.valueOf(Double.MIN_VALUE);
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,75,public BigDecimal multiply(BigDecimal val1, BigDecimal val2) {
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,76,return (((val1 == null) || (val2 == null)) ? null : val1.multiply(val2).setScale(2,
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,77,RoundingMode.HALF_EVEN));
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,81,public BigDecimal increment(BigDecimal val) {
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,82,return ((val == null) ? null : val.add(BigDecimal.ONE));
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,86,public BigDecimal castToReturnType(BigDecimal val) {
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,87,return val;
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,91,public int compare(BigDecimal val1, BigDecimal val2) {
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,92,if ((((val1 == null) ? 1 : 0) ^ ((val2 == null) ? 1 : 0)) != 0) {
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,93,return ((val1 == null) ? -1 : 1);
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,95,if (val1 == null) return 0;
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,96,return val1.compareTo(val2);
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,100,public double divideForAvg(BigDecimal val1, Long paramLong) {
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,101,return (((paramLong == null) || (val1 == null)) ? (Double.NaN) :
src/main/java/org/apache/hadoop/hbase/client/coprocessor/BigDecimalColumnInterpreter.java,102,val1.doubleValue() / paramLong.doubleValue());
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,18,package org.apache.hadoop.hbase.filter;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,20,import java.io.DataInput;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,21,import java.io.DataOutput;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,22,import java.io.IOException;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,23,import java.util.ArrayList;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,24,import java.util.Arrays;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,25,import java.util.List;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,27,import org.apache.hadoop.hbase.KeyValue;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,28,import org.apache.hadoop.hbase.util.Bytes;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,29,import org.apache.hadoop.hbase.util.Pair;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,59,public class FuzzyRowFilter extends FilterBase {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,60,private List<Pair<byte[], byte[]>> fuzzyKeysData;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,61,private boolean done = false;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,66,public FuzzyRowFilter() {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,69,public FuzzyRowFilter(List<Pair<byte[], byte[]>> fuzzyKeysData) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,70,this.fuzzyKeysData = fuzzyKeysData;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,75,public ReturnCode filterKeyValue(KeyValue kv) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,76,byte[] rowKey = kv.getRow();
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,78,SatisfiesCode bestOption = SatisfiesCode.NO_NEXT;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,79,for (Pair<byte[], byte[]> fuzzyData : fuzzyKeysData) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,80,SatisfiesCode satisfiesCode =
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,81,satisfies(rowKey, fuzzyData.getFirst(), fuzzyData.getSecond());
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,82,if (satisfiesCode == SatisfiesCode.YES) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,83,return ReturnCode.INCLUDE;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,86,if (satisfiesCode == SatisfiesCode.NEXT_EXISTS) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,87,bestOption = SatisfiesCode.NEXT_EXISTS;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,91,if (bestOption == SatisfiesCode.NEXT_EXISTS) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,92,return ReturnCode.SEEK_NEXT_USING_HINT;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,96,done = true;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,97,return ReturnCode.NEXT_ROW;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,101,public KeyValue getNextKeyHint(KeyValue currentKV) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,102,byte[] rowKey = currentKV.getRow();
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,103,byte[] nextRowKey = null;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,105,for (Pair<byte[], byte[]> fuzzyData : fuzzyKeysData) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,106,byte[] nextRowKeyCandidate = getNextForFuzzyRule(rowKey,
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,107,fuzzyData.getFirst(), fuzzyData.getSecond());
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,108,if (nextRowKeyCandidate == null) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,109,continue;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,111,if (nextRowKey == null || Bytes.compareTo(nextRowKeyCandidate, nextRowKey) < 0) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,112,nextRowKey = nextRowKeyCandidate;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,116,if (nextRowKey == null) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,119,throw new IllegalStateException("No next row key that satisfies fuzzy exists when" +
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,125,return KeyValue.createFirstOnRow(nextRowKey);
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,129,public boolean filterAllRemaining() {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,130,return done;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,134,public void write(DataOutput dataOutput) throws IOException {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,135,dataOutput.writeInt(this.fuzzyKeysData.size());
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,136,for (Pair<byte[], byte[]> fuzzyData : fuzzyKeysData) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,137,Bytes.writeByteArray(dataOutput, fuzzyData.getFirst());
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,138,Bytes.writeByteArray(dataOutput, fuzzyData.getSecond());
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,143,public void readFields(DataInput dataInput) throws IOException {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,144,int count = dataInput.readInt();
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,145,this.fuzzyKeysData = new ArrayList<Pair<byte[], byte[]>>(count);
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,146,for (int i = 0; i < count; i++) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,147,byte[] keyBytes = Bytes.readByteArray(dataInput);
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,148,byte[] keyMeta = Bytes.readByteArray(dataInput);
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,149,this.fuzzyKeysData.add(new Pair<byte[], byte[]>(keyBytes, keyMeta));
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,154,public String toString() {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,155,final StringBuilder sb = new StringBuilder();
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,156,sb.append("FuzzyRowFilter");
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,157,sb.append("{fuzzyKeysData=");
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,158,for (Pair<byte[], byte[]> fuzzyData : fuzzyKeysData) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,159,sb.append('{').append(Bytes.toStringBinary(fuzzyData.getFirst())).append(":");
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,160,sb.append(Bytes.toStringBinary(fuzzyData.getSecond())).append('}');
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,162,sb.append("}, ");
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,163,return sb.toString();
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,168,static enum SatisfiesCode {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,170,YES,
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,172,NEXT_EXISTS,
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,174,NO_NEXT
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,177,static SatisfiesCode satisfies(byte[] row,
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,178,byte[] fuzzyKeyBytes, byte[] fuzzyKeyMeta) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,179,return satisfies(row, 0, row.length, fuzzyKeyBytes, fuzzyKeyMeta);
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,182,private static SatisfiesCode satisfies(byte[] row, int offset, int length,
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,183,byte[] fuzzyKeyBytes, byte[] fuzzyKeyMeta) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,184,if (row == null) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,186,return SatisfiesCode.YES;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,189,boolean nextRowKeyCandidateExists = false;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,191,for (int i = 0; i < fuzzyKeyMeta.length && i < length; i++) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,193,boolean byteAtPositionFixed = fuzzyKeyMeta[i] == 0;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,194,boolean fixedByteIncorrect = byteAtPositionFixed && fuzzyKeyBytes[i] != row[i + offset];
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,195,if (fixedByteIncorrect) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,197,if (nextRowKeyCandidateExists) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,198,return SatisfiesCode.NEXT_EXISTS;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,204,boolean rowByteLessThanFixed = (row[i + offset] & 0xFF) < (fuzzyKeyBytes[i] & 0xFF);
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,205,return  rowByteLessThanFixed ? SatisfiesCode.NEXT_EXISTS : SatisfiesCode.NO_NEXT;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,214,if (fuzzyKeyMeta[i] == 1 && !isMax(fuzzyKeyBytes[i])) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,215,nextRowKeyCandidateExists = true;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,219,return SatisfiesCode.YES;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,222,private static boolean isMax(byte fuzzyKeyByte) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,223,return (fuzzyKeyByte & 0xFF) == 255;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,226,static byte[] getNextForFuzzyRule(byte[] row, byte[] fuzzyKeyBytes, byte[] fuzzyKeyMeta) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,227,return getNextForFuzzyRule(row, 0, row.length, fuzzyKeyBytes, fuzzyKeyMeta);
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,234,private static byte[] getNextForFuzzyRule(byte[] row, int offset, int length,
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,235,byte[] fuzzyKeyBytes, byte[] fuzzyKeyMeta) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,244,byte[] result = Arrays.copyOf(fuzzyKeyBytes,
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,245,length > fuzzyKeyBytes.length ? length : fuzzyKeyBytes.length);
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,246,int toInc = -1;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,248,boolean increased = false;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,249,for (int i = 0; i < result.length; i++) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,250,if (i >= fuzzyKeyMeta.length || fuzzyKeyMeta[i] == 1) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,251,result[i] = row[offset + i];
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,252,if (!isMax(row[i])) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,254,toInc = i;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,257,if ((row[i + offset] & 0xFF) < (fuzzyKeyBytes[i] & 0xFF)) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,260,increased = true;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,261,break;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,263,if ((row[i + offset] & 0xFF) > (fuzzyKeyBytes[i] & 0xFF)) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,267,break;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,272,if (!increased) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,273,if (toInc < 0) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,274,return null;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,276,result[toInc]++;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,280,for (int i = toInc + 1; i < result.length; i++) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,281,if (i >= fuzzyKeyMeta.length || fuzzyKeyMeta[i] == 1) {
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,282,result[i] = 0;
src/main/java/org/apache/hadoop/hbase/filter/FuzzyRowFilter.java,287,return result;
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,1133,createWithParents(zkw, znode);
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1784,if (this.zooKeeper != null) {
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1785,LOG.info("Closed zookeeper sessionid=0x" +
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1786,Long.toHexString(this.zooKeeper.getRecoverableZooKeeper().getSessionId()));
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1787,this.zooKeeper.close();
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1788,this.zooKeeper = null;
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1790,this.closed = true;
src/main/java/org/apache/hadoop/hbase/metrics/MetricsMBeanBase.java,23,import java.util.HashMap;
src/main/java/org/apache/hadoop/hbase/metrics/MetricsMBeanBase.java,58,protected Map<String,MetricsBase> extendedAttributes =
src/main/java/org/apache/hadoop/hbase/metrics/MetricsMBeanBase.java,59,new HashMap<String,MetricsBase>();
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,18,package org.apache.hadoop.hbase.coprocessor;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,20,import java.net.URL;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,21,import java.net.URLClassLoader;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,22,import java.util.List;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,23,import java.util.regex.Pattern;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,25,import org.apache.commons.logging.Log;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,26,import org.apache.commons.logging.LogFactory;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,48,public class CoprocessorClassLoader extends URLClassLoader {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,49,private static final Log LOG =
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,50,LogFactory.getLog(CoprocessorClassLoader.class);
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,57,private static final String[] CLASS_PREFIX_EXEMPTIONS = new String[] {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,83,private static final Pattern[] RESOURCE_LOAD_PARENT_FIRST_PATTERNS =
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,84,new Pattern[] {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,91,private final ClassLoader parent;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,98,public CoprocessorClassLoader(List<URL> paths, ClassLoader parent) {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,99,super(paths.toArray(new URL[]{}), parent);
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,100,this.parent = parent;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,101,if (parent == null) {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,102,throw new IllegalArgumentException("No parent classloader!");
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,108,throws ClassNotFoundException {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,110,if (isClassExempt(name)) {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,111,if (LOG.isDebugEnabled()) {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,112,LOG.debug("Skipping exempt class " + name +
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,115,return parent.loadClass(name);
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,119,Class<?> clasz = findLoadedClass(name);
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,120,if (clasz != null) {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,121,if (LOG.isDebugEnabled()) {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,122,LOG.debug("Class " + name + " already loaded");
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,125,else {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,126,try {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,129,if (LOG.isDebugEnabled()) {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,130,LOG.debug("Finding class: " + name);
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,132,clasz = findClass(name);
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,135,if (LOG.isDebugEnabled()) {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,136,LOG.debug("Class " + name + " not found - delegating to parent");
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,138,try {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,139,clasz = parent.loadClass(name);
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,143,if (LOG.isDebugEnabled()) {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,144,LOG.debug("Class " + name + " not found in parent loader");
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,146,throw e2;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,151,return clasz;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,155,synchronized public URL getResource(String name) {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,156,URL resource = null;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,157,boolean parentLoaded = false;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,160,if (loadResourceUsingParentFirst(name)) {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,161,if (LOG.isDebugEnabled()) {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,162,LOG.debug("Checking parent first for resource " + name);
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,164,resource = super.getResource(name);
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,165,parentLoaded = true;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,168,if (resource == null) {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,170,resource = findResource(name);
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,171,if ((resource == null) && !parentLoaded) {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,174,resource = super.getResource(name);
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,178,return resource;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,188,protected boolean isClassExempt(String name) {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,189,for (String exemptPrefix : CLASS_PREFIX_EXEMPTIONS) {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,190,if (name.startsWith(exemptPrefix)) {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,191,return true;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,194,return false;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,205,protected boolean loadResourceUsingParentFirst(String name) {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,206,for (Pattern resourcePattern : RESOURCE_LOAD_PARENT_FIRST_PATTERNS) {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,207,if (resourcePattern.matcher(name).matches()) {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,208,return true;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorClassLoader.java,211,return false;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,23,import com.google.common.collect.MapMaker;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,28,import org.apache.hadoop.fs.FileSystem;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,33,import org.apache.hadoop.hbase.HBaseConfiguration;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,35,import org.apache.hadoop.hbase.client.*;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,41,import org.apache.hadoop.hbase.Server;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,42,import org.apache.hadoop.io.IOUtils;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,44,import java.io.File;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,45,import java.io.FileOutputStream;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,46,import java.io.IOException;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,47,import java.net.URL;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,48,import java.util.*;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,49,import java.util.concurrent.ConcurrentMap;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,50,import java.util.jar.JarEntry;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,51,import java.util.jar.JarFile;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,72,private static final String COPROCESSOR_JARS_DIR = File.separator
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,73,+ "coprocessor" + File.separator + "jars" + File.separator;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,90,static ConcurrentMap<Path, ClassLoader> classLoadersCache =
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,91,new MapMaker().concurrencyLevel(3).weakValues().makeMap();
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,190,cl = classLoadersCache.get(path);
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,191,if (cl != null){
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,192,LOG.debug("Found classloader "+ cl + "for "+path.toString());
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,193,try {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,194,implClass = cl.loadClass(className);
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,196,LOG.info("Class " + className + " needs to be loaded from a file - " +
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,197,path + ".");
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,204,if (implClass == null) {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,205,if (path == null) {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,206,throw new IOException("No jar path specified for " + className);
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,209,if (!path.toString().endsWith(".jar")) {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,210,throw new IOException(path.toString() + ": not a jar file?");
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,212,FileSystem fs = path.getFileSystem(this.conf);
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,213,File parentDir = new File(this.conf.get("hbase.local.dir") + COPROCESSOR_JARS_DIR);
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,214,parentDir.mkdirs();
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,215,File dst = new File(parentDir, "." + pathPrefix +
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,217,fs.copyToLocalFile(path, new Path(dst.toString()));
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,218,dst.deleteOnExit();
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,231,List<URL> paths = new ArrayList<URL>();
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,232,URL url = dst.getCanonicalFile().toURL();
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,233,paths.add(url);
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,235,JarFile jarFile = new JarFile(dst.toString());
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,236,Enumeration<JarEntry> entries = jarFile.entries();
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,237,while (entries.hasMoreElements()) {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,238,JarEntry entry = entries.nextElement();
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,239,if (entry.getName().matches("/lib/[^/]+\\.jar")) {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,240,File file = new File(parentDir, "." + pathPrefix +
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,242,IOUtils.copyBytes(jarFile.getInputStream(entry), new FileOutputStream(file), conf, true);
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,243,file.deleteOnExit();
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,244,paths.add(file.toURL());
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,247,jarFile.close();
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,249,cl = new CoprocessorClassLoader(paths, this.getClass().getClassLoader());
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,251,ClassLoader prev = classLoadersCache.putIfAbsent(path, cl);
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,252,if (prev != null) {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,254,cl = prev;
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,113,Thread.sleep(1000);
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,120,LOG.info("Finished lease recover attempt for " + p);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,200,splits = splitLog(logfiles);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,260,private List<Path> splitLog(final FileStatus[] logfiles) throws IOException {
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,74,if (fs instanceof DistributedFileSystem) {
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,75,DistributedFileSystem dfs = (DistributedFileSystem)fs;
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,79,throw new Exception("Not a DistributedFileSystem");
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,83,throw (IOException) ite.getCause();
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,644,return null;
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1164,Map<byte[], HRegionLocation> tableLocations =
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1165,getTableLocations(tableName);
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,535,Class... classes) throws IOException {
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,542,jars.addAll( conf.getStringCollection("tmpjars") );
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,545,for (Class clazz : classes) {
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,548,String pathStr = findOrCreateJar(clazz);
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,549,if (pathStr == null) {
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,554,Path path = new Path(pathStr);
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,560,jars.add(path.makeQualified(localFs).toString());
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationHLogReaderManager.java,69,this.reader.reset();
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,561,if (sleepMultiplier == this.maxRetriesMultiplier) {
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,690,splitLogAndExpireIfOnline(currentMetaServer);
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,720,private void waitForRootAssignment() throws InterruptedException {
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,284,splitLogManager.handleDeadWorkers(serverNames);
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,285,splitTime = EnvironmentEdgeManager.currentTimeMillis();
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,287,splitTime = EnvironmentEdgeManager.currentTimeMillis() - splitTime;
src/main/java/org/apache/hadoop/hbase/master/ServerManager.java,376,if (!services.isServerShutdownHandlerEnabled()) {
src/main/java/org/apache/hadoop/hbase/master/ServerManager.java,385,return;
src/main/java/org/apache/hadoop/hbase/master/ServerManager.java,413,boolean carryingRoot = services.getAssignmentManager().isCarryingRoot(serverName);
security/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlLists.java,137,LOG.warn("No actions associated with user '"+Bytes.toString(userPerm.getUser())+"'");
security/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlLists.java,138,return;
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,69,throw new TableNotFoundException(Bytes.toString(tableName));
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,122,static final byte[] BLOOM_FILTER_TYPE_KEY =
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,27,import java.lang.reflect.Method;
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,42,import org.apache.hadoop.hbase.HConstants;
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,585,try {
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,586,Class<?> jarFinder = Class.forName("org.apache.hadoop.util.JarFinder");
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,593,Method m = jarFinder.getMethod("getJar", Class.class);
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,594,return (String)m.invoke(null,my_class);
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,597,throw new IOException(ite.getCause());
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,602,LOG.debug("New JarFinder: org.apache.hadoop.util.JarFinder.getJar " +
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,604,return findContainingJar(my_class);
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,618,private static String findContainingJar(Class my_class) {
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,621,try {
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,622,for(Enumeration itr = loader.getResources(class_file);
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,623,itr.hasMoreElements();) {
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,624,URL url = (URL) itr.nextElement();
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,625,if ("jar".equals(url.getProtocol())) {
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,626,String toReturn = url.getPath();
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,627,if (toReturn.startsWith("file:")) {
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,628,toReturn = toReturn.substring("file:".length());
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,636,toReturn = toReturn.replaceAll("\\+", "%2B");
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,637,toReturn = URLDecoder.decode(toReturn, "UTF-8");
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,638,return toReturn.replaceAll("!.*$", "");
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,642,throw new RuntimeException(e);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,5625,if (this.regionInfo.isMetaRegion() ||
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,1296,if (txid <= this.syncedTillHere) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,1297,return;
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,108,master.getConfiguration(), master, master.getServerName().toString());
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,112,private ConcurrentMap<String, Task> tasks =
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,117,private Object deadWorkersLock = new Object();
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,132,Stoppable stopper, String serverName) {
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,133,this(zkw, conf, stopper, serverName, new TaskFinisher() {
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,159,Stoppable stopper, String serverName, TaskFinisher tf) {
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,173,LOG.debug("timeout = " + timeout);
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,174,LOG.debug("unassigned timeout = " + unassignedTimeout);
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,563,if ((EnvironmentEdgeManager.currentTimeMillis() - task.last_update) <
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,564,timeout) {
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKSplitLog.java,48,public static final int DEFAULT_TIMEOUT = 25000; // 25 sec
src/main/java/org/apache/hadoop/hbase/client/HConnection.java,195,throws IOException;
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,831,throws IOException {
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,833,return null;
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,840,return null;
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1462,LOG.info(destination.toString() + " unassigned znodes=" + count +
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1529,LOG.warn("rc != 0 for " + path + " -- retryable connectionloss -- " +
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1531,this.zkw.abort("Connectionloss writing unassigned at " + path +
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1539,new ExistsUnassignedAsyncCallback(this.counter, destination), ctx);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1549,private final AtomicInteger counter;
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1552,ExistsUnassignedAsyncCallback(final AtomicInteger counter, ServerName destination) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1553,this.counter = counter;
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1561,LOG.warn("rc != 0 for " + path + " -- retryable connectionloss -- " +
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1573,this.counter.addAndGet(1);
src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java,222,if (!fs.mkdirs(storeArchiveDir)) {
src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java,295,if (!fs.mkdirs(baseArchiveDir)) {
src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java,362,if (!fs.rename(archiveFile, backedupArchiveFile)) {
src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java,366,if (!fs.delete(archiveFile, false)) {
src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java,387,if (!fs.exists(archiveDir)) {
src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java,388,if (fs.mkdirs(archiveDir)) {
src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java,389,LOG.debug("Created archive directory:" + archiveDir);
src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java,425,if (fs.delete(regionDir, true)) {
src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java,559,return fs.rename(p, dest);
src/main/java/org/apache/hadoop/hbase/backup/HFileArchiver.java,590,if (!fs.delete(file, true)) throw new IOException("Failed to delete:" + this.file);
src/main/java/org/apache/hadoop/hbase/io/Reference.java,128,FSDataOutputStream out = fs.create(p, false);
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,136,this.fs.mkdirs(oldLogDir);
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,266,if (!this.fs.rename(logDir, splitDir)) {
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,338,fs.mkdirs(rd);
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,450,fs.delete(new Path(rootdir, Bytes.toString(tableName)), true);
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,468,if (fs.delete(familyDir, true) == false) {
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,277,if (fs.exists(logDir) && !fs.delete(logDir, false)) {
src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java,153,return fs.delete(toCheck, false);
src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java,185,return fs.delete(toCheck, false);
src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java,207,boolean success = this.fs.delete(filePath, true);
src/main/java/org/apache/hadoop/hbase/master/cleaner/CleanerChore.java,233,boolean success = this.fs.delete(filePath, false);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,651,if (!fs.rename(initialFiles, regiondir)) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,783,if (!fs.rename(tmpPath, regioninfoPath)) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2943,if (!this.fs.delete(file, false)) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3135,fs.delete(p, false);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4047,fs.mkdirs(regionDir);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4241,if (!fs.delete(regiondir, true)) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4287,if (!fs.mkdirs(dir)) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4397,fs.mkdirs(newRegionDir);
src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java,559,if (!fs.delete(splitdir, true)) {
src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java,564,if (!fs.mkdirs(splitdir)) throw new IOException("Failed create of " + splitdir);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,278,if (!fs.exists(homedir)) {
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,849,if (!fs.rename(path, dstPath)) {
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1611,if (!fs.rename(origPath, destPath)) {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,642,this.fs.delete(getPath(), true);
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,685,if (!fs.rename(src, tgt)) {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,808,fs.mkdirs(dir);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,412,if (!fs.exists(oldLogDir)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,413,if (!fs.mkdirs(this.oldLogDir)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,414,throw new IOException("Unable to mkdir " + this.oldLogDir);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,694,return createWriter(fs, path, conf);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,913,if (!this.fs.rename(p, newPath)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,965,if (!fs.rename(file.getPath(),p)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,977,if (!fs.delete(dir, true)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,1856,if (!fs.rename(edits, moveAsideName)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,479,if (!fs.delete(dst, false)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,488,if (!fs.rename(wap.p, dst)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,555,fs.delete(stagingDir, true);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,578,if (!fs.mkdirs(corruptDir)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,581,fs.mkdirs(oldLogDir);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,588,if (!fs.rename(corrupted, p)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,599,if (!fs.rename(p, newPath)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,609,if (srcDir != null && !fs.delete(srcDir, true)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,641,if (isCreate && !fs.exists(dir)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,642,if (!fs.mkdirs(dir)) LOG.warn("mkdir failed on " + dir);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,813,return HLog.createWriter(fs, logfile, conf);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,1060,if (!fs.delete(regionedits, false)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,1086,if (!fs.delete(ret, false)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,1091,if (!fs.exists(dir)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,1092,if (!fs.mkdirs(dir)) LOG.warn("mkdir failed on " + dir);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,1185,if (!fs.delete(dst, false)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,1194,if (!fs.rename(wap.p, dst)) {
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,227,if (!this.fs.delete(tabledir, true)) {
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,283,if (!fs.delete(p, false)) {
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,507,if (!fs.rename(p, tableInfoPath)) {
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,533,FSDataOutputStream out = fs.create(p, false);
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,108,fs.mkdirs(dir);
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,153,public static FSDataOutputStream create(FileSystem fs, Path path,
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,154,FsPermission perm, boolean overwrite) throws IOException {
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,157,return fs.create(path, perm, overwrite,
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,158,fs.getConf().getInt("io.file.buffer.size", 4096),
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,159,fs.getDefaultReplication(), fs.getDefaultBlockSize(), null);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKSplitLog.java,167,fs.createNewFile(file);
src/main/java/org/apache/hadoop/hbase/executor/EventHandler.java,139,M_META_SERVER_SHUTDOWN    (72);  // Master is processing shutdown of RS hosting a meta region (-ROOT- or .META.).
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,346,if ((matcher.row == null) || !Bytes.equals(row, offset, length, matcher.row, matcher.rowOffset, matcher.rowLength)) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java,129,this.replicationSink.stopReplicationSinkServices();
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,342,String msg = "File system needs to be upgraded."
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,345,+ ".  Run the '${HBASE_HOME}/bin/hbase migrate' script.";
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,1220,for (WriterAndPath wap : logWriters.values()) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,1221,try {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,1222,wap.w.close();
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,1224,LOG.error("Couldn't close log at " + wap.p, ioe);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,1225,thrown.add(ioe);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,1226,continue;
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,1228,LOG.info("Closed path " + wap.p + " (wrote " + wap.editsWritten
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,1229,+ " edits in " + (wap.nanosSpent / 1000 / 1000) + "ms)");
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,1231,logWritersClosed = true;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3412,failures.add(p);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3440,if (ioes.size() != 0) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3441,LOG.error("There were IO errors when checking if bulk load is ok.  " +
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3443,throw MultipleIOException.createIOException(ioes);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java,219,if (!plugins.contains(ReplicationLogCleaner.class.toString())) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java,220,conf.set(HBASE_MASTER_LOGCLEANER_PLUGINS,
src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java,221,plugins + "," + ReplicationLogCleaner.class.getCanonicalName());
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,111,globalCache = initGlobal(conf);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1678,RegionPlan plan = getRegionPlan(state, !regionAlreadyInTransitionException && forceNewPlan);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1725,if (t instanceof RegionAlreadyInTransitionException) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1726,regionAlreadyInTransitionException = true;
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1727,if (LOG.isDebugEnabled()) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1728,LOG.debug("Failed assignment in: " + plan.getDestination() + " due to "
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1729,+ t.getMessage());
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1733,if (t instanceof java.net.SocketTimeoutException
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1744,+ " to "
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1746,+ ", trying to assign "
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1747,+ (regionAlreadyInTransitionException ? "to the same region server"
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1749,+ "retry=" + i, t);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1758,if (!regionAlreadyInTransitionException) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1760,newPlan = getRegionPlan(state, plan.getDestination(), true);
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,210,protected Map<ImmutableBytesWritable,ImmutableBytesWritable> values =
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,493,setValue(Bytes.toBytes(key), Bytes.toBytes(value));
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,860,for (Map.Entry<ImmutableBytesWritable, ImmutableBytesWritable> e:
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,861,values.entrySet()) {
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,862,String key = Bytes.toString(e.getKey().get());
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,863,String value = Bytes.toString(e.getValue().get());
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,864,s.append(", ");
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,865,s.append(key);
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,866,s.append(" => '");
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,867,s.append(value);
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,868,s.append("'");
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,884,for (Map.Entry<ImmutableBytesWritable, ImmutableBytesWritable> e:
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,885,values.entrySet()) {
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,886,String key = Bytes.toString(e.getKey().get());
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,887,String value = Bytes.toString(e.getValue().get());
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,888,if(DEFAULT_VALUES.get(key) == null || !DEFAULT_VALUES.get(key).equalsIgnoreCase(value)) {
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,891,s.append(" => '");
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,892,s.append(value);
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,893,s.append("'");
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,896,s.append('}');
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,897,return s.toString();
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,68,protected Map<ImmutableBytesWritable, ImmutableBytesWritable> values =
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,429,return Collections.unmodifiableMap(values);
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,469,setValue(Bytes.toBytes(key), Bytes.toBytes(value));
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,681,s.append('{');
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,682,s.append(HConstants.NAME);
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,683,s.append(" => '");
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,684,s.append(Bytes.toString(name));
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,685,s.append("'");
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,686,for (Map.Entry<ImmutableBytesWritable, ImmutableBytesWritable> e:
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,687,values.entrySet()) {
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,688,String key = Bytes.toString(e.getKey().get());
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,689,String value = Bytes.toString(e.getValue().get());
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,690,if (key == null) {
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,691,continue;
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,693,String upperCase = key.toUpperCase();
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,694,if (upperCase.equals(IS_ROOT) || upperCase.equals(IS_META)) {
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,696,if (value.toLowerCase().equals(Boolean.FALSE.toString())) {
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,697,continue;
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,700,s.append(", ");
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,701,s.append(Bytes.toString(e.getKey().get()));
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,702,s.append(" => '");
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,703,s.append(Bytes.toString(e.getValue().get()));
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,704,s.append("'");
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,706,s.append(", ");
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,707,s.append(FAMILIES);
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,708,s.append(" => ");
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,709,s.append(families.values());
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,710,s.append('}');
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,720,s.append('{');
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,721,s.append(HConstants.NAME);
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,722,s.append(" => '");
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,723,s.append(Bytes.toString(name));
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,724,s.append("'");
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,725,for (Map.Entry<ImmutableBytesWritable, ImmutableBytesWritable> e:
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,726,values.entrySet()) {
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,727,String key = Bytes.toString(e.getKey().get());
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,728,String value = Bytes.toString(e.getValue().get());
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,729,if (key == null) {
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,732,String upperCase = key.toUpperCase();
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,733,if (upperCase.equals(IS_ROOT) || upperCase.equals(IS_META)) {
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,740,s.append(Bytes.toString(e.getKey().get()));
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,741,s.append(" => '");
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,742,s.append(Bytes.toString(e.getValue().get()));
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,743,s.append("'");
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,745,s.append(", ");
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,746,s.append(FAMILIES);
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,747,s.append(" => [");
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,748,int size = families.values().size();
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,749,int i = 0;
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,750,for(HColumnDescriptor hcd : families.values()) {
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,751,s.append(hcd.toStringCustomizedValues());
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,752,i++;
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,754,s.append(", ");
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,756,s.append("]}");
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,757,return s.toString();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,416,public HRegion(Path tableDir, HLog log, FileSystem fs, Configuration conf,
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,423,this.conf = conf;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4359,Configuration conf = a.getConf();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,5211,35 * ClassSize.REFERENCE + 2 * Bytes.SIZEOF_INT +
src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java,701,this.parent.getLog(), fs, this.parent.getConf(),
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,183,super(conf, region.getRegionInfo().getTableNameAsString(),
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,191,this.conf = conf;
src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java,682,if (hlogs == null || hlogs.size() == 0) continue; // empty log queue.
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,125,private String[] deadRegionServers;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,203,private void checkIfQueueRecovered(String peerClusterZnode) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,204,String[] parts = peerClusterZnode.split("-");
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,209,this.deadRegionServers = new String[parts.length-1];
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,211,for (int i = 1; i < parts.length; i++) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,212,this.deadRegionServers[i-1] = parts[i];
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,511,LOG.info("NB dead servers : " + deadRegionServers.length);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,512,for (int i = this.deadRegionServers.length - 1; i >= 0; i--) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,515,new Path(manager.getLogDir().getParent(), this.deadRegionServers[i]);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,751,return;
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,777,fileSystemManager.splitLog(sn);
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,257,public void splitLog(final List<ServerName> serverNames) throws IOException {
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,286,splitLogSize = splitLogManager.splitLogDistributed(logDirs);
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,195,private FileStatus[] getFileList(List<Path> logDirs) throws IOException {
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,204,FileStatus[] logfiles = FSUtils.listStatus(fs, hLogDir, null);
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,243,FileStatus[] logfiles = getFileList(logDirs);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,53,private final ServerName serverName;
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,54,private final MasterServices services;
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,55,private final DeadServer deadServers;
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,56,private final boolean shouldSplitHlog; // whether to split HLog or not
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,100,throws InterruptedException, IOException, KeeperException {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,101,long timeout = this.server.getConfiguration().
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,102,getLong("hbase.catalog.verification.timeout", 1000);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,103,if (!this.server.getCatalogTracker().verifyRootRegionLocation(timeout)) {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,104,this.services.getAssignmentManager().assignRoot();
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,106,throw new IOException("-ROOT- is onlined on the dead server "
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,107,+ serverName);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,109,LOG.info("Skip assigning -ROOT-, because it is online on the "
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,110,+ server.getCatalogTracker().getRootLocation());
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,118,private void verifyAndAssignRootWithRetries() throws IOException {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,119,int iTimes = this.server.getConfiguration().getInt(
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,122,long waitTime = this.server.getConfiguration().getLong(
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,125,int iFlag = 0;
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,126,while (true) {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,127,try {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,128,verifyAndAssignRoot();
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,129,break;
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,131,this.server.abort("In server shutdown processing, assigning root", e);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,132,throw new IOException("Aborting", e);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,134,if (iFlag >= iTimes) {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,135,this.server.abort("verifyAndAssignRoot failed after" + iTimes
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,136,+ " times retries, aborting", e);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,137,throw new IOException("Aborting", e);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,139,try {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,140,Thread.sleep(waitTime);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,142,LOG.warn("Interrupted when is the thread sleep", e1);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,143,Thread.currentThread().interrupt();
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,144,throw new IOException("Interrupted", e1);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,146,iFlag++;
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,186,this.services.getExecutorService().submit(this);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,193,if (isCarryingRoot()) { // -ROOT-
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,196,if (this.services.getAssignmentManager().isCarryingRoot(serverName)) {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,197,LOG.info("Server " + serverName
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,198,+ " was carrying ROOT. Trying to assign.");
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,199,this.services.getAssignmentManager().regionOffline(
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,200,HRegionInfo.ROOT_REGIONINFO);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,201,verifyAndAssignRootWithRetries();
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,203,LOG.info("ROOT has been assigned to otherwhere, skip assigning.");
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,208,if (isCarryingMeta()) {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,211,if (this.services.getAssignmentManager().isCarryingMeta(serverName)) {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,212,LOG.info("Server " + serverName
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,213,+ " was carrying META. Trying to assign.");
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,214,this.services.getAssignmentManager().regionOffline(
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,215,HRegionInfo.FIRST_META_REGIONINFO);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,216,this.services.getAssignmentManager().assignMeta();
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,218,LOG.info("META has been assigned to otherwhere, skip assigning.");
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,967,try {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,968,if (this.hlog != null) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,976,LOG.error("Close and delete failed", RemoteExceptionHandler.checkThrowable(e));
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1553,UncaughtExceptionHandler handler = new UncaughtExceptionHandler() {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1554,public void uncaughtException(Thread t, Throwable e) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1555,abort("Uncaught exception in service thread " + t.getName(), e);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1574,Threads.setDaemonThreadRunning(this.hlogRoller.getThread(), n + ".logRoller", handler);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1576,handler);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1581,handler);
src/main/java/org/apache/hadoop/hbase/regionserver/LogRoller.java,49,private final RegionServerServices services;
src/main/java/org/apache/hadoop/hbase/regionserver/LogRoller.java,94,byte [][] regionsToFlush = this.services.getWAL().rollWriter(rollLog.get());
src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerServices.java,23,import java.util.Map;
src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerServices.java,41,public HLog getWAL();
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,46,private final RegionServerServices rsServices;
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,340,this.rsServices.getWAL(), this.server.getConfiguration(),
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,257,private static final Pattern pattern = Pattern.compile(".*\\.\\d*");
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,337,this(fs, dir, oldLogDir, conf, null, true, null);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,362,this(fs, dir, oldLogDir, conf, listeners, true, prefix);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,405,if (failIfLogDirExists && fs.exists(dir)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,408,if (!fs.mkdirs(dir)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,943,return new Path(dir, prefix + "." + filenum);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java,394,return createWriter(fs, path, ostream, blockSize,
src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java,395,compression, encoder, comparator, checksumType, bytesPerChecksum);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java,403,int bytesPerChecksum) throws IOException;
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV1.java,95,final int bytesPerChecksum) throws IOException {
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java,89,private final boolean includeMemstoreTS = true;
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java,104,final int bytesPerChecksum) throws IOException {
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java,105,return new HFileWriterV2(conf, cacheConf, fs, path, ostream, blockSize,
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java,106,compress, blockEncoder, comparator, checksumType, bytesPerChecksum);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java,115,final int bytesPerChecksum) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/Compactor.java,86,byte[] tmp = r.loadFileInfo().get(StoreFile.EARLIEST_PUT_TS);
src/main/java/org/apache/hadoop/hbase/regionserver/Compactor.java,162,writer = store.createWriterInTmp(maxKeyCount, compactionCompression, true);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,883,return createWriterInTmp(maxKeyCount, this.family.getCompression(), false);
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,826,bytesPerChecksum);
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,202,private volatile boolean initialized = false;
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,788,try {
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,790,+ blockBuffer.position() + KEY_VALUE_LEN_SIZE + currKeyLen
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,791,+ currValueLen;
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,792,currMemstoreTS = Bytes.readVLong(blockBuffer.array(),
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,793,memstoreTSOffset);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,794,currMemstoreTSLen = WritableUtils.getVIntSize(currMemstoreTS);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,796,throw new RuntimeException("Error reading memstore timestamp", e);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,835,try {
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,837,+ blockBuffer.position() + KEY_VALUE_LEN_SIZE + klen + vlen;
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,838,memstoreTS = Bytes.readVLong(blockBuffer.array(),
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,839,memstoreTSOffset);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,840,memstoreTSLen = WritableUtils.getVIntSize(memstoreTS);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,842,throw new RuntimeException("Error reading memstore timestamp", e);
src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java,23,import java.util.ArrayList;
src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java,24,import java.util.Arrays;
src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java,43,import org.apache.hadoop.hbase.regionserver.HRegion;
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,867,ZKUtil.createWithParents(zkw, znode);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,869,ZKUtil.setData(zkw, znode, data);
src/main/java/org/apache/hadoop/hbase/util/ClassSize.java,130,STRING = align(OBJECT + ARRAY + REFERENCE + 3 * Bytes.SIZEOF_INT);
src/main/java/org/apache/hadoop/hbase/util/ClassSize.java,132,CONCURRENT_HASHMAP = align((2 * Bytes.SIZEOF_INT) + ARRAY +
src/main/java/org/apache/hadoop/hbase/client/HTable.java,117,private static final int DOPUT_WB_CHECK = 10;    // i.e., doPut checks the writebuffer every X Puts.
src/main/java/org/apache/hadoop/hbase/client/HTable.java,749,doPut(Arrays.asList(put));
src/main/java/org/apache/hadoop/hbase/client/HTable.java,757,doPut(puts);
src/main/java/org/apache/hadoop/hbase/client/HTable.java,760,private void doPut(final List<Put> puts) throws IOException {
src/main/java/org/apache/hadoop/hbase/client/HTable.java,761,int n = 0;
src/main/java/org/apache/hadoop/hbase/client/HTable.java,763,validatePut(put);
src/main/java/org/apache/hadoop/hbase/client/HTable.java,764,writeBuffer.add(put);
src/main/java/org/apache/hadoop/hbase/client/HTable.java,765,currentWriteBufferSize += put.heapSize();
src/main/java/org/apache/hadoop/hbase/client/HTable.java,768,n++;
src/main/java/org/apache/hadoop/hbase/client/HTable.java,769,if (n % DOPUT_WB_CHECK == 0 && currentWriteBufferSize > writeBufferSize) {
src/main/java/org/apache/hadoop/hbase/client/HTable.java,770,flushCommits();
src/main/java/org/apache/hadoop/hbase/client/HTable.java,773,if (autoFlush || currentWriteBufferSize > writeBufferSize) {
src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java,36,import org.apache.hadoop.io.compress.DoNotPool;
src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java,352,if (decompressor.getClass().isAnnotationPresent(DoNotPool.class)) {
src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java,353,decompressor.end();
src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java,286,if (hstoreFilesToSplit.size() == 0) {
src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java,287,String errorMsg = "No store files to split for the region " + this.parent.getRegionInfo();
src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java,288,LOG.error(errorMsg);
src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java,289,throw new IOException(errorMsg);
src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java,675,if (peerIdsToProcess == null) return null; // node already processed
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java,575,Thread.sleep(sleepBeforeFailover);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java,600,if (newQueues.size() == 0) {
src/main/java/org/apache/hadoop/hbase/util/Bytes.java,342,try {
src/main/java/org/apache/hadoop/hbase/util/Bytes.java,343,String first = new String(b, off, len, "ISO-8859-1");
src/main/java/org/apache/hadoop/hbase/util/Bytes.java,344,for (int i = 0; i < first.length() ; ++i ) {
src/main/java/org/apache/hadoop/hbase/util/Bytes.java,345,int ch = first.charAt(i) & 0xFF;
src/main/java/org/apache/hadoop/hbase/util/Bytes.java,349,|| " `~!@#$%^&*()-_=+[]{}\\|;:'\",.<>/?".indexOf(ch) >= 0 ) {
src/main/java/org/apache/hadoop/hbase/util/Bytes.java,350,result.append(first.charAt(i));
src/main/java/org/apache/hadoop/hbase/util/Bytes.java,352,result.append(String.format("\\x%02X", ch));
src/main/java/org/apache/hadoop/hbase/util/Bytes.java,356,LOG.error("ISO-8859-1 not supported?", e);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,333,if(regionsToReopen.get(hri.getEncodedName()) != null) {
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,1377,this.executorService.submit(new ModifyTableHandler(tableName, htd, this, this));
src/main/java/org/apache/hadoop/hbase/master/handler/TableAddFamilyHandler.java,55,.addColumn(tableName, familyDesc);
src/main/java/org/apache/hadoop/hbase/master/handler/TableAddFamilyHandler.java,57,this.masterServices.getTableDescriptors().add(htd);
src/main/java/org/apache/hadoop/hbase/master/handler/TableDeleteFamilyHandler.java,49,HTableDescriptor htd =
src/main/java/org/apache/hadoop/hbase/master/handler/TableDeleteFamilyHandler.java,50,this.masterServices.getMasterFileSystem().deleteColumn(tableName, familyName);
src/main/java/org/apache/hadoop/hbase/master/handler/TableDeleteFamilyHandler.java,52,this.masterServices.getTableDescriptors().add(htd);
src/main/java/org/apache/hadoop/hbase/master/handler/TableEventHandler.java,117,HTable table = new HTable(masterServices.getConfiguration(), tableName);
src/main/java/org/apache/hadoop/hbase/master/handler/TableEventHandler.java,118,TreeMap<ServerName, List<HRegionInfo>> serverToRegions = Maps
src/main/java/org/apache/hadoop/hbase/master/handler/TableEventHandler.java,119,.newTreeMap();
src/main/java/org/apache/hadoop/hbase/master/handler/TableEventHandler.java,120,NavigableMap<HRegionInfo, ServerName> hriHserverMapping = table.getRegionLocations();
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,1718,return timeRangeTracker.maximumTimestamp;
src/main/java/org/apache/hadoop/hbase/rest/RESTServlet.java,73,this.pool = new HTablePool(conf, 10);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperMainServerArg.java,23,import java.util.Properties;
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperMainServerArg.java,47,if (key.startsWith("server.") && host == null) {
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperMainServerArg.java,49,host = parts[0];
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperMainServerArg.java,53,if (host != null && clientPort != null) break;
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperMainServerArg.java,55,return host != null && clientPort != null? host + ":" + clientPort: null;
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV1.java,683,ByteBuffer buf = getMetaBlock(HFileWriterV1.BLOOM_FILTER_META_KEY, true);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,5626,!this.htableDescriptor.isDeferredLogFlush()) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,247,private final LogSyncer logSyncerThread;
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,441,logSyncerThread = new LogSyncer(this.optionalFlushInterval);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,442,Threads.setDaemonThreadRunning(logSyncerThread.getThread(),
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,443,Thread.currentThread().getName() + ".logSyncer");
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,988,try {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,989,logSyncerThread.close();
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,991,logSyncerThread.join(this.optionalFlushInterval*2);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,993,LOG.error("Exception while waiting for syncer thread to die", e);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,1313,pending = logSyncerThread.getPendingWrites();
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,1315,logSyncerThread.hlogFlush(tempWriter, pending);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,1326,logSyncerThread.hlogFlush(tempWriter, pending);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,1476,logSyncerThread.append(new HLog.Entry(logKey, logEdit));
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,1605,logSyncerThread.append(new Entry(key, edit));
src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java,187,public synchronized void requestCompaction(final HRegion r,
src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java,188,final String why) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java,189,for(Store s : r.getStores().values()) {
src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java,190,requestCompaction(r, s, why, Store.NO_PRIORITY);
src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java,194,public synchronized void requestCompaction(final HRegion r, final Store s,
src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java,195,final String why) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java,196,requestCompaction(r, s, why, Store.NO_PRIORITY);
src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java,199,public synchronized void requestCompaction(final HRegion r, final String why,
src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java,200,int p) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java,201,for(Store s : r.getStores().values()) {
src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java,202,requestCompaction(r, s, why, p);
src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java,212,public synchronized void requestCompaction(final HRegion r, final Store s,
src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java,213,final String why, int priority) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java,215,return;
src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java,217,CompactionRequest cr = s.requestCompaction(priority);
src/main/java/org/apache/hadoop/hbase/regionserver/CompactionRequestor.java,30,public void requestCompaction(final HRegion r, final String why) throws IOException;
src/main/java/org/apache/hadoop/hbase/regionserver/CompactionRequestor.java,38,public void requestCompaction(final HRegion r, final Store s, final String why) throws IOException;
src/main/java/org/apache/hadoop/hbase/regionserver/CompactionRequestor.java,46,public void requestCompaction(final HRegion r, final String why, int pri) throws IOException;
src/main/java/org/apache/hadoop/hbase/regionserver/CompactionRequestor.java,55,public void requestCompaction(final HRegion r, final Store s,
src/main/java/org/apache/hadoop/hbase/regionserver/CompactionRequestor.java,56,final String why, int pri) throws IOException;
src/main/java/org/apache/hadoop/hbase/regionserver/Compactor.java,63,StoreFile.Writer compact(final Store store,
src/main/java/org/apache/hadoop/hbase/regionserver/Compactor.java,66,throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/Compactor.java,79,long keyCount = (r.getBloomFilterType() == store.getFamily().getBloomFilterType()) ?
src/main/java/org/apache/hadoop/hbase/regionserver/Compactor.java,133,majorCompaction ? ScanType.MAJOR_COMPACT : ScanType.MINOR_COMPACT, earliestPutTs);
src/main/java/org/apache/hadoop/hbase/regionserver/Compactor.java,145,store.getHRegion().getCoprocessorHost().preCompact(store, scanner);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1274,this.instance.compactSplitThread.requestCompaction(r, s,
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1275,getName() + " requests compaction");
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1277,if (majorCompactPriority == DEFAULT_PRIORITY ||
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1278,majorCompactPriority > r.getCompactPriority()) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1279,this.instance.compactSplitThread.requestCompaction(r, s,
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1280,getName() + " requests major compaction; use default priority");
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1282,this.instance.compactSplitThread.requestCompaction(r, s,
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1283,getName() + " requests major compaction; use configured priority",
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1284,this.majorCompactPriority);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1705,getCompactionRequester().requestCompaction(r, s, "Opening Region");
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,3079,Store.PRIORITY_USER);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,3082,Store.PRIORITY_USER);
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,348,ScanType scanType, long earliestPutTs) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,356,scanType, earliestPutTs, s);
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,376,public boolean preCompactSelection(Store store, List<StoreFile> candidates) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,383,((RegionObserver)env.getInstance()).preCompactSelection(
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,384,ctx, store, candidates);
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,404,public void postCompactSelection(Store store,
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,405,ImmutableList<StoreFile> selected) {
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,411,((RegionObserver)env.getInstance()).postCompactSelection(
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,412,ctx, store, selected);
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,429,public InternalScanner preCompact(Store store, InternalScanner scanner) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,432,for (RegionEnvironment env: coprocessors) {
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,436,scanner = ((RegionObserver)env.getInstance()).preCompact(
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,437,ctx, store, scanner);
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,439,handleCoprocessorThrowable(env,e);
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,456,public void postCompact(Store store, StoreFile resultFile) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,462,((RegionObserver)env.getInstance()).postCompact(ctx, store, resultFile);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1060,StoreFile.Writer writer =
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1061,this.compactor.compact(this, filesToCompact, cr.isMajor(), maxId);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1066,region.getCoprocessorHost().postCompact(this, sf);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1129,StoreFile.Writer writer =
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1130,this.compactor.compact(this, filesToCompact, isMajor, maxId);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1134,region.getCoprocessorHost().postCompact(this, sf);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1285,return requestCompaction(NO_PRIORITY);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1288,public CompactionRequest requestCompaction(int priority) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1294,CompactionRequest ret = null;
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1311,override = region.getCoprocessorHost().preCompactSelection(
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1312,this, candidates);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1324,ImmutableList.copyOf(filesToCompact.getFilesToCompact()));
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1350,ret = new CompactionRequest(region, this, filesToCompact, isMajor, pri);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1355,if (ret != null) {
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1356,CompactionRequest.preRequest(ret);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1358,return ret;
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,54,private final CompactSelection compactSelection;
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,55,private final long totalSize;
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,56,private final boolean isMajor;
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,69,public CompactionRequest(HRegion r, Store s,
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,70,CompactSelection files, boolean isMajor, int p) {
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,71,Preconditions.checkNotNull(r);
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,72,Preconditions.checkNotNull(files);
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,74,this.r = r;
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,75,this.s = s;
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,76,this.compactSelection = files;
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,77,long sz = 0;
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,78,for (StoreFile sf : files.getFilesToCompact()) {
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,79,sz += sf.getReader().length();
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,81,this.totalSize = sz;
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,82,this.isMajor = isMajor;
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,83,this.p = p;
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,84,this.timeInNanos = System.nanoTime();
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,258,server.compactSplitThread
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,259,.requestCompaction(r, s, "Recursive enqueue");
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,262,server.compactSplitThread.requestSplit(r);
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,274,LOG.debug("CompactSplitThread status: " + server.compactSplitThread);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,803,errors.reportError(ERROR_CODE.NO_TABLEINFO_FILE,
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,810,modTInfo.addRegionInfo(hbi);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,1794,modTInfo.addRegionInfo(hbi);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2558,MetaEntry m = new MetaEntry(hri, sn, ts);
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,76,DistributedFileSystem.class.getMethod("recoverLease",
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,77,new Class[] {Path.class}).invoke(dfs, p);
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,85,LOG.debug("Failed fs.recoverLease invocation, " + e.toString() +
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,90,recovered = true;
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,116,iioe.initCause(ex);
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,587,addedSize -= heapSizeChange(kv, true);
src/main/java/org/apache/hadoop/hbase/regionserver/SplitLogWorker.java,110,try {
src/main/java/org/apache/hadoop/hbase/regionserver/SplitLogWorker.java,112,fs.getFileStatus(new Path(filename)), fs, conf, p) == false) {
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,717,private Path flushCache(final long logCacheFlushId,
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,27,import java.util.Set;
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,47,import org.apache.hadoop.hbase.regionserver.wal.OrphanHLogAfterSplitException;
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,62,import org.apache.hadoop.hbase.zookeeper.ZKSplitLog.TaskState;
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,423,deleteNode(path, Long.MAX_VALUE);
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,656,LOG.fatal("logic failure, failing to delete a node should never happen " +
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,73,private ZooKeeper zk;
src/main/java/org/apache/hadoop/hbase/util/ByteBloomFilter.java,421,int bloomBitSize = bloomSize * 8;
src/main/java/org/apache/hadoop/hbase/util/ByteBloomFilter.java,426,long hashLoc = Math.abs((hash1 + i * hash2) % bloomBitSize);
src/main/java/org/apache/hadoop/hbase/util/ByteBloomFilter.java,433,long hashLoc = randomGeneratorForTest.nextInt(bloomBitSize);
src/main/java/org/apache/hadoop/hbase/util/ByteBloomFilter.java,464,static boolean get(long pos, byte[] bloomArray, int bloomOffset) {
src/main/java/org/apache/hadoop/hbase/util/ByteBloomFilter.java,465,int bytePos = (int)(pos / 8);
src/main/java/org/apache/hadoop/hbase/util/ByteBloomFilter.java,466,int bitPos = (int)(pos % 8);
src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java,121,private boolean stopped = false;
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,44,import com.google.common.collect.ClassToInstanceMap;
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,45,import com.google.common.collect.Maps;
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,46,import com.google.common.collect.MutableClassToInstanceMap;
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,556,status.setStatus("Splitting logs after master startup");
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,557,splitLogAfterStartup(this.fileSystemManager);
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,560,assignRootAndMeta(status);
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,637,protected void splitLogAfterStartup(final MasterFileSystem mfs) {
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,638,mfs.splitLogAfterStartup();
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,682,rit = this.assignmentManager.
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,683,processRegionInTransitionAndBlockUntilAssigned(HRegionInfo.FIRST_META_REGIONINFO);
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,686,ServerName currentMetaServer =
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,687,this.catalogTracker.getMetaLocationOrReadLocationFromRoot();
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,688,if (currentMetaServer != null
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,689,&& !currentMetaServer.equals(currentRootServer)) {
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,704,LOG.info(".META. assigned=" + assigned + ", rit=" + rit +
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,706,status.setStatus("META and ROOT assigned.");
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,707,return assigned;
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,1634,LOG.error("ZooKeeper exception trying to set cluster as down in ZK", e);
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,190,void splitLogAfterStartup() {
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,192,HLog.SPLIT_SKIP_ERRORS_DEFAULT);
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,196,LOG.warn("Master stopped while splitting logs");
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,199,List<ServerName> serverNames = new ArrayList<ServerName>();
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,201,if (!this.fs.exists(logsDirPath)) return;
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,206,.keySet();
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,210,return;
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,225,+ " belongs to an existing region server");
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,228,splitLog(serverNames);
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,231,LOG.warn("Failed splitting of " + serverNames, ioe);
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,238,Thread.sleep(conf.getInt(
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,69,import org.apache.hadoop.hbase.HealthCheckChore;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,172,import org.joda.time.field.MillisDurationField;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1679,this.rsHost.preStop(msg);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperNodeTracker.java,48,private boolean stopped = false;
src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java,69,import org.apache.hadoop.hbase.security.User;
src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java,85,import org.apache.hadoop.hbase.util.Strings;
src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java,87,import org.apache.hadoop.net.DNS;
src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java,370,if (User.isSecurityEnabled() && User.isHBaseSecurityEnabled(conf)) {
src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java,371,String machineName = Strings.domainNamePointerToHostName(
src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java,372,DNS.getDefaultHost(conf.get("hbase.thrift.dns.interface", "default"),
src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java,373,conf.get("hbase.thrift.dns.nameserver", "default")));
src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java,374,User.login(conf, "hbase.thrift.keytab.file",
src/main/java/org/apache/hadoop/hbase/filter/Filter.java,177,public boolean isFamilyEssential(byte[] name);
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,323,if (filter.isFamilyEssential(name)) {
src/main/java/org/apache/hadoop/hbase/filter/SkipFilter.java,108,return filter.isFamilyEssential(name);
src/main/java/org/apache/hadoop/hbase/filter/WhileMatchFilter.java,109,return filter.isFamilyEssential(name);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3576,|| this.filter.isFamilyEssential(entry.getKey())) {
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,269,throw new IllegalArgumentException("Failed resolve of " + this.isa);
src/main/java/org/apache/hadoop/hbase/master/ServerManager.java,559,HRegionInterface hri = this.serverConnections.get(sn.toString());
src/main/java/org/apache/hadoop/hbase/rest/Main.java,79,VersionInfo.logVersion();
src/main/java/org/apache/hadoop/hbase/rest/Main.java,174,if (User.isSecurityEnabled() && User.isHBaseSecurityEnabled(conf)) {
src/main/java/org/apache/hadoop/hbase/rest/Main.java,175,String machineName = Strings.domainNamePointerToHostName(
src/main/java/org/apache/hadoop/hbase/rest/Main.java,176,DNS.getDefaultHost(conf.get("hbase.rest.dns.interface", "default"),
src/main/java/org/apache/hadoop/hbase/rest/Main.java,177,conf.get("hbase.rest.dns.nameserver", "default")));
src/main/java/org/apache/hadoop/hbase/rest/Main.java,178,User.login(conf, "hbase.rest.keytab.file", "hbase.rest.kerberos.principal",
src/main/java/org/apache/hadoop/hbase/rest/Main.java,179,machineName);
src/main/java/org/apache/hadoop/hbase/client/Delete.java,85,this(row, HConstants.LATEST_TIMESTAMP, null);
src/main/java/org/apache/hadoop/hbase/zookeeper/MiniZooKeeperCluster.java,124,FileTxnLog.setPreallocSize(100);
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,1201,public static void checkAccess(UserGroupInformation ugi, FileStatus file,
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,1414,UserGroupInformation ugi = User.getCurrent().getUGI();
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,1418,FSUtils.checkAccess(ugi, file, FsAction.WRITE);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,852,(!regionState.isPendingOpen() && !regionState.isOpening())) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,880,(!regionState.isPendingOpen() && !regionState.isOpening())) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,881,LOG.warn("Received OPENING for region " +
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,882,prettyPrintedRegionName +
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,886,return;
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,906,(!regionState.isPendingOpen() && !regionState.isOpening())) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1688,state.update(RegionState.State.PENDING_OPEN, System.currentTimeMillis(),
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1689,plan.getDestination());
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1692,RegionOpeningState regionOpenState = serverManager.sendRegionOpen(plan
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1693,.getDestination(), state.getRegion(), versionOfOfflineNode);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1694,if (regionOpenState == RegionOpeningState.ALREADY_OPENED) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,3199,public List<RegionState> processServerShutdown(final ServerName sn) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,3216,Set<HRegionInfo> deadRegions = null;
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,3217,List<RegionState> rits = new ArrayList<RegionState>();
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,3220,if (assignedRegions == null || assignedRegions.isEmpty()) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,3222,return rits;
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,3224,deadRegions = new TreeSet<HRegionInfo>(assignedRegions);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,3225,for (HRegionInfo region : deadRegions) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,3226,this.regions.remove(region);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,3233,for (RegionState region : this.regionsInTransition.values()) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,3234,if (deadRegions.remove(region.getRegion())) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,3235,rits.add(region);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,3239,return rits;
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,250,List<RegionState> regionsInTransition =
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,251,this.services.getAssignmentManager().
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,252,processServerShutdown(this.serverName);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,286,for (RegionState rit : regionsInTransition) {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,287,if (!rit.isClosing() && !rit.isPendingClose() && !rit.isSplitting()) {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,288,LOG.debug("Removed " + rit.getRegion().getRegionNameAsString() +
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,290,rit.getState());
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,291,if (hris != null) hris.remove(rit.getRegion());
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,295,assert regionsInTransition != null;
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,296,LOG.info("Reassigning " + ((hris == null)? 0: hris.size()) +
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,299,regionsInTransition.size() +
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,303,if (hris != null) {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,304,for (Map.Entry<HRegionInfo, Result> e: hris.entrySet()) {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,305,RegionState rit = this.services.getAssignmentManager().isRegionInTransition(e.getKey());
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,306,if (processDeadRegion(e.getKey(), e.getValue(),
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,307,this.services.getAssignmentManager(),
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,308,this.server.getCatalogTracker())) {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,310,.getRegionServerOfRegion(e.getKey());
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,311,if (rit != null && !rit.isClosing() && !rit.isPendingClose() && !rit.isSplitting()) {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,314,LOG.info("Skip assigning region " + rit.toString());
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,316,&& !addressFromAM.equals(this.serverName)) {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,317,LOG.debug("Skip assigning region "
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,319,+ " because it has been opened in "
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,320,+ addressFromAM.getServerName());
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,322,this.services.getAssignmentManager().assign(e.getKey(), true);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,330,HRegionInfo region = rit.getRegion();
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,331,AssignmentManager am = this.services.getAssignmentManager();
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,332,am.regionOffline(region);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,339,if (rit != null
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,342,.isDisablingOrDisabledTable(rit.getRegion().getTableNameAsString())) {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,343,HRegionInfo hri = rit.getRegion();
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,344,AssignmentManager am = this.services.getAssignmentManager();
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,345,am.deleteClosingOrClosedNode(hri);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,346,am.regionOffline(hri);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2813,checkIfRegionInTransition(region, OPEN);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2830,LOG.info("Received request to open region: " +
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2831,region.getRegionNameAsString());
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2832,HTableDescriptor htd = null;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2833,if (htds == null) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2834,htd = this.tableDescriptors.get(region.getTableName());
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2836,htd = htds.get(region.getTableNameAsString());
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2837,if (htd == null) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2839,htds.put(region.getTableNameAsString(), htd);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2842,this.regionsInTransitionInRS.putIfAbsent(region.getEncodedNameAsBytes(),
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2843,true);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2845,if (region.isRootRegion()) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2846,this.service.submit(new OpenRootHandler(this, this, region, htd,
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2847,versionOfOfflineNode));
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2849,this.service.submit(new OpenMetaHandler(this, this, region, htd,
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2850,versionOfOfflineNode));
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2852,this.service.submit(new OpenRegionHandler(this, this, region, htd,
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2853,versionOfOfflineNode));
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2858,private void checkIfRegionInTransition(HRegionInfo region,
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2859,String currentAction) throws RegionAlreadyInTransitionException {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2860,byte[] encodedName = region.getEncodedNameAsBytes();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2861,if (this.regionsInTransitionInRS.containsKey(encodedName)) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2862,boolean openAction = this.regionsInTransitionInRS.get(encodedName);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2864,throw new RegionAlreadyInTransitionException("Received:" + currentAction +
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2867,(openAction ? OPEN : CLOSE)+ ".");
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2922,checkIfRegionInTransition(region, CLOSE);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2971,if (this.regionsInTransitionInRS.containsKey(region.getEncodedNameAsBytes())) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2972,LOG.warn("Received close for region we are already opening or closing; " +
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2973,region.getEncodedName());
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2976,this.regionsInTransitionInRS.putIfAbsent(region.getEncodedNameAsBytes(), false);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2977,CloseRegionHandler crh = null;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2978,if (region.isRootRegion()) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2979,crh = new CloseRootHandler(this, this, region, abort, zk,
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2980,versionOfClosingNode);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2982,crh = new CloseMetaHandler(this, this, region, abort, zk,
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2983,versionOfClosingNode);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2985,crh = new CloseRegionHandler(this, this, region, abort, zk,
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2986,versionOfClosingNode);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2988,this.service.submit(crh);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,3676,public ConcurrentSkipListMap<byte[], Boolean> getRegionsInTransitionInRS() {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,3677,return this.regionsInTransitionInRS;
src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerServices.java,81,public Map<byte[], Boolean> getRegionsInTransitionInRS();
src/main/java/org/apache/hadoop/hbase/regionserver/handler/CloseRegionHandler.java,151,this.rsServices.getRegionsInTransitionInRS().
src/main/java/org/apache/hadoop/hbase/regionserver/handler/CloseRegionHandler.java,152,remove(this.regionInfo.getEncodedNameAsBytes());
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,56,private volatile int versionOfOfflineNode = -1;
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,65,HTableDescriptor htd, int versionOfOfflineNode) {
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,67,versionOfOfflineNode);
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,73,final int versionOfOfflineNode) {
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,78,this.versionOfOfflineNode = versionOfOfflineNode;
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,101,if (!transitionZookeeperOfflineToOpening(encodedName,
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,102,versionOfOfflineNode)) {
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,103,LOG.warn("Region was hijacked? It no longer exists, encodedName=" +
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,104,encodedName);
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,105,return;
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,147,this.rsServices.getRegionsInTransitionInRS().
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,148,remove(this.regionInfo.getEncodedNameAsBytes());
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,381,boolean transitionZookeeperOfflineToOpening(final String encodedName,
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,382,int versionOfOfflineNode) {
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,384,try {
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,386,this.version = ZKAssign.transitionNode(server.getZooKeeper(), regionInfo,
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,387,server.getServerName(), EventType.M_ZK_REGION_OFFLINE,
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,388,EventType.RS_ZK_REGION_OPENING, versionOfOfflineNode);
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,390,LOG.error("Error transition from OFFLINE to OPENING for region=" +
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,391,encodedName, e);
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,393,boolean b = isGoodVersion();
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,394,if (!b) {
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,395,LOG.warn("Failed transition from OFFLINE to OPENING for region=" +
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,396,encodedName);
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,398,return b;
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,215,keytabFilename, principalName);
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,1203,if (ugi.getUserName().equals(file.getOwner())) {
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,1215,+ " path=" + file.getPath() + " user=" + ugi.getUserName());
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,256,if (filter.filterAllRemaining() || filter.filterRow()) {
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,261,&& !filter.filterRow()) {
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,63,private static final boolean USEMSLAB_DEFAULT = false;
