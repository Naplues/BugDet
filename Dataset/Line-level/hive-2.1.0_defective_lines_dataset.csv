File,Line_number,SRC
orc/src/java/org/apache/orc/impl/RecordReaderUtils.java,251,long start = index.getEntry(group).getPositions(posn);
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,594,if (hasNulls) {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,596,for (int i = 0; batchSize <= result.isNull.length && i < batchSize; i++) {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,597,allNulls = allNulls & result.isNull[i];
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,599,if (allNulls) {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,600,result.vector[0] = Double.NaN;
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,601,result.isRepeating = true;
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,604,result.isRepeating = false;
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,606,for (int i = 0; batchSize <= result.isNull.length
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,607,&& batchSize <= result.vector.length && i < batchSize; i++) {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,608,if (!result.isNull[i]) {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,609,result.vector[i] = utils.readFloat(stream);
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,612,result.vector[i] = Double.NaN;
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,618,boolean repeating = (batchSize > 1);
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,619,final float f1 = utils.readFloat(stream);
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,620,result.vector[0] = f1;
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,622,for (int i = 1; i < batchSize && batchSize <= result.vector.length; i++) {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,623,final float f2 = utils.readFloat(stream);
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,624,repeating = repeating && (f1 == f2);
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,625,result.vector[i] = f2;
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,627,result.isRepeating = repeating;
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,688,if (hasNulls) {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,690,for (int i = 0; i < batchSize && batchSize <= result.isNull.length; i++) {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,691,allNulls = allNulls & result.isNull[i];
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,693,if (allNulls) {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,694,result.vector[0] = Double.NaN;
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,695,result.isRepeating = true;
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,698,result.isRepeating = false;
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,700,for (int i = 0; batchSize <= result.isNull.length
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,701,&& batchSize <= result.vector.length && i < batchSize; i++) {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,702,if (!result.isNull[i]) {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,703,result.vector[i] = utils.readDouble(stream);
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,706,result.vector[i] = Double.NaN;
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,712,boolean repeating = (batchSize > 1);
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,713,final double d1 = utils.readDouble(stream);
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,714,result.vector[0] = d1;
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,716,for (int i = 1; i < batchSize && batchSize <= result.vector.length; i++) {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,717,final double d2 = utils.readDouble(stream);
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,718,repeating = repeating && (d1 == d2);
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,719,result.vector[i] = d2;
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,721,result.isRepeating = repeating;
ql/src/java/org/apache/hadoop/hive/ql/parse/ParseDriver.java,166,return parse(command, ctx, true);
ql/src/java/org/apache/hadoop/hive/ql/parse/ParseDriver.java,192,if ( setTokenRewriteStream) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2330,String viewText = tab.getViewExpandedText();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2333,ASTNode tree = pd.parse(viewText, ctx, false);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2334,tree = ParseUtils.findRootNonNullToken(tree);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2335,viewTree = tree;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,10529,table = getTableObjectByName(tabIdName);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,10569,public ASTNode rewriteASTWithMaskAndFilter(ASTNode ast) throws SemanticException {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,10592,walkASTMarkTABREF(subq, cteAlias);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,10597,walkASTMarkTABREF((ASTNode) ast.getChild(index), cteAlias);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,10602,walkASTMarkTABREF(ast, cteAlias);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,10607,String rewrittenQuery = ctx.getTokenRewriteStream().toString(ast.getTokenStartIndex(),
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,10608,ast.getTokenStopIndex());
orc/src/java/org/apache/orc/OrcFile.java,143,if (val == FUTURE.id) return FUTURE; // Special handling for the magic value.
hplsql/src/main/java/org/apache/hive/hplsql/Select.java,150,exec.closeQuery(query, exec.conf.defaultConnection);
hplsql/src/main/java/org/apache/hive/hplsql/Select.java,153,exec.closeQuery(query, exec.conf.defaultConnection);
hplsql/src/main/java/org/apache/hive/hplsql/Stmt.java,796,exec.closeQuery(query, exec.conf.defaultConnection);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,266,((Connection)jdoConn.getNativeConnection()).createStatement().execute(queryText);
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,398,if (LOG.isDebugEnabled()) {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,399,LOG.debug("Working with ServiceRecord: {}", srv);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/EvictingPriorityBlockingQueue.java,45,deque.offer(e);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/QueryInfo.java,192,if (lastFinishableState != fragmentInfo.canFinish()) {
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/QueryInfo.java,196,return true;
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java,353,if (evictedTask == null || evictedTask != taskWrapper) {
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java,390,boolean stateChanged = taskWrapper.maybeRegisterForFinishedStateNotifications(canFinish);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java,391,if (stateChanged) {
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java,392,if (isDebugEnabled) {
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java,393,LOG.debug("Finishable state of {} updated to {} during registration for state updates",
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java,394,taskWrapper.getRequestId(), !canFinish);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java,396,finishableStateUpdated(taskWrapper, !canFinish);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java,403,knownTasks.remove(evictedTask.getRequestId());
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java,404,evictedTask.maybeUnregisterForFinishedStateNotifications();
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java,405,evictedTask.setIsInWaitQueue(false);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java,406,evictedTask.getTaskRunnerCallable().killTask();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,2841,currentBatch.add(rs.getLong(1));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnUtils.java,188,if (i * batchSize == inList.size()) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnUtils.java,190,return;
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHFileOutputFormat.java,151,Path srcDir = outputdir;
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHFileOutputFormat.java,174,fs.delete(outputdir, true);
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHFileOutputFormat.java,175,fs.createNewFile(outputdir);
shims/common/src/main/java/org/apache/hadoop/hive/thrift/TokenStoreDelegationTokenSecretManager.java,296,try {
shims/common/src/main/java/org/apache/hadoop/hive/thrift/TokenStoreDelegationTokenSecretManager.java,297,while (running) {
shims/common/src/main/java/org/apache/hadoop/hive/thrift/TokenStoreDelegationTokenSecretManager.java,316,.error("InterruptedExcpetion recieved for ExpiredTokenRemover thread "
shims/common/src/main/java/org/apache/hadoop/hive/thrift/TokenStoreDelegationTokenSecretManager.java,321,LOGGER.error("ExpiredTokenRemover thread received unexpected exception. "
shims/common/src/main/java/org/apache/hadoop/hive/thrift/TokenStoreDelegationTokenSecretManager.java,322,+ t, t);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,454,OperationManager operationManager = getOperationManager();
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,458,OperationHandle opHandle = operation.getHandle();
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,467,operationManager.closeOperation(opHandle);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,589,&& statsObj.getColType().equals(newCol.getType())) {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,686,&& statsObj.getColType().equals(newCol.getType())) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,3822,HashMap<String, ASTNode> windowingExprs = parseInfo.getWindowingExprsForClause(dest);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,3833,if (windowingExprs != null && windowingExprs.containsKey(grpbyExpr.toStringTree())) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,3834,if (!isCBOExecuted()) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,3835,throw new SemanticException("SELECT DISTINCT not allowed in the presence of windowing"
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,3836,+ " functions when CBO is off");
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,3838,continue;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,9283,if (qbp.getAggregationExprsForClause(dest).size() != 0
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,9284,|| getGroupByForClause(qbp, dest).size() > 0) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,159,private boolean destroyed;
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,360,close();
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,525,LOG.info("Completed compiling command(queryId=" + queryId + "); Time taken: " + duration + " seconds");
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1054,if (plan != null) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1055,fetchTask = plan.getFetchTask();
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1056,if (fetchTask != null) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1057,fetchTask.setDriverContext(null);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1058,fetchTask.setQueryPlan(null);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1062,if (driverCxt != null) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1063,driverCxt.shutdown();
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1064,driverCxt = null;
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1066,plan = null;
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1082,CommandProcessorResponse cpr;
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1083,try {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1084,cpr = runInternal(command, alreadyCompiled);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1086,releaseResources();
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1145,return createProcessorResponse(compileInternal(command));
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1149,private int compileInternal(String command) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1158,ret = compile(command);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1228,HiveDriverRunHookContext hookContext = new HiveDriverRunHookContextImpl(conf, command);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1230,List<HiveDriverRunHook> driverRunHooks;
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1232,driverRunHooks = getHooks(HiveConf.ConfVars.HIVE_DRIVER_RUN_HOOKS,
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1233,HiveDriverRunHook.class);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1234,for (HiveDriverRunHook driverRunHook : driverRunHooks) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1235,driverRunHook.preDriverRun(hookContext);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1238,errorMessage = "FAILED: Hive Internal Error: " + Utilities.getNameMessage(e);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1239,SQLState = ErrorMsg.findSQLState(e.getMessage());
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1240,downstreamError = e;
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1241,console.printError(errorMessage + "\n"
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1242,+ org.apache.hadoop.util.StringUtils.stringifyException(e));
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1243,return createProcessorResponse(12);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1251,int ret;
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1252,if (!alreadyCompiled) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1253,ret = compileInternal(command);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1254,if (ret != 0) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1255,return createProcessorResponse(ret);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1259,plan.setQueryStartTime(perfLogger.getStartTime(PerfLogger.DRIVER_RUN));
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1264,HiveTxnManager txnManager = SessionState.get().getTxnMgr();
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1265,ctx.setHiveTxnManager(txnManager);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1267,boolean startTxnImplicitly = false;
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1272,if (txnManager.isTxnOpen() && !plan.getOperation().isAllowedInTransaction()) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1273,assert !txnManager.getAutoCommit() : "didn't expect AC=true";
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1274,return rollback(new CommandProcessorResponse(12, ErrorMsg.OP_NOT_ALLOWED_IN_TXN, null,
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1275,plan.getOperationName(), Long.toString(txnManager.getCurrentTxnId())));
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1277,if(!txnManager.isTxnOpen() && plan.getOperation().isRequiresOpenTransaction()) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1278,return rollback(new CommandProcessorResponse(12, ErrorMsg.OP_NOT_ALLOWED_WITHOUT_TXN, null, plan.getOperationName()));
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1280,if(!txnManager.isTxnOpen() && plan.getOperation() == HiveOperation.QUERY && !txnManager.getAutoCommit()) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1283,startTxnImplicitly = true;
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1285,if(txnManager.getAutoCommit() && plan.getOperation() == HiveOperation.START_TRANSACTION) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1289,if(plan.getOperation() == HiveOperation.SET_AUTOCOMMIT) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1290,try {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1291,if(plan.getAutoCommitValue() && !txnManager.getAutoCommit()) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1294,releaseLocksAndCommitOrRollback(true, null);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1295,txnManager.setAutoCommit(true);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1297,else if(!plan.getAutoCommitValue() && txnManager.getAutoCommit()) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1298,txnManager.setAutoCommit(false);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1300,else {/*didn't change autoCommit value - no-op*/}
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1302,catch(LockException e) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1303,return handleHiveException(e, 12);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1307,if (requiresLock()) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1308,ret = acquireLocksAndOpenTxn(startTxnImplicitly);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1313,ret = execute();
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1314,if (ret != 0) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1316,return rollback(createProcessorResponse(ret));
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1320,try {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1321,if(txnManager.getAutoCommit() || plan.getOperation() == HiveOperation.COMMIT) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1322,releaseLocksAndCommitOrRollback(true, null);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1324,else if(plan.getOperation() == HiveOperation.ROLLBACK) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1325,releaseLocksAndCommitOrRollback(false, null);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1327,else {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1331,return handleHiveException(e, 12);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1334,perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.DRIVER_RUN);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1335,queryDisplay.setPerfLogStarts(QueryDisplay.Phase.EXECUTION, perfLogger.getStartTimes());
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1336,queryDisplay.setPerfLogEnds(QueryDisplay.Phase.EXECUTION, perfLogger.getEndTimes());
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1339,try {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1340,for (HiveDriverRunHook driverRunHook : driverRunHooks) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1341,driverRunHook.postDriverRun(hookContext);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1344,errorMessage = "FAILED: Hive Internal Error: " + Utilities.getNameMessage(e);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1345,SQLState = ErrorMsg.findSQLState(e.getMessage());
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1346,downstreamError = e;
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1347,console.printError(errorMessage + "\n"
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1348,+ org.apache.hadoop.util.StringUtils.stringifyException(e));
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1349,return createProcessorResponse(12);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1352,return createProcessorResponse(ret);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1467,String queryId = plan.getQueryId();
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1519,console.printInfo("Query ID = " + plan.getQueryId());
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1557,while (!destroyed && driverCxt.isRunning()) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1729,LOG.info("Completed executing command(queryId=" + queryId + "); Time taken: " + duration + " seconds");
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1732,releasePlan(plan);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1741,private synchronized void releasePlan(QueryPlan plan) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1743,if (plan != null) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1744,plan.setDone();
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1745,if (SessionState.get() != null) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1746,try {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1747,SessionState.get().getHiveHistory().logPlanProgress(plan);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1750,LOG.warn("Could not log query plan progress", e);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1869,if (destroyed) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1870,throw new IOException("FAILED: Operation cancelled");
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1957,public int close() {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1959,try {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1960,releaseResources();
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1962,LOG.info("Exception while releasing resources", e);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1964,if (fetchTask != null) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1965,try {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1966,fetchTask.clearFetch();
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1968,LOG.debug(" Exception while clearing the Fetch task ", e);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1970,fetchTask = null;
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1980,if (null != resStream) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1982,((FSDataInputStream) resStream).close();
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1984,LOG.debug(" Exception while closing the resStream ", e);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1988,console.printError("FAILED: Hive Internal Error: " + Utilities.getNameMessage(e) + "\n"
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1989,+ org.apache.hadoop.util.StringUtils.stringifyException(e));
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1990,return 13;
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1997,if (destroyed) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1998,return;
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,2000,destroyed = true;
ql/src/java/org/apache/hadoop/hive/ql/processors/SetProcessor.java,249,ss.out.println(SYSTEM_PREFIX + propName + "=" + result);
ql/src/java/org/apache/hadoop/hive/ql/processors/SetProcessor.java,258,ss.out.println(ENV_PREFIX + var + "=" + System.getenv(var));
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,2942,int sequenceNumber = 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,2969,hiveScratchDir, alias, sequenceNumber++);
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,2985,path = createDummyFileForEmptyTable(job, work, hiveScratchDir,
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,2986,alias, sequenceNumber++);
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,3000,String newDir = hiveScratchDir + Path.SEPARATOR + sequenceNumber;
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,3045,sequenceNumber, props, oneRow);
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,3083,sequenceNumber, props, false);
beeline/src/java/org/apache/hive/beeline/Commands.java,743,while (rows.hasNext()) {
beeline/src/java/org/apache/hive/beeline/Commands.java,744,Rows.Row row = (Rows.Row) rows.next();
beeline/src/java/org/apache/hive/beeline/Commands.java,745,if (!row.isMeta) {
beeline/src/java/org/apache/hive/beeline/Commands.java,746,result.put(row.values[0], row.values[1]);
beeline/src/java/org/apache/hive/beeline/Commands.java,785,boolean hasResults;
beeline/src/java/org/apache/hive/beeline/Commands.java,786,if (call) {
beeline/src/java/org/apache/hive/beeline/Commands.java,787,stmnt = beeLine.getDatabaseConnection().getConnection().prepareCall("set");
beeline/src/java/org/apache/hive/beeline/Commands.java,788,hasResults = ((CallableStatement) stmnt).execute();
beeline/src/java/org/apache/hive/beeline/Commands.java,790,stmnt = beeLine.createStatement();
beeline/src/java/org/apache/hive/beeline/Commands.java,791,hasResults = stmnt.execute("set");
beeline/src/java/org/apache/hive/beeline/Commands.java,824,hiveConf.set(kv[0], kv[1]);
beeline/src/java/org/apache/hive/beeline/Commands.java,1089,line = substituteVariables(getHiveConf(false), line.trim());
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,357,public void closeSession(SessionHandle sessionHandle) throws HiveSQLException {
service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java,117,private static final AtomicInteger sessionCount = new AtomicInteger();
service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java,320,LOG.info("Opened a session, current sessions: " + sessionCount.incrementAndGet());
service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java,458,LOG.info("Closed a session, current sessions: " + sessionCount.decrementAndGet());
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/DbNotificationListener.java,255,rs.addNotificationEvent(event);
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/DbNotificationListener.java,277,rs.cleanNotificationEvents(ttl);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,276,private final void logAuditEvent(String cmd) {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,299,String getIPAddress() {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,733,private void logInfo(String m) {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,806,logInfo("Metastore shutdown started...");
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,807,RawStore ms = threadLocalMS.get();
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,808,if (ms != null) {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,809,try {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,810,ms.shutdown();
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,812,threadLocalMS.remove();
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,815,logInfo("Metastore shutdown complete.");
hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHBaseTableInputFormat.java,224,String colType = jobConf.get(serdeConstants.LIST_COLUMN_TYPES).split(",")[iKey];
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,5167,int inputField = reduceKeys.size();
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,388,Counters ctrs = th.getCounters();
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,406,if (ctrs != null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,407,Counter counterCpuMsec = ctrs.findCounter("org.apache.hadoop.mapred.Task$Counter",
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,409,if (counterCpuMsec != null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,410,long newCpuMSec = counterCpuMsec.getValue();
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,411,if (newCpuMSec > cpuMsec) {
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,412,cpuMsec = newCpuMSec;
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingHMSHandler.java,205,throw new MetaException(ExceptionUtils.getStackTrace(caughtException));
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,3698,setupHiddenSet();
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,4029,private void setupHiddenSet() {
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,4030,String hiddenListStr = this.getVar(ConfVars.HIVE_CONF_HIDDEN_LIST);
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,4031,hiddenSet.clear();
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,4032,if (hiddenListStr != null) {
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,4033,for (String entry : hiddenListStr.split(",")) {
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,4034,hiddenSet.add(entry.trim());
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,4043,for (String name : hiddenSet) {
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,4044,if (conf.get(name) != null) {
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,4045,conf.set(name, "");
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/tool/TempletonUtils.java,486,sb.append(propKey).append('=').append(map.get(propKey)).append('\n');
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,386,if (shouldRunAsync()) {
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,387,Future<?> backgroundHandle = getBackgroundHandle();
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,388,if (backgroundHandle != null) {
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,389,backgroundHandle.cancel(true);
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,1762,metaStoreClient.addDynamicPartitions(txnId, tbl.getDbName(), tbl.getTableName(),
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java,312,LOG.debug("Setting env: " + env[pos-1]);
ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveSparkClientFactory.java,101,SPARK_DEFAULT_CONF_FILE, propertyName, value));
ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveSparkClientFactory.java,138,propertyName, value));
ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveSparkClientFactory.java,148,sparkMaster, propertyName, value));
ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveSparkClientFactory.java,169,propertyName, value));
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,4422,startFunction("get_column_statistics_by_table: db=" + dbName + " table=" + tableName +
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,4433,endFunction("get_column_statistics_by_table: ", statsObj != null, null, tableName);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,4442,startFunction("get_table_statistics_req: db=" + dbName + " table=" + tblName);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,4453,endFunction("get_table_statistics_req: ", result == null, null, tblName);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,4466,startFunction("get_column_statistics_by_partition: db=" + dbName + " table=" + tableName +
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,4479,endFunction("get_column_statistics_by_partition: ", statsObj != null, null, tableName);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,4489,startFunction("get_partitions_statistics_req: db=" + dbName + " table=" + tblName);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,4510,endFunction("get_partitions_statistics_req: ", result == null, null, tblName);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,4537,startFunction("write_column_statistics:  db=" + dbName + " table=" + tableName +
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,4550,endFunction("write_column_statistics: ", ret != false, null, tableName);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,4578,startFunction("write_partition_column_statistics:  db=" + dbName + " table=" + tableName +
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,4595,endFunction("write_partition_column_statistics: ", ret != false, null, tableName);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,4617,startFunction("delete_column_statistics_by_partition: db=" + dbName + " table=" + tableName +
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,4626,endFunction("delete_column_statistics_by_partition: ", ret != false, null, tableName);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,4641,startFunction("delete_column_statistics_by_table: db=" + dbName + " table=" + tableName +
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,4648,endFunction("delete_column_statistics_by_table: ", ret != false, null, tableName);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5996,startFunction("get_aggr_stats_for: db=" + request.getDbName() + " table=" + request.getTblName());
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,6013,endFunction("get_partitions_statistics_req: ", aggrStats == null, null, request.getTblName());
llap-common/src/java/org/apache/hadoop/hive/llap/security/SecretManager.java,117,public synchronized DelegationKey getCurrentKey() {
llap-common/src/java/org/apache/hadoop/hive/llap/security/SecretManager.java,118,return allKeys.get(getCurrentKeyId());
llap-common/src/java/org/apache/hadoop/hive/llap/security/SigningSecretManager.java,23,DelegationKey getCurrentKey();
service/src/java/org/apache/hive/service/cli/session/SessionManager.java,367,&& (!hiveServer2.isRegisteredWithZooKeeper())) {
service/src/java/org/apache/hive/service/server/HiveServer2.java,95,private boolean registeredWithZooKeeper = false;
service/src/java/org/apache/hive/service/server/HiveServer2.java,312,setRegisteredWithZooKeeper(true);
service/src/java/org/apache/hive/service/server/HiveServer2.java,403,HiveServer2.this.setRegisteredWithZooKeeper(false);
service/src/java/org/apache/hive/service/server/HiveServer2.java,418,setRegisteredWithZooKeeper(false);
service/src/java/org/apache/hive/service/server/HiveServer2.java,426,public boolean isRegisteredWithZooKeeper() {
service/src/java/org/apache/hive/service/server/HiveServer2.java,427,return registeredWithZooKeeper;
service/src/java/org/apache/hive/service/server/HiveServer2.java,430,private void setRegisteredWithZooKeeper(boolean registeredWithZooKeeper) {
service/src/java/org/apache/hive/service/server/HiveServer2.java,431,this.registeredWithZooKeeper = registeredWithZooKeeper;
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,764,Entry<String, NodeInfo>[] all = instanceToNodeMap.entrySet().toArray(new Entry[0]);
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,766,if (all.length > 0) {
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,767,int n = random.nextInt(all.length);
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,769,for (int i = 0; i < all.length; i++) {
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,770,Entry<String, NodeInfo> inst = all[(i + n) % all.length];
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,771,if (inst.getValue().canAcceptTask()) {
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,772,LOG.info(
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,775,((requestedHosts == null || requestedHosts.length == 0) ? "null" :
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,776,Arrays.toString(requestedHosts)));
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,777,return new SelectHostResult(inst.getValue().getServiceInstance(), inst.getValue());
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,781,return SELECT_HOST_RESULT_DELAYED_RESOURCES;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdPredicates.java,21,import java.util.LinkedHashMap;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdPredicates.java,141,Map<String, RexNode> finalPreds = new LinkedHashMap<>();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdPredicates.java,142,Map<String, RexNode> finalResidualPreds = new LinkedHashMap<>();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdPredicates.java,149,Map<String, RexNode> preds = new LinkedHashMap<>();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdPredicates.java,159,finalResidualPreds.put(predString, pred);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdPredicates.java,165,finalResidualPreds.put(e.getKey(), e.getValue());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdPredicates.java,172,RexNode disjPred = RexUtil.composeDisjunction(rB, finalResidualPreds.values(), true);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdPredicates.java,173,if (disjPred != null) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,188,PART_COL, TRUE, FALSE, CONSTANT, UNKNOWN, DIVIDED
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,258,assert (nodeOutputs.length == 1);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,259,NodeInfoWrapper wrapper = (NodeInfoWrapper) nodeOutputs[0];
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,260,if (wrapper.state == WalkState.TRUE) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,261,ExprNodeConstantDesc falseDesc = new ExprNodeConstantDesc(
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,262,wrapper.outExpr.getTypeInfo(), Boolean.FALSE);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,263,return new NodeInfoWrapper(WalkState.FALSE, null, falseDesc);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,265,ExprNodeConstantDesc trueDesc = new ExprNodeConstantDesc(
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,266,wrapper.outExpr.getTypeInfo(), Boolean.TRUE);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,267,return new NodeInfoWrapper(WalkState.TRUE, null, trueDesc);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,271,results[i] = opNot(wrapper.ResultVector[i]);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,273,return new NodeInfoWrapper(WalkState.DIVIDED, results,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,274,getOutExpr(fd, nodeOutputs));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,276,return new NodeInfoWrapper(wrapper.state, null,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,277,getOutExpr(fd, nodeOutputs));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,280,boolean anyUnknown = false; // Whether any of the node outputs is unknown
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,281,boolean allDivided = true; // Whether all of the node outputs are divided
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,282,List<NodeInfoWrapper> newNodeOutputsList =
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,283,new ArrayList<NodeInfoWrapper>(nodeOutputs.length);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,284,for (int i = 0; i < nodeOutputs.length; i++) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,285,NodeInfoWrapper c = (NodeInfoWrapper)nodeOutputs[i];
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,286,if (c.state == WalkState.FALSE) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,287,return c;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,289,if (c.state == WalkState.UNKNOWN) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,290,anyUnknown = true;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,292,if (c.state != WalkState.DIVIDED) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,293,allDivided = false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,295,if (c.state != WalkState.TRUE) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,296,newNodeOutputsList.add(c);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,300,if (newNodeOutputsList.size() == 0) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,301,return new NodeInfoWrapper(WalkState.TRUE, null,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,302,new ExprNodeConstantDesc(fd.getTypeInfo(), Boolean.TRUE));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,305,if (newNodeOutputsList.size() == 1) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,306,return newNodeOutputsList.get(0);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,308,Object[] newNodeOutputs = newNodeOutputsList.toArray();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,309,if (anyUnknown) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,310,return new NodeInfoWrapper(WalkState.UNKNOWN, null, getOutExpr(fd, newNodeOutputs));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,312,if (allDivided) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,313,Boolean[] results = new Boolean[ctx.getPartList().size()];
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,314,for (int i = 0; i < ctx.getPartList().size(); i++) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,315,Boolean[] andArray = new Boolean[newNodeOutputs.length];
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,316,for (int j = 0; j < newNodeOutputs.length; j++) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,317,andArray[j] = ((NodeInfoWrapper) newNodeOutputs[j]).ResultVector[i];
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,319,results[i] = opAnd(andArray);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,321,return getResultWrapFromResults(results, fd, newNodeOutputs);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,325,boolean anyUnknown = false; // Whether any of the node outputs is unknown
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,326,boolean allDivided = true; // Whether all of the node outputs are divided
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,327,List<NodeInfoWrapper> newNodeOutputsList =
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,328,new ArrayList<NodeInfoWrapper>(nodeOutputs.length);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,329,for (int i = 0; i< nodeOutputs.length; i++) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,330,NodeInfoWrapper c = (NodeInfoWrapper)nodeOutputs[i];
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,331,if (c.state == WalkState.TRUE) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,332,return c;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,334,if (c.state == WalkState.UNKNOWN) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,335,anyUnknown = true;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,337,if (c.state != WalkState.DIVIDED) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,338,allDivided = false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,340,if (c.state != WalkState.FALSE) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,341,newNodeOutputsList.add(c);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,345,if (newNodeOutputsList.size() == 0) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,346,return new NodeInfoWrapper(WalkState.FALSE, null,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,347,new ExprNodeConstantDesc(fd.getTypeInfo(), Boolean.FALSE));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,350,if (newNodeOutputsList.size() == 1) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,351,return newNodeOutputsList.get(0);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,353,Object[] newNodeOutputs = newNodeOutputsList.toArray();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,354,if (anyUnknown) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,355,return new NodeInfoWrapper(WalkState.UNKNOWN, null, getOutExpr(fd, newNodeOutputs));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,357,if (allDivided) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,358,Boolean[] results = new Boolean[ctx.getPartList().size()];
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,359,for (int i = 0; i < ctx.getPartList().size(); i++) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,360,Boolean[] orArray = new Boolean[newNodeOutputs.length];
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,361,for (int j = 0; j < newNodeOutputs.length; j++) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,362,orArray[j] = ((NodeInfoWrapper) newNodeOutputs[j]).ResultVector[i];
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,364,results[i] = opOr(orArray);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,366,return getResultWrapFromResults(results, fd, newNodeOutputs);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,370,List<ExprNodeDesc> children = fd.getChildren();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,371,boolean removePredElem = false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,372,ExprNodeDesc lhs = children.get(0);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,374,if (lhs instanceof ExprNodeColumnDesc) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,376,if (((ExprNodeColumnDesc)lhs).getIsPartitionColOrVirtualCol()) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,378,removePredElem = true;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,380,if (removePredElem) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,382,for (int i = 1; i < children.size(); i++) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,383,if (children.get(i) instanceof ExprNodeDynamicListDesc) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,384,removePredElem = false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,385,break;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,393,.getGenericUDF())) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,394,boolean hasOnlyPartCols = true;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,395,boolean hasDynamicListDesc = false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,397,for (ExprNodeDesc ed : ((ExprNodeGenericFuncDesc) lhs).getChildren()) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,401,if (!(ed instanceof ExprNodeColumnDesc &&
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,402,((ExprNodeColumnDesc)ed).getIsPartitionColOrVirtualCol())) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,403,hasOnlyPartCols = false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,404,break;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,409,if (hasOnlyPartCols) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,411,for (int i = 1; i < children.size(); i++) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,412,if (children.get(i) instanceof ExprNodeDynamicListDesc) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,413,hasDynamicListDesc = true;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,414,break;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,419,removePredElem = hasOnlyPartCols && !hasDynamicListDesc;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,425,return removePredElem ?
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,426,new NodeInfoWrapper(WalkState.TRUE, null,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,427,new ExprNodeConstantDesc(fd.getTypeInfo(), Boolean.TRUE)) :
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,428,new NodeInfoWrapper(WalkState.UNKNOWN, null, getOutExpr(fd, nodeOutputs)) ;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,431,return new NodeInfoWrapper(WalkState.UNKNOWN, null,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,435,boolean has_part_col = false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,436,for (Object child : nodeOutputs) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,437,NodeInfoWrapper wrapper = (NodeInfoWrapper) child;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,438,if (wrapper.state == WalkState.UNKNOWN) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,439,return new NodeInfoWrapper(WalkState.UNKNOWN, null, getOutExpr(fd, nodeOutputs));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,441,has_part_col = true;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,445,if (has_part_col && fd.getTypeInfo().getCategory() == Category.PRIMITIVE) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,447,if (fd.getTypeInfo().equals(TypeInfoFactory.booleanTypeInfo)) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,450,Boolean[] results = new Boolean[ctx.getPartList().size()];
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,451,for (int i = 0; i < ctx.getPartList().size(); i++) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,452,results[i] = (Boolean) evalExprWithPart(fd, ctx.getPartList().get(i),
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,453,ctx.getVirtualColumns());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,455,return getResultWrapFromResults(results, fd, nodeOutputs);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,461,Object[] results = new Object[ctx.getPartList().size()];
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,462,for (int i = 0; i < ctx.getPartList().size(); i++) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,463,results[i] = evalExprWithPart(fd, ctx.getPartList().get(i), ctx.getVirtualColumns());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,465,Object result = ifResultsAgree(results);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,466,if (result == null) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,472,return new NodeInfoWrapper(WalkState.UNKNOWN, null, getOutExpr(fd, nodeOutputs));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,474,return new NodeInfoWrapper(WalkState.CONSTANT, null,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,475,new ExprNodeConstantDesc(fd.getTypeInfo(), result));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,479,final ExprNodeGenericFuncDesc desc = getOutExpr(fd, nodeOutputs);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,480,final ExprNodeDesc foldedDesc = ConstantPropagateProcFactory.foldExpr(desc);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,481,if (foldedDesc != null && foldedDesc instanceof ExprNodeConstantDesc) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,482,ExprNodeConstantDesc constant = (ExprNodeConstantDesc) foldedDesc;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,483,if (Boolean.TRUE.equals(constant.getValue())) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,484,return new NodeInfoWrapper(WalkState.TRUE, null, constant);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,486,return new NodeInfoWrapper(WalkState.FALSE, null, constant);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,488,return new NodeInfoWrapper(WalkState.CONSTANT, null, constant);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/pcr/PcrExprProcFactory.java,491,return new NodeInfoWrapper(WalkState.CONSTANT, null, desc);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,3357,private String addForUpdateClause(String selectStatement) throws MetaException {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,3373,return selectStatement + " with(updlock)";
ql/src/java/org/apache/hadoop/hive/ql/debug/Utils.java,29,import com.sun.management.HotSpotDiagnosticMXBean;
ql/src/java/org/apache/hadoop/hive/ql/debug/Utils.java,36,private static volatile HotSpotDiagnosticMXBean hotspotMBean;
ql/src/java/org/apache/hadoop/hive/ql/debug/Utils.java,65,HOTSPOT_BEAN_NAME, HotSpotDiagnosticMXBean.class);
ql/src/java/org/apache/hadoop/hive/ql/debug/Utils.java,67,throw re;
ql/src/java/org/apache/hadoop/hive/ql/debug/Utils.java,69,throw new RuntimeException(exp);
ql/src/java/org/apache/hadoop/hive/ql/debug/Utils.java,72,try {
ql/src/java/org/apache/hadoop/hive/ql/debug/Utils.java,73,hotspotMBean.dumpHeap(fileName, live);
ql/src/java/org/apache/hadoop/hive/ql/processors/ResetProcessor.java,40,if(authErrResp != null){
ql/src/java/org/apache/hadoop/hive/ql/processors/ResetProcessor.java,45,if (ss.getOverriddenConfigurations().isEmpty()) {
ql/src/java/org/apache/hadoop/hive/ql/processors/ResetProcessor.java,50,String value = conf.get(key);
ql/src/java/org/apache/hadoop/hive/ql/processors/ResetProcessor.java,51,if (value != null) {
ql/src/java/org/apache/hadoop/hive/ql/processors/ResetProcessor.java,52,ss.getConf().set(key, value);
ql/src/java/org/apache/hadoop/hive/ql/processors/ResetProcessor.java,56,return new CommandProcessorResponse(0);
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,2623,new StringSet("cache", "allocator", "none"),
llap-server/src/java/org/apache/hadoop/hive/llap/cache/EvictionDispatcher.java,29,private final LowLevelCacheImpl dataCache;
llap-server/src/java/org/apache/hadoop/hive/llap/cache/EvictionDispatcher.java,32,public EvictionDispatcher(LowLevelCacheImpl dataCache, OrcMetadataCache metadataCache) {
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java,70,public void init() {
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java,371,public final void notifyEvicted(LlapDataBuffer buffer) {
llap-server/src/java/org/apache/hadoop/hive/llap/cache/SimpleBufferManager.java,24,import org.apache.hadoop.hive.llap.DebugUtils;
llap-server/src/java/org/apache/hadoop/hive/llap/cache/SimpleBufferManager.java,28,public class SimpleBufferManager implements BufferUsageManager {
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java,69,private static final String MODE_CACHE = "cache", MODE_ALLOCATOR = "allocator";
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java,80,boolean useLowLevelCache = LlapIoImpl.MODE_CACHE.equalsIgnoreCase(ioMode),
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java,81,useAllocOnly = !useLowLevelCache && LlapIoImpl.MODE_ALLOCATOR.equalsIgnoreCase(ioMode);
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java,82,LOG.info("Initializing LLAP IO in {} mode", ioMode);
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java,107,LowLevelCacheImpl orcCache = null;
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java,120,orcCache = new LowLevelCacheImpl(cacheMetrics, cachePolicy, allocator, true);
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java,124,cachePolicy.setEvictionListener(new EvictionDispatcher(orcCache, metadataCache));
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java,125,cachePolicy.setParentDebugDumper(orcCache);
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java,126,orcCache.init(); // Start the cache threads.
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java,127,bufferManager = orcCache; // Cache also serves as buffer manager.
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java,129,if (useAllocOnly) {
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java,130,LowLevelCacheMemoryManager memManager = new LowLevelCacheMemoryManager(
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java,131,conf, null, cacheMetrics);
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java,132,allocator = new BuddyAllocator(conf, memManager, cacheMetrics);
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java,134,allocator = new SimpleAllocator(conf);
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java,136,bufferManager = new SimpleBufferManager(allocator, cacheMetrics);
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java,144,metadataCache, orcCache, bufferManager, conf, cacheMetrics, ioMetrics);
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcColumnVectorProducer.java,27,import org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl;
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcColumnVectorProducer.java,50,LowLevelCacheImpl lowLevelCache, BufferUsageManager bufferManager,
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,822,DiskRangeList result = (lowLevelCache == null) ? range
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,823,: lowLevelCache.getFileData(fileKey, range, baseOffset, factory, counters, gotAllData);
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,833,return (lowLevelCache == null) ? null : lowLevelCache.putFileData(
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java,113,private transient final BloomFilter bloom1;
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java,284,long estimatedTableSize, long keyCount, long memoryAvailable, HybridHashTableConf nwayConf,
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java,334,this.bloom1 = new BloomFilter(newKeyCount);
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java,336,if (LOG.isInfoEnabled()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java,338,newKeyCount, bloom1.sizeInBytes()));
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java,339,LOG.info("Write buffer size: " + writeBufferSize);
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java,476,bloom1.addLong(keyHash);
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java,902,if (!bloom1.testLong(keyHash)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java,1056,if (!bloom1.testLong(keyHash)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java,385,public boolean isEof() {
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java,387,if (!hasRows) {
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java,388,return true;
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java,391,if (!hasList) {
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java,392,return (readIndex > 0);
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java,395,if (readIndex <= 1) {
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java,398,return false;
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java,400,return (nextTailOffset <= 0);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordSource.java,416,keyBinarySortableDeserializeToRow.deserialize(batch, 0);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,653,int i = 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,654,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,655,while (i < count) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,656,if (isConvert[i]) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,657,deserializeConvertRowColumn(batch, batchIndex, i);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,659,deserializeRowColumn(batch, batchIndex, i);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,661,i++;    // Increment after the apply which could throw an exception.
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,664,throwMoreDetailedException(e, i);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,669,private void throwMoreDetailedException(IOException e, int index) throws EOFException {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,670,StringBuilder sb = new StringBuilder();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,671,sb.append("Detail: \"" + e.toString() + "\" occured for field " + index + " of " +  sourceTypeInfos.length + " fields (");
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,672,for (int i = 0; i < sourceTypeInfos.length; i++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,673,if (i > 0) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,674,sb.append(", ");
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,676,sb.append(((PrimitiveTypeInfo) sourceTypeInfos[i]).getPrimitiveCategory().name());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,678,sb.append(")");
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,679,throw new EOFException(sb.toString());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,814,currentVectorDeserializeRow.deserialize(deserializerBatch, deserializerBatch.size++);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinGenerateResultOperator.java,184,byte[] bytes = byteSegmentRef.getBytes();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinGenerateResultOperator.java,185,int offset = (int) byteSegmentRef.getOffset();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinGenerateResultOperator.java,186,int length = byteSegmentRef.getLength();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinGenerateResultOperator.java,187,smallTableVectorDeserializeRow.setBytes(bytes, offset, length);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinGenerateResultOperator.java,189,smallTableVectorDeserializeRow.deserialize(batch, batchIndex);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinGenerateResultOperator.java,251,byte[] bytes = byteSegmentRef.getBytes();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinGenerateResultOperator.java,252,int offset = (int) byteSegmentRef.getOffset();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinGenerateResultOperator.java,253,int length = byteSegmentRef.getLength();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinGenerateResultOperator.java,254,smallTableVectorDeserializeRow.setBytes(bytes, offset, length);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinGenerateResultOperator.java,256,smallTableVectorDeserializeRow.deserialize(overflowBatch, overflowBatch.size);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinGenerateResultOperator.java,302,byte[] bytes = byteSegmentRef.getBytes();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinGenerateResultOperator.java,303,int offset = (int) byteSegmentRef.getOffset();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinGenerateResultOperator.java,304,int length = byteSegmentRef.getLength();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinGenerateResultOperator.java,305,smallTableVectorDeserializeRow.setBytes(bytes, offset, length);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinGenerateResultOperator.java,351,if (hashMapResult.isEof()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinGenerateResultOperator.java,354,byteSegmentRef = hashMapResult.next();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinGenerateResultOperator.java,548,bigTableVectorDeserializeRow.deserialize(spillReplayBatch, spillReplayBatch.size);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastLongHashTable.java,81,if (keyBinarySortableDeserializeRead.readCheckNull()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastLongHashTable.java,82,return;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastStringCommon.java,48,if (keyBinarySortableDeserializeRead.readCheckNull()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastStringCommon.java,49,return;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastStringCommon.java,62,keyBinarySortableDeserializeRead = new BinarySortableDeserializeRead(primitiveTypeInfos);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastValueStore.java,125,private boolean haveReadCurrent;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastValueStore.java,127,private boolean isEof;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastValueStore.java,156,haveReadCurrent = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastValueStore.java,158,isEof = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastValueStore.java,196,haveReadCurrent = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastValueStore.java,198,isEof = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastValueStore.java,366,public boolean isEof() {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastValueStore.java,367,if (!hasRows) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastValueStore.java,368,return true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastValueStore.java,370,return isEof;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/hashtable/VectorMapJoinHashMapResult.java,62,public abstract boolean isEof();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedHashMap.java,91,public boolean isEof() {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedHashMap.java,92,return bytesBytesMultiHashMapResult.isEof();
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,71,private boolean readBeyondBufferRangeWarned;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,95,readBeyondBufferRangeWarned = false;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,110,inputByteBuffer.reset(bytes, offset, offset + length);
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,136,if (!readBeyondBufferRangeWarned) {
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,137,doReadBeyondBufferRangeWarned();
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,301,int start = inputByteBuffer.tell();
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,320,inputByteBuffer.seek(start);
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,395,public boolean readBeyondBufferRangeWarned() {
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,396,return readBeyondBufferRangeWarned;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,414,private void doReadBeyondBufferRangeWarned() {
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,416,int length = inputByteBuffer.tell() - start;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,417,LOG.info("Reading beyond buffer range! Buffer range " +  start
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,418,+ " for length " + length + " but reading more... "
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,419,+ "(total buffer length " + inputByteBuffer.getData().length + ")"
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,420,+ "  Ignoring similar problems.");
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,421,readBeyondBufferRangeWarned = true;
serde/src/java/org/apache/hadoop/hive/serde2/fast/DeserializeRead.java,146,public abstract boolean readBeyondBufferRangeWarned();
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,135,int fieldId = 0;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,143,if (lastColumnTakesRest && fieldId == fieldCount - 1) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,146,startPosition[fieldId] = fieldByteBegin;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,147,fieldId++;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,148,if (fieldId == fieldCount || fieldByteEnd == structByteEnd) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,155,for (int i = fieldId; i <= fieldCount; i++) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,179,if (!missingFieldWarned && fieldId < fieldCount) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,180,doMissingFieldWarned(fieldId);
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,514,public boolean readBeyondBufferRangeWarned() {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,515,return extraFieldWarned;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,519,return false;  // UNDONE: Get rid of...
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,65,private boolean readBeyondBufferRangeWarned;
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,74,readBeyondBufferRangeWarned = false;
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,118,warnBeyondEof();
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,134,warnBeyondEof();
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,152,warnBeyondEof();
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,161,if (offset > end) {
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,162,warnBeyondEof();
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,170,if (offset > end) {
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,171,warnBeyondEof();
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,178,warnBeyondEof();
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,186,warnBeyondEof();
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,201,if (offset > end) {
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,202,warnBeyondEof();
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,209,warnBeyondEof();
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,221,if (offset > end) {
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,222,warnBeyondEof();
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,234,warnBeyondEof();
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,244,if (offset > end) {
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,245,warnBeyondEof();
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,252,if (offset >= end) {
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,254,warnBeyondEof();
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,259,if (offset > end) {
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,260,warnBeyondEof();
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,275,if (offset >= end) {
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,277,warnBeyondEof();
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,281,if (offset >= end) {
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,283,warnBeyondEof();
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,288,warnBeyondEof();
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,330,warnBeyondEof();
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,366,public boolean readBeyondBufferRangeWarned() {
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,367,return readBeyondBufferRangeWarned;
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,374,private void warnBeyondEof() throws EOFException {
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,375,if (!readBeyondBufferRangeWarned) {
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,377,int length = end - start;
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,378,LOG.info("Reading beyond buffer range! Buffer range " +  start
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,379,+ " for length " + length + " but reading more... "
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,380,+ "(total buffer length " + bytes.length + ")"
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,381,+ "  Ignoring similar problems.");
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,382,readBeyondBufferRangeWarned = true;
serde/src/java/org/apache/hadoop/hive/serde2/fast/RandomRowObjectSource.java,19,package org.apache.hadoop.hive.serde2.fast;
serde/src/java/org/apache/hadoop/hive/serde2/fast/RandomRowObjectSource.java,67,public class RandomRowObjectSource {
serde/src/java/org/apache/hadoop/hive/serde2/fast/RandomRowObjectSource.java,242,RandomRowObjectSource.sort(rows, rowStructObjectInspector);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,938,return new ConstantVectorExpression(outCol, ((HiveChar) constantValue));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,940,return new ConstantVectorExpression(outCol, ((HiveVarchar) constantValue));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ConstantVectorExpression.java,75,public ConstantVectorExpression(int outputColumn, HiveChar value) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ConstantVectorExpression.java,76,this(outputColumn, "char");
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ConstantVectorExpression.java,80,public ConstantVectorExpression(int outputColumn, HiveVarchar value) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ConstantVectorExpression.java,81,this(outputColumn, "varchar");
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,369,if (aggr.getParameters().isEmpty() || aggr.getParameters().get(0) instanceof
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,370,ExprNodeConstantDesc || ((aggr.getParameters().get(0) instanceof ExprNodeColumnDesc) &&
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,371,exprMap.get(((ExprNodeColumnDesc)aggr.getParameters().get(0)).getColumn()) instanceof ExprNodeConstantDesc)) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,374,if(rowCnt == null) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,379,ExprNodeColumnDesc desc = (ExprNodeColumnDesc)exprMap.get(((ExprNodeColumnDesc)aggr.getParameters().get(0)).getColumn());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,382,if(!tbl.isPartitioned()) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,398,tbl.getDbName(),tbl.getTableName(), Lists.newArrayList(colName));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,405,Logger.debug("Unsupported type: " + desc.getTypeString() + " encountered in " +
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,412,Set<Partition> parts = pctx.getPrunedPartitions(
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,413,tsOp.getConf().getAlias(), tsOp).getPartitions();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,420,.get(StatsSetupConst.ROW_COUNT));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,427,Collection<List<ColumnStatisticsObj>> result =
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,428,verifyAndGetPartColumnStats(hive, tbl, colName, parts);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,434,if (statData == null) return null;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java,437,Logger.debug("Unsupported type: " + desc.getTypeString() + " encountered in " +
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,133,fileMetadata.getTypes(), stripeMetadata.getEncodings(), batch, codec, skipCorrupt);
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,818,private long base_timestamp;
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,824,this(columnId, null, null, null, null, skipCorrupt);
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,873,private long getBaseTimestamp(String timeZoneId) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,42,protected static class TimestampStreamReader extends TimestampTreeReader
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,50,SettableUncompressedStream data, SettableUncompressedStream nanos, boolean isFileCompressed,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,51,OrcProto.ColumnEncoding encoding, boolean skipCorrupt) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,52,super(columnId, present, data, nanos, encoding, skipCorrupt);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,158,isFileCompressed, columnEncoding, skipCorrupt);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1666,List<OrcProto.Type> types,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1667,List<OrcProto.ColumnEncoding> encodings,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1668,EncodedColumnBatch<OrcBatchKey> batch,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1669,CompressionCodec codec, boolean skipCorrupt) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1094,return ppdResult != null;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1399,final int rootIdx = getRootColumn(isOriginal);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1404,internalColIds.add(rootIdx + i);
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,146,cache = ObjectCacheFactory.getCache(hconf, queryId);
ql/src/java/org/apache/hadoop/hive/ql/exec/ObjectCacheFactory.java,47,public static ObjectCache getCache(Configuration conf, String queryId) {
ql/src/java/org/apache/hadoop/hive/ql/exec/ObjectCacheFactory.java,50,if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_OBJECT_CACHE_ENABLED)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java,56,import org.apache.hadoop.hive.ql.metadata.HiveException;
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java,97,if (LlapProxy.isDaemon()) { // do not cache plan
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java,101,cache = new org.apache.hadoop.hive.ql.exec.mr.ObjectCache();
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java,103,cache = ObjectCacheFactory.getCache(jconf, queryId);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MergeFileRecordProcessor.java,66,ObjectCache cache;
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MergeFileRecordProcessor.java,97,org.apache.hadoop.hive.ql.exec.ObjectCache cache = ObjectCacheFactory
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MergeFileRecordProcessor.java,98,.getCache(jconf, queryId);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordProcessor.java,92,ObjectCache cache;
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordProcessor.java,95,if (LlapProxy.isDaemon()) { // don't cache plan
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordProcessor.java,96,cache = new org.apache.hadoop.hive.ql.exec.mr.ObjectCache();
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordProcessor.java,98,cache = ObjectCacheFactory.getCache(jconf, queryId);
service/src/java/org/apache/hive/service/auth/LdapAuthenticationProviderImpl.java,597,String[] returnAttributes     = new String[0]; //empty set
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2790,if (!ms.dropPartition(db_name, tbl_name, part_vals)) {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2791,throw new MetaException("Unable to drop partition");
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2793,success = ms.commitTransaction();
hcatalog/server-extensions/src/main/java/org/apache/hive/hcatalog/listener/DbNotificationListener.java,279,Thread.sleep(60000);
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java,474,AcidUtils.setTransactionalTableScan(job, ts.getConf().isAcidTable());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinGenerateResultOperator.java,295,while (true) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinGenerateResultOperator.java,307,smallTableVectorDeserializeRow.deserialize(overflowBatch, overflowBatch.DEFAULT_SIZE);
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,96,useLlapIo = ((LlapAwareSplit)split).canUseLlapIo();
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,101,RecordReader<NullWritable, VectorizedRowBatch> rr =
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,102,sourceInputFormat.getRecordReader(split, job, reporter);
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,103,return rr;
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,115,return new LlapRecordReader(job, fileSplit, includedCols, hostName);
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,145,private ConsumerFeedback<ColumnVectorBatch> feedback;
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,149,public LlapRecordReader(
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,151,throws IOException {
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,186,startRead();
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,250,private void startRead() {
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,252,ReadPipeline rp = cvp.createReadPipeline(
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,253,this, split, columnIds, sarg, columnNames, counters);
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,254,feedback = rp;
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,255,ListenableFuture<Void> future = executor.submit(rp.getReadCallable());
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,257,Futures.addCallback(future, new UncaughtErrorHandler());
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/ColumnVectorProducer.java,35,QueryFragmentCounters counters);
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcColumnVectorProducer.java,67,QueryFragmentCounters counters) {
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,165,QueryFragmentCounters counters) {
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,222,orcReader = null;
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,225,fs = split.getPath().getFileSystem(conf);
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,226,fileKey = determineFileId(fs, split,
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,227,HiveConf.getBoolVar(conf, ConfVars.LLAP_CACHE_ALLOW_SYNTHETIC_FILEID));
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,232,fileMetadata = getOrReadFileMetadata();
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,233,consumer.setFileMetadata(fileMetadata);
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,257,boolean[] globalIncludes = null;
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,260,globalIncludes = OrcInputFormat.genIncludedColumns(fileMetadata.getTypes(), columnIds, true);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,2111,int dataColumns
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,2112,) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,2143,throw new IOException(ErrorMsg.SCHEMA_REQUIRED_TO_READ_ACID_TABLES.getErrorCodedMsg());
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,279,LOG.info("QueueName: " + queueName + " nonDefaultUser: " + nonDefaultUser +
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java,452,int hashCode = (keyHashCode == -1) ? writeBuffers.hashCode(keyOffset, keyLength) : keyHashCode;
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java,686,writeBuffers.setReadPoint(getFirstRecordLengthsOffset(ref, null));
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java,687,int valueLength = (int)writeBuffers.readVLong(), keyLength = (int)writeBuffers.readVLong();
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java,766,writeBuffers.setReadPoint(getFirstRecordLengthsOffset(oldRef, null));
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java,769,- writeBuffers.readVLong() - writeBuffers.readVLong() - 4, 4);
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java,788,return writeBuffers.getReadPoint(); // Assumes we are here after key compare.
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java,794,writeBuffers.setReadPoint(firstTailOffset);
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java,795,writeBuffers.skipVLong();
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java,796,writeBuffers.skipVLong();
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java,797,int lengthsLength = (int)(writeBuffers.getReadPoint() - firstTailOffset);
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java,819,long prevHeadOffset = writeBuffers.readNByteLong(lrPtrOffset, 5);
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java,888,writeBuffers.setReadPoint(recOffset);
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java,889,int valueLength = (int)writeBuffers.readVLong(),
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java,890,keyLength = (int)writeBuffers.readVLong();
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java,891,long ptrOffset = writeBuffers.getReadPoint();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapJoinOperator.java,157,int columnIndex;;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedBatchUtil.java,708,if (p == column) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedBatchUtil.java,709,sb.append("(col " + p + ") ");
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedBatchUtil.java,711,sb.append("(proj col " + p + " col " + column + ") ");
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedBatchUtil.java,755,LOG.info(sb.toString());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastBytesHashMap.java,84,long valueRefWord = findReadSlot(keyBytes, keyStart, keyLength, hashCode);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastBytesHashMultiSet.java,78,long count = findReadSlot(keyBytes, keyStart, keyLength, hashCode);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastBytesHashSet.java,68,long existance = findReadSlot(keyBytes, keyStart, keyLength, hashCode);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastBytesHashTable.java,78,keyStore.equalKey(slotTriples[tripleIndex], keyBytes, keyStart, keyLength)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastBytesHashTable.java,167,protected long findReadSlot(byte[] keyBytes, int keyStart, int keyLength, long hashCode) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastBytesHashTable.java,179,if (keyStore.equalKey(slotTriples[tripleIndex], keyBytes, keyStart, keyLength)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastHashTable.java,32,protected int writeBuffersSize;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastKeyStore.java,33,private WriteBuffers.Position readPos;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastKeyStore.java,118,public boolean equalKey(long keyRefWord, byte[] keyBytes, int keyStart, int keyLength) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastKeyStore.java,155,readPos = new WriteBuffers.Position();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastKeyStore.java,162,readPos = new WriteBuffers.Position();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastLongHashTable.java,51,private HashTableKeyType hashTableKeyType;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastLongHashTable.java,53,private boolean isOuterJoin;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastLongHashTable.java,55,private BinarySortableDeserializeRead keyBinarySortableDeserializeRead;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastLongHashTable.java,57,private boolean useMinMax;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastTableContainer.java,61,private final VectorMapJoinFastHashTable VectorMapJoinFastHashTable;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastTableContainer.java,86,VectorMapJoinFastHashTable = createHashTable(newThreshold);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastTableContainer.java,91,return VectorMapJoinFastHashTable;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastTableContainer.java,181,VectorMapJoinFastHashTable.putRow((BytesWritable) currentKey, (BytesWritable) currentValue);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastTableContainer.java,217,return VectorMapJoinFastHashTable.size();
serde/src/java/org/apache/hadoop/hive/serde2/WriteBuffers.java,24,import org.apache.hadoop.hive.serde2.ByteStream.Output;
serde/src/java/org/apache/hadoop/hive/serde2/WriteBuffers.java,26,import org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe;
serde/src/java/org/apache/hadoop/hive/serde2/WriteBuffers.java,28,import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory;
serde/src/java/org/apache/hadoop/hive/serde2/WriteBuffers.java,56,Position defaultReadPos = new Position(); // Position where we'd read (by default).
serde/src/java/org/apache/hadoop/hive/serde2/WriteBuffers.java,67,public int readVInt() {
serde/src/java/org/apache/hadoop/hive/serde2/WriteBuffers.java,68,return (int) readVLong(defaultReadPos);
serde/src/java/org/apache/hadoop/hive/serde2/WriteBuffers.java,75,public long readVLong() {
serde/src/java/org/apache/hadoop/hive/serde2/WriteBuffers.java,76,return readVLong(defaultReadPos);
serde/src/java/org/apache/hadoop/hive/serde2/WriteBuffers.java,100,public void skipVLong() {
serde/src/java/org/apache/hadoop/hive/serde2/WriteBuffers.java,101,skipVLong(defaultReadPos);
serde/src/java/org/apache/hadoop/hive/serde2/WriteBuffers.java,120,public void setReadPoint(long offset) {
serde/src/java/org/apache/hadoop/hive/serde2/WriteBuffers.java,121,setReadPoint(offset, defaultReadPos);
serde/src/java/org/apache/hadoop/hive/serde2/WriteBuffers.java,130,public int hashCode(long offset, int length) {
serde/src/java/org/apache/hadoop/hive/serde2/WriteBuffers.java,131,return hashCode(offset, length, defaultReadPos);
serde/src/java/org/apache/hadoop/hive/serde2/WriteBuffers.java,355,defaultReadPos.clear();
serde/src/java/org/apache/hadoop/hive/serde2/WriteBuffers.java,366,public long getReadPoint() {
serde/src/java/org/apache/hadoop/hive/serde2/WriteBuffers.java,367,return getReadPoint(defaultReadPos);
serde/src/java/org/apache/hadoop/hive/serde2/WriteBuffers.java,521,public long readNByteLong(long offset, int bytes) {
serde/src/java/org/apache/hadoop/hive/serde2/WriteBuffers.java,522,return readNByteLong(offset, bytes, defaultReadPos);
serde/src/java/org/apache/hadoop/hive/serde2/WriteBuffers.java,564,return (int)readNByteLong(offset, 4);
serde/src/java/org/apache/hadoop/hive/serde2/WriteBuffers.java,609,public Position getReadPosition() {
serde/src/java/org/apache/hadoop/hive/serde2/WriteBuffers.java,610,return defaultReadPos;
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,449,LOG.info("Found instance " + inst);
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,453,LOG.info("Ignoring dead instance " + inst);
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,748,LOG.debug("Skipping non-local location allocation for [" + request.task +
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,756,LOG.debug("Not skipping non-local location allocation for [" + request.task +
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,970,if (LOG.isDebugEnabled()) {
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,971,LOG.debug("ScheduleResult for Task: {} = {}", taskInfo, scheduleResult);
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,1017,LOG.info("Attempting to preempt for {}, pendingPreemptions={} on hosts={}",
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,1018,taskInfo.task, pendingPreemptions.get(), Arrays.toString(potentialHosts));
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,1025,LOG.info("Attempting to preempt for {}, pendingPreemptions={} on any host",
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,1026,taskInfo.task, pendingPreemptions.get());
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,1065,LOG.info("Assigned task {} to container {}", taskInfo, container.getId());
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,1153,val = new MutableInt(1);
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,1420,if (LOG.isDebugEnabled()) {
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,1421,LOG.debug("Setting up node: " + serviceInstance + ", with available capacity=" +
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,1422,serviceInstance.getResource().getVirtualCores() + ", pendingQueueCapacity=" +
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,1423,pendingQueueCapacityString);
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,1436,LOG.info("Setting up node: " + serviceInstance + " with schedulableCapacity=" + this.numSchedulableTasks);
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,1469,LOG.info("Disabling instance " + serviceInstance + " for " + delayTime + " milli-seconds");
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,1573,getMSC().add_partition(newTPart.getTPartition());
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,1575,EnvironmentContext environmentContext = null;
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,1576,if (hasFollowingStatsTask) {
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,1577,environmentContext = new EnvironmentContext();
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,1578,environmentContext.putToProperties(StatsSetupConst.DO_NOT_UPDATE_STATS, StatsSetupConst.TRUE);
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,1580,alterPartition(tbl.getDbName(), tbl.getTableName(), new Partition(tbl, newTPart.getTPartition()), environmentContext);
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java,174,assert !currentNotCached.hasData();
llap-server/src/java/org/apache/hadoop/hive/llap/io/metadata/OrcFileEstimateErrors.java,80,check.removeSelf();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,1126,private static class IncompleteCb extends DiskRangeList {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,1127,public IncompleteCb(long offset, long end) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,1128,super(offset, end);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,1132,public String toString() {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,1133,return "incomplete CB start: " + offset + " end: " + end;
ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/WindowingTableFunction.java,1569,out = ObjectInspectorUtils.copyToStandardObject(out, wFn.getOI());
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java,341,new ObjectInspector[] { deserializer.getObjectInspector() }, mapOp
ql/src/java/org/apache/hadoop/hive/ql/QueryDisplay.java,68,private Integer returnVal;  //if set, determines that task is complete.
ql/src/java/org/apache/hadoop/hive/ql/QueryDisplay.java,98,if (returnVal == null) {
ql/src/java/org/apache/hadoop/hive/ql/QueryDisplay.java,103,return "Failure, ReturnVal " + String.valueOf(returnVal);
ql/src/java/org/apache/hadoop/hive/ql/QueryDisplay.java,119,return returnVal;
ql/src/java/org/apache/hadoop/hive/ql/QueryDisplay.java,189,taskDisplay.returnVal = result.getExitVal();
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,764,outputs.add(new WriteEntity(sourceTable, WriteType.DDL_SHARED));
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzerFactory.java,64,commandType.put(HiveParser.TOK_ALTERTABLE_EXCHANGEPARTITION, HiveOperation.ALTERTABLE_EXCHANGEPARTITION);
ql/src/java/org/apache/hadoop/hive/ql/plan/HiveOperation.java,117,ALTERTABLE_EXCHANGEPARTITION("ALTERTABLE_EXCHANGEPARTITION", null, null),
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/Operation2Privilege.java,429,op2Priv.put(HiveOperationType.ALTERTABLE_EXCHANGEPARTITION,
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/Operation2Privilege.java,430,PrivRequirement.newIOPrivRequirement(null, null));
common/src/java/org/apache/hadoop/hive/common/ValidReadTxnList.java,34,this(new long[0], Long.MAX_VALUE);
common/src/java/org/apache/hadoop/hive/common/ValidReadTxnList.java,114,exceptions = new long[values.length - 1];
common/src/java/org/apache/hadoop/hive/common/ValidReadTxnList.java,115,for(int i = 1; i < values.length; ++i) {
common/src/java/org/apache/hadoop/hive/common/ValidReadTxnList.java,116,exceptions[i-1] = Long.parseLong(values[i]);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/GetOpenTxnsResponse.java,56,OPEN_TXNS((short)2, "open_txns");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,357,s = "select txn_id from TXNS where txn_id <= " + hwm;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,361,openList.add(rs.getLong(1));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,365,return new GetOpenTxnsResponse(hwm, openList);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnUtils.java,59,return new ValidReadTxnList(exceptions, highWater);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnUtils.java,79,if (txn.getState() == TxnState.OPEN) minOpenTxn = Math.min(minOpenTxn, txn.getId());
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnUtils.java,80,exceptions[i++] = txn.getId();//todo: only add Aborted
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnUtils.java,83,return new ValidCompactorTxnList(exceptions, -1, highWater);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,592,String value = conf.get(ValidTxnList.VALID_TXNS_KEY,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,593,Long.MAX_VALUE + ":");
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,594,transactionList = new ValidReadTxnList(value);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1767,String txnString = conf.get(ValidTxnList.VALID_TXNS_KEY,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1768,Long.MAX_VALUE + ":");
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1769,ValidTxnList validTxnList = new ValidReadTxnList(txnString);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java,288,.set(0);
spark-client/src/main/java/org/apache/hive/spark/client/SparkClientImpl.java,329,if (hiveConf.getVar(HiveConf.ConfVars.HIVE_SERVER2_AUTHENTICATION).equalsIgnoreCase("kerberos")) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FilterStringColLikeStringScalar.java,144,return new ComplexChecker(UDFLike.likePatternToRegExp(pattern));
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,29,import org.apache.hadoop.hive.llap.metrics.LlapDaemonIOMetrics;
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,700,rowIndexStride, types, globalIncludes.length);
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,2738,throws IOException {
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,47,import org.apache.hadoop.conf.Configuration;
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,48,import org.apache.hadoop.fs.FileSystem;
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,162,sargApp = new SargApplier(
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,163,sarg, options.getColumnNames(), rowIndexStride, types,
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,164,included.length);
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,710,List<OrcProto.Type> types, int includedCount) {
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,752,if (bloomFilterIndices != null && bloomFilterIndices[filterColumns[pred]] != null) {
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,753,bf = bloomFilterIndices[filterColumns[pred]].getBloomFilter(rowGroup);
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,755,leafValues[pred] = evaluatePredicateProto(stats, sargLeaves.get(pred), bf);
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,21,import java.io.IOException;
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,27,import org.apache.commons.logging.Log;
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,28,import org.apache.commons.logging.LogFactory;
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,39,private static final Log LOG = LogFactory.getLog(SchemaEvolution.class);
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,41,public SchemaEvolution(TypeDescription readerSchema, boolean[] included) {
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,44,this.readerSchema = readerSchema;
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,49,boolean[] included) throws IOException {
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,52,if (checkAcidSchema(fileSchema)) {
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,53,this.readerSchema = createEventSchema(readerSchema);
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,55,this.readerSchema = readerSchema;
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,148,throw new IOException(
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1851,return pickStripesInternal(sarg, filterColumns, stripeStats, stripeCount, null);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1856,int stripeCount, Path filePath) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1864,return pickStripesInternal(sarg, filterColumns, stripeStats, stripeCount, filePath);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1868,List<StripeStatistics> stripeStats, int stripeCount, Path filePath) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1872,isStripeSatisfyPredicate(stripeStats.get(i), sarg, filterColumns);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1882,StripeStatistics stripeStatistics, SearchArgument sarg, int[] filterColumns) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1889,ColumnStatistics stats = stripeStatistics.getColumnStatistics()[filterColumns[pred]];
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1890,truthValues[pred] = RecordReaderImpl.evaluatePredicate(stats, predLeaves.get(pred), null);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,532,ArrayList<String> aliases = entry.getValue();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,534,VectorPartitionDesc vectorPartDesc = partDesc.getVectorPartitionDesc();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,535,LOG.info("VectorMapOperator path: " + path +
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,137,SchemaEvolution treeReaderSchema;
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,142,LOG.info("Schema on read not provided -- using file schema " +
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,145,treeReaderSchema = new SchemaEvolution(fileReader.getSchema(), included);
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,151,treeReaderSchema = new SchemaEvolution(fileReader.getSchema(),
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,154,this.schema = treeReaderSchema.getReaderSchema();
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,161,if (sarg != null && rowIndexStride != 0) {
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,208,reader = TreeReaderFactory.createTreeReader(treeReaderSchema.getReaderSchema(),
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,209,treeReaderSchema, included, skipCorrupt);
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,23,import java.util.HashMap;
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,25,import java.util.Map;
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,42,this.included = included;
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,43,readerToFile = null;
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,50,readerToFile = new HashMap<>(readerSchema.getMaximumId() + 1);
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,51,this.included = included;
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,57,buildMapping(fileSchema, this.readerSchema);
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,65,TypeDescription result;
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,66,if (readerToFile == null) {
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,67,if (included == null || included[readerType.getId()]) {
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,68,result = readerType;
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,70,result = null;
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,75,return result;
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,78,void buildMapping(TypeDescription fileType,
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,79,TypeDescription readerType) throws IOException {
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,118,buildMapping(fileChildren.get(i), readerChildren.get(i));
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,131,buildMapping(fileChildren.get(i), readerChildren.get(i));
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,524,Context(Configuration conf) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,528,Context(Configuration conf, final int minSplits) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,533,Context(Configuration conf, final int minSplits, ExternalFooterCachesByConf efc) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,658,boolean isOriginal, List<DeltaMetaData> deltas, boolean hasBase, Path dir,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,659,boolean[] covered, ByteBuffer ppdResult) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,673,boolean isOriginal, ArrayList<DeltaMetaData> deltas, boolean hasBase, Path dir,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,674,boolean[] covered) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,711,List<HdfsFileStatusWithId> children, boolean isOriginal, List<DeltaMetaData> deltas,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,712,boolean[] covered, UserGroupInformation ugi, boolean allowSyntheticFileIds) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,771,result.add(new SplitInfo(context, dir.fs, file, null,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1061,private List<OrcProto.Type> types;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1062,private boolean[] includedCols;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1254,String[] colNames = extractNeededColNames(types, context.conf, includedCols, isOriginal);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1255,if (colNames == null) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1256,LOG.warn("Skipping split elimination for {} as column names is null", file.getPath());
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1258,includeStripe = pickStripes(context.sarg, colNames, writerVersion, isOriginal,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1259,stripeStats, stripes.size(), file.getPath());
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1388,includedCols = genIncludedColumns(types, context.conf, isOriginal);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1397,private long computeProjectionSize(List<OrcProto.Type> types,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1398,List<OrcProto.ColumnStatistics> stats, boolean[] includedCols, boolean isOriginal) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1401,if (includedCols != null) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1402,for (int i = 0; i < includedCols.length; i++) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1403,if (includedCols[i]) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1408,return ReaderImpl.getRawDataSizeFromColIndices(internalColIds, types, stats);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1471,adi.splitPath, adi.acidInfo, adi.baseOrOriginalFiles, ugi, allowSyntheticFileIds);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1528,public Context create(Configuration conf, int numSplits) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1549,List<DeltaMetaData> deltas, boolean[] covered, boolean isOriginal,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1550,UserGroupInformation ugi, boolean allowSyntheticFileIds) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1553,context, fs, dir, files, isOriginal, deltas, covered, ugi, allowSyntheticFileIds);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1556,context, fs, dir, files, isOriginal, deltas, covered, ugi, allowSyntheticFileIds);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1566,context, fs, dir, files, isOriginal, deltas, covered, ugi, allowSyntheticFileIds);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1570,context, fs, dir, files, isOriginal, deltas, covered, ugi, allowSyntheticFileIds);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1905,List<HdfsFileStatusWithId> baseOrOriginalFiles, UserGroupInformation ugi,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1906,boolean allowSyntheticFileIds) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1939,deltas, covered, isOriginal, ugi, allowSyntheticFileIds);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1944,deltas, covered, isOriginal, ugi, allowSyntheticFileIds);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewInputFormat.java,139,private Context createContext(Configuration conf, int numSplits) {
orc/src/java/org/apache/orc/TypeDescription.java,40,public class TypeDescription {
orc/src/java/org/apache/orc/TypeDescription.java,281,return getId();
orc/src/java/org/apache/orc/TypeDescription.java,286,if (other == null || other.getClass() != TypeDescription.class) {
orc/src/java/org/apache/orc/TypeDescription.java,294,getId() != castOther.getId() ||
orc/src/java/org/apache/orc/TypeDescription.java,295,getMaximumId() != castOther.getMaximumId() ||
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,36,private final Map<TypeDescription, TypeDescription> readerToFile;
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,73,result = readerToFile.get(readerType);
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,146,readerToFile.put(readerType, fileType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorExpressionDescriptor.java,166,argType == VARCHAR) {
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,38,import org.apache.hadoop.io.BytesWritable;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,39,import org.apache.hadoop.io.FloatWritable;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,339,public long downCastAnyInteger(long input, TypeDescription readerType) {
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,340,switch (readerType.getCategory()) {
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,342,return input == 0 ? 0 : 1;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,344,return (byte) input;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,346,return (short) input;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,348,return (int) input;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,350,return input;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,352,throw new RuntimeException("Unexpected type kind " + readerType.getCategory().name());
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,442,long[] resultVector = resultColVector.vector;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,445,resultVector[0] = downCastAnyInteger(resultVector[0], readerType);
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,447,resultColVector.noNulls = false;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,448,resultColVector.isNull[0] = true;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,452,resultVector[i] = downCastAnyInteger(resultVector[i], readerType);
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,457,resultVector[i] = downCastAnyInteger(resultVector[i], readerType);
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,459,resultColVector.noNulls = false;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,460,resultColVector.isNull[i] = true;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,473,private FloatWritable floatResult;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,483,floatResult = new FloatWritable();
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,488,float floatValue = (float) doubleColVector.vector[elementNum];
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,489,longColVector.vector[elementNum] =
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,490,downCastAnyInteger(
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,491,(long) floatValue, readerType);
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,528,longColVector.vector[elementNum] =
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,529,downCastAnyInteger(
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,530,(long) doubleColVector.vector[elementNum], readerType);
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,556,private HiveDecimalWritable hiveDecimalResult;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,568,hiveDecimalResult = new HiveDecimalWritable();
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,573,longColVector.vector[elementNum] =
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,574,downCastAnyInteger(
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,575,decimalColVector.vector[elementNum].getHiveDecimal().longValue(),
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,576,readerType);
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,599,private final TypeDescription fileType;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,607,this.fileType = fileType;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,618,longColVector.vector[elementNum] =
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,619,downCastAnyInteger(longValue, readerType);
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,663,longColVector.vector[elementNum] =
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,664,downCastAnyInteger(longValue, readerType);
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,748,resultColVector.noNulls = false;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,749,resultColVector.isNull[0] = true;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,760,resultColVector.noNulls = false;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,761,resultColVector.isNull[i] = true;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,774,private final TypeDescription readerType;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,775,private HiveDecimalWritable hiveDecimalResult;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,784,this.readerType = readerType;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,787,hiveDecimalResult = new HiveDecimalWritable();
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,816,private final TypeDescription fileType;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,823,this.fileType = fileType;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,861,private final TypeDescription readerType;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,865,FloatFromTimestampTreeReader(int columnId, TypeDescription readerType,
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,866,boolean skipCorrupt) throws IOException {
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,868,this.readerType = readerType;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,943,private FloatWritable floatResult;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,949,floatResult = new FloatWritable();
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,967,private final TypeDescription readerType;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,968,private HiveDecimalWritable hiveDecimalResult;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,972,DoubleFromDecimalTreeReader(int columnId, TypeDescription fileType,
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,973,TypeDescription readerType) throws IOException {
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,977,this.readerType = readerType;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,980,hiveDecimalResult = new HiveDecimalWritable();
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1009,private final TypeDescription fileType;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1016,this.fileType = fileType;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1053,private final TypeDescription readerType;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1057,DoubleFromTimestampTreeReader(int columnId, TypeDescription readerType,
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1058,boolean skipCorrupt) throws IOException {
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1060,this.readerType = readerType;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1091,private int precision;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1092,private int scale;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1096,DecimalFromAnyIntegerTreeReader(int columnId, TypeDescription fileType,
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1097,TypeDescription readerType, boolean skipCorrupt) throws IOException {
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1099,this.precision = readerType.getPrecision();
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1100,this.scale = readerType.getScale();
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1109,HiveDecimalWritable hiveDecimalWritable =
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1110,new HiveDecimalWritable(longValue);
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1134,private int precision;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1135,private int scale;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1136,private FloatWritable floatResult;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1143,this.precision = readerType.getPrecision();
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1144,this.scale = readerType.getScale();
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1147,floatResult = new FloatWritable();
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1154,HiveDecimal value =
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1156,if (value != null) {
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1157,decimalColVector.set(elementNum, value);
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1230,private final TypeDescription fileType;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1237,this.fileType = fileType;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1274,private final TypeDescription readerType;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1276,private int precision;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1277,private int scale;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1280,DecimalFromTimestampTreeReader(int columnId, TypeDescription readerType,
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1281,boolean skipCorrupt) throws IOException {
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1283,this.readerType = readerType;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1284,this.precision = readerType.getPrecision();
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1285,this.scale = readerType.getScale();
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1323,private final TypeDescription fileType;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1331,this.fileType = fileType;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1367,private FloatWritable floatResult;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1378,floatResult = new FloatWritable();
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1547,private DateWritable dateWritableResult;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1556,dateWritableResult = new DateWritable();
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1588,private final TypeDescription fileType;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1594,this.fileType = fileType;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1612,resultColVector.noNulls = false;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1613,resultColVector.isNull[0] = true;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1624,resultColVector.noNulls = false;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1625,resultColVector.isNull[i] = true;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1637,private BytesWritable binaryWritableResult;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1647,binaryWritableResult = new BytesWritable();
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1728,private FloatWritable floatResult;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1737,floatResult = new FloatWritable();
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1743,timestampColVector.set(elementNum,
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1744,TimestampUtils.doubleToTimestamp(floatValue));
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1780,timestampColVector.set(elementNum,
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1781,TimestampUtils.doubleToTimestamp(doubleValue));
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1806,private HiveDecimalWritable hiveDecimalResult;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1817,hiveDecimalResult = new HiveDecimalWritable();
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1823,TimestampUtils.decimalToTimestamp(
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1824,decimalColVector.vector[elementNum].getHiveDecimal());
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1848,private final TypeDescription fileType;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1855,this.fileType = fileType;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1893,private DateWritable doubleResult;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1902,doubleResult = new DateWritable();
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1932,private final TypeDescription fileType;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1939,this.fileType = fileType;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1977,private final TypeDescription readerType;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1981,DateFromTimestampTreeReader(int columnId, TypeDescription readerType,
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1982,boolean skipCorrupt) throws IOException {
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1984,this.readerType = readerType;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,2017,private final TypeDescription fileType;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,2022,this.fileType = fileType;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,2067,return new DecimalFromAnyIntegerTreeReader(columnId, fileType, readerType, skipCorrupt);
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,2211,return new DoubleFromDecimalTreeReader(columnId, fileType, readerType);
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,2427,return new FloatFromTimestampTreeReader(columnId, readerType, skipCorrupt);
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,2430,return new DoubleFromTimestampTreeReader(columnId, readerType, skipCorrupt);
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,2433,return new DecimalFromTimestampTreeReader(columnId, readerType, skipCorrupt);
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,2445,return new DateFromTimestampTreeReader(columnId, readerType, skipCorrupt);
storage-api/src/java/org/apache/hadoop/hive/ql/util/TimestampUtils.java,42,long seconds = (long) f;
storage-api/src/java/org/apache/hadoop/hive/ql/util/TimestampUtils.java,47,BigDecimal bd;
storage-api/src/java/org/apache/hadoop/hive/ql/util/TimestampUtils.java,49,bd = new BigDecimal(String.valueOf(f));
storage-api/src/java/org/apache/hadoop/hive/ql/util/TimestampUtils.java,53,bd = bd.subtract(new BigDecimal(seconds)).multiply(new BigDecimal(1000000000));
storage-api/src/java/org/apache/hadoop/hive/ql/util/TimestampUtils.java,54,int nanos = bd.intValue();
storage-api/src/java/org/apache/hadoop/hive/ql/util/TimestampUtils.java,57,long millis = seconds * 1000;
storage-api/src/java/org/apache/hadoop/hive/ql/util/TimestampUtils.java,58,if (nanos < 0) {
storage-api/src/java/org/apache/hadoop/hive/ql/util/TimestampUtils.java,59,millis -= 1000;
storage-api/src/java/org/apache/hadoop/hive/ql/util/TimestampUtils.java,60,nanos += 1000000000;
storage-api/src/java/org/apache/hadoop/hive/ql/util/TimestampUtils.java,62,Timestamp t = new Timestamp(millis);
storage-api/src/java/org/apache/hadoop/hive/ql/util/TimestampUtils.java,65,t.setNanos(nanos);
storage-api/src/java/org/apache/hadoop/hive/ql/util/TimestampUtils.java,66,return t;
storage-api/src/java/org/apache/hadoop/hive/ql/util/TimestampUtils.java,70,BigDecimal nanoInstant = d.bigDecimalValue().multiply(BILLION_BIG_DECIMAL);
storage-api/src/java/org/apache/hadoop/hive/ql/util/TimestampUtils.java,71,int nanos = nanoInstant.remainder(BILLION_BIG_DECIMAL).intValue();
storage-api/src/java/org/apache/hadoop/hive/ql/util/TimestampUtils.java,72,if (nanos < 0) {
storage-api/src/java/org/apache/hadoop/hive/ql/util/TimestampUtils.java,73,nanos += 1000000000;
storage-api/src/java/org/apache/hadoop/hive/ql/util/TimestampUtils.java,75,long seconds =
storage-api/src/java/org/apache/hadoop/hive/ql/util/TimestampUtils.java,76,nanoInstant.subtract(new BigDecimal(nanos)).divide(BILLION_BIG_DECIMAL).longValue();
storage-api/src/java/org/apache/hadoop/hive/ql/util/TimestampUtils.java,77,Timestamp t = new Timestamp(seconds * 1000);
storage-api/src/java/org/apache/hadoop/hive/ql/util/TimestampUtils.java,78,t.setNanos(nanos);
storage-api/src/java/org/apache/hadoop/hive/ql/util/TimestampUtils.java,80,return t;
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,2328,throw new IllegalArgumentException("No conversion of type " +
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,2329,readerType.getCategory() + " to self needed");
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,2385,throw new IllegalArgumentException("No conversion of type " +
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,2386,readerType.getCategory() + " to self needed");
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,104,isOk = fileType.getMaxLength() == readerType.getMaxLength();
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,2037,if (!fileType.getCategory().equals(readerTypeCategory) &&
common/src/java/org/apache/hadoop/hive/ql/log/PerfLogger.java,220,Map<String, MetricsScope> openScopes = new HashMap<String, MetricsScope>();
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,80,private final PerfLogger perfLogger = SessionState.getPerfLogger();
serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroDeserializer.java,311,if (schema.getType().equals(Schema.Type.NULL)) {
serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerdeUtils.java,180,return schema.getType().equals(Schema.Type.UNION) &&
serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerdeUtils.java,181,schema.getTypes().size() == 2 &&
serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerdeUtils.java,182,(schema.getTypes().get(0).getType().equals(Schema.Type.NULL) ||
serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerdeUtils.java,183,schema.getTypes().get(1).getType().equals(Schema.Type.NULL));
serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerdeUtils.java,192,List<Schema> types = schema.getTypes();
serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerdeUtils.java,194,return types.get(0).getType().equals(Schema.Type.NULL) ? types.get(1) : types.get(0);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,21,import com.google.common.util.concurrent.Service;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,3245,acquireLock(key);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,3247,throw new MetaException("This can't happen because checkRetryable() has a retry limit");
common/src/java/org/apache/hadoop/hive/common/FileUtils.java,416,String userName, FsAction action) throws Exception {
common/src/java/org/apache/hadoop/hive/common/FileUtils.java,432,if (!isDir) {
common/src/java/org/apache/hadoop/hive/common/FileUtils.java,440,if (!isActionPermittedForFileHierarchy(fs, childStatus, userName, action)) {
common/src/java/org/apache/hadoop/hive/common/FileUtils.java,486,if (!fileStatus.isDir()) {
common/src/java/org/apache/hadoop/hive/common/FileUtils.java,494,if (!isOwnerOfFileHierarchy(fs, childStatus, userName)) {
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLAuthorizationUtils.java,388,FileStatus fileStatus = FileUtils.getPathOrParentThatExists(fs, filePath);
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLAuthorizationUtils.java,389,if (FileUtils.isOwnerOfFileHierarchy(fs, fileStatus, userName)) {
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLAuthorizationUtils.java,390,availPrivs.addPrivilege(SQLPrivTypeGrant.OWNER_PRIV);
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLAuthorizationUtils.java,392,if (FileUtils.isActionPermittedForFileHierarchy(fs, fileStatus, userName, FsAction.WRITE)) {
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLAuthorizationUtils.java,393,availPrivs.addPrivilege(SQLPrivTypeGrant.INSERT_NOGRANT);
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLAuthorizationUtils.java,394,availPrivs.addPrivilege(SQLPrivTypeGrant.DELETE_NOGRANT);
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLAuthorizationUtils.java,396,if (FileUtils.isActionPermittedForFileHierarchy(fs, fileStatus, userName, FsAction.READ)) {
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLAuthorizationUtils.java,397,availPrivs.addPrivilege(SQLPrivTypeGrant.SELECT_NOGRANT);
ql/src/java/org/apache/hadoop/hive/ql/io/sarg/ConvertAstToSearchArg.java,152,if (lit instanceof Float) {
ql/src/java/org/apache/hadoop/hive/ql/io/sarg/ConvertAstToSearchArg.java,155,return Double.parseDouble(lit.toString());
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRecordUpdater.java,190,try {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRecordUpdater.java,191,FSDataOutputStream strm = fs.create(new Path(path, ACID_FORMAT), false);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRecordUpdater.java,192,strm.writeInt(ORC_ACID_VERSION);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRecordUpdater.java,193,strm.close();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRecordUpdater.java,195,if (LOG.isDebugEnabled()) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRecordUpdater.java,196,LOG.debug("Failed to create " + path + "/" + ACID_FORMAT + " with " +
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java,332,acquireLocks(plan, ctx, username, true);
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java,333,ctx.setHeartbeater(startHeartbeat(delay));
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1199,if (LOG.isDebugEnabled()) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1200,LOG.debug("Waiting to acquire compile lock: " + command);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1218,LOG.debug("Acquired the compile lock");
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java,457,private String getContainerJavaOpts(Configuration conf) {
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,744,LOG.warn("Exception is thrown closing timed-out operation " + operation.getHandle(), e);
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,1545,if (null != newFiles) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,3097,if(ex.getErrorCode() == 1022 && "23000".equals(ex.getSQLState())) {
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,2404,HIVE_MOVE_FILES_THREAD_COUNT("hive.mv.files.thread", 25, new  SizeValidator(0L, true, 1024L, true), "Number of threads"
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,2906,if(!destFs.rename(srcStatus.getPath(), destf)) {
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,2915,final Path destPath = new Path(destf, srcStatus.getPath().getName());
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,2917,if(destFs.rename(srcStatus.getPath(), destf)) {
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,2919,HdfsUtils.setFullFileStatus(conf, desiredStatus, group, destFs, destPath, false);
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,2923,+ destPath + " returned false");
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,587,for (Entity e : sem.getOutputs()) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,589,additionalOutputs.add(new WriteEntity(e.getTable(), WriteEntity.WriteType.DDL_NO_LOCK));
ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java,478,private long txn;
ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java,610,bestBase.status = child;
ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java,611,bestBase.txn = txn;
ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java,613,obsolete.add(bestBase.status);
ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java,614,bestBase.status = child;
ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java,615,bestBase.txn = txn;
ql/src/java/org/apache/hadoop/hive/ql/txn/AcidOpenTxnsCounterService.java,62,LOG.info("OpenTxnsCounter ran for " + (System.currentTimeMillis() - startTime)/1000 + "seconds.  isAliveCounter=" + count);
ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java,235,private boolean hasFollowingStatsTask() {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java,1498,nd.getConf().setGatherStats(true);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java,1499,nd.getConf().setStatsReliable(hconf.getBoolVar(ConfVars.HIVE_STATS_RELIABLE));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java,1739,if (mvTask != null && isInsertTable && hconf.getBoolVar(ConfVars.HIVESTATSAUTOGATHER) &&
ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java,1740,!fsOp.getConf().isMaterialization()) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java,1741,GenMapRedUtils.addStatsTask(fsOp, mvTask, currTask, hconf);
ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezUtils.java,299,desc.setDirName(new Path(path, ""+linked.size()));
ql/src/java/org/apache/hadoop/hive/ql/stats/fs/FSStatsPublisher.java,100,Path statsFile = new Path(statsDir,StatsSetupConst.STATS_FILE_PREFIX +conf.getInt("mapred.task.partition",0));
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,7913,int kindex = exprBack == null ? -1 : ExprNodeDescUtils.indexOf(exprBack, reduceKeysBack);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,7925,int vindex = exprBack == null ? -1 : ExprNodeDescUtils.indexOf(exprBack, reduceValuesBack);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/CustomPartitionVertex.java,496,new CustomEdgeConfiguration(routingTable.keySet().size(), routingTable);
ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java,27,import org.apache.hadoop.hive.conf.HiveConf;
ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java,559,Class<?> functionClass;
ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java,560,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java,561,functionClass = info.getFunctionClass();
ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java,563,return registerToSessionRegistry(qualifiedName, info);
ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java,565,if (functionClass == null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java,566,return registerToSessionRegistry(qualifiedName, info);
ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java,568,return info;
ql/src/java/org/apache/hadoop/hive/ql/util/ResourceDownloader.java,106,FileSystem fs = FileSystem.get(srcUri, conf);
service/src/java/org/apache/hive/service/cli/operation/ExecuteStatementOperation.java,34,super(parentSession, confOverlay, OperationType.EXECUTE_STATEMENT, runInBackground);
service/src/java/org/apache/hive/service/cli/operation/Operation.java,70,protected final boolean runAsync;
service/src/java/org/apache/hive/service/cli/operation/Operation.java,92,protected Operation(HiveSession parentSession, Map<String, String> confOverlay, OperationType opType, boolean runInBackground) {
service/src/java/org/apache/hive/service/cli/operation/Operation.java,97,this.runAsync = runInBackground;
service/src/java/org/apache/hive/service/cli/operation/Operation.java,104,queryState = new QueryState(parentSession.getHiveConf(), confOverlay, runAsync);
service/src/java/org/apache/hive/service/cli/operation/Operation.java,120,return runAsync;
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,275,final SessionState parentSessionState = SessionState.get();
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,278,final Hive parentHive = parentSession.getSessionHive();
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,281,final UserGroupInformation currentUGI = getCurrentUGI();
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,284,Runnable backgroundOperation = new Runnable() {
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,286,public void run() {
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,287,PrivilegedExceptionAction<Object> doAsAction = new PrivilegedExceptionAction<Object>() {
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,289,public Object run() throws HiveSQLException {
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,290,Hive.set(parentHive);
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,291,SessionState.setCurrentSessionState(parentSessionState);
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,293,registerCurrentOperationLog();
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,294,registerLoggingContext();
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,295,try {
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,296,if (asyncPrepare) {
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,297,prepare(queryState);
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,299,runQuery();
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,301,setOperationException(e);
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,302,LOG.error("Error running hive query: ", e);
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,304,unregisterLoggingContext();
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,305,unregisterOperationLog();
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,307,return null;
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,311,try {
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,312,currentUGI.doAs(doAsAction);
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,314,setOperationException(new HiveSQLException(e));
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,315,LOG.error("Error running hive query as user : " + currentUGI.getShortUserName(), e);
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,317,finally {
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,323,if (ThreadWithGarbageCleanup.currentThread() instanceof ThreadWithGarbageCleanup) {
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,324,ThreadWithGarbageCleanup currentThread =
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,325,(ThreadWithGarbageCleanup) ThreadWithGarbageCleanup.currentThread();
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,326,currentThread.cacheThreadLocalRawStore();
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,333,Future<?> backgroundHandle =
service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java,334,getParentSession().getSessionManager().submitBackgroundOperation(backgroundOperation);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,27,import java.util.HashMap;
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,41,import org.apache.hadoop.hive.ql.QueryPlan;
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,42,import org.apache.hadoop.hive.ql.exec.ListSinkOperator;
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,118,public HiveSessionImpl(SessionHandle sessionHandle, TProtocolVersion protocol, String username, String password,
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,119,HiveConf serverhiveConf, String ipAddress) {
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,124,this.sessionConf = new HiveConf(serverhiveConf);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,328,protected synchronized void acquire(boolean userAccess) {
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,348,protected synchronized void release(boolean userAccess) {
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,404,acquire(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,424,release(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,452,acquire(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,455,ExecuteStatementOperation operation =
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,456,operationManager.newExecuteStatementOperation(getSession(), statement, confOverlay,
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,457,runAsync, queryTimeout);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,470,release(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,477,acquire(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,490,release(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,497,acquire(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,510,release(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,517,acquire(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,531,release(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,539,acquire(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,553,release(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,560,acquire(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,573,release(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,580,acquire(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,598,release(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,611,acquire(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,625,release(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,632,acquire(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,674,release(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,735,acquire(false);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,748,release(false);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,754,acquire(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,758,release(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,764,acquire(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,771,release(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,777,acquire(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,781,release(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,788,acquire(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,795,release(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,849,acquire(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,863,release(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,871,acquire(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,887,release(true);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImplwithUGI.java,92,acquire(true);
common/src/java/org/apache/hive/http/HttpServer.java,133,public Builder setConf(HiveConf conf) {
common/src/java/org/apache/hive/http/HttpServer.java,135,this.conf = conf;
service/src/java/org/apache/hive/service/server/HiveServer2.java,138,if (hiveConf.getBoolVar(ConfVars.HIVE_IN_TEST)) {
service/src/java/org/apache/hive/service/server/HiveServer2.java,139,LOG.info("Web UI is disabled since in test mode");
service/src/java/org/apache/hive/service/server/HiveServer2.java,141,int webUIPort =
llap-server/src/java/org/apache/hadoop/hive/llap/cache/BuddyAllocator.java,268,memoryManager.forceReservedMemory(allocationSize * (dest.length - destAllocIx));
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheMemoryManager.java,117,public void forceReservedMemory(int memoryToEvict) {
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheMemoryManager.java,119,while (memoryToEvict > 0) {
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheMemoryManager.java,120,long evicted = evictor.evictSomeBlocks(memoryToEvict);
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheMemoryManager.java,121,if (evicted == 0) return;
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheMemoryManager.java,122,memoryToEvict -= evicted;
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelFifoCachePolicy.java,80,LlapCacheableBuffer candidate = iter.next();
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelFifoCachePolicy.java,81,if (candidate.invalidate()) {
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelFifoCachePolicy.java,83,evicted += candidate.getMemoryUsage();
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelFifoCachePolicy.java,84,evictionListener.notifyEvicted(candidate);
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java,191,long evicted = 0;
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java,193,LlapCacheableBuffer nextCandidate, firstCandidate;
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java,229,if (evicted >= memoryToReserve) return evicted;
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java,232,long time = timer.get();
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java,233,while (evicted < memoryToReserve) {
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java,234,LlapCacheableBuffer buffer = null;
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java,235,synchronized (heap) {
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java,236,buffer = evictFromHeapUnderLock(time);
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java,238,if (buffer == null) return evicted;
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java,239,evicted += buffer.getMemoryUsage();
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java,242,return evicted;
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java,264,private LlapCacheableBuffer evictFromHeapUnderLock(long time) {
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java,265,while (true) {
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java,266,if (heapSize == 0) return null;
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java,267,LlapCacheableBuffer result = heap[0];
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java,268,if (LlapIoImpl.CACHE_LOGGER.isTraceEnabled()) {
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java,269,LlapIoImpl.CACHE_LOGGER.info("Evicting {} at {}", result, time);
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java,271,result.indexInHeap = LlapCacheableBuffer.NOT_IN_CACHE;
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java,272,--heapSize;
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java,273,boolean canEvict = result.invalidate();
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java,274,if (heapSize > 0) {
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java,275,LlapCacheableBuffer newRoot = heap[heapSize];
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java,276,newRoot.indexInHeap = 0;
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java,277,if (newRoot.lastUpdate != time) {
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java,278,newRoot.priority = expirePriority(time, newRoot.lastUpdate, newRoot.priority);
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java,279,newRoot.lastUpdate = time;
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java,281,heapifyDownUnderLock(newRoot, time);
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java,283,if (canEvict) return result;
llap-server/src/java/org/apache/hadoop/hive/llap/cache/MemoryManager.java,25,void forceReservedMemory(int memoryToEvict);
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,397,JobClient jc = new JobClient(job);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,85,int numConcurrentLlapQueries = -1;
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,115,HiveConf newConf = new HiveConf(initConf);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,118,newConf.set("tez.queue.name", sessionState.getQueueName());
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,336,LOG.info("Closing tez session default? " + tezSessionState.isDefault());
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,415,if (doAsEnabled != conf.getBoolVar(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,421,LOG.info("Current queue name is " + queueName + " incoming queue name is "
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,422,+ conf.get("tez.queue.name"));
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,423,if (queueName == null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,424,if (conf.get("tez.queue.name") != null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,426,return false;
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,428,return true;
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,432,if (!queueName.equals(conf.get("tez.queue.name"))) {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,434,return false;
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,438,throw new HiveException("Default queue should always be returned." +
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,442,return true;
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,461,public void closeAndOpen(TezSessionState sessionState, HiveConf conf,
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,464,if (sessionConf != null && sessionConf.get("tez.queue.name") != null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,465,conf.set("tez.queue.name", sessionConf.get("tez.queue.name"));
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,467,closeIfNotDefault(sessionState, keepTmpDir);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,482,private void closeAndReopen(TezSessionPoolSession oldSession) throws Exception {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,505,closeAndReopen(next);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,686,parent.closeAndReopen(this);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java,132,+ ", doAs=" + doAsEnabled + ", isOpen=" + isOpen();
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java,229,this.queueName = conf.get("tez.queue.name");
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java,232,final boolean llapMode = "llap".equals(HiveConf.getVar(
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java,667,defaultQueue  = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,133,session =
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,134,TezSessionPoolManager.getInstance().getSession(session, conf, false,
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,135,getWork().getLlapMode());
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,139,JobConf jobConf = utils.createConfiguration(conf);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,142,String[] inputOutputJars = work.configureJobConfAndExtractJars(jobConf);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,146,Path scratchDir = ctx.getMRScratchDir();
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,149,scratchDir = utils.createTezDir(scratchDir, conf);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,151,Map<String,LocalResource> inputOutputLocalResources =
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,152,getExtraLocalResources(jobConf, scratchDir, inputOutputJars);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,155,updateSession(session, jobConf, scratchDir, inputOutputJars, inputOutputLocalResources);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,157,List<LocalResource> additionalLr = session.getLocalizedResources();
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,160,if (LOG.isDebugEnabled()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,161,if (additionalLr == null || additionalLr.size() == 0) {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,162,LOG.debug("No local resources to process (other than hive-exec)");
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,164,for (LocalResource lr: additionalLr) {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,165,LOG.debug("Adding local resource: " + lr.getResource());
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,172,LocalResource appJarLr = session.getAppJarLr();
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,175,DAG dag = build(jobConf, work, scratchDir, appJarLr, additionalLr, ctx);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,176,CallerContext callerContext = CallerContext.create(
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,179,dag.setCallerContext(callerContext);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,182,addExtraResourcesToDag(session, dag, inputOutputJars, inputOutputLocalResources);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,185,dagClient = submit(jobConf, dag, scratchDir, appJarLr, session,
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,186,additionalLr, inputOutputJars, inputOutputLocalResources);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,189,TezJobMonitor monitor = new TezJobMonitor(work.getWorkMap());
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,190,rc = monitor.monitorExecution(dagClient, conf, dag, ctx);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,191,if (rc != 0) {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,192,this.setException(new HiveException(monitor.getDiagnostics()));
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,196,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,197,Set<StatusGetOpts> statusGetOpts = EnumSet.of(StatusGetOpts.GET_COUNTERS);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,198,counters = dagClient.getDAGStatus(statusGetOpts).getDAGCounters();
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,201,LOG.error("Failed to get counters: " + err, err);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,202,counters = null;
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,204,TezSessionPoolManager.getInstance().returnSession(session, getWork().getLlapMode());
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,207,&& (conf.getBoolVar(conf, HiveConf.ConfVars.TEZ_EXEC_SUMMARY) ||
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,451,TezSessionPoolManager.getInstance().closeAndOpen(sessionState, this.conf, inputOutputJars,
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,452,true);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,462,TezSessionPoolManager.getInstance().closeAndOpen(sessionState, this.conf, inputOutputJars,
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,586,startSs.tezSessionState = new TezSessionState(startSs.getSessionId());
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,1490,TezSessionPoolManager.getInstance().closeIfNotDefault(tezSessionState, false);
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,1495,tezSessionState = null;
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,1581,this.tezSessionState = session;
service/src/java/org/apache/hive/service/cli/operation/MetadataOperation.java,47,super(parentSession, opType, false);
service/src/java/org/apache/hive/service/cli/operation/Operation.java,88,protected Operation(HiveSession parentSession, OperationType opType, boolean runInBackground) {
service/src/java/org/apache/hive/service/cli/operation/Operation.java,89,this(parentSession, null, opType, runInBackground);
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,735,userNameFromPrincipal = getUserNameFromPrincipal(principal);
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,743,private String getUserNameFromPrincipal(String principal) {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,745,String[] components = principal.split("[/@]");
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,746,return (components == null || components.length != 3) ? principal : components[0];
llap-common/src/java/org/apache/hadoop/hive/llap/security/SecretManager.java,69,checkRootAcls(conf, path, UserGroupInformation.getCurrentUser().getShortUserName());
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTokenChecker.java,87,String newUserName = llapId.getRealUser().toString();
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTokenChecker.java,126,String tokenUser = llapId.getRealUser().toString(), tokenAppId = llapId.getAppId();
shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java,1057,distcp.execute();
shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java,1058,return true;
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,190,HiveConf.ConfVars.METASTORE_AUTO_START_MECHANISM_MODE,
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,669,METASTORE_SCHEMA_VERIFICATION("hive.metastore.schema.verification", false,
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,675,METASTORE_SCHEMA_VERIFICATION_RECORD_VERSION("hive.metastore.schema.verification.record.version", true,
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,678,METASTORE_AUTO_START_MECHANISM_MODE("datanucleus.autoStartMechanismMode", "checked",
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,7674,HiveConf.ConfVars.METASTORE_SCHEMA_VERIFICATION + " so setting version.");
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,21,import java.util.ArrayList;
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1379,ArrayList<String> emptyBuckets =
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1407,private static void createEmptyBuckets(Configuration hconf, ArrayList<String> paths,
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1435,for (String p : paths) {
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1436,Path path = new Path(p);
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1457,public static ArrayList<String> removeTempOrDuplicateFiles(FileSystem fs, Path path,
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1463,ArrayList<String> result = new ArrayList<String>();
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1494,String path2 = replaceTaskIdFromFilename(bucketPath.toUri().getPath().toString(), j);
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1495,result.add(path2);
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1512,String path2 = replaceTaskIdFromFilename(bucketPath.toUri().getPath().toString(), j);
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1513,result.add(path2);
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,28,import com.esotericsoftware.kryo.KryoException;
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,243,unwrapContainer[alias] = new UnwrapRowContainer(alias, valueIndex, hasFilter(alias));
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,251,valueOI.add(joinKeysObjectInspectors[bigPos].get(valueIndex[i]));
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/UnwrapRowContainer.java,21,import org.apache.hadoop.hive.ql.metadata.HiveException;
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/UnwrapRowContainer.java,22,import org.apache.hadoop.hive.serde2.SerDeException;
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/UnwrapRowContainer.java,46,public UnwrapRowContainer(byte alias, int[] valueIndex, boolean tagged)  {
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/UnwrapRowContainer.java,75,for (int index : valueIndex) {
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/UnwrapRowContainer.java,77,unwrapped.add(currentKey == null ? null : currentKey[index]);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveProjectMergeRule.java,24,public static final HiveProjectMergeRule INSTANCE = new HiveProjectMergeRule();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveProjectMergeRule.java,26,public HiveProjectMergeRule() {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveProjectMergeRule.java,27,super(true, HiveRelFactories.HIVE_BUILDER);
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,1016,HepMatchOrder.BOTTOM_UP,
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,1017,ProjectRemoveRule.INSTANCE, UnionMergeRule.INSTANCE,
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,1018,new ProjectMergeRule(false, HiveRelFactories.HIVE_PROJECT_FACTORY),
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,1019,HiveAggregateProjectMergeRule.INSTANCE, HiveJoinCommuteRule.INSTANCE);
hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/AbstractRecordWriter.java,72,throws ConnectionError, StreamingException {
hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/AbstractRecordWriter.java,73,this.endPoint = endPoint;
hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/AbstractRecordWriter.java,78,this.tbl = msClient.getTable(endPoint.database, endPoint.table);
hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/AbstractRecordWriter.java,79,this.partitionPath = getPathForEndPoint(msClient, endPoint);
hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/AbstractRecordWriter.java,81,if(totalBuckets <= 0) {
hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/AbstractRecordWriter.java,83,+ endPoint);
hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/AbstractRecordWriter.java,85,this.bucketIds = getBucketColIDs(tbl.getSd().getBucketCols(), tbl.getSd().getCols()) ;
hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/AbstractRecordWriter.java,88,outf = (AcidOutputFormat<?,?>) ReflectionUtils.newInstance(JavaUtils.loadClass(outFormatName), conf);
hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/AbstractRecordWriter.java,91,throw new ConnectionError(endPoint, e);
hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/AbstractRecordWriter.java,93,throw new ConnectionError(endPoint, e);
hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/AbstractRecordWriter.java,95,throw new StreamingException(e.getMessage(), e);
hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/AbstractRecordWriter.java,97,throw new StreamingException(e.getMessage(), e);
hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/DelimitedInputWriter.java,77,throws ClassNotFoundException, ConnectionError, SerializationError,
hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/DelimitedInputWriter.java,78,InvalidColumn, StreamingException {
hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/DelimitedInputWriter.java,79,this(colNamesForFields, delimiter, endPoint, null);
hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/DelimitedInputWriter.java,99,(char) LazySerDeParameters.DefaultSeparators[0]);
hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/DelimitedInputWriter.java,121,super(endPoint, conf);
hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/StrictJsonWriter.java,57,this(endPoint, null);
hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/StrictJsonWriter.java,70,super(endPoint, conf);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,923,if(lc.isSetOperationType() && lc.getOperationType() == DataOperationType.UNSET) {
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,2970,if (!srcFs.getClass().equals(destFs.getClass())) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/ValidCompactorTxnList.java,19,package org.apache.hadoop.hive.metastore.txn;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/ValidCompactorTxnList.java,108,long getMinOpenTxn() {
ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java,30,import org.apache.hadoop.hive.common.ValidReadTxnList;
ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java,626,new ValidReadTxnList(jobConf.get(ValidTxnList.VALID_TXNS_KEY));
ql/src/java/org/apache/hadoop/hive/ql/ppd/ExprWalkerProcFactory.java,26,import org.slf4j.Logger;
ql/src/java/org/apache/hadoop/hive/ql/ppd/ExprWalkerProcFactory.java,27,import org.slf4j.LoggerFactory;
ql/src/java/org/apache/hadoop/hive/ql/ppd/ExprWalkerProcFactory.java,370,ctx.addFinalCandidate(exprInfo.alias, expr);
ql/src/java/org/apache/hadoop/hive/ql/exec/SMBMapJoinOperator.java,353,if (this.candidateStorage[pos].hasRows()) {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,93,private final static String ROOT_NAMESPACE = "llap";
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,132,private final ACLProvider zooKeeperAclProvider = new ACLProvider() {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,135,public List<ACL> getDefaultAcl() {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,137,LOG.warn("getDefaultAcl was called");
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,138,return Lists.newArrayList(ZooDefs.Ids.OPEN_ACL_UNSAFE);
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,142,public List<ACL> getAclForPath(String path) {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,143,if (!UserGroupInformation.isSecurityEnabled() || path == null
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,144,|| !path.contains(userPathPrefix)) {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,146,return Lists.newArrayList(ZooDefs.Ids.OPEN_ACL_UNSAFE);
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,148,return createSecureAcls();
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,172,.build();
llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapServiceDriver.java,198,propsDirectOptions.setProperty(ConfVars.LLAP_DAEMON_SERVICE_HOSTS.varname, "@" + options.getName());
llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapServiceDriver.java,208,LOG.warn("Note that this might need YARN physical memory monitoring to be turned off (yarn.nodemanager.pmem-check-enabled=false)");
llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapServiceDriver.java,543,String err =
llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapServiceDriver.java,545,+ "); not packaging the jar";
llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapServiceDriver.java,546,LOG.error(err, t);
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,151,if (isEmbeddedMode) {
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,152,EmbeddedThriftBinaryCLIService embeddedClient = new EmbeddedThriftBinaryCLIService();
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,153,embeddedClient.init(null);
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,154,client = embeddedClient;
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,157,openTransport();
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,159,client = new TCLIService.Client(new TBinaryProtocol(transport));
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,172,openSession();
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,175,client = newSynchronizedClient(client);
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,178,private void openTransport() throws SQLException {
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,179,int maxRetries = 1;
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,180,try {
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,181,String strRetries = sessConfMap.get(JdbcConnectionParams.RETRIES);
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,182,if (StringUtils.isNotBlank(strRetries)) {
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,183,maxRetries = Integer.parseInt(strRetries);
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,188,for (int numRetries = 0;;) {
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,190,assumeSubject =
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,191,JdbcConnectionParams.AUTH_KERBEROS_AUTH_TYPE_FROM_SUBJECT.equals(sessConfMap
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,192,.get(JdbcConnectionParams.AUTH_KERBEROS_AUTH_TYPE));
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,193,transport = isHttpTransportMode() ? createHttpTransport() : createBinaryTransport();
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,194,if (!transport.isOpen()) {
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,195,transport.open();
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,196,logZkDiscoveryMessage("Connected to " + connParams.getHost() + ":" + connParams.getPort());
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,251,try {
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,252,transport = new THttpClient(getServerHttpUrl(useSsl), httpClient);
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,257,TCLIService.Iface client = new TCLIService.Client(new TBinaryProtocol(transport));
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,258,TOpenSessionResp openResp = client.OpenSession(new TOpenSessionReq());
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,259,if (openResp != null) {
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,260,client.CloseSession(new TCloseSessionReq(openResp.getSessionHandle()));
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,263,catch (TException e) {
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,264,LOG.info("JDBC Connection Parameters used : useSSL = " + useSsl + " , httpPath  = " +
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,265,sessConfMap.get(JdbcConnectionParams.HTTP_PATH) + " Authentication type = " +
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,266,sessConfMap.get(JdbcConnectionParams.AUTH_TYPE));
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,267,String msg =  "Could not create http connection to " +
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,268,jdbcUriString + ". " + e.getMessage();
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,269,throw new TTransportException(msg, e);
service/src/java/org/apache/hive/service/cli/operation/Operation.java,107,public QueryState getQueryState() {
service/src/java/org/apache/hive/service/cli/operation/Operation.java,108,return queryState;
service/src/java/org/apache/hive/service/cli/operation/SQLOperationDisplay.java,44,this.executionEngine = sqlOperation.getQueryState().getConf().getVar(HiveConf.ConfVars.HIVE_EXECUTION_ENGINE);
hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/HiveEndPoint.java,432,return new TransactionBatchImpl(username, ugi, endPt, numTransactions, msClient
hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/HiveEndPoint.java,433,, recordWriter, agentInfo);
hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/HiveEndPoint.java,575,final int numTxns, final IMetaStoreClient msClient, RecordWriter recordWriter,
hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/HiveEndPoint.java,577,throws StreamingException, TransactionBatchUnAvailable, InterruptedException {
hcatalog/streaming/src/java/org/apache/hive/hcatalog/streaming/HiveEndPoint.java,940,HeartbeatTxnRangeResponse resp = msClient.heartbeatTxnRange(first, last);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnDbUtil.java,303,for(int colPos = 1; colPos <= rsmd.getColumnCount(); colPos++) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnDbUtil.java,304,sb.append(rsmd.getColumnName(colPos)).append("   ");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnDbUtil.java,306,sb.append('\n');
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java,421,client.heartbeat(txnId, lockId);
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java,448,Heartbeater heartbeater = new Heartbeater(this, conf);
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java,555,if (heartbeatExecutorService != null
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java,561,int threadPoolSize = conf.getIntVar(HiveConf.ConfVars.HIVE_TXN_HEARTBEAT_THREADPOOL_SIZE);
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java,563,Executors.newScheduledThreadPool(threadPoolSize, new ThreadFactory() {
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java,567,return new Thread(r, "Heartbeater-" + threadCounter.getAndIncrement());
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java,614,Heartbeater(HiveTxnManager txnMgr, HiveConf conf) {
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java,634,LOG.error("Failed trying to heartbeat " + e.getMessage());
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,27,import java.util.Comparator;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,28,import java.util.Iterator;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,31,import java.util.StringTokenizer;
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,250,dumpConfig(this, sb);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,255,sb.append("START========\"new HiveConf()\"========\n");
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,256,HiveConf c = new HiveConf();
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,257,sb.append("hiveDefaultUrl=").append(c.getHiveDefaultLocation()).append('\n');
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,258,sb.append("hiveSiteURL=").append(HiveConf.getHiveSiteLocation()).append('\n');
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,259,sb.append("hiveServer2SiteUrl=").append(HiveConf.getHiveServer2SiteLocation()).append('\n');
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,260,sb.append("hivemetastoreSiteUrl=").append(HiveConf.getMetastoreSiteLocation()).append('\n');
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,261,dumpConfig(c, sb);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,262,sb.append("END========\"new HiveConf()\"========\n");
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,265,private static void dumpConfig(Configuration conf, StringBuilder sb) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,266,Iterator<Map.Entry<String, String>> configIter = conf.iterator();
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,267,List<Map.Entry<String, String>>configVals = new ArrayList<>();
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,268,while(configIter.hasNext()) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,269,configVals.add(configIter.next());
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,271,Collections.sort(configVals, new Comparator<Map.Entry<String, String>> () {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,273,public int compare(Map.Entry<String, String> ent, Map.Entry<String, String> ent2) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,274,return ent.getKey().compareTo(ent2.getKey());
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,277,for(Map.Entry<String, String> entry : configVals) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,279,if(entry.getKey().toLowerCase().contains("path")) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,280,StringTokenizer st = new StringTokenizer(conf.get(entry.getKey()), File.pathSeparator);
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,281,sb.append(entry.getKey()).append("=\n");
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,282,while(st.hasMoreTokens()) {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,283,sb.append("    ").append(st.nextToken()).append(File.pathSeparator).append('\n');
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,286,else {
hcatalog/webhcat/svr/src/main/java/org/apache/hive/hcatalog/templeton/AppConfig.java,287,sb.append(entry.getKey()).append('=').append(conf.get(entry.getKey())).append('\n');
ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/HouseKeeperServiceBase.java,51,return new Thread(r, this.getClass().getName() + "-" + threadCounter.getAndIncrement());
llap-common/src/java/org/apache/hadoop/hive/llap/security/SecretManager.java,155,zkConf.set(SecretManager.ZK_DTSM_ZK_KERBEROS_PRINCIPAL, principal);
llap-common/src/java/org/apache/hadoop/hive/llap/security/SecretManager.java,173,public static SecretManager createSecretManager(final Configuration conf, String clusterId) {
llap-common/src/java/org/apache/hadoop/hive/llap/security/SecretManager.java,180,final Configuration conf, String llapPrincipal, String llapKeytab, final String clusterId) {
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,235,import com.google.common.collect.ImmutableList;
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,2764,String fullF1 = getQualifiedPathWithoutSchemeAndAuthority(srcf, srcFs);
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,2765,String fullF2 = getQualifiedPathWithoutSchemeAndAuthority(destf, destFs);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,1951,viewAliasToInput.put(getAliasId(alias, qb), viewInput);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2003,new ReadEntity(tab, parentViewInfo, parentViewInfo == null));
ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java,925,public static ReadEntity addInput(Set<ReadEntity> inputs, ReadEntity newInput) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4281,ArrayList<ColumnInfo> newSchema = new ArrayList<ColumnInfo>();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4301,for(String f : targetTableColNames) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4311,TypeCheckCtx tcCtx = new TypeCheckCtx(inputRR);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4312,CommonToken t = new CommonToken(HiveParser.TOK_NULL);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4313,t.setText("TOK_NULL");
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,4314,ExprNodeDesc exp = genExprNodeDesc(new ASTNode(t), inputRR, tcCtx);
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,138,hiveSiteURL = classLoader.getResource("hive-site.xml");
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,139,hivemetastoreSiteUrl = classLoader.getResource("hivemetastore-site.xml");
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,140,hiveServer2SiteUrl = classLoader.getResource("hiveserver2-site.xml");
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,379,Table table, ExpressionTree tree, Integer max) throws MetaException {
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,380,assert tree != null;
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,381,List<Object> params = new ArrayList<Object>();
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,382,List<String> joins = new ArrayList<String>();
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,384,boolean dbHasJoinCastBug = (dbType == DB.DERBY || dbType == DB.ORACLE);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,385,String sqlFilter = PartitionFilterGenerator.generateSqlFilter(
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,386,table, tree, params, joins, dbHasJoinCastBug, defaultPartName, dbType);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,387,if (sqlFilter == null) {
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,388,return null; // Cannot make SQL filter to push down.
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,390,Boolean isViewTable = isViewTable(table);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,391,return getPartitionsViaSqlFilterInternal(table.getDbName(), table.getTableName(),
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,392,isViewTable, sqlFilter, params, joins, max);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,395,public int getNumPartitionsViaSqlFilter(Table table, ExpressionTree tree) throws MetaException {
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,396,List<Object> params = new ArrayList<Object>();
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,397,List<String>joins = new ArrayList<String>();
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,400,String sqlFilter = PartitionFilterGenerator.generateSqlFilter(
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,401,table, tree, params, joins, dbHasJoinCastBug, defaultPartName, dbType);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,402,if (sqlFilter == null) {
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,403,return 0; // Cannot make SQL filter to push down.
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,405,return getNumPartitionsViaSqlFilterInternal(table.getDbName(), table.getTableName(), sqlFilter, params, joins);
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,830,private int getNumPartitionsViaSqlFilterInternal(String dbName, String tblName,
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,831,String sqlFilter, List<Object> paramsForFilter,
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,832,List<String> joinsForFilter) throws MetaException {
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,834,dbName = dbName.toLowerCase();
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,835,tblName = tblName.toLowerCase();
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,844,+ (sqlFilter == null ? "" : (" where " + sqlFilter));
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,846,Object[] params = new Object[paramsForFilter.size() + 2];
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,849,for (int i = 0; i < paramsForFilter.size(); ++i) {
metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java,850,params[i + 2] = paramsForFilter.get(i);
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2415,result = directSql.getPartitionsViaSqlFilter(ctx.getTable(), exprTree, null);
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2417,if (result == null) {
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2419,List<String> partNames = new LinkedList<String>();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2420,hasUnknownPartitions.set(getPartitionNamesPrunedByExprNoTxn(
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2421,ctx.getTable(), expr, defaultPartitionName, maxParts, partNames));
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2422,result = directSql.getPartitionsViaSqlFilter(dbName, tblName, partNames);
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2424,return result;
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2664,setResult(getSqlResult(this));
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2670,setResult(getJdoResult(this));
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2693,private boolean setResult(T results) {
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2694,this.results = results;
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2695,return this.results != null;
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2742,public void disableDirectSql() {
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2743,this.doUseDirectSql = false;
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2827,return null;
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2832,return directSql.getNumPartitionsViaSqlFilter(ctx.getTable(), tree);
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2851,List<Partition> parts = directSql.getPartitionsViaSqlFilter(
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2852,ctx.getTable(), tree, (maxParts < 0) ? null : (int)maxParts);
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2853,if (parts == null) {
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2856,ctx.disableDirectSql();
metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java,2858,return parts;
ql/src/java/org/apache/hadoop/hive/llap/LlapOutputFormat.java,41,implements OutputFormat<K, V> {
ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java,148,RecordWriter[] outWriters;
ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java,185,public void setOutWriters(RecordWriter[] out) {
ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java,186,outWriters = out;
ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java,189,public RecordWriter[] getOutWriters() {
ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java,190,return outWriters;
ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java,1013,createBucketFiles(fsp);
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,80,private static ColumnVector createColumn(OrcProto.Type type,
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,81,int batchSize) {
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,156,cvb.cols[idx] = createColumn(types.get(columnMapping[idx]), batchSize);
beeline/src/java/org/apache/hive/beeline/BeeLine.java,142,public static final String BEELINE_DEFAULT_JDBC_URL = "jdbc:hive2://";
beeline/src/java/org/apache/hive/beeline/BeeLine.java,897,if (!execCommandWithPrefix("!connect " + BEELINE_DEFAULT_JDBC_URL + " '' ''")) {
beeline/src/java/org/apache/hive/beeline/Commands.java,1317,props.setProperty("url", saveUrl);
beeline/src/java/org/apache/hive/beeline/Commands.java,1320,props.setProperty("driver", driver);
beeline/src/java/org/apache/hive/beeline/Commands.java,1323,props.setProperty("user", user);
beeline/src/java/org/apache/hive/beeline/Commands.java,1326,props.setProperty("password", pass);
beeline/src/java/org/apache/hive/beeline/Commands.java,1400,String auth = getProperty(props, new String[] {"auth"});
beeline/src/java/org/apache/hive/beeline/Commands.java,1411,beeLine.info("Connecting to " + url);
beeline/src/java/org/apache/hive/beeline/Commands.java,1413,if (username == null) {
beeline/src/java/org/apache/hive/beeline/Commands.java,1414,username = beeLine.getConsoleReader().readLine("Enter username for " + url + ": ");
beeline/src/java/org/apache/hive/beeline/Commands.java,1416,props.setProperty("user", username);
beeline/src/java/org/apache/hive/beeline/Commands.java,1417,if (password == null) {
beeline/src/java/org/apache/hive/beeline/Commands.java,1418,password = beeLine.getConsoleReader().readLine("Enter password for " + url + ": ",
beeline/src/java/org/apache/hive/beeline/Commands.java,1419,new Character('*'));
beeline/src/java/org/apache/hive/beeline/Commands.java,1421,props.setProperty("password", password);
beeline/src/java/org/apache/hive/beeline/Commands.java,1426,if (auth != null) {
beeline/src/java/org/apache/hive/beeline/Commands.java,1427,props.setProperty("auth", auth);
jdbc/src/java/org/apache/hive/jdbc/Utils.java,40,class Utils {
jdbc/src/java/org/apache/hive/jdbc/Utils.java,45,static final String URL_PREFIX = "jdbc:hive2://";
jdbc/src/java/org/apache/hive/jdbc/Utils.java,66,static class JdbcConnectionParams {
jdbc/src/java/org/apache/hive/jdbc/Utils.java,79,static final String AUTH_TYPE = "auth";
jdbc/src/java/org/apache/hive/jdbc/Utils.java,81,static final String AUTH_QOP_DEPRECATED = "sasl.qop";
jdbc/src/java/org/apache/hive/jdbc/Utils.java,82,static final String AUTH_QOP = "saslQop";
jdbc/src/java/org/apache/hive/jdbc/Utils.java,83,static final String AUTH_SIMPLE = "noSasl";
jdbc/src/java/org/apache/hive/jdbc/Utils.java,84,static final String AUTH_TOKEN = "delegationToken";
jdbc/src/java/org/apache/hive/jdbc/Utils.java,85,static final String AUTH_USER = "user";
jdbc/src/java/org/apache/hive/jdbc/Utils.java,86,static final String AUTH_PRINCIPAL = "principal";
jdbc/src/java/org/apache/hive/jdbc/Utils.java,87,static final String AUTH_PASSWD = "password";
jdbc/src/java/org/apache/hive/jdbc/Utils.java,88,static final String AUTH_KERBEROS_AUTH_TYPE = "kerberosAuthType";
jdbc/src/java/org/apache/hive/jdbc/Utils.java,89,static final String AUTH_KERBEROS_AUTH_TYPE_FROM_SUBJECT = "fromSubject";
jdbc/src/java/org/apache/hive/jdbc/Utils.java,90,static final String ANONYMOUS_USER = "anonymous";
jdbc/src/java/org/apache/hive/jdbc/Utils.java,91,static final String ANONYMOUS_PASSWD = "anonymous";
jdbc/src/java/org/apache/hive/jdbc/Utils.java,92,static final String USE_SSL = "ssl";
jdbc/src/java/org/apache/hive/jdbc/Utils.java,93,static final String SSL_TRUST_STORE = "sslTrustStore";
jdbc/src/java/org/apache/hive/jdbc/Utils.java,94,static final String SSL_TRUST_STORE_PASSWORD = "trustStorePassword";
jdbc/src/java/org/apache/hive/jdbc/Utils.java,98,static final String TRANSPORT_MODE = "transportMode";
jdbc/src/java/org/apache/hive/jdbc/Utils.java,102,static final String HTTP_PATH = "httpPath";
jdbc/src/java/org/apache/hive/jdbc/Utils.java,103,static final String SERVICE_DISCOVERY_MODE = "serviceDiscoveryMode";
orc/src/java/org/apache/orc/FileMetaInfo.java,19,package org.apache.orc;
orc/src/java/org/apache/orc/FileMetaInfo.java,21,import java.nio.ByteBuffer;
orc/src/java/org/apache/orc/FileMetaInfo.java,22,import java.util.List;
orc/src/java/org/apache/orc/FileMetaInfo.java,29,public class FileMetaInfo {
orc/src/java/org/apache/orc/FileMetaInfo.java,30,public ByteBuffer footerMetaAndPsBuffer;
orc/src/java/org/apache/orc/FileMetaInfo.java,31,public final String compressionType;
orc/src/java/org/apache/orc/FileMetaInfo.java,32,public final int bufferSize;
orc/src/java/org/apache/orc/FileMetaInfo.java,33,public final int metadataSize;
orc/src/java/org/apache/orc/FileMetaInfo.java,34,public final ByteBuffer footerBuffer;
orc/src/java/org/apache/orc/FileMetaInfo.java,35,public final List<Integer> versionList;
orc/src/java/org/apache/orc/FileMetaInfo.java,36,public final OrcFile.WriterVersion writerVersion;
orc/src/java/org/apache/orc/FileMetaInfo.java,40,public FileMetaInfo(String compressionType, int bufferSize, int metadataSize,
orc/src/java/org/apache/orc/FileMetaInfo.java,41,ByteBuffer footerBuffer, OrcFile.WriterVersion writerVersion) {
orc/src/java/org/apache/orc/FileMetaInfo.java,42,this(compressionType, bufferSize, metadataSize, footerBuffer, null,
orc/src/java/org/apache/orc/FileMetaInfo.java,43,writerVersion, null);
orc/src/java/org/apache/orc/FileMetaInfo.java,47,public FileMetaInfo(String compressionType, int bufferSize, int metadataSize,
orc/src/java/org/apache/orc/FileMetaInfo.java,48,ByteBuffer footerBuffer, List<Integer> versionList,
orc/src/java/org/apache/orc/FileMetaInfo.java,49,OrcFile.WriterVersion writerVersion,
orc/src/java/org/apache/orc/FileMetaInfo.java,50,ByteBuffer fullFooterBuffer) {
orc/src/java/org/apache/orc/FileMetaInfo.java,51,this.compressionType = compressionType;
orc/src/java/org/apache/orc/FileMetaInfo.java,52,this.bufferSize = bufferSize;
orc/src/java/org/apache/orc/FileMetaInfo.java,53,this.metadataSize = metadataSize;
orc/src/java/org/apache/orc/FileMetaInfo.java,54,this.footerBuffer = footerBuffer;
orc/src/java/org/apache/orc/FileMetaInfo.java,55,this.versionList = versionList;
orc/src/java/org/apache/orc/FileMetaInfo.java,56,this.writerVersion = writerVersion;
orc/src/java/org/apache/orc/FileMetaInfo.java,57,this.footerMetaAndPsBuffer = fullFooterBuffer;
orc/src/java/org/apache/orc/FileMetaInfo.java,60,public OrcFile.WriterVersion getWriterVersion() {
orc/src/java/org/apache/orc/FileMetaInfo.java,61,return writerVersion;
orc/src/java/org/apache/orc/OrcFile.java,163,private FileMetaInfo fileMetaInfo; // TODO: this comes from some place.
orc/src/java/org/apache/orc/OrcFile.java,165,private FileMetadata fullFileMetadata; // Propagate from LLAP cache.
orc/src/java/org/apache/orc/OrcFile.java,171,public ReaderOptions fileMetaInfo(FileMetaInfo info) {
orc/src/java/org/apache/orc/OrcFile.java,172,fileMetaInfo = info;
orc/src/java/org/apache/orc/OrcFile.java,173,return this;
orc/src/java/org/apache/orc/OrcFile.java,186,public ReaderOptions fileMetadata(FileMetadata metadata) {
orc/src/java/org/apache/orc/OrcFile.java,187,this.fullFileMetadata = metadata;
orc/src/java/org/apache/orc/OrcFile.java,199,public FileMetaInfo getFileMetaInfo() {
orc/src/java/org/apache/orc/OrcFile.java,200,return fileMetaInfo;
orc/src/java/org/apache/orc/OrcFile.java,208,return fullFileMetadata;
orc/src/java/org/apache/orc/Reader.java,357,List<StripeStatistics> getStripeStatistics();
orc/src/java/org/apache/orc/impl/ReaderImpl.java,38,import org.apache.orc.FileMetaInfo;
orc/src/java/org/apache/orc/impl/ReaderImpl.java,39,import org.apache.orc.FileMetadata;
orc/src/java/org/apache/orc/impl/ReaderImpl.java,66,protected final CompressionCodec codec;
orc/src/java/org/apache/orc/impl/ReaderImpl.java,67,protected final int bufferSize;
orc/src/java/org/apache/orc/impl/ReaderImpl.java,68,private final List<OrcProto.StripeStatistics> stripeStats;
orc/src/java/org/apache/orc/impl/ReaderImpl.java,71,private final TypeDescription schema;
orc/src/java/org/apache/orc/impl/ReaderImpl.java,86,private final ByteBuffer footerMetaAndPsBuffer;
orc/src/java/org/apache/orc/impl/ReaderImpl.java,337,this.footerMetaAndPsBuffer = null;
orc/src/java/org/apache/orc/impl/ReaderImpl.java,339,FileMetaInfo footerMetaData;
orc/src/java/org/apache/orc/impl/ReaderImpl.java,340,if (options.getFileMetaInfo() != null) {
orc/src/java/org/apache/orc/impl/ReaderImpl.java,341,footerMetaData = options.getFileMetaInfo();
orc/src/java/org/apache/orc/impl/ReaderImpl.java,342,this.footerMetaAndPsBuffer = null;
orc/src/java/org/apache/orc/impl/ReaderImpl.java,344,footerMetaData = extractMetaInfoFromFooter(fs, path,
orc/src/java/org/apache/orc/impl/ReaderImpl.java,345,options.getMaxLength());
orc/src/java/org/apache/orc/impl/ReaderImpl.java,346,this.footerMetaAndPsBuffer = footerMetaData.footerMetaAndPsBuffer;
orc/src/java/org/apache/orc/impl/ReaderImpl.java,348,options.fileMetaInfo(footerMetaData);
orc/src/java/org/apache/orc/impl/ReaderImpl.java,349,MetaInfoObjExtractor rInfo =
orc/src/java/org/apache/orc/impl/ReaderImpl.java,350,new MetaInfoObjExtractor(footerMetaData.compressionType,
orc/src/java/org/apache/orc/impl/ReaderImpl.java,351,footerMetaData.bufferSize,
orc/src/java/org/apache/orc/impl/ReaderImpl.java,352,footerMetaData.metadataSize,
orc/src/java/org/apache/orc/impl/ReaderImpl.java,353,footerMetaData.footerBuffer
orc/src/java/org/apache/orc/impl/ReaderImpl.java,354,);
orc/src/java/org/apache/orc/impl/ReaderImpl.java,355,this.compressionKind = rInfo.compressionKind;
orc/src/java/org/apache/orc/impl/ReaderImpl.java,356,this.codec = rInfo.codec;
orc/src/java/org/apache/orc/impl/ReaderImpl.java,357,this.bufferSize = rInfo.bufferSize;
orc/src/java/org/apache/orc/impl/ReaderImpl.java,358,this.metadataSize = rInfo.metadataSize;
orc/src/java/org/apache/orc/impl/ReaderImpl.java,359,this.stripeStats = rInfo.metadata.getStripeStatsList();
orc/src/java/org/apache/orc/impl/ReaderImpl.java,360,this.types = rInfo.footer.getTypesList();
orc/src/java/org/apache/orc/impl/ReaderImpl.java,361,this.rowIndexStride = rInfo.footer.getRowIndexStride();
orc/src/java/org/apache/orc/impl/ReaderImpl.java,362,this.contentLength = rInfo.footer.getContentLength();
orc/src/java/org/apache/orc/impl/ReaderImpl.java,363,this.numberOfRows = rInfo.footer.getNumberOfRows();
orc/src/java/org/apache/orc/impl/ReaderImpl.java,364,this.userMetadata = rInfo.footer.getMetadataList();
orc/src/java/org/apache/orc/impl/ReaderImpl.java,365,this.fileStats = rInfo.footer.getStatisticsList();
orc/src/java/org/apache/orc/impl/ReaderImpl.java,366,this.versionList = footerMetaData.versionList;
orc/src/java/org/apache/orc/impl/ReaderImpl.java,367,this.writerVersion = footerMetaData.writerVersion;
orc/src/java/org/apache/orc/impl/ReaderImpl.java,368,this.stripes = convertProtoStripesToStripes(rInfo.footer.getStripesList());
orc/src/java/org/apache/orc/impl/ReaderImpl.java,395,private static OrcProto.Metadata extractMetadata(ByteBuffer bb, int metadataAbsPos,
orc/src/java/org/apache/orc/impl/ReaderImpl.java,428,private static FileMetaInfo extractMetaInfoFromFooter(FileSystem fs,
orc/src/java/org/apache/orc/impl/ReaderImpl.java,429,Path path,
orc/src/java/org/apache/orc/impl/ReaderImpl.java,430,long maxFileLength
orc/src/java/org/apache/orc/impl/ReaderImpl.java,431,) throws IOException {
orc/src/java/org/apache/orc/impl/ReaderImpl.java,433,ByteBuffer buffer = null, fullFooterBuffer = null;
orc/src/java/org/apache/orc/impl/ReaderImpl.java,434,OrcProto.PostScript ps = null;
orc/src/java/org/apache/orc/impl/ReaderImpl.java,435,OrcFile.WriterVersion writerVersion = null;
orc/src/java/org/apache/orc/impl/ReaderImpl.java,440,size = fs.getFileStatus(path).getLen();
orc/src/java/org/apache/orc/impl/ReaderImpl.java,462,writerVersion = extractWriterVersion(ps);
orc/src/java/org/apache/orc/impl/ReaderImpl.java,476,fullFooterBuffer = buffer.slice();
orc/src/java/org/apache/orc/impl/ReaderImpl.java,477,buffer.limit(footerSize + metadataSize);
orc/src/java/org/apache/orc/impl/ReaderImpl.java,481,fullFooterBuffer = buffer.slice();
orc/src/java/org/apache/orc/impl/ReaderImpl.java,482,buffer.limit(psOffset);
orc/src/java/org/apache/orc/impl/ReaderImpl.java,495,return new FileMetaInfo(
orc/src/java/org/apache/orc/impl/ReaderImpl.java,496,ps.getCompression().toString(),
orc/src/java/org/apache/orc/impl/ReaderImpl.java,497,(int) ps.getCompressionBlockSize(),
orc/src/java/org/apache/orc/impl/ReaderImpl.java,498,(int) ps.getMetadataLength(),
orc/src/java/org/apache/orc/impl/ReaderImpl.java,499,buffer,
orc/src/java/org/apache/orc/impl/ReaderImpl.java,500,ps.getVersionList(),
orc/src/java/org/apache/orc/impl/ReaderImpl.java,501,writerVersion,
orc/src/java/org/apache/orc/impl/ReaderImpl.java,502,fullFooterBuffer
orc/src/java/org/apache/orc/impl/ReaderImpl.java,503,);
orc/src/java/org/apache/orc/impl/ReaderImpl.java,506,protected static OrcFile.WriterVersion extractWriterVersion(OrcProto.PostScript ps) {
orc/src/java/org/apache/orc/impl/ReaderImpl.java,508,? getWriterVersion(ps.getWriterVersion()) : OrcFile.WriterVersion.ORIGINAL);
orc/src/java/org/apache/orc/impl/ReaderImpl.java,511,protected static List<StripeInformation> convertProtoStripesToStripes(
orc/src/java/org/apache/orc/impl/ReaderImpl.java,512,List<OrcProto.StripeInformation> stripes) {
orc/src/java/org/apache/orc/impl/ReaderImpl.java,513,List<StripeInformation> result = new ArrayList<StripeInformation>(stripes.size());
orc/src/java/org/apache/orc/impl/ReaderImpl.java,514,for (OrcProto.StripeInformation info : stripes) {
orc/src/java/org/apache/orc/impl/ReaderImpl.java,515,result.add(new StripeInformationImpl(info));
orc/src/java/org/apache/orc/impl/ReaderImpl.java,517,return result;
orc/src/java/org/apache/orc/impl/ReaderImpl.java,527,private static class MetaInfoObjExtractor{
orc/src/java/org/apache/orc/impl/ReaderImpl.java,528,final org.apache.orc.CompressionKind compressionKind;
orc/src/java/org/apache/orc/impl/ReaderImpl.java,529,final CompressionCodec codec;
orc/src/java/org/apache/orc/impl/ReaderImpl.java,530,final int bufferSize;
orc/src/java/org/apache/orc/impl/ReaderImpl.java,531,final int metadataSize;
orc/src/java/org/apache/orc/impl/ReaderImpl.java,532,final OrcProto.Metadata metadata;
orc/src/java/org/apache/orc/impl/ReaderImpl.java,533,final OrcProto.Footer footer;
orc/src/java/org/apache/orc/impl/ReaderImpl.java,535,MetaInfoObjExtractor(String codecStr, int bufferSize, int metadataSize,
orc/src/java/org/apache/orc/impl/ReaderImpl.java,536,ByteBuffer footerBuffer) throws IOException {
orc/src/java/org/apache/orc/impl/ReaderImpl.java,538,this.compressionKind = org.apache.orc.CompressionKind.valueOf(codecStr.toUpperCase());
orc/src/java/org/apache/orc/impl/ReaderImpl.java,539,this.bufferSize = bufferSize;
orc/src/java/org/apache/orc/impl/ReaderImpl.java,540,this.codec = WriterImpl.createCodec(compressionKind);
orc/src/java/org/apache/orc/impl/ReaderImpl.java,541,this.metadataSize = metadataSize;
orc/src/java/org/apache/orc/impl/ReaderImpl.java,543,int position = footerBuffer.position();
orc/src/java/org/apache/orc/impl/ReaderImpl.java,544,int footerBufferSize = footerBuffer.limit() - footerBuffer.position() - metadataSize;
orc/src/java/org/apache/orc/impl/ReaderImpl.java,546,this.metadata = extractMetadata(footerBuffer, position, metadataSize, codec, bufferSize);
orc/src/java/org/apache/orc/impl/ReaderImpl.java,547,this.footer = extractFooter(
orc/src/java/org/apache/orc/impl/ReaderImpl.java,548,footerBuffer, position + metadataSize, footerBufferSize, codec, bufferSize);
orc/src/java/org/apache/orc/impl/ReaderImpl.java,550,footerBuffer.position(position);
orc/src/java/org/apache/orc/impl/ReaderImpl.java,556,return footerMetaAndPsBuffer;
orc/src/java/org/apache/orc/impl/ReaderImpl.java,725,public List<StripeStatistics> getStripeStatistics() {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ExternalCache.java,39,import org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.FileInfo;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ExternalCache.java,47,import org.apache.orc.FileMetaInfo;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ExternalCache.java,78,throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ExternalCache.java,79,localCache.put(fileId, file, fileMetaInfo, orcReader);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ExternalCache.java,80,if (fileId != null) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ExternalCache.java,82,externalCacheSrc.getCache(conf).putFileMetadata(Lists.newArrayList(fileId),
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ExternalCache.java,83,Lists.newArrayList(((ReaderImpl)orcReader).getSerializedFileFooter()));
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ExternalCache.java,111,FileInfo[] result, ByteBuffer[] ppdResult) throws IOException, HiveException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ExternalCache.java,158,List<HdfsFileStatusWithId> files, FileInfo[] result, Long fileId) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ExternalCache.java,166,ByteBuffer bb, int ix, HdfsFileStatusWithId file, FileInfo[] result) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ExternalCache.java,168,result[ix] = createFileInfoFromMs(file, bb);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ExternalCache.java,178,int ix, FileInfo[] result, ByteBuffer[] ppdResult) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ExternalCache.java,183,result[ix] = createFileInfoFromMs(file, mpr.bufferForMetadata());
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ExternalCache.java,191,List<HdfsFileStatusWithId> files, FileInfo[] result, HashMap<Long, Integer> posMap) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ExternalCache.java,296,private static FileInfo createFileInfoFromMs(
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ExternalCache.java,300,ReaderImpl.FooterInfo fi = null;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ExternalCache.java,303,fi = ReaderImpl.extractMetaInfoFromFooter(copy, fs.getPath());
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ExternalCache.java,312,return new FileInfo(fs.getModificationTime(), fs.getLen(), fi.getStripes(), fi.getMetadata(),
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ExternalCache.java,313,fi.getFooter().getTypesList(), fi.getFooter().getStatisticsList(), fi.getFileMetaInfo(),
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ExternalCache.java,314,fi.getFileMetaInfo().writerVersion, file.getFileId());
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,27,import org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.FileInfo;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,28,import org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.FooterCache;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,29,import org.apache.hadoop.hive.shims.HadoopShims.HdfsFileStatusWithId;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,30,import org.apache.orc.FileMetaInfo;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,38,class LocalCache implements FooterCache {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,40,private static boolean isDebugEnabled = LOG.isDebugEnabled();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,42,private final Cache<Path, FileInfo> cache;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,44,public LocalCache(int numThreads, int cacheStripeDetailsSize) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,50,.build();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,58,public void getAndValidate(List<HdfsFileStatusWithId> files, boolean isOriginal,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,59,FileInfo[] result, ByteBuffer[] ppdResult) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,63,for (HdfsFileStatusWithId fileWithId : files) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,67,Long fileId = fileWithId.getFileId();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,68,FileInfo fileInfo = cache.getIfPresent(path);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,69,if (isDebugEnabled) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,70,LOG.debug("Info " + (fileInfo == null ? "not " : "") + "cached for path: " + path);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,72,if (fileInfo == null) continue;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,74,|| (fileInfo.modificationTime == file.getModificationTime() &&
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,75,fileInfo.size == file.getLen())) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,76,result[i] = fileInfo;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,81,if (isDebugEnabled) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,83,+ fileInfo.modificationTime + ", CurrentModificationTime: "
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,84,+ file.getModificationTime() + ", CachedLength: " + fileInfo.size
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,90,public void put(Path path, FileInfo fileInfo) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,91,cache.put(path, fileInfo);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,96,throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,97,cache.put(file.getPath(), new FileInfo(file.getModificationTime(), file.getLen(),
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,98,orcReader.getStripes(), orcReader.getStripeStatistics(), orcReader.getTypes(),
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,99,orcReader.getOrcProtoFileStatistics(), fileMetaInfo, orcReader.getWriterVersion(),
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,100,fileId));
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,109,public boolean hasPpd() {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/LocalCache.java,110,return false;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFileFormatProxy.java,44,ReaderImpl.FooterInfo fi = ReaderImpl.extractMetaInfoFromFooter(fileMetadata, null);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFileFormatProxy.java,45,OrcProto.Footer footer = fi.getFooter();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFileFormatProxy.java,48,sarg, fi.getFileMetaInfo().getWriterVersion(),
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFileFormatProxy.java,49,footer.getTypesList(), fi.getMetadata(), stripeCount);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcFileFormatProxy.java,52,List<StripeInformation> stripes = fi.getStripes();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,57,import org.apache.orc.FileMetaInfo;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,577,localCache = new LocalCache(numThreads, cacheStripeDetailsSize);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,651,private final FileInfo fileInfo;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,657,SplitInfo(Context context, FileSystem fs, HdfsFileStatusWithId fileWithId, FileInfo fileInfo,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,664,this.fileInfo = fileInfo;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,672,public SplitInfo(Context context, FileSystem fs, FileStatus fileStatus, FileInfo fileInfo,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,676,fileInfo, isOriginal, deltas, hasBase, dir, covered, null);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,731,FileInfo[] infos = new FileInfo[files.size()];
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,737,cache.getAndValidate(files, isOriginal, infos, ppdResults);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,748,FileInfo info = infos[i];
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,751,if (info != null) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,757,result.add(new SplitInfo(context, dir.fs, file, info,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,924,deltas, -1);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,966,splits.add(new OrcSplit(dir, null, b, 0, new String[0], null, false, false, deltas, -1));
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1057,private final FileInfo fileInfo;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1059,private FileMetaInfo fileMetaInfo;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1081,this.fileInfo = splitInfo.fileInfo;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1135,OrcSplit createSplit(long offset, long length,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1136,FileMetaInfo fileMetaInfo) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1199,fileMetaInfo, isOriginal, hasBase, deltas, scaledProjSize);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1274,splits.add(createSplit(current.offset, current.length, null));
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1308,splits.add(createSplit(current.offset, current.length, fileMetaInfo));
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1315,splits, current, stripe.getOffset(), stripe.getLength(), fileMetaInfo);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1317,generateLastSplit(splits, current, fileMetaInfo);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1326,long length, FileMetaInfo fileMetaInfo) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1331,splits.add(createSplit(current.offset, current.length, fileMetaInfo));
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1342,splits.add(createSplit(current.offset, current.length, fileMetaInfo));
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1349,FileMetaInfo fileMetaInfo) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1351,splits.add(createSplit(current.offset, current.length, fileMetaInfo));
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1356,List<OrcProto.ColumnStatistics> colStatsLocal;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1357,List<OrcProto.Type> typesLocal;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1358,if (fileInfo != null) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1359,stripes = fileInfo.stripeInfos;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1360,stripeStats = fileInfo.stripeStats;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1361,fileMetaInfo = fileInfo.fileMetaInfo;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1362,typesLocal = types = fileInfo.types;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1363,colStatsLocal = fileInfo.fileStats;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1364,writerVersion = fileInfo.writerVersion;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1366,if (fileMetaInfo == null && context.footerInSplits) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1367,Reader orcReader = createOrcReader();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1368,fileInfo.fileMetaInfo = ((ReaderImpl) orcReader).getFileMetaInfo();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1369,assert fileInfo.stripeStats != null && fileInfo.types != null
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1370,&& fileInfo.writerVersion != null;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1373,context.footerCache.put(fsFileId, file, fileInfo.fileMetaInfo, orcReader);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1376,Reader orcReader = createOrcReader();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1377,stripes = orcReader.getStripes();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1378,typesLocal = types = orcReader.getTypes();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1379,colStatsLocal = orcReader.getOrcProtoFileStatistics();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1380,writerVersion = orcReader.getWriterVersion();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1381,stripeStats = orcReader.getStripeStatistics();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1382,fileMetaInfo = context.footerInSplits ?
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1383,((ReaderImpl) orcReader).getFileMetaInfo() : null;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1385,context.footerCache.put(fsFileId, file, fileMetaInfo, orcReader);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1389,projColsUncompressedSize = computeProjectionSize(typesLocal, colStatsLocal, includedCols, isOriginal);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1392,private Reader createOrcReader() throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1393,return OrcFile.createReader(file.getPath(),
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1394,OrcFile.readerOptions(context.conf).filesystem(fs).maxLength(file.getLen()));
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1604,static class FileInfo {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1605,final long modificationTime;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1606,final long size;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1607,final Long fileId;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1608,private final List<StripeInformation> stripeInfos;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1609,private FileMetaInfo fileMetaInfo;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1610,private final List<StripeStatistics> stripeStats;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1611,private final List<OrcProto.ColumnStatistics> fileStats;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1612,private final List<OrcProto.Type> types;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1613,private final OrcFile.WriterVersion writerVersion;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1616,FileInfo(long modificationTime, long size, List<StripeInformation> stripeInfos,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1617,List<StripeStatistics> stripeStats, List<OrcProto.Type> types,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1618,List<OrcProto.ColumnStatistics> fileStats, FileMetaInfo fileMetaInfo,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1619,OrcFile.WriterVersion writerVersion, Long fileId) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1620,this.modificationTime = modificationTime;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1621,this.size = size;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1622,this.fileId = fileId;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1623,this.stripeInfos = stripeInfos;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1624,this.fileMetaInfo = fileMetaInfo;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1625,this.stripeStats = stripeStats;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1626,this.types = types;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1627,this.fileStats = fileStats;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1628,this.writerVersion = writerVersion;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1653,OrcFile.readerOptions(conf)), conf, (FileSplit) inputSplit);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1657,OrcSplit split = (OrcSplit) inputSplit;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1762,reader = OrcFile.createReader(path, OrcFile.readerOptions(conf));
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1985,static final ByteBuffer NO_SPLIT_AFTER_PPD = ByteBuffer.wrap(new byte[0]);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1988,FileInfo[] result, ByteBuffer[] ppdResult) throws IOException, HiveException;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1992,throws IOException;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewSplit.java,23,import java.nio.ByteBuffer;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewSplit.java,28,import org.apache.hadoop.io.Text;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewSplit.java,31,import org.apache.orc.FileMetaInfo;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewSplit.java,38,private FileMetaInfo fileMetaInfo;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewSplit.java,43,private OrcFile.WriterVersion writerVersion;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewSplit.java,55,this.fileMetaInfo = inner.getFileMetaInfo();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewSplit.java,77,Text.writeString(out, fileMetaInfo.compressionType);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewSplit.java,78,WritableUtils.writeVInt(out, fileMetaInfo.bufferSize);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewSplit.java,79,WritableUtils.writeVInt(out, fileMetaInfo.metadataSize);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewSplit.java,82,ByteBuffer footerBuff = fileMetaInfo.footerBuffer;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewSplit.java,83,footerBuff.reset();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewSplit.java,86,WritableUtils.writeVInt(out, footerBuff.limit() - footerBuff.position());
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewSplit.java,87,out.write(footerBuff.array(), footerBuff.position(),
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewSplit.java,88,footerBuff.limit() - footerBuff.position());
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewSplit.java,89,WritableUtils.writeVInt(out, fileMetaInfo.writerVersion.getId());
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewSplit.java,112,String compressionType = Text.readString(in);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewSplit.java,113,int bufferSize = WritableUtils.readVInt(in);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewSplit.java,114,int metadataSize = WritableUtils.readVInt(in);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewSplit.java,117,int footerBuffSize = WritableUtils.readVInt(in);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewSplit.java,118,ByteBuffer footerBuff = ByteBuffer.allocate(footerBuffSize);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewSplit.java,119,in.readFully(footerBuff.array(), 0, footerBuffSize);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewSplit.java,120,OrcFile.WriterVersion writerVersion =
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewSplit.java,121,ReaderImpl.getWriterVersion(WritableUtils.readVInt(in));
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewSplit.java,123,fileMetaInfo = new FileMetaInfo(compressionType, bufferSize,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewSplit.java,124,metadataSize, footerBuff, writerVersion);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewSplit.java,128,FileMetaInfo getFileMetaInfo(){
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcNewSplit.java,129,return fileMetaInfo;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,24,import java.nio.ByteBuffer;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,28,import org.apache.orc.FileMetaInfo;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,30,import org.apache.hadoop.hive.ql.io.ColumnarSplit;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,34,import org.apache.hadoop.io.Text;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,46,private FileMetaInfo fileMetaInfo;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,68,FileMetaInfo fileMetaInfo, boolean isOriginal, boolean hasBase,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,69,List<AcidInputFormat.DeltaMetaData> deltas, long projectedDataSize) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,74,this.fileMetaInfo = fileMetaInfo;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,75,hasFooter = this.fileMetaInfo != null;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,85,super.write(out);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,100,Text.writeString(out, fileMetaInfo.compressionType);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,101,WritableUtils.writeVInt(out, fileMetaInfo.bufferSize);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,102,WritableUtils.writeVInt(out, fileMetaInfo.metadataSize);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,105,ByteBuffer footerBuff = fileMetaInfo.footerBuffer;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,106,footerBuff.reset();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,109,WritableUtils.writeVInt(out, footerBuff.limit() - footerBuff.position());
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,110,out.write(footerBuff.array(), footerBuff.position(),
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,111,footerBuff.limit() - footerBuff.position());
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,112,WritableUtils.writeVInt(out, fileMetaInfo.writerVersion.getId());
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,145,String compressionType = Text.readString(in);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,146,int bufferSize = WritableUtils.readVInt(in);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,147,int metadataSize = WritableUtils.readVInt(in);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,150,int footerBuffSize = WritableUtils.readVInt(in);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,151,ByteBuffer footerBuff = ByteBuffer.allocate(footerBuffSize);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,152,in.readFully(footerBuff.array(), 0, footerBuffSize);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,153,OrcFile.WriterVersion writerVersion =
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,154,ReaderImpl.getWriterVersion(WritableUtils.readVInt(in));
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,156,fileMetaInfo = new FileMetaInfo(compressionType, bufferSize,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,157,metadataSize, footerBuff, writerVersion);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,168,FileMetaInfo getFileMetaInfo(){
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,169,return fileMetaInfo;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,218,+ ", isOriginal=" + isOriginal + ", hasBase=" + hasBase + ", deltas="
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcSplit.java,219,+ (deltas == null ? 0 : deltas.size()) + "]";
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,23,import java.util.ArrayList;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,25,import java.util.List;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,27,import org.apache.orc.impl.BufferChunk;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,28,import org.apache.orc.CompressionCodec;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,29,import org.apache.orc.FileMetaInfo;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,30,import org.apache.orc.FileMetadata;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,31,import org.apache.orc.impl.InStream;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,32,import org.apache.orc.StripeInformation;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,33,import org.apache.orc.StripeStatistics;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,34,import org.slf4j.Logger;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,35,import org.slf4j.LoggerFactory;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,36,import org.apache.hadoop.fs.FSDataInputStream;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,39,import org.apache.hadoop.hive.common.io.DiskRange;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,42,import org.apache.orc.OrcProto;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,44,import com.google.common.collect.Lists;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,45,import com.google.protobuf.CodedInputStream;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,52,private static final int DIRECTORY_SIZE_GUESS = 16 * 1024;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,59,private ByteBuffer footerByteBuffer;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,62,private ByteBuffer footerMetaAndPsBuffer;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,86,public ReaderImpl(Path path,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,87,OrcFile.ReaderOptions options) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,89,FileMetadata fileMetadata = options.getFileMetadata();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,90,if (fileMetadata != null) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,91,this.inspector =  OrcStruct.createObjectInspector(0, fileMetadata.getTypes());
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,93,FileMetaInfo footerMetaData;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,94,if (options.getFileMetaInfo() != null) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,95,footerMetaData = options.getFileMetaInfo();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,97,footerMetaData = extractMetaInfoFromFooter(fileSystem, path,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,98,options.getMaxLength());
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,100,this.footerMetaAndPsBuffer = footerMetaData.footerMetaAndPsBuffer;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,101,MetaInfoObjExtractor rInfo =
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,102,new MetaInfoObjExtractor(footerMetaData.compressionType,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,103,footerMetaData.bufferSize,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,104,footerMetaData.metadataSize,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,105,footerMetaData.footerBuffer
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,106,);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,107,this.footerByteBuffer = footerMetaData.footerBuffer;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,108,this.inspector = rInfo.inspector;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,113,public static FooterInfo extractMetaInfoFromFooter(
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,114,ByteBuffer bb, Path srcPath) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,117,int baseOffset = bb.position();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,118,int lastByteAbsPos = baseOffset + bb.remaining() - 1;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,119,int psLen = bb.get(lastByteAbsPos) & 0xff;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,120,int psAbsPos = lastByteAbsPos - psLen;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,121,OrcProto.PostScript ps = extractPostScript(bb, srcPath, psLen, psAbsPos);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,122,assert baseOffset == bb.position();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,125,int footerSize = (int)ps.getFooterLength(), metadataSize = (int)ps.getMetadataLength(),
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,126,footerAbsPos = psAbsPos - footerSize, metadataAbsPos = footerAbsPos - metadataSize;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,127,String compressionType = ps.getCompression().toString();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,128,CompressionCodec codec =
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,129,WriterImpl.createCodec(org.apache.orc.CompressionKind.valueOf
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,130,(compressionType));
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,131,int bufferSize = (int)ps.getCompressionBlockSize();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,132,bb.position(metadataAbsPos);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,133,bb.mark();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,136,OrcProto.Metadata metadata = extractMetadata(
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,137,bb, metadataAbsPos, metadataSize, codec, bufferSize);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,138,List<StripeStatistics> stats = new ArrayList<>(metadata.getStripeStatsCount());
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,139,for (OrcProto.StripeStatistics ss : metadata.getStripeStatsList()) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,140,stats.add(new StripeStatistics(ss.getColStatsList()));
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,142,OrcProto.Footer footer = extractFooter(bb, footerAbsPos, footerSize, codec, bufferSize);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,143,bb.position(metadataAbsPos);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,144,bb.limit(psAbsPos);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,146,FileMetaInfo fmi = new FileMetaInfo(
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,147,compressionType, bufferSize, metadataSize, bb, extractWriterVersion(ps));
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,148,return new FooterInfo(stats, footer, fmi);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,151,private static OrcProto.Footer extractFooter(ByteBuffer bb, int footerAbsPos,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,152,int footerSize, CompressionCodec codec, int bufferSize) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,153,bb.position(footerAbsPos);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,154,bb.limit(footerAbsPos + footerSize);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,155,return OrcProto.Footer.parseFrom(InStream.createCodedInputStream("footer",
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,156,Lists.<DiskRange>newArrayList(new BufferChunk(bb, 0)), footerSize, codec, bufferSize));
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,159,private static OrcProto.Metadata extractMetadata(ByteBuffer bb, int metadataAbsPos,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,160,int metadataSize, CompressionCodec codec, int bufferSize) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,161,bb.position(metadataAbsPos);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,162,bb.limit(metadataAbsPos + metadataSize);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,163,return OrcProto.Metadata.parseFrom(InStream.createCodedInputStream("metadata",
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,164,Lists.<DiskRange>newArrayList(new BufferChunk(bb, 0)), metadataSize, codec, bufferSize));
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,167,private static OrcProto.PostScript extractPostScript(ByteBuffer bb, Path path,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,168,int psLen, int psAbsOffset) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,170,assert bb.hasArray();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,171,CodedInputStream in = CodedInputStream.newInstance(
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,172,bb.array(), bb.arrayOffset() + psAbsOffset, psLen);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,173,OrcProto.PostScript ps = OrcProto.PostScript.parseFrom(in);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,174,checkOrcVersion(LOG, path, ps.getVersionList());
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,177,switch (ps.getCompression()) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,178,case NONE:
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,179,break;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,180,case ZLIB:
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,181,break;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,182,case SNAPPY:
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,183,break;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,184,case LZO:
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,185,break;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,186,default:
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,187,throw new IllegalArgumentException("Unknown compression");
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,189,return ps;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,192,private static FileMetaInfo extractMetaInfoFromFooter(FileSystem fs,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,193,Path path,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,194,long maxFileLength
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,195,) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,196,FSDataInputStream file = fs.open(path);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,197,ByteBuffer buffer = null, fullFooterBuffer = null;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,198,OrcProto.PostScript ps = null;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,199,OrcFile.WriterVersion writerVersion = null;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,200,try {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,202,long size;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,203,if (maxFileLength == Long.MAX_VALUE) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,204,size = fs.getFileStatus(path).getLen();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,206,size = maxFileLength;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,210,int readSize = (int) Math.min(size, DIRECTORY_SIZE_GUESS);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,211,buffer = ByteBuffer.allocate(readSize);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,212,assert buffer.position() == 0;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,213,file.readFully((size - readSize),
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,214,buffer.array(), buffer.arrayOffset(), readSize);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,215,buffer.position(0);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,219,int psLen = buffer.get(readSize - 1) & 0xff;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,220,ensureOrcFooter(file, path, psLen, buffer);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,221,int psOffset = readSize - 1 - psLen;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,222,ps = extractPostScript(buffer, path, psLen, psOffset);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,224,int footerSize = (int) ps.getFooterLength();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,225,int metadataSize = (int) ps.getMetadataLength();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,226,writerVersion = extractWriterVersion(ps);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,229,int extra = Math.max(0, psLen + 1 + footerSize + metadataSize - readSize);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,230,if (extra > 0) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,232,ByteBuffer extraBuf = ByteBuffer.allocate(extra + readSize);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,233,file.readFully((size - readSize - extra), extraBuf.array(),
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,234,extraBuf.arrayOffset() + extraBuf.position(), extra);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,235,extraBuf.position(extra);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,237,extraBuf.put(buffer);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,238,buffer = extraBuf;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,239,buffer.position(0);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,240,fullFooterBuffer = buffer.slice();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,241,buffer.limit(footerSize + metadataSize);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,244,buffer.position(psOffset - footerSize - metadataSize);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,245,fullFooterBuffer = buffer.slice();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,246,buffer.limit(psOffset);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,250,buffer.mark();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,252,try {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,253,file.close();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,255,LOG.error("Failed to close the file after another error", ex);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,259,return new FileMetaInfo(
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,260,ps.getCompression().toString(),
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,261,(int) ps.getCompressionBlockSize(),
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,262,(int) ps.getMetadataLength(),
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,263,buffer,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,264,ps.getVersionList(),
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,265,writerVersion,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,266,fullFooterBuffer
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,267,);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,277,private static class MetaInfoObjExtractor{
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,278,final org.apache.orc.CompressionKind compressionKind;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,279,final CompressionCodec codec;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,280,final int bufferSize;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,281,final int metadataSize;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,282,final OrcProto.Metadata metadata;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,283,final OrcProto.Footer footer;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,284,final ObjectInspector inspector;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,286,MetaInfoObjExtractor(String codecStr, int bufferSize, int metadataSize,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,287,ByteBuffer footerBuffer) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,289,this.compressionKind = org.apache.orc.CompressionKind.valueOf(codecStr.toUpperCase());
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,290,this.bufferSize = bufferSize;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,291,this.codec = WriterImpl.createCodec(compressionKind);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,292,this.metadataSize = metadataSize;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,294,int position = footerBuffer.position();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,295,int footerBufferSize = footerBuffer.limit() - footerBuffer.position() - metadataSize;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,297,this.metadata = extractMetadata(footerBuffer, position, metadataSize, codec, bufferSize);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,298,this.footer = extractFooter(
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,299,footerBuffer, position + metadataSize, footerBufferSize, codec, bufferSize);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,301,footerBuffer.position(position);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,302,this.inspector = OrcStruct.createObjectInspector(0, footer.getTypesList());
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,306,public FileMetaInfo getFileMetaInfo() {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,307,return new FileMetaInfo(compressionKind.toString(), bufferSize,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,308,getMetadataSize(), footerByteBuffer, getVersionList(),
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,309,getWriterVersion(), footerMetaAndPsBuffer);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,314,public static final class FooterInfo {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,315,private final OrcProto.Footer footer;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,316,private final List<StripeStatistics> metadata;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,317,private final List<StripeInformation> stripes;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,318,private final FileMetaInfo fileMetaInfo;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,320,private FooterInfo(
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,321,List<StripeStatistics> metadata, OrcProto.Footer footer, FileMetaInfo fileMetaInfo) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,322,this.metadata = metadata;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,323,this.footer = footer;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,324,this.fileMetaInfo = fileMetaInfo;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,325,this.stripes = convertProtoStripesToStripes(footer.getStripesList());
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,328,public OrcProto.Footer getFooter() {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,329,return footer;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,332,public List<StripeStatistics> getMetadata() {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,333,return metadata;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,336,public FileMetaInfo getFileMetaInfo() {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,337,return fileMetaInfo;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,340,public List<StripeInformation> getStripes() {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,341,return stripes;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,347,return footerMetaAndPsBuffer;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcInputFormat.java,185,opts.fileMetaInfo(orcSplit.getFileMetaInfo());
service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java,240,if (hiveAuthFactory == null) {
service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java,264,if (hiveAuthFactory == null) {
service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java,283,if (hiveAuthFactory == null) {
service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java,388,KerberosNameShim fullKerberosName = ShimLoader.getHadoopShims().getKerberosNameShim(userName);
service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java,389,ret = fullKerberosName.getShortName();
shims/common/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java,539,authenticationMethod.set(AuthenticationMethod.KERBEROS);
shims/common/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java,543,if(saslServer.getMechanismName().equals("DIGEST-MD5")) {
shims/common/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java,553,Socket socket = ((TSocket)(saslTrans.getUnderlyingTransport())).getSocket();
shims/common/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java,554,remoteAddress.set(socket.getInetAddress());
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,86,private boolean loadCalled;
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,209,if (!loadCalled && spilled) {
ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java,293,loadCalled = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java,318,LOG.info("Total available memory is: " + memoryThreshold);
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java,344,memoryUsed = 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java,422,public long refreshMemoryUsed() {
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java,423,long memUsed = 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java,558,return refreshMemoryUsed() + writeBufferSize * numPartitionsInMem >= memoryThreshold;
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java,660,if (memoryThreshold < dataSize) {
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java,661,while (dataSize / numPartitions > memoryThreshold) {
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java,711,for (HashPartition hp : hashPartitions) {
ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java,504,boolean readAllColumns = readColIds.isEmpty() ? true : false;
ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java,505,newjob.setBoolean(ColumnProjectionUtils.READ_ALL_COLUMNS, readAllColumns);
ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java,510,LOG.info("{} = {}", ColumnProjectionUtils.READ_ALL_COLUMNS, readAllColumns);
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMetaStoreChecker.java,409,allDirs.put(path, null);
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMetaStoreChecker.java,432,allDirs.put(path, null);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTaskReporter.java,303,if (LOG.isDebugEnabled()) {
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTaskReporter.java,304,LOG.debug("Routing events from heartbeat response to task" + ", currentTaskAttemptId="
ql/src/java/org/apache/hadoop/hive/ql/exec/MapredContext.java,55,logger.debug("MapredContext initialized.");
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java,206,aborted.set(true);
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2030,outStream.write(createDb_str.toString().getBytes("UTF-8"));
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2079,outStream.writeBytes(createTab_stmt.toString());
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2227,outStream.writeBytes(createTab_stmt.render());
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,3046,OutputStreamWriter writer = new OutputStreamWriter(out, "UTF-8");
serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerDe.java,110,properties.setProperty(AvroSerdeUtils.AvroTableProperties.SCHEMA_LITERAL.getPropName(), schema.toString());
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,3211,fs.mkdirs(tempPath);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java,274,final TezConfiguration tezConfig = new TezConfiguration(conf);
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,579,Path placeholder = new Path(parentDir, "_placeholder");
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java,22,import com.google.common.base.Function;
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java,23,import com.google.common.collect.Iterators;
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java,24,import com.google.common.collect.Lists;
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java,26,import javax.security.auth.login.LoginException;
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java,843,addTempResources(conf, tmpResources, hdfsDirPathStr, LocalResourceType.FILE, getTempFilesFromConf(conf));
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,1266,List<URI> downloadedURLs = resolveAndDownload(value, convertToUnix);
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,1314,return resourceDownloader.resolveAndDownload(value, convertToUnix);
ql/src/java/org/apache/hadoop/hive/ql/util/ResourceDownloader.java,94,case OTHER: return Lists.newArrayList(
ql/src/java/org/apache/hadoop/hive/ql/util/ResourceDownloader.java,127,private enum UriType { IVY, FILE, OTHER };
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1068,private final List<OrcSplit> deltaSplits;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1295,if (includeStripe == null) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1296,includeStripe = new boolean[stripes.size()];
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1297,Arrays.fill(includeStripe, true);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1300,OffsetAndLength current = new OffsetAndLength();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1301,int idx = -1;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1302,for (StripeInformation stripe : stripes) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1303,idx++;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1305,if (!includeStripe[idx]) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1307,if (current.offset != -1) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1309,current.offset = -1;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1311,continue;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1314,current = generateOrUpdateSplit(
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,6651,conf.getVar(HiveConf.ConfVars.METASTORE_KERBEROS_PRINCIPAL));
service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java,125,conf.getVar(ConfVars.HIVE_SERVER2_KERBEROS_PRINCIPAL));
shims/common/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java,100,public Server createServer(String keytabFile, String principalConf) throws TTransportException {
shims/common/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java,101,return new Server(keytabFile, principalConf);
shims/common/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java,343,return new TUGIAssumingTransportFactory(transFactory, realUgi);
shims/common/src/main/java/org/apache/hadoop/hive/thrift/HadoopThriftAuthBridge.java,355,String kerberosName = realUgi.getUserName();
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java,603,if (conf.get(HCatConstants.HCAT_KEY_HIVE_CONF) == null) {
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java,604,conf.set(HCatConstants.HCAT_KEY_HIVE_CONF,
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java,605,HCatUtil.serialize(hiveConf.getAllProperties()));
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java,609,conf.get(HCatConstants.HCAT_KEY_HIVE_CONF));
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java,611,for (Map.Entry<Object, Object> prop : properties.entrySet()) {
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java,612,if (prop.getValue() instanceof String) {
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java,613,hiveConf.set((String) prop.getKey(), (String) prop.getValue());
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java,615,hiveConf.setInt((String) prop.getKey(),
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java,616,(Integer) prop.getValue());
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java,618,hiveConf.setBoolean((String) prop.getKey(),
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java,619,(Boolean) prop.getValue());
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java,621,hiveConf.setLong((String) prop.getKey(), (Long) prop.getValue());
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java,623,hiveConf.setFloat((String) prop.getKey(),
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java,624,(Float) prop.getValue());
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java,79,import javax.security.auth.login.LoginException;
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatBaseInputFormat.java,126,JobConf jobConf;
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/HCatBaseInputFormat.java,129,jobConf = HCatUtil.getJobConfFromContext(jobContext);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2847,if (depth > 0 && parent != null && wh.isWritable(parent) && wh.isEmpty(parent)) {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2848,wh.deleteDir(parent, true, mustPurge);
ql/src/java/org/apache/hadoop/hive/ql/exec/AbstractFileMergeOperator.java,197,checkPartitionsMatch(path);
ql/src/java/org/apache/hadoop/hive/ql/exec/AbstractFileMergeOperator.java,200,int depthDiff = path.depth() - tmpPath.depth();
ql/src/java/org/apache/hadoop/hive/ql/exec/AbstractFileMergeOperator.java,201,fixTmpPath(path, depthDiff);
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,334,String grpName, FsPermission perms) throws IOException {
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,371,applyGroupAndPerms(fs, partPath, perms, grpName, false);
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,381,applyGroupAndPerms(fs, partPath, perms, grpName, true);
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,400,private void applyGroupAndPerms(FileSystem fs, Path dir, FsPermission permission,
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,404,LOG.debug("applyGroupAndPerms : " + dir +
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,408,fs.setPermission(dir, permission);
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,410,for (FileStatus fileStatus : fs.listStatus(dir)) {
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,412,applyGroupAndPerms(fs, fileStatus.getPath(), permission, group, true);
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,414,fs.setPermission(fileStatus.getPath(), permission);
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,786,FileStatus tblStat = fs.getFileStatus(tblPath);
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,787,String grpName = tblStat.getGroup();
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,788,FsPermission perms = tblStat.getPermission();
hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/FileOutputCommitterContainer.java,944,applyGroupAndPerms(fs,new Path(p.getSd().getLocation()),tblStat.getPermission(),tblStat.getGroup(),true);
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,2723,HdfsUtils.setFullFileStatus(conf, fullDestStatus, srcGroup, destFs, destPath, false);
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,61,setFullFileStatus(conf, sourceStatus, null, fs, target, recursion);
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,77,removeBaseAclEntries(aclEntries);
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,80,aclEntries.add(newAclEntry(AclEntryScope.ACCESS, AclEntryType.USER, sourcePerm.getUserAction()));
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,81,aclEntries.add(newAclEntry(AclEntryScope.ACCESS, AclEntryType.GROUP, sourcePerm.getGroupAction()));
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,82,aclEntries.add(newAclEntry(AclEntryScope.ACCESS, AclEntryType.OTHER, sourcePerm.getOtherAction()));
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,96,if (aclEnabled) {
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,97,if (null != aclEntries) {
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,99,try {
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,101,String aclEntry = Joiner.on(",").join(aclEntries);
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,102,run(fsShell, new String[]{"-setfacl", "-R", "--set", aclEntry, target.toString()});
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,105,LOG.info("Skipping ACL inheritance: File system for path " + target + " " +
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,107,LOG.debug("The details are: " + e, e);
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,119,if (targetGroup == null ||
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,120,!group.equals(targetGroup)) {
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,124,if (aclEnabled) {
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,125,if (null != aclEntries) {
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,126,fs.setAcl(target, aclEntries);
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,154,private static void removeBaseAclEntries(List<AclEntry> entries) {
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,155,Iterables.removeIf(entries, new Predicate<AclEntry>() {
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,157,public boolean apply(AclEntry input) {
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,158,if (input.getName() == null) {
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,171,public static class HadoopFileStatus {
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,173,private final FileStatus fileStatus;
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,174,private final AclStatus aclStatus;
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,176,public HadoopFileStatus(Configuration conf, FileSystem fs, Path file) throws IOException {
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,178,FileStatus fileStatus = fs.getFileStatus(file);
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,179,AclStatus aclStatus = null;
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,180,if (Objects.equal(conf.get("dfs.namenode.acls.enabled"), "true")) {
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,182,try {
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,183,aclStatus = fs.getAclStatus(file);
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,185,LOG.info("Skipping ACL inheritance: File system for path " + file + " " +
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,187,LOG.debug("The details are: " + e, e);
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,190,this.aclStatus = aclStatus;
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,193,public FileStatus getFileStatus() {
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,194,return fileStatus;
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,142,host = connParams.getHost();
jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java,211,host = connParams.getHost();
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java,266,metastoreUris[i++] = tmpUri;
shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java,1158,return compareKeyStrength(zone1.getKeyName(), zone2.getKeyName());
shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java,1215,private int compareKeyStrength(String keyname1, String keyname2) throws IOException {
shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java,1216,KeyProvider.Metadata meta1, meta2;
shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java,1218,if (keyProvider == null) {
shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java,1219,throw new IOException("HDFS security key provider is not configured on your server.");
shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java,1222,meta1 = keyProvider.getMetadata(keyname1);
shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java,1223,meta2 = keyProvider.getMetadata(keyname2);
shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java,1225,if (meta1.getBitLength() < meta2.getBitLength()) {
shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java,1226,return -1;
llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapServiceDriver.java,70,import org.json.JSONObject;
ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java,100,List<Map<String, String>> inputTableInfo = new ArrayList<Map<String, String>>();
ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java,101,List<Map<String, String>> inputPartitionInfo = new ArrayList<Map<String, String>>();
ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java,106,Map<String, String> tableInfo = new LinkedHashMap<String, String>();
ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java,112,inputTableInfo.add(tableInfo);
ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java,115,Map<String, String> partitionInfo = new HashMap<String, String>();
ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java,120,inputPartitionInfo.add(partitionInfo);
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,559,boolean hasDecimalOrSecondVInt = hasDecimalOrSecondVInt(bytes[offset]);
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,560,long seconds = (long) TimestampWritable.getSeconds(bytes, offset);
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,561,int nanos = 0;
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,562,if (hasDecimalOrSecondVInt) {
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,563,nanos = TimestampWritable.getNanos(bytes, offset + 4);
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,564,if (hasSecondVInt(bytes[offset + 4])) {
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,565,seconds += LazyBinaryUtils.readVLongFromByteArray(bytes,
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,566,offset + 4 + WritableUtils.decodeVIntSize(bytes[offset + 4]));
llap-server/src/java/org/apache/hadoop/hive/llap/cache/BuddyAllocator.java,121,int arenaSizeVal = (arenaCount == 0) ? MAX_ARENA_SIZE : (int)(maxSizeVal / arenaCount);
llap-server/src/java/org/apache/hadoop/hive/llap/cache/BuddyAllocator.java,150,arenaSize = arenaSizeVal;
storage-api/src/java/org/apache/hadoop/hive/common/DiskRangeInfo.java,24,import com.google.common.collect.Lists;
storage-api/src/java/org/apache/hadoop/hive/common/DiskRangeInfo.java,34,this.diskRanges = Lists.newArrayList();
storage-api/src/java/org/apache/hadoop/hive/ql/util/JavaDataModel.java,21,import com.google.common.annotations.VisibleForTesting;
storage-api/src/java/org/apache/hive/common/util/BloomFilter.java,24,import static com.google.common.base.Preconditions.checkArgument;
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java,969,FileSystem sourceFS = src.getFileSystem(conf);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java,989,if (src != null && !checkPreExisting(src, dest, conf)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java,1001,&& checkOrWaitForTheFile(src, dest, conf, notifierOld, 1, 150, false)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java,1018,src, dest, conf, notifierOld, waitAttempts, sleepInterval, true)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java,1033,public boolean checkOrWaitForTheFile(Path src, Path dest, Configuration conf, Object notifier,
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java,1034,int waitAttempts, long sleepInterval, boolean doLog) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java,1036,if (checkPreExisting(src, dest, conf)) return true;
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java,1055,return checkPreExisting(src, dest, conf); // One last check.
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java,372,JobConf parentConf = workToConf.get(unionWorkItems.get(0));
ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorMR.java,297,RunningJob rj = JobClient.runJob(job);
llap-server/src/java/org/apache/hadoop/hive/llap/configuration/LlapDaemonConfiguration.java,39,super(false);
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcColumnVectorProducer.java,71,OrcEncodedDataReader reader = new OrcEncodedDataReader(lowLevelCache, bufferManager,
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,135,private final Configuration conf;
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,163,OrcMetadataCache metadataCache, Configuration conf, FileSplit split, List<Integer> columnIds,
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,164,SearchArgument sarg, String[] columnNames, OrcEncodedDataConsumer consumer,
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,169,this.conf = conf;
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,489,long minAllocSize = HiveConf.getSizeVar(conf, ConfVars.LLAP_ALLOCATOR_MIN_ALLOC);
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,572,if (fileKey instanceof Long && HiveConf.getBoolVar(conf, ConfVars.LLAP_IO_USE_FILEID_PATH)) {
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,577,ReaderOptions opts = OrcFile.readerOptions(conf).filesystem(fs).fileMetadata(fileMetadata);
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,656,boolean useZeroCopy = (conf != null) && OrcConf.USE_ZEROCOPY.getBoolean(conf);
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,2627,LLAP_ALLOCATOR_MIN_ALLOC("hive.llap.io.allocator.alloc.min", "16Kb", new SizeValidator(),
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheMemoryManager.java,40,public LowLevelCacheMemoryManager(
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheMemoryManager.java,41,Configuration conf, LowLevelCachePolicy evictor, LlapDaemonCacheMetrics metrics) {
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheMemoryManager.java,42,this(HiveConf.getSizeVar(conf, ConfVars.LLAP_IO_MEMORY_MAX_SIZE), evictor, metrics);
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheMemoryManager.java,52,metrics.setCacheCapacityTotal(maxSize);
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelFifoCachePolicy.java,37,public LowLevelFifoCachePolicy(Configuration conf) {
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java,69,public LowLevelLrfuCachePolicy(Configuration conf) {
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java,70,this((int)HiveConf.getSizeVar(conf, ConfVars.LLAP_ALLOCATOR_MIN_ALLOC),
llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java,71,HiveConf.getSizeVar(conf, ConfVars.LLAP_IO_MEMORY_MAX_SIZE), conf);
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java,112,LowLevelCachePolicy cachePolicy =
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java,113,useLrfu ? new LowLevelLrfuCachePolicy(conf) : new LowLevelFifoCachePolicy(conf);
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java,116,conf, cachePolicy, cacheMetrics);
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java,122,metadataCache = new OrcMetadataCache(memManager, cachePolicy, useGapCache);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordProcessor.java,247,redWork.getVectorizedRowBatchCtx());
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordSource.java,464,+ tag + ") " + rowString, e);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,870,ObjectInspector keyObjectInspector = reduceWork.getKeyObjectInspector();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,871,if (keyObjectInspector == null || !(keyObjectInspector instanceof StructObjectInspector)) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,872,return false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,874,StructObjectInspector keyStructObjectInspector = (StructObjectInspector)keyObjectInspector;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,875,List<? extends StructField> keyFields = keyStructObjectInspector.getAllStructFieldRefs();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,878,if (reduceWork.getNeedsTagging()) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,883,ObjectInspector valueObjectInspector = reduceWork.getValueObjectInspector();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,884,if (valueObjectInspector == null ||
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,885,!(valueObjectInspector instanceof StructObjectInspector)) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,888,StructObjectInspector valueStructObjectInspector = (StructObjectInspector)valueObjectInspector;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,889,List<? extends StructField> valueFields = valueStructObjectInspector.getAllStructFieldRefs();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,895,for (StructField field: valueFields) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,896,reduceColumnNames.add(Utilities.ReduceField.VALUE.toString() + "." + field.getFieldName());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,897,reduceTypeInfos.add(TypeInfoUtils.getTypeInfoFromTypeString(field.getFieldObjectInspector().getTypeName()));
ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceWork.java,106,private ObjectInspector getObjectInspector(TableDesc desc) {
ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceWork.java,107,ObjectInspector objectInspector;
ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceWork.java,108,try {
ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceWork.java,109,Deserializer deserializer = ReflectionUtil.newInstance(desc
ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceWork.java,110,.getDeserializerClass(), null);
ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceWork.java,111,SerDeUtils.initializeSerDe(deserializer, null, desc.getProperties(), null);
ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceWork.java,112,objectInspector = deserializer.getObjectInspector();
ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceWork.java,114,return null;
ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceWork.java,116,return objectInspector;
ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceWork.java,119,public ObjectInspector getKeyObjectInspector() {
ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceWork.java,120,if (keyObjectInspector == null) {
ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceWork.java,121,keyObjectInspector = getObjectInspector(keyDesc);
ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceWork.java,123,return keyObjectInspector;
ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceWork.java,127,public ObjectInspector getValueObjectInspector() {
ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceWork.java,128,if (needsTagging) {
ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceWork.java,129,return null;
ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceWork.java,131,if (valueObjectInspector == null) {
ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceWork.java,132,valueObjectInspector = getObjectInspector(tagToValueDesc.get(tag));
ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceWork.java,134,return valueObjectInspector;
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,102,private final Map<LlapNodeId, AMNodeInfo> knownAppMasters = new HashMap<>();
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,176,LOG.trace("Registering for heartbeat: " + amLocation + ":" + port + " for queryIdentifier=" + queryIdentifier);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,181,amNodeInfo = knownAppMasters.get(amNodeId);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,186,knownAppMasters.put(amNodeId, amNodeInfo);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,192,amNodeInfo.setCurrentQueryIdentifier(queryIdentifier);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,197,public void unregisterTask(String amLocation, int port) {
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,199,LOG.trace("Un-registering for heartbeat: " + amLocation + ":" + port);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,202,LlapNodeId amNodeId = LlapNodeId.getInstance(amLocation, port);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,204,amNodeInfo = knownAppMasters.get(amNodeId);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,254,amNodeInfo.amNodeId, amNodeInfo.getCurrentQueryIdentifier(), amNodeInfo.getTaskCount(),
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,255,amNodeInfo.hasAmFailed(), amNodeInfo);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,257,knownAppMasters.remove(amNodeInfo.amNodeId);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,341,QueryIdentifier currentQueryIdentifier = amNodeInfo.getCurrentQueryIdentifier();
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,352,if (LOG.isDebugEnabled()) {
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,353,LOG.debug("Skipping node heartbeat to AM: " + amNodeInfo + ", since ref count is 0");
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,372,private QueryIdentifier currentQueryIdentifier;
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,386,this.currentQueryIdentifier = currentQueryIdentifier;
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,437,int getTaskCount() {
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,438,return taskCount.get();
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,441,public synchronized QueryIdentifier getCurrentQueryIdentifier() {
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,442,return currentQueryIdentifier;
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,445,public synchronized void setCurrentQueryIdentifier(QueryIdentifier queryIdentifier) {
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,446,this.currentQueryIdentifier = queryIdentifier;
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,472,return "AMInfo: " + amNodeId + ", taskCount=" + getTaskCount();
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java,301,this.amReporter.unregisterTask(request.getAmHost(), request.getAmPort());
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java,184,int numHandlers = conf.getInt(TezConfiguration.TEZ_AM_TASK_LISTENER_THREAD_COUNT,
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java,185,TezConfiguration.TEZ_AM_TASK_LISTENER_THREAD_COUNT_DEFAULT);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,10632,processPositionAlias(ast);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,11876,private void processPositionAlias(ASTNode ast) throws SemanticException {
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapContainerLauncher.java,33,LOG.info("No-op launch for container: " + containerLaunchRequest.getContainerId() +
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapContainerLauncher.java,40,LOG.info("DEBUG: Ignoring STOP_REQUEST for event: " + containerStopRequest);
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapContainerLauncher.java,41,getContext().containerStopRequested(containerStopRequest.getContainerId());
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,666,LOG.debug("Ignoring deallocateContainer for containerId: " + containerId);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,220,AMNodeInfo amNodeInfo =
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,221,new AMNodeInfo(amNodeId, user, jobToken, queryIdentifier, retryPolicy, retryTimeout, socketFactory,
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,222,conf);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,249,if (amNodeInfo.getTaskCount() == 0 || amNodeInfo.hasAmFailed()) {
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,259,amNodeInfo.stopUmbilical();
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,262,long next = System.currentTimeMillis() + heartbeatInterval;
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,263,amNodeInfo.setNextHeartbeatTime(next);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,264,pendingHeartbeatQueeu.add(amNodeInfo);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,265,ListenableFuture<Void> future = executor.submit(new AMHeartbeatCallable(amNodeInfo));
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,266,Futures.addCallback(future, new FutureCallback<Void>() {
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,268,public void onSuccess(Void result) {
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,273,public void onFailure(Throwable t) {
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,274,QueryIdentifier currentQueryIdentifier = amNodeInfo.getCurrentQueryIdentifier();
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,275,amNodeInfo.setAmFailed(true);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,276,LOG.warn("Heartbeat failed to AM {}. Killing all other tasks for the query={}",
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,278,queryFailedHandler.queryFailed(currentQueryIdentifier);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/ContainerRunnerImpl.java,313,List<QueryFragmentInfo> knownFragments = queryTracker.queryComplete(
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/ContainerRunnerImpl.java,314,queryIdentifier, request.getDeleteDelay(), false);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/ContainerRunnerImpl.java,315,LOG.info("DBG: Pending fragment count for completed query {} = {}", queryIdentifier,
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/ContainerRunnerImpl.java,317,for (QueryFragmentInfo fragmentInfo : knownFragments) {
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/ContainerRunnerImpl.java,318,LOG.info("Issuing killFragment for completed query {} {}", queryIdentifier,
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/ContainerRunnerImpl.java,320,executorService.killFragment(fragmentInfo.getFragmentIdentifierString());
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/QueryTracker.java,121,String user, SignableVertexSpec vertex, Token<JobTokenIdentifier> appToken,
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/QueryTracker.java,195,List<QueryFragmentInfo> queryComplete(QueryIdentifier queryIdentifier, long deleteDelay,
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/QueryTracker.java,211,return Collections.emptyList();
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/QueryTracker.java,230,return queryInfo.getRegisteredFragments();
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,129,this.socketFactory = NetUtils.getDefaultSocketFactory(conf);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/ContainerRunnerImpl.java,222,vertex.getUser(), vertex, jobToken, fragmentIdString, tokenInfo);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/QueryTracker.java,122,String fragmentIdString, LlapTokenInfo tokenInfo) throws IOException {
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java,46,import org.apache.hadoop.security.SecurityUtil;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java,1170,cs.setAvgColLen(StatsUtils.getAvgColLenOfFixedLengthTypes(colType));
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,427,partCS.setAvgColLen(StatsUtils.getAvgColLenOfVariableLengthTypes(conf,
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,546,avgRowSize += getAvgColLenOfVariableLengthTypes(conf, oi, colTypeLowerCase);
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,808,public static long getAvgColLenOfVariableLengthTypes(HiveConf conf, ObjectInspector oi,
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,875,return 0;
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,898,int avgColLen = (int) getAvgColLenOfVariableLengthTypes(conf, oi, colTypeLowerCase);
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,901,int avgColLen = (int) getAvgColLenOfVariableLengthTypes(conf, oi, colTypeLowerCase);
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,1006,return 0;
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,1228,ObjectInspector oi = null;
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,1247,oi = encd.getWritableObjectInspector();
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,1268,colName = encd.getName();
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,1269,colType = serdeConstants.VOID_TYPE_NAME;
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,1272,colName = encd.getName();
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,1273,colType = encd.getTypeString();
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,1275,oi = encd.getWritableObjectInspector();
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,1284,oi = engfd.getWritableObjectInspector();
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,1292,oi = encd.getWritableObjectInspector();
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,1300,oi = enfd.getWritableObjectInspector();
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,1313,|| colType.startsWith(serdeConstants.UNION_TYPE_NAME)) {
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,1314,avgColSize = getAvgColLenOfVariableLengthTypes(conf, oi, colType);
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,1316,avgColSize = getAvgColLenOfFixedLengthTypes(colType);
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,1459,long nonNullCount = numRows - cs.getNumNulls();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFAverage.java,335,private static class AverageAggregationBuffer<TYPE> implements AggregationBuffer {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFAverage.java,336,private Object previousValue;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFAverage.java,337,private long count;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java,371,return cs.getNumTrues();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java,376,return stats.getNumRows() / 2;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java,382,return 0;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java,384,return stats.getNumRows();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java,469,float factor = 1;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java,473,float columnFactor = dvs == 0 ? 0.5f : ((float)dvs / numRows) * values.get(i).size();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java,474,factor *= columnFactor;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java,1771,LOG.info("STATS-" + jop.toString() + ": Overflow in number of rows."
ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java,1775,LOG.info("STATS-" + jop.toString() + ": Equals 0 in number of rows."
ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java,2155,LOG.info("STATS-" + op.toString() + ": Overflow in number of rows."
ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java,2160,LOG.info("STATS-" + op.toString() + ": Equals 0 in number of rows."
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java,741,long capacity = refs.length << 1;
ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/BytesBytesMultiHashMap.java,742,expandAndRehashImpl(capacity);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,403,ParseDriver pd = new ParseDriver();
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,404,ASTNode tree = pd.parse(command, ctx);
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,405,tree = ParseUtils.findRootNonNullToken(tree);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTBuilder.java,39,class ASTBuilder {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTBuilder.java,41,static ASTBuilder construct(int tokenType, String text) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTBuilder.java,47,static ASTNode createAST(int tokenType, String text) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTBuilder.java,51,static ASTNode destNode() {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTBuilder.java,84,static ASTNode join(ASTNode left, ASTNode right, JoinRelType joinType, ASTNode cond,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTBuilder.java,111,static ASTNode subQuery(ASTNode qry, String alias) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTBuilder.java,116,static ASTNode qualifiedName(String tableName, String colName) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTBuilder.java,125,static ASTNode unqualifiedName(String colName) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTBuilder.java,131,static ASTNode where(ASTNode cond) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTBuilder.java,135,static ASTNode having(ASTNode cond) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTBuilder.java,139,static ASTNode limit(Object offset, Object limit) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTBuilder.java,145,static ASTNode selectExpr(ASTNode expr, String alias) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTBuilder.java,150,static ASTNode literal(RexLiteral literal) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTBuilder.java,154,static ASTNode literal(RexLiteral literal, boolean useTypeQualInLiteral) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTBuilder.java,288,ASTNode node() {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTBuilder.java,292,ASTBuilder add(int tokenType, String text) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTBuilder.java,297,ASTBuilder add(ASTBuilder b) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/ASTBuilder.java,302,ASTBuilder add(ASTNode n) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/index/RewriteParseContextGenerator.java,65,ParseDriver pd = new ParseDriver();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/index/RewriteParseContextGenerator.java,66,ASTNode tree = pd.parse(command, ctx);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/index/RewriteParseContextGenerator.java,67,tree = ParseUtils.findRootNonNullToken(tree);
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,660,static class ASTSearcher {
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsAutoGatherContext.java,132,ParseDriver pd = new ParseDriver();
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsAutoGatherContext.java,133,ASTNode tree = pd.parse(analyzeCommand, ctx);
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsAutoGatherContext.java,134,tree = ParseUtils.findRootNonNullToken(tree);
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,279,ParseDriver pd = new ParseDriver();
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,282,rewrittenTree = pd.parse(rewrittenQuery, ctx);
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,286,rewrittenTree = ParseUtils.findRootNonNullToken(rewrittenTree);
ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java,80,public static ASTNode findRootNonNullToken(ASTNode tree) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2324,ParseDriver pd = new ParseDriver();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,10614,rewrittenTree = pd.parse(rewrittenQuery);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,10618,rewrittenTree = ParseUtils.findRootNonNullToken(rewrittenTree);
ql/src/java/org/apache/hadoop/hive/ql/tools/LineageInfo.java,112,ParseDriver pd = new ParseDriver();
ql/src/java/org/apache/hadoop/hive/ql/tools/LineageInfo.java,113,ASTNode tree = pd.parse(query);
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapRegistryService.java,61,String name = hosts.substring(1);
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapRegistryService.java,62,if (yarnRegistries.containsKey(name) && yarnRegistries.get(name).isInState(STATE.STARTED)) {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapRegistryService.java,63,registry = yarnRegistries.get(name);
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapRegistryService.java,68,yarnRegistries.put(name, registry);
service/src/java/org/apache/hive/service/cli/CLIService.java,407,String primaryCatalog,
service/src/java/org/apache/hive/service/cli/CLIService.java,408,String primarySchema, String primaryTable, String foreignCatalog,
service/src/java/org/apache/hive/service/cli/operation/Operation.java,208,public boolean isRunning() {
service/src/java/org/apache/hive/service/cli/operation/Operation.java,209,return OperationState.RUNNING.equals(state);
service/src/java/org/apache/hive/service/cli/operation/Operation.java,212,public boolean isFinished() {
service/src/java/org/apache/hive/service/cli/operation/Operation.java,213,return OperationState.FINISHED.equals(state);
service/src/java/org/apache/hive/service/cli/operation/Operation.java,216,public boolean isCanceled() {
service/src/java/org/apache/hive/service/cli/operation/Operation.java,217,return OperationState.CANCELED.equals(state);
service/src/java/org/apache/hive/service/cli/operation/Operation.java,220,public boolean isFailed() {
service/src/java/org/apache/hive/service/cli/operation/Operation.java,221,return OperationState.ERROR.equals(state);
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingMetaStoreClient.java,199,LOG.warn("MetaStoreClient lost connection. Attempting to reconnect.",
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingMetaStoreClient.java,200,caughtException);
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbLockManager.java,49,public class DbLockManager implements HiveLockManager{
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbLockManager.java,63,this.client = client;
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbLockManager.java,103,LockResponse res = client.lock(lock);
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbLockManager.java,115,res = client.checkLock(res.getLockid());
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbLockManager.java,206,return client.checkLock(extLockId).getState();
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbLockManager.java,219,client.unlock(lockId);
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbLockManager.java,286,return client.showLocks(showLocksRequest);
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java,58,public class DbTxnManager extends HiveTxnManagerImpl {
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java,63,private DbLockManager lockMgr = null;
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java,64,private SynchronizedMetaStoreClient client = null;
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java,69,private long txnId = 0;
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java,120,txnId = client.openTxn(user);
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java,133,lockMgr = new DbLockManager(client, conf);
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java,354,client.commitTxn(txnId);
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java,380,client.rollbackTxn(txnId);
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java,493,return client.getValidTxns(txnId);
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java,537,if (client == null) {
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java,538,if (conf == null) {
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java,539,throw new RuntimeException("Must call setHiveConf before any other " +
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java,542,try {
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java,543,Hive db = Hive.get(conf);
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java,544,client = new SynchronizedMetaStoreClient(db.getMSC());
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java,545,initHeartbeatExecutorService();
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java,547,throw new LockException(ErrorMsg.METASTORE_COULD_NOT_INITIATE.getMsg(), e);
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java,549,throw new LockException(ErrorMsg.METASTORE_COULD_NOT_INITIATE.getMsg(), e);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/SqlFunctionConverter.java,433,SqlOperator calciteOp = hiveToCalcite.get(hiveUdfName);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/SqlFunctionConverter.java,434,if (calciteOp == null) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/SqlFunctionConverter.java,435,CalciteUDFInfo uInf = getUDFInfo(hiveUdfName, calciteArgTypes, calciteRetType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,102,import org.apache.hadoop.hive.ql.udf.UDFConv;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,103,import org.apache.hadoop.hive.ql.udf.UDFFromUnixTime;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,104,import org.apache.hadoop.hive.ql.udf.UDFHex;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,105,import org.apache.hadoop.hive.ql.udf.UDFRegExpExtract;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,106,import org.apache.hadoop.hive.ql.udf.UDFRegExpReplace;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,107,import org.apache.hadoop.hive.ql.udf.UDFSign;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,108,import org.apache.hadoop.hive.ql.udf.UDFToBoolean;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,109,import org.apache.hadoop.hive.ql.udf.UDFToByte;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,110,import org.apache.hadoop.hive.ql.udf.UDFToDouble;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,111,import org.apache.hadoop.hive.ql.udf.UDFToFloat;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,112,import org.apache.hadoop.hive.ql.udf.UDFToInteger;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,113,import org.apache.hadoop.hive.ql.udf.UDFToLong;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,114,import org.apache.hadoop.hive.ql.udf.UDFToShort;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,115,import org.apache.hadoop.hive.ql.udf.UDFToString;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,674,castType = updatePrecision(inputTypeInfo, (DecimalTypeInfo) castType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,675,GenericUDFToDecimal castToDecimalUDF = new GenericUDFToDecimal();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,676,castToDecimalUDF.setTypeInfo(castType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,677,List<ExprNodeDesc> children = new ArrayList<ExprNodeDesc>();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,678,children.add(child);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,679,ExprNodeDesc desc = new ExprNodeGenericFuncDesc(castType, castToDecimalUDF, children);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,680,return desc;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,684,GenericUDF genericUdf = getGenericUDFForCast(castType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,685,List<ExprNodeDesc> children = new ArrayList<ExprNodeDesc>();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,686,children.add(child);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,687,ExprNodeDesc desc = new ExprNodeGenericFuncDesc(castType, genericUdf, children);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,688,return desc;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastBytesHashMap.java,102,int initialCapacity, float loadFactor, int writeBuffersSize) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastBytesHashMap.java,103,super(initialCapacity, loadFactor, writeBuffersSize);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastBytesHashMultiSet.java,95,int initialCapacity, float loadFactor, int writeBuffersSize) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastBytesHashMultiSet.java,96,super(initialCapacity, loadFactor, writeBuffersSize);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastBytesHashSet.java,82,int initialCapacity, float loadFactor, int writeBuffersSize) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastBytesHashSet.java,83,super(initialCapacity, loadFactor, writeBuffersSize);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastBytesHashTable.java,208,int initialCapacity, float loadFactor, int writeBuffersSize) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastBytesHashTable.java,209,super(initialCapacity, loadFactor, writeBuffersSize);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastHashMap.java,35,int initialCapacity, float loadFactor, int writeBuffersSize) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastHashMap.java,36,super(initialCapacity, loadFactor, writeBuffersSize);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastHashMultiSet.java,45,int initialCapacity, float loadFactor, int writeBuffersSize) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastHashMultiSet.java,46,super(initialCapacity, loadFactor, writeBuffersSize);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastHashSet.java,41,int initialCapacity, float loadFactor, int writeBuffersSize) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastHashSet.java,42,super(initialCapacity, loadFactor, writeBuffersSize);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastHashTable.java,54,int initialCapacity, float loadFactor, int writeBuffersSize) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastLongHashMap.java,110,int initialCapacity, float loadFactor, int writeBuffersSize) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastLongHashMap.java,112,initialCapacity, loadFactor, writeBuffersSize);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastLongHashMultiSet.java,99,int initialCapacity, float loadFactor, int writeBuffersSize) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastLongHashMultiSet.java,101,initialCapacity, loadFactor, writeBuffersSize);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastLongHashSet.java,95,int initialCapacity, float loadFactor, int writeBuffersSize) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastLongHashSet.java,97,initialCapacity, loadFactor, writeBuffersSize);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastLongHashTable.java,258,int initialCapacity, float loadFactor, int writeBuffersSize) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastLongHashTable.java,259,super(initialCapacity, loadFactor, writeBuffersSize);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastMultiKeyHashMap.java,53,int initialCapacity, float loadFactor, int writeBuffersSize) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastMultiKeyHashMap.java,54,super(initialCapacity, loadFactor, writeBuffersSize);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastMultiKeyHashMultiSet.java,51,int initialCapacity, float loadFactor, int writeBuffersSize) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastMultiKeyHashMultiSet.java,52,super(initialCapacity, loadFactor, writeBuffersSize);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastMultiKeyHashSet.java,51,int initialCapacity, float loadFactor, int writeBuffersSize) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastMultiKeyHashSet.java,52,super(initialCapacity, loadFactor, writeBuffersSize);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastStringHashMap.java,42,int initialCapacity, float loadFactor, int writeBuffersSize) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastStringHashMap.java,43,super(initialCapacity, loadFactor, writeBuffersSize);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastStringHashMultiSet.java,42,int initialCapacity, float loadFactor, int writeBuffersSize) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastStringHashMultiSet.java,43,super(initialCapacity, loadFactor, writeBuffersSize);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastStringHashSet.java,42,int initialCapacity, float loadFactor, int writeBuffersSize) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastStringHashSet.java,43,super(initialCapacity, loadFactor, writeBuffersSize);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastTableContainer.java,58,private final long keyCount;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastTableContainer.java,64,long keyCount) throws SerDeException {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastTableContainer.java,74,this.keyCount = keyCount;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastTableContainer.java,82,keyCountAdj, threshold, loadFactor, keyCount);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastTableContainer.java,117,newThreshold, loadFactor, writeBufferSize);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastTableContainer.java,122,newThreshold, loadFactor, writeBufferSize);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastTableContainer.java,127,newThreshold, loadFactor, writeBufferSize);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastTableContainer.java,137,newThreshold, loadFactor, writeBufferSize);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastTableContainer.java,142,newThreshold, loadFactor, writeBufferSize);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastTableContainer.java,147,newThreshold, loadFactor, writeBufferSize);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastTableContainer.java,157,newThreshold, loadFactor, writeBufferSize);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastTableContainer.java,162,newThreshold, loadFactor, writeBufferSize);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/fast/VectorMapJoinFastTableContainer.java,167,newThreshold, loadFactor, writeBufferSize);
llap-common/src/java/org/apache/hadoop/hive/llap/security/SecretManager.java,56,super(conf);
llap-common/src/java/org/apache/hadoop/hive/llap/security/SecretManager.java,160,setZkConfIfNotSet(zkConf, SecretManager.ZK_DTSM_ZK_AUTH_TYPE, "sasl");
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,825,return (metadataCache == null) ? range
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,826,: metadataCache.getIncompleteCbs(fileKey, range, baseOffset, factory, gotAllData);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezJobExecHelper.java,34,public static void killRunningJobs() {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezJobExecHelper.java,41,.getMethod("killRunningJobs");
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezJobExecHelper.java,42,method.invoke(null, null);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezJobExecHelper.java,44,catch (Exception e) {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezJobExecHelper.java,46,LOG.debug("Could not stop tez dags: ", e);
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,262,if (cboCtx.type == PreCboCtx.Type.CTAS) {
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,286,init(false);
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,287,if (cboCtx.type == PreCboCtx.Type.CTAS) {
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,289,setAST(newAST);
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,290,newAST = reAnalyzeCtasAfterCbo(newAST);
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,393,boolean result = isSupportedRoot && isSupportedType && getCreateViewDesc() == null
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,394,&& noBadTokens;
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,405,if (getCreateViewDesc() != null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,406,msg += "has create view; ";
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,615,case CTAS: {
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,701,ASTNode getOptimizedAST() throws SemanticException {
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,702,ASTNode optiqOptimizedAST = null;
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,942,HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_RESULTSET_USE_UNIQUE_COLUMN_NAMES));
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,2302,Map<ASTNode, ExprNodeDesc> astToExprNDescMap = TypeCheckProcFactory.genExprNode(
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,2303,grpbyExpr, new TypeCheckCtx(groupByInputRowResolver));
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,2452,Map<ASTNode, ExprNodeDesc> astToExprNDescMap = TypeCheckProcFactory.genExprNode(
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,2453,obASTExpr, new TypeCheckCtx(inputRR));
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,283,private CreateViewDesc createVwDesc;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,10647,child = analyzeCreateView(ast, qb);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,10758,if (createVwDesc != null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,10786,saveViewDefinition();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,10944,private void saveViewDefinition() throws SemanticException {
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcColumnVectorProducer.java,72,metadataCache, conf, split, columnIds, sarg, columnNames, edc, counters);
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,264,sarg.getLeaves(), columnNames, 0);
orc/src/java/org/apache/orc/impl/ReaderImpl.java,567,boolean[] include = options.getInclude();
orc/src/java/org/apache/orc/impl/ReaderImpl.java,569,if (include == null) {
orc/src/java/org/apache/orc/impl/ReaderImpl.java,570,include = new boolean[types.size()];
orc/src/java/org/apache/orc/impl/ReaderImpl.java,571,Arrays.fill(include, true);
orc/src/java/org/apache/orc/impl/ReaderImpl.java,572,options.include(include);
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,104,static int findColumns(String[] columnNames,
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,105,String columnName,
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,106,int rootColumn) {
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,107,for(int i=0; i < columnNames.length; ++i) {
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,108,if (columnName.equals(columnNames[i])) {
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,109,return i + rootColumn;
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,124,String[] columnNames,
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,125,int rootColumn) {
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,130,result[i] = findColumns(columnNames, colName, rootColumn);
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,138,this.included = options.getInclude();
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,139,included[0] = true;
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,152,options.getSchema(),included);
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,709,public SargApplier(SearchArgument sarg, String[] columnNames, long rowIndexStride,
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,713,filterColumns = mapSargColumnsToOrcInternalColIdx(sargLeaves, columnNames, 0);
orc/src/java/org/apache/orc/impl/RecordReaderImpl.java,716,sargColumns = new boolean[includedCount];
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,37,private final boolean[] included;
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,81,if (included != null && !included[readerType.getId()]) {
orc/src/java/org/apache/orc/impl/SchemaEvolution.java,152,readerType.toString(), readerType.getId()));
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,2032,if (fileType == null ||
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,2033,(included != null && !included[readerType.getId()])) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,310,List<OrcProto.Type> types = file.getTypes();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,311,options.include(genIncludedColumns(types, conf, isOriginal));
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,327,private static void includeColumnRecursive(List<OrcProto.Type> types,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,328,boolean[] result,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,329,int typeId,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,330,int rootColumn) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,331,result[typeId - rootColumn] = true;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,332,OrcProto.Type type = types.get(typeId);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,333,int children = type.getSubtypesCount();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,334,for(int i=0; i < children; ++i) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,335,includeColumnRecursive(types, result, type.getSubtypes(i), rootColumn);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,339,public static boolean[] genIncludedColumns(
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,340,List<OrcProto.Type> types, List<Integer> included, boolean isOriginal) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,341,int rootColumn = getRootColumn(isOriginal);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,342,int numColumns = types.size() - rootColumn;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,343,boolean[] result = new boolean[numColumns];
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,344,result[0] = true;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,345,OrcProto.Type root = types.get(rootColumn);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,346,for (int i = 0; i < root.getSubtypesCount(); ++i) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,347,if (included.contains(i)) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,348,includeColumnRecursive(types, result, root.getSubtypes(i), rootColumn);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,351,return result;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,360,public static boolean[] genIncludedColumns(
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,361,List<OrcProto.Type> types, Configuration conf, boolean isOriginal) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,364,return genIncludedColumns(types, included, isOriginal);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1721,private static boolean SCHEMA_TYPES_IS_ORIGINAL = true;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1756,readOptions.include(genIncludedColumns(schemaTypes, conf, SCHEMA_TYPES_IS_ORIGINAL));
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1757,setSearchArgument(readOptions, schemaTypes, conf, SCHEMA_TYPES_IS_ORIGINAL);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1854,private static boolean[] pickStripes(SearchArgument sarg, String[] sargColNames,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1855,OrcFile.WriterVersion writerVersion, boolean isOriginal, List<StripeStatistics> stripeStats,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1863,sargColNames, getRootColumn(isOriginal));
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java,397,if (options.getInclude() != null) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java,398,boolean[] orig = options.getInclude();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java,400,orig[0] = true;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java,401,boolean[] include = new boolean[orig.length + OrcRecordUpdater.FIELDS];
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java,402,Arrays.fill(include, 0, OrcRecordUpdater.FIELDS, true);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java,403,for(int i= 0; i < orig.length; ++i) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java,404,include[i + OrcRecordUpdater.FIELDS] = orig[i];
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java,406,result.include(include);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,358,boolean[] include = options.getInclude();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,360,if (include == null) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,361,include = new boolean[types.size()];
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,362,Arrays.fill(include, true);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/ReaderImpl.java,363,options.include(include);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcInputFormat.java,78,List<OrcProto.Type> types = file.getTypes();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcInputFormat.java,100,options.include(OrcInputFormat.genIncludedColumns(types, conf, true));
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java,489,return null;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSortProjectTransposeRule.java,20,import org.apache.calcite.plan.RelOptRule;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSortProjectTransposeRule.java,22,import org.apache.calcite.plan.RelOptUtil;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSortProjectTransposeRule.java,23,import org.apache.calcite.rel.RelCollation;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSortProjectTransposeRule.java,24,import org.apache.calcite.rel.RelCollationTraitDef;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSortProjectTransposeRule.java,25,import org.apache.calcite.rel.RelFieldCollation;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSortProjectTransposeRule.java,26,import org.apache.calcite.rel.RelNode;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSortProjectTransposeRule.java,27,import org.apache.calcite.rex.RexUtil;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSortProjectTransposeRule.java,28,import org.apache.calcite.util.mapping.Mappings;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSortProjectTransposeRule.java,33,import com.google.common.collect.ImmutableList;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSortProjectTransposeRule.java,35,public class HiveSortProjectTransposeRule extends RelOptRule {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSortProjectTransposeRule.java,67,public void onMatch(RelOptRuleCall call) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSortProjectTransposeRule.java,68,final HiveSortLimit sort = call.rel(0);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSortProjectTransposeRule.java,69,final HiveProject project = call.rel(1);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSortProjectTransposeRule.java,73,final Mappings.TargetMapping map =
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSortProjectTransposeRule.java,74,RelOptUtil.permutation(
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSortProjectTransposeRule.java,75,project.getProjects(), project.getInput().getRowType());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSortProjectTransposeRule.java,76,for (RelFieldCollation fc : sort.getCollation().getFieldCollations()) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSortProjectTransposeRule.java,77,if (map.getTargetOpt(fc.getFieldIndex()) < 0) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSortProjectTransposeRule.java,78,return;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSortProjectTransposeRule.java,83,final RelCollation newCollation =
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSortProjectTransposeRule.java,84,RelCollationTraitDef.INSTANCE.canonize(
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSortProjectTransposeRule.java,85,RexUtil.apply(map, sort.getCollation()));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSortProjectTransposeRule.java,88,final HiveSortLimit newSort = sort.copy(sort.getTraitSet().replace(newCollation),
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSortProjectTransposeRule.java,89,project.getInput(), newCollation, sort.offset, sort.fetch);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSortProjectTransposeRule.java,90,final RelNode newProject = project.copy(sort.getTraitSet(),
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSortProjectTransposeRule.java,91,ImmutableList.<RelNode>of(newSort));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveSortProjectTransposeRule.java,93,call.transformTo(newProject);
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,214,List<String> colTypes = new LinkedList<String>();
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,217,for (String colName : colNames) {
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,218,for (FieldSchema col: cols) {
ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java,220,colTypes.add(new String(col.getType()));
cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java,387,for (String oneCmd : line.split(";")) {
beeline/src/java/org/apache/hive/beeline/BeeLine.java,1045,Runtime.getRuntime().addShutdownHook(new Thread(new Runnable() {
cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java,811,Runtime.getRuntime().addShutdownHook(new Thread(new Runnable() {
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,21,import com.google.common.annotations.VisibleForTesting;
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,24,import java.util.ArrayList;
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,26,import java.util.Comparator;
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,28,import java.util.List;
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,30,import java.util.concurrent.atomic.AtomicBoolean;
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,32,import org.slf4j.Logger;
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,33,import org.slf4j.LoggerFactory;
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,48,private static final ShutdownHookManager MGR = new ShutdownHookManager();
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,52,private static final Logger LOG = LoggerFactory.getLogger(ShutdownHookManager.class);
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,55,MGR.addShutdownHookInternal(DELETE_ON_EXIT_HOOK, -1);
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,56,Runtime.getRuntime().addShutdownHook(
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,57,new Thread() {
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,59,public void run() {
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,60,MGR.shutdownInProgress.set(true);
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,61,for (Runnable hook : getShutdownHooksInOrder()) {
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,62,try {
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,63,hook.run();
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,65,LOG.warn("ShutdownHook '" + hook.getClass().getSimpleName() +
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,71,);
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,78,private static class HookEntry {
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,79,Runnable hook;
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,80,int priority;
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,82,public HookEntry(Runnable hook, int priority) {
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,83,this.hook = hook;
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,84,this.priority = priority;
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,88,public int hashCode() {
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,89,return hook.hashCode();
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,93,public boolean equals(Object obj) {
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,94,boolean eq = false;
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,95,if (obj != null) {
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,96,if (obj instanceof HookEntry) {
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,97,eq = (hook == ((HookEntry)obj).hook);
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,100,return eq;
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,105,private final Set<HookEntry> hooks =
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,106,Collections.synchronizedSet(new HashSet<HookEntry>());
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,108,private final AtomicBoolean shutdownInProgress = new AtomicBoolean(false);
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,111,private ShutdownHookManager() {
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,120,static List<Runnable> getShutdownHooksInOrder() {
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,121,return MGR.getShutdownHooksInOrderInternal();
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,124,private List<Runnable> getShutdownHooksInOrderInternal() {
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,125,List<HookEntry> list;
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,126,synchronized (MGR.hooks) {
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,127,list = new ArrayList<HookEntry>(MGR.hooks);
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,129,Collections.sort(list, new Comparator<HookEntry>() {
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,133,public int compare(HookEntry o1, HookEntry o2) {
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,134,return o2.priority - o1.priority;
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,137,List<Runnable> ordered = new ArrayList<Runnable>();
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,138,for (HookEntry entry: list) {
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,139,ordered.add(entry.hook);
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,141,return ordered;
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,157,MGR.addShutdownHookInternal(shutdownHook, priority);
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,160,private void addShutdownHookInternal(Runnable shutdownHook, int priority) {
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,161,if (shutdownHook == null) {
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,162,throw new IllegalArgumentException("shutdownHook cannot be NULL");
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,164,if (shutdownInProgress.get()) {
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,165,throw new IllegalStateException("Shutdown in progress, cannot add a shutdownHook");
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,167,hooks.add(new HookEntry(shutdownHook, priority));
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,181,return MGR.removeShutdownHookInternal(shutdownHook);
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,184,private boolean removeShutdownHookInternal(Runnable shutdownHook) {
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,185,if (shutdownInProgress.get()) {
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,186,throw new IllegalStateException("Shutdown in progress, cannot remove a shutdownHook");
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,188,return hooks.remove(new HookEntry(shutdownHook, 0));
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,197,public static boolean hasShutdownHook(Runnable shutdownHook) {
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,198,return MGR.hasShutdownHookInternal(shutdownHook);
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,201,public boolean hasShutdownHookInternal(Runnable shutdownHook) {
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,202,return hooks.contains(new HookEntry(shutdownHook, 0));
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,210,public static boolean isShutdownInProgress() {
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,211,return MGR.isShutdownInProgressInternal();
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,214,private boolean isShutdownInProgressInternal() {
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,215,return shutdownInProgress.get();
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,224,if (isShutdownInProgress()) {
common/src/java/org/apache/hive/common/util/ShutdownHookManager.java,234,if (isShutdownInProgress()) {
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,166,Runtime.getRuntime().addShutdownHook(cleanupHiveClientShutdownThread);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,6536,Runtime.getRuntime().addShutdownHook(new Thread() {
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java,176,Runtime.getRuntime().addShutdownHook(new Thread() {
ql/src/java/org/apache/hadoop/hive/ql/exec/spark/session/SparkSessionManagerImpl.java,49,Runtime.getRuntime().addShutdownHook(new Thread() {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezJobMonitor.java,137,Runtime.getRuntime().addShutdownHook(new Thread() {
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,77,Runtime.getRuntime().addShutdownHook(new Thread() {
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/CuratorFrameworkSingleton.java,37,Runtime.getRuntime().addShutdownHook(new Thread() {
service/src/java/org/apache/hive/service/server/HiveServer2.java,194,Runtime.getRuntime().addShutdownHook(new Thread() {
jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java,251,waitForOperationToComplete();
jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java,254,if (!stmtHandle.isHasResultSet()) {
jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java,321,void waitForOperationToComplete() throws SQLException {
jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java,323,TGetOperationStatusResp statusResp;
service-rpc/src/gen/thrift/gen-javabean/org/apache/hive/service/rpc/thrift/TGetOperationStatusResp.java,78,OPERATION_COMPLETED((short)8, "operationCompleted");
service-rpc/src/gen/thrift/gen-javabean/org/apache/hive/service/rpc/thrift/TGetOperationStatusResp.java,153,private static final _Fields optionals[] = {_Fields.OPERATION_STATE,_Fields.SQL_STATE,_Fields.ERROR_CODE,_Fields.ERROR_MESSAGE,_Fields.TASK_STATUS,_Fields.OPERATION_STARTED,_Fields.OPERATION_COMPLETED};
service-rpc/src/gen/thrift/gen-javabean/org/apache/hive/service/rpc/thrift/TGetOperationStatusResp.java,1072,oprot.writeBitSet(optionals, 7);
service-rpc/src/gen/thrift/gen-javabean/org/apache/hive/service/rpc/thrift/TGetOperationStatusResp.java,1102,BitSet incoming = iprot.readBitSet(7);
service/src/java/org/apache/hive/service/cli/OperationStatus.java,33,public OperationStatus(OperationState state, String taskStatus, long operationStarted, long operationCompleted, HiveSQLException operationException) {
service/src/java/org/apache/hive/service/cli/operation/Operation.java,147,return new OperationStatus(state, taskStatus, operationStart, operationComplete, operationException);
service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIServiceClient.java,373,resp.getOperationCompleted(), opException);
orc/src/java/org/apache/orc/impl/WriterImpl.java,2776,memoryManager.addedRow(batch.size);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,178,SettableUncompressedStream dictionary,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,179,boolean isFileCompressed, OrcProto.ColumnEncoding encoding) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,319,isFileCompressed, columnEncoding);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,336,OrcProto.ColumnEncoding encoding) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,417,columnEncoding);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,433,OrcProto.ColumnEncoding encoding, boolean skipCorrupt) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,520,columnEncoding, skipCorrupt);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,536,OrcProto.ColumnEncoding encoding) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,617,columnEncoding);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,633,SettableUncompressedStream data, boolean isFileCompressed) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,707,return new FloatStreamReader(columnIndex, present, data, isFileCompressed);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,723,SettableUncompressedStream data, boolean isFileCompressed) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,797,return new DoubleStreamReader(columnIndex, present, data, isFileCompressed);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,816,OrcProto.ColumnEncoding encoding) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,927,valueInStream,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,928,scaleInStream, isFileCompressed, columnEncoding);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,944,OrcProto.ColumnEncoding encoding) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1025,columnEncoding);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1045,boolean isFileCompressed, OrcProto.ColumnEncoding encoding) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1192,dictionary, isFileCompressed, columnEncoding);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1213,boolean isFileCompressed, OrcProto.ColumnEncoding encoding) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1360,dictionary, isFileCompressed, columnEncoding);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1376,SettableUncompressedStream data, boolean isFileCompressed) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1450,return new ByteStreamReader(columnIndex, present, data, isFileCompressed);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1468,OrcProto.ColumnEncoding encoding) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1567,columnEncoding);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1582,SettableUncompressedStream data, boolean isFileCompressed) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1656,return new BooleanStreamReader(columnIndex, present, data, isFileCompressed);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/Reader.java,70,EncodedReader encodedReader(
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/Reader.java,71,Object fileKey, DataCache dataCache, DataReader dataReader, PoolFactory pf) throws IOException;
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/DecimalColumnVector.java,20,import java.math.BigInteger;
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/DoubleColumnVector.java,20,import java.io.IOException;
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/LongColumnVector.java,20,import java.io.IOException;
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMetaStoreChecker.java,31,import java.util.concurrent.ExecutorService;
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMetaStoreChecker.java,378,final ExecutorService pool = conf.getInt(ConfVars.HIVE_MOVE_FILES_THREAD_COUNT.varname, 25) > 0 ? Executors
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMetaStoreChecker.java,379,.newFixedThreadPool(conf.getInt(ConfVars.HIVE_MOVE_FILES_THREAD_COUNT.varname, 25),
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMetaStoreChecker.java,381,: null;
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMetaStoreChecker.java,386,+ ((ThreadPoolExecutor) pool).getPoolSize());
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMetaStoreChecker.java,397,if (null == pool) {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/ServiceRegistry.java,28,public void start() throws IOException;
llap-client/src/java/org/apache/hadoop/hive/llap/registry/ServiceRegistry.java,35,public void stop() throws IOException;
llap-client/src/java/org/apache/hadoop/hive/llap/registry/ServiceRegistry.java,44,public String register() throws IOException;
llap-client/src/java/org/apache/hadoop/hive/llap/registry/ServiceRegistry.java,51,public void unregister() throws IOException;
llap-client/src/java/org/apache/hadoop/hive/llap/registry/ServiceRegistry.java,60,public ServiceInstanceSet getInstances(String component) throws IOException;
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapFixedRegistryImpl.java,264,public ServiceInstanceSet getInstances(String component) throws IOException {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapRegistryService.java,129,return this.registry.getInstances("LLAP");
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,640,public ServiceInstanceSet getInstances(String component) throws IOException {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,641,checkPathChildrenCache();
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,654,checkPathChildrenCache();
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,659,private synchronized void checkPathChildrenCache() throws IOException {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,661,zooKeeperClient.getState() == CuratorFrameworkState.STARTED,
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,665,if (instancesCache == null) {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,668,instancesCache.getListenable().addListener(new InstanceStateChangeListener(),
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,672,.build()));
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,674,this.instancesCache.start(PathChildrenCache.StartMode.BUILD_INITIAL_CACHE);
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,676,LOG.error("Unable to start curator PathChildrenCache. Exception: {}", e);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,11877,boolean isByPos = false;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,11878,if (HiveConf.getBoolVar(conf,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,11879,HiveConf.ConfVars.HIVE_GROUPBY_ORDERBY_POSITION_ALIAS) == true) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,11880,isByPos = true;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,11920,if (isByPos) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,11951,if (node.getToken().getType() == HiveParser.Number) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,11952,if( isByPos ) {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/ServiceInstanceSet.java,46,public ServiceInstance getInstance(String name);
llap-client/src/java/org/apache/hadoop/hive/llap/registry/ServiceInstanceSet.java,56,public Set<ServiceInstance> getByHost(String host);
llap-client/src/java/org/apache/hadoop/hive/llap/registry/ServiceInstanceSet.java,63,public int size();
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1514,int jobs = mrJobs
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1516,+ Utilities.getSparkTasks(plan.getRootTasks()).size();
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/Utils.java,38,LlapRegistryService serviceRegistry;
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/Utils.java,39,serviceRegistry = LlapRegistryService.getClient(conf);
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,22,import java.util.concurrent.Executors;
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,25,import org.apache.hadoop.conf.Configuration;
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,34,import org.apache.hadoop.util.StringUtils;
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/comparator/FirstInFirstOutComparator.java,56,if (fri1.getWithinDagPriority() < fri2.getWithinDagPriority()) {
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/comparator/FirstInFirstOutComparator.java,57,return -1;
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/comparator/FirstInFirstOutComparator.java,59,return 1;
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/comparator/ShortestJobFirstComparator.java,46,if (fri1.getWithinDagPriority() < fri2.getWithinDagPriority()) {
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/comparator/ShortestJobFirstComparator.java,47,return -1;
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/comparator/ShortestJobFirstComparator.java,49,return 1;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,7003,.mapDirToFop(ltd.getSourcePath(), (FileSinkOperator) output);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,7016,.mapDirToFop(tlocation, (FileSinkOperator) output);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,10806,if (HiveConf.getVar(conf, HiveConf.ConfVars.POSTEXECHOOKS).contains(
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,10808,ArrayList<Transform> transformations = new ArrayList<Transform>();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,10809,transformations.add(new HiveOpConverterPostProc());
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,10810,transformations.add(new Generator());
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,10811,for (Transform t : transformations) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,10812,pCtx = t.transform(pCtx);
ql/src/java/org/apache/hadoop/hive/ql/session/LineageState.java,46,private final Map<Path, FileSinkOperator> dirToFop;
ql/src/java/org/apache/hadoop/hive/ql/session/LineageState.java,63,dirToFop = new HashMap<Path, FileSinkOperator>();
ql/src/java/org/apache/hadoop/hive/ql/session/LineageState.java,74,public void mapDirToFop(Path dir, FileSinkOperator fop) {
ql/src/java/org/apache/hadoop/hive/ql/session/LineageState.java,88,FileSinkOperator fop = dirToFop.get(dir);
ql/src/java/org/apache/hadoop/hive/ql/session/LineageState.java,92,if (fop == null) {
ql/src/java/org/apache/hadoop/hive/ql/session/LineageState.java,96,List<ColumnInfo> signature = fop.getSchema().getSignature();
ql/src/java/org/apache/hadoop/hive/ql/session/LineageState.java,99,linfo.putDependency(dc, fs, index.getDependency(fop, signature.get(i++)));
ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java,3008,List<Map<String, String>> specs = getPartitionSpecs(getTable(tableName), ast);
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/Operation2Privilege.java,166,(ADMIN_PRIV_AR, null));
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/BytesColumnVector.java,109,return;
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/BytesColumnVector.java,113,int bufferSize = this.vector.length * (int)(estimatedValueSize * EXTRA_SPACE_FACTOR);
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/BytesColumnVector.java,114,if (bufferSize < DEFAULT_BUFFER_SIZE) {
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/BytesColumnVector.java,115,bufferSize = DEFAULT_BUFFER_SIZE;
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/BytesColumnVector.java,117,buffer = new byte[bufferSize];
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/BytesColumnVector.java,212,int newLength = 2 * buffer.length;
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/BytesColumnVector.java,213,while((nextFree + nextElemLength) > newLength) {
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/BytesColumnVector.java,214,newLength *= 2;
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/BytesColumnVector.java,218,byte[] newBuffer = new byte[newLength];
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/BytesColumnVector.java,219,System.arraycopy(buffer, 0, newBuffer, 0, nextFree);
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/BytesColumnVector.java,220,buffer = newBuffer;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorExpressionDescriptor.java,182,SCALAR(2);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1942,if (!(childExpr.get(2) instanceof ExprNodeConstantDesc) ||
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1987,cl = FilterLongColumnBetween.class;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1991,cl = FilterDoubleColumnBetween.class;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1995,cl = FilterStringColumnBetween.class;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1999,cl = FilterVarCharColumnBetween.class;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2003,cl = FilterCharColumnBetween.class;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2007,cl = FilterTimestampColumnBetween.class;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2011,cl = FilterDecimalColumnBetween.class;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2015,cl = FilterLongColumnBetween.class;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorExpression.java,34,INTERVAL_YEAR_MONTH, INTERVAL_DAY_TIME, OTHER;
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/comparator/ShortestJobFirstComparator.java,57,if (knownPending1 < knownPending2) {
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/comparator/ShortestJobFirstComparator.java,58,return -1;
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/comparator/ShortestJobFirstComparator.java,60,return 1;
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/comparator/ShortestJobFirstComparator.java,63,if (fri1.getFirstAttemptStartTime() < fri2.getFirstAttemptStartTime()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,879,return true;
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,2954,private final String altName;
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,39,import java.util.Iterator;
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,264,private TezSessionState getSession(HiveConf conf, boolean doOpen,
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,277,if (forceCreate || nonDefaultUser || !hasInitialSessions
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,278,|| ((queueName != null) && !queueName.isEmpty())) {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,384,public TezSessionState getSession(TezSessionState session, HiveConf conf, boolean doOpen,
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,385,boolean llap) throws Exception {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,386,return getSession(session, conf, doOpen, false, llap);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,445,public TezSessionState getSession(TezSessionState session, HiveConf conf, boolean doOpen,
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,446,boolean forceCreate, boolean llap) throws Exception {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,458,return getSession(conf, doOpen, forceCreate);
ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java,1362,throw new SemanticException(SemanticAnalyzer.generateErrorMessage(sqNode,
ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java,1363,ErrorMsg.UNSUPPORTED_SUBQUERY_EXPRESSION.getMsg()));
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingHMSHandler.java,78,baseHandler.init();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java,764,public int getFailedCompactionRetention() {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,501,LOG.debug("Going to rollback");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,502,dbConn.rollback();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,531,LOG.warn("Abort Transactions command only abort " + numAborted + " out of " +
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,598,ensureValidTxn(dbConn, txnid, stmt);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1111,LOG.error("No lock in " + LOCK_WAITING + " mode found for unlock(" + rqst + ")");
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1112,throw new NoSuchLockException("No such lock " + JavaUtils.lockIdToString(extLockId));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,2674,dbConn.rollback();
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,2683,dbConn.rollback();
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/ContainerRunnerImpl.java,183,if (LOG.isInfoEnabled()) {
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/ContainerRunnerImpl.java,184,LOG.info("Queueing container for execution: " + stringifySubmitRequest(request, vertex));
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/ContainerRunnerImpl.java,413,try {
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/ContainerRunnerImpl.java,414,knownFragments = queryTracker.queryComplete(queryIdentifier, -1, true);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/ContainerRunnerImpl.java,416,throw new RuntimeException(e); // Should never happen here, no permission check.
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/QueryTracker.java,129,throw new RuntimeException(
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/QueryTracker.java,131,+ vertexName + ", " + fragmentNumber + ", " + attemptNumber + "]");
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,315,if (llap && (this.numConcurrentLlapQueries > 0)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,316,llapQueue.release();
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,318,if (tezSessionState.isDefault() && tezSessionState instanceof TezSessionPoolSession) {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,320,+ " belongs to the pool. Put it back in");
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,321,SessionState sessionState = SessionState.get();
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,322,if (sessionState != null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,323,sessionState.setTezSession(null);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,325,TezSessionPoolSession poolSession = (TezSessionPoolSession)tezSessionState;
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,326,if (poolSession.returnAfterUse()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java,327,defaultQueuePool.put(poolSession);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,78,Use a cachedThreadPool so that a few AMs going down does not affect other AppMasters.
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,105,public AMReporter(AtomicReference<InetSocketAddress> localAddress,
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,111,ExecutorService rawExecutor = Executors.newCachedThreadPool(
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,112,new ThreadFactoryBuilder().setDaemon(true).setNameFormat("AMReporter %d").build());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/CalciteSemanticException.java,33,Distinct_without_an_aggreggation, Duplicates_in_RR, Filter_expression_with_non_boolean_return_type,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/CalciteSemanticException.java,34,Having_clause_without_any_groupby, Hint, Invalid_column_reference, Invalid_decimal, Invalid_interval,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/CalciteSemanticException.java,36,Schema_less_table, Select_alias_in_having_clause, Select_transform, Subquery,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/RexNodeConverter.java,520,throw new CalciteSemanticException("INTERVAL_DAY_TIME is not well supported",
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/RexNodeConverter.java,521,UnsupportedFeature.Invalid_interval);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/SqlFunctionConverter.java,312,registerFunction("-", SqlStdOperatorTable.MINUS, hToken(HiveParser.MINUS, "-"));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/SqlFunctionConverter.java,372,private boolean isDistinct;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/SqlFunctionConverter.java,436,calciteOp = new CalciteSqlFn(uInf.udfName, SqlKind.OTHER_FUNCTION, uInf.returnTypeInference,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/SqlFunctionConverter.java,437,uInf.operandTypeInference, uInf.operandTypeChecker,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/SqlFunctionConverter.java,438,SqlFunctionCategory.USER_DEFINED_FUNCTION, deterministic);
service/src/java/org/apache/hive/service/cli/thrift/ThriftHttpServlet.java,117,this.isCookieSecure = hiveConf.getBoolVar(
service/src/java/org/apache/hive/service/cli/thrift/ThriftHttpServlet.java,118,ConfVars.HIVE_SERVER2_THRIFT_HTTP_COOKIE_IS_SECURE);
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,65,public ATSHook() {
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,69,executor = Executors.newSingleThreadExecutor(
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,70,new ThreadFactoryBuilder().setDaemon(true).setNameFormat("ATS Logger %d").build());
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,100,executor.submit(new Runnable() {
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,102,public void run() {
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,103,try {
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,104,QueryPlan plan = hookContext.getQueryPlan();
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,105,if (plan == null) {
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,106,return;
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,108,String queryId = plan.getQueryId();
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,109,String opId = hookContext.getOperationId();
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,110,long queryStartTime = plan.getQueryStartTime();
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,111,String user = hookContext.getUgi().getUserName();
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,112,String requestuser = hookContext.getUserName();
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,113,if (hookContext.getUserName() == null ){
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,114,requestuser = hookContext.getUgi().getUserName() ;
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,116,int numMrJobs = Utilities.getMRTasks(plan.getRootTasks()).size();
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,117,int numTezJobs = Utilities.getTezTasks(plan.getRootTasks()).size();
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,118,if (numMrJobs + numTezJobs <= 0) {
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,119,return; // ignore client only queries
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,122,switch(hookContext.getHookType()) {
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,123,case PRE_EXEC_HOOK:
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,124,ExplainWork work = new ExplainWork(null,// resFile
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,125,null,// pCtx
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,126,plan.getRootTasks(),// RootTasks
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,127,plan.getFetchTask(),// FetchTask
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,128,null,// analyzer
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,129,false,// extended
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,130,true,// formatted
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,131,false,// dependency
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,132,false,// logical
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,133,false,// authorize
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,134,false,// userLevelExplain
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,135,null// cboInfo
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,136,);
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,138,ExplainTask explain = (ExplainTask) TaskFactory.get(work, conf);
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,139,explain.initialize(queryState, plan, null, null);
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,140,String query = plan.getQueryStr();
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,141,JSONObject explainPlan = explain.getJSONPlan(null, work);
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,145,break;
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,146,case POST_EXEC_HOOK:
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,148,break;
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,149,case ON_FAILURE_HOOK:
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,151,break;
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,152,default:
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,154,break;
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,157,LOG.info("Failed to submit plan to ATS: " + StringUtils.stringifyException(e));
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,224,synchronized void fireAndForget(Configuration conf, TimelineEntity entity) throws Exception {
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,225,timelineClient.putEntities(entity);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/AggregateDefinition.java,28,private GroupByDesc.Mode mode;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/AggregateDefinition.java,31,AggregateDefinition(String name, VectorExpressionDescriptor.ArgumentType type,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/AggregateDefinition.java,32,GroupByDesc.Mode mode, Class<? extends VectorAggregateExpression> aggClass) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/AggregateDefinition.java,35,this.mode = mode;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/AggregateDefinition.java,45,GroupByDesc.Mode getMode() {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/AggregateDefinition.java,46,return mode;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java,545,changeToUnsortedStreamingMode();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java,556,private class ProcessingModeUnsortedStreaming extends ProcessingModeBase {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java,678,private class ProcessingModeReduceMergePartialKeys extends ProcessingModeBase {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java,764,aggregators[i] =
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java,765,vContext.getAggregatorExpression(aggDesc, desc.getVectorDesc().isReduceMergePartial());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java,813,if (outputKeyLength > 0 && !conf.getVectorDesc().isReduceMergePartial()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java,815,keyWrappersBatch = VectorHashKeyWrapperBatch.compileKeyWrapperBatch(keyExpressions);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java,816,aggregationBatchInfo = new VectorAggregationBufferBatch();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java,817,aggregationBatchInfo.compileAggregationBatchInfo(aggregators);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java,838,if (outputKeyLength == 0) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java,843,processingMode = this.new ProcessingModeReduceMergePartialKeys();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java,845,processingMode = this.new ProcessingModeUnsortedStreaming();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java,859,private void changeToUnsortedStreamingMode() throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java,860,processingMode = this.new ProcessingModeUnsortedStreaming();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,51,import org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.Mode;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,407,exprDesc, Mode mode) throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,428,Mode.PROJECTION, null);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,446,return getVectorExpressions(exprNodes, Mode.PROJECTION);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,464,return getVectorExpression(exprDesc, Mode.PROJECTION);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,475,public VectorExpression getVectorExpression(ExprNodeDesc exprDesc, Mode mode) throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,888,Mode mode) throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,895,if (mode == Mode.PROJECTION) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,904,if (mode == Mode.FILTER) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,976,Class<?> udfClass, List<ExprNodeDesc> childExpr, Mode mode,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1002,Mode childrenMode = getChildrenMode(mode, udfClass);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1042,Mode childrenMode = getChildrenMode(mode, udfClass);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1078,List<ExprNodeDesc> childExpr, Mode childrenMode, TypeInfo returnType) throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1136,private Mode getChildrenMode(Mode mode, Class<?> udf) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1137,if (mode.equals(Mode.FILTER) && (udf.equals(GenericUDFOPAnd.class) || udf.equals(GenericUDFOPOr.class))) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1138,return Mode.FILTER;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1140,return Mode.PROJECTION;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1213,List<ExprNodeDesc> childExpr, Mode mode, TypeInfo returnType) throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1221,if (udf instanceof GenericUDFBetween && mode == Mode.FILTER) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1266,List<ExprNodeDesc> childExpr, Mode mode, TypeInfo returnType) throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1272,childExpr, Mode.PROJECTION, returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1501,Class<?> cl = (mode == Mode.FILTER ? FilterStructColumnInList.class : StructColumnInList.class);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1503,expr = createVectorExpression(cl, null, Mode.PROJECTION, returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1557,cl = (mode == Mode.FILTER ? FilterLongColumnInList.class : LongColumnInList.class);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1562,expr = createVectorExpression(cl, childExpr.subList(0, 1), Mode.PROJECTION, returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1565,cl = (mode == Mode.FILTER ? FilterTimestampColumnInList.class : TimestampColumnInList.class);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1570,expr = createVectorExpression(cl, childExpr.subList(0, 1), Mode.PROJECTION, returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1573,cl = (mode == Mode.FILTER ? FilterStringColumnInList.class : StringColumnInList.class);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1578,expr = createVectorExpression(cl, childExpr.subList(0, 1), Mode.PROJECTION, returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1581,cl = (mode == Mode.FILTER ? FilterDoubleColumnInList.class : DoubleColumnInList.class);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1586,expr = createVectorExpression(cl, childExpr.subList(0, 1), Mode.PROJECTION, returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1589,cl = (mode == Mode.FILTER ? FilterDecimalColumnInList.class : DecimalColumnInList.class);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1595,expr = createVectorExpression(cl, childExpr.subList(0, 1), Mode.PROJECTION, returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1598,cl = (mode == Mode.FILTER ? FilterLongColumnInList.class : LongColumnInList.class);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1603,expr = createVectorExpression(cl, childExpr.subList(0, 1), Mode.PROJECTION, returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1626,List<ExprNodeDesc> childExpr, Mode mode, TypeInfo returnType) throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1760,return getConstantVectorExpression(decimalValue, returnType, Mode.PROJECTION);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1763,return createVectorExpression(CastLongToDecimal.class, childExpr, Mode.PROJECTION, returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1765,return createVectorExpression(CastDoubleToDecimal.class, childExpr, Mode.PROJECTION, returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1767,return createVectorExpression(CastDecimalToDecimal.class, childExpr, Mode.PROJECTION,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1770,return createVectorExpression(CastStringToDecimal.class, childExpr, Mode.PROJECTION, returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1772,return createVectorExpression(CastTimestampToDecimal.class, childExpr, Mode.PROJECTION, returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1774,throw null;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1785,return getConstantVectorExpression(strValue, returnType, Mode.PROJECTION);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1789,return createVectorExpression(CastBooleanToStringViaLongToString.class, childExpr, Mode.PROJECTION, null);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1791,return createVectorExpression(CastLongToString.class, childExpr, Mode.PROJECTION, returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1793,return createVectorExpression(CastDecimalToString.class, childExpr, Mode.PROJECTION, returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1795,return createVectorExpression(CastDateToString.class, childExpr, Mode.PROJECTION, returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1797,return createVectorExpression(CastStringGroupToString.class, childExpr, Mode.PROJECTION, returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1813,return createVectorExpression(CastBooleanToCharViaLongToChar.class, childExpr, Mode.PROJECTION, returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1815,return createVectorExpression(CastLongToChar.class, childExpr, Mode.PROJECTION, returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1817,return createVectorExpression(CastDecimalToChar.class, childExpr, Mode.PROJECTION, returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1819,return createVectorExpression(CastDateToChar.class, childExpr, Mode.PROJECTION, returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1821,return createVectorExpression(CastStringGroupToChar.class, childExpr, Mode.PROJECTION, returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1837,return createVectorExpression(CastBooleanToVarCharViaLongToVarChar.class, childExpr, Mode.PROJECTION, returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1839,return createVectorExpression(CastLongToVarChar.class, childExpr, Mode.PROJECTION, returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1841,return createVectorExpression(CastDecimalToVarChar.class, childExpr, Mode.PROJECTION, returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1843,return createVectorExpression(CastDateToVarChar.class, childExpr, Mode.PROJECTION, returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1845,return createVectorExpression(CastStringGroupToVarChar.class, childExpr, Mode.PROJECTION, returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1858,return getConstantVectorExpression(doubleValue, returnType, Mode.PROJECTION);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1863,return createVectorExpression(CastLongToFloatViaLongToDouble.class, childExpr, Mode.PROJECTION, returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1865,return createVectorExpression(CastLongToDouble.class, childExpr, Mode.PROJECTION, returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1868,return createVectorExpression(CastTimestampToDouble.class, childExpr, Mode.PROJECTION,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1884,return getConstantVectorExpression(null, TypeInfoFactory.booleanTypeInfo, Mode.PROJECTION);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1894,Mode.PROJECTION, null);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1914,return getConstantVectorExpression(longValue, TypeInfoFactory.longTypeInfo, Mode.PROJECTION);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1934,if (mode == Mode.PROJECTION) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2019,return createVectorExpression(cl, childrenAfterNot, Mode.PROJECTION, returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2029,if (mode == Mode.FILTER) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2062,VectorExpression e = getVectorExpression(child, Mode.PROJECTION);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2408,add(new AggregateDefinition("min",         VectorExpressionDescriptor.ArgumentType.INT_DATE_INTERVAL_YEAR_MONTH,    null,                          VectorUDAFMinLong.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2409,add(new AggregateDefinition("min",         VectorExpressionDescriptor.ArgumentType.FLOAT_FAMILY,           null,                          VectorUDAFMinDouble.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2410,add(new AggregateDefinition("min",         VectorExpressionDescriptor.ArgumentType.STRING_FAMILY,          null,                          VectorUDAFMinString.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2411,add(new AggregateDefinition("min",         VectorExpressionDescriptor.ArgumentType.DECIMAL,                null,                          VectorUDAFMinDecimal.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2412,add(new AggregateDefinition("min",         VectorExpressionDescriptor.ArgumentType.TIMESTAMP,              null,                          VectorUDAFMinTimestamp.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2413,add(new AggregateDefinition("max",         VectorExpressionDescriptor.ArgumentType.INT_DATE_INTERVAL_YEAR_MONTH,    null,                          VectorUDAFMaxLong.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2414,add(new AggregateDefinition("max",         VectorExpressionDescriptor.ArgumentType.FLOAT_FAMILY,           null,                          VectorUDAFMaxDouble.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2415,add(new AggregateDefinition("max",         VectorExpressionDescriptor.ArgumentType.STRING_FAMILY,          null,                          VectorUDAFMaxString.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2416,add(new AggregateDefinition("max",         VectorExpressionDescriptor.ArgumentType.DECIMAL,                null,                          VectorUDAFMaxDecimal.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2417,add(new AggregateDefinition("max",         VectorExpressionDescriptor.ArgumentType.TIMESTAMP,              null,                          VectorUDAFMaxTimestamp.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2418,add(new AggregateDefinition("count",       VectorExpressionDescriptor.ArgumentType.NONE,                   GroupByDesc.Mode.HASH,         VectorUDAFCountStar.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2419,add(new AggregateDefinition("count",       VectorExpressionDescriptor.ArgumentType.INT_DATE_INTERVAL_YEAR_MONTH,    GroupByDesc.Mode.HASH,         VectorUDAFCount.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2420,add(new AggregateDefinition("count",       VectorExpressionDescriptor.ArgumentType.INT_FAMILY,             GroupByDesc.Mode.MERGEPARTIAL, VectorUDAFCountMerge.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2421,add(new AggregateDefinition("count",       VectorExpressionDescriptor.ArgumentType.FLOAT_FAMILY,           GroupByDesc.Mode.HASH,         VectorUDAFCount.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2422,add(new AggregateDefinition("count",       VectorExpressionDescriptor.ArgumentType.STRING_FAMILY,          GroupByDesc.Mode.HASH,         VectorUDAFCount.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2423,add(new AggregateDefinition("count",       VectorExpressionDescriptor.ArgumentType.DECIMAL,                GroupByDesc.Mode.HASH,         VectorUDAFCount.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2424,add(new AggregateDefinition("count",       VectorExpressionDescriptor.ArgumentType.TIMESTAMP,              GroupByDesc.Mode.HASH,         VectorUDAFCount.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2425,add(new AggregateDefinition("count",       VectorExpressionDescriptor.ArgumentType.INTERVAL_DAY_TIME,      GroupByDesc.Mode.HASH,         VectorUDAFCount.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2426,add(new AggregateDefinition("sum",         VectorExpressionDescriptor.ArgumentType.INT_FAMILY,             null,                          VectorUDAFSumLong.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2427,add(new AggregateDefinition("sum",         VectorExpressionDescriptor.ArgumentType.FLOAT_FAMILY,           null,                          VectorUDAFSumDouble.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2428,add(new AggregateDefinition("sum",         VectorExpressionDescriptor.ArgumentType.DECIMAL,                null,                          VectorUDAFSumDecimal.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2429,add(new AggregateDefinition("avg",         VectorExpressionDescriptor.ArgumentType.INT_FAMILY,             GroupByDesc.Mode.HASH,         VectorUDAFAvgLong.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2430,add(new AggregateDefinition("avg",         VectorExpressionDescriptor.ArgumentType.FLOAT_FAMILY,           GroupByDesc.Mode.HASH,         VectorUDAFAvgDouble.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2431,add(new AggregateDefinition("avg",         VectorExpressionDescriptor.ArgumentType.DECIMAL,                GroupByDesc.Mode.HASH,         VectorUDAFAvgDecimal.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2432,add(new AggregateDefinition("avg",         VectorExpressionDescriptor.ArgumentType.TIMESTAMP,              GroupByDesc.Mode.HASH,         VectorUDAFAvgTimestamp.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2433,add(new AggregateDefinition("variance",    VectorExpressionDescriptor.ArgumentType.INT_FAMILY,             GroupByDesc.Mode.HASH,         VectorUDAFVarPopLong.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2434,add(new AggregateDefinition("var_pop",     VectorExpressionDescriptor.ArgumentType.INT_FAMILY,             GroupByDesc.Mode.HASH,         VectorUDAFVarPopLong.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2435,add(new AggregateDefinition("variance",    VectorExpressionDescriptor.ArgumentType.FLOAT_FAMILY,           GroupByDesc.Mode.HASH,         VectorUDAFVarPopDouble.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2436,add(new AggregateDefinition("var_pop",     VectorExpressionDescriptor.ArgumentType.FLOAT_FAMILY,           GroupByDesc.Mode.HASH,         VectorUDAFVarPopDouble.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2437,add(new AggregateDefinition("variance",    VectorExpressionDescriptor.ArgumentType.DECIMAL,                GroupByDesc.Mode.HASH,         VectorUDAFVarPopDecimal.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2438,add(new AggregateDefinition("var_pop",     VectorExpressionDescriptor.ArgumentType.DECIMAL,                GroupByDesc.Mode.HASH,         VectorUDAFVarPopDecimal.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2439,add(new AggregateDefinition("variance",    VectorExpressionDescriptor.ArgumentType.TIMESTAMP,              GroupByDesc.Mode.HASH,         VectorUDAFVarPopTimestamp.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2440,add(new AggregateDefinition("var_pop",     VectorExpressionDescriptor.ArgumentType.TIMESTAMP,              GroupByDesc.Mode.HASH,         VectorUDAFVarPopTimestamp.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2441,add(new AggregateDefinition("var_samp",    VectorExpressionDescriptor.ArgumentType.INT_FAMILY,             GroupByDesc.Mode.HASH,         VectorUDAFVarSampLong.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2442,add(new AggregateDefinition("var_samp" ,   VectorExpressionDescriptor.ArgumentType.FLOAT_FAMILY,           GroupByDesc.Mode.HASH,         VectorUDAFVarSampDouble.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2443,add(new AggregateDefinition("var_samp" ,   VectorExpressionDescriptor.ArgumentType.DECIMAL,                GroupByDesc.Mode.HASH,         VectorUDAFVarSampDecimal.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2444,add(new AggregateDefinition("var_samp" ,   VectorExpressionDescriptor.ArgumentType.TIMESTAMP,              GroupByDesc.Mode.HASH,         VectorUDAFVarSampTimestamp.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2445,add(new AggregateDefinition("std",         VectorExpressionDescriptor.ArgumentType.INT_FAMILY,             GroupByDesc.Mode.HASH,         VectorUDAFStdPopLong.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2446,add(new AggregateDefinition("stddev",      VectorExpressionDescriptor.ArgumentType.INT_FAMILY,             GroupByDesc.Mode.HASH,         VectorUDAFStdPopLong.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2447,add(new AggregateDefinition("stddev_pop",  VectorExpressionDescriptor.ArgumentType.INT_FAMILY,             GroupByDesc.Mode.HASH,         VectorUDAFStdPopLong.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2448,add(new AggregateDefinition("std",         VectorExpressionDescriptor.ArgumentType.FLOAT_FAMILY,           GroupByDesc.Mode.HASH,         VectorUDAFStdPopDouble.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2449,add(new AggregateDefinition("stddev",      VectorExpressionDescriptor.ArgumentType.FLOAT_FAMILY,           GroupByDesc.Mode.HASH,         VectorUDAFStdPopDouble.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2450,add(new AggregateDefinition("stddev_pop",  VectorExpressionDescriptor.ArgumentType.FLOAT_FAMILY,           GroupByDesc.Mode.HASH,         VectorUDAFStdPopDouble.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2451,add(new AggregateDefinition("std",         VectorExpressionDescriptor.ArgumentType.DECIMAL,                GroupByDesc.Mode.HASH,         VectorUDAFStdPopDecimal.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2452,add(new AggregateDefinition("stddev",      VectorExpressionDescriptor.ArgumentType.DECIMAL,                GroupByDesc.Mode.HASH,         VectorUDAFStdPopDecimal.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2453,add(new AggregateDefinition("stddev_pop",  VectorExpressionDescriptor.ArgumentType.DECIMAL,                GroupByDesc.Mode.HASH,         VectorUDAFStdPopDecimal.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2454,add(new AggregateDefinition("std",         VectorExpressionDescriptor.ArgumentType.TIMESTAMP,              GroupByDesc.Mode.HASH,         VectorUDAFStdPopTimestamp.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2455,add(new AggregateDefinition("stddev",      VectorExpressionDescriptor.ArgumentType.TIMESTAMP,              GroupByDesc.Mode.HASH,         VectorUDAFStdPopTimestamp.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2456,add(new AggregateDefinition("stddev_pop",  VectorExpressionDescriptor.ArgumentType.TIMESTAMP,              GroupByDesc.Mode.HASH,         VectorUDAFStdPopTimestamp.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2457,add(new AggregateDefinition("stddev_samp", VectorExpressionDescriptor.ArgumentType.INT_FAMILY,             GroupByDesc.Mode.HASH,         VectorUDAFStdSampLong.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2458,add(new AggregateDefinition("stddev_samp", VectorExpressionDescriptor.ArgumentType.FLOAT_FAMILY,           GroupByDesc.Mode.HASH,         VectorUDAFStdSampDouble.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2459,add(new AggregateDefinition("stddev_samp", VectorExpressionDescriptor.ArgumentType.DECIMAL,                GroupByDesc.Mode.HASH,         VectorUDAFStdSampDecimal.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2460,add(new AggregateDefinition("stddev_samp", VectorExpressionDescriptor.ArgumentType.TIMESTAMP,              GroupByDesc.Mode.HASH,         VectorUDAFStdSampTimestamp.class));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2471,vectorParams[i] = this.getVectorExpression(exprDesc, Mode.PROJECTION);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2491,if (aggDef.getMode() == GroupByDesc.Mode.HASH && isReduceMergePartial) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2492,continue;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1545,LOG.info("Cannot vectorize groupby key expression");
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1549,if (!isReduce) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1553,ret = validateHashAggregationDesc(desc.getAggregators());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1554,if (!ret) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1555,return false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1561,boolean isComplete = desc.getMode() == GroupByDesc.Mode.COMPLETE;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1562,if (desc.getMode() != GroupByDesc.Mode.HASH) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1571,if (desc.isDistinct()) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1572,LOG.info("Vectorized Reduce MergePartial GROUP BY does not support DISTINCT");
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1573,return false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1576,boolean hasKeys = (desc.getKeys().size() > 0);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1579,ret = validateReduceMergePartialAggregationDesc(desc.getAggregators(), hasKeys);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1580,if (!ret) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1581,return false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1584,if (hasKeys) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1585,if (op.getParentOperators().size() > 0 && !isComplete) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1586,LOG.info("Vectorized Reduce MergePartial GROUP BY keys can only handle a key group when it is fed by reduce-shuffle");
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1587,return false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1590,LOG.info("Vectorized Reduce MergePartial GROUP BY will process key groups");
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1594,vectorDesc.setVectorOutput(true);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1596,LOG.info("Vectorized Reduce MergePartial GROUP BY will do global aggregation");
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1598,if (!isComplete) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1599,vectorDesc.setIsReduceMergePartial(true);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1601,vectorDesc.setIsReduceStreaming(true);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1607,ret = validateHashAggregationDesc(desc.getAggregators());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1608,if (!ret) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1609,return false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1637,private boolean validateHashAggregationDesc(List<AggregationDesc> descs) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1638,return validateAggregationDesc(descs, /* isReduceMergePartial */ false, false);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1641,private boolean validateReduceMergePartialAggregationDesc(List<AggregationDesc> descs, boolean hasKeys) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1642,return validateAggregationDesc(descs, /* isReduceMergePartial */ true, hasKeys);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1645,private boolean validateAggregationDesc(List<AggregationDesc> descs, boolean isReduceMergePartial, boolean hasKeys) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1647,boolean ret = validateAggregationDesc(d, isReduceMergePartial, hasKeys);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1648,if (!ret) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1649,return false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1652,return true;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1790,private boolean validateAggregationDesc(AggregationDesc aggDesc, boolean isReduceMergePartial,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1791,boolean hasKeys) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1796,return false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1800,return false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1807,vectorAggrExpr = vc.getAggregatorExpression(aggDesc, isReduceMergePartial);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1811,LOG.debug("Vectorization of aggreation should have succeeded ", e);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1813,return false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1816,if (isReduceMergePartial && hasKeys && !validateAggregationIsPrimitive(vectorAggrExpr)) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1818,return false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1821,return true;
ql/src/java/org/apache/hadoop/hive/ql/plan/VectorGroupByDesc.java,33,private boolean isReduceMergePartial;
ql/src/java/org/apache/hadoop/hive/ql/plan/VectorGroupByDesc.java,35,private boolean isVectorOutput;
ql/src/java/org/apache/hadoop/hive/ql/plan/VectorGroupByDesc.java,37,private boolean isReduceStreaming;
ql/src/java/org/apache/hadoop/hive/ql/plan/VectorGroupByDesc.java,40,this.isReduceMergePartial = false;
ql/src/java/org/apache/hadoop/hive/ql/plan/VectorGroupByDesc.java,44,public boolean isReduceMergePartial() {
ql/src/java/org/apache/hadoop/hive/ql/plan/VectorGroupByDesc.java,45,return isReduceMergePartial;
ql/src/java/org/apache/hadoop/hive/ql/plan/VectorGroupByDesc.java,48,public void setIsReduceMergePartial(boolean isReduceMergePartial) {
ql/src/java/org/apache/hadoop/hive/ql/plan/VectorGroupByDesc.java,49,this.isReduceMergePartial = isReduceMergePartial;
ql/src/java/org/apache/hadoop/hive/ql/plan/VectorGroupByDesc.java,60,public void setIsReduceStreaming(boolean isReduceStreaming) {
ql/src/java/org/apache/hadoop/hive/ql/plan/VectorGroupByDesc.java,61,this.isReduceStreaming = isReduceStreaming;
ql/src/java/org/apache/hadoop/hive/ql/plan/VectorGroupByDesc.java,64,public boolean isReduceStreaming() {
ql/src/java/org/apache/hadoop/hive/ql/plan/VectorGroupByDesc.java,65,return isReduceStreaming;
orc/src/java/org/apache/orc/impl/WriterImpl.java,1202,protected boolean useDictionaryEncoding = true;
orc/src/java/org/apache/orc/impl/WriterImpl.java,1230,doneDictionaryCheck = false;
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatch.java,167,cv.stringifyValue(b, i);
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,72,codec = WriterImpl.createCodec(fileMetadata.getCompressionKind());
orc/src/java/org/apache/orc/impl/ReaderImpl.java,324,this.codec = WriterImpl.createCodec(compressionKind);
orc/src/java/org/apache/orc/impl/RecordReaderUtils.java,74,this.codec = WriterImpl.createCodec(properties.getCompression());
orc/src/java/org/apache/orc/impl/WriterImpl.java,24,import java.io.OutputStream;
orc/src/java/org/apache/orc/impl/WriterImpl.java,29,import java.util.EnumSet;
orc/src/java/org/apache/orc/impl/WriterImpl.java,38,import org.apache.orc.CompressionCodec;
orc/src/java/org/apache/orc/impl/WriterImpl.java,39,import org.apache.orc.CompressionKind;
orc/src/java/org/apache/orc/impl/WriterImpl.java,51,import org.apache.hadoop.fs.FSDataOutputStream;
orc/src/java/org/apache/orc/impl/WriterImpl.java,72,import com.google.protobuf.CodedOutputStream;
orc/src/java/org/apache/orc/impl/WriterImpl.java,97,private static final int HDFS_BUFFER_SIZE = 256 * 1024;
orc/src/java/org/apache/orc/impl/WriterImpl.java,101,private static final int COLUMN_COUNT_THRESHOLD = 1000;
orc/src/java/org/apache/orc/impl/WriterImpl.java,103,private final FileSystem fs;
orc/src/java/org/apache/orc/impl/WriterImpl.java,105,private final long defaultStripeSize;
orc/src/java/org/apache/orc/impl/WriterImpl.java,106,private long adjustedStripeSize;
orc/src/java/org/apache/orc/impl/WriterImpl.java,108,private final CompressionKind compress;
orc/src/java/org/apache/orc/impl/WriterImpl.java,109,private final CompressionCodec codec;
orc/src/java/org/apache/orc/impl/WriterImpl.java,110,private final boolean addBlockPadding;
orc/src/java/org/apache/orc/impl/WriterImpl.java,111,private final int bufferSize;
orc/src/java/org/apache/orc/impl/WriterImpl.java,112,private final long blockSize;
orc/src/java/org/apache/orc/impl/WriterImpl.java,113,private final double paddingTolerance;
orc/src/java/org/apache/orc/impl/WriterImpl.java,117,private final Map<StreamName, BufferedStream> streams =
orc/src/java/org/apache/orc/impl/WriterImpl.java,118,new TreeMap<StreamName, BufferedStream>();
orc/src/java/org/apache/orc/impl/WriterImpl.java,120,private FSDataOutputStream rawWriter = null;
orc/src/java/org/apache/orc/impl/WriterImpl.java,122,private OutStream writer = null;
orc/src/java/org/apache/orc/impl/WriterImpl.java,124,private CodedOutputStream protobufWriter = null;
orc/src/java/org/apache/orc/impl/WriterImpl.java,125,private long headerLength;
orc/src/java/org/apache/orc/impl/WriterImpl.java,145,private final OrcFile.CompressionStrategy compressionStrategy;
orc/src/java/org/apache/orc/impl/WriterImpl.java,153,this.fs = fs;
orc/src/java/org/apache/orc/impl/WriterImpl.java,169,this.adjustedStripeSize = opts.getStripeSize();
orc/src/java/org/apache/orc/impl/WriterImpl.java,170,this.defaultStripeSize = opts.getStripeSize();
orc/src/java/org/apache/orc/impl/WriterImpl.java,173,this.compressionStrategy = opts.getCompressionStrategy();
orc/src/java/org/apache/orc/impl/WriterImpl.java,174,this.addBlockPadding = opts.getBlockPadding();
orc/src/java/org/apache/orc/impl/WriterImpl.java,175,this.blockSize = opts.getBlockSize();
orc/src/java/org/apache/orc/impl/WriterImpl.java,176,this.paddingTolerance = opts.getPaddingTolerance();
orc/src/java/org/apache/orc/impl/WriterImpl.java,177,this.compress = opts.getCompress();
orc/src/java/org/apache/orc/impl/WriterImpl.java,181,codec = createCodec(compress);
orc/src/java/org/apache/orc/impl/WriterImpl.java,182,int numColumns = schema.getMaximumId() + 1;
orc/src/java/org/apache/orc/impl/WriterImpl.java,183,if (opts.isEnforceBufferSize()) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,184,this.bufferSize = opts.getBufferSize();
orc/src/java/org/apache/orc/impl/WriterImpl.java,186,this.bufferSize = getEstimatedBufferSize(defaultStripeSize,
orc/src/java/org/apache/orc/impl/WriterImpl.java,187,numColumns, opts.getBufferSize());
orc/src/java/org/apache/orc/impl/WriterImpl.java,205,LOG.info("ORC writer created for path: {} with stripeSize: {} blockSize: {}" +
orc/src/java/org/apache/orc/impl/WriterImpl.java,207,compress, bufferSize);
orc/src/java/org/apache/orc/impl/WriterImpl.java,211,public static int getEstimatedBufferSize(long stripeSize, int numColumns,
orc/src/java/org/apache/orc/impl/WriterImpl.java,212,int bs) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,217,int estBufferSize = (int) (stripeSize / (20 * numColumns));
orc/src/java/org/apache/orc/impl/WriterImpl.java,218,estBufferSize = getClosestBufferSize(estBufferSize);
orc/src/java/org/apache/orc/impl/WriterImpl.java,219,return estBufferSize > bs ? bs : estBufferSize;
orc/src/java/org/apache/orc/impl/WriterImpl.java,222,private static int getClosestBufferSize(int estBufferSize) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,223,final int kb4 = 4 * 1024;
orc/src/java/org/apache/orc/impl/WriterImpl.java,224,final int kb8 = 8 * 1024;
orc/src/java/org/apache/orc/impl/WriterImpl.java,225,final int kb16 = 16 * 1024;
orc/src/java/org/apache/orc/impl/WriterImpl.java,226,final int kb32 = 32 * 1024;
orc/src/java/org/apache/orc/impl/WriterImpl.java,227,final int kb64 = 64 * 1024;
orc/src/java/org/apache/orc/impl/WriterImpl.java,228,final int kb128 = 128 * 1024;
orc/src/java/org/apache/orc/impl/WriterImpl.java,229,final int kb256 = 256 * 1024;
orc/src/java/org/apache/orc/impl/WriterImpl.java,230,if (estBufferSize <= kb4) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,231,return kb4;
orc/src/java/org/apache/orc/impl/WriterImpl.java,233,return kb8;
orc/src/java/org/apache/orc/impl/WriterImpl.java,235,return kb16;
orc/src/java/org/apache/orc/impl/WriterImpl.java,237,return kb32;
orc/src/java/org/apache/orc/impl/WriterImpl.java,239,return kb64;
orc/src/java/org/apache/orc/impl/WriterImpl.java,241,return kb128;
orc/src/java/org/apache/orc/impl/WriterImpl.java,243,return kb256;
orc/src/java/org/apache/orc/impl/WriterImpl.java,247,public static CompressionCodec createCodec(CompressionKind kind) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,248,switch (kind) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,249,case NONE:
orc/src/java/org/apache/orc/impl/WriterImpl.java,250,return null;
orc/src/java/org/apache/orc/impl/WriterImpl.java,251,case ZLIB:
orc/src/java/org/apache/orc/impl/WriterImpl.java,252,return new ZlibCodec();
orc/src/java/org/apache/orc/impl/WriterImpl.java,253,case SNAPPY:
orc/src/java/org/apache/orc/impl/WriterImpl.java,254,return new SnappyCodec();
orc/src/java/org/apache/orc/impl/WriterImpl.java,255,case LZO:
orc/src/java/org/apache/orc/impl/WriterImpl.java,256,try {
orc/src/java/org/apache/orc/impl/WriterImpl.java,257,ClassLoader loader = Thread.currentThread().getContextClassLoader();
orc/src/java/org/apache/orc/impl/WriterImpl.java,258,if (loader == null) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,259,loader = WriterImpl.class.getClassLoader();
orc/src/java/org/apache/orc/impl/WriterImpl.java,262,Class<? extends CompressionCodec> lzo =
orc/src/java/org/apache/orc/impl/WriterImpl.java,264,loader.loadClass("org.apache.hadoop.hive.ql.io.orc.LzoCodec");
orc/src/java/org/apache/orc/impl/WriterImpl.java,265,return lzo.newInstance();
orc/src/java/org/apache/orc/impl/WriterImpl.java,267,throw new IllegalArgumentException("LZO is not available.", e);
orc/src/java/org/apache/orc/impl/WriterImpl.java,269,throw new IllegalArgumentException("Problem initializing LZO", e);
orc/src/java/org/apache/orc/impl/WriterImpl.java,271,throw new IllegalArgumentException("Insufficient access to LZO", e);
orc/src/java/org/apache/orc/impl/WriterImpl.java,273,default:
orc/src/java/org/apache/orc/impl/WriterImpl.java,274,throw new IllegalArgumentException("Unknown compression codec: " +
orc/src/java/org/apache/orc/impl/WriterImpl.java,275,kind);
orc/src/java/org/apache/orc/impl/WriterImpl.java,281,long limit = (long) Math.round(adjustedStripeSize * newScale);
orc/src/java/org/apache/orc/impl/WriterImpl.java,300,private class BufferedStream implements OutStream.OutputReceiver {
orc/src/java/org/apache/orc/impl/WriterImpl.java,301,private final OutStream outStream;
orc/src/java/org/apache/orc/impl/WriterImpl.java,302,private final List<ByteBuffer> output = new ArrayList<ByteBuffer>();
orc/src/java/org/apache/orc/impl/WriterImpl.java,304,BufferedStream(String name, int bufferSize,
orc/src/java/org/apache/orc/impl/WriterImpl.java,305,CompressionCodec codec) throws IOException {
orc/src/java/org/apache/orc/impl/WriterImpl.java,306,outStream = new OutStream(name, bufferSize, codec, this);
orc/src/java/org/apache/orc/impl/WriterImpl.java,314,public void output(ByteBuffer buffer) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,315,output.add(buffer);
orc/src/java/org/apache/orc/impl/WriterImpl.java,322,public long getBufferSize() {
orc/src/java/org/apache/orc/impl/WriterImpl.java,323,long result = 0;
orc/src/java/org/apache/orc/impl/WriterImpl.java,324,for(ByteBuffer buf: output) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,325,result += buf.capacity();
orc/src/java/org/apache/orc/impl/WriterImpl.java,327,return outStream.getBufferSize() + result;
orc/src/java/org/apache/orc/impl/WriterImpl.java,334,public void flush() throws IOException {
orc/src/java/org/apache/orc/impl/WriterImpl.java,335,outStream.flush();
orc/src/java/org/apache/orc/impl/WriterImpl.java,342,public void clear() throws IOException {
orc/src/java/org/apache/orc/impl/WriterImpl.java,343,outStream.clear();
orc/src/java/org/apache/orc/impl/WriterImpl.java,344,output.clear();
orc/src/java/org/apache/orc/impl/WriterImpl.java,351,public boolean isSuppressed() {
orc/src/java/org/apache/orc/impl/WriterImpl.java,352,return outStream.isSuppressed();
orc/src/java/org/apache/orc/impl/WriterImpl.java,360,public long getOutputSize() {
orc/src/java/org/apache/orc/impl/WriterImpl.java,361,long result = 0;
orc/src/java/org/apache/orc/impl/WriterImpl.java,362,for(ByteBuffer buffer: output) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,363,result += buffer.remaining();
orc/src/java/org/apache/orc/impl/WriterImpl.java,365,return result;
orc/src/java/org/apache/orc/impl/WriterImpl.java,373,void spillTo(OutputStream out) throws IOException {
orc/src/java/org/apache/orc/impl/WriterImpl.java,374,for(ByteBuffer buffer: output) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,375,out.write(buffer.array(), buffer.arrayOffset() + buffer.position(),
orc/src/java/org/apache/orc/impl/WriterImpl.java,376,buffer.remaining());
orc/src/java/org/apache/orc/impl/WriterImpl.java,381,public String toString() {
orc/src/java/org/apache/orc/impl/WriterImpl.java,382,return outStream.toString();
orc/src/java/org/apache/orc/impl/WriterImpl.java,390,private class DirectStream implements OutStream.OutputReceiver {
orc/src/java/org/apache/orc/impl/WriterImpl.java,391,private final FSDataOutputStream output;
orc/src/java/org/apache/orc/impl/WriterImpl.java,393,DirectStream(FSDataOutputStream output) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,394,this.output = output;
orc/src/java/org/apache/orc/impl/WriterImpl.java,398,public void output(ByteBuffer buffer) throws IOException {
orc/src/java/org/apache/orc/impl/WriterImpl.java,399,output.write(buffer.array(), buffer.arrayOffset() + buffer.position(),
orc/src/java/org/apache/orc/impl/WriterImpl.java,400,buffer.remaining());
orc/src/java/org/apache/orc/impl/WriterImpl.java,433,final EnumSet<CompressionCodec.Modifier> modifiers;
orc/src/java/org/apache/orc/impl/WriterImpl.java,435,switch (kind) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,436,case BLOOM_FILTER:
orc/src/java/org/apache/orc/impl/WriterImpl.java,437,case DATA:
orc/src/java/org/apache/orc/impl/WriterImpl.java,438,case DICTIONARY_DATA:
orc/src/java/org/apache/orc/impl/WriterImpl.java,439,if (getCompressionStrategy() == OrcFile.CompressionStrategy.SPEED) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,440,modifiers = EnumSet.of(CompressionCodec.Modifier.FAST,
orc/src/java/org/apache/orc/impl/WriterImpl.java,441,CompressionCodec.Modifier.TEXT);
orc/src/java/org/apache/orc/impl/WriterImpl.java,443,modifiers = EnumSet.of(CompressionCodec.Modifier.DEFAULT,
orc/src/java/org/apache/orc/impl/WriterImpl.java,444,CompressionCodec.Modifier.TEXT);
orc/src/java/org/apache/orc/impl/WriterImpl.java,446,break;
orc/src/java/org/apache/orc/impl/WriterImpl.java,447,case LENGTH:
orc/src/java/org/apache/orc/impl/WriterImpl.java,448,case DICTIONARY_COUNT:
orc/src/java/org/apache/orc/impl/WriterImpl.java,449,case PRESENT:
orc/src/java/org/apache/orc/impl/WriterImpl.java,450,case ROW_INDEX:
orc/src/java/org/apache/orc/impl/WriterImpl.java,451,case SECONDARY:
orc/src/java/org/apache/orc/impl/WriterImpl.java,453,modifiers = EnumSet.of(CompressionCodec.Modifier.FASTEST,
orc/src/java/org/apache/orc/impl/WriterImpl.java,454,CompressionCodec.Modifier.BINARY);
orc/src/java/org/apache/orc/impl/WriterImpl.java,455,break;
orc/src/java/org/apache/orc/impl/WriterImpl.java,456,default:
orc/src/java/org/apache/orc/impl/WriterImpl.java,457,LOG.warn("Missing ORC compression modifiers for " + kind);
orc/src/java/org/apache/orc/impl/WriterImpl.java,458,modifiers = null;
orc/src/java/org/apache/orc/impl/WriterImpl.java,459,break;
orc/src/java/org/apache/orc/impl/WriterImpl.java,462,BufferedStream result = streams.get(name);
orc/src/java/org/apache/orc/impl/WriterImpl.java,463,if (result == null) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,464,result = new BufferedStream(name.toString(), bufferSize,
orc/src/java/org/apache/orc/impl/WriterImpl.java,465,codec == null ? codec : codec.modify(modifiers));
orc/src/java/org/apache/orc/impl/WriterImpl.java,466,streams.put(name, result);
orc/src/java/org/apache/orc/impl/WriterImpl.java,468,return result.outStream;
orc/src/java/org/apache/orc/impl/WriterImpl.java,499,return codec != null;
orc/src/java/org/apache/orc/impl/WriterImpl.java,514,public OrcFile.CompressionStrategy getCompressionStrategy() {
orc/src/java/org/apache/orc/impl/WriterImpl.java,515,return compressionStrategy;
orc/src/java/org/apache/orc/impl/WriterImpl.java,575,private final PositionedOutputStream rowIndexStream;
orc/src/java/org/apache/orc/impl/WriterImpl.java,576,private final PositionedOutputStream bloomFilterStream;
orc/src/java/org/apache/orc/impl/WriterImpl.java,618,if (streamFactory.buildIndex()) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,619,rowIndexStream = streamFactory.createStream(id, OrcProto.Stream.Kind.ROW_INDEX);
orc/src/java/org/apache/orc/impl/WriterImpl.java,621,rowIndexStream = null;
orc/src/java/org/apache/orc/impl/WriterImpl.java,626,bloomFilterStream = streamFactory.createStream(id, OrcProto.Stream.Kind.BLOOM_FILTER);
orc/src/java/org/apache/orc/impl/WriterImpl.java,632,bloomFilterStream = null;
orc/src/java/org/apache/orc/impl/WriterImpl.java,761,if(!foundNulls) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,765,if (rowIndexStream != null) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,784,if (rowIndexStream != null) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,790,rowIndex.build().writeTo(rowIndexStream);
orc/src/java/org/apache/orc/impl/WriterImpl.java,791,rowIndexStream.flush();
orc/src/java/org/apache/orc/impl/WriterImpl.java,797,if (bloomFilterStream != null) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,798,bloomFilterIndex.build().writeTo(bloomFilterStream);
orc/src/java/org/apache/orc/impl/WriterImpl.java,799,bloomFilterStream.flush();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2466,public FSDataOutputStream getStream() throws IOException {
orc/src/java/org/apache/orc/impl/WriterImpl.java,2467,if (rawWriter == null) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,2468,rawWriter = fs.create(path, false, HDFS_BUFFER_SIZE,
orc/src/java/org/apache/orc/impl/WriterImpl.java,2469,fs.getDefaultReplication(path), blockSize);
orc/src/java/org/apache/orc/impl/WriterImpl.java,2470,rawWriter.writeBytes(OrcFile.MAGIC);
orc/src/java/org/apache/orc/impl/WriterImpl.java,2471,headerLength = rawWriter.getPos();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2472,writer = new OutStream("metadata", bufferSize, codec,
orc/src/java/org/apache/orc/impl/WriterImpl.java,2473,new DirectStream(rawWriter));
orc/src/java/org/apache/orc/impl/WriterImpl.java,2474,protobufWriter = CodedOutputStream.newInstance(writer);
orc/src/java/org/apache/orc/impl/WriterImpl.java,2476,return rawWriter;
orc/src/java/org/apache/orc/impl/WriterImpl.java,2485,getStream();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2496,OrcProto.StripeFooter.Builder builder =
orc/src/java/org/apache/orc/impl/WriterImpl.java,2497,OrcProto.StripeFooter.newBuilder();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2499,long indexSize = 0;
orc/src/java/org/apache/orc/impl/WriterImpl.java,2500,long dataSize = 0;
orc/src/java/org/apache/orc/impl/WriterImpl.java,2501,for(Map.Entry<StreamName, BufferedStream> pair: streams.entrySet()) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,2502,BufferedStream stream = pair.getValue();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2503,if (!stream.isSuppressed()) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,2504,stream.flush();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2505,StreamName name = pair.getKey();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2506,long streamSize = pair.getValue().getOutputSize();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2510,.setLength(streamSize));
orc/src/java/org/apache/orc/impl/WriterImpl.java,2511,if (StreamName.Area.INDEX == name.getArea()) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,2512,indexSize += streamSize;
orc/src/java/org/apache/orc/impl/WriterImpl.java,2514,dataSize += streamSize;
orc/src/java/org/apache/orc/impl/WriterImpl.java,2518,OrcProto.StripeFooter footer = builder.build();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2522,long start = rawWriter.getPos();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2523,final long currentStripeSize = indexSize + dataSize + footer.getSerializedSize();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2524,final long available = blockSize - (start % blockSize);
orc/src/java/org/apache/orc/impl/WriterImpl.java,2525,final long overflow = currentStripeSize - adjustedStripeSize;
orc/src/java/org/apache/orc/impl/WriterImpl.java,2526,final float availRatio = (float) available / (float) defaultStripeSize;
orc/src/java/org/apache/orc/impl/WriterImpl.java,2528,if (availRatio > 0.0f && availRatio < 1.0f
orc/src/java/org/apache/orc/impl/WriterImpl.java,2529,&& availRatio > paddingTolerance) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,2535,double correction = overflow > 0 ? (double) overflow
orc/src/java/org/apache/orc/impl/WriterImpl.java,2536,/ (double) adjustedStripeSize : 0.0;
orc/src/java/org/apache/orc/impl/WriterImpl.java,2540,correction = correction > paddingTolerance ? paddingTolerance
orc/src/java/org/apache/orc/impl/WriterImpl.java,2541,: correction;
orc/src/java/org/apache/orc/impl/WriterImpl.java,2544,adjustedStripeSize = (long) ((1.0f - correction) * (availRatio * defaultStripeSize));
orc/src/java/org/apache/orc/impl/WriterImpl.java,2546,adjustedStripeSize = defaultStripeSize;
orc/src/java/org/apache/orc/impl/WriterImpl.java,2549,if (availRatio < paddingTolerance && addBlockPadding) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,2550,long padding = blockSize - (start % blockSize);
orc/src/java/org/apache/orc/impl/WriterImpl.java,2551,byte[] pad = new byte[(int) Math.min(HDFS_BUFFER_SIZE, padding)];
orc/src/java/org/apache/orc/impl/WriterImpl.java,2552,LOG.info(String.format("Padding ORC by %d bytes (<=  %.2f * %d)",
orc/src/java/org/apache/orc/impl/WriterImpl.java,2553,padding, availRatio, defaultStripeSize));
orc/src/java/org/apache/orc/impl/WriterImpl.java,2554,start += padding;
orc/src/java/org/apache/orc/impl/WriterImpl.java,2555,while (padding > 0) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,2556,int writeLen = (int) Math.min(padding, pad.length);
orc/src/java/org/apache/orc/impl/WriterImpl.java,2557,rawWriter.write(pad, 0, writeLen);
orc/src/java/org/apache/orc/impl/WriterImpl.java,2558,padding -= writeLen;
orc/src/java/org/apache/orc/impl/WriterImpl.java,2560,adjustedStripeSize = defaultStripeSize;
orc/src/java/org/apache/orc/impl/WriterImpl.java,2562,&& (start % blockSize) + currentStripeSize > blockSize) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,2565,adjustedStripeSize = defaultStripeSize;
orc/src/java/org/apache/orc/impl/WriterImpl.java,2569,for(Map.Entry<StreamName, BufferedStream> pair: streams.entrySet()) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,2570,BufferedStream stream = pair.getValue();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2571,if (!stream.isSuppressed()) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,2572,stream.spillTo(rawWriter);
orc/src/java/org/apache/orc/impl/WriterImpl.java,2574,stream.clear();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2576,footer.writeTo(protobufWriter);
orc/src/java/org/apache/orc/impl/WriterImpl.java,2577,protobufWriter.flush();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2578,writer.flush();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2579,long footerLength = rawWriter.getPos() - start - dataSize - indexSize;
orc/src/java/org/apache/orc/impl/WriterImpl.java,2580,OrcProto.StripeInformation dirEntry =
orc/src/java/org/apache/orc/impl/WriterImpl.java,2586,.setFooterLength(footerLength).build();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2587,stripes.add(dirEntry);
orc/src/java/org/apache/orc/impl/WriterImpl.java,2648,private OrcProto.CompressionKind writeCompressionKind(CompressionKind kind) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,2649,switch (kind) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,2650,case NONE: return OrcProto.CompressionKind.NONE;
orc/src/java/org/apache/orc/impl/WriterImpl.java,2651,case ZLIB: return OrcProto.CompressionKind.ZLIB;
orc/src/java/org/apache/orc/impl/WriterImpl.java,2652,case SNAPPY: return OrcProto.CompressionKind.SNAPPY;
orc/src/java/org/apache/orc/impl/WriterImpl.java,2653,case LZO: return OrcProto.CompressionKind.LZO;
orc/src/java/org/apache/orc/impl/WriterImpl.java,2654,default:
orc/src/java/org/apache/orc/impl/WriterImpl.java,2655,throw new IllegalArgumentException("Unknown compression " + kind);
orc/src/java/org/apache/orc/impl/WriterImpl.java,2667,private int writeMetadata() throws IOException {
orc/src/java/org/apache/orc/impl/WriterImpl.java,2668,getStream();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2674,long startPosn = rawWriter.getPos();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2675,OrcProto.Metadata metadata = builder.build();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2676,metadata.writeTo(protobufWriter);
orc/src/java/org/apache/orc/impl/WriterImpl.java,2677,protobufWriter.flush();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2678,writer.flush();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2679,return (int) (rawWriter.getPos() - startPosn);
orc/src/java/org/apache/orc/impl/WriterImpl.java,2682,private int writeFooter(long bodyLength) throws IOException {
orc/src/java/org/apache/orc/impl/WriterImpl.java,2683,getStream();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2685,builder.setContentLength(bodyLength);
orc/src/java/org/apache/orc/impl/WriterImpl.java,2686,builder.setHeaderLength(headerLength);
orc/src/java/org/apache/orc/impl/WriterImpl.java,2704,long startPosn = rawWriter.getPos();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2705,OrcProto.Footer footer = builder.build();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2706,footer.writeTo(protobufWriter);
orc/src/java/org/apache/orc/impl/WriterImpl.java,2707,protobufWriter.flush();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2708,writer.flush();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2709,return (int) (rawWriter.getPos() - startPosn);
orc/src/java/org/apache/orc/impl/WriterImpl.java,2712,private int writePostScript(int footerLength, int metadataLength) throws IOException {
orc/src/java/org/apache/orc/impl/WriterImpl.java,2722,if (compress != CompressionKind.NONE) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,2723,builder.setCompressionBlockSize(bufferSize);
orc/src/java/org/apache/orc/impl/WriterImpl.java,2725,OrcProto.PostScript ps = builder.build();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2727,long startPosn = rawWriter.getPos();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2728,ps.writeTo(rawWriter);
orc/src/java/org/apache/orc/impl/WriterImpl.java,2729,long length = rawWriter.getPos() - startPosn;
orc/src/java/org/apache/orc/impl/WriterImpl.java,2730,if (length > 255) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,2731,throw new IllegalArgumentException("PostScript too large at " + length);
orc/src/java/org/apache/orc/impl/WriterImpl.java,2733,return (int) length;
orc/src/java/org/apache/orc/impl/WriterImpl.java,2737,long result = 0;
orc/src/java/org/apache/orc/impl/WriterImpl.java,2738,for(BufferedStream stream: streams.values()) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,2739,result += stream.getBufferSize();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2741,result += treeWriter.estimateMemory();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2742,return result;
orc/src/java/org/apache/orc/impl/WriterImpl.java,2788,int metadataLength = writeMetadata();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2789,int footerLength = writeFooter(rawWriter.getPos() - metadataLength);
orc/src/java/org/apache/orc/impl/WriterImpl.java,2790,rawWriter.writeByte(writePostScript(footerLength, metadataLength));
orc/src/java/org/apache/orc/impl/WriterImpl.java,2791,rawWriter.close();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2822,int metaLength = writeMetadata();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2823,int footLength = writeFooter(rawWriter.getPos() - metaLength);
orc/src/java/org/apache/orc/impl/WriterImpl.java,2824,rawWriter.writeByte(writePostScript(footLength, metaLength));
orc/src/java/org/apache/orc/impl/WriterImpl.java,2826,rawWriter.hflush();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2828,return rawWriter.getPos();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2842,getStream();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2843,long start = rawWriter.getPos();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2844,long availBlockSpace = blockSize - (start % blockSize);
orc/src/java/org/apache/orc/impl/WriterImpl.java,2848,if (length < blockSize && length > availBlockSpace &&
orc/src/java/org/apache/orc/impl/WriterImpl.java,2849,addBlockPadding) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,2850,byte[] pad = new byte[(int) Math.min(HDFS_BUFFER_SIZE, availBlockSpace)];
orc/src/java/org/apache/orc/impl/WriterImpl.java,2851,LOG.info(String.format("Padding ORC by %d bytes while merging..",
orc/src/java/org/apache/orc/impl/WriterImpl.java,2852,availBlockSpace));
orc/src/java/org/apache/orc/impl/WriterImpl.java,2853,start += availBlockSpace;
orc/src/java/org/apache/orc/impl/WriterImpl.java,2854,while (availBlockSpace > 0) {
orc/src/java/org/apache/orc/impl/WriterImpl.java,2855,int writeLen = (int) Math.min(availBlockSpace, pad.length);
orc/src/java/org/apache/orc/impl/WriterImpl.java,2856,rawWriter.write(pad, 0, writeLen);
orc/src/java/org/apache/orc/impl/WriterImpl.java,2857,availBlockSpace -= writeLen;
orc/src/java/org/apache/orc/impl/WriterImpl.java,2861,rawWriter.write(stripe);
orc/src/java/org/apache/orc/impl/WriterImpl.java,2872,OrcProto.StripeInformation dirEntry = OrcProto.StripeInformation
orc/src/java/org/apache/orc/impl/WriterImpl.java,2879,.build();
orc/src/java/org/apache/orc/impl/WriterImpl.java,2880,stripes.add(dirEntry);
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,301,disableJoinMerge = false;
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,346,disableJoinMerge = false;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,404,disableJoinMerge = false;
llap-common/src/java/org/apache/hadoop/hive/llap/security/LlapTokenIdentifier.java,108,return KIND + "; " + super.toString() + ", cluster " + clusterId + ", app ID " + appId;
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/ContainerRunnerImpl.java,178,LlapTokenInfo tokenInfo = LlapTokenChecker.getTokenInfo(clusterId);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/ContainerRunnerImpl.java,271,signer.checkSignature(vertexBinary.toByteArray(),
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/ContainerRunnerImpl.java,272,request.getWorkSpecSignature().toByteArray(), (int)vertex.getSignatureKeyId());
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTokenChecker.java,59,private static List<LlapTokenIdentifier> getLlapTokens(
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTokenChecker.java,68,if (!clusterId.equals(llapId.getClusterId())) continue;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapJoinOuterFilteredOperator.java,98,vectorExtractRow.init((StructObjectInspector) inputObjInspectors[0], vContext.getProjectedColumns());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/udf/VectorUDFAdaptor.java,334,throw new RuntimeException("Unhandled object type " + outputOI.getTypeName());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1776,GenericUDF genericUDF = genericUDFExpr.getGenericUDF();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1777,if (genericUDF instanceof GenericUDFBridge) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1778,Class<? extends UDF> udf = ((GenericUDFBridge) genericUDF).getUdfClass();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1779,return supportedGenericUDFs.contains(udf);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1781,return supportedGenericUDFs.contains(genericUDF.getClass());
ql/src/java/org/apache/hadoop/hive/ql/io/avro/AvroGenericRecordReader.java,146,private boolean pathIsInPartition(Path split, String partitionPath) {
ql/src/java/org/apache/hadoop/hive/ql/io/avro/AvroGenericRecordReader.java,149,String schemelessPartitionPath = new Path(partitionPath).toUri().getPath();
ql/src/java/org/apache/hadoop/hive/ql/io/avro/AvroGenericRecordReader.java,150,return split.toString().startsWith(schemelessPartitionPath);
ql/src/java/org/apache/hadoop/hive/ql/io/avro/AvroGenericRecordReader.java,152,return split.toString().startsWith(partitionPath);
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,254,if (mWork.getNumMapTasks() != null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,255,job.setNumMapTasks(mWork.getNumMapTasks().intValue());
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,258,if (mWork.getMaxSplitSize() != null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,259,HiveConf.setLongVar(job, HiveConf.ConfVars.MAPREDMAXSPLITSIZE, mWork.getMaxSplitSize().longValue());
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,262,if (mWork.getMinSplitSize() != null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,263,HiveConf.setLongVar(job, HiveConf.ConfVars.MAPREDMINSPLITSIZE, mWork.getMinSplitSize().longValue());
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,266,if (mWork.getMinSplitSizePerNode() != null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,267,HiveConf.setLongVar(job, HiveConf.ConfVars.MAPREDMINSPLITSIZEPERNODE, mWork.getMinSplitSizePerNode().longValue());
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,270,if (mWork.getMinSplitSizePerRack() != null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,271,HiveConf.setLongVar(job, HiveConf.ConfVars.MAPREDMINSPLITSIZEPERRACK, mWork.getMinSplitSizePerRack().longValue());
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,71,AclStatus aclStatus;
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,73,aclStatus =  sourceStatus.getAclStatus();
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,74,if (aclStatus != null) {
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,75,LOG.trace(aclStatus.toString());
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,76,aclEntries = aclStatus.getEntries();
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,196,public AclStatus getAclStatus() {
shims/common/src/main/java/org/apache/hadoop/hive/io/HdfsUtils.java,197,return aclStatus;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,94,boolean[] isConvert;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,97,int[] projectionColumnNums;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,101,Category[] sourceCategories;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,104,PrimitiveCategory[] sourcePrimitiveCategories;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,107,int[] maxLengths;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,228,if (columnsToIncludeTruncated != null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,229,deserializeRead.setColumnsToInclude(columnsToIncludeTruncated);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,232,final int columnCount = (columnsToIncludeTruncated == null ?
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,233,sourceTypeInfos.length : columnsToIncludeTruncated.length);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,266,public int initConversion(TypeInfo[] targetTypeInfos,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,269,if (columnsToIncludeTruncated != null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,270,deserializeRead.setColumnsToInclude(columnsToIncludeTruncated);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,273,int targetColumnCount;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,274,if (columnsToIncludeTruncated == null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,275,targetColumnCount = targetTypeInfos.length;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,277,targetColumnCount = Math.min(targetTypeInfos.length, columnsToIncludeTruncated.length);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,280,int sourceColumnCount = Math.min(sourceTypeInfos.length, targetColumnCount);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,281,allocateArrays(sourceColumnCount);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,282,allocateConvertArrays(sourceColumnCount);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,285,for (int i = 0; i < sourceColumnCount; i++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,328,return sourceColumnCount;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,343,private void deserializeRowColumn(VectorizedRowBatch batch, int batchIndex,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,360,switch (sourceCategory) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,463,throw new RuntimeException("Category " + sourceCategory.name() + " not supported");
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,481,private void deserializeConvertRowColumn(VectorizedRowBatch batch, int batchIndex,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,499,switch (sourceCategory) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,502,PrimitiveCategory sourcePrimitiveCategory = sourcePrimitiveCategories[logicalColumnIndex];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,503,switch (sourcePrimitiveCategory) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,616,throw new RuntimeException("Primitive category " + sourcePrimitiveCategory.name() +
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,622,throw new RuntimeException("Category " + sourceCategory.name() + " not supported");
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,666,deserializeRead.extraFieldsCheck();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,266,readerColumnCount =
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,282,readerColumnCount =
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,71,public void adaptPutRow(VectorMapJoinOptimizedHashTable hashTable,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,73,throws SerDeException, HiveException, IOException {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,75,if (useMinMax) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,77,byte[] keyBytes = currentKey.getBytes();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,78,int keyLength = currentKey.getLength();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,79,keyBinarySortableDeserializeRead.set(keyBytes, 0, keyLength);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,80,if (keyBinarySortableDeserializeRead.readCheckNull()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,81,if (isOuterJoin) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,82,return;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,85,throw new HiveException("Unexpected NULL");
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,88,long key = 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,89,switch (hashTableKeyType) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,90,case BOOLEAN:
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,91,key = (keyBinarySortableDeserializeRead.readBoolean() ? 1 : 0);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,92,break;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,93,case BYTE:
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,94,key = (long) keyBinarySortableDeserializeRead.readByte();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,95,break;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,96,case SHORT:
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,97,key = (long) keyBinarySortableDeserializeRead.readShort();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,98,break;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,99,case INT:
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,100,key = (long) keyBinarySortableDeserializeRead.readInt();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,101,break;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,102,case LONG:
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,103,key = keyBinarySortableDeserializeRead.readLong();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,104,break;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,105,default:
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,106,throw new RuntimeException("Unexpected hash table key type " + hashTableKeyType.name());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,108,if (key < min) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,109,min = key;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,111,if (key > max) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,112,max = key;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedLongCommon.java,120,hashTable.putRowInternal(currentKey, currentValue);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedStringCommon.java,49,private BytesWritable bytesWritable;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedStringCommon.java,51,public void adaptPutRow(VectorMapJoinOptimizedHashTable hashTable,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedStringCommon.java,53,throws SerDeException, HiveException, IOException {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedStringCommon.java,55,byte[] keyBytes = currentKey.getBytes();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedStringCommon.java,56,int keyLength = currentKey.getLength();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedStringCommon.java,57,keyBinarySortableDeserializeRead.set(keyBytes, 0, keyLength);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedStringCommon.java,58,if (keyBinarySortableDeserializeRead.readCheckNull()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedStringCommon.java,59,if (isOuterJoin) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedStringCommon.java,60,return;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedStringCommon.java,63,throw new HiveException("Unexpected NULL");
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedStringCommon.java,66,keyBinarySortableDeserializeRead.readString(readStringResults);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedStringCommon.java,68,bytesWritable.set(readStringResults.bytes, readStringResults.start, readStringResults.length);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/optimized/VectorMapJoinOptimizedStringCommon.java,70,hashTable.putRowInternal(bytesWritable, currentValue);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,608,pd.setVectorPartitionDesc(
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,609,VectorPartitionDesc.createVectorDeserialize(
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,610,inputFileFormatClassName, VectorDeserializeType.LAZY_SIMPLE));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,612,return true;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,70,private boolean readBeyondConfiguredFieldsWarned;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,72,private boolean bufferRangeHasExtraDataWarned;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,94,readBeyondConfiguredFieldsWarned = false;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,96,bufferRangeHasExtraDataWarned = false;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,121,public boolean readCheckNull() throws IOException {
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,129,if (!readBeyondConfiguredFieldsWarned) {
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,130,doReadBeyondConfiguredFieldsWarned();
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,132,return true;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,140,return true;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,145,return true;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,151,boolean isNull = false;    // Assume.
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,155,break;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,158,break;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,166,break;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,176,break;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,186,break;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,196,break;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,208,break;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,225,break;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,242,break;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,257,break;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,267,break;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,281,break;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,347,isNull = true;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,353,break;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,361,if (columnsToInclude != null && !columnsToInclude[fieldIndex]) {
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,362,isNull = true;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,365,return isNull;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,371,public void extraFieldsCheck() {
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,372,if (!inputByteBuffer.isEof()) {
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,374,if (!bufferRangeHasExtraDataWarned) {
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,376,int length = inputByteBuffer.getEnd() - start;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,377,int remaining = inputByteBuffer.getEnd() - inputByteBuffer.tell();
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,378,LOG.info("Not all fields were read in the buffer range! Buffer range " +  start
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,379,+ " for length " + length + " but " + remaining + " bytes remain. "
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,380,+ "(total buffer length " + inputByteBuffer.getData().length + ")"
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,381,+ "  Ignoring similar problems.");
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,382,bufferRangeHasExtraDataWarned = true;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,391,public boolean readBeyondConfiguredFieldsWarned() {
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,392,return readBeyondConfiguredFieldsWarned;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,399,public boolean bufferRangeHasExtraDataWarned() {
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,400,return bufferRangeHasExtraDataWarned;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,407,private void doReadBeyondConfiguredFieldsWarned() {
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,409,LOG.info("Reading beyond configured fields! Configured " + fieldCount + " fields but "
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,410,+ " reading more (NULLs returned).  Ignoring similar problems.");
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,411,readBeyondConfiguredFieldsWarned = true;
serde/src/java/org/apache/hadoop/hive/serde2/fast/DeserializeRead.java,50,protected boolean[] columnsToInclude;
serde/src/java/org/apache/hadoop/hive/serde2/fast/DeserializeRead.java,101,columnsToInclude = null;
serde/src/java/org/apache/hadoop/hive/serde2/fast/DeserializeRead.java,120,public void setColumnsToInclude(boolean[] columnsToInclude) {
serde/src/java/org/apache/hadoop/hive/serde2/fast/DeserializeRead.java,121,this.columnsToInclude = columnsToInclude;
serde/src/java/org/apache/hadoop/hive/serde2/fast/DeserializeRead.java,127,public abstract void set(byte[] bytes, int offset, int length);
serde/src/java/org/apache/hadoop/hive/serde2/fast/DeserializeRead.java,135,public abstract boolean readCheckNull() throws IOException;
serde/src/java/org/apache/hadoop/hive/serde2/fast/DeserializeRead.java,140,public abstract void extraFieldsCheck();
serde/src/java/org/apache/hadoop/hive/serde2/fast/DeserializeRead.java,145,public abstract boolean readBeyondConfiguredFieldsWarned();
serde/src/java/org/apache/hadoop/hive/serde2/fast/DeserializeRead.java,147,public abstract boolean bufferRangeHasExtraDataWarned();
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,62,private byte separator;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,63,private boolean isEscaped;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,64,private byte escapeChar;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,65,private byte[] nullSequenceBytes;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,66,private boolean isExtendedBooleanLiteral;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,67,private boolean lastColumnTakesRest;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,71,private int offset;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,73,private int fieldCount;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,74,private int fieldIndex;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,75,private int fieldStart;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,76,private int fieldLength;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,79,private TimestampParser timestampParser;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,81,private boolean extraFieldWarned;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,82,private boolean missingFieldWarned;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,89,startPosition = new int[typeInfos.length + 1];
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,94,escapeChar = lazyParams.getEscapeChar();
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,97,lastColumnTakesRest = lazyParams.isLastColumnTakesRest();
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,99,fieldCount = typeInfos.length;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,101,extraFieldWarned = false;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,102,missingFieldWarned = false;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,110,private LazySimpleDeserializeRead() {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,111,super();
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,120,this.offset = offset;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,123,fieldIndex = -1;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,134,int structByteEnd = end;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,140,while (fieldByteEnd <= structByteEnd) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,141,if (fieldByteEnd == structByteEnd || bytes[fieldByteEnd] == separator) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,144,fieldByteEnd = structByteEnd;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,156,startPosition[i] = fieldByteEnd + 1;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,158,break;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,160,fieldByteBegin = fieldByteEnd + 1;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,161,fieldByteEnd++;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,163,if (isEscaped && bytes[fieldByteEnd] == escapeChar
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,164,&& fieldByteEnd + 1 < structByteEnd) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,174,if (!extraFieldWarned && fieldByteEnd < structByteEnd) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,175,doExtraFieldWarned();
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,191,public boolean readCheckNull() {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,192,if (fieldIndex == -1) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,194,fieldIndex = 0;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,196,return true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,198,fieldIndex++;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,202,if (columnsToInclude != null && !columnsToInclude[fieldIndex]) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,206,fieldStart = startPosition[fieldIndex];
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,207,fieldLength = startPosition[fieldIndex + 1] - startPosition[fieldIndex] - 1;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,209,return true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,214,if (fieldLength == nullSequenceBytes.length) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,215,int i = 0;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,216,while (true) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,217,if (bytes[fieldStart + i] != nullSequenceBytes[i]) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,218,break;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,220,i++;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,221,if (i >= fieldLength) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,222,return true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,231,switch (primitiveCategories[fieldIndex]) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,232,case BOOLEAN:
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,234,int i = fieldStart;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,235,if (fieldLength == 4) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,236,if ((bytes[i] == 'T' || bytes[i] == 't') &&
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,237,(bytes[i + 1] == 'R' || bytes[i + 1] == 'r') &&
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,238,(bytes[i + 2] == 'U' || bytes[i + 1] == 'u') &&
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,239,(bytes[i + 3] == 'E' || bytes[i + 3] == 'e')) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,240,currentBoolean = true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,243,return true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,246,if ((bytes[i] == 'F' || bytes[i] == 'f') &&
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,247,(bytes[i + 1] == 'A' || bytes[i + 1] == 'a') &&
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,248,(bytes[i + 2] == 'L' || bytes[i + 2] == 'l') &&
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,249,(bytes[i + 3] == 'S' || bytes[i + 3] == 's') &&
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,250,(bytes[i + 4] == 'E' || bytes[i + 4] == 'e')) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,251,currentBoolean = false;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,254,return true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,257,byte b = bytes[fieldStart];
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,258,if (b == '1' || b == 't' || b == 'T') {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,259,currentBoolean = true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,261,currentBoolean = false;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,264,return true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,268,return true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,271,break;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,272,case BYTE:
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,273,if (!LazyUtils.isNumberMaybe(bytes, fieldStart, fieldLength)) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,276,try {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,279,logExceptionMessage(bytes, fieldStart, fieldLength, "TINYINT");
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,280,return true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,282,break;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,283,case SHORT:
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,284,if (!LazyUtils.isNumberMaybe(bytes, fieldStart, fieldLength)) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,287,try {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,290,logExceptionMessage(bytes, fieldStart, fieldLength, "SMALLINT");
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,293,break;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,294,case INT:
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,295,if (!LazyUtils.isNumberMaybe(bytes, fieldStart, fieldLength)) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,296,return true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,298,try {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,301,logExceptionMessage(bytes, fieldStart, fieldLength, "INT");
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,304,break;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,305,case LONG:
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,306,if (!LazyUtils.isNumberMaybe(bytes, fieldStart, fieldLength)) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,307,return true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,309,try {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,312,logExceptionMessage(bytes, fieldStart, fieldLength, "BIGINT");
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,315,break;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,316,case FLOAT:
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,319,return true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,321,String byteData = null;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,322,try {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,323,byteData = Text.decode(bytes, fieldStart, fieldLength);
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,324,currentFloat = Float.parseFloat(byteData);
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,326,LOG.debug("Data not in the Float data type range so converted to null. Given data is :"
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,327,+ byteData, e);
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,328,return true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,330,LOG.debug("Data not in the Float data type range so converted to null.", e);
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,331,return true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,334,break;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,335,case DOUBLE:
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,338,return true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,340,String byteData = null;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,341,try {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,342,byteData = Text.decode(bytes, fieldStart, fieldLength);
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,343,currentDouble = Double.parseDouble(byteData);
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,345,LOG.debug("Data not in the Double data type range so converted to null. Given data is :"
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,346,+ byteData, e);
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,347,return true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,349,LOG.debug("Data not in the Double data type range so converted to null.", e);
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,350,return true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,353,break;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,355,case STRING:
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,356,case CHAR:
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,357,case VARCHAR:
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,369,break;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,370,case BINARY:
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,372,byte[] recv = new byte[fieldLength];
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,373,System.arraycopy(bytes, fieldStart, recv, 0, fieldLength);
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,374,byte[] decoded = LazyBinary.decodeIfNeeded(recv);
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,376,decoded = decoded.length > 0 ? decoded : recv;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,377,currentBytes = decoded;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,378,currentBytesStart = 0;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,379,currentBytesLength = decoded.length;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,381,break;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,382,case DATE:
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,384,if (fieldLength == 0) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,385,return true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,387,String s = null;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,388,try {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,389,s = Text.decode(bytes, fieldStart, fieldLength);
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,390,currentDateWritable.set(Date.valueOf(s));
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,392,logExceptionMessage(bytes, fieldStart, fieldLength, "DATE");
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,393,return true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,396,break;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,397,case TIMESTAMP:
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,399,if (fieldLength == 0) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,400,return true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,402,String s = null;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,403,try {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,404,s = new String(bytes, fieldStart, fieldLength, "US-ASCII");
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,406,LOG.error("Unsupported encoding found ", e);
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,407,s = "";
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,410,if (s.compareTo("NULL") == 0) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,411,logExceptionMessage(bytes, fieldStart, fieldLength, "TIMESTAMP");
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,412,return true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,415,if (timestampParser == null) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,416,timestampParser = new TimestampParser();
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,421,return true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,425,break;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,426,case INTERVAL_YEAR_MONTH:
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,428,String s = null;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,430,s = Text.decode(bytes, fieldStart, fieldLength);
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,434,return true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,437,break;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,438,case INTERVAL_DAY_TIME:
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,440,String s = null;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,442,s = Text.decode(bytes, fieldStart, fieldLength);
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,446,return true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,449,break;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,450,case DECIMAL:
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,452,if (!LazyUtils.isNumberMaybe(bytes, fieldStart, fieldLength)) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,472,return true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,476,break;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,478,default:
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,479,throw new Error("Unexpected primitive category " + primitiveCategories[fieldIndex].name());
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,482,return false;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,487,if(LOG.isDebugEnabled()) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,502,public void extraFieldsCheck() {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,510,public boolean readBeyondConfiguredFieldsWarned() {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,511,return missingFieldWarned;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,518,public boolean bufferRangeHasExtraDataWarned() {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,522,private void doExtraFieldWarned() {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,523,extraFieldWarned = true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,524,LOG.warn("Extra bytes detected at the end of the row! Ignoring similar "
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,525,+ "problems.");
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,528,private void doMissingFieldWarned(int fieldId) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,529,missingFieldWarned = true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,530,LOG.info("Missing fields! Expected " + fieldCount + " fields but "
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,531,+ "only got " + fieldId + "! Ignoring similar problems.");
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,64,private boolean readBeyondConfiguredFieldsWarned;
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,66,private boolean bufferRangeHasExtraDataWarned;
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,73,readBeyondConfiguredFieldsWarned = false;
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,75,bufferRangeHasExtraDataWarned = false;
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,102,public boolean readCheckNull() throws IOException {
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,105,if (!readBeyondConfiguredFieldsWarned) {
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,107,LOG.info("Reading beyond configured fields! Configured " + fieldCount + " fields but "
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,108,+ " reading more (NULLs returned).  Ignoring similar problems.");
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,109,readBeyondConfiguredFieldsWarned = true;
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,111,return true;
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,124,boolean isNull;
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,126,isNull = true;
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,128,isNull = false;    // Assume.
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,302,isNull = true;
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,317,if (columnsToInclude != null && !columnsToInclude[fieldIndex]) {
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,318,isNull = true;
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,336,return isNull;
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,342,public void extraFieldsCheck() {
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,343,if (offset < end) {
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,345,if (!bufferRangeHasExtraDataWarned) {
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,347,int length = end - start;
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,348,int remaining = end - offset;
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,349,LOG.info("Not all fields were read in the buffer range! Buffer range " +  start
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,350,+ " for length " + length + " but " + remaining + " bytes remain. "
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,351,+ "(total buffer length " + bytes.length + ")"
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,352,+ "  Ignoring similar problems.");
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,353,bufferRangeHasExtraDataWarned = true;
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,362,public boolean readBeyondConfiguredFieldsWarned() {
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,363,return readBeyondConfiguredFieldsWarned;
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,370,public boolean bufferRangeHasExtraDataWarned() {
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,371,return bufferRangeHasExtraDataWarned;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1281,VectorExpression[] vectorChildren = null;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1282,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1283,vectorChildren = getVectorExpressions(childExpr, Mode.PROJECTION);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1285,int i = 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1286,for (VectorExpression ve : vectorChildren) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1287,inputColumns[i++] = ve.getOutputColumn();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1291,VectorCoalesce vectorCoalesce = new VectorCoalesce(inputColumns, outColumn);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1292,vectorCoalesce.setOutputType(returnType.getTypeName());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1293,vectorCoalesce.setChildExpressions(vectorChildren);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1294,return vectorCoalesce;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1297,if (vectorChildren != null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1298,for (VectorExpression v : vectorChildren) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1299,ocm.freeOutputColumn(v.getOutputColumn());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1308,VectorExpression[] vectorChildren = null;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1309,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1310,vectorChildren = getVectorExpressions(childExpr, Mode.PROJECTION);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1312,int i = 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1313,for (VectorExpression ve : vectorChildren) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1314,inputColumns[i++] = ve.getOutputColumn();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1318,VectorElt vectorElt = new VectorElt(inputColumns, outColumn);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1319,vectorElt.setOutputType(returnType.getTypeName());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1320,vectorElt.setChildExpressions(vectorChildren);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1321,return vectorElt;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1324,if (vectorChildren != null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1325,for (VectorExpression v : vectorChildren) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1326,ocm.freeOutputColumn(v.getOutputColumn());
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/services/impl/LlapWebServices.java,91,builder.setSPNEGOPrincipal(HiveConf.getVar(conf, ConfVars.LLAP_KERBEROS_PRINCIPAL));
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/services/impl/LlapWebServices.java,92,builder.setSPNEGOKeytab(HiveConf.getVar(conf, ConfVars.LLAP_KERBEROS_KEYTAB_FILE));
ql/src/java/org/apache/hadoop/hive/ql/exec/StatsNoJobTask.java,78,private static ConcurrentMap<String, Partition> partUpdates;
ql/src/java/org/apache/hadoop/hive/ql/exec/StatsNoJobTask.java,79,private static Table table;
ql/src/java/org/apache/hadoop/hive/ql/exec/StatsNoJobTask.java,80,private static String tableFullName;
ql/src/java/org/apache/hadoop/hive/ql/exec/StatsNoJobTask.java,81,private static JobConf jc = null;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/PlanModifierForASTConv.java,193,if (colAlias.startsWith("_")) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/PlanModifierForASTConv.java,194,colAlias = colAlias.substring(1);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/PlanModifierForASTConv.java,195,colAlias = getNewColAlias(newSelAliases, colAlias);
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java,701,LlapNodeId llapNodeId = attemptToNodeMap.remove(attemptId);
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java,702,if (llapNodeId == null) {
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java,704,return;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,288,if (isTracingEnabled && LOG.isInfoEnabled()) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,289,LOG.trace("Resulting disk ranges to read (file " + fileKey + "): "
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,295,if (isTracingEnabled && LOG.isInfoEnabled()) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,296,LOG.trace("Disk ranges after cache (file " + fileKey + ", base offset " + stripeOffset
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,297,+ "): " + RecordReaderUtils.stringifyDiskRanges(toRead.next));
storage-api/src/java/org/apache/hadoop/hive/common/io/DiskRangeList.java,163,LOG.info("Creating new range; last range (which can include some previous adds) was "
orc/src/java/org/apache/orc/impl/ColumnStatisticsImpl.java,576,private HiveDecimal minimum = null;
orc/src/java/org/apache/orc/impl/ColumnStatisticsImpl.java,577,private HiveDecimal maximum = null;
orc/src/java/org/apache/orc/impl/ColumnStatisticsImpl.java,578,private HiveDecimal sum = HiveDecimal.ZERO;
orc/src/java/org/apache/orc/impl/ColumnStatisticsImpl.java,587,maximum = HiveDecimal.create(dec.getMaximum());
orc/src/java/org/apache/orc/impl/ColumnStatisticsImpl.java,590,minimum = HiveDecimal.create(dec.getMinimum());
orc/src/java/org/apache/orc/impl/ColumnStatisticsImpl.java,593,sum = HiveDecimal.create(dec.getSum());
orc/src/java/org/apache/orc/impl/ColumnStatisticsImpl.java,604,sum = HiveDecimal.ZERO;
orc/src/java/org/apache/orc/impl/ColumnStatisticsImpl.java,608,public void updateDecimal(HiveDecimal value) {
orc/src/java/org/apache/orc/impl/ColumnStatisticsImpl.java,610,minimum = value;
orc/src/java/org/apache/orc/impl/ColumnStatisticsImpl.java,611,maximum = value;
orc/src/java/org/apache/orc/impl/ColumnStatisticsImpl.java,613,minimum = value;
orc/src/java/org/apache/orc/impl/ColumnStatisticsImpl.java,615,maximum = value;
orc/src/java/org/apache/orc/impl/ColumnStatisticsImpl.java,618,sum = sum.add(value);
orc/src/java/org/apache/orc/impl/ColumnStatisticsImpl.java,627,minimum = dec.minimum;
orc/src/java/org/apache/orc/impl/ColumnStatisticsImpl.java,628,maximum = dec.maximum;
orc/src/java/org/apache/orc/impl/ColumnStatisticsImpl.java,632,minimum = dec.minimum;
orc/src/java/org/apache/orc/impl/ColumnStatisticsImpl.java,635,maximum = dec.maximum;
orc/src/java/org/apache/orc/impl/ColumnStatisticsImpl.java,640,sum = sum.add(dec.sum);
orc/src/java/org/apache/orc/impl/ColumnStatisticsImpl.java,660,if (sum != null) {
orc/src/java/org/apache/orc/impl/ColumnStatisticsImpl.java,669,return minimum;
orc/src/java/org/apache/orc/impl/ColumnStatisticsImpl.java,674,return maximum;
orc/src/java/org/apache/orc/impl/ColumnStatisticsImpl.java,679,return sum;
orc/src/java/org/apache/orc/impl/ColumnStatisticsImpl.java,990,public void updateDecimal(HiveDecimal value) {
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,793,(float) decimalColVector.vector[elementNum].getHiveDecimal().doubleValue();
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,986,decimalColVector.vector[elementNum].getHiveDecimal().doubleValue();
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1479,String string = decimalColVector.vector[elementNum].getHiveDecimal().toString();
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1480,byte[] bytes = string.getBytes();
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,1481,assignStringGroupVectorEntry(bytesColVector, elementNum, readerType, bytes);
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,1112,BigInteger bInt = SerializationUtils.readBigInteger(valueStream);
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,1113,HiveDecimal dec = HiveDecimal.create(bInt, scratchScaleVector[r]);
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,1114,result.set(r, dec);
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,1119,BigInteger bInt = SerializationUtils.readBigInteger(valueStream);
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,1120,HiveDecimal dec = HiveDecimal.create(bInt, scratchScaleVector[r]);
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,1121,result.set(r, dec);
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,1131,SerializationUtils.readBigInteger(valueStream);
orc/src/java/org/apache/orc/impl/WriterImpl.java,1910,HiveDecimal value = vec.vector[0].getHiveDecimal();
orc/src/java/org/apache/orc/impl/WriterImpl.java,1913,bloomFilter.addString(value.toString());
orc/src/java/org/apache/orc/impl/WriterImpl.java,1916,SerializationUtils.writeBigInteger(valueStream,
orc/src/java/org/apache/orc/impl/WriterImpl.java,1917,value.unscaledValue());
orc/src/java/org/apache/orc/impl/WriterImpl.java,1924,HiveDecimal value = vec.vector[i + offset].getHiveDecimal();
orc/src/java/org/apache/orc/impl/WriterImpl.java,1925,SerializationUtils.writeBigInteger(valueStream,
orc/src/java/org/apache/orc/impl/WriterImpl.java,1926,value.unscaledValue());
orc/src/java/org/apache/orc/impl/WriterImpl.java,1930,bloomFilter.addString(value.toString());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,446,batchIndex, deserializeRead.currentHiveDecimalWritable.getHiveDecimal());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorExtractRow.java,314,((DecimalColumnVector) batch.cols[projectionColumnNum]).vector[adjustedIndex].getHiveDecimal());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorHashKeyWrapper.java,112,hashcode ^= decimalValues[i].getHiveDecimal().hashCode();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorHashKeyWrapperBatch.java,776,kw.getDecimal(klh.decimalIndex).getHiveDecimal());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorSerializeRow.java,500,serializeWrite.writeHiveDecimal(colVector.vector[0].getHiveDecimal(), colVector.scale);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorSerializeRow.java,508,serializeWrite.writeHiveDecimal(colVector.vector[batchIndex].getHiveDecimal(), colVector.scale);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,333,private String [] outputColumnsTypes = new String[100];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,337,int allocateOutputColumn(String hiveTypeName) throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,344,String normalizedTypeName = getNormalizedName(hiveTypeName);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,345,int relativeCol = allocateOutputColumnInternal(normalizedTypeName);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,355,!(outputColumnsTypes)[i].equalsIgnoreCase(columnType)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,363,if (outputColCount < outputColumnsTypes.length) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,365,outputColumnsTypes[outputColCount++] = columnType;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,370,outputColumnsTypes = Arrays.copyOf(outputColumnsTypes, 2*outputColCount);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,372,outputColumnsTypes[outputColCount++] = columnType;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,398,public int allocateScratchColumn(String hiveTypeName) throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,399,return ocm.allocateOutputColumn(hiveTypeName);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,896,outCol = ocm.allocateOutputColumn(typeName);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1178,String outType;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1182,if (returnType != null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1183,outType = getNormalizedName(returnType.getTypeName()).toLowerCase();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1184,if (outType == null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1185,throw new HiveException("No vector type for type name " + returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1188,outType = ((VectorExpression) vclass.newInstance()).getOutputType();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1190,int outputCol = ocm.allocateOutputColumn(outType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1195,ve.setOutputType(outType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1290,int outColumn = ocm.allocateOutputColumn(returnType.getTypeName());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1317,int outColumn = ocm.allocateOutputColumn(returnType.getTypeName());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1499,int scratchBytesCol = ocm.allocateOutputColumn("string");
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1630,ve = getCastToLongExpression(childExpr);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1730,private Long castConstantToLong(Object scalar, TypeInfo type) throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1896,int outputCol = ocm.allocateOutputColumn("Long");
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1913,Long longValue = castConstantToLong(constantValue, child.getTypeInfo());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2083,outputCol = ocm.allocateOutputColumn(resultTypeName);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2086,String normalizedName = getNormalizedName(resultTypeName);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2088,VectorExpression ve = new VectorUDFAdaptor(expr, outputCol, normalizedName, argDescs);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2282,static String getNormalizedName(String hiveTypeName) throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2283,VectorExpressionDescriptor.ArgumentType argType = VectorExpressionDescriptor.ArgumentType.fromHiveTypeName(hiveTypeName);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2284,switch (argType) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2285,case INT_FAMILY:
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2286,return "Long";
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2287,case FLOAT_FAMILY:
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2288,return "Double";
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2289,case DECIMAL:
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2291,return hiveTypeName;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2292,case STRING:
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2293,return "String";
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2294,case CHAR:
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2296,return hiveTypeName;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2297,case VARCHAR:
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2299,return hiveTypeName;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2300,case DATE:
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2301,return "Date";
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2302,case TIMESTAMP:
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2303,return "Timestamp";
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2304,case INTERVAL_YEAR_MONTH:
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2305,case INTERVAL_DAY_TIME:
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2306,return hiveTypeName;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2307,default:
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2308,throw new HiveException("Unexpected hive type name " + hiveTypeName);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2524,String typeName = ocm.outputColumnsTypes[i];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2525,if (typeName.equalsIgnoreCase("long")) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,2526,typeName = "bigint";   // Convert our synonym to a real Hive type name.
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDecimalToBoolean.java,44,outV.vector[i] = inV.vector[i].getHiveDecimal().signum() == 0 ? 0 : 1;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDecimalToChar.java,40,protected void assign(BytesColumnVector outV, int i, byte[] bytes, int length) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDecimalToChar.java,41,StringExpr.rightTrimAndTruncate(outV, i, bytes, 0, length, maxLength);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDecimalToDouble.java,37,outV.vector[i] = inV.vector[i].getHiveDecimal().doubleValue();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDecimalToLong.java,40,outV.vector[i] = inV.vector[i].getHiveDecimal().longValue();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDecimalToString.java,40,protected void assign(BytesColumnVector outV, int i, byte[] bytes, int length) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDecimalToString.java,41,outV.setVal(i, bytes, 0, length);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDecimalToString.java,46,String s = inV.vector[i].getHiveDecimal().toString();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDecimalToString.java,47,byte[] b = null;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDecimalToString.java,48,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDecimalToString.java,49,b = s.getBytes("UTF-8");
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDecimalToString.java,52,throw new RuntimeException("Internal error:  unable to convert decimal to string", e);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDecimalToString.java,54,assign(outV, i, b, b.length);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDecimalToTimestamp.java,44,Timestamp timestamp = TimestampUtils.decimalToTimestamp(inV.vector[i].getHiveDecimal());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDecimalToVarChar.java,40,protected void assign(BytesColumnVector outV, int i, byte[] bytes, int length) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDecimalToVarChar.java,41,StringExpr.truncate(outV, i, bytes, 0, length, maxLength);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDoubleToDecimal.java,21,import org.apache.hadoop.hive.common.type.HiveDecimal;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDoubleToDecimal.java,42,String s = ((Double) inV.vector[i]).toString();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/CastDoubleToDecimal.java,43,outV.vector[i].set(HiveDecimal.create(s));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalColumnInList.java,40,private transient HashSet<HiveDecimal> inSet;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalColumnInList.java,64,inSet = new HashSet<HiveDecimal>(inListValues.length);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalColumnInList.java,66,inSet.add(val);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalColumnInList.java,91,outputVector[0] = inSet.contains(vector[0].getHiveDecimal()) ? 1 : 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalColumnInList.java,96,outputVector[i] = inSet.contains(vector[i].getHiveDecimal()) ? 1 : 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalColumnInList.java,100,outputVector[i] = inSet.contains(vector[i].getHiveDecimal()) ? 1 : 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalColumnInList.java,109,outputVector[0] = inSet.contains(vector[0].getHiveDecimal()) ? 1 : 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalColumnInList.java,120,outputVector[i] = inSet.contains(vector[i].getHiveDecimal()) ? 1 : 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalColumnInList.java,127,outputVector[i] = inSet.contains(vector[i].getHiveDecimal()) ? 1 : 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,37,return left.compareTo(writableRight.getHiveDecimal());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,43,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,44,outputColVector.set(i, left.add(right));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,53,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,54,outputColVector.set(i, left.getHiveDecimal().add(right.getHiveDecimal()));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,63,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,64,outputColVector.set(i, left.getHiveDecimal().add(right));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,73,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,74,outputColVector.set(i, left.add(right.getHiveDecimal()));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,84,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,85,outputColVector.set(i, left.subtract(right));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,94,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,95,outputColVector.set(i, left.getHiveDecimal().subtract(right.getHiveDecimal()));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,104,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,105,outputColVector.set(i, left.getHiveDecimal().subtract(right));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,114,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,115,outputColVector.set(i, left.subtract(right.getHiveDecimal()));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,125,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,126,outputColVector.set(i, left.multiply(right));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,135,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,136,outputColVector.set(i, left.getHiveDecimal().multiply(right.getHiveDecimal()));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,145,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,146,outputColVector.set(i, left.getHiveDecimal().multiply(right));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,155,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,156,outputColVector.set(i, left.multiply(right.getHiveDecimal()));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,166,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,167,outputColVector.set(i, left.divide(right));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,176,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,177,outputColVector.set(i, left.getHiveDecimal().divide(right.getHiveDecimal()));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,186,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,187,outputColVector.set(i, left.getHiveDecimal().divide(right));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,196,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,197,outputColVector.set(i, left.divide(right.getHiveDecimal()));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,207,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,208,outputColVector.set(i, left.remainder(right));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,217,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,218,outputColVector.set(i, left.getHiveDecimal().remainder(right.getHiveDecimal()));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,227,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,228,outputColVector.set(i, left.getHiveDecimal().remainder(right));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,237,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,238,outputColVector.set(i, left.remainder(right.getHiveDecimal()));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,246,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,247,outputColVector.set(i, input.setScale(0, HiveDecimal.ROUND_FLOOR));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,255,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,256,outputColVector.set(i, input.getHiveDecimal().setScale(0, HiveDecimal.ROUND_FLOOR));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,264,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,265,outputColVector.set(i, input.setScale(0, HiveDecimal.ROUND_CEILING));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,273,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,274,outputColVector.set(i, input.getHiveDecimal().setScale(0, HiveDecimal.ROUND_CEILING));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,282,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,283,outputColVector.set(i, RoundUtils.round(input, decimalPlaces));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,291,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,292,outputColVector.set(i, RoundUtils.round(input.getHiveDecimal(), decimalPlaces));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,300,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,301,outputColVector.set(i, RoundUtils.round(input, outputColVector.scale));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,309,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,310,outputColVector.set(i, RoundUtils.round(input.getHiveDecimal(), outputColVector.scale));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,318,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,319,outputColVector.set(i, RoundUtils.bround(input.getHiveDecimal(), decimalPlaces));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,327,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,328,outputColVector.set(i, RoundUtils.bround(input.getHiveDecimal(), outputColVector.scale));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,340,outputColVector.vector[i] = input.getHiveDecimal().signum();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,344,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,345,outputColVector.set(i, input.abs());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,347,outputColVector.noNulls = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,348,outputColVector.isNull[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,353,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,354,outputColVector.set(i, input.getHiveDecimal().abs());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,356,outputColVector.noNulls = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,357,outputColVector.isNull[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,362,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,363,outputColVector.set(i, input.negate());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,365,outputColVector.noNulls = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,366,outputColVector.isNull[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,371,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,372,outputColVector.set(i, input.getHiveDecimal().negate());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,374,outputColVector.noNulls = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/DecimalUtil.java,375,outputColVector.isNull[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FilterDecimalColumnInList.java,38,private transient HashSet<HiveDecimal> inSet;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FilterDecimalColumnInList.java,61,inSet = new HashSet<HiveDecimal>(inListValues.length);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FilterDecimalColumnInList.java,63,inSet.add(val);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FilterDecimalColumnInList.java,84,if (!(inSet.contains(vector[0].getHiveDecimal()))) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FilterDecimalColumnInList.java,92,if (inSet.contains(vector[i].getHiveDecimal())) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FilterDecimalColumnInList.java,100,if (inSet.contains(vector[i].getHiveDecimal())) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FilterDecimalColumnInList.java,115,if (!inSet.contains(vector[0].getHiveDecimal())) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FilterDecimalColumnInList.java,128,if (inSet.contains(vector[i].getHiveDecimal())) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FilterDecimalColumnInList.java,140,if (inSet.contains(vector[i].getHiveDecimal())) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FilterStructColumnInList.java,115,decColVector.vector[adjustedIndex].getHiveDecimal(), decColVector.scale);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FuncDecimalToLong.java,120,public String getOutputType() {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/FuncDecimalToLong.java,121,return "long";
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/StructColumnInList.java,116,decColVector.vector[adjustedIndex].getHiveDecimal(), decColVector.scale);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,64,transient private boolean isOutOfRange;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,66,public void sumValueWithNullCheck(HiveDecimalWritable writable, short scale) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,67,if (isOutOfRange) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,68,return;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,70,HiveDecimal value = writable.getHiveDecimal();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,72,sum.set(value);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,76,HiveDecimal result;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,77,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,78,result = sum.getHiveDecimal().add(value);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,80,isOutOfRange = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,81,return;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,83,sum.set(result);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,88,public void sumValueNoNullCheck(HiveDecimalWritable writable, short scale) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,89,HiveDecimal value = writable.getHiveDecimal();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,90,HiveDecimal result;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,91,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,92,result = sum.getHiveDecimal().add(value);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,94,isOutOfRange = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,95,return;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,97,sum.set(result);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,110,isOutOfRange = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,111,sum.set(HiveDecimal.ZERO);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,112,count = 0L;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,254,myagg.sumValueWithNullCheck(value, this.sumScale);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,270,myagg.sumValueWithNullCheck(values[selection[i]], this.sumScale);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,284,myagg.sumValueWithNullCheck(values[i], this.sumScale);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,305,myagg.sumValueWithNullCheck(value, this.sumScale);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,326,myagg.sumValueWithNullCheck(value, this.sumScale);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,345,myagg.sumValueWithNullCheck(values[i], this.sumScale);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,363,myagg.sumValueWithNullCheck(values[i], this.sumScale);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,392,myagg.sum.set(HiveDecimal.ZERO);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,396,HiveDecimal multiple;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,397,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,398,multiple = value.multiply(HiveDecimal.create(batchSize));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,400,myagg.isOutOfRange = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,401,return;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,403,HiveDecimal result;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,404,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,405,result = myagg.sum.getHiveDecimal().add(multiple);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,407,myagg.isOutOfRange = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,408,return;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,410,myagg.sum.set(result);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,440,HiveDecimalWritable value = vector[i];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,441,myagg.sumValueWithNullCheck(value, this.sumScale);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,454,myagg.sum.set(HiveDecimal.ZERO);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,459,HiveDecimalWritable value = vector[selected[i]];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,460,myagg.sumValueNoNullCheck(value, this.sumScale);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,472,HiveDecimalWritable value = vector[i];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,473,myagg.sumValueWithNullCheck(value, this.sumScale);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,484,myagg.sum.set(HiveDecimal.ZERO);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,489,HiveDecimalWritable value = vector[i];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,490,myagg.sumValueNoNullCheck(value, this.sumScale);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,509,if (myagg.isNull || myagg.isOutOfRange) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFAvgDecimal.java,515,resultSum.set(myagg.sum.getHiveDecimal());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,51,transient private HiveDecimalWritable sum = new HiveDecimalWritable();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,55,transient private boolean isOutOfRange;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,57,public void sumValue(HiveDecimalWritable writable, short scale) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,58,if (isOutOfRange) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,59,return;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,61,HiveDecimal value = writable.getHiveDecimal();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,63,sum.set(value);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,66,HiveDecimal result;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,67,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,68,result = sum.getHiveDecimal().add(value);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,70,isOutOfRange = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,71,return;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,73,sum.set(result);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,85,isOutOfRange = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,86,sum.set(HiveDecimal.ZERO);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,91,transient private final HiveDecimalWritable scratchDecimal;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,100,scratchDecimal = new HiveDecimalWritable();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,134,vector[0], inputVector.scale,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,140,vector, inputVector.scale,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,145,vector, inputVector.scale,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,154,vector[0], inputVector.scale,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,159,vector[0], inputVector.scale,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,166,vector, inputVector.scale,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,171,vector,inputVector.scale,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,182,short scale,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,190,myagg.sumValue(value, scale);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,198,short scale,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,207,myagg.sumValue(values[selection[i]], scale);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,215,short scale,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,222,myagg.sumValue(values[i], scale);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,230,short scale,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,244,myagg.sumValue(value, scale);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,253,short scale,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,266,myagg.sumValue(value, scale);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,274,short scale,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,286,myagg.sumValue(values[i], scale);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,295,short scale,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,305,myagg.sumValue(values[i], scale);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,327,if (myagg.isOutOfRange) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,328,return;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,337,myagg.sum.set(HiveDecimal.ZERO);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,340,HiveDecimal multiple;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,341,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,342,multiple = value.multiply(HiveDecimal.create(batchSize));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,344,myagg.isOutOfRange = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,345,return;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,347,HiveDecimal result;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,348,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,349,result = myagg.sum.getHiveDecimal().add(multiple);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,351,myagg.isOutOfRange = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,352,return;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,354,myagg.sum.set(result);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,360,iterateNoSelectionNoNulls(myagg, vector, inputVector.scale, batchSize);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,363,iterateNoSelectionHasNulls(myagg, vector, inputVector.scale, batchSize, inputVector.isNull);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,366,iterateSelectionNoNulls(myagg, vector, inputVector.scale, batchSize, batch.selected);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,369,iterateSelectionHasNulls(myagg, vector, inputVector.scale, batchSize, inputVector.isNull, batch.selected);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,376,short scale,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,386,myagg.sum.set(HiveDecimal.ZERO);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,388,HiveDecimal value = vector[i].getHiveDecimal();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,389,HiveDecimal result;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,390,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,391,result = myagg.sum.getHiveDecimal().add(value);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,393,myagg.isOutOfRange = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,394,return;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,396,myagg.sum.set(result);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,404,short scale,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,409,myagg.sum.set(HiveDecimal.ZERO);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,414,HiveDecimal value = vector[selected[i]].getHiveDecimal();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,415,HiveDecimal result;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,416,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,417,result = myagg.sum.getHiveDecimal().add(value);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,419,myagg.isOutOfRange = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,420,return;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,422,myagg.sum.set(result);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,429,short scale,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,436,myagg.sum.set(HiveDecimal.ZERO);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,439,HiveDecimal value = vector[i].getHiveDecimal();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,440,HiveDecimal result;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,441,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,442,result = myagg.sum.getHiveDecimal().add(value);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,444,myagg.isOutOfRange = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,445,return;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,447,myagg.sum.set(result);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,455,short scale,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,458,myagg.sum.set(HiveDecimal.ZERO);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,463,HiveDecimal value = vector[i].getHiveDecimal();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,464,HiveDecimal result;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,465,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,466,result = myagg.sum.getHiveDecimal().add(value);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,468,myagg.isOutOfRange = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,469,return;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,471,myagg.sum.set(result);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/aggregates/VectorUDAFSumDecimal.java,489,if (myagg.isNull || myagg.isOutOfRange) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinCommonOperator.java,372,int scratchColumn = vOutContext.allocateScratchColumn(typeName);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinCommonOperator.java,388,int scratchColumn = vOutContext.allocateScratchColumn(typeName);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinCommonOperator.java,404,String typeName = smallTableExprs.get(i).getTypeString();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinCommonOperator.java,405,int scratchColumn = vOutContext.allocateScratchColumn(typeName);
ql/src/java/org/apache/hadoop/hive/ql/io/parquet/write/DataWritableWriter.java,520,byte[] decimalBytes = hiveDecimal.setScale(scale).unscaledValue().toByteArray();
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFLog.java,62,double base = baseWritable.getHiveDecimal().bigDecimalValue().doubleValue();
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFLog.java,63,double d = writable.getHiveDecimal().bigDecimalValue().doubleValue();
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFMath.java,57,double d = writable.getHiveDecimal().bigDecimalValue().doubleValue();
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFSign.java,67,public IntWritable evaluate(HiveDecimalWritable dec) {
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFSign.java,68,if (dec == null || dec.getHiveDecimal() == null) {
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFSign.java,72,intWritable.set(dec.getHiveDecimal().signum());
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToBoolean.java,194,if (i == null) {
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToBoolean.java,197,booleanWritable.set(HiveDecimal.ZERO.compareTo(i.getHiveDecimal()) != 0);
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToByte.java,195,if (i == null) {
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToByte.java,198,byteWritable.set(i.getHiveDecimal().byteValue());
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToDouble.java,198,if (i == null) {
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToDouble.java,201,doubleWritable.set(i.getHiveDecimal().doubleValue());
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToFloat.java,199,if (i == null) {
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToFloat.java,202,floatWritable.set(i.getHiveDecimal().floatValue());
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToInteger.java,204,if (i == null) {
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToInteger.java,207,intWritable.set(i.getHiveDecimal().intValue());
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToLong.java,207,if (i == null) {
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToLong.java,210,longWritable.set(i.getHiveDecimal().longValue());
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToShort.java,197,if (i == null) {
ql/src/java/org/apache/hadoop/hive/ql/udf/UDFToShort.java,200,shortWritable.set(i.getHiveDecimal().shortValue());
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java,136,protected ObjectInspector outputOI;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java,178,result = new HiveDecimalWritable(HiveDecimal.ZERO);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java,180,outputOI = ObjectInspectorUtils.getStandardObjectInspector(inputOI,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java,181,ObjectInspectorCopyOption.JAVA);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java,191,return PrimitiveObjectInspectorFactory.getPrimitiveWritableObjectInspector(outputTypeInfo);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java,196,static class SumHiveDecimalAgg extends SumAgg<HiveDecimal> {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java,201,SumHiveDecimalAgg agg = new SumHiveDecimalAgg();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java,208,SumAgg<HiveDecimal> bdAgg = (SumAgg<HiveDecimal>) agg;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java,210,bdAgg.sum = HiveDecimal.ZERO;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java,219,if (checkDistinct((SumAgg) agg, parameters[0])) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java,242,SumHiveDecimalAgg myagg = (SumHiveDecimalAgg) agg;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java,243,if (myagg.sum == null) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java,248,myagg.sum = myagg.sum.add(PrimitiveObjectInspectorUtils.getHiveDecimal(partial, inputOI));
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java,254,SumHiveDecimalAgg myagg = (SumHiveDecimalAgg) agg;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java,255,if (myagg.empty || myagg.sum == null) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java,271,SumHiveDecimalAgg myagg = (SumHiveDecimalAgg) ss.wrappedBuf;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java,272,HiveDecimal r = myagg.empty ? null : myagg.sum;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java,285,SumHiveDecimalAgg myagg = (SumHiveDecimalAgg) ss.wrappedBuf;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java,286,return myagg.empty ? null : myagg.sum;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java,304,outputOI = ObjectInspectorUtils.getStandardObjectInspector(inputOI,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFSum.java,418,outputOI = ObjectInspectorUtils.getStandardObjectInspector(inputOI,
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFAbs.java,141,resultDecimal.set(val.getHiveDecimal().abs());
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFBRound.java,40,protected HiveDecimal round(HiveDecimal input, int scale) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFBRound.java,41,return RoundUtils.bround(input, scale);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCeil.java,53,HiveDecimal bd = input.getHiveDecimal();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCeil.java,54,decimalWritable.set(bd.setScale(0, HiveDecimal.ROUND_CEILING));
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFFloor.java,53,HiveDecimal bd = input.getHiveDecimal();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFFloor.java,54,decimalWritable.set(bd.setScale(0, HiveDecimal.ROUND_FLOOR));
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPNegative.java,85,HiveDecimal dec = ((HiveDecimalWritable)input).getHiveDecimal();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPNegative.java,86,decimalWritable.set(dec.negate());
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPNumericMinus.java,81,HiveDecimal dec = left.subtract(right);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPNumericMinus.java,82,if (dec == null) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPNumericMinus.java,83,return null;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPNumericMinus.java,85,decimalWritable.set(dec);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPNumericPlus.java,90,HiveDecimal dec = left.add(right);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPNumericPlus.java,92,if (dec == null) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPNumericPlus.java,93,return null;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPNumericPlus.java,96,decimalWritable.set(dec);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFRound.java,204,HiveDecimalWritable decimalWritable = (HiveDecimalWritable) inputOI.getPrimitiveWritableObject(input);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFRound.java,205,HiveDecimal dec = round(decimalWritable.getHiveDecimal(), scale);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFRound.java,206,if (dec == null) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFRound.java,207,return null;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFRound.java,209,return new HiveDecimalWritable(dec);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFRound.java,258,protected HiveDecimal round(HiveDecimal input, int scale) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFRound.java,259,return RoundUtils.round(input, scale);
serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerdeUtils.java,219,dec = dec.setScale(scale);
serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerdeUtils.java,220,return AvroSerdeUtils.getBufferFromBytes(dec.unscaledValue().toByteArray());
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/BinarySortableSerDe.java,983,public static void serializeHiveDecimal(ByteStream.Output buffer, HiveDecimal dec, boolean invert) {
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/BinarySortableSerDe.java,985,int sign = dec.compareTo(HiveDecimal.ZERO);
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/BinarySortableSerDe.java,988,dec = dec.abs();
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/BinarySortableSerDe.java,993,int factor = dec.bigDecimalValue().precision() - dec.bigDecimalValue().scale();
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/BinarySortableSerDe.java,997,dec.scaleByPowerOfTen(Math.abs(dec.scale()));
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/BinarySortableSerDe.java,998,String digits = dec.unscaledValue().toString();
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,27,import org.apache.hadoop.hive.common.type.HiveDecimal;
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,328,String digits = new String(tempDecimalBuffer, 0, length, BinarySortableSerDe.decimalCharSet);
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,329,BigInteger bi = new BigInteger(digits);
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,330,HiveDecimal bd = HiveDecimal.create(bi).scaleByPowerOfTen(factor-length);
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,332,if (!positive) {
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,333,bd = bd.negate();
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,338,currentHiveDecimalWritable.set(bd);
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,340,DecimalTypeInfo decimalTypeInfo = (DecimalTypeInfo) typeInfos[fieldIndex];
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,342,int precision = decimalTypeInfo.getPrecision();
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,343,int scale = decimalTypeInfo.getScale();
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,345,HiveDecimal decimal = currentHiveDecimalWritable.getHiveDecimal(precision, scale);
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,346,if (decimal == null) {
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableDeserializeRead.java,350,currentHiveDecimalWritable.set(decimal);
serde/src/java/org/apache/hadoop/hive/serde2/binarysortable/fast/BinarySortableSerializeWrite.java,410,BinarySortableSerDe.serializeHiveDecimal(output, dec, invert);
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveDecimal.java,69,String byteData = null;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveDecimal.java,70,try {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveDecimal.java,71,byteData = Text.decode(bytes.getData(), start, length);
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveDecimal.java,73,isNull = true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveDecimal.java,74,LOG.debug("Data not in the HiveDecimal data type range so converted to null.", e);
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveDecimal.java,75,return;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveDecimal.java,78,HiveDecimal dec = HiveDecimal.create(byteData);
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveDecimal.java,79,dec = enforcePrecisionScale(dec);
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveDecimal.java,80,if (dec != null) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveDecimal.java,81,data.set(dec);
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveDecimal.java,82,isNull = false;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveDecimal.java,85,+ byteData);
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveDecimal.java,86,isNull = true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveDecimal.java,90,private HiveDecimal enforcePrecisionScale(HiveDecimal dec) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveDecimal.java,91,return HiveDecimal.enforcePrecisionScale(dec, precision, scale);
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveDecimal.java,110,ByteBuffer b = Text.encode(hiveDecimal.toFormatString(scale));
serde/src/java/org/apache/hadoop/hive/serde2/lazy/LazyHiveDecimal.java,111,outputStream.write(b.array(), 0, b.limit());
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,26,import org.apache.hadoop.hive.common.type.HiveDecimal;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,453,return true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,455,String byteData = null;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,456,try {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,457,byteData = Text.decode(bytes, fieldStart, fieldLength);
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,459,LOG.debug("Data not in the HiveDecimal data type range so converted to null.", e);
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,460,return true;
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,463,HiveDecimal decimal = HiveDecimal.create(byteData);
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,464,DecimalTypeInfo decimalTypeInfo = (DecimalTypeInfo) typeInfos[fieldIndex];
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,465,int precision = decimalTypeInfo.getPrecision();
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,466,int scale = decimalTypeInfo.getScale();
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,467,decimal = HiveDecimal.enforcePrecisionScale(
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,468,decimal, precision, scale);
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,469,if (decimal == null) {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,470,LOG.debug("Data not in the HiveDecimal data type range so converted to null. Given data is :"
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,471,+ byteData);
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleDeserializeRead.java,474,currentHiveDecimalWritable.set(decimal);
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleSerializeWrite.java,480,public void writeHiveDecimal(HiveDecimal v, int scale) throws IOException {
serde/src/java/org/apache/hadoop/hive/serde2/lazy/fast/LazySimpleSerializeWrite.java,485,LazyHiveDecimal.writeUTF8(output, v, scale);
serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyHiveDecimalObjectInspector.java,46,HiveDecimal dec = ((LazyHiveDecimal)o).getWritableObject().getHiveDecimal();
serde/src/java/org/apache/hadoop/hive/serde2/lazy/objectinspector/primitive/LazyHiveDecimalObjectInspector.java,47,return HiveDecimalUtils.enforcePrecisionScale(dec, (DecimalTypeInfo) typeInfo);
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryHiveDecimal.java,47,LazyBinarySerDe.setFromBytes(bytes.getData(), start, length, data);
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryHiveDecimal.java,48,HiveDecimal dec = data.getHiveDecimal(precision, scale);
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinaryHiveDecimal.java,49,data = dec == null ? null : new HiveDecimalWritable(dec);
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinarySerDe.java,319,public static void setFromBytes(byte[] bytes, int offset, int length,
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinarySerDe.java,327,byte[] internalStorage = dec.getInternalStorage();
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinarySerDe.java,328,if (internalStorage.length != vInt.value) {
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinarySerDe.java,329,internalStorage = new byte[vInt.value];
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinarySerDe.java,331,System.arraycopy(bytes, offset, internalStorage, 0, vInt.value);
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinarySerDe.java,332,dec.set(internalStorage, scale);
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinarySerDe.java,336,HiveDecimalWritable dec) {
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinarySerDe.java,337,LazyBinaryUtils.writeVInt(byteStream, dec.getScale());
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinarySerDe.java,338,byte[] internalStorage = dec.getInternalStorage();
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinarySerDe.java,339,LazyBinaryUtils.writeVInt(byteStream, internalStorage.length);
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/LazyBinarySerDe.java,340,byteStream.write(internalStorage, 0, internalStorage.length);
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,25,import org.apache.hadoop.hive.common.type.HiveDecimal;
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,273,int saveStart = offset;
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,280,offset += tempVInt.length;
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,292,LazyBinarySerDe.setFromBytes(bytes, saveStart, length,
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,293,currentHiveDecimalWritable);
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,295,DecimalTypeInfo decimalTypeInfo = (DecimalTypeInfo) typeInfos[fieldIndex];
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,297,int precision = decimalTypeInfo.getPrecision();
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,298,int scale = decimalTypeInfo.getScale();
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,300,HiveDecimal decimal = currentHiveDecimalWritable.getHiveDecimal(precision, scale);
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,301,if (decimal == null) {
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinaryDeserializeRead.java,305,currentHiveDecimalWritable.set(decimal);
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinarySerializeWrite.java,59,private HiveDecimalWritable hiveDecimalWritable;
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinarySerializeWrite.java,680,public void writeHiveDecimal(HiveDecimal v, int scale) throws IOException {
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinarySerializeWrite.java,697,if (hiveDecimalWritable == null) {
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinarySerializeWrite.java,698,hiveDecimalWritable = new HiveDecimalWritable();
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinarySerializeWrite.java,700,hiveDecimalWritable.set(v);
serde/src/java/org/apache/hadoop/hive/serde2/lazybinary/fast/LazyBinarySerializeWrite.java,701,LazyBinarySerDe.writeToByteStream(output, hiveDecimalWritable);
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,582,return (byte) getInt(o, oi);
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,591,return (short) getInt(o, oi);
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,656,.getPrimitiveJavaObject(o).intValue();
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/PrimitiveObjectInspectorUtils.java,720,.getPrimitiveJavaObject(o).longValue();
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantHiveDecimalObjectInspector.java,50,DecimalTypeInfo decTypeInfo = (DecimalTypeInfo)typeInfo;
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantHiveDecimalObjectInspector.java,51,HiveDecimal dec = value == null ? null :
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantHiveDecimalObjectInspector.java,52,value.getHiveDecimal(decTypeInfo.precision(), decTypeInfo.scale());
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantHiveDecimalObjectInspector.java,53,if (dec == null) {
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantHiveDecimalObjectInspector.java,56,return new HiveDecimalWritable(dec);
serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantHiveDecimalObjectInspector.java,64,return value.getHiveDecimal().precision();
serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/HiveDecimalUtils.java,37,HiveDecimal dec = enforcePrecisionScale(writable.getHiveDecimal(), typeInfo);
serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/HiveDecimalUtils.java,38,return dec == null ? null : new HiveDecimalWritable(dec);
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,22,import java.math.RoundingMode;
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,30,public class HiveDecimal implements Comparable<HiveDecimal> {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,48,public static final HiveDecimal ZERO = new HiveDecimal(BigDecimal.ZERO);
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,49,public static final HiveDecimal ONE = new HiveDecimal(BigDecimal.ONE);
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,56,private BigDecimal bd = BigDecimal.ZERO;
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,58,private HiveDecimal(BigDecimal bd) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,59,this.bd = bd;
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,62,public static HiveDecimal create(BigDecimal b) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,63,return create(b, true);
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,66,public static HiveDecimal create(BigDecimal b, boolean allowRounding) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,67,BigDecimal bd = normalize(b, allowRounding);
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,68,return bd == null ? null : new HiveDecimal(bd);
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,71,public static HiveDecimal create(BigInteger unscaled, int scale) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,72,BigDecimal bd = normalize(new BigDecimal(unscaled, scale), true);
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,73,return bd == null ? null : new HiveDecimal(bd);
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,76,public static HiveDecimal create(String dec) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,77,BigDecimal bd;
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,78,try {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,79,bd = new BigDecimal(dec.trim());
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,84,bd = normalize(bd, true);
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,85,return bd == null ? null : new HiveDecimal(bd);
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,88,public static HiveDecimal create(BigInteger bi) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,89,BigDecimal bd = normalize(new BigDecimal(bi), true);
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,90,return bd == null ? null : new HiveDecimal(bd);
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,93,public static HiveDecimal create(int i) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,94,return new HiveDecimal(new BigDecimal(i));
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,97,public static HiveDecimal create(long l) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,98,return new HiveDecimal(new BigDecimal(l));
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,103,return bd.toPlainString();
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,112,public String toFormatString(int scale) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,113,return (bd.scale() == scale ? bd :
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,114,bd.setScale(scale, RoundingMode.HALF_UP)).toPlainString();
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,117,public HiveDecimal setScale(int i) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,118,return new HiveDecimal(bd.setScale(i, RoundingMode.HALF_UP));
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,123,return bd.compareTo(dec.bd);
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,128,return bd.hashCode();
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,136,return bd.equals(((HiveDecimal) obj).bd);
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,140,return bd.scale();
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,152,int bdPrecision = bd.precision();
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,153,int bdScale = bd.scale();
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,155,if (bdPrecision < bdScale) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,159,return bdScale;
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,161,return bdPrecision;
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,164,public int intValue() {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,165,return bd.intValue();
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,168,public double doubleValue() {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,169,return bd.doubleValue();
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,172,public long longValue() {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,173,return bd.longValue();
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,177,return bd.shortValue();
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,180,public float floatValue() {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,181,return bd.floatValue();
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,184,public BigDecimal bigDecimalValue() {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,185,return bd;
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,188,public byte byteValue() {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,189,return bd.byteValue();
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,192,public HiveDecimal setScale(int adjustedScale, int rm) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,193,return create(bd.setScale(adjustedScale, rm));
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,196,public HiveDecimal subtract(HiveDecimal dec) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,197,return create(bd.subtract(dec.bd));
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,200,public HiveDecimal multiply(HiveDecimal dec) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,201,return create(bd.multiply(dec.bd), false);
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,204,public BigInteger unscaledValue() {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,205,return bd.unscaledValue();
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,208,public HiveDecimal scaleByPowerOfTen(int n) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,209,return create(bd.scaleByPowerOfTen(n));
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,212,public HiveDecimal abs() {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,213,return create(bd.abs());
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,216,public HiveDecimal negate() {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,217,return create(bd.negate());
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,221,return create(bd.add(dec.bd));
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,224,public HiveDecimal pow(int n) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,225,BigDecimal result = normalize(bd.pow(n), false);
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,226,return result == null ? null : new HiveDecimal(result);
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,229,public HiveDecimal remainder(HiveDecimal dec) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,230,return create(bd.remainder(dec.bd));
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,233,public HiveDecimal divide(HiveDecimal dec) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,234,return create(bd.divide(dec.bd, MAX_SCALE, RoundingMode.HALF_UP), true);
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,241,public int signum() {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,242,return bd.signum();
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,245,private static BigDecimal trim(BigDecimal d) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,246,if (d.compareTo(BigDecimal.ZERO) == 0) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,248,d = BigDecimal.ZERO;
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,250,d = d.stripTrailingZeros();
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,251,if (d.scale() < 0) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,253,d = d.setScale(0);
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,256,return d;
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,259,private static BigDecimal normalize(BigDecimal bd, boolean allowRounding) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,260,if (bd == null) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,261,return null;
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,264,bd = trim(bd);
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,266,int intDigits = bd.precision() - bd.scale();
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,268,if (intDigits > MAX_PRECISION) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,272,int maxScale = Math.min(MAX_SCALE, Math.min(MAX_PRECISION - intDigits, bd.scale()));
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,273,if (bd.scale() > maxScale ) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,274,if (allowRounding) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,275,bd = bd.setScale(maxScale, RoundingMode.HALF_UP);
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,277,bd = trim(bd);
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,279,bd = null;
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,283,return bd;
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,286,private static BigDecimal enforcePrecisionScale(BigDecimal bd, int maxPrecision, int maxScale) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,287,if (bd == null) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,295,if (bd.compareTo(BigDecimal.ZERO) == 0 && bd.scale() == 0 && maxPrecision == maxScale) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,296,return bd.setScale(maxScale);
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,299,bd = trim(bd);
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,301,if (bd.scale() > maxScale) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,302,bd = bd.setScale(maxScale, RoundingMode.HALF_UP);
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,305,int maxIntDigits = maxPrecision - maxScale;
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,306,int intDigits = bd.precision() - bd.scale();
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,307,if (intDigits > maxIntDigits) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,308,return null;
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,311,return bd;
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,314,public static HiveDecimal enforcePrecisionScale(HiveDecimal dec, int maxPrecision, int maxScale) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,320,if (dec.precision() - dec.scale() <= maxPrecision - maxScale &&
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,321,dec.scale() <= maxScale) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,325,BigDecimal bd = enforcePrecisionScale(dec.bd, maxPrecision, maxScale);
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,326,if (bd == null) {
storage-api/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java,330,return HiveDecimal.create(bd);
storage-api/src/java/org/apache/hadoop/hive/common/type/RandomTypeUtil.java,60,public static class HiveDecimalAndPrecisionScale {
storage-api/src/java/org/apache/hadoop/hive/common/type/RandomTypeUtil.java,61,public HiveDecimal hiveDecimal;
storage-api/src/java/org/apache/hadoop/hive/common/type/RandomTypeUtil.java,62,public int precision;
storage-api/src/java/org/apache/hadoop/hive/common/type/RandomTypeUtil.java,63,public int scale;
storage-api/src/java/org/apache/hadoop/hive/common/type/RandomTypeUtil.java,65,HiveDecimalAndPrecisionScale(HiveDecimal hiveDecimal, int precision, int scale) {
storage-api/src/java/org/apache/hadoop/hive/common/type/RandomTypeUtil.java,66,this.hiveDecimal = hiveDecimal;
storage-api/src/java/org/apache/hadoop/hive/common/type/RandomTypeUtil.java,67,this.precision = precision;
storage-api/src/java/org/apache/hadoop/hive/common/type/RandomTypeUtil.java,68,this.scale = scale;
storage-api/src/java/org/apache/hadoop/hive/common/type/RandomTypeUtil.java,72,public static HiveDecimalAndPrecisionScale getRandHiveDecimal(Random r) {
storage-api/src/java/org/apache/hadoop/hive/common/type/RandomTypeUtil.java,96,HiveDecimal bd = HiveDecimal.create(sb.toString());
storage-api/src/java/org/apache/hadoop/hive/common/type/RandomTypeUtil.java,97,precision = bd.precision();
storage-api/src/java/org/apache/hadoop/hive/common/type/RandomTypeUtil.java,98,scale = bd.scale();
storage-api/src/java/org/apache/hadoop/hive/common/type/RandomTypeUtil.java,99,if (scale > precision) {
storage-api/src/java/org/apache/hadoop/hive/common/type/RandomTypeUtil.java,101,continue;
storage-api/src/java/org/apache/hadoop/hive/common/type/RandomTypeUtil.java,105,precision = HiveDecimal.SYSTEM_DEFAULT_PRECISION;
storage-api/src/java/org/apache/hadoop/hive/common/type/RandomTypeUtil.java,106,scale = HiveDecimal.SYSTEM_DEFAULT_SCALE;
storage-api/src/java/org/apache/hadoop/hive/common/type/RandomTypeUtil.java,107,return new HiveDecimalAndPrecisionScale(bd, precision, scale);
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/DecimalColumnVector.java,48,vector[i] = new HiveDecimalWritable(HiveDecimal.ZERO);
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/DecimalColumnVector.java,74,HiveDecimal hiveDec =
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/DecimalColumnVector.java,75,((DecimalColumnVector) inputVector).vector[inputElementNum]
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/DecimalColumnVector.java,76,.getHiveDecimal(precision, scale);
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/DecimalColumnVector.java,77,if (hiveDec == null) {
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/DecimalColumnVector.java,82,vector[outElementNum].set(hiveDec);
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/DecimalColumnVector.java,103,if (writeable == null) {
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/DecimalColumnVector.java,107,HiveDecimal hiveDec = writeable.getHiveDecimal(precision, scale);
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/DecimalColumnVector.java,108,if (hiveDec == null) {
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/DecimalColumnVector.java,109,noNulls = false;
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/DecimalColumnVector.java,110,isNull[elementNum] = true;
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/DecimalColumnVector.java,112,vector[elementNum].set(hiveDec);
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/DecimalColumnVector.java,118,HiveDecimal checkedDec = HiveDecimal.enforcePrecisionScale(hiveDec, precision, scale);
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/DecimalColumnVector.java,119,if (checkedDec == null) {
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/DecimalColumnVector.java,123,vector[elementNum].set(checkedDec);
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/DecimalColumnVector.java,129,HiveDecimal minimumNonZeroValue = HiveDecimal.create(BigInteger.ONE, scale);
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/DecimalColumnVector.java,130,vector[elementNum].set(minimumNonZeroValue);
storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/DecimalColumnVector.java,147,vector[i] = new HiveDecimalWritable(HiveDecimal.ZERO);
storage-api/src/java/org/apache/hadoop/hive/ql/util/TimestampUtils.java,69,public static Timestamp decimalToTimestamp(HiveDecimal d) {
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,30,public class HiveDecimalWritable implements WritableComparable<HiveDecimalWritable> {
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,32,private byte[] internalStorage = new byte[0];
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,33,private int scale;
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,38,public HiveDecimalWritable(String value) {
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,39,set(HiveDecimal.create(value));
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,42,public HiveDecimalWritable(byte[] bytes, int scale) {
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,43,set(bytes, scale);
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,47,set(writable.getHiveDecimal());
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,54,public HiveDecimalWritable(long value) {
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,55,set((HiveDecimal.create(value)));
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,59,set(value.unscaledValue().toByteArray(), value.scale());
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,63,set(HiveDecimal.enforcePrecisionScale(value, maxPrecision, maxScale));
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,67,set(writable.getHiveDecimal());
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,70,public void set(byte[] bytes, int scale) {
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,71,this.internalStorage = bytes;
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,72,this.scale = scale;
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,76,return HiveDecimal.create(new BigInteger(internalStorage), scale);
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,87,return HiveDecimal.enforcePrecisionScale(HiveDecimal.
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,88,create(new BigInteger(internalStorage), scale),
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,89,maxPrecision, maxScale);
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,94,scale = WritableUtils.readVInt(in);
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,96,if (internalStorage.length != byteArrayLen) {
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,97,internalStorage = new byte[byteArrayLen];
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,99,in.readFully(internalStorage);
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,104,WritableUtils.writeVInt(out, scale);
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,105,WritableUtils.writeVInt(out, internalStorage.length);
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,106,out.write(internalStorage);
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,110,public int compareTo(HiveDecimalWritable that) {
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,111,return getHiveDecimal().compareTo(that.getHiveDecimal());
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,116,return getHiveDecimal().toString();
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,127,HiveDecimalWritable bdw = (HiveDecimalWritable) other;
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,133,return getHiveDecimal().compareTo(bdw.getHiveDecimal()) == 0;
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,138,return getHiveDecimal().hashCode();
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,147,return internalStorage;
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,155,return scale;
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,158,public static
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,159,HiveDecimalWritable enforcePrecisionScale(HiveDecimalWritable writable,
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,160,int precision, int scale) {
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,161,if (writable == null) {
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,162,return null;
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,165,HiveDecimal dec =
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,166,HiveDecimal.enforcePrecisionScale(writable.getHiveDecimal(), precision,
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,167,scale);
storage-api/src/java/org/apache/hadoop/hive/serde2/io/HiveDecimalWritable.java,168,return dec == null ? null : new HiveDecimalWritable(dec);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/Optimizer.java,80,|| postExecHooks.contains("org.apache.hadoop.hive.ql.hooks.LineageLogger")) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorExtractRow.java,309,hiveCharWritable.set(new String(bytes, start, adjustedLength, Charsets.UTF_8), -1);
service/src/java/org/apache/hive/service/cli/CLIService.java,434,long timeout = HiveConf.getTimeVar(conf,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinGenerateResultOperator.java,418,if (batch.cols[projectedColumn] != null) {
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,727,cs.setRange(csd.getDateStats().getLowValue().getDaysSinceEpoch(),
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,728,csd.getDateStats().getHighValue().getDaysSinceEpoch());
beeline/src/java/org/apache/hive/beeline/BeeLineOpts.java,71,private boolean autoCommit = false;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,7926,if (vindex >= 0) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,7927,index[i] = -vindex - 1;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,7928,continue;
ql/src/java/org/apache/hadoop/hive/ql/parse/QBSubQuery.java,38,import org.apache.hadoop.hive.ql.parse.SubQueryDiagnostic.QBSubQueryRewrite;
ql/src/java/org/apache/hadoop/hive/ql/parse/QBSubQuery.java,330,ExprNodeDesc idDesc = new ExprNodeConstantDesc(TypeInfoFactory.stringTypeInfo,
ql/src/java/org/apache/hadoop/hive/ql/parse/QBSubQuery.java,331,str.toLowerCase());
ql/src/java/org/apache/hadoop/hive/ql/parse/QBSubQuery.java,333,defaultExprProcessor.process(node, stack, tcCtx, (Object) null, idDesc);
ql/src/java/org/apache/hadoop/hive/ql/parse/QBSubQuery.java,334,if ( colDesc != null ) {
ql/src/java/org/apache/hadoop/hive/ql/parse/QBSubQuery.java,335,String[] qualName = parentQueryRR.reverseLookup(colDesc.getColumn());
ql/src/java/org/apache/hadoop/hive/ql/parse/QBSubQuery.java,336,return parentQueryRR.get(qualName[0], qualName[1]);
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,539,ChildData childData = instancesCache.getCurrentData(name);
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,540,if (childData != null) {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,541,byte[] data = childData.getData();
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,542,if (data != null) {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,543,try {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,544,ServiceRecord srv = encoder.fromBytes(name, data);
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,545,ServiceInstance instance = new DynamicServiceInstance(srv);
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,546,return instance;
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,548,LOG.error("Unable to decode data for zkpath: {}", name);
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,549,return null;
service/src/java/org/apache/hive/service/cli/operation/GetColumnsOperation.java,182,DatabaseMetaData.columnNullable, // NULLABLE
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,1433,HIVEMETADATAONLYQUERIES("hive.optimize.metadataonly", true, ""),
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,2267,public static boolean isEmptyPath(JobConf job, Path dirPath) throws Exception {
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,3048,LOG.info("Changed input file " + strPath + " to empty file " + newPath);
ql/src/java/org/apache/hadoop/hive/ql/io/NullRowsInputFormat.java,77,LOG.debug("Null record reader in " + (isVectorMode ? "" : "non-") + "vector mode");
ql/src/java/org/apache/hadoop/hive/ql/io/OneNullRowInputFormat.java,51,private boolean processed;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java,89,private PartitionDesc changePartitionToMetadataOnly(PartitionDesc desc) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java,90,if (desc != null) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java,91,desc.setInputFileFormatClass(OneNullRowInputFormat.class);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java,92,desc.setOutputFileFormatClass(HiveIgnoreKeyTextOutputFormat.class);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java,93,desc.getProperties().setProperty(serdeConstants.SERIALIZATION_LIB,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java,94,NullStructSerDe.class.getName());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/NullScanTaskDispatcher.java,110,PartitionDesc newPartition = changePartitionToMetadataOnly(partDesc);
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java,576,IMetaStoreClient imsc = getHiveMetastoreClient(hiveConf);
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java,580,if (imsc instanceof HiveMetaStoreClient){
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java,581,return (HiveMetaStoreClient)imsc;
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java,583,return new HiveMetaStoreClient(hiveConf);
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,59,final private Cache<HiveClientCacheKey, ICacheableMetaStoreClient> hiveCache;
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,89,this(hiveConf.getInt(HCatConstants.HCAT_HIVE_CLIENT_EXPIRY_TIME,
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,90,DEFAULT_HIVE_CACHE_EXPIRY_TIME_SECONDS));
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,97,RemovalListener<HiveClientCacheKey, ICacheableMetaStoreClient> removalListener =
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,113,.build();
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,119,hiveCache.cleanUp();
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,130,long cleanupInterval = DEFAULT_HIVE_CACHE_EXPIRY_TIME_SECONDS;
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,132,if (timeout > cleanupInterval){
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,133,cleanupInterval = timeout;
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,145,.setNameFormat("HiveClientCache-cleaner-%d").build();
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,147,cleanupHandle = Executors.newScheduledThreadPool(1, daemonThreadFactory).scheduleWithFixedDelay(
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,148,cleanupThread,
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,149,timeout + 5, cleanupInterval, TimeUnit.SECONDS);
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,196,public ICacheableMetaStoreClient get(final HiveConf hiveConf) throws MetaException, IOException, LoginException {
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,258,public static class HiveClientCacheKey {
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,304,void release();
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,327,private final long expiryTime;
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,328,private static final int EXPIRY_TIME_EXTENSION_IN_MILLIS = 60 * 1000;
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,335,this.expiryTime = System.currentTimeMillis() + timeout * 1000 + EXPIRY_TIME_EXTENSION_IN_MILLIS;
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,338,public void acquire() {
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,342,public void release() {
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,343,users.decrementAndGet();
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,346,public void setExpiredFromCache() {
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,381,public void close() {
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,384,setExpiredFromCache();
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,393,public void tearDownIfUnused() {
hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HiveClientCache.java,402,public synchronized void tearDown() {
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingMetaStoreClient.java,145,if (retriesMade > 0 || hasConnectionLifeTimeReached(method)) {
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingMetaStoreClient.java,146,base.reconnect();
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingMetaStoreClient.java,147,lastConnectionTime = System.currentTimeMillis();
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingMetaStoreClient.java,231,if (connectionLifeTimeInMillis <= 0 || localMetaStore ||
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingMetaStoreClient.java,232,method.getName().equalsIgnoreCase("close")) {
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,704,METASTORE_EVENT_LISTENERS("hive.metastore.event.listeners", "", ""),
metastore/src/java/org/apache/hadoop/hive/metastore/AlterHandler.java,57,public abstract void alterTable(RawStore msdb, Warehouse wh, String dbname,
metastore/src/java/org/apache/hadoop/hive/metastore/AlterHandler.java,58,String name, Table newTable, EnvironmentContext envContext) throws InvalidOperationException,
metastore/src/java/org/apache/hadoop/hive/metastore/AlterHandler.java,59,MetaException;
metastore/src/java/org/apache/hadoop/hive/metastore/AlterHandler.java,81,public abstract Partition alterPartition(final RawStore msdb, Warehouse wh, final String dbname,
metastore/src/java/org/apache/hadoop/hive/metastore/AlterHandler.java,83,throws InvalidOperationException, InvalidObjectException, AlreadyExistsException,
metastore/src/java/org/apache/hadoop/hive/metastore/AlterHandler.java,84,MetaException;
metastore/src/java/org/apache/hadoop/hive/metastore/AlterHandler.java,104,public abstract List<Partition> alterPartitions(final RawStore msdb, Warehouse wh,
metastore/src/java/org/apache/hadoop/hive/metastore/AlterHandler.java,106,throws InvalidOperationException, InvalidObjectException, AlreadyExistsException,
metastore/src/java/org/apache/hadoop/hive/metastore/AlterHandler.java,107,MetaException;
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,78,String name, Table newt, EnvironmentContext environmentContext) throws InvalidOperationException, MetaException {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,323,throws InvalidOperationException, InvalidObjectException, AlreadyExistsException,
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,324,MetaException {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,330,FileSystem destFs = null;
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,371,Partition check_part = null;
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,382,if (tbl == null) {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,383,throw new InvalidObjectException(
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,415,srcPath = new Path(oldPartLoc);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,423,throw new InvalidOperationException("table new location " + destPath
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,425,+ srcPath + ". This operation is not supported");
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,428,srcFs.exists(srcPath); // check that src exists and also checks
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,471,LOG.info("rename done!");
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,480,LOG.error("Reverting metadata opeation failed During HDFS operation failed", e1);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,497,throws InvalidOperationException, InvalidObjectException, AlreadyExistsException,
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,498,MetaException {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,523,throw new InvalidOperationException("alter is not possible");
metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java,526,throw new InvalidOperationException("alter is not possible");
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,92,import org.apache.hadoop.hive.thrift.TUGIContainingTransport;
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1406,success = ms.commitTransaction();
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,1638,success = ms.commitTransaction();
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2119,success = ms.addPartition(part);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2120,if (success) {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2273,throws MetaException, InvalidObjectException, AlreadyExistsException, TException {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2317,if (!result.isEmpty()) {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2318,success = ms.addPartitions(dbName, tblName, result);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2322,success = success && ms.commitTransaction();
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2334,fireMetaStoreAddPartitionEvent(tbl, result, null, true);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2341,return result;
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,2601,success = success && ms.commitTransaction();
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,3354,throws InvalidOperationException, MetaException,
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,3355,TException {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,3379,throws InvalidOperationException, MetaException,
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,3380,TException {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,3409,oldPart = alterHandler.alterPartition(getMS(), wh, db_name, tbl_name, part_vals, new_part, envContext);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,3442,return;
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,3447,final List<Partition> new_parts) throws InvalidOperationException, MetaException,
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,3448,TException {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,3455,throws InvalidOperationException, MetaException,
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,3456,TException {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,3473,oldParts = alterHandler.alterPartitions(getMS(), wh, db_name, tbl_name, new_parts, environmentContext);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,3515,return;
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,3535,getMS().alterIndex(dbname, base_table_name, index_name, newIndex);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,3536,success = true;
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,3556,return;
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,3619,alterHandler.alterTable(getMS(), wh, dbname, name, newTable, envContext);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,4212,ms.addIndex(index);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5584,tbl = getMS().markPartitionForEvent(db_name, tbl_name, partName, evtType);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5588,for (MetaStoreEventListener listener : listeners) {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5589,listener.onLoadPartitionDone(new LoadPartitionDoneEvent(true, tbl, partName, this));
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5611,endFunction("markPartitionForEvent", tbl != null, ex, tbl_name);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPDivide.java,120,int intDig = Math.min(HiveDecimal.MAX_SCALE, prec1 - scale1 + scale2);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPDivide.java,121,int decDig = Math.min(HiveDecimal.MAX_SCALE, Math.max(6, scale1 + prec2 + 1));
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPDivide.java,122,int diff = intDig + decDig -  HiveDecimal.MAX_SCALE;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPDivide.java,123,if (diff > 0) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPDivide.java,124,decDig -= diff/2 + 1; // Slight negative bias.
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPDivide.java,125,intDig = HiveDecimal.MAX_SCALE - decDig;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPDivide.java,127,return TypeInfoFactory.getDecimalTypeInfo(intDig + decDig, decDig);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPMod.java,123,int prec = Math.min(HiveDecimal.MAX_PRECISION, Math.min(prec1 - scale1, prec2 - scale2) + scale);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPMod.java,124,return TypeInfoFactory.getDecimalTypeInfo(prec, scale);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPMultiply.java,101,int scale = Math.min(HiveDecimal.MAX_SCALE, scale1 + scale2 );
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPMultiply.java,102,int prec = Math.min(HiveDecimal.MAX_PRECISION, prec1 + prec2 + 1);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPMultiply.java,103,return TypeInfoFactory.getDecimalTypeInfo(prec, scale);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPNumericMinus.java,93,int prec =  Math.min(intPart + scale + 1, HiveDecimal.MAX_PRECISION);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPNumericMinus.java,94,return TypeInfoFactory.getDecimalTypeInfo(prec, scale);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPNumericPlus.java,104,int prec =  Math.min(intPart + scale + 1, HiveDecimal.MAX_PRECISION);
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFOPNumericPlus.java,105,return TypeInfoFactory.getDecimalTypeInfo(prec, scale);
common/src/java/org/apache/hive/common/util/HiveStringUtils.java,893,sb.append(org.apache.commons.lang.StringUtils.capitalize(word));
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1734,public static Set<String> getJarFilesByPath(String path){
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1735,Set<String> result = new HashSet<String>();
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1736,if (path == null || path.isEmpty()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1737,return result;
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1740,File paths = new File(path);
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1741,if (paths.exists() && paths.isDirectory()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1743,Set<File> jarFiles = new HashSet<File>();
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1744,jarFiles.addAll(org.apache.commons.io.FileUtils.listFiles(
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1745,paths, new String[]{"jar"}, true));
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1746,for (File f : jarFiles) {
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1747,result.add(f.getAbsolutePath());
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1750,String[] files = path.split(",");
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1751,Collections.addAll(result, files);
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1753,return result;
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,159,String addedFiles = Utilities.getResourceFiles(job, SessionState.ResourceType.FILE);
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,160,if (StringUtils.isNotBlank(addedFiles)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,161,HiveConf.setVar(job, ConfVars.HIVEADDEDFILES, addedFiles);
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,163,String addedJars = Utilities.getResourceFiles(job, SessionState.ResourceType.JAR);
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,164,if (StringUtils.isNotBlank(addedJars)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,165,HiveConf.setVar(job, ConfVars.HIVEADDEDJARS, addedJars);
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,167,String addedArchives = Utilities.getResourceFiles(job, SessionState.ResourceType.ARCHIVE);
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,168,if (StringUtils.isNotBlank(addedArchives)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,169,HiveConf.setVar(job, ConfVars.HIVEADDEDARCHIVES, addedArchives);
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,306,String auxJars = HiveConf.getVar(job, HiveConf.ConfVars.HIVEAUXJARS);
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,307,String addedJars = HiveConf.getVar(job, HiveConf.ConfVars.HIVEADDEDJARS);
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,308,if (StringUtils.isNotBlank(auxJars) || StringUtils.isNotBlank(addedJars)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,309,String allJars = StringUtils.isNotBlank(auxJars) ? (StringUtils.isNotBlank(addedJars) ? addedJars
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,310,+ "," + auxJars
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,312,: addedJars;
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,313,LOG.info("adding libjars: " + allJars);
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,314,initializeFiles("tmpjars", allJars);
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,318,String addedFiles = HiveConf.getVar(job, HiveConf.ConfVars.HIVEADDEDFILES);
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,319,if (StringUtils.isNotBlank(addedFiles)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,320,initializeFiles("tmpfiles", addedFiles);
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,329,String addedArchives = HiveConf.getVar(job, HiveConf.ConfVars.HIVEADDEDARCHIVES);
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,331,if (StringUtils.isNotBlank(addedArchives)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,332,initializeFiles("tmparchives", addedArchives);
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,725,String auxJars = HiveConf.getVar(conf, HiveConf.ConfVars.HIVEAUXJARS);
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,726,String addedJars = HiveConf.getVar(conf, HiveConf.ConfVars.HIVEADDEDJARS);
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,730,if (StringUtils.isNotBlank(auxJars)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,731,loader = Utilities.addToClassPath(loader, StringUtils.split(auxJars, ","));
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,733,if (StringUtils.isNotBlank(addedJars)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java,734,loader = Utilities.addToClassPath(loader, StringUtils.split(addedJars, ","));
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java,151,String libJarsOption;
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java,152,String addedJars = Utilities.getResourceFiles(conf, SessionState.ResourceType.JAR);
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java,153,conf.setVar(ConfVars.HIVEADDEDJARS, addedJars);
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java,154,String auxJars = conf.getAuxJars();
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java,156,if (StringUtils.isEmpty(addedJars)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java,157,if (StringUtils.isEmpty(auxJars)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java,158,libJarsOption = " ";
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java,160,libJarsOption = " -libjars " + auxJars + " ";
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java,163,if (StringUtils.isEmpty(auxJars)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java,164,libJarsOption = " -libjars " + addedJars + " ";
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java,166,libJarsOption = " -libjars " + addedJars + "," + auxJars + " ";
ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java,197,String files = Utilities.getResourceFiles(conf, SessionState.ResourceType.FILE);
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,1120,Set<String> jarPaths = Utilities.getJarFilesByPath(renewableJarPath);
common/src/java/org/apache/hive/common/util/HiveStringUtils.java,50,import org.apache.hadoop.util.StringUtils;
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1775,public static ClassLoader addToClassPath(ClassLoader cloader, String[] newPaths) throws Exception {
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1806,public static void removeFromClassPath(String[] pathsToRemove) throws Exception {
ql/src/java/org/apache/hadoop/hive/ql/processors/ReloadProcessor.java,42,ss.reloadAuxJars();
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,27,import java.net.InetAddress;
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,47,import org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,79,import org.apache.hadoop.hive.ql.plan.HiveOperation;
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,1111,public void reloadAuxJars() throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,1128,try {
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,1129,if (preReloadableAuxJars != null && !preReloadableAuxJars.isEmpty()) {
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,1130,Utilities.removeFromClassPath(preReloadableAuxJars.toArray(new String[0]));
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,1133,String msg = "Fail to remove the reloaded jars loaded last time: " + e;
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,1134,throw new IOException(msg, e);
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,1137,try {
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,1138,if (reloadedAuxJars != null && !reloadedAuxJars.isEmpty()) {
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,1139,URLClassLoader currentCLoader =
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,1140,(URLClassLoader) SessionState.get().getConf().getClassLoader();
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,1141,currentCLoader =
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,1142,(URLClassLoader) Utilities.addToClassPath(currentCLoader,
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,1143,reloadedAuxJars.toArray(new String[0]));
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,1144,sessionConf.setClassLoader(currentCLoader);
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,1145,Thread.currentThread().setContextClassLoader(currentCLoader);
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,1147,preReloadableAuxJars.clear();
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,1148,preReloadableAuxJars.addAll(reloadedAuxJars);
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,1150,String msg =
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,1152,throw new IOException(msg, e);
service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java,167,sessionState.reloadAuxJars();
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFRowNumber.java,67,RowNumberBuffer() {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFRowNumber.java,69,nextRow = 1;
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFRowNumber.java,73,rowNums.add(new IntWritable(nextRow++));
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFRowNumber.java,77,public static class GenericUDAFRowNumberEvaluator extends GenericUDAFEvaluator {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFRowNumber.java,92,return new RowNumberBuffer();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java,1074,List<String> childColLists = cppCtx.genColLists(op);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java,1075,if (childColLists == null) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java,1107,if (!childColLists.contains(internalName)) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ColumnPrunerProcFactory.java,1165,boolean[] flags = getPruneReduceSinkOpRetainFlags(childColLists,
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,250,JoinUtil.getStandardObjectInspectors(rowContainerObjectInspectors,NOTSKIPBIGTABLE, tagLen);
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,429,private void createForwardJoinObject(boolean[] skip) throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,442,internalForward(forwardCache, outputObjInspector);
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,443,countAfterReport = 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,468,if (aliasNum < numAliases) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,470,boolean[] skip = skipVectors[aliasNum];
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,471,boolean[] prevSkip = skipVectors[aliasNum - 1];
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,473,JoinCondDesc joinCond = condn[aliasNum - 1];
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,474,int type = joinCond.getType();
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,475,int left = joinCond.getLeft();
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,476,int right = joinCond.getRight();
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,479,AbstractRowContainer<List<Object>> aliasRes = storage[order[aliasNum]];
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,481,boolean done = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,482,boolean loopAgain = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,483,boolean tryLOForFO = type == JoinDesc.FULL_OUTER_JOIN;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,485,boolean rightFirst = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,486,AbstractRowContainer.RowIterator<List<Object>> iter = aliasRes.rowIter();
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,487,for (List<Object> rightObj = iter.first(); !done && rightObj != null;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,488,rightObj = loopAgain ? rightObj : iter.next(), rightFirst = loopAgain = false) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,489,System.arraycopy(prevSkip, 0, skip, 0, prevSkip.length);
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,491,boolean rightNull = rightObj == dummyObj[aliasNum];
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,492,if (hasFilter(order[aliasNum])) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,493,filterTags[aliasNum] = getFilterTag(rightObj);
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,495,skip[right] = rightNull;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,497,if (type == JoinDesc.INNER_JOIN) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,498,innerJoin(skip, left, right);
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,500,if (innerJoin(skip, left, right)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,503,done = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,506,(type == JoinDesc.FULL_OUTER_JOIN && rightNull)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,507,int result = leftOuterJoin(skip, left, right);
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,508,if (result < 0) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,509,continue;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,511,done = result > 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,513,(type == JoinDesc.FULL_OUTER_JOIN && allLeftNull)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,514,if (allLeftFirst && !rightOuterJoin(skip, left, right) ||
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,515,!allLeftFirst && !innerJoin(skip, left, right)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,516,continue;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,519,if (tryLOForFO && leftOuterJoin(skip, left, right) > 0) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,520,loopAgain = allLeftFirst;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,521,done = !loopAgain;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,522,tryLOForFO = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,524,!allLeftFirst && !innerJoin(skip, left, right)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,528,intermediate[aliasNum] = rightObj;
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,531,genObject(aliasNum + 1, allLeftFirst && rightFirst, allLeftNull && rightNull);
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,534,createForwardJoinObject(skipVectors[numAliases - 1]);
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java,756,if (!hasEmpty && !mayHasMoreThanOne) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8048,Operator op = joinOp;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8050,op = genFilterPlan(qb, condn, op, false);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8052,return op;
ql/src/java/org/apache/hadoop/hive/ql/exec/JoinOperator.java,106,if (sz == joinEmitInterval && !hasFilter(alias)) {
common/src/java/org/apache/hive/http/HttpServer.java,389,contexts.addHandler(webAppContext);
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,399,computeMaxEntriesHashAggr(hconf);
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,402,maxMemory = memoryMXBean.getHeapMemoryUsage().getMax();
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,415,private void computeMaxEntriesHashAggr(Configuration hconf) throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,417,maxHashTblMemory = (long) (memoryPercentage * Runtime.getRuntime().maxMemory());
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,975,LOG.info("Hash Table completed flushed");
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,993,if (isLogInfoEnabled) {
ql/src/java/org/apache/hadoop/hive/ql/exec/GroupByOperator.java,994,LOG.info("Hash Table flushed: new size = " + hashAggregations.size());
ql/src/java/org/apache/hadoop/hive/ql/exec/PTFTopNHash.java,43,int topN, float memUsage, boolean isMapGroupBy, BinaryCollector collector) {
ql/src/java/org/apache/hadoop/hive/ql/exec/PTFTopNHash.java,44,super.initialize(topN, memUsage, isMapGroupBy, collector);
ql/src/java/org/apache/hadoop/hive/ql/exec/PTFTopNHash.java,79,partHeap.initialize(topN, memUsage, isMapGroupBy, collector);
ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java,241,reducerHash.initialize(limit, memUsage, conf.isMapGroupBy(), this);
ql/src/java/org/apache/hadoop/hive/ql/exec/TopNHash.java,24,import java.util.List;
ql/src/java/org/apache/hadoop/hive/ql/exec/TopNHash.java,25,import java.util.Map;
ql/src/java/org/apache/hadoop/hive/ql/exec/TopNHash.java,26,import java.util.SortedSet;
ql/src/java/org/apache/hadoop/hive/ql/exec/TopNHash.java,28,import java.util.TreeSet;
ql/src/java/org/apache/hadoop/hive/ql/exec/TopNHash.java,30,import com.google.common.collect.MinMaxPriorityQueue;
ql/src/java/org/apache/hadoop/hive/ql/exec/TopNHash.java,32,import org.slf4j.Logger;
ql/src/java/org/apache/hadoop/hive/ql/exec/TopNHash.java,33,import org.slf4j.LoggerFactory;
ql/src/java/org/apache/hadoop/hive/ql/exec/TopNHash.java,37,import org.apache.hadoop.io.BinaryComparable;
ql/src/java/org/apache/hadoop/hive/ql/exec/TopNHash.java,95,int topN, float memUsage, boolean isMapGroupBy, BinaryCollector collector) {
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskExecutorService.java,259,if (numSlotsAvailable.get() == 0 && preemptionQueue.isEmpty()) {
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapDaemon.java,115,boolean ioEnabled, boolean isDirectCache, long ioMemoryBytes, String[] localDirs, int srvPort,
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapDaemon.java,116,int mngPort, int shufflePort, int webPort, String appName) {
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapDaemon.java,155,this.executorMemoryPerInstance = executorMemoryBytes;
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapDaemon.java,186,memRequired);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapDaemon.java,235,this.metrics.setMemoryPerInstance(executorMemoryBytes);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapDaemon.java,250,enablePreemption, localDirs, this.shufflePort, srvAddress, executorMemoryBytes, metrics,
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapDaemon.java,439,appName);
llap-common/src/java/org/apache/hadoop/hive/llap/LlapUtil.java,37,String principal, String keytabFile) throws IOException {
llap-common/src/java/org/apache/hadoop/hive/llap/LlapUtil.java,38,if (!UserGroupInformation.isSecurityEnabled()) return null;
llap-common/src/java/org/apache/hadoop/hive/llap/LlapUtil.java,39,if (principal.isEmpty() || keytabFile.isEmpty()) {
llap-common/src/java/org/apache/hadoop/hive/llap/LlapUtil.java,40,throw new RuntimeException("Kerberos principal and/or keytab are empty");
llap-common/src/java/org/apache/hadoop/hive/llap/LlapUtil.java,42,LOG.info("Logging in as " + principal + " via " + keytabFile);
llap-common/src/java/org/apache/hadoop/hive/llap/LlapUtil.java,43,return UserGroupInformation.loginUserFromKeytabAndReturnUGI(
llap-common/src/java/org/apache/hadoop/hive/llap/LlapUtil.java,44,SecurityUtil.getServerPrincipal(principal, "0.0.0.0"), keytabFile);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapDaemon.java,146,daemonId = new DaemonId(UserGroupInformation.getCurrentUser().getShortUserName(),
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapDaemon.java,147,LlapUtil.generateClusterName(daemonConf), hostName, appName, System.currentTimeMillis());
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,955,txnMgr.openTxn(userFromUGI);
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,2573,os.writeBytes("Last Hearbeat");
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java,112,public long openTxn(String user) throws LockException {
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DummyTxnManager.java,52,public long openTxn(String user) throws LockException {
ql/src/java/org/apache/hadoop/hive/ql/lockmgr/HiveTxnManager.java,45,long openTxn(String user) throws LockException;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/RexNodeConverter.java,151,if (rexNode instanceof RexCall) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,30,import com.google.common.annotations.VisibleForTesting;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,32,import org.slf4j.Logger;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java,33,import org.slf4j.LoggerFactory;
metastore/src/java/org/apache/hadoop/hive/metastore/RetryingMetaStoreClient.java,195,if (retriesMade >= retryLimit) {
ql/src/java/org/apache/hadoop/hive/ql/parse/UpdateDeleteSemanticAnalyzer.java,337,if (inputIsPartitioned(inputs)) {
ql/src/java/org/apache/hadoop/hive/ql/parse/UpdateDeleteSemanticAnalyzer.java,340,outputs.clear();
ql/src/java/org/apache/hadoop/hive/ql/parse/UpdateDeleteSemanticAnalyzer.java,341,for (ReadEntity input : inputs) {
ql/src/java/org/apache/hadoop/hive/ql/parse/UpdateDeleteSemanticAnalyzer.java,342,if (input.getTyp() == Entity.Type.PARTITION) {
ql/src/java/org/apache/hadoop/hive/ql/parse/UpdateDeleteSemanticAnalyzer.java,343,WriteEntity.WriteType writeType = deleting() ? WriteEntity.WriteType.DELETE :
ql/src/java/org/apache/hadoop/hive/ql/parse/UpdateDeleteSemanticAnalyzer.java,344,WriteEntity.WriteType.UPDATE;
ql/src/java/org/apache/hadoop/hive/ql/parse/UpdateDeleteSemanticAnalyzer.java,401,private boolean inputIsPartitioned(Set<ReadEntity> inputs) {
ql/src/java/org/apache/hadoop/hive/ql/parse/UpdateDeleteSemanticAnalyzer.java,404,for (ReadEntity re : inputs) {
ql/src/java/org/apache/hadoop/hive/ql/parse/UpdateDeleteSemanticAnalyzer.java,405,if (re.getTyp() == Entity.Type.PARTITION) {
ql/src/java/org/apache/hadoop/hive/ql/parse/UpdateDeleteSemanticAnalyzer.java,406,return true;
ql/src/java/org/apache/hadoop/hive/ql/parse/UpdateDeleteSemanticAnalyzer.java,409,return false;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,493,public void abortTxn(AbortTxnRequest rqst) throws NoSuchTxnException, MetaException {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,503,throw new NoSuchTxnException("No such transaction " + JavaUtils.txnIdToString(txnid));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,515,closeDbConn(dbConn);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnStore.java,99,public void abortTxn(AbortTxnRequest rqst) throws NoSuchTxnException, MetaException;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveRelFactories.java,147,return HiveJoin.getJoin(left.getCluster(), left, right, condition, joinType, false);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/HiveRelFactories.java,155,return HiveJoin.getJoin(left.getCluster(), left, right, condition, joinType, semiJoinDone);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveJoin.java,38,import org.apache.calcite.rel.type.RelDataType;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveJoin.java,61,private final boolean leftSemiJoin;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveJoin.java,69,RexNode condition, JoinRelType joinType, boolean leftSemiJoin) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveJoin.java,73,DefaultJoinAlgorithm.INSTANCE, leftSemiJoin);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveJoin.java,82,JoinAlgorithm joinAlgo, boolean leftSemiJoin) throws InvalidRelException, CalciteSemanticException {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveJoin.java,95,this.leftSemiJoin = leftSemiJoin;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveJoin.java,108,variablesStopped, joinAlgorithm, leftSemiJoin);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveJoin.java,220,public boolean isLeftSemiJoin() {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveJoin.java,221,return leftSemiJoin;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveJoin.java,245,public RelDataType deriveRowType() {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveJoin.java,246,if (leftSemiJoin) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveJoin.java,247,return deriveJoinRowType(left.getRowType(), null, JoinRelType.INNER,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveJoin.java,248,getCluster().getTypeFactory(), null,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveJoin.java,249,Collections.<RelDataTypeField> emptyList());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveJoin.java,251,return super.deriveRowType();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdDistinctRowCount.java,98,if (hjRel.isLeftSemiJoin()) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdSelectivity.java,79,double ndvCrossProduct = 1;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdSelectivity.java,121,else
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdSelectivity.java,203,private Pair<Boolean,RexNode> getCombinedPredicateForJoin(HiveJoin j, RexNode additionalPredicate) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdSize.java,85,List<Double> rights = null;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdSize.java,86,if (!rel.isLeftSemiJoin()) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdSize.java,87,rights = mq.getAverageColumnSizes(right);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/stats/HiveRelMdSize.java,131,return Math.min((double) type.getPrecision(), 100d);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/HiveOpConverter.java,166,HiveSemiJoin sj = (HiveSemiJoin) rn;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/HiveOpConverter.java,167,HiveJoin hj = HiveJoin.getJoin(sj.getCluster(), sj.getLeft(), sj.getRight(),
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/HiveOpConverter.java,168,sj.getCondition(), sj.getJoinType(), true);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/HiveOpConverter.java,169,return visit(hj);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/HiveOpConverter.java,348,extractJoinType((HiveJoin)joinRel) != JoinType.LEFTSEMI) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/HiveOpConverter.java,885,JoinType joinType = extractJoinType((HiveJoin)join);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/HiveOpConverter.java,887,semiJoin = joinType == JoinType.LEFTSEMI;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/HiveOpConverter.java,928,posToAliasMap.put(pos, new HashSet<String>(inputRS.getSchema().getTableNames()));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/HiveOpConverter.java,1075,private static JoinType extractJoinType(HiveJoin join) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/HiveOpConverter.java,1077,if (join.isLeftSemiJoin()) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/HiveOpConverter.java,1078,return JoinType.LEFTSEMI;
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,1545,topRel = HiveJoin.getJoin(cluster, leftRel, rightRel, calciteJoinCond, calciteJoinType,
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,1546,leftSemiJoin);
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,2975,HadoopShims.HdfsEncryptionShim hdfsEncryptionShim = SessionState.get().getHdfsEncryptionShim();
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,2978,&& !hdfsEncryptionShim.arePathsOnSameEncryptionZone(srcf, destf);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2180,HadoopShims.HdfsEncryptionShim hdfsEncryptionShim;
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2182,hdfsEncryptionShim = SessionState.get().getHdfsEncryptionShim();
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2183,if (hdfsEncryptionShim != null) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2184,try {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2189,throw new HiveException("Unable to determine if " + path + " is encrypted: " + e, e);
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,216,private HadoopShims.HdfsEncryptionShim hdfsEncryptionShim;
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,443,if (hdfsEncryptionShim == null) {
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,445,FileSystem fs = FileSystem.get(sessionConf);
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,447,hdfsEncryptionShim = ShimLoader.getHadoopShims().createHdfsEncryptionShim(fs, sessionConf);
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,449,LOG.debug("Could not get hdfsEncryptionShim, it is only applicable to hdfs filesystem.");
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,456,return hdfsEncryptionShim;
shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java,1129,EncryptionZone zone1, zone2;
shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java,1131,zone1 = hdfsAdmin.getEncryptionZoneForPath(path1);
shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java,1132,zone2 = hdfsAdmin.getEncryptionZoneForPath(path2);
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,1716,RelDataType rowType = TypeConverter.getType(cluster, rr, null);
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,1719,String fullyQualifiedTabName = tabMetaData.getDbName();
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,1720,if (fullyQualifiedTabName != null && !fullyQualifiedTabName.isEmpty()) {
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,1721,fullyQualifiedTabName = fullyQualifiedTabName + "." + tabMetaData.getTableName();
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,1723,else {
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,1724,fullyQualifiedTabName = tabMetaData.getTableName();
ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java,1726,RelOptHiveTable optTable = new RelOptHiveTable(relOptSchema, fullyQualifiedTabName,
ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMetaStoreChecker.java,389,pool.shutdown();
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordProcessor.java,32,import org.apache.hadoop.hive.llap.io.api.LlapProxy;
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordProcessor.java,44,import org.apache.hadoop.hive.ql.metadata.HiveException;
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/ReduceRecordProcessor.java,188,List<HashTableDummyOperator> dummyOps = redWork.getDummyOps();
ql/src/java/org/apache/hadoop/hive/ql/exec/CommonMergeJoinOperator.java,390,joinFinalLeftData();
ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java,675,if (isLogDebugEnabled) {
ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java,676,LOG.debug(id + " finished. closing... ");
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,150,mapJoinOp.setOpTraits(new OpTraits(null, -1, null));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,165,if (context.conf.getBoolVar(HiveConf.ConfVars.HIVE_AUTO_SORTMERGE_JOIN) == false) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,239,OpTraits opTraits =
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,241,.getSortCols());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,307,currentOp.setOpTraits(new OpTraits(opTraits.getBucketColNames(), opTraits.getNumBuckets(), opTraits.getSortCols()));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,330,tezBucketJoinProcCtx.getNumBuckets(), null);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java,850,null);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,113,OpTraits opTraits = new OpTraits(listBucketCols, numBuckets, listBucketCols);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,138,List<String> fileNames =
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,139,AbstractBucketJoinProc.getBucketFilePathsOfPartition(p.getDataLocation(),
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,190,OpTraits opTraits = new OpTraits(bucketColsList, numBuckets, sortedColsList);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,216,OpTraits opTraits = new OpTraits(listBucketCols, -1, listBucketCols);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,273,OpTraits opTraits = new OpTraits(listBucketCols, numBuckets, listSortCols);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,305,joinOp.setOpTraits(new OpTraits(bucketColsList, -1, bucketColsList));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java,358,OpTraits opTraits = new OpTraits(null, -1, null);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SparkMapJoinOptimizer.java,116,OpTraits opTraits = new OpTraits(bucketColNames, numBuckets, null);
ql/src/java/org/apache/hadoop/hive/ql/plan/OpTraits.java,29,public OpTraits(List<List<String>> bucketColNames, int numBuckets, List<List<String>> sortColNames) {
llap-client/src/java/org/apache/hadoop/hive/llap/ext/LlapTaskUmbilicalExternalClient.java,210,private void updateHeartbeatInfo(String hostname, int port) {
llap-client/src/java/org/apache/hadoop/hive/llap/ext/LlapTaskUmbilicalExternalClient.java,217,&& pendingEventData.heartbeatInfo.port == port) {
llap-client/src/java/org/apache/hadoop/hive/llap/ext/LlapTaskUmbilicalExternalClient.java,218,pendingEventData.heartbeatInfo.lastHeartbeat.set(System.currentTimeMillis());
llap-client/src/java/org/apache/hadoop/hive/llap/ext/LlapTaskUmbilicalExternalClient.java,225,TaskHeartbeatInfo heartbeatInfo = registeredTasks.get(key);
llap-client/src/java/org/apache/hadoop/hive/llap/ext/LlapTaskUmbilicalExternalClient.java,226,if (heartbeatInfo != null) {
llap-client/src/java/org/apache/hadoop/hive/llap/ext/LlapTaskUmbilicalExternalClient.java,228,&& heartbeatInfo.port == port) {
llap-client/src/java/org/apache/hadoop/hive/llap/ext/LlapTaskUmbilicalExternalClient.java,229,heartbeatInfo.lastHeartbeat.set(System.currentTimeMillis());
llap-client/src/java/org/apache/hadoop/hive/llap/ext/LlapTaskUmbilicalExternalClient.java,380,public void nodeHeartbeat(Text hostname, int port) throws IOException {
llap-client/src/java/org/apache/hadoop/hive/llap/ext/LlapTaskUmbilicalExternalClient.java,381,updateHeartbeatInfo(hostname.toString(), port);
llap-common/src/gen/protobuf/gen-java/org/apache/hadoop/hive/llap/daemon/rpc/LlapDaemonProtocolProtos.java,16251,new java.lang.String[] { "SubmissionState", });
llap-common/src/java/org/apache/hadoop/hive/llap/DaemonId.java,22,private final long startTime;
llap-common/src/java/org/apache/hadoop/hive/llap/DaemonId.java,24,public DaemonId(String userName, String clusterName, String hostName, String appId,
llap-common/src/java/org/apache/hadoop/hive/llap/DaemonId.java,25,long startTime) {
llap-common/src/java/org/apache/hadoop/hive/llap/DaemonId.java,30,this.startTime = startTime;
llap-common/src/java/org/apache/hadoop/hive/llap/protocol/LlapTaskUmbilicalProtocol.java,38,public void nodeHeartbeat(Text hostname, int port) throws IOException;
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,88,private volatile LlapNodeId nodeId;
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,106,QueryFailedHandler queryFailedHandler, Configuration conf) {
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,158,LOG.info("AMReporter running with NodeId: {}", nodeId);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/AMReporter.java,339,nodeId.getPort());
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/ContainerRunnerImpl.java,257,responseBuilder.setSubmissionState(SubmissionStateProto.valueOf(submissionState.name()));
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/ContainerRunnerImpl.java,258,return responseBuilder.build();
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapDaemon.java,244,this.amReporter = new AMReporter(srvAddress, new QueryFailedHandlerProxy(), daemonConf);
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java,280,.taskStartedRemotely(taskSpec.getTaskAttemptID(), containerId);
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java,546,void nodePinged(String hostname, int port) {
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java,554,getContext().taskAlive(entry.getValue());
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java,555,getContext().containerAlive(entry.getKey());
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java,638,public void nodeHeartbeat(Text hostname, int port) throws IOException {
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java,639,nodePinged(hostname.toString(), port);
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java,641,LOG.debug("Received heartbeat from [" + hostname + ":" + port +"]");
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java,808,BiMap<ContainerId, TezTaskAttemptID> biMap = nodeMap.get(llapNodeId);
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java,809,return biMap;
llap-server/src/java/org/apache/hadoop/hive/llap/shufflehandler/ShuffleHandler.java,841,LOG.info("Content Length in shuffle : " + contentLength);
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,1124,LOG.info("No tasks qualify as killable to schedule tasks at priority {}", forPriority);
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,1528,if (LOG.isInfoEnabled()) {
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,1529,LOG.info("Node[" + serviceInstance.getHost() + ":" + serviceInstance.getRpcPort() + ", " +
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,2965,LOG.info("Adding input file " + path);
ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java,504,if (useFileIds) {
ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java,509,useFileIds = false;
ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java,668,List<HdfsFileStatusWithId> original, boolean useFileIds) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java,671,if (useFileIds) {
ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java,676,useFileIds = false;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,987,private final boolean useFileIds;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1027,Path base, boolean useFileIds) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1028,if (useFileIds) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1030,return SHIMS.listLocatedHdfsStatus(fs, base, AcidUtils.hiddenFileFilter);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1425,boolean useFileIds = HiveConf.getBoolVar(conf, ConfVars.HIVE_ORC_INCLUDE_FILE_ID_IN_SPLITS);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,1426,boolean allowSyntheticFileIds = useFileIds && HiveConf.getBoolVar(
ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java,72,INVALID_JOIN_CONDITION_1(10017, "Both left and right aliases encountered in JOIN"),
ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java,73,INVALID_JOIN_CONDITION_2(10018, "Neither left nor right aliases encountered in JOIN"),
ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java,74,INVALID_JOIN_CONDITION_3(10019, "OR not supported in JOIN currently"),
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,21,import java.util.Collection;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,22,import java.util.HashMap;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,23,import java.util.HashSet;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,26,import java.util.Set;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,31,import org.apache.hadoop.hive.ql.exec.FunctionInfo;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,44,import org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,45,import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,46,import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseCompare;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,47,import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,48,import org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,120,throw new SemanticException(ErrorMsg.INVALID_JOIN_CONDITION_1.getMsg(expr));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,135,throw new SemanticException(ErrorMsg.INVALID_JOIN_CONDITION_1.getMsg(expr));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,197,throw new SemanticException(ErrorMsg.INVALID_JOIN_CONDITION_1.getMsg(expr));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,207,protected void validateUDF(ASTNode expr, boolean isFunction, TypeCheckCtx ctx, FunctionInfo fi,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,208,List<ExprNodeDesc> children, GenericUDF genericUDF) throws SemanticException {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,209,super.validateUDF(expr, isFunction, ctx, fi, children, genericUDF);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,211,JoinTypeCheckCtx jCtx = (JoinTypeCheckCtx) ctx;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,214,if (genericUDF instanceof GenericUDFOPOr) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,215,throw new SemanticException(ErrorMsg.INVALID_JOIN_CONDITION_3.getMsg(expr));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,219,if (!(genericUDF instanceof GenericUDFOPAnd)) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,221,if (!(genericUDF instanceof GenericUDFBaseCompare)) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,222,if (genericUDFargsRefersToBothInput(genericUDF, children, jCtx.getInputRRList())) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,223,throw new SemanticException(ErrorMsg.INVALID_JOIN_CONDITION_1.getMsg(expr));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,229,&& !(children.get(1) instanceof ExprNodeConstantDesc)) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,230,if (comparisonUDFargsRefersToBothInput((GenericUDFBaseCompare) genericUDF, children,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,231,jCtx.getInputRRList())) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,232,throw new SemanticException(ErrorMsg.INVALID_JOIN_CONDITION_1.getMsg(expr));
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,239,private static boolean genericUDFargsRefersToBothInput(GenericUDF udf,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,240,List<ExprNodeDesc> children, List<RowResolver> inputRRList) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,241,boolean argsRefersToBothInput = false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,243,Map<Integer, ExprNodeDesc> hasCodeToColDescMap = new HashMap<Integer, ExprNodeDesc>();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,244,for (ExprNodeDesc child : children) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,245,ExprNodeDescUtils.getExprNodeColumnDesc(child, hasCodeToColDescMap);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,247,Set<Integer> inputRef = getInputRef(hasCodeToColDescMap.values(), inputRRList);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,250,argsRefersToBothInput = true;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,252,return argsRefersToBothInput;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,255,private static boolean comparisonUDFargsRefersToBothInput(GenericUDFBaseCompare comparisonUDF,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,256,List<ExprNodeDesc> children, List<RowResolver> inputRRList) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,257,boolean argsRefersToBothInput = false;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,259,Map<Integer, ExprNodeDesc> lhsHashCodeToColDescMap = new HashMap<Integer, ExprNodeDesc>();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,260,Map<Integer, ExprNodeDesc> rhsHashCodeToColDescMap = new HashMap<Integer, ExprNodeDesc>();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,261,ExprNodeDescUtils.getExprNodeColumnDesc(children.get(0), lhsHashCodeToColDescMap);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,262,ExprNodeDescUtils.getExprNodeColumnDesc(children.get(1), rhsHashCodeToColDescMap);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,263,Set<Integer> lhsInputRef = getInputRef(lhsHashCodeToColDescMap.values(), inputRRList);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,264,Set<Integer> rhsInputRef = getInputRef(rhsHashCodeToColDescMap.values(), inputRRList);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,267,argsRefersToBothInput = true;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,269,return argsRefersToBothInput;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,272,private static Set<Integer> getInputRef(Collection<ExprNodeDesc> colDescSet,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,273,List<RowResolver> inputRRList) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,274,String tableAlias;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,275,RowResolver inputRR;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,276,Set<Integer> inputLineage = new HashSet<Integer>();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,278,for (ExprNodeDesc col : colDescSet) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,279,ExprNodeColumnDesc colDesc = (ExprNodeColumnDesc) col;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,280,tableAlias = colDesc.getTabAlias();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,282,for (int i = 0; i < inputRRList.size(); i++) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,283,inputRR = inputRRList.get(i);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,289,if (tableAlias != null) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,290,if (inputRR.hasTableAlias(tableAlias)) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,291,if (inputRR.doesInvRslvMapContain(colDesc.getColumn())) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,292,inputLineage.add(i);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,296,if (inputRR.doesInvRslvMapContain(colDesc.getColumn())) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,297,inputLineage.add(i);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinCondTypeCheckProcFactory.java,303,return inputLineage;
ql/src/java/org/apache/hadoop/hive/ql/parse/QBJoinTree.java,423,cloned.getPostJoinFilters().add(filter);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2531,throw new SemanticException(ErrorMsg.INVALID_JOIN_CONDITION_1
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2532,.getMsg(condn));
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2546,throw new SemanticException(ErrorMsg.INVALID_JOIN_CONDITION_2
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2547,.getMsg(condn));
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2761,throw new SemanticException(ErrorMsg.INVALID_JOIN_CONDITION_3
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2762,.getMsg(joinCond));
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2791,throw new SemanticException(ErrorMsg.INVALID_JOIN_CONDITION_1
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2792,.getMsg(joinCond));
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2795,applyEqualityPredicateToQBJoinTree(joinTree, type, leftSrc,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2796,joinCond, leftCondn, rightCondn,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2797,leftCondAl1, leftCondAl2,
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2798,rightCondAl1, rightCondAl2);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2841,throw new SemanticException(ErrorMsg.INVALID_JOIN_CONDITION_1
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2842,.getMsg(joinCond));
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2845,if (!leftAliasNull) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2847,|| type.equals(JoinType.FULLOUTER)) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2848,if (conf.getBoolVar(HiveConf.ConfVars.HIVEOUTERJOINSUPPORTSFILTERS)) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2849,joinTree.getFilters().get(0).add(joinCond);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2851,LOG.warn(ErrorMsg.OUTERJOIN_USES_FILTERS.getErrorCodedMsg());
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2855,joinTree.getFiltersForPushing().get(0).add(joinCond);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2859,|| type.equals(JoinType.FULLOUTER)) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2860,if (conf.getBoolVar(HiveConf.ConfVars.HIVEOUTERJOINSUPPORTSFILTERS)) {
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2861,joinTree.getFilters().get(1).add(joinCond);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2863,LOG.warn(ErrorMsg.OUTERJOIN_USES_FILTERS.getErrorCodedMsg());
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,2867,joinTree.getFiltersForPushing().get(1).add(joinCond);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,8049,for(ASTNode condn : joinTree.getPostJoinFilters() ) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,2784,Collections.sort(extLockIDs);////easier to read logs
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,2785,LOG.info("Deleted " + deletedLocks + " ext locks from HIVE_LOCKS due to timeout (vs. " +
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,2786,extLockIDs.size() + " found. List: " + extLockIDs + ") maxHeartbeatTime=" + maxHeartbeatTime);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,3226,LOG.info(quoteString(key) + " locked by " + quoteString(TxnHandler.hostname));
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,3284,LOG.info(quoteString(key) + " unlocked by " + quoteString(TxnHandler.hostname));
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ShowCompactResponseElement.java,90,HADOOP_JOB_ID((short)12, "hadoopJobId");
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ShowCompactResponseElement.java,173,private static final _Fields optionals[] = {_Fields.PARTITIONNAME,_Fields.WORKERID,_Fields.START,_Fields.RUN_AS,_Fields.HIGHTEST_TXN_ID,_Fields.META_INFO,_Fields.END_TIME,_Fields.HADOOP_JOB_ID};
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ShowCompactResponseElement.java,1467,oprot.writeBitSet(optionals, 8);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ShowCompactResponseElement.java,1505,BitSet incoming = iprot.readBitSet(8);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167249,tmpMap.put(_Fields.SUCCESS, new org.apache.thrift.meta_data.FieldMetaData("success", org.apache.thrift.TFieldRequirementType.DEFAULT,
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167250,new org.apache.thrift.meta_data.StructMetaData(org.apache.thrift.protocol.TType.STRUCT, HeartbeatTxnRangeResponse.class)));
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167252,org.apache.thrift.meta_data.FieldMetaData.addStructMetaDataMap(heartbeat_txn_range_result.class, metaDataMap);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167255,public heartbeat_txn_range_result() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167258,public heartbeat_txn_range_result(
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167261,this();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167262,this.success = success;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167268,public heartbeat_txn_range_result(heartbeat_txn_range_result other) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167269,if (other.isSetSuccess()) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167270,this.success = new HeartbeatTxnRangeResponse(other.success);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167274,public heartbeat_txn_range_result deepCopy() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167275,return new heartbeat_txn_range_result(this);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167280,this.success = null;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167283,public HeartbeatTxnRangeResponse getSuccess() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167284,return this.success;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167287,public void setSuccess(HeartbeatTxnRangeResponse success) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167288,this.success = success;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167291,public void unsetSuccess() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167292,this.success = null;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167296,public boolean isSetSuccess() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167297,return this.success != null;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167300,public void setSuccessIsSet(boolean value) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167301,if (!value) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167302,this.success = null;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167308,case SUCCESS:
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167309,if (value == null) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167310,unsetSuccess();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167312,setSuccess((HeartbeatTxnRangeResponse)value);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167314,break;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167321,case SUCCESS:
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167322,return getSuccess();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167335,case SUCCESS:
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167336,return isSetSuccess();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167346,return this.equals((heartbeat_txn_range_result)that);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167350,public boolean equals(heartbeat_txn_range_result that) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167354,boolean this_present_success = true && this.isSetSuccess();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167355,boolean that_present_success = true && that.isSetSuccess();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167356,if (this_present_success || that_present_success) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167358,return false;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167360,return false;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167370,boolean present_success = true && (isSetSuccess());
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167371,list.add(present_success);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167373,list.add(success);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167379,public int compareTo(heartbeat_txn_range_result other) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167386,lastComparison = Boolean.valueOf(isSetSuccess()).compareTo(other.isSetSuccess());
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167387,if (lastComparison != 0) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167388,return lastComparison;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167390,if (isSetSuccess()) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167391,lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.success, other.success);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167392,if (lastComparison != 0) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167393,return lastComparison;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167413,StringBuilder sb = new StringBuilder("heartbeat_txn_range_result(");
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167416,sb.append("success:");
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167417,if (this.success == null) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167418,sb.append("null");
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167420,sb.append(this.success);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167422,first = false;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167430,if (success != null) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167431,success.validate();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167451,private static class heartbeat_txn_range_resultStandardSchemeFactory implements SchemeFactory {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167452,public heartbeat_txn_range_resultStandardScheme getScheme() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167453,return new heartbeat_txn_range_resultStandardScheme();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167457,private static class heartbeat_txn_range_resultStandardScheme extends StandardScheme<heartbeat_txn_range_result> {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167459,public void read(org.apache.thrift.protocol.TProtocol iprot, heartbeat_txn_range_result struct) throws org.apache.thrift.TException {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167469,case 0: // SUCCESS
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167470,if (schemeField.type == org.apache.thrift.protocol.TType.STRUCT) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167471,struct.success = new HeartbeatTxnRangeResponse();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167472,struct.success.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167473,struct.setSuccessIsSet(true);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167475,org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167477,break;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167487,public void write(org.apache.thrift.protocol.TProtocol oprot, heartbeat_txn_range_result struct) throws org.apache.thrift.TException {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167491,if (struct.success != null) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167492,oprot.writeFieldBegin(SUCCESS_FIELD_DESC);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167493,struct.success.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167494,oprot.writeFieldEnd();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167502,private static class heartbeat_txn_range_resultTupleSchemeFactory implements SchemeFactory {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167503,public heartbeat_txn_range_resultTupleScheme getScheme() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167504,return new heartbeat_txn_range_resultTupleScheme();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167508,private static class heartbeat_txn_range_resultTupleScheme extends TupleScheme<heartbeat_txn_range_result> {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167511,public void write(org.apache.thrift.protocol.TProtocol prot, heartbeat_txn_range_result struct) throws org.apache.thrift.TException {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167513,BitSet optionals = new BitSet();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167514,if (struct.isSetSuccess()) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167515,optionals.set(0);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167517,oprot.writeBitSet(optionals, 1);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167518,if (struct.isSetSuccess()) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167519,struct.success.write(oprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167524,public void read(org.apache.thrift.protocol.TProtocol prot, heartbeat_txn_range_result struct) throws org.apache.thrift.TException {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167526,BitSet incoming = iprot.readBitSet(1);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167527,if (incoming.get(0)) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167528,struct.success = new HeartbeatTxnRangeResponse();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167529,struct.success.read(iprot);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167530,struct.setSuccessIsSet(true);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167537,public static class compact_args implements org.apache.thrift.TBase<compact_args, compact_args._Fields>, java.io.Serializable, Cloneable, Comparable<compact_args>   {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167538,private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("compact_args");
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167544,schemes.put(StandardScheme.class, new compact_argsStandardSchemeFactory());
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167545,schemes.put(TupleScheme.class, new compact_argsTupleSchemeFactory());
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167615,org.apache.thrift.meta_data.FieldMetaData.addStructMetaDataMap(compact_args.class, metaDataMap);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167618,public compact_args() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167621,public compact_args(
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167631,public compact_args(compact_args other) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167637,public compact_args deepCopy() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167638,return new compact_args(this);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167709,return this.equals((compact_args)that);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167713,public boolean equals(compact_args that) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167742,public int compareTo(compact_args other) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167776,StringBuilder sb = new StringBuilder("compact_args(");
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167814,private static class compact_argsStandardSchemeFactory implements SchemeFactory {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167815,public compact_argsStandardScheme getScheme() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167816,return new compact_argsStandardScheme();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167820,private static class compact_argsStandardScheme extends StandardScheme<compact_args> {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167822,public void read(org.apache.thrift.protocol.TProtocol iprot, compact_args struct) throws org.apache.thrift.TException {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167850,public void write(org.apache.thrift.protocol.TProtocol oprot, compact_args struct) throws org.apache.thrift.TException {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167865,private static class compact_argsTupleSchemeFactory implements SchemeFactory {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167866,public compact_argsTupleScheme getScheme() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167867,return new compact_argsTupleScheme();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167871,private static class compact_argsTupleScheme extends TupleScheme<compact_args> {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167874,public void write(org.apache.thrift.protocol.TProtocol prot, compact_args struct) throws org.apache.thrift.TException {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167887,public void read(org.apache.thrift.protocol.TProtocol prot, compact_args struct) throws org.apache.thrift.TException {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167900,public static class compact_result implements org.apache.thrift.TBase<compact_result, compact_result._Fields>, java.io.Serializable, Cloneable, Comparable<compact_result>   {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167901,private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("compact_result");
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167906,schemes.put(StandardScheme.class, new compact_resultStandardSchemeFactory());
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167907,schemes.put(TupleScheme.class, new compact_resultTupleSchemeFactory());
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167913,;
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167970,org.apache.thrift.meta_data.FieldMetaData.addStructMetaDataMap(compact_result.class, metaDataMap);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167973,public compact_result() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167979,public compact_result(compact_result other) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167982,public compact_result deepCopy() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,167983,return new compact_result(this);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,168017,return this.equals((compact_result)that);
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,168021,public boolean equals(compact_result that) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,168036,public int compareTo(compact_result other) {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,168060,StringBuilder sb = new StringBuilder("compact_result(");
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,168088,private static class compact_resultStandardSchemeFactory implements SchemeFactory {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,168089,public compact_resultStandardScheme getScheme() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,168090,return new compact_resultStandardScheme();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,168094,private static class compact_resultStandardScheme extends StandardScheme<compact_result> {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,168096,public void read(org.apache.thrift.protocol.TProtocol iprot, compact_result struct) throws org.apache.thrift.TException {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,168115,public void write(org.apache.thrift.protocol.TProtocol oprot, compact_result struct) throws org.apache.thrift.TException {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,168125,private static class compact_resultTupleSchemeFactory implements SchemeFactory {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,168126,public compact_resultTupleScheme getScheme() {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,168127,return new compact_resultTupleScheme();
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,168131,private static class compact_resultTupleScheme extends TupleScheme<compact_result> {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,168134,public void write(org.apache.thrift.protocol.TProtocol prot, compact_result struct) throws org.apache.thrift.TException {
metastore/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java,168139,public void read(org.apache.thrift.protocol.TProtocol prot, compact_result struct) throws org.apache.thrift.TException {
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java,5913,getTxnHandler().compact(rqst);
metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java,2171,client.compact(cr);
metastore/src/java/org/apache/hadoop/hive/metastore/IMetaStoreClient.java,1457,void compact(String dbname, String tableName, String partitionName, CompactionType type,
metastore/src/java/org/apache/hadoop/hive/metastore/IMetaStoreClient.java,1458,Map<String, String> tblproperties) throws TException;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1343,public long compact(CompactionRequest rqst) throws MetaException {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1405,return id;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1445,switch (rs.getString(4).charAt(0)) {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1446,case INITIATED_STATE: e.setState(INITIATED_RESPONSE); break;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1447,case WORKING_STATE: e.setState(WORKING_RESPONSE); break;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1448,case READY_FOR_CLEANING: e.setState(CLEANING_RESPONSE); break;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1449,case FAILED_STATE: e.setState(FAILED_RESPONSE); break;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1450,case SUCCEEDED_STATE: e.setState(SUCCEEDED_RESPONSE); break;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1451,case ATTEMPTED_STATE: e.setState(ATTEMPTED_RESPONSE); break;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,1452,default:
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnStore.java,50,public static enum MUTEX_KEY {Initiator, Cleaner, HouseKeeper, CompactionHistory, CheckLock, WriteSetCleaner}
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnStore.java,191,public long compact(CompactionRequest rqst) throws MetaException;
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1792,db.compact(tbl.getDbName(), tbl.getTableName(), partName, desc.getCompactionType(), desc.getProps());
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1793,console.printInfo("Compaction enqueued.");
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,3512,public void compact(String dbname, String tableName, String partName, String compactType,
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,3520,getMSC().compact(dbname, tableName, partName, cr, tblproperties);
ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java,339,ci.id = txnHandler.compact(rqst);
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,82,switch (type.getKind()) {
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,83,case BOOLEAN:
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,84,case BYTE:
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,85,case SHORT:
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,86,case INT:
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,87,case LONG:
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,88,case DATE:
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,89,return new LongColumnVector(batchSize);
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,90,case FLOAT:
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,91,case DOUBLE:
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,92,return new DoubleColumnVector(batchSize);
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,93,case BINARY:
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,94,case STRING:
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,95,case CHAR:
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,96,case VARCHAR:
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,97,return new BytesColumnVector(batchSize);
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,98,case TIMESTAMP:
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,99,return new TimestampColumnVector(batchSize);
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,100,case DECIMAL:
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,101,return new DecimalColumnVector(batchSize, type.getPrecision(),
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,102,type.getScale());
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,103,default:
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,104,throw new IllegalArgumentException("LLAP does not support " +
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,105,type.getKind());
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,130,int numCols = batch.getColumnIxs().length;
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,132,this.columnReaders = EncodedTreeReaderFactory.createEncodedTreeReader(numCols,
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,134,positionInStreams(columnReaders, batch, numCols, stripeMetadata);
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,136,repositionInStreams(this.columnReaders, batch, sameStripe, numCols, stripeMetadata);
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,148,assert cvb.cols.length == batch.getColumnIxs().length; // Must be constant per split.
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,150,List<OrcProto.Type> types = fileMetadata.getTypes();
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,151,int[] columnMapping = batch.getColumnIxs();
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,152,for (int idx = 0; idx < batch.getColumnIxs().length; idx++) {
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,159,columnReaders[idx].nextVector(cvb.cols[idx], null, batchSize);
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,176,EncodedColumnBatch<OrcBatchKey> batch, int numCols,
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,177,OrcStripeMetadata stripeMetadata) throws IOException {
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,178,for (int i = 0; i < numCols; i++) {
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,179,int columnIndex = batch.getColumnIxs()[i];
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,180,int rowGroupIndex = batch.getBatchKey().rgIx;
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,181,OrcProto.RowIndex rowIndex = stripeMetadata.getRowIndexes()[columnIndex];
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,182,OrcProto.RowIndexEntry rowIndexEntry = rowIndex.getEntry(rowGroupIndex);
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,183,columnReaders[i].seek(new RecordReaderImpl.PositionProviderImpl(rowIndexEntry));
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,188,EncodedColumnBatch<OrcBatchKey> batch, boolean sameStripe, int numCols,
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,190,for (int i = 0; i < numCols; i++) {
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,191,int columnIndex = batch.getColumnIxs()[i];
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,192,int rowGroupIndex = batch.getBatchKey().rgIx;
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,193,ColumnStreamData[] streamBuffers = batch.getColumnData()[i];
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,194,OrcProto.RowIndex rowIndex = stripeMetadata.getRowIndexes()[columnIndex];
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,195,OrcProto.RowIndexEntry rowIndexEntry = rowIndex.getEntry(rowGroupIndex);
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,196,((SettableTreeReader)columnReaders[i]).setBuffers(streamBuffers, sameStripe);
llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcEncodedDataConsumer.java,197,columnReaders[i].seek(new RecordReaderImpl.PositionProviderImpl(rowIndexEntry));
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,137,private List<Integer> columnIds;
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,171,this.columnIds = columnIds;
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,172,if (this.columnIds != null) {
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,173,Collections.sort(this.columnIds);
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,235,if (columnIds == null) {
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,236,columnIds = createColumnIds(fileMetadata);
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,302,List<Integer>[] stripeColsToRead = null;
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,330,boolean[] stripeIncludes = null;
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,334,List<Integer> cols = stripeColsToRead == null ? null : stripeColsToRead[stripeIxMod];
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,335,if (cols != null && cols.isEmpty()) continue; // No need to read this stripe.
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,349,if (cols == null || cols.size() == colRgs.length) {
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,350,cols = columnIds;
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,351,stripeIncludes = globalIncludes;
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,355,stripeIncludes = OrcInputFormat.genIncludedColumns(fileMetadata.getTypes(), cols, true);
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,356,colRgs = genStripeColRgs(cols, colRgs);
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,374,metadataReader, stripe, stripeIncludes, sargColumns);
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,380,stripeKey.stripeIx, DebugUtils.toString(stripeIncludes));
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,386,if (!stripeMetadata.hasAllIndexes(stripeIncludes)) {
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,389,stripeKey.stripeIx, DebugUtils.toString(stripeIncludes));
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,394,updateLoadedIndexes(stripeMetadata, stripe, stripeIncludes, sargColumns);
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,418,stripeMetadata.getEncodings(), stripeMetadata.getStreams(), stripeIncludes,
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,516,private boolean[][] genStripeColRgs(List<Integer> stripeCols, boolean[][] globalColRgs) {
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,517,boolean[][] stripeColRgs = new boolean[stripeCols.size()][];
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,518,for (int i = 0, i2 = -1; i < globalColRgs.length; ++i) {
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,519,if (globalColRgs[i] == null) continue;
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,520,stripeColRgs[i2] = globalColRgs[i];
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,521,++i2;
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,523,return stripeColRgs;
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,529,private static List<Integer> createColumnIds(OrcFileMetadata metadata) {
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,530,List<Integer> columnIds = new ArrayList<Integer>(metadata.getTypes().size());
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,531,for (int i = 1; i < metadata.getTypes().size(); ++i) {
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,671,for (ColumnStreamData[] datas : ecb.getColumnData()) {
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,672,if (datas == null) continue;
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,728,readState[stripeIxMod] = new boolean[columnIds.size()][];
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,729,for (int j = 0; j < columnIds.size(); ++j) {
llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java,730,readState[stripeIxMod][j] = (isAll || isNone) ? rgsToRead :
orc/src/java/org/apache/orc/impl/ConvertTreeReaderFactory.java,281,void seek(PositionProvider[] index) throws IOException {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,83,static IntegerReader createIntegerReader(OrcProto.ColumnEncoding.Kind kind,
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,118,void seek(PositionProvider[] index) throws IOException {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,265,void seek(PositionProvider[] index) throws IOException {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,316,void seek(PositionProvider[] index) throws IOException {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,383,void seek(PositionProvider[] index) throws IOException {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,450,void seek(PositionProvider[] index) throws IOException {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,518,void seek(PositionProvider[] index) throws IOException {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,572,void seek(PositionProvider[] index) throws IOException {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,666,void seek(PositionProvider[] index) throws IOException {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,777,void seek(PositionProvider[] index) throws IOException {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,900,void seek(PositionProvider[] index) throws IOException {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,1009,void seek(PositionProvider[] index) throws IOException {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,1085,void seek(PositionProvider[] index) throws IOException {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,1199,void seek(PositionProvider[] index) throws IOException {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,1347,void seek(PositionProvider[] index) throws IOException {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,1496,void seek(PositionProvider[] index) throws IOException {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,1702,protected static class StructTreeReader extends TreeReader {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,1721,void seek(PositionProvider[] index) throws IOException {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,1803,void seek(PositionProvider[] index) throws IOException {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,1876,void seek(PositionProvider[] index) throws IOException {
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,1957,void seek(PositionProvider[] index) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,278,static int getRootColumn(boolean isOriginal) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,131,OrcProto.RowIndex rowIndex) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,211,LOG.trace("The following columns have PRESENT streams: " + arrayToString(hasNull));
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,217,int colRgIx = -1, lastColIx = -1;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,218,ColumnReadContext[] colCtxs = new ColumnReadContext[colRgs.length];
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,219,boolean[] includedRgs = null;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,230,hasIndexOnlyCols = hasIndexOnlyCols | included[colIx];
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,232,LOG.trace("Skipping stream: " + streamKind + " at " + offset + ", " + length);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,237,ColumnReadContext ctx = null;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,238,if (lastColIx != colIx) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,239,++colRgIx;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,240,assert colCtxs[colRgIx] == null;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,241,lastColIx = colIx;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,242,includedRgs = colRgs[colRgIx];
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,243,ctx = colCtxs[colRgIx] = new ColumnReadContext(
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,244,colIx, encodings.get(colIx), indexes[colIx]);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,245,if (isTracingEnabled) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,246,LOG.trace("Creating context " + colRgIx + " for column " + colIx + ":" + ctx.toString());
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,249,ctx = colCtxs[colRgIx];
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,250,assert ctx != null;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,278,ecb.init(fileKey, stripeIx, OrcEncodedColumnBatch.ALL_RGS, colRgs.length);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,312,for (int colIxMod = 0; colIxMod < colRgs.length; ++colIxMod) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,313,ColumnReadContext ctx = colCtxs[colIxMod];
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,337,ecb.init(fileKey, stripeIx, rgIx, colRgs.length);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,339,for (int colIxMod = 0; colIxMod < colRgs.length; ++colIxMod) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,341,if (colRgs[colIxMod] != null && !colRgs[colIxMod][rgIx]) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,344,continue;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,346,ColumnReadContext ctx = colCtxs[colIxMod];
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,349,ecb.initColumn(colIxMod, ctx.colIx, OrcEncodedColumnBatch.MAX_DATA_STREAMS);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,39,void setBuffers(ColumnStreamData[] streamBuffers, boolean sameStripe) throws IOException;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,203,if (_dataStream.available() > 0) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,214,if (_dataStream.available() > 0) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,221,if (_lengthStream.available() > 0) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1665,public static TreeReader[] createEncodedTreeReader(int numCols,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1670,TreeReader[] treeReaders = new TreeReader[numCols];
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1671,for (int i = 0; i < numCols; i++) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1672,int columnIndex = batch.getColumnIxs()[i];
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1673,ColumnStreamData[] streamBuffers = batch.getColumnData()[i];
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1674,OrcProto.Type columnType = types.get(columnIndex);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1682,OrcProto.ColumnEncoding columnEncoding = encodings.get(columnIndex);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1685,ColumnStreamData present = streamBuffers[OrcProto.Stream.Kind.PRESENT_VALUE],
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1686,data = streamBuffers[OrcProto.Stream.Kind.DATA_VALUE],
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1687,dictionary = streamBuffers[OrcProto.Stream.Kind.DICTIONARY_DATA_VALUE],
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1688,lengths = streamBuffers[OrcProto.Stream.Kind.LENGTH_VALUE],
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1689,secondary = streamBuffers[OrcProto.Stream.Kind.SECONDARY_VALUE];
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1691,switch (columnType.getKind()) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1692,case BINARY:
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1700,.build();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1701,break;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1702,case BOOLEAN:
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1708,.build();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1709,break;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1710,case BYTE:
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1716,.build();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1717,break;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1718,case SHORT:
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1725,.build();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1726,break;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1727,case INT:
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1734,.build();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1735,break;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1736,case LONG:
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1744,.build();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1745,break;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1746,case FLOAT:
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1752,.build();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1753,break;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1754,case DOUBLE:
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1760,.build();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1761,break;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1762,case CHAR:
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1772,.build();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1773,break;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1774,case VARCHAR:
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1784,.build();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1785,break;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1786,case STRING:
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1795,.build();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1796,break;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1797,case DECIMAL:
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1807,.build();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1808,break;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1809,case TIMESTAMP:
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1818,.build();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1819,break;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1820,case DATE:
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1828,break;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1829,default:
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1830,throw new UnsupportedOperationException("Data type not supported yet! " + columnType);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedTreeReaderFactory.java,1834,return treeReaders;
storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java,84,protected int[] columnIxs;
storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java,96,public void initColumn(int colIxMod, int colIx, int streamCount) {
storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java,97,columnIxs[colIxMod] = colIx;
storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java,98,if (columnData[colIxMod] == null || columnData[colIxMod].length != streamCount) {
storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java,99,columnData[colIxMod] = new ColumnStreamData[streamCount];
storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java,103,public void setStreamData(int colIxMod, int streamKind, ColumnStreamData csd) {
storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java,104,columnData[colIxMod][streamKind] = csd;
storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java,107,public void setAllStreamsData(int colIxMod, int colIx, ColumnStreamData[] sbs) {
storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java,108,columnIxs[colIxMod] = colIx;
storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java,109,columnData[colIxMod] = sbs;
storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java,116,public ColumnStreamData[][] getColumnData() {
storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java,117,return columnData;
storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java,120,public int[] getColumnIxs() {
storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java,121,return columnIxs;
storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java,125,if (columnIxs != null && columnCount == columnIxs.length) return;
storage-api/src/java/org/apache/hadoop/hive/common/io/encoded/EncodedColumnBatch.java,126,columnIxs = new int[columnCount];
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,353,if (RecordReaderUtils.isDictionary(sctx.kind, ctx.encoding)) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,355,if (isTracingEnabled) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,356,LOG.trace("Getting stripe-level stream [" + sctx.kind + ", " + ctx.encoding + "] for"
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,357,+ " column " + ctx.colIx + " RG " + rgIx + " at " + sctx.offset + ", " + sctx.length);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,359,if (sctx.stripeLevelStream == null) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,360,sctx.stripeLevelStream = POOLS.csdPool.take();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,365,sctx.stripeLevelStream.incRef();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,368,long unlockUntilCOffset = sctx.offset + sctx.length;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,369,DiskRangeList lastCached = readEncodedStream(stripeOffset, iter,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,370,sctx.offset, sctx.offset + sctx.length, sctx.stripeLevelStream,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,373,iter = lastCached;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,376,if (!isLastRg) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,377,sctx.stripeLevelStream.incRef();
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,379,cb = sctx.stripeLevelStream;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,383,long cOffset = sctx.offset + index.getPositions(sctx.streamIndexOffset);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,385,long nextCOffsetRel = isLastRg ? sctx.length
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,386,: nextIndex.getPositions(sctx.streamIndexOffset);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,389,long endCOffset = sctx.offset + RecordReaderUtils.estimateRgEndOffset(
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,390,isCompressed, isLastRg, nextCOffsetRel, sctx.length, bufferSize);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,393,long unlockUntilCOffset = sctx.offset + nextCOffsetRel;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,394,cb = createRgColumnStreamData(
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,395,rgIx, isLastRg, ctx.colIx, sctx, cOffset, endCOffset, isCompressed);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,396,boolean isStartOfStream = sctx.bufferIter == null;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,397,DiskRangeList lastCached = readEncodedStream(stripeOffset,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,398,(isStartOfStream ? iter : sctx.bufferIter), cOffset, endCOffset, cb,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,399,unlockUntilCOffset, sctx.offset);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,400,if (lastCached != null) {
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,401,sctx.bufferIter = iter = lastCached;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,404,ecb.setStreamData(colIxMod, sctx.kind.getNumber(), cb);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,615,lastUncompressed = isCompressed ?
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,616,prepareRangesForCompressedRead(cOffset, endCOffset, streamOffset,
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,618,: prepareRangesForUncompressedRead(
ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java,619,cOffset, endCOffset, streamOffset, unlockUntilCOffset, current, csd);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/reloperators/HiveTableScan.java,138,return super.explainTerms(pw);
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,200,ds = betterDS < 1 ? ds : betterDS;
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,301,stats.setDataSize(betterDS < 1 ? ds : betterDS);
ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java,1452,if (numRows <= 0 || colStats == null || colStats.isEmpty()) {
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,297,public static abstract class GenericUDAFNumericStatsEvaluator<V, OI extends ObjectInspector>
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,373,foi.add(getValueObjectInspector());
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,374,foi.add(getValueObjectInspector());
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,398,foi.add(getValueObjectInspector());
ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFComputeStats.java,399,foi.add(getValueObjectInspector());
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,240,FunctionRegistry.registerPermanentFunction(FunctionUtils.qualifyFunctionName(
ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java,241,functionName, function.getDbName()), function.getClassName(), false,
ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java,239,addFunction(functionName, function);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/Utils.java,54,splitLocationProvider = null;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java,32,import org.apache.hadoop.fs.ContentSummary;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java,49,import org.apache.hadoop.hive.ql.exec.UDTFOperator;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java,429,Utilities.setColumnNameList(jobConf, scanOp, true);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java,430,Utilities.setColumnTypeList(jobConf, scanOp, true);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java,431,HiveStorageHandler handler = table.getStorageHandler();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java,432,if (handler instanceof InputEstimator) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java,433,InputEstimator estimator = (InputEstimator) handler;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java,434,TableDesc tableDesc = Utilities.getTableDesc(table);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java,435,PlanUtils.configureInputJobPropertiesForStorageHandler(tableDesc);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java,436,Utilities.copyTableJobPropertiesToConf(tableDesc, jobConf);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java,439,if (table.isNonNative()) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/SimpleFetchOptimizer.java,442,if (!table.isPartitioned()) {
ql/src/java/org/apache/hadoop/hive/ql/security/authorization/StorageBasedAuthorizationProvider.java,181,if (privExtractor.hasDropPrivilege() && table.getTableType() != TableType.EXTERNAL_TABLE) {
ql/src/java/org/apache/hadoop/hive/ql/Driver.java,1493,ss.getUserIpAddress(), operationId);
ql/src/java/org/apache/hadoop/hive/ql/hooks/ATSHook.java,142,String logID = conf.getLogIdVar(SessionState.get().getSessionId());
ql/src/java/org/apache/hadoop/hive/ql/hooks/HookContext.java,67,String operationId) throws Exception {
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1879,String[] cols = colNames.trim().split(",");
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1880,if (cols != null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1881,for (String col : cols) {
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1882,if (col != null && !col.trim().equals("")) {
ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java,1883,names.add(col);
llap-server/src/java/org/apache/hadoop/hive/llap/cache/BuddyAllocator.java,343,ByteBuffer rwbuf = rwf.getChannel().map(MapMode.PRIVATE, 0, arenaSize);
ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java,698,if (pgA == PrimitiveGrouping.STRING_GROUP && pgB == PrimitiveGrouping.STRING_GROUP) {
ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java,699,return getTypeInfoForPrimitiveCategory(
ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java,700,(PrimitiveTypeInfo)a, (PrimitiveTypeInfo)b,PrimitiveCategory.STRING);
ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java,703,if (TypeInfoUtils.implicitConvertible(a, b)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java,704,return getTypeInfoForPrimitiveCategory((PrimitiveTypeInfo)a, (PrimitiveTypeInfo)b, pcB);
ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java,706,if (TypeInfoUtils.implicitConvertible(b, a)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java,707,return getTypeInfoForPrimitiveCategory((PrimitiveTypeInfo)a, (PrimitiveTypeInfo)b, pcA);
ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java,709,for (PrimitiveCategory t : TypeInfoUtils.numericTypeList) {
ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java,711,&& TypeInfoUtils.implicitConvertible(pcB, t)) {
ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java,712,return getTypeInfoForPrimitiveCategory((PrimitiveTypeInfo)a, (PrimitiveTypeInfo)b, t);
ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java,716,return null;
ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java,512,assert refCount != null;
ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java,513,if (refCount == 1) {
ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java,514,persistent.remove(functionClass);
ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java,516,persistent.put(functionClass, Integer.valueOf(refCount - 1));
ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java,527,private static void start(SessionState startSs, boolean isAsync, LogHelper console) {
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1855,for (CheckResult.PartitionResult part : partsNotInMs) {
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1856,apd.addPartition(Warehouse.makeSpecFromName(part.getPartitionName()), null);
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1857,repairOutput.add("Repair: Added partition to metastore "
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1858,+ msckDesc.getTableName() + ':' + part.getPartitionName());
ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java,1860,db.createPartitions(apd);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,983,doPhase1QBExpr(subqref, qbexpr, qb.getId(), alias);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,864,FSDataOutputStream out = fs.create(dataFile);
ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java,888,out.close();
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,570,if (nanos != 0) {
serde/src/java/org/apache/hadoop/hive/serde2/io/TimestampWritable.java,571,t.setNanos(nanos);
ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexResult.java,48,public class HiveIndexResult {
ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexedInputFormat.java,43,import org.apache.hadoop.io.SequenceFile;
ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexedInputFormat.java,151,ArrayList<HiveInputSplit> newSplits = new ArrayList<HiveInputSplit>(
ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexedInputFormat.java,152,numSplits);
ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexedInputFormat.java,159,long sumSplitLengths = 0;
ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexedInputFormat.java,160,for (HiveInputSplit split : splits) {
ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexedInputFormat.java,161,l4j.info("split start : " + split.getStart());
ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexedInputFormat.java,162,l4j.info("split end : " + (split.getStart() + split.getLength()));
ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexedInputFormat.java,164,try {
ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexedInputFormat.java,165,if (hiveIndexResult.contains(split)) {
ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexedInputFormat.java,167,HiveInputSplit newSplit = split;
ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexedInputFormat.java,169,|| split.inputFormatClassName().contains("SequenceFile")) {
ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexedInputFormat.java,170,if (split.getStart() > SequenceFile.SYNC_INTERVAL) {
ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexedInputFormat.java,171,newSplit = new HiveInputSplit(new FileSplit(split.getPath(),
ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexedInputFormat.java,172,split.getStart() - SequenceFile.SYNC_INTERVAL,
ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexedInputFormat.java,173,split.getLength() + SequenceFile.SYNC_INTERVAL,
ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexedInputFormat.java,174,split.getLocations()),
ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexedInputFormat.java,175,split.inputFormatClassName());
ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexedInputFormat.java,178,sumSplitLengths += newSplit.getLength();
ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexedInputFormat.java,179,if (sumSplitLengths > maxInputSize) {
ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexedInputFormat.java,180,throw new IOException(
ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexedInputFormat.java,182,+ maxInputSize + " set in " + ConfVars.HIVE_INDEX_COMPACT_QUERY_MAX_SIZE.varname);
ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexedInputFormat.java,184,newSplits.add(newSplit);
ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexedInputFormat.java,187,throw new RuntimeException(
ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexedInputFormat.java,191,InputSplit retA[] = newSplits.toArray((new FileSplit[newSplits.size()]));
ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexedInputFormat.java,192,l4j.info("Number of input splits: " + splits.length + " new input splits: "
ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexedInputFormat.java,193,+ retA.length + ", sum of split lengths: " + sumSplitLengths);
ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexedInputFormat.java,194,return retA;
ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionInfo.java,76,public FunctionInfo(boolean isNative, String displayName,
ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionInfo.java,78,this.functionType = isNative ? FunctionType.BUILTIN : FunctionType.TEMPORARY;
ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionInfo.java,85,public FunctionInfo(boolean isNative, String displayName,
ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionInfo.java,87,this.functionType = isNative ? FunctionType.BUILTIN : FunctionType.TEMPORARY;
ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionInfo.java,94,public FunctionInfo(boolean isNative, String displayName,
ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionInfo.java,96,this.functionType = isNative ? FunctionType.BUILTIN : FunctionType.TEMPORARY;
ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionInfo.java,103,public FunctionInfo(boolean isNative, String displayName, Class<? extends TableFunctionResolver> tFnCls,
ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionInfo.java,105,this.functionType = isNative ? FunctionType.BUILTIN : FunctionType.TEMPORARY;
ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java,1524,return system.isPermanentFunc(clazz);
ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java,115,functionName, (Class<? extends UDF>) udfClass, false, resources);
ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java,118,functionName, (Class<? extends GenericUDF>) udfClass, resources);
ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java,121,functionName, (Class<? extends GenericUDTF>) udfClass, resources);
ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java,124,functionName, (Class<? extends UDAF>) udfClass, resources);
ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java,128,ReflectionUtil.newInstance(udfClass, null), resources);
ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java,131,return registerTableFunction(functionName,
ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java,147,FunctionInfo fI = new FunctionInfo(isNative, displayName,
ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java,156,FunctionInfo fI = new FunctionInfo(isNative, functionName,
ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java,181,FunctionInfo fI = new FunctionInfo(isNative, functionName,
ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java,190,new WindowFunctionInfo(isNative, functionName, genericUDAFResolver, resources);
ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java,199,FunctionInfo function = new WindowFunctionInfo(isNative, functionName,
ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java,209,FunctionInfo function = new FunctionInfo(isNative, functionName, tFnCls, resources);
ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java,222,FunctionInfo fI = new FunctionInfo(isNative, macroName, macro, resources);
ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java,254,addFunction(WINDOW_FUNC_PREFIX + name, new WindowFunctionInfo(isNative, name, wFn, null));
ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java,450,if (isNative != function.isNative()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/Registry.java,607,ret = FunctionRegistry.registerTemporaryUDF(qualifiedName, udfClass, resources);
ql/src/java/org/apache/hadoop/hive/ql/exec/WindowFunctionInfo.java,31,public WindowFunctionInfo(boolean isNative, String functionName,
ql/src/java/org/apache/hadoop/hive/ql/exec/WindowFunctionInfo.java,33,super(isNative, functionName, resolver, resources);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/SqlFunctionConverter.java,193,fi.isNative(), fi.getDisplayName(), (GenericUDF) udf, fi.getResources());
llap-ext-client/src/java/org/apache/hadoop/hive/llap/LlapBaseInputFormat.java,337,builder.setAmHost(address.getHostName());
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java,200,LOG.info(
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java,588,builder.setAmHost(getAddress().getHostName());
ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java,772,fpaths.updaters[++fpaths.acidFileOffset] = HiveFileFormatUtils.getAcidRecordUpdater(
ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java,773,jc, conf.getTableInfo(), bucketNum, conf, fpaths.outPaths[fpaths.acidFileOffset],
ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java,777,fpaths.outPaths[fpaths.acidFileOffset]);
ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java,782,fpaths.updaters[fpaths.acidFileOffset].update(conf.getTransactionId(), row);
ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java,784,fpaths.updaters[fpaths.acidFileOffset].delete(conf.getTransactionId(), row);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/SortedDynPartitionOptimizer.java,247,if (!bucketColumns.isEmpty()) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/SortedDynPartitionOptimizer.java,268,if (bucketColumns.size() > 0) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/SortedDynPartitionOptimizer.java,441,if (!bucketColumns.isEmpty()) {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/ServiceInstanceSet.java,17,import java.util.List;
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapFixedRegistryImpl.java,230,public List<ServiceInstance> getAllInstancesOrdered() {
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/services/impl/LlapWebServices.java,229,for (ServiceInstance s : registry.getInstances().getAllInstancesOrdered()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HostAffinitySplitLocationProvider.java,41,private final Logger LOG = LoggerFactory.getLogger(HostAffinitySplitLocationProvider.class);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HostAffinitySplitLocationProvider.java,44,private final String[] knownLocations;
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HostAffinitySplitLocationProvider.java,46,public HostAffinitySplitLocationProvider(String[] knownLocations) {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HostAffinitySplitLocationProvider.java,47,Preconditions.checkState(knownLocations != null && knownLocations.length != 0,
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HostAffinitySplitLocationProvider.java,50,this.knownLocations = knownLocations;
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HostAffinitySplitLocationProvider.java,55,if (split instanceof FileSplit) {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HostAffinitySplitLocationProvider.java,56,FileSplit fsplit = (FileSplit) split;
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HostAffinitySplitLocationProvider.java,60,if (isDebugEnabled) {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HostAffinitySplitLocationProvider.java,61,LOG.debug(
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HostAffinitySplitLocationProvider.java,63,fsplit.getLength() + " mapped to index=" + index + ", location=" +
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HostAffinitySplitLocationProvider.java,64,knownLocations[index]);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HostAffinitySplitLocationProvider.java,66,return new String[]{knownLocations[index]};
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/Utils.java,42,serviceRegistry.getInstances().getAllInstancesOrdered();
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/Utils.java,43,String[] locations = new String[serviceInstances.size()];
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/Utils.java,44,int i = 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/Utils.java,50,locations[i++] = serviceInstance.getHost();
llap-client/src/java/org/apache/hadoop/hive/llap/registry/ServiceInstanceSet.java,16,import java.io.IOException;
llap-client/src/java/org/apache/hadoop/hive/llap/registry/ServiceInstanceSet.java,18,import java.util.Map;
llap-client/src/java/org/apache/hadoop/hive/llap/registry/ServiceInstanceSet.java,31,public Map<String, ServiceInstance> getAll();
llap-client/src/java/org/apache/hadoop/hive/llap/registry/ServiceInstanceSet.java,38,public List<ServiceInstance> getAllInstancesOrdered();
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapFixedRegistryImpl.java,225,public Map<String, ServiceInstance> getAll() {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapFixedRegistryImpl.java,226,return instances;
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,100,private final String pathPrefix, userPathPrefix;
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,291,znode = new PersistentEphemeralNode(zooKeeperClient,
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,292,PersistentEphemeralNode.Mode.EPHEMERAL_SEQUENTIAL, pathPrefix, encoder.toBytes(srv));
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,334,String pathToCheck = znodePath;
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,337,int ix = pathToCheck.lastIndexOf('/');
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,338,if (ix > 0) {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,339,pathToCheck = pathToCheck.substring(0, ix);
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,504,public Map<String, ServiceInstance> getAll() {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,505,Map<String, ServiceInstance> instances = new LinkedHashMap<>();
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,507,if (childData != null) {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,508,byte[] data = childData.getData();
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,509,if (data != null) {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,510,try {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,511,ServiceRecord srv = encoder.fromBytes(childData.getPath(), data);
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,512,ServiceInstance instance = new DynamicServiceInstance(srv);
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,513,instances.put(childData.getPath(), instance);
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,515,LOG.error("Unable to decode data for zkpath: {}." +
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,525,public List<ServiceInstance> getAllInstancesOrdered() {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,526,List<ServiceInstance> list = new LinkedList<>();
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,527,list.addAll(instances.getAll().values());
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,528,Collections.sort(list, new Comparator<ServiceInstance>() {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,530,public int compare(ServiceInstance o1, ServiceInstance o2) {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,531,return o2.getWorkerIdentity().compareTo(o2.getWorkerIdentity());
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,534,return list;
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,560,if (childData != null) {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,561,byte[] data = childData.getData();
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,562,if (data != null) {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,563,try {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,564,ServiceRecord srv = encoder.fromBytes(childData.getPath(), data);
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,565,ServiceInstance instance = new DynamicServiceInstance(srv);
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,566,if (host.equals(instance.getHost())) {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,567,byHost.add(instance);
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,569,if (LOG.isDebugEnabled()) {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,570,LOG.debug("Locality comparing " + host + " to " + instance.getHost());
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,573,LOG.error("Unable to decode data for zkpath: {}." +
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,602,if (!stateChangeListeners.isEmpty()) {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,603,ServiceInstance instance = null;
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,604,ChildData childData = event.getData();
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,605,if (childData != null) {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,606,byte[] data = childData.getData();
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,607,if (data != null) {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,608,try {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,609,ServiceRecord srv = encoder.fromBytes(event.getData().getPath(), data);
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,610,instance = new DynamicServiceInstance(srv);
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,612,LOG.error("Unable to decode data for zknode: {}." +
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,619,for (ServiceInstanceStateChangeListener listener : stateChangeListeners) {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,620,if (event.getType() == PathChildrenCacheEvent.Type.CHILD_ADDED) {
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,621,LOG.info("Added zknode {} to llap namespace. Notifying state change listener.",
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,622,event.getData().getPath());
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,623,listener.onCreate(instance);
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,625,LOG.info("Updated zknode {} in llap namespace. Notifying state change listener.",
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,626,event.getData().getPath());
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,627,listener.onUpdate(instance);
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,629,LOG.info("Removed zknode {} from llap namespace. Notifying state change listener.",
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,630,event.getData().getPath());
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,631,listener.onRemove(instance);
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,666,this.instancesCache = new PathChildrenCache(zooKeeperClient,
llap-client/src/java/org/apache/hadoop/hive/llap/registry/impl/LlapZookeeperRegistryImpl.java,667,RegistryPathUtils.parentOf(pathPrefix).toString(), true);
llap-client/src/java/org/apache/hadoop/hive/llap/security/LlapTokenClient.java,140,Map<String, ServiceInstance> daemons = activeInstances.getAll();
llap-client/src/java/org/apache/hadoop/hive/llap/security/LlapTokenClient.java,144,lastKnownInstances = daemons.values();
llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapStatusServiceDriver.java,403,Map<String, ServiceInstance> serviceInstanceMap;
llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapStatusServiceDriver.java,405,serviceInstanceMap = llapRegistry.getInstances().getAll();
llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapStatusServiceDriver.java,410,if (serviceInstanceMap == null || serviceInstanceMap.isEmpty()) {
llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapStatusServiceDriver.java,423,for (Map.Entry<String, ServiceInstance> serviceInstanceEntry : serviceInstanceMap
llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapStatusServiceDriver.java,424,.entrySet()) {
llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapStatusServiceDriver.java,426,ServiceInstance serviceInstance = serviceInstanceEntry.getValue();
llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapStatusServiceDriver.java,427,String containerIdString = serviceInstance.getProperties().get(HiveConf.ConfVars.LLAP_DAEMON_CONTAINER_ID.varname);
llap-server/src/java/org/apache/hadoop/hive/llap/cli/LlapStatusServiceDriver.java,430,LlapInstance llapInstance = appStatusBuilder.removeAndgetLlapInstanceForContainer(containerIdString);
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,316,for (ServiceInstance inst : activeInstances.getAll().values()) {
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,329,public void onCreate(final ServiceInstance serviceInstance) {
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,336,public void onUpdate(final ServiceInstance serviceInstance) {
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,343,public void onRemove(final ServiceInstance serviceInstance) {
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,446,for (ServiceInstance inst : activeInstances.getAll().values()) {
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,494,for (ServiceInstance inst : activeInstances.getAll().values()) {
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,791,for (ServiceInstance inst : activeInstances.getAll().values()) {
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskSchedulerService.java,805,LOG.info("Adding node: " + inst);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/Utils.java,21,import java.util.List;
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/Utils.java,41,List<ServiceInstance> serviceInstances =
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HostAffinitySplitLocationProvider.java,20,import org.apache.hadoop.io.DataOutputBuffer;
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HostAffinitySplitLocationProvider.java,57,long hash = generateHash(fsplit.getPath().toString(), fsplit.getStart());
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HostAffinitySplitLocationProvider.java,58,int indexRaw = (int) (hash % knownLocations.length);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HostAffinitySplitLocationProvider.java,59,int index = Math.abs(indexRaw);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HostAffinitySplitLocationProvider.java,75,private long generateHash(String path, long startOffset) throws IOException {
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HostAffinitySplitLocationProvider.java,81,DataOutputBuffer dob = new DataOutputBuffer();
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HostAffinitySplitLocationProvider.java,82,dob.writeLong(startOffset);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HostAffinitySplitLocationProvider.java,83,dob.writeUTF(path);
ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HostAffinitySplitLocationProvider.java,84,return Murmur3.hash64(dob.getData(), 0, dob.getLength());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/SortedDynPartitionOptimizer.java,215,List<ColumnInfo> colInfos = fsParent.getSchema().getSignature();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/SortedDynPartitionOptimizer.java,216,ArrayList<ExprNodeDesc> bucketColumns = getPositionsToExprNodes(bucketPositions, colInfos);
llap-client/src/java/org/apache/hadoop/hive/llap/tez/LlapProtocolClientProxy.java,477,nodeId.getPort(), retryPolicy, socketFactory);
llap-client/src/java/org/apache/hadoop/hive/llap/tez/LlapProtocolClientProxy.java,479,UserGroupInformation ugi;
llap-client/src/java/org/apache/hadoop/hive/llap/tez/LlapProtocolClientProxy.java,480,try {
llap-client/src/java/org/apache/hadoop/hive/llap/tez/LlapProtocolClientProxy.java,481,ugi = UserGroupInformation.getCurrentUser();
llap-client/src/java/org/apache/hadoop/hive/llap/tez/LlapProtocolClientProxy.java,483,throw new RuntimeException(e);
llap-client/src/java/org/apache/hadoop/hive/llap/tez/LlapProtocolClientProxy.java,496,nodeId.getPort(), retryPolicy, socketFactory);
llap-common/src/java/org/apache/hadoop/hive/llap/impl/LlapProtocolClientImpl.java,50,LlapProtocolBlockingPB proxy;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorGroupByOperator.java,770,vOutContext = new VectorizationContext(getName(), desc.getOutputColumnNames());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapJoinBaseOperator.java,91,vOutContext = new VectorizationContext(getName(), desc.getOutputColumnNames());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorSMBMapJoinOperator.java,129,vOutContext = new VectorizationContext(getName(), desc.getOutputColumnNames());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,170,public VectorizationContext(String contextName, List<String> initialColumnNames) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,190,public VectorizationContext(String contextName) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,497,ve = getCustomUDFExpression(expr, mode);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1282,private ValidatorVectorizationContext() {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1283,super("No Name");
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1750,VectorizationContext vc = new ValidatorVectorizationContext();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1804,VectorizationContext vc = new ValidatorVectorizationContext();
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,134,private final boolean[] columnsToIncludeTruncated;
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,176,columnsToIncludeTruncated = rbCtx.getColumnsToIncludeTruncated(job);
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,297,return rbCtx.createVectorizedRowBatch(columnsToIncludeTruncated);
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,1735,batch.cols[i].reset();
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,1736,batch.cols[i].ensureSize((int) batchSize, false);
orc/src/java/org/apache/orc/impl/TreeReaderFactory.java,1737,fields[i].nextVector(batch.cols[i], null, batchSize);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorAssignRow.java,338,final int projectionColumnNum = projectionColumnNums[logicalColumnIndex];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorAssignRow.java,339,if (object == null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorAssignRow.java,340,VectorizedBatchUtil.setNullColIsNullValue(batch.cols[projectionColumnNum], batchIndex);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorAssignRow.java,341,return;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorAssignRow.java,502,Category targetCategory = targetCategories[logicalColumnIndex];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorAssignRow.java,503,if (targetCategory == null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorAssignRow.java,507,return;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,345,final int projectionColumnNum = projectionColumnNums[logicalColumnIndex];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,346,if (deserializeRead.readCheckNull()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,347,VectorizedBatchUtil.setNullColIsNullValue(batch.cols[projectionColumnNum], batchIndex);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,348,return;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,483,final int projectionColumnNum = projectionColumnNums[logicalColumnIndex];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,484,if (deserializeRead.readCheckNull()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,485,VectorizedBatchUtil.setNullColIsNullValue(batch.cols[projectionColumnNum], batchIndex);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorDeserializeRow.java,486,return;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,125,private transient boolean[] columnsToIncludeTruncated;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,267,vectorDeserializeRow.initConversion(tableRowTypeInfos, columnsToIncludeTruncated);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,283,vectorDeserializeRow.initConversion(tableRowTypeInfos, columnsToIncludeTruncated);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,354,vectorAssign.initConversion(dataTypeInfos, tableRowTypeInfos, columnsToIncludeTruncated);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,399,private void determineColumnsToInclude(Configuration hconf) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,401,columnsToIncludeTruncated = null;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,403,List<Integer> columnsToIncludeTruncatedList = ColumnProjectionUtils.getReadColumnIDs(hconf);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,404,if (columnsToIncludeTruncatedList != null &&
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,405,columnsToIncludeTruncatedList.size() > 0 && columnsToIncludeTruncatedList.size() < dataColumnCount ) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,409,boolean[] columnsToInclude = new boolean[dataColumnCount];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,410,Arrays.fill(columnsToInclude, false);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,411,for (int columnNum : columnsToIncludeTruncatedList) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,412,columnsToInclude[columnNum] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,417,int highestWantedColumnNum = -1;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,418,for (int i = dataColumnCount - 1; i >= 0; i--) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,419,if (columnsToInclude[i]) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,420,highestWantedColumnNum = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,421,break;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,424,if (highestWantedColumnNum == -1) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,425,throw new RuntimeException("No columns to include?");
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,427,int newColumnCount = highestWantedColumnNum + 1;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,428,if (newColumnCount == dataColumnCount) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,429,columnsToIncludeTruncated = columnsToInclude;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,431,columnsToIncludeTruncated = Arrays.copyOf(columnsToInclude, newColumnCount);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,482,determineColumnsToInclude(hconf);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,492,batchContext.createVectorizedRowBatch(columnsToIncludeTruncated);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,498,deserializerBatch = batchContext.createVectorizedRowBatch(columnsToIncludeTruncated);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,511,Arrays.asList(batchContext.getRowColumnNames()),
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,512,Arrays.asList(batchContext.getRowColumnTypeInfos()));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,664,colVector.isNull[0] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,665,colVector.noNulls = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,666,colVector.isRepeating = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,791,deserializerBatch.cols[c].reset();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java,792,deserializerBatch.cols[c].init();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,83,int partitionColumnCount, String[] scratchColumnTypeNames) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,188,int totalColumnCount = rowColumnTypeInfos.length + scratchColumnTypeNames.length;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,189,VectorizedRowBatch result = new VectorizedRowBatch(totalColumnCount);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,191,LOG.info("createVectorizedRowBatch columnsToIncludeTruncated NONE");
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,192,for (int i = 0; i < rowColumnTypeInfos.length; i++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,193,TypeInfo typeInfo = rowColumnTypeInfos[i];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,194,result.cols[i] = VectorizedBatchUtil.createColumnVector(typeInfo);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,197,for (int i = 0; i < scratchColumnTypeNames.length; i++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,198,String typeName = scratchColumnTypeNames[i];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,199,result.cols[rowColumnTypeInfos.length + i] =
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,200,VectorizedBatchUtil.createColumnVector(typeName);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,203,result.setPartitionInfo(dataColumnCount, partitionColumnCount);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,205,result.reset();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,206,return result;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,211,if (columnsToIncludeTruncated == null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,212,return createVectorizedRowBatch();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,215,LOG.info("createVectorizedRowBatch columnsToIncludeTruncated " + Arrays.toString(columnsToIncludeTruncated));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,216,int totalColumnCount = rowColumnTypeInfos.length + scratchColumnTypeNames.length;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,218,for (int i = 0; i < dataColumnCount; i++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,219,TypeInfo typeInfo = rowColumnTypeInfos[i];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,220,result.cols[i] = VectorizedBatchUtil.createColumnVector(typeInfo);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,223,for (int i = dataColumnCount; i < dataColumnCount + partitionColumnCount; i++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,224,TypeInfo typeInfo = rowColumnTypeInfos[i];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,225,result.cols[i] = VectorizedBatchUtil.createColumnVector(typeInfo);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,240,public boolean[] getColumnsToIncludeTruncated(Configuration conf) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,241,boolean[] columnsToIncludeTruncated = null;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,243,List<Integer> columnsToIncludeTruncatedList = ColumnProjectionUtils.getReadColumnIDs(conf);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,244,if (columnsToIncludeTruncatedList != null && columnsToIncludeTruncatedList.size() > 0 ) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,248,boolean[] columnsToInclude = new boolean[dataColumnCount];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,249,Arrays.fill(columnsToInclude, false);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,250,for (int columnNum : columnsToIncludeTruncatedList) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,251,if (columnNum < dataColumnCount) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,252,columnsToInclude[columnNum] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,258,int highestWantedColumnNum = -1;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,259,for (int i = dataColumnCount - 1; i >= 0; i--) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,260,if (columnsToInclude[i]) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,261,highestWantedColumnNum = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,262,break;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,265,if (highestWantedColumnNum == -1) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,266,throw new RuntimeException("No columns to include?");
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,268,int newColumnCount = highestWantedColumnNum + 1;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,269,if (newColumnCount == dataColumnCount) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,271,columnsToIncludeTruncated = columnsToInclude;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,273,columnsToIncludeTruncated = Arrays.copyOf(columnsToInclude, newColumnCount);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,276,return columnsToIncludeTruncated;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,307,break;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java,320,break;
ql/src/java/org/apache/hadoop/hive/ql/io/NullRowsInputFormat.java,70,private final boolean[] columnsToIncludeTruncated;
ql/src/java/org/apache/hadoop/hive/ql/io/NullRowsInputFormat.java,81,columnsToIncludeTruncated = rbCtx.getColumnsToIncludeTruncated(conf);
ql/src/java/org/apache/hadoop/hive/ql/io/NullRowsInputFormat.java,91,columnsToIncludeTruncated = null;
ql/src/java/org/apache/hadoop/hive/ql/io/NullRowsInputFormat.java,108,rbCtx.createVectorizedRowBatch(columnsToIncludeTruncated);
ql/src/java/org/apache/hadoop/hive/ql/io/NullRowsInputFormat.java,146,if (columnsToIncludeTruncated != null
ql/src/java/org/apache/hadoop/hive/ql/io/NullRowsInputFormat.java,147,&& (columnsToIncludeTruncated.length <= i || !columnsToIncludeTruncated[i])) {
ql/src/java/org/apache/hadoop/hive/ql/io/NullRowsInputFormat.java,150,ColumnVector cv = vrb.cols[i];
ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcInputFormat.java,62,private final boolean[] columnsToIncludeTruncated;
ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcInputFormat.java,105,columnsToIncludeTruncated = rbCtx.getColumnsToIncludeTruncated(conf);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/VectorizedOrcInputFormat.java,148,return rbCtx.createVectorizedRowBatch(columnsToIncludeTruncated);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,352,List<String> columnNames;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,353,List<TypeInfo> typeInfos;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,365,public void setColumnNames(List<String> columnNames) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,366,this.columnNames = columnNames;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,368,public void setTypeInfos(List<TypeInfo> typeInfos) {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,369,this.typeInfos = typeInfos;
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,390,String[] columnNameArray = columnNames.toArray(new String[0]);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,391,TypeInfo[] typeInfoArray = typeInfos.toArray(new TypeInfo[0]);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,395,columnNameArray,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,396,typeInfoArray,
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,778,vectorTaskColumnInfo.setColumnNames(allColumnNameList);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,779,vectorTaskColumnInfo.setTypeInfos(allTypeInfoList);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,903,vectorTaskColumnInfo.setColumnNames(reduceColumnNames);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,904,vectorTaskColumnInfo.setTypeInfos(reduceTypeInfos);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1241,LOG.info("ReduceWorkVectorizationNodeProcessor process reduceColumnNames " + vectorTaskColumnInfo.columnNames.toString());
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1243,vContext = new VectorizationContext("__Reduce_Shuffle__", vectorTaskColumnInfo.columnNames);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,1836,VectorizationContext vContext = new VectorizationContext(contextName, vectorTaskColumnInfo.columnNames);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,2401,String[] columnNames = vectorizedRowBatchCtx.getRowColumnNames();
ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java,2406,LOG.debug("debugDisplayAllMaps columnNames " + Arrays.toString(columnNames));
llap-common/src/java/org/apache/hadoop/hive/llap/counters/LlapIOCounters.java,22,NUM_VECTOR_BATCHES,
llap-common/src/java/org/apache/hadoop/hive/llap/counters/LlapIOCounters.java,23,NUM_DECODED_BATCHES,
llap-common/src/java/org/apache/hadoop/hive/llap/counters/LlapIOCounters.java,24,SELECTED_ROWGROUPS,
llap-common/src/java/org/apache/hadoop/hive/llap/counters/LlapIOCounters.java,25,NUM_ERRORS,
llap-common/src/java/org/apache/hadoop/hive/llap/counters/LlapIOCounters.java,26,ROWS_EMITTED,
llap-common/src/java/org/apache/hadoop/hive/llap/counters/LlapIOCounters.java,27,METADATA_CACHE_HIT,
llap-common/src/java/org/apache/hadoop/hive/llap/counters/LlapIOCounters.java,28,METADATA_CACHE_MISS,
llap-common/src/java/org/apache/hadoop/hive/llap/counters/LlapIOCounters.java,29,CACHE_HIT_BYTES,
llap-common/src/java/org/apache/hadoop/hive/llap/counters/LlapIOCounters.java,30,CACHE_MISS_BYTES,
llap-common/src/java/org/apache/hadoop/hive/llap/counters/LlapIOCounters.java,31,ALLOCATED_BYTES,
llap-common/src/java/org/apache/hadoop/hive/llap/counters/LlapIOCounters.java,32,ALLOCATED_USED_BYTES,
llap-common/src/java/org/apache/hadoop/hive/llap/counters/LlapIOCounters.java,33,TOTAL_IO_TIME_NS,
llap-common/src/java/org/apache/hadoop/hive/llap/counters/LlapIOCounters.java,34,DECODE_TIME_NS,
llap-common/src/java/org/apache/hadoop/hive/llap/counters/LlapIOCounters.java,35,HDFS_TIME_NS,
llap-common/src/java/org/apache/hadoop/hive/llap/counters/LlapIOCounters.java,36,CONSUMER_TIME_NS
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTaskReporter.java,83,private final String fragmentFullId;
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTaskReporter.java,99,this.fragmentFullId = fragFullId;
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTaskReporter.java,111,TezCounters tezCounters = task.addAndGetTezCounter(fragmentFullId);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTaskReporter.java,112,FragmentCountersMap.registerCountersForFragment(fragmentFullId, tezCounters);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTaskReporter.java,113,LOG.info("Registered counters for fragment: {} vertexName: {}", fragmentFullId, task.getVertexName());
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTaskReporter.java,126,LOG.info("Unregistered counters for fragment: {}", fragmentFullId);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTaskReporter.java,127,FragmentCountersMap.unregisterCountersForFragment(fragmentFullId);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java,26,import java.util.concurrent.Executors;
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java,102,private volatile ListeningExecutorService executor;
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java,182,ExecutorService executorReal = Executors.newFixedThreadPool(1,
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java,187,executor = MoreExecutors.listeningDecorator(executorReal);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java,213,TezTaskAttemptID taskAttemptID = taskSpec.getTaskAttemptID();
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java,214,TezTaskID taskId = taskAttemptID.getTaskID();
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java,215,TezVertexID tezVertexID = taskId.getVertexID();
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java,216,TezDAGID tezDAGID = tezVertexID.getDAGId();
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java,217,String fragFullId = Joiner.on('_').join(tezDAGID.getId(), tezVertexID.getId(), taskId.getId(),
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java,218,taskAttemptID.getId());
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,74,private final ListeningExecutorService executor;
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,79,ListeningExecutorService executor) {
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,156,String dagId = job.get("tez.mapreduce.dag.index");
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,157,String vertexId = job.get("tez.mapreduce.vertex.index");
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,158,String taskId = job.get("tez.mapreduce.task.index");
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,159,String taskAttemptId = job.get("tez.mapreduce.task.attempt.index");
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,161,if (dagId != null && vertexId != null && taskId != null && taskAttemptId != null) {
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,162,String fullId = Joiner.on('_').join(dagId, vertexId, taskId, taskAttemptId);
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,163,taskCounters = FragmentCountersMap.getCountersForFragment(fullId);
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,164,LOG.info("Received dagid_vertexid_taskid_attempid: {}", fullId);
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,166,LOG.warn("Not using tez counters as some identifier is null." +
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,168,dagId, vertexId, taskId, taskAttemptId);
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,236,private final class UncaughtErrorHandler implements FutureCallback<Void> {
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,238,public void onSuccess(Void result) {
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,243,public void onFailure(Throwable t) {
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,245,LlapIoImpl.LOG.error("Unhandled error from reader thread " + t.getMessage());
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,246,setError(t);
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java,72,private final ListeningExecutorService executor;
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java,140,executor = MoreExecutors.listeningDecorator(Executors.newFixedThreadPool(numThreads,
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java,141,new ThreadFactoryBuilder().setNameFormat("IO-Elevator-Thread-%d").setDaemon(true).build()));
ql/src/java/org/apache/hadoop/hive/ql/hooks/PostExecTezSummaryPrinter.java,61,if ("HIVE".equals(group.getDisplayName())) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,125,import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,981,if (genericeUdf != null &&
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,988,if (mode == Mode.PROJECTION) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,989,vclass = ColOrCol.class;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,991,vclass = FilterExprOrExpr.class;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,994,if (mode == Mode.PROJECTION) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,995,vclass = ColAndCol.class;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,997,vclass = FilterExprAndExpr.class;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1003,if (mode == Mode.PROJECTION) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1004,return createVectorMultiAndOrProjectionExpr(vclass, childExpr, childrenMode, returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1006,return createVectorExpression(vclass, childExpr, childrenMode, returnType);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1046,private void determineChildrenVectorExprAndArguments(Class<?> vectorClass,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1047,List<ExprNodeDesc> childExpr, int numChildren, Mode childrenMode,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1049,throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1050,for (int i = 0; i < numChildren; i++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1051,ExprNodeDesc child = childExpr.get(i);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1052,String undecoratedName = getUndecoratedName(child.getTypeInfo().getTypeName());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1053,inputTypes[i] = VectorExpression.Type.getValue(undecoratedName);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1054,if (inputTypes[i] == VectorExpression.Type.OTHER){
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1055,throw new HiveException("No vector type for " + vectorClass.getSimpleName() + " argument #" + i + " type name " + undecoratedName);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1057,if (child instanceof ExprNodeGenericFuncDesc) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1058,VectorExpression vChild = getVectorExpression(child, childrenMode);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1059,children.add(vChild);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1060,arguments[i] = vChild.getOutputColumn();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1062,int colIndex = getInputColumnIndex((ExprNodeColumnDesc) child);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1063,if (childrenMode == Mode.FILTER) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1065,children.add(new SelectColumnIsTrue(colIndex));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1067,arguments[i] = colIndex;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1069,Object scalarValue = getVectorTypeScalarValue((ExprNodeConstantDesc) child);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1070,arguments[i] = (null == scalarValue) ? getConstantVectorExpression(null, child.getTypeInfo(), childrenMode) : scalarValue;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1072,throw new HiveException("Cannot handle expression type: " + child.getClass().getSimpleName());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1084,determineChildrenVectorExprAndArguments(vectorClass, childExpr, numChildren, childrenMode,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1085,inputTypes, children, arguments);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1086,VectorExpression  vectorExpression = instantiateExpression(vectorClass, returnType, arguments);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1087,vectorExpression.setInputTypes(inputTypes);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1088,if ((vectorExpression != null) && !children.isEmpty()) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1089,vectorExpression.setChildExpressions(children.toArray(new VectorExpression[0]));
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1091,return vectorExpression;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1093,throw new HiveException(ex);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1095,for (VectorExpression ve : children) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1096,ocm.freeOutputColumn(ve.getOutputColumn());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1101,private VectorExpression createVectorMultiAndOrProjectionExpr(Class<?> vectorClass,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1102,List<ExprNodeDesc> childExpr, Mode childrenMode, TypeInfo returnType) throws HiveException {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1103,int numChildren = childExpr == null ? 0: childExpr.size();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1104,VectorExpression.Type [] inputTypes = new VectorExpression.Type[numChildren];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1105,List<VectorExpression> children = new ArrayList<VectorExpression>();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1106,Object[] arguments = new Object[numChildren];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1107,try {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1108,determineChildrenVectorExprAndArguments(vectorClass, childExpr, numChildren, childrenMode,
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1109,inputTypes, children, arguments);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1112,int[] colNums = new int[numChildren];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1114,colNums[i] = (Integer) arguments[i];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1116,arguments = new Object[1];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1117,arguments[0] = colNums;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,20,import java.util.Arrays;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,26,import com.google.common.base.Preconditions;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,39,private int[] colNums;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,41,private int[] mapToChildExpression;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,42,private int[] andSelected;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,43,private boolean[] intermediateNulls;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,45,public ColAndCol(int[] colNums, int outputColumn) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,47,this.colNums = colNums;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,49,mapToChildExpression = null;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,50,andSelected = new int[VectorizedRowBatch.DEFAULT_SIZE];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,51,intermediateNulls = new boolean[VectorizedRowBatch.DEFAULT_SIZE];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,61,Preconditions.checkState(colNums.length >= 2);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,68,if (childExpressions != null && mapToChildExpression == null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,70,mapToChildExpression = new int [colNums.length];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,74,int outputColumn = ve.getOutputColumn();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,89,final int n = batch.size;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,90,if (n <= 0) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,92,return;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,100,int childExpressionIndex = mapToChildExpression[0];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,101,if (childExpressionIndex != -1) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,102,VectorExpression ve = childExpressions[childExpressionIndex];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,103,Preconditions.checkState(ve.getOutputColumn() == colNums[0]);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,104,ve.evaluate(batch);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,129,boolean andRepeating = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,130,boolean andRepeatingIsNull = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,136,int andSel = 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,138,Arrays.fill(intermediateNulls, 0, VectorizedRowBatch.DEFAULT_SIZE, false);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,141,outV.reset();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,143,LongColumnVector firstColVector = (LongColumnVector) batch.cols[colNums[0]];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,144,long[] firstVector = firstColVector.vector;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,150,if (firstColVector.isRepeating) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,151,if (firstColVector.noNulls || !firstColVector.isNull[0]) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,152,if (firstVector[0] == 0) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,154,outV.isRepeating = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,155,outputVector[0] = 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,156,return;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,161,Preconditions.checkState(firstColVector.isNull[0]);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,164,andRepeatingIsNull = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,166,andRepeating = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,172,if (batch.selectedInUse) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,173,for (int j = 0; j != n; j++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,174,int i = sel[j];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,175,if (firstVector[i] == 1) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,176,andSelected[andSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,180,for (int i = 0; i != n; i++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,181,if (firstVector[i] == 1) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,182,andSelected[andSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,192,if (batch.selectedInUse) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,193,for (int j = 0; j != n; j++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,194,int i = sel[j];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,195,if (firstColVector.isNull[i]) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,196,intermediateNulls[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,197,andSelected[andSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,199,andSelected[andSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,203,for (int i = 0; i != n; i++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,204,if (firstColVector.isNull[i]) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,205,intermediateNulls[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,206,andSelected[andSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,208,andSelected[andSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,217,int colNum = 1;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,218,do {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,219,if (!andRepeating && andSel == 0) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,225,break;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,228,if (childExpressions != null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,229,int childExpressionIndex = mapToChildExpression[colNum];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,230,if (childExpressionIndex != -1) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,231,if (andRepeating) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,236,VectorExpression ve = childExpressions[childExpressionIndex];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,237,Preconditions.checkState(ve.getOutputColumn() == colNums[colNum]);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,238,ve.evaluate(batch);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,245,boolean saveSelectedInUse = batch.selectedInUse;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,246,int[] saveSelected = sel;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,247,batch.selectedInUse = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,248,batch.selected = andSelected;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,250,VectorExpression ve = childExpressions[childExpressionIndex];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,251,Preconditions.checkState(ve.getOutputColumn() == colNums[colNum]);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,252,ve.evaluate(batch);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,254,batch.selectedInUse = saveSelectedInUse;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,255,batch.selected = saveSelected;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,260,LongColumnVector nextColVector = (LongColumnVector) batch.cols[colNums[colNum]];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,261,long[] nextVector = nextColVector.vector;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,263,if (andRepeating) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,269,if (nextColVector.isRepeating) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,274,if (nextColVector.noNulls || !nextColVector.isNull[0]) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,275,if (nextVector[0] == 0) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,277,outV.isRepeating = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,278,outputVector[0] = 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,279,return;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,284,Preconditions.checkState(nextColVector.isNull[0]);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,287,andRepeatingIsNull = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,296,if (nextColVector.noNulls) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,302,Preconditions.checkState(andSel == 0);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,303,andRepeating = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,305,if (andRepeatingIsNull) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,311,if (batch.selectedInUse) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,312,for (int j = 0; j != n; j++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,313,int i = sel[j];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,314,if (nextVector[i] == 1) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,315,intermediateNulls[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,316,andSelected[andSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,320,for (int i = 0; i != n; i++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,321,if (nextVector[i] == 1) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,322,intermediateNulls[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,323,andSelected[andSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,327,andRepeatingIsNull = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,334,if (batch.selectedInUse) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,335,for (int j = 0; j != n; j++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,336,int i = sel[j];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,337,if (nextVector[i] == 1) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,338,andSelected[andSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,342,for (int i = 0; i != n; i++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,343,if (nextVector[i] == 1) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,344,andSelected[andSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,355,Preconditions.checkState(andSel == 0);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,356,andRepeating = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,358,if (andRepeatingIsNull) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,364,if (batch.selectedInUse) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,365,for (int j = 0; j != n; j++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,366,int i = sel[j];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,367,if (nextColVector.isNull[i] || nextVector[i] == 1) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,368,intermediateNulls[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,369,andSelected[andSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,373,for (int i = 0; i != n; i++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,374,if (nextColVector.isNull[i] || nextVector[i] == 1) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,375,intermediateNulls[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,376,andSelected[andSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,380,andRepeatingIsNull = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,388,if (batch.selectedInUse) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,389,for (int j = 0; j != n; j++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,390,int i = sel[j];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,391,if (nextColVector.isNull[i]) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,392,intermediateNulls[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,393,andSelected[andSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,395,andSelected[andSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,399,for (int i = 0; i != n; i++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,400,if (nextColVector.isNull[i]) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,401,intermediateNulls[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,402,andSelected[andSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,404,andSelected[andSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,417,if (nextColVector.isRepeating) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,422,if (nextColVector.noNulls || !nextColVector.isNull[0]) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,424,if (nextVector[0] == 0) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,426,outV.isRepeating = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,427,outputVector[0] = 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,428,return;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,433,Preconditions.checkState(nextColVector.isNull[0]);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,437,for (int j = 0; j < andSel; j++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,438,int i = andSelected[j];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,439,intermediateNulls[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,451,int newSel = 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,452,for (int j = 0; j < andSel; j++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,453,int i = andSelected[j];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,454,if (nextVector[i] == 1) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,455,andSelected[newSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,458,andSel = newSel;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,468,int newSel = 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,469,for (int j = 0; j < andSel; j++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,470,int i = andSelected[j];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,471,if (nextColVector.isNull[i]) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,473,intermediateNulls[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,474,andSelected[newSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,476,andSelected[newSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,479,andSel = newSel;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,487,if (andRepeating) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,488,outV.isRepeating = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,489,if (andRepeatingIsNull) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,491,outV.noNulls = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,492,outV.isNull[0] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,495,outputVector[0] = 1;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,499,outV.isRepeating = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,500,outputVector[0] = 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,503,int andIndex = 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,504,if (batch.selectedInUse) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,517,for (int j = 0; j != n; j++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,518,int i = sel[j];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,519,if (andIndex < andSel && andSelected[andIndex] == i) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,522,if (intermediateNulls[i]) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,523,outV.noNulls = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,524,outV.isNull[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,526,outputVector[i] = 1;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,528,andIndex++;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,531,outputVector[i] = 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,534,Preconditions.checkState(andIndex == andSel);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,543,Arrays.fill(outputVector, 0, n, 0);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,544,for (int j = 0; j < andSel; j++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,545,int i = andSelected[j];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,546,if (intermediateNulls[i]) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,547,outV.noNulls = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,548,outV.isNull[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,550,outputVector[i] = 1;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,21,import java.util.Arrays;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,27,import com.google.common.base.Preconditions;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,40,private int[] colNums;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,42,private int[] mapToChildExpression;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,43,private int[] orSelected;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,44,private boolean[] intermediateNulls;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,46,public ColOrCol(int[] colNums, int outputColumn) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,48,this.colNums = colNums;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,50,mapToChildExpression = null;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,51,orSelected = new int[VectorizedRowBatch.DEFAULT_SIZE];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,52,intermediateNulls = new boolean[VectorizedRowBatch.DEFAULT_SIZE];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,62,Preconditions.checkState(colNums.length >= 2);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,69,if (childExpressions != null && mapToChildExpression == null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,71,mapToChildExpression = new int [colNums.length];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,75,int outputColumn = ve.getOutputColumn();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,90,final int n = batch.size;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,91,if (n <= 0) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,93,return;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,101,int childExpressionIndex = mapToChildExpression[0];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,102,if (childExpressionIndex != -1) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,103,VectorExpression ve = childExpressions[childExpressionIndex];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,104,Preconditions.checkState(ve.getOutputColumn() == colNums[0]);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,105,ve.evaluate(batch);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,133,boolean orRepeating = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,134,boolean orRepeatingHasNulls = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,140,int orSel = 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,142,Arrays.fill(intermediateNulls, 0, VectorizedRowBatch.DEFAULT_SIZE, false);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,145,outV.reset();
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,148,if (batch.selectedInUse) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,149,for (int j = 0; j != n; j++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,150,int i = sel[j];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,151,outputVector[i] = 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,154,Arrays.fill(outputVector, 0, n, 0);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,157,LongColumnVector firstColVector = (LongColumnVector) batch.cols[colNums[0]];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,158,long[] firstVector = firstColVector.vector;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,164,if (firstColVector.isRepeating) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,165,if (firstColVector.noNulls || !firstColVector.isNull[0]) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,166,if (firstVector[0] == 1) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,168,outV.isRepeating = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,169,outputVector[0] = 1;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,170,return;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,175,Preconditions.checkState(firstColVector.isNull[0]);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,178,orRepeatingHasNulls = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,180,orRepeating = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,187,if (batch.selectedInUse) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,188,for (int j = 0; j != n; j++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,189,int i = sel[j];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,190,if (firstVector[i] == 0) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,191,orSelected[orSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,193,outputVector[i] = 1;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,197,for (int i = 0; i != n; i++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,198,if (firstVector[i] == 0) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,199,orSelected[orSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,201,outputVector[i] = 1;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,212,if (batch.selectedInUse) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,213,for (int j = 0; j != n; j++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,214,int i = sel[j];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,215,if (firstColVector.isNull[i]) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,216,intermediateNulls[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,217,orSelected[orSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,219,orSelected[orSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,221,outputVector[i] = 1;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,225,for (int i = 0; i != n; i++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,226,if (firstColVector.isNull[i]) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,227,intermediateNulls[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,228,orSelected[orSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,230,orSelected[orSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,232,outputVector[i] = 1;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,241,int colNum = 1;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,242,do {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,243,if (!orRepeating && orSel == 0) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,249,break;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,252,if (childExpressions != null) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,253,int childExpressionIndex = mapToChildExpression[colNum];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,254,if (childExpressionIndex != -1) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,255,if (orRepeating) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,259,VectorExpression ve = childExpressions[childExpressionIndex];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,260,Preconditions.checkState(ve.getOutputColumn() == colNums[colNum]);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,261,ve.evaluate(batch);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,267,boolean saveSelectedInUse = batch.selectedInUse;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,268,int[] saveSelected = sel;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,269,batch.selectedInUse = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,270,batch.selected = orSelected;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,272,VectorExpression ve = childExpressions[childExpressionIndex];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,273,Preconditions.checkState(ve.getOutputColumn() == colNums[colNum]);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,274,ve.evaluate(batch);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,276,batch.selectedInUse = saveSelectedInUse;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,277,batch.selected = saveSelected;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,282,LongColumnVector nextColVector = (LongColumnVector) batch.cols[colNums[colNum]];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,283,long[] nextVector = nextColVector.vector;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,285,if (orRepeating) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,291,if (nextColVector.isRepeating) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,296,if (nextColVector.noNulls || !nextColVector.isNull[0]) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,297,if (nextVector[0] == 1) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,298,outV.isRepeating = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,299,outputVector[0] = 1;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,300,return;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,305,Preconditions.checkState(nextColVector.isNull[0]);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,308,orRepeatingHasNulls = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,317,if (nextColVector.noNulls) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,323,Preconditions.checkState(orSel == 0);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,324,orRepeating = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,326,if (orRepeatingHasNulls) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,333,if (batch.selectedInUse) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,334,for (int j = 0; j != n; j++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,335,int i = sel[j];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,336,if (nextVector[i] == 0) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,337,intermediateNulls[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,338,orSelected[orSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,340,outputVector[i] = 1;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,344,for (int i = 0; i != n; i++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,345,if (nextVector[i] == 0) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,346,intermediateNulls[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,347,orSelected[orSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,349,outputVector[i] = 1;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,353,orRepeatingHasNulls = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,361,if (batch.selectedInUse) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,362,for (int j = 0; j != n; j++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,363,int i = sel[j];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,364,if (nextVector[i] == 0) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,365,orSelected[orSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,367,outputVector[i] = 1;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,371,for (int i = 0; i != n; i++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,372,if (nextVector[i] == 0) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,373,orSelected[orSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,375,outputVector[i] = 1;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,386,Preconditions.checkState(orSel == 0);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,387,orRepeating = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,389,if (orRepeatingHasNulls) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,396,if (batch.selectedInUse) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,397,for (int j = 0; j != n; j++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,398,int i = sel[j];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,399,if (nextColVector.isNull[i] || nextVector[i] == 0) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,400,intermediateNulls[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,401,orSelected[orSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,403,outputVector[i] = 1;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,407,for (int i = 0; i != n; i++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,408,if (nextColVector.isNull[i] || nextVector[i] == 0) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,409,intermediateNulls[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,410,orSelected[orSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,412,outputVector[i] = 1;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,416,orRepeatingHasNulls = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,425,if (batch.selectedInUse) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,426,for (int j = 0; j != n; j++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,427,int i = sel[j];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,428,if (nextColVector.isNull[i]) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,429,intermediateNulls[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,430,orSelected[orSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,432,orSelected[orSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,434,outputVector[i] = 1;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,438,for (int i = 0; i != n; i++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,439,if (nextColVector.isNull[i]) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,440,intermediateNulls[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,441,orSelected[orSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,443,orSelected[orSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,445,outputVector[i] = 1;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,458,if (nextColVector.isRepeating) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,460,if (nextColVector.noNulls || !nextColVector.isNull[0]) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,462,if (nextVector[0] == 1) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,464,outV.isRepeating = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,465,outputVector[0] = 1;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,466,return;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,471,Preconditions.checkState(nextColVector.isNull[0]);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,475,for (int j = 0; j < orSel; j++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,476,int i = orSelected[j];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,477,intermediateNulls[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,489,int newSel = 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,490,for (int j = 0; j < orSel; j++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,491,int i = orSelected[j];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,492,if (nextVector[i] == 0) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,493,orSelected[newSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,495,outputVector[i] = 1;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,498,orSel = newSel;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,509,int newSel = 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,510,for (int j = 0; j < orSel; j++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,511,int i = orSelected[j];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,512,if (nextColVector.isNull[i]) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,514,intermediateNulls[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,515,orSelected[newSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,517,orSelected[newSel++] = i;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,519,outputVector[i] = 1;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,522,orSel = newSel;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,530,if (orRepeating) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,535,outV.isRepeating = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,536,if (orRepeatingHasNulls) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,537,outV.noNulls = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,538,outV.isNull[0] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,540,outputVector[0] = 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,548,for (int j = 0; j < orSel; j++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,549,int i = orSelected[j];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,550,Preconditions.checkState(outputVector[i] == 0);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,551,if (intermediateNulls[i]) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,552,outV.noNulls = false;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,553,outV.isNull[i] = true;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1128,if (childrenMode != VectorExpressionDescriptor.Mode.PROJECTION){
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1129,for (VectorExpression ve : children) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java,1130,ocm.freeOutputColumn(ve.getOutputColumn());
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,71,Arrays.fill(mapToChildExpression, -1);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,72,for (int c = 0; c < childExpressions.length; c++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,73,VectorExpression ve = childExpressions[c];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,75,int i = 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,76,while (true) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,77,if (i >= colNums.length) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,78,throw new RuntimeException("Vectorized child expression output not found");
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,80,if (colNums[i] == outputColumn) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,81,mapToChildExpression[i] = c;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,82,break;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColAndCol.java,84,i++;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,72,Arrays.fill(mapToChildExpression, -1);
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,73,for (int c = 0; c < childExpressions.length; c++) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,74,VectorExpression ve = childExpressions[c];
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,76,int i = 0;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,77,while (true) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,78,if (i >= colNums.length) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,79,throw new RuntimeException("Vectorized child expression output not found");
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,81,if (colNums[i] == outputColumn) {
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,82,mapToChildExpression[i] = c;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,83,break;
ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ColOrCol.java,85,i++;
llap-client/src/java/org/apache/hadoop/hive/llap/LlapBaseRecordReader.java,133,throw new IOException("Received reader event error: " + event.getMessage());
llap-client/src/java/org/apache/hadoop/hive/llap/LlapBaseRecordReader.java,135,throw new IOException("Got reader event type " + event.getEventType() + ", expected error event");
llap-client/src/java/org/apache/hadoop/hive/llap/LlapBaseRecordReader.java,195,getReaderThread().interrupt();
common/src/java/org/apache/hadoop/hive/conf/HiveConf.java,658,METASTORE_CONNECTION_POOLING_TYPE("datanucleus.connectionPoolingType", "BONECP",
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,2914,String user = HiveConf.getVar(conf, HiveConf.ConfVars.METASTORE_CONNECTION_USER_NAME);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,2915,String passwd;
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,2916,try {
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,2917,passwd = ShimLoader.getHadoopShims().getPassword(conf,
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,2918,HiveConf.ConfVars.METASTOREPWD.varname);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,2920,throw new SQLException("Error getting metastore password", err);
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,2922,String connectionPooler = HiveConf.getVar(conf,
metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java,2943,new PoolableConnectionFactory(connFactory, objectPool, null, null, false, true);
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,29,import org.apache.hadoop.hive.llap.DebugUtils;
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,60,import com.google.common.base.Joiner;
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,61,import com.google.common.base.Preconditions;
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,62,import com.google.common.util.concurrent.FutureCallback;
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,63,import com.google.common.util.concurrent.Futures;
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,64,import com.google.common.util.concurrent.ListenableFuture;
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,65,import com.google.common.util.concurrent.ListeningExecutorService;
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,105,boolean isVectorMode = Utilities.getUseVectorizedInputFileFormat(job);
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,106,if (!isVectorMode) {
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,107,LlapIoImpl.LOG.error("No LLAP IO in non-vectorized mode");
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,108,throw new UnsupportedOperationException("No LLAP IO in non-vectorized mode");
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapInputFormat.java,174,rbCtx = mapWork.getVectorizedRowBatchCtx();
ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java,494,String message = toErrorMessage(value, row, current.rowObjectInspector);
ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java,227,public static boolean canWrapAnyForLlap(Configuration conf, MapWork mapWork) {
ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java,228,return Utilities.getUseVectorizedInputFileFormat(conf, mapWork);
ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java,231,public static boolean canWrapForLlap(Class<? extends InputFormat> inputFormatClass) {
ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java,232,return LlapWrappableInputFormatInterface.class.isAssignableFrom(inputFormatClass);
ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java,144,CombineHiveInputFormat.AvoidSplitCombination {
ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java,910,work.getMapWork().deriveLlap(conf);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java,915,((MapWork)w).deriveLlap(conf);
ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java,922,((MapWork) w).deriveLlap(conf);
ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java,212,public void deriveLlap(Configuration conf) {
ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java,215,boolean isLlapOn = HiveConf.getBoolVar(conf, ConfVars.LLAP_IO_ENABLED, llapMode),
ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java,216,canWrapAny = isLlapOn && HiveInputFormat.canWrapAnyForLlap(conf, this);
ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java,221,boolean isUsingLlapIo = HiveInputFormat.canWrapForLlap(part.getInputFileFormatClass());
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java,403,LOG.warn("Failed to indicate dag complete dagId={} to node {}", dagIdentifier, llapNodeId);
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java,436,LOG.error(
llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java,438,nodeId, request, t);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/ContainerRunnerImpl.java,105,AMReporter amReporter, ClassLoader classLoader, DaemonId daemonId) {
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/ContainerRunnerImpl.java,236,this, tezHadoopShim, attemptId, vertex);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/ContainerRunnerImpl.java,292,HiveConf.getVar(source.getConf(), HiveConf.ConfVars.HIVEQUERYID));
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapDaemon.java,251,amReporter, executorClassLoader, daemonId);
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java,128,SignableVertexSpec vertex) {
llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java,191,UserGroupInformation taskUgi = UserGroupInformation.createRemoteUser(vertex.getUser());
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java,73,private LlapDaemonCacheMetrics cacheMetrics;
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java,74,private LlapDaemonIOMetrics ioMetrics;
llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java,76,private Allocator allocator;
shims/common/src/main/java/org/apache/hadoop/hive/shims/ShimLoader.java,95,hadoopShims = loadShims(HADOOP_SHIM_CLASSES, HadoopShims.class);
