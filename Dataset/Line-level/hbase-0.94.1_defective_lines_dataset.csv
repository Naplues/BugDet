File,Line_number,SRC
src/main/java/org/apache/hadoop/hbase/client/HTablePool.java,260,this.tables.remove(tableName, table);
src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java,636,connections.remove(remoteId, this);
src/main/java/org/apache/hadoop/hbase/mapreduce/hadoopbackport/InputSampler.java,344,K[] samples = sampler.getSample(inf, job);
src/main/java/org/apache/hadoop/hbase/util/PoolMap.java,89,remove((K) key, pool.get());
src/main/java/org/apache/hadoop/hbase/util/PoolMap.java,94,public boolean remove(K key, V value) {
src/main/java/org/apache/hadoop/hbase/client/replication/ReplicationAdmin.java,88,throw new IOException("Unable setup the ZooKeeper connection", e);
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureServer.java,608,SecureCall call = new SecureCall(id, param, this, responder, buf.length);
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,206,private int maxQueueSize;
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,1553,private void setupResponse(ByteArrayOutputStream response,
src/main/java/org/apache/hadoop/hbase/util/HBaseFsckRepair.java,150,rs.closeRegion(region, false);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java,149,this.identifier = descriptor;
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java,336,this.identifier = this.identifier + "-0x" +
src/main/java/org/apache/hadoop/hbase/thrift/generated/AlreadyExists.java,229,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/BatchMutation.java,317,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/BatchMutation.java,440,Mutation _elem2; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/BatchMutation.java,537,Mutation _elem7; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/ColumnDescriptor.java,733,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,4736,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,5089,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,5466,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,5819,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,6196,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,6611,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,6692,__isset_bit_vector = new BitSet(1);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,7013,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,7366,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,7731,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,8084,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,8374,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,8763,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,8874,org.apache.thrift.protocol.TList _list26 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,8875,struct.success = new ArrayList<ByteBuffer>(_list26.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,8878,ByteBuffer _elem28; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,8879,_elem28 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,8880,struct.success.add(_elem28);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,8919,oprot.writeBinary(_iter29);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,8960,oprot.writeBinary(_iter30);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,8975,org.apache.thrift.protocol.TList _list31 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,8976,struct.success = new ArrayList<ByteBuffer>(_list31.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,8979,ByteBuffer _elem33; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,8980,_elem33 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,8981,struct.success.add(_elem33);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9213,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9650,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9761,org.apache.thrift.protocol.TMap _map34 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9762,struct.success = new HashMap<ByteBuffer,ColumnDescriptor>(2*_map34.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9765,ByteBuffer _key36; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9766,ColumnDescriptor _val37; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9767,_key36 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9768,_val37 = new ColumnDescriptor();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9769,_val37.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9770,struct.success.put(_key36, _val37);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9809,oprot.writeBinary(_iter38.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9810,_iter38.getValue().write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9851,oprot.writeBinary(_iter39.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9852,_iter39.getValue().write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9867,org.apache.thrift.protocol.TMap _map40 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9868,struct.success = new HashMap<ByteBuffer,ColumnDescriptor>(2*_map40.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9871,ByteBuffer _key42; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9872,ColumnDescriptor _val43; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9873,_key42 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9874,_val43 = new ColumnDescriptor();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9875,_val43.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,9876,struct.success.put(_key42, _val43);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10108,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10540,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10651,org.apache.thrift.protocol.TList _list44 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10652,struct.success = new ArrayList<TRegionInfo>(_list44.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10655,TRegionInfo _elem46; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10656,_elem46 = new TRegionInfo();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10657,_elem46.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10658,struct.success.add(_elem46);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10697,_iter47.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10738,_iter48.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10753,org.apache.thrift.protocol.TList _list49 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10754,struct.success = new ArrayList<TRegionInfo>(_list49.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10757,TRegionInfo _elem51; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10758,_elem51 = new TRegionInfo();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10759,_elem51.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,10760,struct.success.add(_elem51);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11083,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11202,org.apache.thrift.protocol.TList _list52 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11203,struct.columnFamilies = new ArrayList<ColumnDescriptor>(_list52.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11206,ColumnDescriptor _elem54; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11207,_elem54 = new ColumnDescriptor();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11208,_elem54.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11209,struct.columnFamilies.add(_elem54);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11244,_iter55.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11283,_iter56.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11299,org.apache.thrift.protocol.TList _list57 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11300,struct.columnFamilies = new ArrayList<ColumnDescriptor>(_list57.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11303,ColumnDescriptor _elem59; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11304,_elem59 = new ColumnDescriptor();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11305,_elem59.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11306,struct.columnFamilies.add(_elem59);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,11629,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,12092,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,12445,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13080,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13251,org.apache.thrift.protocol.TMap _map60 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13252,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map60.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13255,ByteBuffer _key62; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13256,ByteBuffer _val63; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13257,_key62 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13258,_val63 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13259,struct.attributes.put(_key62, _val63);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13304,oprot.writeBinary(_iter64.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13305,oprot.writeBinary(_iter64.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13356,oprot.writeBinary(_iter65.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13357,oprot.writeBinary(_iter65.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13381,org.apache.thrift.protocol.TMap _map66 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13382,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map66.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13385,ByteBuffer _key68; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13386,ByteBuffer _val69; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13387,_key68 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13388,_val69 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13389,struct.attributes.put(_key68, _val69);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13673,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13784,org.apache.thrift.protocol.TList _list70 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13785,struct.success = new ArrayList<TCell>(_list70.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13788,TCell _elem72; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13789,_elem72 = new TCell();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13790,_elem72.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13791,struct.success.add(_elem72);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13830,_iter73.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13871,_iter74.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13886,org.apache.thrift.protocol.TList _list75 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13887,struct.success = new ArrayList<TCell>(_list75.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13890,TCell _elem77; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13891,_elem77 = new TCell();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13892,_elem77.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,13893,struct.success.add(_elem77);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14457,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14652,org.apache.thrift.protocol.TMap _map78 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14653,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map78.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14656,ByteBuffer _key80; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14657,ByteBuffer _val81; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14658,_key80 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14659,_val81 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14660,struct.attributes.put(_key80, _val81);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14708,oprot.writeBinary(_iter82.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14709,oprot.writeBinary(_iter82.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14766,oprot.writeBinary(_iter83.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14767,oprot.writeBinary(_iter83.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14795,org.apache.thrift.protocol.TMap _map84 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14796,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map84.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14799,ByteBuffer _key86; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14800,ByteBuffer _val87; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14801,_key86 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14802,_val87 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,14803,struct.attributes.put(_key86, _val87);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15087,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15198,org.apache.thrift.protocol.TList _list88 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15199,struct.success = new ArrayList<TCell>(_list88.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15202,TCell _elem90; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15203,_elem90 = new TCell();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15204,_elem90.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15205,struct.success.add(_elem90);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15244,_iter91.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15285,_iter92.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15300,org.apache.thrift.protocol.TList _list93 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15301,struct.success = new ArrayList<TCell>(_list93.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15304,TCell _elem95; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15305,_elem95 = new TCell();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15306,_elem95.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15307,struct.success.add(_elem95);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,15942,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16159,org.apache.thrift.protocol.TMap _map96 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16160,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map96.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16163,ByteBuffer _key98; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16164,ByteBuffer _val99; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16165,_key98 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16166,_val99 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16167,struct.attributes.put(_key98, _val99);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16218,oprot.writeBinary(_iter100.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16219,oprot.writeBinary(_iter100.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16282,oprot.writeBinary(_iter101.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16283,oprot.writeBinary(_iter101.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16315,org.apache.thrift.protocol.TMap _map102 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16316,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map102.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16319,ByteBuffer _key104; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16320,ByteBuffer _val105; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16321,_key104 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16322,_val105 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16323,struct.attributes.put(_key104, _val105);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16607,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16718,org.apache.thrift.protocol.TList _list106 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16719,struct.success = new ArrayList<TCell>(_list106.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16722,TCell _elem108; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16723,_elem108 = new TCell();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16724,_elem108.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16725,struct.success.add(_elem108);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16764,_iter109.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16805,_iter110.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16820,org.apache.thrift.protocol.TList _list111 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16821,struct.success = new ArrayList<TCell>(_list111.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16824,TCell _elem113; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16825,_elem113 = new TCell();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16826,_elem113.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,16827,struct.success.add(_elem113);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17236,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17381,org.apache.thrift.protocol.TMap _map114 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17382,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map114.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17385,ByteBuffer _key116; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17386,ByteBuffer _val117; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17387,_key116 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17388,_val117 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17389,struct.attributes.put(_key116, _val117);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17429,oprot.writeBinary(_iter118.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17430,oprot.writeBinary(_iter118.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17475,oprot.writeBinary(_iter119.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17476,oprot.writeBinary(_iter119.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17496,org.apache.thrift.protocol.TMap _map120 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17497,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map120.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17500,ByteBuffer _key122; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17501,ByteBuffer _val123; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17502,_key122 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17503,_val123 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17504,struct.attributes.put(_key122, _val123);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17788,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17899,org.apache.thrift.protocol.TList _list124 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17900,struct.success = new ArrayList<TRowResult>(_list124.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17903,TRowResult _elem126; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17904,_elem126 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17905,_elem126.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17906,struct.success.add(_elem126);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17945,_iter127.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,17986,_iter128.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18001,org.apache.thrift.protocol.TList _list129 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18002,struct.success = new ArrayList<TRowResult>(_list129.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18005,TRowResult _elem131; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18006,_elem131 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18007,_elem131.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18008,struct.success.add(_elem131);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18508,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18671,org.apache.thrift.protocol.TList _list132 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18672,struct.columns = new ArrayList<ByteBuffer>(_list132.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18675,ByteBuffer _elem134; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18676,_elem134 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18677,struct.columns.add(_elem134);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18689,org.apache.thrift.protocol.TMap _map135 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18690,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map135.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18693,ByteBuffer _key137; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18694,ByteBuffer _val138; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18695,_key137 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18696,_val138 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18697,struct.attributes.put(_key137, _val138);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18737,oprot.writeBinary(_iter139);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18749,oprot.writeBinary(_iter140.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18750,oprot.writeBinary(_iter140.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18798,oprot.writeBinary(_iter141);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18807,oprot.writeBinary(_iter142.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18808,oprot.writeBinary(_iter142.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18828,org.apache.thrift.protocol.TList _list143 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18829,struct.columns = new ArrayList<ByteBuffer>(_list143.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18832,ByteBuffer _elem145; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18833,_elem145 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18834,struct.columns.add(_elem145);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18841,org.apache.thrift.protocol.TMap _map146 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18842,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map146.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18845,ByteBuffer _key148; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18846,ByteBuffer _val149; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18847,_key148 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18848,_val149 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,18849,struct.attributes.put(_key148, _val149);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19133,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19244,org.apache.thrift.protocol.TList _list150 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19245,struct.success = new ArrayList<TRowResult>(_list150.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19248,TRowResult _elem152; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19249,_elem152 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19250,_elem152.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19251,struct.success.add(_elem152);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19290,_iter153.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19331,_iter154.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19346,org.apache.thrift.protocol.TList _list155 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19347,struct.success = new ArrayList<TRowResult>(_list155.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19350,TRowResult _elem157; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19351,_elem157 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19352,_elem157.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19353,struct.success.add(_elem157);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,19836,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20005,org.apache.thrift.protocol.TMap _map158 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20006,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map158.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20009,ByteBuffer _key160; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20010,ByteBuffer _val161; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20011,_key160 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20012,_val161 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20013,struct.attributes.put(_key160, _val161);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20056,oprot.writeBinary(_iter162.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20057,oprot.writeBinary(_iter162.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20108,oprot.writeBinary(_iter163.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20109,oprot.writeBinary(_iter163.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20133,org.apache.thrift.protocol.TMap _map164 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20134,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map164.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20137,ByteBuffer _key166; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20138,ByteBuffer _val167; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20139,_key166 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20140,_val167 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20141,struct.attributes.put(_key166, _val167);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20425,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20536,org.apache.thrift.protocol.TList _list168 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20537,struct.success = new ArrayList<TRowResult>(_list168.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20540,TRowResult _elem170; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20541,_elem170 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20542,_elem170.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20543,struct.success.add(_elem170);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20582,_iter171.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20623,_iter172.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20638,org.apache.thrift.protocol.TList _list173 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20639,struct.success = new ArrayList<TRowResult>(_list173.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20642,TRowResult _elem175; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20643,_elem175 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20644,_elem175.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,20645,struct.success.add(_elem175);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21207,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21386,org.apache.thrift.protocol.TList _list176 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21387,struct.columns = new ArrayList<ByteBuffer>(_list176.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21390,ByteBuffer _elem178; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21391,_elem178 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21392,struct.columns.add(_elem178);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21412,org.apache.thrift.protocol.TMap _map179 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21413,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map179.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21416,ByteBuffer _key181; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21417,ByteBuffer _val182; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21418,_key181 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21419,_val182 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21420,struct.attributes.put(_key181, _val182);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21460,oprot.writeBinary(_iter183);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21475,oprot.writeBinary(_iter184.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21476,oprot.writeBinary(_iter184.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21527,oprot.writeBinary(_iter185);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21539,oprot.writeBinary(_iter186.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21540,oprot.writeBinary(_iter186.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21560,org.apache.thrift.protocol.TList _list187 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21561,struct.columns = new ArrayList<ByteBuffer>(_list187.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21564,ByteBuffer _elem189; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21565,_elem189 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21566,struct.columns.add(_elem189);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21577,org.apache.thrift.protocol.TMap _map190 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21578,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map190.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21581,ByteBuffer _key192; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21582,ByteBuffer _val193; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21583,_key192 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21584,_val193 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21585,struct.attributes.put(_key192, _val193);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21869,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21980,org.apache.thrift.protocol.TList _list194 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21981,struct.success = new ArrayList<TRowResult>(_list194.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21984,TRowResult _elem196; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21985,_elem196 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21986,_elem196.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,21987,struct.success.add(_elem196);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22026,_iter197.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22067,_iter198.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22082,org.apache.thrift.protocol.TList _list199 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22083,struct.success = new ArrayList<TRowResult>(_list199.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22086,TRowResult _elem201; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22087,_elem201 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22088,_elem201.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22089,struct.success.add(_elem201);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22508,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22645,org.apache.thrift.protocol.TList _list202 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22646,struct.rows = new ArrayList<ByteBuffer>(_list202.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22649,ByteBuffer _elem204; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22650,_elem204 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22651,struct.rows.add(_elem204);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22663,org.apache.thrift.protocol.TMap _map205 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22664,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map205.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22667,ByteBuffer _key207; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22668,ByteBuffer _val208; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22669,_key207 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22670,_val208 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22671,struct.attributes.put(_key207, _val208);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22706,oprot.writeBinary(_iter209);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22718,oprot.writeBinary(_iter210.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22719,oprot.writeBinary(_iter210.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22761,oprot.writeBinary(_iter211);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22770,oprot.writeBinary(_iter212.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22771,oprot.writeBinary(_iter212.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22787,org.apache.thrift.protocol.TList _list213 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22788,struct.rows = new ArrayList<ByteBuffer>(_list213.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22791,ByteBuffer _elem215; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22792,_elem215 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22793,struct.rows.add(_elem215);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22800,org.apache.thrift.protocol.TMap _map216 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22801,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map216.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22804,ByteBuffer _key218; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22805,ByteBuffer _val219; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22806,_key218 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22807,_val219 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,22808,struct.attributes.put(_key218, _val219);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23092,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23203,org.apache.thrift.protocol.TList _list220 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23204,struct.success = new ArrayList<TRowResult>(_list220.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23207,TRowResult _elem222; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23208,_elem222 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23209,_elem222.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23210,struct.success.add(_elem222);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23249,_iter223.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23290,_iter224.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23305,org.apache.thrift.protocol.TList _list225 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23306,struct.success = new ArrayList<TRowResult>(_list225.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23309,TRowResult _elem227; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23310,_elem227 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23311,_elem227.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23312,struct.success.add(_elem227);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23822,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23977,org.apache.thrift.protocol.TList _list228 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23978,struct.rows = new ArrayList<ByteBuffer>(_list228.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23981,ByteBuffer _elem230; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23982,_elem230 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23983,struct.rows.add(_elem230);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23995,org.apache.thrift.protocol.TList _list231 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23996,struct.columns = new ArrayList<ByteBuffer>(_list231.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,23999,ByteBuffer _elem233; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24000,_elem233 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24001,struct.columns.add(_elem233);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24013,org.apache.thrift.protocol.TMap _map234 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24014,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map234.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24017,ByteBuffer _key236; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24018,ByteBuffer _val237; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24019,_key236 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24020,_val237 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24021,struct.attributes.put(_key236, _val237);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24056,oprot.writeBinary(_iter238);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24068,oprot.writeBinary(_iter239);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24080,oprot.writeBinary(_iter240.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24081,oprot.writeBinary(_iter240.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24126,oprot.writeBinary(_iter241);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24135,oprot.writeBinary(_iter242);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24144,oprot.writeBinary(_iter243.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24145,oprot.writeBinary(_iter243.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24161,org.apache.thrift.protocol.TList _list244 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24162,struct.rows = new ArrayList<ByteBuffer>(_list244.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24165,ByteBuffer _elem246; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24166,_elem246 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24167,struct.rows.add(_elem246);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24174,org.apache.thrift.protocol.TList _list247 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24175,struct.columns = new ArrayList<ByteBuffer>(_list247.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24178,ByteBuffer _elem249; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24179,_elem249 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24180,struct.columns.add(_elem249);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24187,org.apache.thrift.protocol.TMap _map250 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24188,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map250.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24191,ByteBuffer _key252; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24192,ByteBuffer _val253; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24193,_key252 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24194,_val253 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24195,struct.attributes.put(_key252, _val253);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24479,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24590,org.apache.thrift.protocol.TList _list254 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24591,struct.success = new ArrayList<TRowResult>(_list254.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24594,TRowResult _elem256; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24595,_elem256 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24596,_elem256.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24597,struct.success.add(_elem256);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24636,_iter257.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24677,_iter258.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24692,org.apache.thrift.protocol.TList _list259 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24693,struct.success = new ArrayList<TRowResult>(_list259.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24696,TRowResult _elem261; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24697,_elem261 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24698,_elem261.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,24699,struct.success.add(_elem261);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25192,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25345,org.apache.thrift.protocol.TList _list262 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25346,struct.rows = new ArrayList<ByteBuffer>(_list262.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25349,ByteBuffer _elem264; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25350,_elem264 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25351,struct.rows.add(_elem264);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25371,org.apache.thrift.protocol.TMap _map265 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25372,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map265.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25375,ByteBuffer _key267; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25376,ByteBuffer _val268; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25377,_key267 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25378,_val268 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25379,struct.attributes.put(_key267, _val268);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25414,oprot.writeBinary(_iter269);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25429,oprot.writeBinary(_iter270.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25430,oprot.writeBinary(_iter270.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25475,oprot.writeBinary(_iter271);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25487,oprot.writeBinary(_iter272.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25488,oprot.writeBinary(_iter272.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25504,org.apache.thrift.protocol.TList _list273 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25505,struct.rows = new ArrayList<ByteBuffer>(_list273.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25508,ByteBuffer _elem275; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25509,_elem275 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25510,struct.rows.add(_elem275);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25521,org.apache.thrift.protocol.TMap _map276 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25522,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map276.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25525,ByteBuffer _key278; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25526,ByteBuffer _val279; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25527,_key278 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25528,_val279 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25529,struct.attributes.put(_key278, _val279);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25813,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25924,org.apache.thrift.protocol.TList _list280 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25925,struct.success = new ArrayList<TRowResult>(_list280.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25928,TRowResult _elem282; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25929,_elem282 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25930,_elem282.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25931,struct.success.add(_elem282);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,25970,_iter283.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26011,_iter284.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26026,org.apache.thrift.protocol.TList _list285 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26027,struct.success = new ArrayList<TRowResult>(_list285.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26030,TRowResult _elem287; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26031,_elem287 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26032,_elem287.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26033,struct.success.add(_elem287);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26605,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26776,org.apache.thrift.protocol.TList _list288 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26777,struct.rows = new ArrayList<ByteBuffer>(_list288.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26780,ByteBuffer _elem290; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26781,_elem290 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26782,struct.rows.add(_elem290);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26794,org.apache.thrift.protocol.TList _list291 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26795,struct.columns = new ArrayList<ByteBuffer>(_list291.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26798,ByteBuffer _elem293; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26799,_elem293 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26800,struct.columns.add(_elem293);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26820,org.apache.thrift.protocol.TMap _map294 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26821,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map294.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26824,ByteBuffer _key296; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26825,ByteBuffer _val297; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26826,_key296 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26827,_val297 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26828,struct.attributes.put(_key296, _val297);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26863,oprot.writeBinary(_iter298);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26875,oprot.writeBinary(_iter299);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26890,oprot.writeBinary(_iter300.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26891,oprot.writeBinary(_iter300.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26939,oprot.writeBinary(_iter301);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26948,oprot.writeBinary(_iter302);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26960,oprot.writeBinary(_iter303.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26961,oprot.writeBinary(_iter303.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26977,org.apache.thrift.protocol.TList _list304 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26978,struct.rows = new ArrayList<ByteBuffer>(_list304.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26981,ByteBuffer _elem306; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26982,_elem306 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26983,struct.rows.add(_elem306);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26990,org.apache.thrift.protocol.TList _list307 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26991,struct.columns = new ArrayList<ByteBuffer>(_list307.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26994,ByteBuffer _elem309; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26995,_elem309 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,26996,struct.columns.add(_elem309);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27007,org.apache.thrift.protocol.TMap _map310 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27008,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map310.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27011,ByteBuffer _key312; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27012,ByteBuffer _val313; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27013,_key312 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27014,_val313 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27015,struct.attributes.put(_key312, _val313);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27299,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27410,org.apache.thrift.protocol.TList _list314 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27411,struct.success = new ArrayList<TRowResult>(_list314.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27414,TRowResult _elem316; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27415,_elem316 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27416,_elem316.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27417,struct.success.add(_elem316);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27456,_iter317.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27497,_iter318.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27512,org.apache.thrift.protocol.TList _list319 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27513,struct.success = new ArrayList<TRowResult>(_list319.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27516,TRowResult _elem321; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27517,_elem321 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27518,_elem321.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,27519,struct.success.add(_elem321);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28019,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28182,org.apache.thrift.protocol.TList _list322 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28183,struct.mutations = new ArrayList<Mutation>(_list322.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28186,Mutation _elem324; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28187,_elem324 = new Mutation();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28188,_elem324.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28189,struct.mutations.add(_elem324);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28201,org.apache.thrift.protocol.TMap _map325 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28202,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map325.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28205,ByteBuffer _key327; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28206,ByteBuffer _val328; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28207,_key327 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28208,_val328 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28209,struct.attributes.put(_key327, _val328);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28249,_iter329.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28261,oprot.writeBinary(_iter330.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28262,oprot.writeBinary(_iter330.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28310,_iter331.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28319,oprot.writeBinary(_iter332.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28320,oprot.writeBinary(_iter332.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28340,org.apache.thrift.protocol.TList _list333 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28341,struct.mutations = new ArrayList<Mutation>(_list333.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28344,Mutation _elem335; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28345,_elem335 = new Mutation();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28346,_elem335.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28347,struct.mutations.add(_elem335);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28354,org.apache.thrift.protocol.TMap _map336 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28355,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map336.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28358,ByteBuffer _key338; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28359,ByteBuffer _val339; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28360,_key338 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28361,_val339 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28362,struct.attributes.put(_key338, _val339);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,28626,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29388,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29523,__isset_bit_vector = new BitSet(1);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29567,org.apache.thrift.protocol.TList _list340 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29568,struct.mutations = new ArrayList<Mutation>(_list340.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29571,Mutation _elem342; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29572,_elem342 = new Mutation();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29573,_elem342.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29574,struct.mutations.add(_elem342);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29594,org.apache.thrift.protocol.TMap _map343 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29595,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map343.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29598,ByteBuffer _key345; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29599,ByteBuffer _val346; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29600,_key345 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29601,_val346 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29602,struct.attributes.put(_key345, _val346);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29642,_iter347.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29657,oprot.writeBinary(_iter348.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29658,oprot.writeBinary(_iter348.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29709,_iter349.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29721,oprot.writeBinary(_iter350.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29722,oprot.writeBinary(_iter350.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29742,org.apache.thrift.protocol.TList _list351 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29743,struct.mutations = new ArrayList<Mutation>(_list351.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29746,Mutation _elem353; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29747,_elem353 = new Mutation();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29748,_elem353.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29749,struct.mutations.add(_elem353);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29760,org.apache.thrift.protocol.TMap _map354 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29761,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map354.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29764,ByteBuffer _key356; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29765,ByteBuffer _val357; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29766,_key356 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29767,_val357 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,29768,struct.attributes.put(_key356, _val357);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30032,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30639,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30776,org.apache.thrift.protocol.TList _list358 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30777,struct.rowBatches = new ArrayList<BatchMutation>(_list358.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30780,BatchMutation _elem360; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30781,_elem360 = new BatchMutation();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30782,_elem360.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30783,struct.rowBatches.add(_elem360);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30795,org.apache.thrift.protocol.TMap _map361 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30796,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map361.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30799,ByteBuffer _key363; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30800,ByteBuffer _val364; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30801,_key363 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30802,_val364 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30803,struct.attributes.put(_key363, _val364);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30838,_iter365.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30850,oprot.writeBinary(_iter366.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30851,oprot.writeBinary(_iter366.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30893,_iter367.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30902,oprot.writeBinary(_iter368.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30903,oprot.writeBinary(_iter368.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30919,org.apache.thrift.protocol.TList _list369 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30920,struct.rowBatches = new ArrayList<BatchMutation>(_list369.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30923,BatchMutation _elem371; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30924,_elem371 = new BatchMutation();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30925,_elem371.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30926,struct.rowBatches.add(_elem371);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30933,org.apache.thrift.protocol.TMap _map372 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30934,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map372.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30937,ByteBuffer _key374; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30938,ByteBuffer _val375; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30939,_key374 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30940,_val375 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,30941,struct.attributes.put(_key374, _val375);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,31205,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,31886,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32037,org.apache.thrift.protocol.TList _list376 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32038,struct.rowBatches = new ArrayList<BatchMutation>(_list376.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32041,BatchMutation _elem378; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32042,_elem378 = new BatchMutation();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32043,_elem378.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32044,struct.rowBatches.add(_elem378);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32064,org.apache.thrift.protocol.TMap _map379 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32065,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map379.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32068,ByteBuffer _key381; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32069,ByteBuffer _val382; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32070,_key381 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32071,_val382 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32072,struct.attributes.put(_key381, _val382);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32107,_iter383.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32122,oprot.writeBinary(_iter384.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32123,oprot.writeBinary(_iter384.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32168,_iter385.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32180,oprot.writeBinary(_iter386.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32181,oprot.writeBinary(_iter386.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32197,org.apache.thrift.protocol.TList _list387 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32198,struct.rowBatches = new ArrayList<BatchMutation>(_list387.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32201,BatchMutation _elem389; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32202,_elem389 = new BatchMutation();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32203,_elem389.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32204,struct.rowBatches.add(_elem389);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32215,org.apache.thrift.protocol.TMap _map390 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32216,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map390.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32219,ByteBuffer _key392; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32220,ByteBuffer _val393; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32221,_key392 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32222,_val393 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32223,struct.attributes.put(_key392, _val393);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,32487,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,33143,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,33736,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,33835,__isset_bit_vector = new BitSet(1);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34451,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34622,org.apache.thrift.protocol.TMap _map394 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34623,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map394.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34626,ByteBuffer _key396; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34627,ByteBuffer _val397; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34628,_key396 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34629,_val397 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34630,struct.attributes.put(_key396, _val397);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34675,oprot.writeBinary(_iter398.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34676,oprot.writeBinary(_iter398.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34727,oprot.writeBinary(_iter399.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34728,oprot.writeBinary(_iter399.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34752,org.apache.thrift.protocol.TMap _map400 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34753,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map400.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34756,ByteBuffer _key402; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34757,ByteBuffer _val403; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34758,_key402 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34759,_val403 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34760,struct.attributes.put(_key402, _val403);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,34965,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,35674,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,35869,org.apache.thrift.protocol.TMap _map404 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,35870,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map404.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,35873,ByteBuffer _key406; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,35874,ByteBuffer _val407; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,35875,_key406 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,35876,_val407 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,35877,struct.attributes.put(_key406, _val407);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,35925,oprot.writeBinary(_iter408.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,35926,oprot.writeBinary(_iter408.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,35983,oprot.writeBinary(_iter409.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,35984,oprot.writeBinary(_iter409.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36012,org.apache.thrift.protocol.TMap _map410 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36013,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map410.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36016,ByteBuffer _key412; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36017,ByteBuffer _val413; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36018,_key412 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36019,_val413 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36020,struct.attributes.put(_key412, _val413);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36225,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36779,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36924,org.apache.thrift.protocol.TMap _map414 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36925,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map414.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36928,ByteBuffer _key416; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36929,ByteBuffer _val417; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36930,_key416 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36931,_val417 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36932,struct.attributes.put(_key416, _val417);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36972,oprot.writeBinary(_iter418.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,36973,oprot.writeBinary(_iter418.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,37018,oprot.writeBinary(_iter419.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,37019,oprot.writeBinary(_iter419.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,37039,org.apache.thrift.protocol.TMap _map420 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,37040,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map420.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,37043,ByteBuffer _key422; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,37044,ByteBuffer _val423; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,37045,_key422 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,37046,_val423 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,37047,struct.attributes.put(_key422, _val423);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,37252,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,37619,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,37974,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38361,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38454,org.apache.thrift.protocol.TList _list424 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38455,struct.increments = new ArrayList<TIncrement>(_list424.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38458,TIncrement _elem426; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38459,_elem426 = new TIncrement();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38460,_elem426.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38461,struct.increments.add(_elem426);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38491,_iter427.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38524,_iter428.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38536,org.apache.thrift.protocol.TList _list429 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38537,struct.increments = new ArrayList<TIncrement>(_list429.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38540,TIncrement _elem431; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38541,_elem431 = new TIncrement();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38542,_elem431.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38543,struct.increments.add(_elem431);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,38748,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39376,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39545,org.apache.thrift.protocol.TMap _map432 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39546,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map432.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39549,ByteBuffer _key434; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39550,ByteBuffer _val435; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39551,_key434 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39552,_val435 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39553,struct.attributes.put(_key434, _val435);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39596,oprot.writeBinary(_iter436.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39597,oprot.writeBinary(_iter436.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39648,oprot.writeBinary(_iter437.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39649,oprot.writeBinary(_iter437.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39673,org.apache.thrift.protocol.TMap _map438 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39674,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map438.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39677,ByteBuffer _key440; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39678,ByteBuffer _val441; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39679,_key440 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39680,_val441 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39681,struct.attributes.put(_key440, _val441);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,39886,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40430,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40576,org.apache.thrift.protocol.TMap _map442 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40577,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map442.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40580,ByteBuffer _key444; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40581,ByteBuffer _val445; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40582,_key444 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40583,_val445 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40584,struct.attributes.put(_key444, _val445);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40624,oprot.writeBinary(_iter446.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40625,oprot.writeBinary(_iter446.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40670,oprot.writeBinary(_iter447.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40671,oprot.writeBinary(_iter447.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40692,org.apache.thrift.protocol.TMap _map448 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40693,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map448.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40696,ByteBuffer _key450; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40697,ByteBuffer _val451; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40698,_key450 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40699,_val451 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40700,struct.attributes.put(_key450, _val451);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,40967,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41048,__isset_bit_vector = new BitSet(1);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41661,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41824,org.apache.thrift.protocol.TList _list452 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41825,struct.columns = new ArrayList<ByteBuffer>(_list452.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41828,ByteBuffer _elem454; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41829,_elem454 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41830,struct.columns.add(_elem454);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41842,org.apache.thrift.protocol.TMap _map455 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41843,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map455.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41846,ByteBuffer _key457; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41847,ByteBuffer _val458; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41848,_key457 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41849,_val458 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41850,struct.attributes.put(_key457, _val458);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41890,oprot.writeBinary(_iter459);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41902,oprot.writeBinary(_iter460.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41903,oprot.writeBinary(_iter460.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41951,oprot.writeBinary(_iter461);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41960,oprot.writeBinary(_iter462.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41961,oprot.writeBinary(_iter462.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41981,org.apache.thrift.protocol.TList _list463 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41982,struct.columns = new ArrayList<ByteBuffer>(_list463.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41985,ByteBuffer _elem465; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41986,_elem465 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41987,struct.columns.add(_elem465);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41994,org.apache.thrift.protocol.TMap _map466 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41995,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map466.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41998,ByteBuffer _key468; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,41999,ByteBuffer _val469; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,42000,_key468 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,42001,_val469 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,42002,struct.attributes.put(_key468, _val469);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,42269,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,42350,__isset_bit_vector = new BitSet(1);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43048,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43237,org.apache.thrift.protocol.TList _list470 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43238,struct.columns = new ArrayList<ByteBuffer>(_list470.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43241,ByteBuffer _elem472; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43242,_elem472 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43243,struct.columns.add(_elem472);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43255,org.apache.thrift.protocol.TMap _map473 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43256,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map473.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43259,ByteBuffer _key475; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43260,ByteBuffer _val476; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43261,_key475 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43262,_val476 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43263,struct.attributes.put(_key475, _val476);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43308,oprot.writeBinary(_iter477);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43320,oprot.writeBinary(_iter478.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43321,oprot.writeBinary(_iter478.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43375,oprot.writeBinary(_iter479);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43384,oprot.writeBinary(_iter480.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43385,oprot.writeBinary(_iter480.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43409,org.apache.thrift.protocol.TList _list481 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43410,struct.columns = new ArrayList<ByteBuffer>(_list481.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43413,ByteBuffer _elem483; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43414,_elem483 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43415,struct.columns.add(_elem483);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43422,org.apache.thrift.protocol.TMap _map484 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43423,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map484.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43426,ByteBuffer _key486; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43427,ByteBuffer _val487; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43428,_key486 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43429,_val487 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43430,struct.attributes.put(_key486, _val487);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43697,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,43778,__isset_bit_vector = new BitSet(1);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44379,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44542,org.apache.thrift.protocol.TList _list488 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44543,struct.columns = new ArrayList<ByteBuffer>(_list488.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44546,ByteBuffer _elem490; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44547,_elem490 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44548,struct.columns.add(_elem490);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44560,org.apache.thrift.protocol.TMap _map491 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44561,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map491.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44564,ByteBuffer _key493; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44565,ByteBuffer _val494; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44566,_key493 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44567,_val494 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44568,struct.attributes.put(_key493, _val494);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44608,oprot.writeBinary(_iter495);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44620,oprot.writeBinary(_iter496.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44621,oprot.writeBinary(_iter496.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44669,oprot.writeBinary(_iter497);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44678,oprot.writeBinary(_iter498.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44679,oprot.writeBinary(_iter498.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44699,org.apache.thrift.protocol.TList _list499 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44700,struct.columns = new ArrayList<ByteBuffer>(_list499.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44703,ByteBuffer _elem501; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44704,_elem501 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44705,struct.columns.add(_elem501);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44712,org.apache.thrift.protocol.TMap _map502 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44713,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map502.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44716,ByteBuffer _key504; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44717,ByteBuffer _val505; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44718,_key504 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44719,_val505 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44720,struct.attributes.put(_key504, _val505);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,44987,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,45068,__isset_bit_vector = new BitSet(1);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,45755,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,45934,org.apache.thrift.protocol.TList _list506 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,45935,struct.columns = new ArrayList<ByteBuffer>(_list506.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,45938,ByteBuffer _elem508; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,45939,_elem508 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,45940,struct.columns.add(_elem508);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,45960,org.apache.thrift.protocol.TMap _map509 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,45961,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map509.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,45964,ByteBuffer _key511; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,45965,ByteBuffer _val512; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,45966,_key511 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,45967,_val512 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,45968,struct.attributes.put(_key511, _val512);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46008,oprot.writeBinary(_iter513);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46023,oprot.writeBinary(_iter514.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46024,oprot.writeBinary(_iter514.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46075,oprot.writeBinary(_iter515);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46087,oprot.writeBinary(_iter516.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46088,oprot.writeBinary(_iter516.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46108,org.apache.thrift.protocol.TList _list517 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46109,struct.columns = new ArrayList<ByteBuffer>(_list517.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46112,ByteBuffer _elem519; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46113,_elem519 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46114,struct.columns.add(_elem519);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46125,org.apache.thrift.protocol.TMap _map520 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46126,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map520.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46129,ByteBuffer _key522; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46130,ByteBuffer _val523; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46131,_key522 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46132,_val523 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46133,struct.attributes.put(_key522, _val523);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46400,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,46481,__isset_bit_vector = new BitSet(1);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47253,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47456,org.apache.thrift.protocol.TList _list524 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47457,struct.columns = new ArrayList<ByteBuffer>(_list524.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47460,ByteBuffer _elem526; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47461,_elem526 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47462,struct.columns.add(_elem526);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47482,org.apache.thrift.protocol.TMap _map527 = iprot.readMapBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47483,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map527.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47486,ByteBuffer _key529; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47487,ByteBuffer _val530; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47488,_key529 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47489,_val530 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47490,struct.attributes.put(_key529, _val530);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47535,oprot.writeBinary(_iter531);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47550,oprot.writeBinary(_iter532.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47551,oprot.writeBinary(_iter532.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47608,oprot.writeBinary(_iter533);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47620,oprot.writeBinary(_iter534.getKey());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47621,oprot.writeBinary(_iter534.getValue());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47645,org.apache.thrift.protocol.TList _list535 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47646,struct.columns = new ArrayList<ByteBuffer>(_list535.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47649,ByteBuffer _elem537; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47650,_elem537 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47651,struct.columns.add(_elem537);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47662,org.apache.thrift.protocol.TMap _map538 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47663,struct.attributes = new HashMap<ByteBuffer,ByteBuffer>(2*_map538.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47666,ByteBuffer _key540; // required
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47667,ByteBuffer _val541; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47668,_key540 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47669,_val541 = iprot.readBinary();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47670,struct.attributes.put(_key540, _val541);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,47937,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,48018,__isset_bit_vector = new BitSet(1);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,48344,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,48831,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,48960,org.apache.thrift.protocol.TList _list542 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,48961,struct.success = new ArrayList<TRowResult>(_list542.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,48964,TRowResult _elem544; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,48965,_elem544 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,48966,_elem544.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,48967,struct.success.add(_elem544);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,49015,_iter545.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,49064,_iter546.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,49082,org.apache.thrift.protocol.TList _list547 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,49083,struct.success = new ArrayList<TRowResult>(_list547.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,49086,TRowResult _elem549; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,49087,_elem549 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,49088,_elem549.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,49089,struct.success.add(_elem549);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,49390,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,49910,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50039,org.apache.thrift.protocol.TList _list550 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50040,struct.success = new ArrayList<TRowResult>(_list550.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50043,TRowResult _elem552; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50044,_elem552 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50045,_elem552.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50046,struct.success.add(_elem552);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50094,_iter553.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50143,_iter554.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50161,org.apache.thrift.protocol.TList _list555 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50162,struct.success = new ArrayList<TRowResult>(_list555.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50165,TRowResult _elem557; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50166,_elem557 = new TRowResult();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50167,_elem557.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50168,struct.success.add(_elem557);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50398,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,50806,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,51388,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,51902,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52013,org.apache.thrift.protocol.TList _list558 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52014,struct.success = new ArrayList<TCell>(_list558.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52017,TCell _elem560; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52018,_elem560 = new TCell();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52019,_elem560.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52020,struct.success.add(_elem560);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52059,_iter561.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52100,_iter562.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52115,org.apache.thrift.protocol.TList _list563 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52116,struct.success = new ArrayList<TCell>(_list563.size);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52119,TCell _elem565; // optional
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52120,_elem565 = new TCell();
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52121,_elem565.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52122,struct.success.add(_elem565);
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52354,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Hbase.java,52766,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/IOError.java,230,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/IllegalArgument.java,229,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/Mutation.java,432,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/TCell.java,303,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/TIncrement.java,439,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/TRegionInfo.java,625,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/TRowResult.java,322,return 0;
src/main/java/org/apache/hadoop/hbase/thrift/generated/TScan.java,558,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TColumn.java,369,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TColumnIncrement.java,371,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TColumnValue.java,439,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TDelete.java,530,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TGet.java,501,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,1683,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,2130,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,2614,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,3058,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,3578,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,4074,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,4598,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,4983,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,5768,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,6370,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,6874,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,7291,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,7740,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,8125,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,8594,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,9090,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,9950,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,10552,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,11036,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,11480,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,11972,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,12419,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,12897,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,13428,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,13916,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,13980,__isset_bit_vector = new BitSet(1);
src/main/java/org/apache/hadoop/hbase/thrift2/generated/THBaseService.java,14330,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIOError.java,224,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIllegalArgument.java,223,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIncrement.java,383,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIncrement.java,526,org.apache.thrift.protocol.TList _list32 = iprot.readListBegin();
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIncrement.java,527,struct.columns = new ArrayList<TColumnIncrement>(_list32.size);
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIncrement.java,531,_elem34 = new TColumnIncrement();
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIncrement.java,532,_elem34.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIncrement.java,533,struct.columns.add(_elem34);
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIncrement.java,576,_iter35.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIncrement.java,609,_iter36.write(oprot);
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIncrement.java,628,org.apache.thrift.protocol.TList _list37 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIncrement.java,629,struct.columns = new ArrayList<TColumnIncrement>(_list37.size);
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIncrement.java,633,_elem39 = new TColumnIncrement();
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIncrement.java,634,_elem39.read(iprot);
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TIncrement.java,635,struct.columns.add(_elem39);
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TPut.java,444,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TResult.java,317,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TScan.java,553,return 0;
src/main/java/org/apache/hadoop/hbase/thrift2/generated/TTimeRange.java,287,return 0;
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,171,ct.start();
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,180,private void cleanupCatalogTracker(final CatalogTracker ct) {
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,906,LOG.warn("Encountered problems when prefetch META table: ", e);
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,522,private final Map<Integer, SoftValueSortedMap<byte [], HRegionLocation>>
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,524,new HashMap<Integer, SoftValueSortedMap<byte [], HRegionLocation>>();
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,535,private final Set<Integer> regionCachePrefetchDisabledTables =
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,536,new CopyOnWriteArraySet<Integer>();
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1179,Integer key = Bytes.mapKey(tableName);
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1204,this.cachedRegionLocations.remove(Bytes.mapKey(tableName));
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1612,Integer key = Bytes.mapKey(tableName);
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1615,this.cachedRegionLocations.get(key);
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1639,regionCachePrefetchDisabledTables.add(Bytes.mapKey(tableName));
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1642,regionCachePrefetchDisabledTables.remove(Bytes.mapKey(tableName));
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1647,return !regionCachePrefetchDisabledTables.contains(Bytes.mapKey(tableName));
src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java,292,&& !(cause instanceof RegionServerStoppedException))) {
src/main/java/org/apache/hadoop/hbase/client/ScannerCallable.java,126,rrs = server.next(scannerId, caching);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,286,final Map<String, RegionScanner> scanners =
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,287,new ConcurrentHashMap<String, RegionScanner>();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,513,RegionScanner scanner = scanners.get(scannerIdString);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,928,for (Map.Entry<String, RegionScanner> e : this.scanners.entrySet()) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,930,e.getValue().close();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2332,scanners.put(scannerName, s);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2345,public Result[] next(final long scannerId, int nbRows) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2347,RegionScanner s = this.scanners.get(scannerName);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2474,RegionScanner s = scanners.remove(this.scannerName);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2475,if (s != null) {
src/main/java/org/apache/hadoop/hbase/util/JVMClusterUtil.java,84,server = hrsc.getConstructor(Configuration.class).newInstance(c);
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,1392,LOG.debug(getName()+", call "+call+": error: " + e, e);
src/main/java/org/apache/hadoop/hbase/ipc/ExecRPCInvoker.java,81,LOG.debug("Result is region="+ Bytes.toStringBinary(regionName) +
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,112,zk.delete(path, version);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,150,return zk.exists(path, watcher);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,176,return zk.exists(path, watch);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,212,return zk.getChildren(path, watcher);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,238,return zk.getChildren(path, watch);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,264,byte[] revData = zk.getData(path, watcher, stat);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,291,byte[] revData = zk.getData(path, watch, stat);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,321,return zk.setData(path, newData, version);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,332,byte[] revData = zk.getData(path, false, stat);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,397,return zk.create(path, data, acl, createMode);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,405,byte[] currentData = zk.getData(path, false, null);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,451,return zk.create(newPath, data, acl, createMode);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,475,List<String> nodes = zk.getChildren(parent, false);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,479,Stat stat = zk.exists(nodePath, false);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,524,return zk.getSessionId();
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,528,zk.close();
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,532,return zk.getState();
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,540,return zk.getSessionPasswd();
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,543,public void sync(String path, AsyncCallback.VoidCallback cb, Object ctx) {
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,544,this.zk.sync(path, null, null);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java,199,try {
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java,201,InetAddress.getByName(host);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java,202,anyValid = true;
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKConfig.java,204,LOG.warn(StringUtils.stringifyException(e));
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java,392,public void sync(String path) {
src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java,328,p.add(HConstants.CATALOG_FAMILY, HConstants.SERVER_QUALIFIER,
src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java,330,p.add(HConstants.CATALOG_FAMILY, HConstants.STARTCODE_QUALIFIER,
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2385,: results.toArray(new Result[0]);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2415,: results.toArray(new Result[0]);
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,365,FSDataOutputStream s = fs.create(versionFile);
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,369,s.close();
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,462,Path filePath = new Path(rootdir, HConstants.CLUSTER_ID_FILE_NAME);
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,463,FSDataOutputStream s = fs.create(filePath);
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,467,LOG.debug("Created cluster ID file at " + filePath.toString() +
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,109,private long lastNodeCreateTime = Long.MAX_VALUE;
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,283,LOG.warn("returning success without actually splitting and " +
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,434,lastNodeCreateTime = EnvironmentEdgeManager.currentTimeMillis();
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,449,new GetDataAsyncCallback(), retry_count);
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,457,new GetDataAsyncCallback(), new Long(-1) /* retry count */);
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,680,lastNodeCreateTime = EnvironmentEdgeManager.currentTimeMillis();
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,983,((EnvironmentEdgeManager.currentTimeMillis() - lastNodeCreateTime) >
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,1060,getDataSetWatchSuccess(path, null, Integer.MIN_VALUE);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1328,int readRequestsCount = 0;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1329,int writeRequestsCount = 0;
src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java,548,markClosed(e);
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,291,handleSaslConnectionFailure(numRetries++, MAX_RETRIES, ex, rand,
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,292,ticket);
src/main/java/org/apache/hadoop/hbase/thrift/TBoundedThreadPoolServer.java,287,while (!stopped && processor.process(inputProtocol, outputProtocol)) {}
src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java,272,protocolFactory = new TBinaryProtocol.Factory();
src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java,282,transportFactory = new TFramedTransport.Factory();
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftServer.java,118,private static TTransportFactory getTTransportFactory(boolean framed) {
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftServer.java,121,return new TFramedTransport.Factory();
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftServer.java,235,TTransportFactory transportFactory = getTTransportFactory(framed);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,65,private static final char ZNODE_PATH_SEPARATOR = '/';
src/main/java/org/apache/hadoop/hbase/io/encoding/FastDiffDeltaEncoder.java,392,return ByteBuffer.wrap(block.array(), pos, keyLength).slice();
src/main/java/org/apache/hadoop/hbase/io/encoding/PrefixKeyDeltaEncoder.java,157,return ByteBuffer.wrap(block.array(), pos, keyLength).slice();
src/main/java/org/apache/hadoop/hbase/mapreduce/KeyValueSortReducer.java,47,if (index > 0 && index % 100 == 0) context.setStatus("Wrote " + index);
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,53,private static final Configuration conf = HBaseConfiguration.create();
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,281,HbaseObjectWritable.writeObject(out, filter, Writable.class, conf);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,616,this.metrics.shippedOpsRate.inc(
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,617,this.currentNbOperations);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,618,this.metrics.setAgeOfLastShippedOp(
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1255,public boolean flushcache() throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1258,LOG.debug("Skipping flush on " + this + " because closing");
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1259,return false;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1266,LOG.debug("Skipping flush on " + this + " because closed");
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1267,status.abort("Skipped: closed");
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1268,return false;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1287,status.abort("Not flushing since "
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1289,: "writes not enabled"));
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1290,return false;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1294,boolean result = internalFlushcache(status);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1302,return result;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1365,protected boolean internalFlushcache(
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1374,return false;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1513,return compactionRequested;
src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java,406,boolean shouldCompact = region.flushcache();
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,399,max = Math.max(max, sf.getMaxSequenceId());
src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java,53,private List<TaskAndWeakRefPair> tasks =
src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java,54,Lists.newArrayList();
src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java,96,int size = 0;
src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java,115,size++;
src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java,119,if (size > MAX_TASKS) {
src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java,120,LOG.warn("Too many actions in action monitor! Purging some.");
src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java,121,tasks = tasks.subList(size - MAX_TASKS, size);
src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java,133,for (TaskAndWeakRefPair pair : tasks) {
src/main/java/org/apache/hadoop/hbase/mapreduce/RowCounter.java,122,String [] fields = columnName.split(":");
src/main/java/org/apache/hadoop/hbase/mapreduce/RowCounter.java,123,if(fields.length == 1) {
src/main/java/org/apache/hadoop/hbase/mapreduce/RowCounter.java,124,scan.addFamily(Bytes.toBytes(fields[0]));
src/main/java/org/apache/hadoop/hbase/mapreduce/RowCounter.java,126,scan.addColumn(Bytes.toBytes(fields[0]), Bytes.toBytes(fields[1]));
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,294,if (retainDeletesInOutput
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,296,|| kv.getMemstoreTS() > maxReadPointToTrackVersions) {
src/main/java/org/apache/hadoop/hbase/filter/NullComparator.java,41,throw new UnsupportedOperationException();
src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java,102,if (topScanner == null ||
src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java,103,this.comparator.compare(kvNext, topScanner.peek()) >= 0) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,866,boolean wasFlushing = false;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,871,wasFlushing = writestate.flushing;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,887,if (!abort && !wasFlushing && worthPreFlushing()) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,890,internalFlushcache(status);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,905,internalFlushcache(status);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1399,long flushsize = this.memstoreSize.get();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1463,this.addAndGetGlobalMemstoreSize(-flushsize);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1503,StringUtils.humanReadableInt(flushsize) + "/" + flushsize +
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1511,this.recentFlushes.add(new Pair<Long,Long>(time/1000, flushsize));
src/main/java/org/apache/hadoop/hbase/regionserver/ShutdownHook.java,194,if (!fsShutdownHooks.containsKey(hdfsClientFinalizer) &&
src/main/java/org/apache/hadoop/hbase/regionserver/ShutdownHook.java,195,!ShutdownHookManager.deleteShutdownHook(hdfsClientFinalizer)) {
src/main/java/org/apache/hadoop/hbase/regionserver/ShutdownHook.java,196,throw new RuntimeException("Failed suppression of fs shutdown hook: " +
src/main/java/org/apache/hadoop/hbase/regionserver/ShutdownHook.java,197,hdfsClientFinalizer);
src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java,224,NavigableSet<HostAndWeight> orderedHosts = new TreeSet<HostAndWeight>(
src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java,225,new HostAndWeight.WeightComparator());
src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java,226,orderedHosts.addAll(this.hostAndWeights.values());
src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java,227,List<String> topHosts = new ArrayList<String>(orderedHosts.size());
src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java,228,for(HostAndWeight haw : orderedHosts.descendingSet()) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,654,HDFSBlocksDistribution hdfsBlocksDistribution =
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,655,new HDFSBlocksDistribution();
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,79,int maxVersions, long oldestUnexpiredTS) {
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,167,return ScanQueryMatcher.MatchCode.SEEK_NEXT_COL;
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,176,this.columns = new ExplicitColumnTracker(columns,
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,177,scanInfo.getMinVersions(), maxVersions, oldestUnexpiredTS);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,71,import org.apache.hadoop.hbase.util.Bytes;
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,72,import org.apache.hadoop.hbase.util.ChecksumType;
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,73,import org.apache.hadoop.hbase.util.ClassSize;
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,74,import org.apache.hadoop.hbase.util.CollectionBackedScanner;
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,75,import org.apache.hadoop.hbase.util.EnvironmentEdgeManager;
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,76,import org.apache.hadoop.hbase.util.FSUtils;
src/main/java/org/apache/hadoop/hbase/regionserver/TimeRangeTracker.java,136,public void write(final DataOutput out) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/TimeRangeTracker.java,141,public void readFields(final DataInput in) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/TimeRangeTracker.java,147,public String toString() {
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,368,public static byte [] isLegalTableName(final byte [] tableName) {
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,378,if (Character.isLetterOrDigit(tableName[i]) || tableName[i] == '_' ||
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,379,tableName[i] == '-' || tableName[i] == '.') {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,245,if (!this.isActive()) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,246,return;
src/main/java/org/apache/hadoop/hbase/regionserver/TimeRangeTracker.java,98,private void includeTimestamp(final long timestamp) {
src/main/java/org/apache/hadoop/hbase/regionserver/TimeRangeTracker.java,117,public boolean includesTimeRange(final TimeRange tr) {
src/main/java/org/apache/hadoop/hbase/regionserver/TimeRangeTracker.java,125,public long getMinimumTimestamp() {
src/main/java/org/apache/hadoop/hbase/regionserver/TimeRangeTracker.java,132,public long getMaximumTimestamp() {
src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java,200,return new HTable(catalogTracker.getConnection().getConfiguration(), tableName);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,255,hris = MetaReader.getServerUserRegions(this.server.getCatalogTracker(),
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,256,this.serverName);
src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java,238,if (this.scanMetrics == null) {
src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java,371,try {
src/main/java/org/apache/hadoop/hbase/client/ClientScanner.java,372,writeScanMetrics();
src/main/java/org/apache/hadoop/hbase/regionserver/IncreasingToUpperBoundRegionSplitPolicy.java,43,private long flushSize;
src/main/java/org/apache/hadoop/hbase/regionserver/IncreasingToUpperBoundRegionSplitPolicy.java,88,long getSizeToCheck(final int tableRegionsCount) {
src/main/java/org/apache/hadoop/hbase/regionserver/IncreasingToUpperBoundRegionSplitPolicy.java,89,return tableRegionsCount == 0? getDesiredMaxFileSize():
src/main/java/org/apache/hadoop/hbase/regionserver/IncreasingToUpperBoundRegionSplitPolicy.java,91,this.flushSize * (tableRegionsCount * tableRegionsCount));
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,692,return this.memstoreSize.getAndAdd(memStoreSize);
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,130,boolean blockUntilBecomingActiveMaster(MonitoredTask startupStatus,
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,401,return this.activeMasterManager.blockUntilBecomingActiveMaster(startupStatus,
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,402,this.clusterStatusTracker);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,699,if (oldRequestCount == this.requestCount.get()) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,703,oldRequestCount = this.requestCount.get();
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3243,try {
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3244,LOG.info("Sleeping " + sleepBeforeRerun + "ms before re-checking after fix...");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3245,Thread.sleep(sleepBeforeRerun);
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,71,for (KeyValue kv : value.raw()) {
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,72,context.write(row, convertKv(kv, cfRenameMap));
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,115,for (KeyValue kv : result.raw()) {
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,116,kv = convertKv(kv, cfRenameMap);
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,118,if (kv.isDelete()) {
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,119,if (delete == null) {
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,120,delete = new Delete(key.get());
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,122,delete.addDeleteMarker(kv);
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,124,if (put == null) {
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,125,put = new Put(key.get());
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,127,put.add(kv);
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,130,if (put != null) {
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,131,context.write(key, put);
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,133,if (delete != null) {
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,134,context.write(key, delete);
src/main/java/org/apache/hadoop/hbase/master/HMasterCommandLine.java,140,LocalHBaseCluster cluster = new LocalHBaseCluster(conf, 1, 1,
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,334,private int webuiport = -1;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1007,private void createMyEphemeralNode() throws KeeperException {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1008,ZKUtil.createEphemeralNodeAndWatch(this.zooKeeper, getMyEphemeralNodePath(),
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1009,HConstants.EMPTY_BYTE_ARRAY);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1524,this.webuiport = putUpWebUI();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1577,return port;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,3344,this.startcode, this.webuiport);
src/main/java/org/apache/hadoop/hbase/zookeeper/RegionServerTracker.java,25,import java.util.NavigableSet;
src/main/java/org/apache/hadoop/hbase/zookeeper/RegionServerTracker.java,26,import java.util.TreeSet;
src/main/java/org/apache/hadoop/hbase/zookeeper/RegionServerTracker.java,47,private NavigableSet<ServerName> regionServers = new TreeSet<ServerName>();
src/main/java/org/apache/hadoop/hbase/zookeeper/RegionServerTracker.java,78,this.regionServers.add(sn);
src/main/java/org/apache/hadoop/hbase/zookeeper/RegionServerTracker.java,126,return new ArrayList<ServerName>(this.regionServers);
src/main/java/org/apache/hadoop/hbase/mapreduce/hadoopbackport/InputSampler.java,167,Constructor<TaskAttemptContext> c;
src/main/java/org/apache/hadoop/hbase/mapreduce/hadoopbackport/InputSampler.java,169,c = TaskAttemptContext.class.getConstructor(Configuration.class, TaskAttemptID.class);
src/main/java/org/apache/hadoop/hbase/mapreduce/hadoopbackport/InputSampler.java,174,return c.newInstance(job.getConfiguration(), new TaskAttemptID());
src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java,113,try {
src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java,114,ReplicationZookeeper zk = new ReplicationZookeeper(conn, conf,
src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java,115,conn.getZooKeeperWatcher());
src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java,116,ReplicationPeer peer = zk.getPeer(conf.get(NAME+".peerId"));
src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java,117,HTable replicatedTable = new HTable(peer.getConfiguration(),
src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java,118,conf.get(NAME+".tableName"));
src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java,119,scan.setStartRow(value.getRow());
src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java,120,replicatedScanner = replicatedTable.getScanner(scan);
src/main/java/org/apache/hadoop/hbase/mapreduce/replication/VerifyReplication.java,122,throw new IOException("Got a ZK exception", e);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,776,if(!watchAndCheckExists(zkw, znode)) {
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,778,return createEphemeralNodeAndWatch(zkw, znode, data);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,780,return false;
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,785,return true;
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,816,try {
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,817,zkw.getRecoverableZooKeeper().exists(znode, zkw);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,819,zkw.interruptedException(e);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,820,return false;
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,827,return true;
src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java,491,boolean success = svrCallable.withRetries();
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,1576,LOG.debug("Contained region dir after close and pause");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,1585,LOG.warn("HDFS region dir " + contained.getHdfsRegionDir() + " already sidelined.");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,1606,LOG.info("Moving files from " + src + " into containing region " + dst);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,1617,LOG.debug("Sideline directory contents:");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,1623,LOG.info("Sidelined region dir "+ contained.getHdfsRegionDir() + " into " +
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,1882,LOG.info("== Merging regions into one region: "
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,1900,LOG.debug("Closing region before moving data around: " +  hi);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,1901,LOG.debug("Contained region dir before close");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,1904,LOG.info("Closing region: " + hi);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,1907,LOG.warn("Was unable to close region " + hi
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,1910,LOG.warn("Was unable to close region " + hi
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,1915,LOG.info("Offlining region: " + hi);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,1918,LOG.warn("Unable to offline region from master: " + hi
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,1929,LOG.info("Created new empty container region: " +
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,1937,LOG.info("Merging " + contained  + " into " + target );
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2087,for (Collection<HbckInfo> overlap : overlapGroups.asMap().values()) {
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2088,handler.handleOverlapGroup(overlap);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,1581,FileStatus[] dirs = fs.listStatus(contained.getHdfsRegionDir());
src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java,643,if (parent.equals(rsServerNameZnode)) {
src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java,644,LOG.warn("Won't lock because this is us, we're dead!");
src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java,645,return false;
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,22,import java.util.ArrayList;
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,23,import java.util.List;
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,60,private final List<ColumnCount> columns;
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,83,this.columns = new ArrayList<ColumnCount>(columns.size());
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,180,this.column = this.columns.get(this.index);
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,202,this.column = this.columns.get(this.index);
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,250,this.column = this.columns.get(this.index);
src/main/java/org/apache/hadoop/hbase/zookeeper/MiniZooKeeperCluster.java,200,FileUtil.fullyDelete(dir);
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,271,Filter filter = (Filter)HbaseObjectWritable.readObject(in, conf);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2079,this.regions.remove(region);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1149,long now = System.currentTimeMillis();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2362,matches = compareResult <= 0;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2365,matches = compareResult < 0;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2374,matches = compareResult > 0;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2377,matches = compareResult >= 0;
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,266,OutputStream outStream = NetUtils.getOutputStream(socket);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2639,new EnableTableHandler(this.master, tableName.getBytes(),
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2640,catalogTracker, this, true).process();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,141,import org.apache.hadoop.hbase.security.User;
src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java,365,this.out = new DataOutputStream
src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java,366,(new BufferedOutputStream(NetUtils.getOutputStream(socket)));
src/main/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java,663,throw new IOException("Can't find class " + className, e);
src/main/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java,674,throw new IOException("Can't find class " + className, e);
src/main/java/org/apache/hadoop/hbase/io/HbaseObjectWritable.java,704,throw new IOException("Class not found when attempting to " +
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/SchemaConfigured.java,129,cfName = cfName.intern();
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/SchemaConfigured.java,131,tableName = splits[splits.length - 4].intern();
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/SchemaConfigured.java,158,this.tableName = tableName != null ? tableName.intern() : tableName;
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/SchemaConfigured.java,159,this.cfName = cfName != null ? cfName.intern() : cfName;
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/SchemaConfigured.java,163,tableName = that.getTableName().intern();
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/SchemaConfigured.java,164,cfName = that.getColumnFamilyName().intern();
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/SchemaConfigured.java,215,target.tableName = tableName.intern();
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/SchemaConfigured.java,216,target.cfName = cfName.intern();
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/SchemaMetrics.java,287,blockMetricNames[i] = sb.toString().intern();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3324,private Filter filter;
src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java,248,c = query.charAt(i);
src/main/java/org/apache/hadoop/hbase/rest/RowSpec.java,260,c = query.charAt(i);
src/main/java/org/apache/hadoop/hbase/client/Scan.java,268,this.startRow = startRow;
src/main/java/org/apache/hadoop/hbase/client/Scan.java,279,this.stopRow = stopRow;
src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java,75,synchronized (this) {
src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java,76,tasks.add(pair);
src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java,89,synchronized (this) {
src/main/java/org/apache/hadoop/hbase/monitoring/TaskMonitor.java,90,tasks.add(pair);
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,264,public synchronized KeyValue peek() {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,278,public synchronized void close() {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,291,public synchronized boolean seek(KeyValue key) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,309,public synchronized boolean next(List<KeyValue> outResult, int limit) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,435,public synchronized boolean next(List<KeyValue> outResult) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,441,public synchronized void updateReaders() throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,515,public synchronized boolean reseek(KeyValue kv) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2028,boolean walSyncSuccessful = false;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2222,walSyncSuccessful = true;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2255,if (!walSyncSuccessful) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1751,this.replicationSourceHandler.stopReplicationService();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1753,this.replicationSinkHandler.stopReplicationService();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1530,this.replicationSourceHandler.startReplicationService();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1532,this.replicationSinkHandler.startReplicationService();
src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java,259,if (TsvParser.ROWKEY_COLUMN_SPEC.equals(aColumn)) continue;
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,71,import org.apache.hadoop.hbase.security.User;
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,953,synchronized (regionLockObject) {
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,965,if (useCache) {
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,966,location = getCachedLocation(tableName, row);
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,967,if (location != null) {
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,968,return location;
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,975,regionInfoRow = server.getClosestRowBefore(
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,976,metaLocation.getRegionInfo().getRegionName(), metaKey,
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,977,HConstants.CATALOG_FAMILY);
src/main/java/org/apache/hadoop/hbase/client/Append.java,26,import java.util.Map;
src/main/java/org/apache/hadoop/hbase/client/Append.java,30,import org.apache.hadoop.io.Writable;
src/main/java/org/apache/hadoop/hbase/client/Append.java,108,int numFamilies = in.readInt();
src/main/java/org/apache/hadoop/hbase/client/Append.java,110,for(int i=0;i<numFamilies;i++) {
src/main/java/org/apache/hadoop/hbase/client/Append.java,111,byte [] family = Bytes.readByteArray(in);
src/main/java/org/apache/hadoop/hbase/client/Append.java,112,int numKeys = in.readInt();
src/main/java/org/apache/hadoop/hbase/client/Append.java,113,List<KeyValue> keys = new ArrayList<KeyValue>(numKeys);
src/main/java/org/apache/hadoop/hbase/client/Append.java,114,int totalLen = in.readInt();
src/main/java/org/apache/hadoop/hbase/client/Append.java,115,byte [] buf = new byte[totalLen];
src/main/java/org/apache/hadoop/hbase/client/Append.java,116,int offset = 0;
src/main/java/org/apache/hadoop/hbase/client/Append.java,117,for (int j = 0; j < numKeys; j++) {
src/main/java/org/apache/hadoop/hbase/client/Append.java,118,int keyLength = in.readInt();
src/main/java/org/apache/hadoop/hbase/client/Append.java,119,in.readFully(buf, offset, keyLength);
src/main/java/org/apache/hadoop/hbase/client/Append.java,120,keys.add(new KeyValue(buf, offset, keyLength));
src/main/java/org/apache/hadoop/hbase/client/Append.java,121,offset += keyLength;
src/main/java/org/apache/hadoop/hbase/client/Append.java,123,this.familyMap.put(family, keys);
src/main/java/org/apache/hadoop/hbase/client/Append.java,136,out.writeInt(familyMap.size());
src/main/java/org/apache/hadoop/hbase/client/Append.java,137,for (Map.Entry<byte [], List<KeyValue>> entry : familyMap.entrySet()) {
src/main/java/org/apache/hadoop/hbase/client/Append.java,138,Bytes.writeByteArray(out, entry.getKey());
src/main/java/org/apache/hadoop/hbase/client/Append.java,139,List<KeyValue> keys = entry.getValue();
src/main/java/org/apache/hadoop/hbase/client/Append.java,140,out.writeInt(keys.size());
src/main/java/org/apache/hadoop/hbase/client/Append.java,141,int totalLen = 0;
src/main/java/org/apache/hadoop/hbase/client/Append.java,142,for(KeyValue kv : keys) {
src/main/java/org/apache/hadoop/hbase/client/Append.java,143,totalLen += kv.getLength();
src/main/java/org/apache/hadoop/hbase/client/Append.java,145,out.writeInt(totalLen);
src/main/java/org/apache/hadoop/hbase/client/Append.java,146,for(KeyValue kv : keys) {
src/main/java/org/apache/hadoop/hbase/client/Append.java,147,out.writeInt(kv.getLength());
src/main/java/org/apache/hadoop/hbase/client/Append.java,148,out.write(kv.getBuffer(), kv.getOffset(), kv.getLength());
src/main/java/org/apache/hadoop/hbase/client/Delete.java,129,if (!kv.isDelete()) {
src/main/java/org/apache/hadoop/hbase/client/Delete.java,133,if (Bytes.compareTo(this.row, 0, row.length, kv.getBuffer(),
src/main/java/org/apache/hadoop/hbase/client/Delete.java,134,kv.getRowOffset(), kv.getRowLength()) != 0) {
src/main/java/org/apache/hadoop/hbase/client/Delete.java,136,+ Bytes.toStringBinary(kv.getBuffer(), kv.getRowOffset(),
src/main/java/org/apache/hadoop/hbase/client/Delete.java,137,kv.getRowLength()) + " doesn't match the original one "
src/main/java/org/apache/hadoop/hbase/client/Delete.java,278,int numFamilies = in.readInt();
src/main/java/org/apache/hadoop/hbase/client/Delete.java,279,for(int i=0;i<numFamilies;i++) {
src/main/java/org/apache/hadoop/hbase/client/Delete.java,280,byte [] family = Bytes.readByteArray(in);
src/main/java/org/apache/hadoop/hbase/client/Delete.java,281,int numColumns = in.readInt();
src/main/java/org/apache/hadoop/hbase/client/Delete.java,282,List<KeyValue> list = new ArrayList<KeyValue>(numColumns);
src/main/java/org/apache/hadoop/hbase/client/Delete.java,283,for(int j=0;j<numColumns;j++) {
src/main/java/org/apache/hadoop/hbase/client/Delete.java,284,KeyValue kv = new KeyValue();
src/main/java/org/apache/hadoop/hbase/client/Delete.java,285,kv.readFields(in);
src/main/java/org/apache/hadoop/hbase/client/Delete.java,286,list.add(kv);
src/main/java/org/apache/hadoop/hbase/client/Delete.java,288,this.familyMap.put(family, list);
src/main/java/org/apache/hadoop/hbase/client/Delete.java,301,out.writeInt(familyMap.size());
src/main/java/org/apache/hadoop/hbase/client/Delete.java,302,for(Map.Entry<byte [], List<KeyValue>> entry : familyMap.entrySet()) {
src/main/java/org/apache/hadoop/hbase/client/Delete.java,303,Bytes.writeByteArray(out, entry.getKey());
src/main/java/org/apache/hadoop/hbase/client/Delete.java,304,List<KeyValue> list = entry.getValue();
src/main/java/org/apache/hadoop/hbase/client/Delete.java,305,out.writeInt(list.size());
src/main/java/org/apache/hadoop/hbase/client/Delete.java,306,for(KeyValue kv : list) {
src/main/java/org/apache/hadoop/hbase/client/Delete.java,307,kv.write(out);
src/main/java/org/apache/hadoop/hbase/client/Put.java,157,int res = Bytes.compareTo(this.row, 0, row.length,
src/main/java/org/apache/hadoop/hbase/client/Put.java,158,kv.getBuffer(), kv.getRowOffset(), kv.getRowLength());
src/main/java/org/apache/hadoop/hbase/client/Put.java,159,if(res != 0) {
src/main/java/org/apache/hadoop/hbase/client/Put.java,160,throw new IOException("The row in the recently added KeyValue " +
src/main/java/org/apache/hadoop/hbase/client/Put.java,161,Bytes.toStringBinary(kv.getBuffer(), kv.getRowOffset(),
src/main/java/org/apache/hadoop/hbase/client/Put.java,162,kv.getRowLength()) + " doesn't match the original one " +
src/main/java/org/apache/hadoop/hbase/client/Put.java,163,Bytes.toStringBinary(this.row));
src/main/java/org/apache/hadoop/hbase/client/Put.java,177,return  new KeyValue(this.row, family, qualifier, ts, KeyValue.Type.Put,
src/main/java/org/apache/hadoop/hbase/client/Put.java,375,int numFamilies = in.readInt();
src/main/java/org/apache/hadoop/hbase/client/Put.java,377,for(int i=0;i<numFamilies;i++) {
src/main/java/org/apache/hadoop/hbase/client/Put.java,378,byte [] family = Bytes.readByteArray(in);
src/main/java/org/apache/hadoop/hbase/client/Put.java,379,int numKeys = in.readInt();
src/main/java/org/apache/hadoop/hbase/client/Put.java,380,List<KeyValue> keys = new ArrayList<KeyValue>(numKeys);
src/main/java/org/apache/hadoop/hbase/client/Put.java,381,int totalLen = in.readInt();
src/main/java/org/apache/hadoop/hbase/client/Put.java,382,byte [] buf = new byte[totalLen];
src/main/java/org/apache/hadoop/hbase/client/Put.java,383,int offset = 0;
src/main/java/org/apache/hadoop/hbase/client/Put.java,384,for (int j = 0; j < numKeys; j++) {
src/main/java/org/apache/hadoop/hbase/client/Put.java,385,int keyLength = in.readInt();
src/main/java/org/apache/hadoop/hbase/client/Put.java,386,in.readFully(buf, offset, keyLength);
src/main/java/org/apache/hadoop/hbase/client/Put.java,387,keys.add(new KeyValue(buf, offset, keyLength));
src/main/java/org/apache/hadoop/hbase/client/Put.java,388,offset += keyLength;
src/main/java/org/apache/hadoop/hbase/client/Put.java,390,this.familyMap.put(family, keys);
src/main/java/org/apache/hadoop/hbase/client/Put.java,404,out.writeInt(familyMap.size());
src/main/java/org/apache/hadoop/hbase/client/Put.java,405,for (Map.Entry<byte [], List<KeyValue>> entry : familyMap.entrySet()) {
src/main/java/org/apache/hadoop/hbase/client/Put.java,406,Bytes.writeByteArray(out, entry.getKey());
src/main/java/org/apache/hadoop/hbase/client/Put.java,407,List<KeyValue> keys = entry.getValue();
src/main/java/org/apache/hadoop/hbase/client/Put.java,408,out.writeInt(keys.size());
src/main/java/org/apache/hadoop/hbase/client/Put.java,409,int totalLen = 0;
src/main/java/org/apache/hadoop/hbase/client/Put.java,410,for(KeyValue kv : keys) {
src/main/java/org/apache/hadoop/hbase/client/Put.java,411,totalLen += kv.getLength();
src/main/java/org/apache/hadoop/hbase/client/Put.java,413,out.writeInt(totalLen);
src/main/java/org/apache/hadoop/hbase/client/Put.java,414,for(KeyValue kv : keys) {
src/main/java/org/apache/hadoop/hbase/client/Put.java,415,out.writeInt(kv.getLength());
src/main/java/org/apache/hadoop/hbase/client/Put.java,416,out.write(kv.getBuffer(), kv.getOffset(), kv.getLength());
src/main/java/org/apache/hadoop/hbase/KeyValue.java,67,public class KeyValue implements Writable, HeapSize {
src/main/java/org/apache/hadoop/hbase/KeyValue.java,822,private int keyLength = 0;
src/main/java/org/apache/hadoop/hbase/KeyValue.java,825,if (keyLength == 0) {
src/main/java/org/apache/hadoop/hbase/KeyValue.java,826,keyLength = Bytes.toInt(this.bytes, this.offset);
src/main/java/org/apache/hadoop/hbase/KeyValue.java,828,return keyLength;
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,219,int initialOffset = offset;
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,254,int qualLength = keyLength + KeyValue.ROW_OFFSET -
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,257,long timestamp = kv.getTimestamp();
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,276,byte type = kv.getType();
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,74,private final ArrayList<KeyValue> kvs = new ArrayList<KeyValue>();
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,98,public List<KeyValue> getKeyValues() {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,175,long ret = 0;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,27,import java.util.Arrays;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,82,private HLog.Entry[] entriesArray;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,132,private int currentNbEntries = 0;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,164,this.entriesArray = new HLog.Entry[this.replicationQueueNbCapacity];
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,165,for (int i = 0; i < this.replicationQueueNbCapacity; i++) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,166,this.entriesArray[i] = new HLog.Entry();
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,301,currentNbEntries = 0;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,323,LOG.warn(peerClusterZnode + " Got EOF while reading, " +
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,325,considerDumping = true;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,326,currentNbEntries = 0;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,353,if (this.isActive() && (gotIOE || currentNbEntries == 0)) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,413,currentNbEntries++;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,421,currentNbEntries >= this.replicationQueueNbCapacity) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,560,List<KeyValue> kvs = edit.getKeyValues();
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,561,for (int i = edit.size()-1; i >= 0; i--) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,594,if (this.currentNbEntries == 0) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,607,LOG.debug("Replicating " + currentNbEntries);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,608,rrs.replicateLogEntries(Arrays.copyOf(this.entriesArray, currentNbEntries));
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,614,this.totalReplicatedEdits += currentNbEntries;
src/main/java/org/apache/hadoop/hbase/master/ServerManager.java,211,long skew = System.currentTimeMillis() - serverCurrentTime;
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,219,public static void initCredentials(Job job) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,971,createMyEphemeralNode();
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1787,if (this.master == null) {
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1788,this.master = getMaster();
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1790,HTableDescriptor[] htd = master.getHTableDescriptors();
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1797,if (this.master == null) {
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1798,this.master = getMaster();
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1800,return master.getHTableDescriptors(tableNames);
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1812,if (this.master == null) {
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1813,this.master = getMaster();
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,481,ByteBuffer bb = getKey();
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,483,length, bb.array(), bb.arrayOffset(), bb.limit());
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,183,if (isEnablingTable(tableName)) {
src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java,528,switch (trailer.getMajorVersion()) {
src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java,529,case 1:
src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java,530,return new HFileReaderV1(path, trailer, fsdis, size, closeIStream,
src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java,531,cacheConf);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java,532,case 2:
src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java,533,return new HFileReaderV2(path, trailer, fsdis, fsdisNoFsChecksum,
src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java,534,size, closeIStream,
src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java,535,cacheConf, preferredEncodingInCache, hfs);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java,536,default:
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4330,if (!walSyncSuccessful) {
src/main/java/org/apache/hadoop/hbase/filter/PrefixFilter.java,23,import org.apache.hadoop.hbase.KeyValue;
src/main/java/org/apache/hadoop/hbase/filter/PrefixFilter.java,24,import org.apache.hadoop.hbase.util.Bytes;
src/main/java/org/apache/hadoop/hbase/filter/PrefixFilter.java,28,import java.io.DataInput;
src/main/java/org/apache/hadoop/hbase/filter/PrefixFilter.java,29,import java.util.List;
src/main/java/org/apache/hadoop/hbase/filter/PrefixFilter.java,66,return cmp != 0;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4862,if (LOG.isDebugEnabled()) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4863,LOG.debug("Received dynamic protocol exec call with protocolName " + protocolName);
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,641,new MemStoreScanner());
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,711,MemStoreScanner() {
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,719,long readPoint = MultiVersionConsistencyControl.getThreadReadPoint();
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,1287,isCompaction), !isCompaction);
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java,68,public StoreFileScanner(StoreFile.Reader reader, HFileScanner hfs, boolean useMVCC) {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java,132,skipKVsNewerThanReadpoint();
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java,153,return skipKVsNewerThanReadpoint();
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFileScanner.java,173,return skipKVsNewerThanReadpoint();
src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java,93,private static final int  TABLE_CREATE_MAX_RETRIES = 20;
src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java,94,private static final long TABLE_CREATE_SLEEP = 60000;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,535,long storeSeqId = store.getMaxSequenceId();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,539,if (maxSeqId == -1 || storeSeqId > maxSeqId) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,540,maxSeqId = storeSeqId;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2655,loaded = region.bulkLoadHFiles(familyPaths);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,297,long getMaxSequenceId() {
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,298,return StoreFile.getMaxSequenceIdInList(this.getStorefiles());
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,535,void bulkLoadHFile(String srcPathStr) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,549,Path dstPath = StoreFile.getRandomFilename(fs, homedir);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,986,long maxId = StoreFile.getMaxSequenceIdInList(filesToCompact);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1047,maxId = StoreFile.getMaxSequenceIdInList(filesToCompact);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1050,Collections.sort(filesCompacting, StoreFile.Comparators.FLUSH_TIME);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1267,Collections.sort(filesCompacting, StoreFile.Comparators.FLUSH_TIME);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1742,Collections.sort(storeFiles, StoreFile.Comparators.FLUSH_TIME);
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,395,public static long getMaxSequenceIdInList(Collection<StoreFile> sfs) {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,398,if (!sf.isBulkLoadResult()) {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,1721,static final Comparator<StoreFile> FLUSH_TIME =
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,1723,Ordering.natural().onResultOf(new GetBulkTime()),
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,1728,private static class GetBulkTime implements Function<StoreFile, Long> {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,1731,if (!sf.isBulkLoadResult()) return Long.MAX_VALUE;
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,1732,return sf.getBulkLoadTimestamp();
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,1735,private static class GetSeqId implements Function<StoreFile, Long> {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,1738,if (sf.isBulkLoadResult()) return -1L;
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,1739,return sf.getMaxSequenceId();
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,1490,private long obtainSeqNum() {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4025,if (srcFiles.size() == 2) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4026,long seqA = srcFiles.get(0).getMaxSequenceId();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4027,long seqB = srcFiles.get(1).getMaxSequenceId();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4028,if (seqA == seqB) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4032,throw new IOException("Files have same sequenceid: " + seqA);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFilePrettyPrinter.java,340,System.out.println("Mid-key: " + Bytes.toStringBinary(reader.midkey()));
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,843,TaskBatch batch;
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,1110,LOG.debug(path +
src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueSkipListSet.java,46,class KeyValueSkipListSet implements NavigableSet<KeyValue> {
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,879,ClassSize.COPYONWRITE_ARRAYSET + ClassSize.COPYONWRITE_ARRAYLIST +
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,880,(2 * ClassSize.CONCURRENT_SKIPLISTMAP));
src/main/java/org/apache/hadoop/hbase/client/HTable.java,166,this.pool = new ThreadPoolExecutor(1, maxThreads,
src/main/java/org/apache/hadoop/hbase/client/HTable.java,170,((ThreadPoolExecutor)this.pool).allowCoreThreadTimeOut(true);
src/main/java/org/apache/hadoop/hbase/client/HTable.java,172,this.finishSetup();
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,365,public HTableWrapper(byte[] tableName) throws IOException {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,367,this.table = new HTable(conf, tableName);
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,372,table.close();
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,639,return new HTableWrapper(tableName);
src/main/java/org/apache/hadoop/hbase/regionserver/ColumnTracker.java,58,public ScanQueryMatcher.MatchCode checkColumn(byte[] bytes, int offset,
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,107,int length, long timestamp, byte type, boolean ignoreCount) {
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,129,if (ignoreCount) return ScanQueryMatcher.MatchCode.INCLUDE;
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,132,if (sameAsPreviousTS(timestamp)) {
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,134,return ScanQueryMatcher.MatchCode.SKIP;
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,136,int count = this.column.increment();
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,137,if(count >= maxVersions || (count >= minVersions && isExpired(timestamp))) {
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,145,resetTS();
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,148,this.column = null;
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,149,return ScanQueryMatcher.MatchCode.INCLUDE_AND_SEEK_NEXT_ROW;
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,153,this.column = this.columns.get(this.index);
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,154,return ScanQueryMatcher.MatchCode.INCLUDE_AND_SEEK_NEXT_COL;
src/main/java/org/apache/hadoop/hbase/regionserver/ExplicitColumnTracker.java,157,setTS(timestamp);
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,342,if (filter != null) {
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,344,if (filterResponse == ReturnCode.SKIP) {
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,345,return MatchCode.SKIP;
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,347,return columns.getNextRowOrNextColumn(bytes, offset, qualLength);
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,349,stickyNextRow = true;
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,350,return MatchCode.SEEK_NEXT_ROW;
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,352,return MatchCode.SEEK_NEXT_USING_HINT;
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,356,MatchCode colChecker = columns.checkColumn(bytes, offset, qualLength,
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,357,timestamp, type, kv.getMemstoreTS() > maxReadPointToTrackVersions);
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,363,if (colChecker == MatchCode.SEEK_NEXT_ROW) {
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,364,stickyNextRow = true;
src/main/java/org/apache/hadoop/hbase/regionserver/ScanWildcardColumnTracker.java,68,public MatchCode checkColumn(byte[] bytes, int offset, int length,
src/main/java/org/apache/hadoop/hbase/regionserver/ScanWildcardColumnTracker.java,69,long timestamp, byte type, boolean ignoreCount) throws IOException {
src/main/java/org/apache/hadoop/hbase/io/ImmutableBytesWritable.java,69,this(ibw.get(), 0, ibw.getSize());
src/main/java/org/apache/hadoop/hbase/filter/RegexStringComparator.java,96,return pattern.matcher(new String(value, offset, length, charset)).find() ? 0
src/main/java/org/apache/hadoop/hbase/filter/RegexStringComparator.java,97,: 1;
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,75,private NavigableMap<byte[], Integer> scopes;
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,102,public NavigableMap<byte[], Integer> getScopes() {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,103,return scopes;
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,107,public void setScopes (NavigableMap<byte[], Integer> scopes) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,110,this.scopes = scopes;
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,115,if (scopes != null) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,116,scopes.clear();
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,131,int numFamilies = in.readInt();
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,132,if (numFamilies > 0) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,133,if (scopes == null) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,134,scopes = new TreeMap<byte[], Integer>(Bytes.BYTES_COMPARATOR);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,136,for (int i = 0; i < numFamilies; i++) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,137,byte[] fam = Bytes.readByteArray(in);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,139,scopes.put(fam, scope);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,163,if (scopes == null) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,164,out.writeInt(0);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,166,out.writeInt(scopes.size());
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,167,for (byte[] key : scopes.keySet()) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,168,Bytes.writeByteArray(out, key);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,169,out.writeInt(scopes.get(key));
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,179,if (scopes != null) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,180,ret += ClassSize.TREEMAP;
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,181,ret += ClassSize.align(scopes.size() * ClassSize.MAP_ENTRY);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,195,if (scopes != null) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/WALEdit.java,196,sb.append(" scopes: " + scopes.toString());
src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java,172,NavigableMap<byte[], Integer> scopes =
src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java,173,new TreeMap<byte[], Integer>(Bytes.BYTES_COMPARATOR);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java,179,!scopes.containsKey(family)) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java,180,scopes.put(family, scope);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java,183,if (!scopes.isEmpty()) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java,184,logEdit.setScopes(scopes);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,31,import java.util.NavigableMap;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,59,import org.apache.hadoop.hbase.zookeeper.ClusterId;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,398,if (!logKey.getClusterId().equals(peerClusterId)) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,559,NavigableMap<byte[], Integer> scopes = edit.getScopes();
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,565,if (scopes == null || !scopes.containsKey(kv.getFamily())) {
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureServer.java,88,public static final Set<Byte> INSECURE_VERSIONS = ImmutableSet.of((byte) 3);
src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java,168,MetaEditor.addRegionsToMeta(this.catalogTracker, regionInfos);
src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java,111,HBaseRPC.setRpcTimeout(this.callTimeout);
src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java,112,this.startTime = System.currentTimeMillis();
src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java,117,this.endTime = System.currentTimeMillis();
src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java,120,public void shouldRetry(Throwable throwable) throws IOException {
src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java,122,if (throwable instanceof SocketTimeoutException
src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java,123,|| (this.endTime - this.startTime > this.callTimeout)) {
src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java,124,throw (SocketTimeoutException) (SocketTimeoutException) new SocketTimeoutException(
src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java,128,.initCause(throwable);
src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java,130,this.callTimeout = ((int) (this.endTime - this.startTime));
src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java,165,shouldRetry(t);
src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java,180,System.currentTimeMillis(), toString());
src/main/java/org/apache/hadoop/hbase/client/ServerCallable.java,189,Thread.sleep(ConnectionUtils.getPauseTime(pause, tries));
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,754,if (info != null) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,25,import java.lang.Thread.UncaughtExceptionHandler;
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2368,static class GeneralBulkAssigner extends StartupBulkAssigner {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2369,GeneralBulkAssigner(final Server server,
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2370,final Map<ServerName, List<HRegionInfo>> bulkPlan,
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2371,final AssignmentManager am) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2372,super(server, bulkPlan, am);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2376,protected UncaughtExceptionHandler getUncaughtExceptionHandler() {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2377,return new UncaughtExceptionHandler() {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2379,public void uncaughtException(Thread t, Throwable e) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2380,LOG.warn("Assigning regions in " + t.getName(), e);
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,701,void fixupDaughters(final MonitoredTask status) throws IOException {
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,725,fixups += ServerShutdownHandler.fixupDaughters(
src/main/java/org/apache/hadoop/hbase/regionserver/IncreasingToUpperBoundRegionSplitPolicy.java,77,break;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,99,Map<byte[], List<Row>> rows = new TreeMap<byte[], List<Row>>(Bytes.BYTES_COMPARATOR);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,111,del.setClusterId(entry.getKey().getClusterId());
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,112,addToMultiMap(rows, table, del);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,115,put.setClusterId(entry.getKey().getClusterId());
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,116,addToMultiMap(rows, table, put);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,128,for(byte [] table : rows.keySet()) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,129,batch(table, rows.get(table));
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,149,private <K, V> List<V> addToMultiMap(Map<K, List<V>> map, K key, V value) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,150,List<V> values = map.get(key);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,153,map.put(key, values);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,165,private void batch(byte[] tableName, List<Row> rows) throws IOException {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,166,if (rows.isEmpty()) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,172,table.batch(rows);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSink.java,173,this.metrics.appliedOpsRate.inc(rows.size());
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,217,switch (filter.filterKeyValue(v)) {
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,290,if (curKeyHint == null && operator == Operator.MUST_PASS_ONE) {
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,301,if (operator == Operator.MUST_PASS_ALL &&
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,302,KeyValue.COMPARATOR.compare(keyHint, curKeyHint) < 0) {
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,304,keyHint = curKeyHint;
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,306,KeyValue.COMPARATOR.compare(keyHint, curKeyHint) > 0) {
src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat.java,96,final int blocksize = conf.getInt("hbase.mapreduce.hfileoutputformat.blocksize",
src/main/java/org/apache/hadoop/hbase/mapreduce/HFileOutputFormat.java,97,HFile.DEFAULT_BLOCKSIZE);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,156,private final CopyOnWriteArraySet<ChangedReadersObserver> changedReaderObservers =
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,157,new CopyOnWriteArraySet<ChangedReadersObserver>();
src/main/java/org/apache/hadoop/hbase/client/Append.java,91,list.add(new KeyValue(
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,24,import java.lang.reflect.InvocationTargetException;
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,29,import org.apache.hadoop.fs.FSDataOutputStream;
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,32,import org.apache.hadoop.hbase.RemoteExceptionHandler;
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,34,import org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException;
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,52,public static final long LEASE_SOFTLIMIT_PERIOD = 60 * 1000;
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,56,throws IOException{
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,63,if (!(fs instanceof DistributedFileSystem)) {
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,64,return;
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,66,LOG.info("Recovering file " + p);
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,67,long startWaiting = System.currentTimeMillis();
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,71,while (!recovered) {
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,73,try {
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,87,FSDataOutputStream out = fs.append(p);
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,88,out.close();
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,92,e = RemoteExceptionHandler.checkIOException(e);
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,93,if (e instanceof AlreadyBeingCreatedException) {
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,98,long waitedFor = System.currentTimeMillis() - startWaiting;
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,99,if (waitedFor > LEASE_SOFTLIMIT_PERIOD) {
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,100,LOG.warn("Waited " + waitedFor + "ms for lease recovery on " + p +
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,104,e.getMessage().contains("File does not exist")) {
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,106,throw new FileNotFoundException(
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,109,throw new IOException("Failed to open " + p + " for append", e);
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,112,try {
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,173,boolean roundRobinAssignment = this.server.getConfiguration().getBoolean(
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,177,for (HRegionInfo region : regions) {
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,178,if (assignmentManager.isRegionInTransition(region) != null) {
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,179,continue;
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,181,final HRegionInfo hri = region;
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,182,pool.execute(new Runnable() {
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,183,public void run() {
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,189,try {
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,190,assignmentManager.assignUserRegionsToOnlineServers(regions);
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,192,LOG.warn("Assignment was interrupted");
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,193,Thread.currentThread().interrupt();
src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java,905,call.wait();                           // wait for the result
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,1643,if (this.connection != null) {
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,768,MetaScanner.metaScan(conf, visitor);
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,903,MetaScanner.metaScan(conf, visitor, tableName, row,
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,904,this.prefetchRegionLimit);
src/main/java/org/apache/hadoop/hbase/client/HTable.java,28,import java.util.Arrays;
src/main/java/org/apache/hadoop/hbase/client/HTable.java,35,import java.util.concurrent.ThreadFactory;
src/main/java/org/apache/hadoop/hbase/client/HTable.java,38,import java.util.concurrent.atomic.AtomicInteger;
src/main/java/org/apache/hadoop/hbase/client/HTable.java,212,if (pool == null || pool.isShutdown()) {
src/main/java/org/apache/hadoop/hbase/client/HTable.java,213,throw new IllegalArgumentException("Pool is null or shut down.");
src/main/java/org/apache/hadoop/hbase/client/HTable.java,483,return MetaScanner.allTableRegions(getConfiguration(), getTableName(), false);
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,38,import org.apache.hadoop.hbase.client.HConnectionManager.HConnectable;
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,67,metaScan(configuration, visitor, null);
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,80,public static void metaScan(Configuration configuration,
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,83,metaScan(configuration, visitor, userTableName, null, Integer.MAX_VALUE);
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,105,metaScan(configuration, visitor, userTableName, row, rowLimit,
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,125,public static void metaScan(Configuration configuration,
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,126,final MetaScannerVisitor visitor, final byte[] tableName,
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,128,throws IOException {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,129,try {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,130,HConnectionManager.execute(new HConnectable<Void>(configuration) {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,132,public Void connect(HConnection connection) throws IOException {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,133,metaScan(conf, connection, visitor, tableName, row, rowLimit,
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,134,metaTableName);
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,135,return null;
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,139,visitor.close();
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,143,private static void metaScan(Configuration configuration, HConnection connection,
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,147,int rowUpperLimit = rowLimit > 0 ? rowLimit: Integer.MAX_VALUE;
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,151,byte[] startRow;
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,152,if (row != null) {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,154,assert tableName != null;
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,155,byte[] searchRow =
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,156,HRegionInfo.createRegionName(tableName, row, HConstants.NINES,
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,157,false);
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,158,HTable metaTable = null;
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,159,try {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,160,metaTable = new HTable(configuration, HConstants.META_TABLE_NAME);
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,179,if (metaTable != null) {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,180,metaTable.close();
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,185,startRow = HConstants.EMPTY_START_ROW;
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,188,startRow = HRegionInfo.createRegionName(
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,189,tableName, HConstants.EMPTY_START_ROW, HConstants.ZEROES, false);
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,193,ScannerCallable callable;
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,194,int rows = Math.min(rowLimit, configuration.getInt(
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,195,HConstants.HBASE_META_SCANNER_CACHING,
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,196,HConstants.DEFAULT_HBASE_META_SCANNER_CACHING));
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,197,do {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,198,final Scan scan = new Scan(startRow).addFamily(HConstants.CATALOG_FAMILY);
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,199,if (LOG.isDebugEnabled()) {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,200,LOG.debug("Scanning " + Bytes.toString(metaTableName) +
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,202,rowUpperLimit + " rows using " + connection.toString());
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,204,callable = new ScannerCallable(connection, metaTableName, scan, null);
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,206,callable.withRetries();
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,208,int processedRows = 0;
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,209,try {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,210,callable.setCaching(rows);
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,211,done: do {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,212,if (processedRows >= rowUpperLimit) {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,213,break;
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,216,Result [] rrs = callable.withRetries();
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,217,if (rrs == null || rrs.length == 0 || rrs[0].size() == 0) {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,218,break; //exit completely
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,220,for (Result rr : rrs) {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,222,break done;
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,225,break done; //exit completely
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,226,processedRows++;
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,231,startRow = callable.getHRegionInfo().getEndKey();
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,234,callable.setClose();
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,235,callable.withRetries();
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,247,throws IOException {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,248,return listAllRegions(conf, true);
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,294,final byte [] tablename, final boolean offlined) throws IOException {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,323,metaScan(conf, visitor, tablename);
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1159,if (e.getValue().getHostnamePort().equals(server)) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1803,LOG.info("Attempting connect to Master server at " +
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1804,this.masterAddressManager.getMasterAddress());
src/main/java/org/apache/hadoop/hbase/mapreduce/TableRecordReaderImpl.java,49,static final Log LOG = LogFactory.getLog(TableRecordReader.class);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2348,if (s == null) throw new UnknownScannerException("Name: " + scannerName);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2366,lease = this.leases.removeLease(scannerName);
src/main/java/org/apache/hadoop/hbase/thrift/ThriftServer.java,160,conf.setBoolean(
src/main/java/org/apache/hadoop/hbase/thrift/ThriftServer.java,161,ThriftServerRunner.COMPACT_CONF_KEY, cmd.hasOption(COMPACT_OPTION));
src/main/java/org/apache/hadoop/hbase/thrift/ThriftServer.java,162,conf.setBoolean(
src/main/java/org/apache/hadoop/hbase/thrift/ThriftServer.java,163,ThriftServerRunner.FRAMED_CONF_KEY, cmd.hasOption(FRAMED_OPTION));
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,1515,if (!this.isActiveMaster) {
src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java,94,if (i.hasNext()) {
src/main/java/org/apache/hadoop/hbase/rest/client/RemoteHTable.java,95,sb.append('/');
src/main/java/org/apache/hadoop/hbase/filter/DependentColumnFilter.java,248,Bytes.toStringBinary(this.comparator.getValue()));
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,163,LOG.warn("Unknown check value: " + check + ", ignored");
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,77,private final Map<Integer, ResultScanner> scannerMap = new ConcurrentHashMap<Integer, ResultScanner>();
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,79,public static THBaseService.Iface newInstance(
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,80,Configuration conf, ThriftMetrics metrics) {
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,82,return (THBaseService.Iface) Proxy.newProxyInstance(
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,83,handler.getClass().getClassLoader(),
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,84,new Class[]{THBaseService.Iface.class},
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,85,new THBaseServiceMetricsProxy(handler, metrics));
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,92,private THBaseServiceMetricsProxy(
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,93,THBaseService.Iface handler, ThriftMetrics metrics) {
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,100,throws Throwable {
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,105,int processTime = (int)(now() - start);
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,110,throw new RuntimeException(
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,125,private HTableInterface getTable(byte[] tableName) {
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,126,return htablePool.getTable(tableName);
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,177,HTableInterface htable = getTable(table.array());
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,189,HTableInterface htable = getTable(table.array());
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,201,HTableInterface htable = getTable(table.array());
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,213,HTableInterface htable = getTable(table.array());
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,225,throws TIOError, TException {
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,226,HTableInterface htable = getTable(table.array());
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,228,return htable.checkAndPut(row.array(), family.array(), qualifier.array(), (value == null) ? null : value.array(), putFromThrift(put));
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,238,HTableInterface htable = getTable(table.array());
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,250,HTableInterface htable = getTable(table.array());
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,261,public List<TDelete> deleteMultiple(ByteBuffer table, List<TDelete> deletes) throws TIOError, TException {
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,262,HTableInterface htable = getTable(table.array());
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,275,public boolean checkAndDelete(ByteBuffer table, ByteBuffer row, ByteBuffer family, ByteBuffer qualifier, ByteBuffer value,
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,276,TDelete deleteSingle) throws TIOError, TException {
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,277,HTableInterface htable = getTable(table.array());
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,281,return htable.checkAndDelete(row.array(), family.array(), qualifier.array(), null, deleteFromThrift(deleteSingle));
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,283,return htable.checkAndDelete(row.array(), family.array(), qualifier.array(), value.array(), deleteFromThrift(deleteSingle));
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,294,HTableInterface htable = getTable(table.array());
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,306,HTableInterface htable = getTable(table.array());
src/main/java/org/apache/hadoop/hbase/thrift2/ThriftHBaseServiceHandler.java,319,public List<TResult> getScannerRows(int scannerId, int numRows) throws TIOError, TIllegalArgument, TException {
src/main/java/org/apache/hadoop/hbase/HConstants.java,411,public static int RETRY_BACKOFF[] = { 1, 1, 1, 2, 2, 4, 4, 8, 16, 32 };
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,366,Call call = calls.remove(id);
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,384,markClosed(new RemoteException(WritableUtils.readString(in),
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,385,WritableUtils.readString(in)));
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,455,private void setValue(final ImmutableBytesWritable key,
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,439,throws IOException {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,459,return null;
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,241,String hostname = Strings.domainNamePointerToHostName(DNS.getDefaultHost(
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,242,conf.get("hbase.master.dns.interface", "default"),
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,243,conf.get("hbase.master.dns.nameserver", "default")));
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,956,if (Bytes.equals(parentTable, HConstants.META_TABLE_NAME) &&
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,957,(getRegionCachePrefetch(tableName)) )  {
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,958,prefetchRegionCache(tableName, row);
src/main/java/org/apache/hadoop/hbase/client/ScannerCallable.java,104,private void checkIfRegionServerIsRemote() throws UnknownHostException {
src/main/java/org/apache/hadoop/hbase/client/ScannerCallable.java,105,String myAddress = DNS.getDefaultHost("default", "default");
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,156,return true;
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,161,return false;
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,165,return this.operator == Operator.MUST_PASS_ONE;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,168,this.maxRetriesMultiplier =
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,169,this.conf.getInt("replication.source.maxretriesmultiplier", 10);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,170,this.socketTimeoutMultiplier = maxRetriesMultiplier * maxRetriesMultiplier;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,438,try {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,439,chooseSinks();
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,440,Thread.sleep(this.sleepForRetries);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,442,LOG.error("Interrupted while trying to connect to sinks", e);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2216,region.flushcache();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2229,if (region.getLastFlushTime() < ifOlderThanTS) region.flushcache();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2901,region.flushcache();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,506,long minSeqId = -1;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,536,if (minSeqId == -1 || storeSeqId < minSeqId) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,537,minSeqId = storeSeqId;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,558,this.regiondir, minSeqId, reporter, status));
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2737,final long minSeqId, final CancelableProgressable reporter,
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2740,long seqid = minSeqId;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2753,if (maxSeqId <= minSeqId) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2755,+ " and minimum sequenceid for the region is " + minSeqId
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2762,seqid = replayRecoveredEdits(edits, seqid, reporter);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2779,if (seqid > minSeqId) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2806,String msg = "Replaying edits from " + edits + "; minSequenceid=" +
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2807,minSeqId + "; path=" + edits;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2815,long currentEditSeqId = minSeqId;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2875,if (key.getLogSeqNum() <= currentEditSeqId) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2876,skippedEdits++;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2877,continue;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2879,currentEditSeqId = key.getLogSeqNum();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1613,this.readRequestsCount.increment();
src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java,286,public static void deleteDaughtersReferencesInParent(CatalogTracker catalogTracker,
src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java,288,throws NotAllMetaRegionsOnlineException, IOException {
src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java,289,Delete delete = new Delete(parent.getRegionName());
src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java,290,delete.deleteColumns(HConstants.CATALOG_FAMILY, HConstants.SPLITA_QUALIFIER);
src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java,291,delete.deleteColumns(HConstants.CATALOG_FAMILY, HConstants.SPLITB_QUALIFIER);
src/main/java/org/apache/hadoop/hbase/catalog/MetaEditor.java,293,LOG.info("Deleted daughters references, qualifier=" + Bytes.toStringBinary(HConstants.SPLITA_QUALIFIER) +
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,406,HRegionInfo splitA = Writables.getHRegionInfo(rowResult.getValue(HConstants.CATALOG_FAMILY,
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,408,HRegionInfo splitB = Writables.getHRegionInfo(rowResult.getValue(HConstants.CATALOG_FAMILY,
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,413,Result resultA = getRegionResultBlocking(metaTable, blockingTimeout,
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,414,splitA.getRegionName());
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,415,if (resultA != null) {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,416,processRow(resultA);
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,417,daughterRegions.add(splitA.getRegionName());
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,419,throw new RegionOfflineException("Split daughter region " +
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,420,splitA.getRegionNameAsString() + " cannot be found in META.");
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,424,Result resultB = getRegionResultBlocking(metaTable, rem,
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,425,splitB.getRegionName());
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,426,if (resultB != null) {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,427,processRow(resultB);
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,428,daughterRegions.add(splitB.getRegionName());
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,430,throw new RegionOfflineException("Split daughter region " +
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,431,splitB.getRegionNameAsString() + " cannot be found in META.");
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,440,if (LOG.isDebugEnabled()) {
src/main/java/org/apache/hadoop/hbase/client/MetaScanner.java,441,LOG.debug("blocking until region is in META: " + Bytes.toStringBinary(regionName));
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,44,import org.apache.hadoop.hbase.catalog.MetaReader;
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,112,MetaReader.Visitor visitor = new MetaReader.Visitor() {
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,114,public boolean visit(Result r) throws IOException {
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,125,MetaReader.fullScan(this.server.getCatalogTracker(), visitor);
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,181,result = Bytes.compareTo(left.getEndKey(), right.getEndKey());
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,182,if (result != 0) {
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,183,if (left.getStartKey().length != 0
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,184,&& left.getEndKey().length == 0) {
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,185,return -1;  // left is last region
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,187,if (right.getStartKey().length != 0
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,188,&& right.getEndKey().length == 0) {
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,189,return 1;  // right is the last region
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,191,return -result; // Flip the result so parent comes first.
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,193,return result;
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,239,removeDaughtersFromParent(parent);
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,288,throws IOException {
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,289,MetaEditor.deleteDaughtersReferencesInParent(this.server.getCatalogTracker(), parent);
src/main/java/org/apache/hadoop/hbase/util/Bytes.java,36,import org.apache.commons.lang.StringUtils;
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2157,while(!regions.containsKey(regionInfo)) {
src/main/java/org/apache/hadoop/hbase/io/hfile/LruBlockCache.java,271,throw new RuntimeException("Cached an already cached block");
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,398,getDefaultBlockSize());
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,419,this.fs.getDefaultReplication());
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,448,private long getDefaultBlockSize() throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,449,Method m = null;
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,450,Class<? extends FileSystem> cls = this.fs.getClass();
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,451,try {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,452,m = cls.getMethod("getDefaultBlockSize",
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,453,new Class<?>[] { Path.class });
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,455,LOG.info("FileSystem doesn't support getDefaultBlockSize");
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,457,LOG.info("Doesn't have access to getDefaultBlockSize on "
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,458,+ "FileSystems", e);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,461,if (null == m) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,462,return this.fs.getDefaultBlockSize();
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,464,try {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,465,Object ret = m.invoke(this.fs, this.dir);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,466,return ((Long)ret).longValue();
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,468,throw new IOException(e);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java,164,fs.getDefaultReplication())),
src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java,166,fs.getDefaultBlockSize())),
src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java,185,fs.getDefaultReplication()),
src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogWriter.java,187,fs.getDefaultBlockSize()),
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,947,createWithParents(zkw, znode);
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1748,if (this.zooKeeper != null) {
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1749,LOG.info("Closed zookeeper sessionid=0x" +
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1750,Long.toHexString(this.zooKeeper.getRecoverableZooKeeper().getSessionId()));
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1751,this.zooKeeper.close();
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1752,this.zooKeeper = null;
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1754,this.closed = true;
src/main/java/org/apache/hadoop/hbase/metrics/MetricsMBeanBase.java,23,import java.util.HashMap;
src/main/java/org/apache/hadoop/hbase/metrics/MetricsMBeanBase.java,56,protected Map<String,MetricsBase> extendedAttributes =
src/main/java/org/apache/hadoop/hbase/metrics/MetricsMBeanBase.java,57,new HashMap<String,MetricsBase>();
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,26,import org.apache.hadoop.fs.FileSystem;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,31,import org.apache.hadoop.hbase.HBaseConfiguration;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,33,import org.apache.hadoop.hbase.client.*;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,39,import org.apache.hadoop.hbase.Server;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,40,import org.apache.hadoop.io.IOUtils;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,42,import java.io.File;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,43,import java.io.FileOutputStream;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,44,import java.io.IOException;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,45,import java.net.URL;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,47,import java.util.*;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,48,import java.util.jar.JarEntry;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,49,import java.util.jar.JarFile;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,172,if (implClass == null) {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,174,if (!path.toString().endsWith(".jar")) {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,175,throw new IOException(path.toString() + ": not a jar file?");
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,196,List<URL> paths = new ArrayList<URL>();
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,199,JarFile jarFile = new JarFile(dst.toString());
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,200,Enumeration<JarEntry> entries = jarFile.entries();
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,201,while (entries.hasMoreElements()) {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,202,JarEntry entry = entries.nextElement();
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,203,if (entry.getName().matches("/lib/[^/]+\\.jar")) {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,207,IOUtils.copyBytes(jarFile.getInputStream(entry), new FileOutputStream(file), conf, true);
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,208,file.deleteOnExit();
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,209,paths.add(file.toURL());
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,113,Thread.sleep(1000);
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,120,LOG.info("Finished lease recover attempt for " + p);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,200,splits = splitLog(logfiles);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,260,private List<Path> splitLog(final FileStatus[] logfiles) throws IOException {
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,74,if (fs instanceof DistributedFileSystem) {
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,75,DistributedFileSystem dfs = (DistributedFileSystem)fs;
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,79,throw new Exception("Not a DistributedFileSystem");
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,83,throw (IOException) ite.getCause();
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,569,return null;
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1120,Map<byte[], HRegionLocation> tableLocations =
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1121,getTableLocations(tableName);
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,460,Class... classes) throws IOException {
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,467,jars.addAll( conf.getStringCollection("tmpjars") );
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,470,for (Class clazz : classes) {
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,473,String pathStr = findOrCreateJar(clazz);
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,474,if (pathStr == null) {
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,479,Path path = new Path(pathStr);
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,485,jars.add(path.makeQualified(localFs).toString());
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,530,if (sleepMultiplier == this.maxRetriesMultiplier) {
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,659,splitLogAndExpireIfOnline(currentMetaServer);
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,689,private void waitForRootAssignment() throws InterruptedException {
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,281,splitLogManager.handleDeadWorkers(serverNames);
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,282,splitTime = EnvironmentEdgeManager.currentTimeMillis();
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,284,splitTime = EnvironmentEdgeManager.currentTimeMillis() - splitTime;
src/main/java/org/apache/hadoop/hbase/master/ServerManager.java,364,if (!services.isServerShutdownHandlerEnabled()) {
src/main/java/org/apache/hadoop/hbase/master/ServerManager.java,373,return;
src/main/java/org/apache/hadoop/hbase/master/ServerManager.java,401,boolean carryingRoot = services.getAssignmentManager().isCarryingRoot(serverName);
security/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlLists.java,137,LOG.warn("No actions associated with user '"+Bytes.toString(userPerm.getUser())+"'");
security/src/main/java/org/apache/hadoop/hbase/security/access/AccessControlLists.java,138,return;
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,61,throw new TableNotFoundException(Bytes.toString(tableName));
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,122,static final byte[] BLOOM_FILTER_TYPE_KEY =
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,27,import java.lang.reflect.Method;
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,40,import org.apache.hadoop.hbase.HConstants;
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,510,try {
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,511,Class<?> jarFinder = Class.forName("org.apache.hadoop.util.JarFinder");
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,518,Method m = jarFinder.getMethod("getJar", Class.class);
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,519,return (String)m.invoke(null,my_class);
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,522,throw new IOException(ite.getCause());
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,527,LOG.debug("New JarFinder: org.apache.hadoop.util.JarFinder.getJar " +
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,529,return findContainingJar(my_class);
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,543,private static String findContainingJar(Class my_class) {
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,546,try {
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,547,for(Enumeration itr = loader.getResources(class_file);
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,548,itr.hasMoreElements();) {
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,549,URL url = (URL) itr.nextElement();
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,550,if ("jar".equals(url.getProtocol())) {
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,551,String toReturn = url.getPath();
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,552,if (toReturn.startsWith("file:")) {
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,553,toReturn = toReturn.substring("file:".length());
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,561,toReturn = toReturn.replaceAll("\\+", "%2B");
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,562,toReturn = URLDecoder.decode(toReturn, "UTF-8");
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,563,return toReturn.replaceAll("!.*$", "");
src/main/java/org/apache/hadoop/hbase/mapreduce/TableMapReduceUtil.java,567,throw new RuntimeException(e);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,1284,if (txid <= this.syncedTillHere) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,1285,return;
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,105,master.getConfiguration(), master, master.getServerName().toString());
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,112,private ConcurrentMap<String, Task> tasks =
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,117,private Object deadWorkersLock = new Object();
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,132,Stoppable stopper, String serverName) {
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,133,this(zkw, conf, stopper, serverName, new TaskFinisher() {
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,159,Stoppable stopper, String serverName, TaskFinisher tf) {
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,173,LOG.debug("timeout = " + timeout);
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,174,LOG.debug("unassigned timeout = " + unassignedTimeout);
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,561,if ((EnvironmentEdgeManager.currentTimeMillis() - task.last_update) <
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,562,timeout) {
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKSplitLog.java,48,public static final int DEFAULT_TIMEOUT = 25000; // 25 sec
src/main/java/org/apache/hadoop/hbase/client/HConnection.java,195,throws IOException;
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,795,throws IOException {
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,797,return null;
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,804,return null;
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1405,LOG.info(destination.toString() + " unassigned znodes=" + count +
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1472,LOG.warn("rc != 0 for " + path + " -- retryable connectionloss -- " +
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1474,this.zkw.abort("Connectionloss writing unassigned at " + path +
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1482,new ExistsUnassignedAsyncCallback(this.counter, destination), ctx);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1492,private final AtomicInteger counter;
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1495,ExistsUnassignedAsyncCallback(final AtomicInteger counter, ServerName destination) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1496,this.counter = counter;
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1504,LOG.warn("rc != 0 for " + path + " -- retryable connectionloss -- " +
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1516,this.counter.addAndGet(1);
src/main/java/org/apache/hadoop/hbase/io/Reference.java,128,FSDataOutputStream out = fs.create(p, false);
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,133,this.fs.mkdirs(oldLogDir);
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,263,if (!this.fs.rename(logDir, splitDir)) {
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,335,fs.mkdirs(rd);
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,447,fs.delete(new Path(rootdir, Bytes.toString(tableName)), true);
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,277,if (fs.exists(logDir) && !fs.delete(logDir, false)) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,604,if (!fs.rename(initialFiles, regiondir)) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,736,if (!fs.rename(tmpPath, regioninfoPath)) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2785,if (!this.fs.delete(file, false)) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2977,fs.delete(p, false);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3667,fs.mkdirs(regionDir);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3851,if (!fs.delete(regiondir, true)) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3897,if (!fs.mkdirs(dir)) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4007,fs.mkdirs(newRegionDir);
src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java,547,if (!fs.delete(splitdir, true)) {
src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java,552,if (!fs.mkdirs(splitdir)) throw new IOException("Failed create of " + splitdir);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,782,if (!fs.rename(path, dstPath)) {
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1675,if (!fs.rename(origPath, destPath)) {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,633,this.fs.delete(getPath(), true);
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,676,if (!fs.rename(src, tgt)) {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,799,fs.mkdirs(dir);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,411,if (!fs.exists(oldLogDir)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,412,if (!fs.mkdirs(this.oldLogDir)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,413,throw new IOException("Unable to mkdir " + this.oldLogDir);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,692,return createWriter(fs, path, conf);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,908,if (!this.fs.rename(p, newPath)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,960,if (!fs.rename(file.getPath(),p)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,972,if (!fs.delete(dir, true)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,1831,if (!fs.rename(edits, moveAsideName)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,479,if (!fs.delete(dst, false)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,488,if (!fs.rename(wap.p, dst)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,555,fs.delete(stagingDir, true);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,578,if (!fs.mkdirs(corruptDir)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,581,fs.mkdirs(oldLogDir);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,588,if (!fs.rename(corrupted, p)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,599,if (!fs.rename(p, newPath)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,609,if (srcDir != null && !fs.delete(srcDir, true)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,641,if (isCreate && !fs.exists(dir)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,642,if (!fs.mkdirs(dir)) LOG.warn("mkdir failed on " + dir);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,813,return HLog.createWriter(fs, logfile, conf);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,1060,if (!fs.delete(regionedits, false)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,1086,if (!fs.delete(ret, false)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,1091,if (!fs.exists(dir)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,1092,if (!fs.mkdirs(dir)) LOG.warn("mkdir failed on " + dir);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,1185,if (!fs.delete(dst, false)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,1194,if (!fs.rename(wap.p, dst)) {
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,217,if (!this.fs.delete(tabledir, true)) {
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,273,if (!fs.delete(p, false)) {
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,477,if (!fs.rename(p, tableInfoPath)) {
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,503,FSDataOutputStream out = fs.create(p, false);
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,102,fs.mkdirs(dir);
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,147,public static FSDataOutputStream create(FileSystem fs, Path path,
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,148,FsPermission perm, boolean overwrite) throws IOException {
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,151,return fs.create(path, perm, overwrite,
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,152,fs.getConf().getInt("io.file.buffer.size", 4096),
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,153,fs.getDefaultReplication(), fs.getDefaultBlockSize(), null);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKSplitLog.java,167,fs.createNewFile(file);
src/main/java/org/apache/hadoop/hbase/executor/EventHandler.java,139,M_META_SERVER_SHUTDOWN    (72);  // Master is processing shutdown of RS hosting a meta region (-ROOT- or .META.).
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,312,String msg = "File system needs to be upgraded."
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,315,+ ".  Run the '${HBASE_HOME}/bin/hbase migrate' script.";
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,1220,for (WriterAndPath wap : logWriters.values()) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,1221,try {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,1222,wap.w.close();
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,1224,LOG.error("Couldn't close log at " + wap.p, ioe);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,1225,thrown.add(ioe);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,1226,continue;
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,1228,LOG.info("Closed path " + wap.p + " (wrote " + wap.editsWritten
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,1229,+ " edits in " + (wap.nanosSpent / 1000 / 1000) + "ms)");
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLogSplitter.java,1231,logWritersClosed = true;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3238,failures.add(p);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3266,if (ioes.size() != 0) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3267,LOG.error("There were IO errors when checking if bulk load is ok.  " +
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3269,throw MultipleIOException.createIOException(ioes);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java,218,if (!plugins.contains(ReplicationLogCleaner.class.toString())) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java,219,conf.set(HBASE_MASTER_LOGCLEANER_PLUGINS,
src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java,220,plugins + "," + ReplicationLogCleaner.class.getCanonicalName());
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1667,if (t instanceof RegionAlreadyInTransitionException) {
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,210,protected Map<ImmutableBytesWritable,ImmutableBytesWritable> values =
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,493,setValue(Bytes.toBytes(key), Bytes.toBytes(value));
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,860,for (Map.Entry<ImmutableBytesWritable, ImmutableBytesWritable> e:
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,861,values.entrySet()) {
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,862,String key = Bytes.toString(e.getKey().get());
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,863,String value = Bytes.toString(e.getValue().get());
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,864,s.append(", ");
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,865,s.append(key);
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,866,s.append(" => '");
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,867,s.append(value);
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,868,s.append("'");
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,884,for (Map.Entry<ImmutableBytesWritable, ImmutableBytesWritable> e:
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,885,values.entrySet()) {
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,886,String key = Bytes.toString(e.getKey().get());
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,887,String value = Bytes.toString(e.getValue().get());
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,888,if(DEFAULT_VALUES.get(key) == null || !DEFAULT_VALUES.get(key).equalsIgnoreCase(value)) {
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,891,s.append(" => '");
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,892,s.append(value);
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,893,s.append("'");
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,896,s.append('}');
src/main/java/org/apache/hadoop/hbase/HColumnDescriptor.java,897,return s.toString();
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,67,protected Map<ImmutableBytesWritable, ImmutableBytesWritable> values =
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,428,return Collections.unmodifiableMap(values);
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,468,setValue(Bytes.toBytes(key), Bytes.toBytes(value));
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,678,s.append('{');
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,679,s.append(HConstants.NAME);
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,680,s.append(" => '");
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,681,s.append(Bytes.toString(name));
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,682,s.append("'");
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,683,for (Map.Entry<ImmutableBytesWritable, ImmutableBytesWritable> e:
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,684,values.entrySet()) {
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,685,String key = Bytes.toString(e.getKey().get());
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,686,String value = Bytes.toString(e.getValue().get());
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,687,if (key == null) {
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,688,continue;
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,690,String upperCase = key.toUpperCase();
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,691,if (upperCase.equals(IS_ROOT) || upperCase.equals(IS_META)) {
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,693,if (value.toLowerCase().equals(Boolean.FALSE.toString())) {
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,694,continue;
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,697,s.append(", ");
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,698,s.append(Bytes.toString(e.getKey().get()));
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,699,s.append(" => '");
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,700,s.append(Bytes.toString(e.getValue().get()));
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,701,s.append("'");
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,703,s.append(", ");
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,704,s.append(FAMILIES);
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,705,s.append(" => ");
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,706,s.append(families.values());
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,707,s.append('}');
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,717,s.append('{');
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,718,s.append(HConstants.NAME);
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,719,s.append(" => '");
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,720,s.append(Bytes.toString(name));
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,721,s.append("'");
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,722,for (Map.Entry<ImmutableBytesWritable, ImmutableBytesWritable> e:
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,723,values.entrySet()) {
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,724,String key = Bytes.toString(e.getKey().get());
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,725,String value = Bytes.toString(e.getValue().get());
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,726,if (key == null) {
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,729,String upperCase = key.toUpperCase();
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,730,if (upperCase.equals(IS_ROOT) || upperCase.equals(IS_META)) {
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,737,s.append(Bytes.toString(e.getKey().get()));
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,738,s.append(" => '");
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,739,s.append(Bytes.toString(e.getValue().get()));
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,740,s.append("'");
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,742,s.append(", ");
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,743,s.append(FAMILIES);
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,744,s.append(" => [");
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,745,int size = families.values().size();
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,746,int i = 0;
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,747,for(HColumnDescriptor hcd : families.values()) {
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,748,s.append(hcd.toStringCustomizedValues());
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,749,i++;
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,751,s.append(", ");
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,753,s.append("]}");
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,754,return s.toString();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,382,public HRegion(Path tableDir, HLog log, FileSystem fs, Configuration conf,
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,389,this.conf = conf;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3969,Configuration conf = a.getConf();
src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java,689,this.parent.getLog(), fs, this.parent.getConf(),
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,198,this.conf = conf;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,126,private String[] deadRegionServers;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,200,private void checkIfQueueRecovered(String peerClusterZnode) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,201,String[] parts = peerClusterZnode.split("-");
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,206,this.deadRegionServers = new String[parts.length-1];
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,208,for (int i = 1; i < parts.length; i++) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,209,this.deadRegionServers[i-1] = parts[i];
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,481,LOG.info("NB dead servers : " + deadRegionServers.length);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,482,for (int i = this.deadRegionServers.length - 1; i >= 0; i--) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,485,new Path(manager.getLogDir().getParent(), this.deadRegionServers[i]);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,704,return;
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,746,fileSystemManager.splitLog(sn);
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,254,public void splitLog(final List<ServerName> serverNames) throws IOException {
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,283,splitLogSize = splitLogManager.splitLogDistributed(logDirs);
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,195,private FileStatus[] getFileList(List<Path> logDirs) throws IOException {
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,204,FileStatus[] logfiles = FSUtils.listStatus(fs, hLogDir, null);
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,243,FileStatus[] logfiles = getFileList(logDirs);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,53,private final ServerName serverName;
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,54,private final MasterServices services;
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,55,private final DeadServer deadServers;
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,56,private final boolean shouldSplitHlog; // whether to split HLog or not
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,100,throws InterruptedException, IOException, KeeperException {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,101,long timeout = this.server.getConfiguration().
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,102,getLong("hbase.catalog.verification.timeout", 1000);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,103,if (!this.server.getCatalogTracker().verifyRootRegionLocation(timeout)) {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,104,this.services.getAssignmentManager().assignRoot();
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,112,private void verifyAndAssignRootWithRetries() throws IOException {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,113,int iTimes = this.server.getConfiguration().getInt(
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,116,long waitTime = this.server.getConfiguration().getLong(
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,119,int iFlag = 0;
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,120,while (true) {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,121,try {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,122,verifyAndAssignRoot();
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,123,break;
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,125,this.server.abort("In server shutdown processing, assigning root", e);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,126,throw new IOException("Aborting", e);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,128,if (iFlag >= iTimes) {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,129,this.server.abort("verifyAndAssignRoot failed after" + iTimes
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,130,+ " times retries, aborting", e);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,131,throw new IOException("Aborting", e);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,133,try {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,134,Thread.sleep(waitTime);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,136,LOG.warn("Interrupted when is the thread sleep", e1);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,137,Thread.currentThread().interrupt();
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,138,throw new IOException("Interrupted", e1);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,140,iFlag++;
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,180,this.services.getExecutorService().submit(this);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,187,if (isCarryingRoot()) { // -ROOT-
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,196,if (isCarryingMeta()) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,912,try {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,913,if (this.hlog != null) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,921,LOG.error("Close and delete failed", RemoteExceptionHandler.checkThrowable(e));
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1490,UncaughtExceptionHandler handler = new UncaughtExceptionHandler() {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1491,public void uncaughtException(Thread t, Throwable e) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1492,abort("Uncaught exception in service thread " + t.getName(), e);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1511,Threads.setDaemonThreadRunning(this.hlogRoller.getThread(), n + ".logRoller", handler);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1513,handler);
src/main/java/org/apache/hadoop/hbase/regionserver/LogRoller.java,49,private final RegionServerServices services;
src/main/java/org/apache/hadoop/hbase/regionserver/LogRoller.java,94,byte [][] regionsToFlush = this.services.getWAL().rollWriter(rollLog.get());
src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerServices.java,23,import java.util.Map;
src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerServices.java,41,public HLog getWAL();
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,46,private final RegionServerServices rsServices;
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,333,this.rsServices.getWAL(), this.server.getConfiguration(),
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,256,private static final Pattern pattern = Pattern.compile(".*\\.\\d*");
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,336,this(fs, dir, oldLogDir, conf, null, true, null);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,361,this(fs, dir, oldLogDir, conf, listeners, true, prefix);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,404,if (failIfLogDirExists && fs.exists(dir)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,407,if (!fs.mkdirs(dir)) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,938,return new Path(dir, prefix + "." + filenum);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java,395,return createWriter(fs, path, ostream, blockSize,
src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java,396,compression, encoder, comparator, checksumType, bytesPerChecksum);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFile.java,404,int bytesPerChecksum) throws IOException;
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV1.java,95,final int bytesPerChecksum) throws IOException {
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java,87,private final boolean includeMemstoreTS = true;
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java,100,final int bytesPerChecksum) throws IOException {
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java,101,return new HFileWriterV2(conf, cacheConf, fs, path, ostream, blockSize,
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java,102,compress, blockEncoder, comparator, checksumType, bytesPerChecksum);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java,111,final int bytesPerChecksum) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,817,bytesPerChecksum);
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,192,private volatile boolean initialized = false;
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,742,try {
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,744,+ blockBuffer.position() + KEY_VALUE_LEN_SIZE + currKeyLen
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,745,+ currValueLen;
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,746,currMemstoreTS = Bytes.readVLong(blockBuffer.array(),
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,747,memstoreTSOffset);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,748,currMemstoreTSLen = WritableUtils.getVIntSize(currMemstoreTS);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,750,throw new RuntimeException("Error reading memstore timestamp", e);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,789,try {
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,791,+ blockBuffer.position() + KEY_VALUE_LEN_SIZE + klen + vlen;
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,792,memstoreTS = Bytes.readVLong(blockBuffer.array(),
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,793,memstoreTSOffset);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,794,memstoreTSLen = WritableUtils.getVIntSize(memstoreTS);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,796,throw new RuntimeException("Error reading memstore timestamp", e);
src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java,23,import java.util.ArrayList;
src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java,24,import java.util.Arrays;
src/main/java/org/apache/hadoop/hbase/master/handler/CreateTableHandler.java,43,import org.apache.hadoop.hbase.regionserver.HRegion;
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,680,ZKUtil.createWithParents(zkw, znode);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,682,ZKUtil.setData(zkw, znode, data);
src/main/java/org/apache/hadoop/hbase/util/ClassSize.java,130,STRING = align(OBJECT + ARRAY + REFERENCE + 3 * Bytes.SIZEOF_INT);
src/main/java/org/apache/hadoop/hbase/util/ClassSize.java,132,CONCURRENT_HASHMAP = align((2 * Bytes.SIZEOF_INT) + ARRAY +
src/main/java/org/apache/hadoop/hbase/client/HTable.java,115,private static final int DOPUT_WB_CHECK = 10;    // i.e., doPut checks the writebuffer every X Puts.
src/main/java/org/apache/hadoop/hbase/client/HTable.java,747,doPut(Arrays.asList(put));
src/main/java/org/apache/hadoop/hbase/client/HTable.java,755,doPut(puts);
src/main/java/org/apache/hadoop/hbase/client/HTable.java,758,private void doPut(final List<Put> puts) throws IOException {
src/main/java/org/apache/hadoop/hbase/client/HTable.java,759,int n = 0;
src/main/java/org/apache/hadoop/hbase/client/HTable.java,761,validatePut(put);
src/main/java/org/apache/hadoop/hbase/client/HTable.java,762,writeBuffer.add(put);
src/main/java/org/apache/hadoop/hbase/client/HTable.java,763,currentWriteBufferSize += put.heapSize();
src/main/java/org/apache/hadoop/hbase/client/HTable.java,766,n++;
src/main/java/org/apache/hadoop/hbase/client/HTable.java,767,if (n % DOPUT_WB_CHECK == 0 && currentWriteBufferSize > writeBufferSize) {
src/main/java/org/apache/hadoop/hbase/client/HTable.java,768,flushCommits();
src/main/java/org/apache/hadoop/hbase/client/HTable.java,771,if (autoFlush || currentWriteBufferSize > writeBufferSize) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java,561,Thread.sleep(sleepBeforeFailover);
src/main/java/org/apache/hadoop/hbase/util/Bytes.java,342,try {
src/main/java/org/apache/hadoop/hbase/util/Bytes.java,343,String first = new String(b, off, len, "ISO-8859-1");
src/main/java/org/apache/hadoop/hbase/util/Bytes.java,344,for (int i = 0; i < first.length() ; ++i ) {
src/main/java/org/apache/hadoop/hbase/util/Bytes.java,345,int ch = first.charAt(i) & 0xFF;
src/main/java/org/apache/hadoop/hbase/util/Bytes.java,349,|| " `~!@#$%^&*()-_=+[]{}\\|;:'\",.<>/?".indexOf(ch) >= 0 ) {
src/main/java/org/apache/hadoop/hbase/util/Bytes.java,350,result.append(first.charAt(i));
src/main/java/org/apache/hadoop/hbase/util/Bytes.java,352,result.append(String.format("\\x%02X", ch));
src/main/java/org/apache/hadoop/hbase/util/Bytes.java,356,LOG.error("ISO-8859-1 not supported?", e);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,309,if(regionsToReopen.get(hri.getEncodedName()) != null) {
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,1324,this.executorService.submit(new ModifyTableHandler(tableName, htd, this, this));
src/main/java/org/apache/hadoop/hbase/master/handler/TableAddFamilyHandler.java,55,.addColumn(tableName, familyDesc);
src/main/java/org/apache/hadoop/hbase/master/handler/TableAddFamilyHandler.java,57,this.masterServices.getTableDescriptors().add(htd);
src/main/java/org/apache/hadoop/hbase/master/handler/TableDeleteFamilyHandler.java,48,HTableDescriptor htd =
src/main/java/org/apache/hadoop/hbase/master/handler/TableDeleteFamilyHandler.java,49,this.masterServices.getMasterFileSystem().deleteColumn(tableName, familyName);
src/main/java/org/apache/hadoop/hbase/master/handler/TableDeleteFamilyHandler.java,51,this.masterServices.getTableDescriptors().add(htd);
src/main/java/org/apache/hadoop/hbase/master/handler/TableEventHandler.java,117,HTable table = new HTable(masterServices.getConfiguration(), tableName);
src/main/java/org/apache/hadoop/hbase/master/handler/TableEventHandler.java,118,TreeMap<ServerName, List<HRegionInfo>> serverToRegions = Maps
src/main/java/org/apache/hadoop/hbase/master/handler/TableEventHandler.java,119,.newTreeMap();
src/main/java/org/apache/hadoop/hbase/master/handler/TableEventHandler.java,120,NavigableMap<HRegionInfo, ServerName> hriHserverMapping = table.getRegionLocations();
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,1702,return timeRangeTracker.maximumTimestamp;
src/main/java/org/apache/hadoop/hbase/rest/RESTServlet.java,71,this.pool = new HTablePool(conf, 10);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperMainServerArg.java,23,import java.util.Properties;
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperMainServerArg.java,47,if (key.startsWith("server.") && host == null) {
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperMainServerArg.java,49,host = parts[0];
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperMainServerArg.java,53,if (host != null && clientPort != null) break;
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperMainServerArg.java,55,return host != null && clientPort != null? host + ":" + clientPort: null;
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV1.java,687,ByteBuffer buf = getMetaBlock(HFileWriterV1.BLOOM_FILTER_META_KEY, true);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,246,private final LogSyncer logSyncerThread;
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,440,logSyncerThread = new LogSyncer(this.optionalFlushInterval);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,441,Threads.setDaemonThreadRunning(logSyncerThread.getThread(),
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,442,Thread.currentThread().getName() + ".logSyncer");
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,983,try {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,984,logSyncerThread.close();
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,986,logSyncerThread.join(this.optionalFlushInterval*2);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,988,LOG.error("Exception while waiting for syncer thread to die", e);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,1301,logSyncerThread.hlogFlush(tempWriter, pending);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,1306,logSyncerThread.hlogFlush(tempWriter, pending);
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,1456,logSyncerThread.append(new HLog.Entry(logKey, logEdit));
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,1585,logSyncerThread.append(new Entry(key, edit));
src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java,205,public synchronized void requestCompaction(final HRegion r,
src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java,206,final String why) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java,207,for(Store s : r.getStores().values()) {
src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java,208,requestCompaction(r, s, why, Store.NO_PRIORITY);
src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java,212,public synchronized void requestCompaction(final HRegion r, final Store s,
src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java,213,final String why) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java,214,requestCompaction(r, s, why, Store.NO_PRIORITY);
src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java,217,public synchronized void requestCompaction(final HRegion r, final String why,
src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java,218,int p) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java,219,for(Store s : r.getStores().values()) {
src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java,220,requestCompaction(r, s, why, p);
src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java,230,public synchronized void requestCompaction(final HRegion r, final Store s,
src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java,231,final String why, int priority) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java,233,return;
src/main/java/org/apache/hadoop/hbase/regionserver/CompactSplitThread.java,235,CompactionRequest cr = s.requestCompaction(priority);
src/main/java/org/apache/hadoop/hbase/regionserver/CompactionRequestor.java,30,public void requestCompaction(final HRegion r, final String why) throws IOException;
src/main/java/org/apache/hadoop/hbase/regionserver/CompactionRequestor.java,38,public void requestCompaction(final HRegion r, final Store s, final String why) throws IOException;
src/main/java/org/apache/hadoop/hbase/regionserver/CompactionRequestor.java,46,public void requestCompaction(final HRegion r, final String why, int pri) throws IOException;
src/main/java/org/apache/hadoop/hbase/regionserver/CompactionRequestor.java,55,public void requestCompaction(final HRegion r, final Store s,
src/main/java/org/apache/hadoop/hbase/regionserver/CompactionRequestor.java,56,final String why, int pri) throws IOException;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1218,this.instance.compactSplitThread.requestCompaction(r, s,
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1219,getName() + " requests compaction");
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1221,if (majorCompactPriority == DEFAULT_PRIORITY ||
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1222,majorCompactPriority > r.getCompactPriority()) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1223,this.instance.compactSplitThread.requestCompaction(r, s,
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1224,getName() + " requests major compaction; use default priority");
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1226,this.instance.compactSplitThread.requestCompaction(r, s,
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1227,getName() + " requests major compaction; use configured priority",
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1228,this.majorCompactPriority);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1632,getCompactionRequester().requestCompaction(r, s, "Opening Region");
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,314,public boolean preCompactSelection(Store store, List<StoreFile> candidates) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,321,((RegionObserver)env.getInstance()).preCompactSelection(
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,322,ctx, store, candidates);
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,342,public void postCompactSelection(Store store,
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,343,ImmutableList<StoreFile> selected) {
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,349,((RegionObserver)env.getInstance()).postCompactSelection(
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,350,ctx, store, selected);
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,367,public InternalScanner preCompact(Store store, InternalScanner scanner) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,370,for (RegionEnvironment env: coprocessors) {
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,374,scanner = ((RegionObserver)env.getInstance()).preCompact(
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,375,ctx, store, scanner);
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,377,handleCoprocessorThrowable(env,e);
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,394,public void postCompact(Store store, StoreFile resultFile) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,400,((RegionObserver)env.getInstance()).postCompact(ctx, store, resultFile);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1062,region.getCoprocessorHost().postCompact(this, sf);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1213,return requestCompaction(NO_PRIORITY);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1216,public CompactionRequest requestCompaction(int priority) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1222,CompactionRequest ret = null;
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1239,override = region.getCoprocessorHost().preCompactSelection(
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1240,this, candidates);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1252,ImmutableList.copyOf(filesToCompact.getFilesToCompact()));
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1278,ret = new CompactionRequest(region, this, filesToCompact, isMajor, pri);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1283,if (ret != null) {
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1284,CompactionRequest.preRequest(ret);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1286,return ret;
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,54,private final CompactSelection compactSelection;
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,55,private final long totalSize;
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,56,private final boolean isMajor;
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,69,public CompactionRequest(HRegion r, Store s,
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,70,CompactSelection files, boolean isMajor, int p) {
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,71,Preconditions.checkNotNull(r);
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,72,Preconditions.checkNotNull(files);
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,74,this.r = r;
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,75,this.s = s;
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,76,this.compactSelection = files;
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,77,long sz = 0;
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,78,for (StoreFile sf : files.getFilesToCompact()) {
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,79,sz += sf.getReader().length();
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,81,this.totalSize = sz;
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,82,this.isMajor = isMajor;
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,83,this.p = p;
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,84,this.timeInNanos = System.nanoTime();
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,258,server.compactSplitThread
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,259,.requestCompaction(r, s, "Recursive enqueue");
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,262,server.compactSplitThread.requestSplit(r);
src/main/java/org/apache/hadoop/hbase/regionserver/compactions/CompactionRequest.java,274,LOG.debug("CompactSplitThread status: " + server.compactSplitThread);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,695,modTInfo.addRegionInfo(hbi);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,1555,modTInfo.addRegionInfo(hbi);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2317,MetaEntry m = new MetaEntry(hri, sn, ts);
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,76,DistributedFileSystem.class.getMethod("recoverLease",
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,77,new Class[] {Path.class}).invoke(dfs, p);
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,85,LOG.debug("Failed fs.recoverLease invocation, " + e.toString() +
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,90,recovered = true;
src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java,116,iioe.initCause(ex);
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,587,addedSize -= heapSizeChange(kv, true);
src/main/java/org/apache/hadoop/hbase/regionserver/SplitLogWorker.java,110,try {
src/main/java/org/apache/hadoop/hbase/regionserver/SplitLogWorker.java,112,fs.getFileStatus(new Path(filename)), fs, conf, p) == false) {
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,666,private Path flushCache(final long logCacheFlushId,
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,27,import java.util.Set;
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,47,import org.apache.hadoop.hbase.regionserver.wal.OrphanHLogAfterSplitException;
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,62,import org.apache.hadoop.hbase.zookeeper.ZKSplitLog.TaskState;
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,421,deleteNode(path, Long.MAX_VALUE);
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,654,LOG.fatal("logic failure, failing to delete a node should never happen " +
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,67,private ZooKeeper zk;
src/main/java/org/apache/hadoop/hbase/util/ByteBloomFilter.java,421,int bloomBitSize = bloomSize * 8;
src/main/java/org/apache/hadoop/hbase/util/ByteBloomFilter.java,426,long hashLoc = Math.abs((hash1 + i * hash2) % bloomBitSize);
src/main/java/org/apache/hadoop/hbase/util/ByteBloomFilter.java,433,long hashLoc = randomGeneratorForTest.nextInt(bloomBitSize);
src/main/java/org/apache/hadoop/hbase/util/ByteBloomFilter.java,464,static boolean get(long pos, byte[] bloomArray, int bloomOffset) {
src/main/java/org/apache/hadoop/hbase/util/ByteBloomFilter.java,465,int bytePos = (int)(pos / 8);
src/main/java/org/apache/hadoop/hbase/util/ByteBloomFilter.java,466,int bitPos = (int)(pos % 8);
src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java,126,private boolean stopped = false;
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,525,status.setStatus("Splitting logs after master startup");
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,526,splitLogAfterStartup(this.fileSystemManager);
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,529,assignRootAndMeta(status);
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,606,protected void splitLogAfterStartup(final MasterFileSystem mfs) {
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,607,mfs.splitLogAfterStartup();
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,651,rit = this.assignmentManager.
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,652,processRegionInTransitionAndBlockUntilAssigned(HRegionInfo.FIRST_META_REGIONINFO);
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,655,ServerName currentMetaServer =
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,656,this.catalogTracker.getMetaLocationOrReadLocationFromRoot();
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,657,if (currentMetaServer != null
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,658,&& !currentMetaServer.equals(currentRootServer)) {
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,673,LOG.info(".META. assigned=" + assigned + ", rit=" + rit +
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,675,status.setStatus("META and ROOT assigned.");
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,676,return assigned;
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,1581,LOG.error("ZooKeeper exception trying to set cluster as down in ZK", e);
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,187,void splitLogAfterStartup() {
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,189,HLog.SPLIT_SKIP_ERRORS_DEFAULT);
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,193,LOG.warn("Master stopped while splitting logs");
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,196,List<ServerName> serverNames = new ArrayList<ServerName>();
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,198,if (!this.fs.exists(logsDirPath)) return;
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,203,.keySet();
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,207,return;
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,222,+ " belongs to an existing region server");
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,225,splitLog(serverNames);
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,228,LOG.warn("Failed splitting of " + serverNames, ioe);
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,235,Thread.sleep(conf.getInt(
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperNodeTracker.java,48,private boolean stopped = false;
src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java,69,import org.apache.hadoop.hbase.security.User;
src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java,85,import org.apache.hadoop.hbase.util.Strings;
src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java,87,import org.apache.hadoop.net.DNS;
src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java,367,if (User.isSecurityEnabled() && User.isHBaseSecurityEnabled(conf)) {
src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java,368,String machineName = Strings.domainNamePointerToHostName(
src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java,369,DNS.getDefaultHost(conf.get("hbase.thrift.dns.interface", "default"),
src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java,370,conf.get("hbase.thrift.dns.nameserver", "default")));
src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java,371,User.login(conf, "hbase.thrift.keytab.file",
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,248,throw new IllegalArgumentException("Failed resolve of " + this.isa);
src/main/java/org/apache/hadoop/hbase/master/ServerManager.java,547,HRegionInterface hri = this.serverConnections.get(sn.toString());
src/main/java/org/apache/hadoop/hbase/rest/Main.java,77,VersionInfo.logVersion();
src/main/java/org/apache/hadoop/hbase/rest/Main.java,151,if (User.isSecurityEnabled() && User.isHBaseSecurityEnabled(conf)) {
src/main/java/org/apache/hadoop/hbase/rest/Main.java,152,String machineName = Strings.domainNamePointerToHostName(
src/main/java/org/apache/hadoop/hbase/rest/Main.java,153,DNS.getDefaultHost(conf.get("hbase.rest.dns.interface", "default"),
src/main/java/org/apache/hadoop/hbase/rest/Main.java,154,conf.get("hbase.rest.dns.nameserver", "default")));
src/main/java/org/apache/hadoop/hbase/rest/Main.java,155,User.login(conf, "hbase.rest.keytab.file", "hbase.rest.kerberos.principal",
src/main/java/org/apache/hadoop/hbase/rest/Main.java,156,machineName);
src/main/java/org/apache/hadoop/hbase/client/Delete.java,85,this(row, HConstants.LATEST_TIMESTAMP, null);
src/main/java/org/apache/hadoop/hbase/zookeeper/MiniZooKeeperCluster.java,124,FileTxnLog.setPreallocSize(100);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,822,(!regionState.isPendingOpen() && !regionState.isOpening())) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,850,(!regionState.isPendingOpen() && !regionState.isOpening())) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,851,LOG.warn("Received OPENING for region " +
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,852,prettyPrintedRegionName +
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,856,return;
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,876,(!regionState.isPendingOpen() && !regionState.isOpening())) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1630,state.update(RegionState.State.PENDING_OPEN, System.currentTimeMillis(),
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1631,plan.getDestination());
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1634,RegionOpeningState regionOpenState = serverManager.sendRegionOpen(plan
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1635,.getDestination(), state.getRegion(), versionOfOfflineNode);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1636,if (regionOpenState == RegionOpeningState.ALREADY_OPENED) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,3064,public List<RegionState> processServerShutdown(final ServerName sn) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,3081,Set<HRegionInfo> deadRegions = null;
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,3082,List<RegionState> rits = new ArrayList<RegionState>();
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,3085,if (assignedRegions == null || assignedRegions.isEmpty()) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,3087,return rits;
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,3089,deadRegions = new TreeSet<HRegionInfo>(assignedRegions);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,3090,for (HRegionInfo region : deadRegions) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,3091,this.regions.remove(region);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,3098,for (RegionState region : this.regionsInTransition.values()) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,3099,if (deadRegions.remove(region.getRegion())) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,3100,rits.add(region);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,3104,return rits;
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,232,List<RegionState> regionsInTransition =
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,233,this.services.getAssignmentManager().
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,234,processServerShutdown(this.serverName);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,268,for (RegionState rit : regionsInTransition) {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,269,if (!rit.isClosing() && !rit.isPendingClose() && !rit.isSplitting()) {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,270,LOG.debug("Removed " + rit.getRegion().getRegionNameAsString() +
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,272,rit.getState());
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,273,if (hris != null) hris.remove(rit.getRegion());
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,277,assert regionsInTransition != null;
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,278,LOG.info("Reassigning " + ((hris == null)? 0: hris.size()) +
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,281,regionsInTransition.size() +
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,285,if (hris != null) {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,286,for (Map.Entry<HRegionInfo, Result> e: hris.entrySet()) {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,287,RegionState rit = this.services.getAssignmentManager().isRegionInTransition(e.getKey());
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,288,if (processDeadRegion(e.getKey(), e.getValue(),
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,289,this.services.getAssignmentManager(),
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,290,this.server.getCatalogTracker())) {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,292,.getRegionServerOfRegion(e.getKey());
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,293,if (rit != null && !rit.isClosing() && !rit.isPendingClose() && !rit.isSplitting()) {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,296,LOG.info("Skip assigning region " + rit.toString());
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,298,&& !addressFromAM.equals(this.serverName)) {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,299,LOG.debug("Skip assigning region "
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,301,+ " because it has been opened in "
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,302,+ addressFromAM.getServerName());
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,304,this.services.getAssignmentManager().assign(e.getKey(), true);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,312,HRegionInfo region = rit.getRegion();
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,313,AssignmentManager am = this.services.getAssignmentManager();
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,314,am.regionOffline(region);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,321,if (rit != null
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,324,.isDisablingOrDisabledTable(rit.getRegion().getTableNameAsString())) {
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,325,HRegionInfo hri = rit.getRegion();
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,326,AssignmentManager am = this.services.getAssignmentManager();
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,327,am.deleteClosingOrClosedNode(hri);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,328,am.regionOffline(hri);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2706,checkIfRegionInTransition(region, OPEN);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2723,LOG.info("Received request to open region: " +
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2724,region.getRegionNameAsString());
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2725,HTableDescriptor htd = null;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2726,if (htds == null) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2727,htd = this.tableDescriptors.get(region.getTableName());
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2729,htd = htds.get(region.getTableNameAsString());
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2730,if (htd == null) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2735,this.regionsInTransitionInRS.putIfAbsent(region.getEncodedNameAsBytes(),
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2736,true);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2738,if (region.isRootRegion()) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2739,this.service.submit(new OpenRootHandler(this, this, region, htd,
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2740,versionOfOfflineNode));
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2742,this.service.submit(new OpenMetaHandler(this, this, region, htd,
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2743,versionOfOfflineNode));
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2745,this.service.submit(new OpenRegionHandler(this, this, region, htd,
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2746,versionOfOfflineNode));
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2751,private void checkIfRegionInTransition(HRegionInfo region,
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2752,String currentAction) throws RegionAlreadyInTransitionException {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2753,byte[] encodedName = region.getEncodedNameAsBytes();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2754,if (this.regionsInTransitionInRS.containsKey(encodedName)) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2755,boolean openAction = this.regionsInTransitionInRS.get(encodedName);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2757,throw new RegionAlreadyInTransitionException("Received:" + currentAction +
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2760,(openAction ? OPEN : CLOSE)+ ".");
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2810,checkIfRegionInTransition(region, CLOSE);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2848,if (this.regionsInTransitionInRS.containsKey(region.getEncodedNameAsBytes())) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2849,LOG.warn("Received close for region we are already opening or closing; " +
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2850,region.getEncodedName());
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2853,this.regionsInTransitionInRS.putIfAbsent(region.getEncodedNameAsBytes(), false);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2854,CloseRegionHandler crh = null;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2855,if (region.isRootRegion()) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2856,crh = new CloseRootHandler(this, this, region, abort, zk,
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2857,versionOfClosingNode);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2859,crh = new CloseMetaHandler(this, this, region, abort, zk,
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2860,versionOfClosingNode);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2862,crh = new CloseRegionHandler(this, this, region, abort, zk,
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2863,versionOfClosingNode);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2865,this.service.submit(crh);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,3520,public ConcurrentSkipListMap<byte[], Boolean> getRegionsInTransitionInRS() {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,3521,return this.regionsInTransitionInRS;
src/main/java/org/apache/hadoop/hbase/regionserver/RegionServerServices.java,81,public Map<byte[], Boolean> getRegionsInTransitionInRS();
src/main/java/org/apache/hadoop/hbase/regionserver/handler/CloseRegionHandler.java,151,this.rsServices.getRegionsInTransitionInRS().
src/main/java/org/apache/hadoop/hbase/regionserver/handler/CloseRegionHandler.java,152,remove(this.regionInfo.getEncodedNameAsBytes());
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,56,private volatile int versionOfOfflineNode = -1;
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,65,HTableDescriptor htd, int versionOfOfflineNode) {
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,67,versionOfOfflineNode);
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,73,final int versionOfOfflineNode) {
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,78,this.versionOfOfflineNode = versionOfOfflineNode;
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,99,if (!transitionZookeeperOfflineToOpening(encodedName,
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,100,versionOfOfflineNode)) {
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,101,LOG.warn("Region was hijacked? It no longer exists, encodedName=" +
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,102,encodedName);
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,103,return;
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,143,this.rsServices.getRegionsInTransitionInRS().
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,144,remove(this.regionInfo.getEncodedNameAsBytes());
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,374,boolean transitionZookeeperOfflineToOpening(final String encodedName,
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,375,int versionOfOfflineNode) {
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,377,try {
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,379,this.version = ZKAssign.transitionNode(server.getZooKeeper(), regionInfo,
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,380,server.getServerName(), EventType.M_ZK_REGION_OFFLINE,
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,381,EventType.RS_ZK_REGION_OPENING, versionOfOfflineNode);
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,383,LOG.error("Error transition from OFFLINE to OPENING for region=" +
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,384,encodedName, e);
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,386,boolean b = isGoodVersion();
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,387,if (!b) {
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,388,LOG.warn("Failed transition from OFFLINE to OPENING for region=" +
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,389,encodedName);
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,391,return b;
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,251,if (filter.filterAllRemaining() || filter.filterRow()) {
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,256,&& !filter.filterRow()) {
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,63,private static final boolean USEMSLAB_DEFAULT = false;
src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java,82,public static String ROWKEY_COLUMN_SPEC="HBASE_ROW_KEY";
src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java,99,families = new byte[columnStrings.size()][];
src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java,100,qualifiers = new byte[columnStrings.size()][];
src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java,132,ArrayList<Integer> tabOffsets = new ArrayList<Integer>(families.length);
src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java,144,if (tabOffsets.size() > families.length) {
src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java,348,if (columns.length < 2) {
src/main/java/org/apache/hadoop/hbase/mapreduce/ImportTsv.java,349,usage("One or more columns in addition to the row key are required");
src/main/java/org/apache/hadoop/hbase/mapreduce/TsvImporterMapper.java,104,ts = conf.getLong(ImportTsv.TIMESTAMP_CONF_KEY, System.currentTimeMillis());
src/main/java/org/apache/hadoop/hbase/mapreduce/TsvImporterMapper.java,130,if (i == parser.getRowKeyColumnIndex()) continue;
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2093,System.out.println("---- Table '"  +  this.tableName
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2096,System.out.println("---- Table '"  +  this.tableName
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2105,System.out.println("---- Table '"  +  this.tableName
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2121,System.out.print(Bytes.toStringBinary(k) + ":\t");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2123,System.out.print("[ "+ r.toString() + ", "
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2126,System.out.println();
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2135,System.out.print(Bytes.toStringBinary(k) + ":\n");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2137,System.out.print("[ " + r.toString() + ", "
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2138,+ Bytes.toStringBinary(r.getEndKey()) + "]\n");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2140,System.out.println("----");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2148,System.out.println("This sidelined region dir should be bulk loaded: "
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2150,System.out.println("Bulk load command looks like: "
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2581,System.out.println("Summary:");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2584,System.out.println("Table " + tInfo.getName() + " is inconsistent.");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2586,System.out.println("  " + tInfo.getName() + " is okay.");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2588,System.out.println("    Number of regions: " + tInfo.getNumRegions());
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2589,System.out.print("    Deployed on: ");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2591,System.out.print(" " + server.toString());
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2593,System.out.println();
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3069,System.err.println("Usage: fsck [opts] {only tables}");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3070,System.err.println(" where [opts] are:");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3071,System.err.println("   -help Display help options (this)");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3072,System.err.println("   -details Display full report of all regions.");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3073,System.err.println("   -timelag <timeInSeconds>  Process only regions that " +
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3076,System.err.println("   -sleepBeforeRerun <timeInSeconds> Sleep this many seconds" +
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3078,System.err.println("   -summary Print only summary of the tables and status.");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3079,System.err.println("   -metaonly Only check the state of ROOT and META tables.");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3080,System.err.println("   -sidelineDir <hdfs://> HDFS path to backup existing meta and root.");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3083,System.err.println("   -fix              Try to fix region assignments.  This is for backwards compatiblity");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3084,System.err.println("   -fixAssignments   Try to fix region assignments.  Replaces the old -fix");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3085,System.err.println("   -fixMeta          Try to fix meta problems.  This assumes HDFS region info is good.");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3086,System.err.println("   -fixHdfsHoles     Try to fix region holes in hdfs.");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3087,System.err.println("   -fixHdfsOrphans   Try to fix region dirs with no .regioninfo file in hdfs");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3088,System.err.println("   -fixHdfsOverlaps  Try to fix region overlaps in hdfs.");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3089,System.err.println("   -fixVersionFile   Try to fix missing hbase.version file in hdfs.");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3090,System.err.println("   -maxMerge <n>     When fixing region overlaps, allow at most <n> regions to merge. (n=" + DEFAULT_MAX_MERGE +" by default)");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3091,System.err.println("   -sidelineBigOverlaps  When fixing region overlaps, allow to sideline big overlaps");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3092,System.err.println("   -maxOverlapsToSideline <n>  When fixing region overlaps, allow at most <n> regions to sideline per group. (n=" + DEFAULT_OVERLAPS_TO_SIDELINE +" by default)");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3093,System.err.println("   -fixSplitParents  Try to force offline split parents to be online.");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3094,System.err.println("");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3095,System.err.println("   -repair           Shortcut for -fixAssignments -fixMeta -fixHdfsHoles " +
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3127,System.err.println("HBaseFsck: -timelag needs a value.");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3134,System.err.println("-timelag needs a numeric value.");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3140,System.err.println("HBaseFsck: -sleepBeforeRerun needs a value.");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3146,System.err.println("-sleepBeforeRerun needs a numeric value.");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3152,System.err.println("HBaseFsck: -sidelineDir needs a value.");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3158,System.err.println("This option is deprecated, please use " +
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3199,System.err.println("-maxOverlapsToSideline needs a numeric value argument.");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3206,System.err.println("-maxOverlapsToSideline needs a numeric value argument.");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3212,System.err.println("-maxMerge needs a numeric value argument.");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3219,System.err.println("-maxMerge needs a numeric value argument.");
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3228,System.err.println("Unrecognized option:" + cmd);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3232,System.out.println("Allow checking/fixes for table: " + cmd);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3272,public static void debugLsr(Configuration conf, Path p) throws IOException {
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3282,System.out.println(p);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3291,debugLsr(conf, status.getPath());
src/main/java/org/apache/hadoop/hbase/regionserver/handler/OpenRegionHandler.java,362,private void cleanupFailedOpen(final HRegion region) throws IOException {
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,660,if (clusterId.hasId()) {
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,661,conf.set(HConstants.CLUSTER_ID, clusterId.getId());
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1287,if (clusterId.hasId()) {
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1288,conf.set(HConstants.CLUSTER_ID, clusterId.getId());
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,738,throws InterruptedException {
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,739,if (isSecureZooKeeper(zkw.getConfiguration())) {
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,740,LOG.debug("Waiting for ZooKeeperWatcher to authenticate");
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,741,zkw.saslLatch.await();
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,742,LOG.debug("Done waiting.");
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,772,waitForZKConnectionIfAuthenticating(zkw);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,812,waitForZKConnectionIfAuthenticating(zkw);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,850,waitForZKConnectionIfAuthenticating(zkw);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,878,try {
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,879,waitForZKConnectionIfAuthenticating(zkw);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,880,zkw.getRecoverableZooKeeper().getZooKeeper().create(znode, data,
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,881,createACL(zkw, znode), CreateMode.PERSISTENT, cb, ctx);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,883,zkw.interruptedException(e);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,902,waitForZKConnectionIfAuthenticating(zkw);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,940,waitForZKConnectionIfAuthenticating(zkw);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperNodeTracker.java,74,try {
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperNodeTracker.java,75,ZKUtil.waitForZKConnectionIfAuthenticating(watcher);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperNodeTracker.java,77,throw new IllegalStateException("ZookeeperNodeTracker on " + this.node
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperNodeTracker.java,78,+ " interuppted while waiting for SASL Authentication", e);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java,76,public CountDownLatch saslLatch = new CountDownLatch(1);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java,342,case SaslAuthenticated:
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java,343,if (ZKUtil.isSecureZooKeeper(this.conf)) {
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java,345,saslLatch.countDown();
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java,347,break;
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java,349,case AuthFailed:
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java,350,if (ZKUtil.isSecureZooKeeper(this.conf)) {
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java,354,saslLatch.countDown();
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java,356,break;
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java,364,if (ZKUtil.isSecureZooKeeper(this.conf)) {
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java,368,saslLatch.countDown();
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,1299,List<Entry> pending = logSyncerThread.getPendingWrites();
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,1303,synchronized (this.updateLock) {
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,1383,if (synchronous == false) {
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,1384,return balanceSwitch(on);
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,1386,return getMaster().synchronousBalanceSwitch(on);
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,237,isInSafeMode = dfs.setSafeMode(org.apache.hadoop.hdfs.protocol.FSConstants.SafeModeAction.SAFEMODE_GET);
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,524,while (dfs.setSafeMode(org.apache.hadoop.hdfs.protocol.FSConstants.SafeModeAction.SAFEMODE_GET)) {
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,415,if (connections.get(remoteId) == this) {
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,416,connections.remove(remoteId);
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,378,call.setValue(value);
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,380,call.setException(new RemoteException(WritableUtils.readString(in),
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,381,WritableUtils.readString(in)));
src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogReader.java,254,return reader.getPosition();
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,1325,private void updateCallQueueLenMetrics(BlockingQueue<Call> queue) {
src/main/java/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.java,206,hostName = DNS.reverseDns(ipAddress, this.nameServer);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,480,public boolean isMetaRegion(byte[] regionName) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,487,return region.getRegionInfo().isMetaRegion();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,514,if (scanner != null && scanner.getRegionInfo().isMetaRegion()) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,522,if (isMetaRegion((byte[]) inv.getParameters()[0])) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,539,if (isMetaRegion(region)) {
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,113,static final int HEADER_SIZE = HEADER_SIZE_NO_CHECKSUM + Bytes.SIZEOF_BYTE +
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,120,public static final int ENCODED_HEADER_SIZE = HEADER_SIZE
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,124,static final byte[] DUMMY_HEADER = new byte[HEADER_SIZE];
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,750,if (bytesPerChecksum < HEADER_SIZE) {
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,783,baosInMemory.write(DUMMY_HEADER);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,853,compressedByteStream.write(DUMMY_HEADER);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,857,compressionStream.write(uncompressedBytesWithHeader, HEADER_SIZE,
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,858,uncompressedBytesWithHeader.length - HEADER_SIZE);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,872,putHeader(onDiskBytesWithHeader, 0, onDiskBytesWithHeader.length,
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,886,putHeader(uncompressedBytesWithHeader, 0,
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,887,onDiskBytesWithHeader.length + onDiskChecksum.length,
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,888,uncompressedBytesWithHeader.length, onDiskDataSizeWithHeader);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,902,putHeader(uncompressedBytesWithHeader, 0,
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,903,onDiskBytesWithHeader.length + onDiskChecksum.length,
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,904,uncompressedBytesWithHeader.length, onDiskDataSizeWithHeader);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,924,HEADER_SIZE, uncompressedBytesWithHeader.length -
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,925,HEADER_SIZE).slice();
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,928,includesMemstoreTS, DUMMY_HEADER);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,941,uncompressedBytesWithHeader.length - HEADER_SIZE) {
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,944,+ (uncompressedBytesWithHeader.length - HEADER_SIZE));
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,957,private void putHeader(byte[] dest, int offset, int onDiskSize,
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,958,int uncompressedSize, int onDiskDataSize) {
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,960,offset = Bytes.putInt(dest, offset, onDiskSize - HEADER_SIZE);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,961,offset = Bytes.putInt(dest, offset, uncompressedSize - HEADER_SIZE);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,1000,if (compressAlgo == NONE) {
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,1063,return onDiskBytesWithHeader.length + onDiskChecksum.length - HEADER_SIZE;
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,1083,return uncompressedBytesWithHeader.length - HEADER_SIZE;
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,1159,includesMemstoreTS, MINOR_VERSION_WITH_CHECKSUM,
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,1537,byte[] header = new byte[HEADER_SIZE];
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,1538,ByteBuffer buf = ByteBuffer.wrap(header, 0, HEADER_SIZE);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,1604,this(istream, istream, compressAlgo, fileSize,
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,2071,return HEADER_SIZE;
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlock.java,2088,return DUMMY_HEADER;
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java,128,includeMemstoreTS, checksumType, bytesPerChecksum);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java,367,FixedFileTrailer trailer = new FixedFileTrailer(2,
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileWriterV2.java,368,HFileReaderV2.MAX_MINOR_VERSION);
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/SchemaConfigured.java,77,if (conf != null) {
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/SchemaConfigured.java,78,SchemaMetrics.configureGlobally(conf);
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/SchemaMetrics.java,516,final boolean useTableNameNew =
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/SchemaMetrics.java,517,conf.getBoolean(SHOW_TABLE_NAME_CONF_KEY, false);
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/SchemaMetrics.java,518,setUseTableName(useTableNameNew);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,674,return internalFlushCache(
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,675,snapshot, logCacheFlushId, snapshotTimeRangeTracker, flushedSize, status);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,778,validateStoreFile(path);
src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java,99,private transient CompressionCodec lzoCodec;
src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java,104,try {
src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java,105,Class<?> externalCodec =
src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java,106,getClassLoaderForCodec().loadClass("com.hadoop.compression.lzo.LzoCodec");
src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java,107,lzoCodec = (CompressionCodec) ReflectionUtils.newInstance(externalCodec,
src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java,108,new Configuration(conf));
src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java,110,throw new RuntimeException(e);
src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java,117,private transient GzipCodec codec;
src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java,122,codec = new ReusableStreamGzipCodec();
src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java,123,codec.setConf(new Configuration(conf));
src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java,164,private transient CompressionCodec snappyCodec;
src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java,167,CompressionCodec getCodec(Configuration conf) {
src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java,168,if (snappyCodec == null) {
src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java,169,try {
src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java,170,Class<?> externalCodec =
src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java,171,getClassLoaderForCodec().loadClass("org.apache.hadoop.io.compress.SnappyCodec");
src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java,172,snappyCodec = (CompressionCodec) ReflectionUtils.newInstance(externalCodec,
src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java,173,conf);
src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java,175,throw new RuntimeException(e);
src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java,178,return snappyCodec;
src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java,183,private transient CompressionCodec lz4Codec;
src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java,188,try {
src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java,189,Class<?> externalCodec =
src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java,190,getClassLoaderForCodec().loadClass("org.apache.hadoop.io.compress.Lz4Codec");
src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java,191,lz4Codec = (CompressionCodec) ReflectionUtils.newInstance(externalCodec,
src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java,192,conf);
src/main/java/org/apache/hadoop/hbase/io/hfile/Compression.java,194,throw new RuntimeException(e);
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1815,HTableDescriptor hTableDescriptor = null;
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1820,hTableDescriptor = htd;
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1825,if (hTableDescriptor == null) {
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1826,throw new TableNotFoundException(Bytes.toString(tableName));
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1828,return hTableDescriptor;
src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java,124,private final int defaultTimeout;
src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java,163,this(zk, conf, abortable,
src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java,164,conf.getInt("hbase.catalogtracker.default.timeout", 1000));
src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java,180,public CatalogTracker(final ZooKeeperWatcher zk, final Configuration conf,
src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java,182,throws IOException {
src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java,183,this(zk, conf, HConnectionManager.getConnection(conf), abortable, defaultTimeout);
src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java,225,this.defaultTimeout = defaultTimeout;
src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java,382,throws NotAllMetaRegionsOnlineException, IOException {
src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java,383,try {
src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java,384,return getRootServerConnection(this.defaultTimeout);
src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java,386,throw new NotAllMetaRegionsOnlineException("Interrupted");
src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java,479,long stop = System.currentTimeMillis() + timeout;
src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java,482,while(!stopped && (timeout == 0 || System.currentTimeMillis() < stop)) {
src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java,522,throws NotAllMetaRegionsOnlineException, IOException {
src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java,523,try {
src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java,524,return getCachedConnection(waitForMeta(defaultTimeout));
src/main/java/org/apache/hadoop/hbase/catalog/CatalogTracker.java,526,throw new NotAllMetaRegionsOnlineException("Interrupted");
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,412,this.catalogTracker = new CatalogTracker(this.zooKeeper, this.conf,
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,413,this, conf.getInt("hbase.master.catalog.timeout", Integer.MAX_VALUE));
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,600,this.catalogTracker = new CatalogTracker(this.zooKeeper, this.conf,
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,601,this, this.conf.getInt("hbase.regionserver.catalog.timeout", Integer.MAX_VALUE));
src/main/java/org/apache/hadoop/hbase/HConstants.java,22,import java.util.ArrayList;
src/main/java/org/apache/hadoop/hbase/util/FSUtils.java,967,if (d.getName().equals(HConstants.HREGION_LOGDIR_NAME)) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1173,ZKUtil.listChildrenAndWatchThem(watcher,
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,177,FileSystem fs = path.getFileSystem(HBaseConfiguration.create());
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,178,Path dst = new Path(System.getProperty("java.io.tmpdir") +
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,179,java.io.File.separator +"." + pathPrefix +
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,181,fs.copyToLocalFile(path, dst);
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,204,File file = new File(System.getProperty("java.io.tmpdir") +
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,205,java.io.File.separator +"." + pathPrefix +
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,188,LOG.info("Server " + serverName +
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,190,this.services.getAssignmentManager().
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,191,regionOffline(HRegionInfo.ROOT_REGIONINFO);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,192,verifyAndAssignRootWithRetries();
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,197,LOG.info("Server " + serverName +
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,199,this.services.getAssignmentManager().
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,200,regionOffline(HRegionInfo.FIRST_META_REGIONINFO);
src/main/java/org/apache/hadoop/hbase/master/handler/ServerShutdownHandler.java,201,this.services.getAssignmentManager().assignMeta();
security/src/main/java/org/apache/hadoop/hbase/security/HBasePolicyProvider.java,47,conf.set("hadoop.policy.file", "hbase-policy.xml");
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,391,Future<StoreFile> future = completionService.take();
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,392,StoreFile storeFile = future.get();
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,393,long length = storeFile.getReader().length();
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,394,this.storeSize += length;
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,395,this.totalUncompressedBytes +=
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,396,storeFile.getReader().getTotalUncompressedBytes();
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,397,if (LOG.isDebugEnabled()) {
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,398,LOG.debug("loaded " + storeFile.toStringDetailed());
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,400,results.add(storeFile);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,403,throw new IOException(e);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,405,throw new IOException(e.getCause());
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,629,Future<Void> future = completionService.take();
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,630,future.get();
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,633,throw new IOException(e);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,635,throw new IOException(e.getCause());
src/main/java/org/apache/hadoop/hbase/regionserver/StoreFile.java,602,this.reader = open();
src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java,123,throw new NullPointerException("empty hosts");
src/main/java/org/apache/hadoop/hbase/HDFSBlocksDistribution.java,147,throw new NullPointerException("Passed hostname is null");
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,290,try {
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,344,10 * 1000), c.getInt(HConstants.VERSION_FILE_WRITE_ATTEMPTS,
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,352,10 * 1000), c.getInt(HConstants.VERSION_FILE_WRITE_ATTEMPTS,
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureRpcEngine.java,77,private Map<SocketFactory, SecureClient> clients =
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureRpcEngine.java,78,new HashMap<SocketFactory, SecureClient>();
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureRpcEngine.java,97,SecureClient client = clients.get(factory);
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureRpcEngine.java,101,clients.put(factory, client);
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureRpcEngine.java,128,clients.remove(client.getSocketFactory());
src/main/java/org/apache/hadoop/hbase/rest/ExistsResource.java,27,import javax.ws.rs.WebApplicationException;
src/main/java/org/apache/hadoop/hbase/rest/ExistsResource.java,61,throw new WebApplicationException(Response.Status.NOT_FOUND);
src/main/java/org/apache/hadoop/hbase/rest/ExistsResource.java,64,throw new WebApplicationException(Response.Status.SERVICE_UNAVAILABLE);
src/main/java/org/apache/hadoop/hbase/rest/MultiRowResource.java,32,import javax.ws.rs.WebApplicationException;
src/main/java/org/apache/hadoop/hbase/rest/MultiRowResource.java,77,ResultGenerator generator = ResultGenerator.fromRowSpec(this.tableResource.getName(), rowSpec, null);
src/main/java/org/apache/hadoop/hbase/rest/MultiRowResource.java,79,throw new WebApplicationException(Response.Status.NOT_FOUND);
src/main/java/org/apache/hadoop/hbase/rest/MultiRowResource.java,96,throw new WebApplicationException(e,
src/main/java/org/apache/hadoop/hbase/rest/MultiRowResource.java,97,Response.Status.SERVICE_UNAVAILABLE);
src/main/java/org/apache/hadoop/hbase/rest/RegionsResource.java,29,import javax.ws.rs.WebApplicationException;
src/main/java/org/apache/hadoop/hbase/rest/RegionsResource.java,107,throw new WebApplicationException(Response.Status.NOT_FOUND);
src/main/java/org/apache/hadoop/hbase/rest/RegionsResource.java,110,throw new WebApplicationException(e,
src/main/java/org/apache/hadoop/hbase/rest/RegionsResource.java,111,Response.Status.SERVICE_UNAVAILABLE);
src/main/java/org/apache/hadoop/hbase/rest/RootResource.java,29,import javax.ws.rs.WebApplicationException;
src/main/java/org/apache/hadoop/hbase/rest/RootResource.java,87,throw new WebApplicationException(e,
src/main/java/org/apache/hadoop/hbase/rest/RootResource.java,88,Response.Status.SERVICE_UNAVAILABLE);
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,33,import javax.ws.rs.WebApplicationException;
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,92,throw new WebApplicationException(Response.Status.NOT_FOUND);
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,116,servlet.getMetrics().incrementFailedGetRequests(1);
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,117,throw new WebApplicationException(e,
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,118,Response.Status.SERVICE_UNAVAILABLE);
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,132,throw new WebApplicationException(Response.Status.BAD_REQUEST);
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,138,throw new WebApplicationException(Response.Status.NOT_FOUND);
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,147,throw new WebApplicationException(e,
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,148,Response.Status.SERVICE_UNAVAILABLE);
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,155,throw new WebApplicationException(Response.Status.FORBIDDEN);
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,177,throw new WebApplicationException(Response.Status.BAD_REQUEST);
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,189,throw new WebApplicationException(Response.Status.BAD_REQUEST);
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,211,throw new WebApplicationException(e,
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,212,Response.Status.SERVICE_UNAVAILABLE);
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,225,throw new WebApplicationException(Response.Status.FORBIDDEN);
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,250,throw new WebApplicationException(Response.Status.BAD_REQUEST);
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,268,throw new WebApplicationException(e,
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,269,Response.Status.SERVICE_UNAVAILABLE);
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,326,throw new WebApplicationException(Response.Status.FORBIDDEN);
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,361,throw new WebApplicationException(e,
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,362,Response.Status.SERVICE_UNAVAILABLE);
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,383,throw new WebApplicationException(Response.Status.BAD_REQUEST);
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,395,throw new WebApplicationException(Response.Status.BAD_REQUEST);
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,415,throw new WebApplicationException(Response.Status.BAD_REQUEST);
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,418,throw new WebApplicationException(Response.Status.BAD_REQUEST);
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,434,throw new WebApplicationException(e, Response.Status.SERVICE_UNAVAILABLE);
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,455,throw new WebApplicationException(Response.Status.BAD_REQUEST);
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,463,throw new WebApplicationException(Response.Status.BAD_REQUEST);
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,473,throw new WebApplicationException(Response.Status.BAD_REQUEST);
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,480,throw new WebApplicationException(Response.Status.BAD_REQUEST);
src/main/java/org/apache/hadoop/hbase/rest/RowResource.java,497,throw new WebApplicationException(e, Response.Status.SERVICE_UNAVAILABLE);
src/main/java/org/apache/hadoop/hbase/rest/ScannerInstanceResource.java,29,import javax.ws.rs.WebApplicationException;
src/main/java/org/apache/hadoop/hbase/rest/ScannerInstanceResource.java,57,ResultGenerator generator;
src/main/java/org/apache/hadoop/hbase/rest/ScannerInstanceResource.java,58,String id;
src/main/java/org/apache/hadoop/hbase/rest/ScannerInstanceResource.java,95,throw new WebApplicationException(Response.Status.GONE);
src/main/java/org/apache/hadoop/hbase/rest/ScannerInstanceResource.java,164,throw new WebApplicationException(Response.Status.GONE);
src/main/java/org/apache/hadoop/hbase/rest/ScannerInstanceResource.java,175,throw new WebApplicationException(Response.Status.FORBIDDEN);
src/main/java/org/apache/hadoop/hbase/rest/ScannerResource.java,34,import javax.ws.rs.WebApplicationException;
src/main/java/org/apache/hadoop/hbase/rest/ScannerResource.java,80,throw new WebApplicationException(Response.Status.FORBIDDEN);
src/main/java/org/apache/hadoop/hbase/rest/ScannerResource.java,103,servlet.getMetrics().incrementFailedPutRequests(1);
src/main/java/org/apache/hadoop/hbase/rest/ScannerResource.java,104,throw new WebApplicationException(e,
src/main/java/org/apache/hadoop/hbase/rest/ScannerResource.java,105,Response.Status.SERVICE_UNAVAILABLE);
src/main/java/org/apache/hadoop/hbase/rest/ScannerResource.java,109,throw new WebApplicationException(e, Response.Status.NOT_FOUND);
src/main/java/org/apache/hadoop/hbase/rest/ScannerResource.java,111,throw new WebApplicationException(e, Response.Status.BAD_REQUEST);
src/main/java/org/apache/hadoop/hbase/rest/ScannerResource.java,114,throw new WebApplicationException(e, Response.Status.BAD_REQUEST);
src/main/java/org/apache/hadoop/hbase/rest/ScannerResource.java,140,final @PathParam("scanner") String id) {
src/main/java/org/apache/hadoop/hbase/rest/ScannerResource.java,144,throw new WebApplicationException(Response.Status.NOT_FOUND);
src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java,32,import javax.ws.rs.WebApplicationException;
src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java,103,throw new WebApplicationException(Response.Status.NOT_FOUND);
src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java,106,throw new WebApplicationException(e,
src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java,107,Response.Status.SERVICE_UNAVAILABLE);
src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java,114,throw new WebApplicationException(Response.Status.FORBIDDEN);
src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java,138,throw new WebApplicationException(e, Response.Status.NOT_MODIFIED);
src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java,142,throw new WebApplicationException(e,
src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java,143,Response.Status.SERVICE_UNAVAILABLE);
src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java,150,throw new WebApplicationException(Response.Status.FORBIDDEN);
src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java,168,throw new WebApplicationException(e,
src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java,169,Response.Status.INTERNAL_SERVER_ERROR);
src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java,176,throw new WebApplicationException(e,
src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java,177,Response.Status.SERVICE_UNAVAILABLE);
src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java,193,throw new WebApplicationException(e,
src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java,194,Response.Status.SERVICE_UNAVAILABLE);
src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java,243,throw new WebApplicationException(Response.Status.NOT_FOUND);
src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java,246,throw new WebApplicationException(e,
src/main/java/org/apache/hadoop/hbase/rest/SchemaResource.java,247,Response.Status.SERVICE_UNAVAILABLE);
src/main/java/org/apache/hadoop/hbase/rest/StorageClusterStatusResource.java,27,import javax.ws.rs.WebApplicationException;
src/main/java/org/apache/hadoop/hbase/rest/StorageClusterStatusResource.java,104,throw new WebApplicationException(e,
src/main/java/org/apache/hadoop/hbase/rest/StorageClusterStatusResource.java,105,Response.Status.SERVICE_UNAVAILABLE);
src/main/java/org/apache/hadoop/hbase/rest/StorageClusterVersionResource.java,27,import javax.ws.rs.WebApplicationException;
src/main/java/org/apache/hadoop/hbase/rest/StorageClusterVersionResource.java,76,throw new WebApplicationException(e,
src/main/java/org/apache/hadoop/hbase/rest/StorageClusterVersionResource.java,77,Response.Status.SERVICE_UNAVAILABLE);
src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeer.java,82,ZKUtil.createAndWatch(zookeeper, peerStateNode,
src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeer.java,83,Bytes.toBytes(PeerState.ENABLED.name())); // enabled by default
src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java,401,ZKUtil.createAndWatch(this.zookeeper,
src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java,402,ZKUtil.joinZNode(this.peersZNode, id), Bytes.toBytes(clusterKey));
src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java,403,ZKUtil.createAndWatch(this.zookeeper, getPeerStateNode(id),
src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java,404,Bytes.toBytes(PeerState.ENABLED.name())); // enabled by default
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,36,import org.apache.hadoop.hbase.io.hfile.Compression;
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,37,import org.apache.hadoop.hbase.regionserver.StoreFile;
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,602,return Long.valueOf(Bytes.toString(value)).longValue();
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,603,return HConstants.DEFAULT_MAX_FILE_SIZE;
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,635,return Long.valueOf(Bytes.toString(value)).longValue();
src/main/java/org/apache/hadoop/hbase/HTableDescriptor.java,636,return DEFAULT_MEMSTORE_FLUSH_SIZE;
src/main/java/org/apache/hadoop/hbase/regionserver/ConstantSizeRegionSplitPolicy.java,37,long maxFileSize = region.getTableDesc().getMaxFileSize();
src/main/java/org/apache/hadoop/hbase/regionserver/ConstantSizeRegionSplitPolicy.java,40,if (maxFileSize == HConstants.DEFAULT_MAX_FILE_SIZE) {
src/main/java/org/apache/hadoop/hbase/regionserver/ConstantSizeRegionSplitPolicy.java,41,maxFileSize = getConf().getLong(HConstants.HREGION_MAX_FILESIZE,
src/main/java/org/apache/hadoop/hbase/regionserver/ConstantSizeRegionSplitPolicy.java,44,this.desiredMaxFileSize = maxFileSize;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,430,if (flushSize == HTableDescriptor.DEFAULT_MEMSTORE_FLUSH_SIZE) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,432,HTableDescriptor.DEFAULT_MEMSTORE_FLUSH_SIZE);
src/main/java/org/apache/hadoop/hbase/regionserver/IncreasingToUpperBoundRegionSplitPolicy.java,48,this.flushSize = region.getTableDesc() != null?
src/main/java/org/apache/hadoop/hbase/regionserver/IncreasingToUpperBoundRegionSplitPolicy.java,49,region.getTableDesc().getMemStoreFlushSize():
src/main/java/org/apache/hadoop/hbase/regionserver/IncreasingToUpperBoundRegionSplitPolicy.java,50,getConf().getLong(HConstants.HREGION_MEMSTORE_FLUSH_SIZE,
src/main/java/org/apache/hadoop/hbase/mapreduce/Import.java,140,cfRenameMap = createCfRenameMap(context.getConfiguration());
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,101,this.useSasl = User.isSecurityEnabled();
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureServer.java,674,this.isSecurityEnabled = UserGroupInformation.isSecurityEnabled();
src/main/java/org/apache/hadoop/hbase/security/User.java,224,return "kerberos".equalsIgnoreCase(conf.get(HBASE_SECURITY_CONF_KEY));
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java,296,midKey = blockKeys[(rootCount - 1) / 2];
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,163,try {
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,164,implClass = getClass().getClassLoader().loadClass(className);
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,166,LOG.info("Class " + className + " needs to be loaded from a file - " +
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,197,paths.add(new File(dst.toString()).getCanonicalFile().toURL());
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,218,this.getClass().getClassLoader());
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,219,Thread.currentThread().setContextClassLoader(cl);
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,223,throw new IOException(e);
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,227,return loadInstance(implClass, priority, conf);
src/main/java/org/apache/hadoop/hbase/catalog/MetaReader.java,435,if (this.current.isSplitParent()) return true;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3404,results.clear();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3408,outResults.addAll(results);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3409,resetFilters();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3410,if (isFilterDone()) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3411,return false;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3413,return returnResult;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,5080,private void closeRegionOperation(){
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2389,for (int i = 0; i < nbRows
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2390,&& currentScanResultSize < maxScannerResultSize; i++) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2391,requestCount.incrementAndGet();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2394,if (!values.isEmpty()) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2395,for (KeyValue kv : values) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2396,currentScanResultSize += kv.heapSize();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2398,results.add(new Result(values));
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2400,if (!moreRows) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2401,break;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2403,values.clear();
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,144,Set<String> enablingTables = new HashSet<String>(1);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,354,recoverTableInEnablingState(this.enablingTables, isWatcherCreated);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2253,Set<String> disablingAndDisabledTables = new HashSet<String>(this.disablingTables);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2254,disablingAndDisabledTables.addAll(this.zkTable.getDisabledTables());
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2257,disablingAndDisabledTables, true);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2505,if (false == checkIfRegionsBelongsToEnabling(regionInfo)) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2506,LOG.warn("Region " + regionInfo.getEncodedName() +
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2510,addTheTablesInPartialState(this.disablingTables, this.enablingTables, regionInfo,
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2511,tableName);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2522,disablingOrEnabling = addTheTablesInPartialState(this.disablingTables,
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2523,this.enablingTables, regionInfo, tableName);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2545,&& false == checkIfRegionsBelongsToEnabling(regionInfo)) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2551,disablingOrEnabling = addTheTablesInPartialState(this.disablingTables,
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2552,this.enablingTables, regionInfo, tableName);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2553,disabled = checkIfRegionBelongsToDisabled(regionInfo);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2571,private Boolean addTheTablesInPartialState(Set<String> disablingTables,
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2572,Set<String> enablingTables, HRegionInfo regionInfo,
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2573,String disablingTableName) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2575,disablingTables.add(disablingTableName);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2578,enablingTables.add(disablingTableName);
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,105,private void handleEnableTable() throws IOException, KeeperException {
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,114,List<HRegionInfo> regionsInMeta;
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,115,regionsInMeta = MetaReader.getTableRegions(this.ct, tableName, true);
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,116,int countOfRegionsInTable = regionsInMeta.size();
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,117,List<HRegionInfo> regions = regionsToAssign(regionsInMeta);
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,124,BulkEnabler bd = new BulkEnabler(this.server, regions,
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,125,countOfRegionsInTable);
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,147,private List<HRegionInfo> regionsToAssign(
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,149,throws IOException {
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,150,final List<HRegionInfo> onlineRegions =
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,151,this.assignmentManager.getRegionsOfTable(tableName);
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,152,regionsInMeta.removeAll(onlineRegions);
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,153,return regionsInMeta;
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,165,final int countOfRegionsInTable) {
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,176,if (!roundRobinAssignment) {
src/main/java/org/apache/hadoop/hbase/master/handler/EnableTableHandler.java,184,assignmentManager.assign(hri, true);
src/main/java/org/apache/hadoop/hbase/KeyValue.java,220,private volatile byte [] rowCache = null;
src/main/java/org/apache/hadoop/hbase/KeyValue.java,967,timestampCache = -1L;
src/main/java/org/apache/hadoop/hbase/KeyValue.java,1017,if (rowCache == null) {
src/main/java/org/apache/hadoop/hbase/KeyValue.java,1018,int o = getRowOffset();
src/main/java/org/apache/hadoop/hbase/KeyValue.java,1019,short l = getRowLength();
src/main/java/org/apache/hadoop/hbase/KeyValue.java,1022,byte local[] = new byte[l];
src/main/java/org/apache/hadoop/hbase/KeyValue.java,1023,System.arraycopy(getBuffer(), o, local, 0, l);
src/main/java/org/apache/hadoop/hbase/KeyValue.java,1024,rowCache = local; // volatile assign
src/main/java/org/apache/hadoop/hbase/KeyValue.java,1026,return rowCache;
src/main/java/org/apache/hadoop/hbase/KeyValue.java,1033,private long timestampCache = -1;
src/main/java/org/apache/hadoop/hbase/KeyValue.java,1035,if (timestampCache == -1) {
src/main/java/org/apache/hadoop/hbase/KeyValue.java,1036,timestampCache = getTimestamp(getKeyLength());
src/main/java/org/apache/hadoop/hbase/KeyValue.java,1038,return timestampCache;
src/main/java/org/apache/hadoop/hbase/KeyValue.java,2240,return ClassSize.align(ClassSize.OBJECT + (2 * ClassSize.REFERENCE) +
src/main/java/org/apache/hadoop/hbase/KeyValue.java,2241,ClassSize.align(ClassSize.ARRAY) + ClassSize.align(length) +
src/main/java/org/apache/hadoop/hbase/KeyValue.java,2242,(3 * Bytes.SIZEOF_INT) +
src/main/java/org/apache/hadoop/hbase/KeyValue.java,2243,ClassSize.align(ClassSize.ARRAY) +
src/main/java/org/apache/hadoop/hbase/KeyValue.java,2244,(2 * Bytes.SIZEOF_LONG));
src/main/java/org/apache/hadoop/hbase/KeyValue.java,2251,this.rowCache = null;
src/main/java/org/apache/hadoop/hbase/client/Result.java,99,this(kvs.toArray(new KeyValue[0]));
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3445,if (isStopRow(currentRow)) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3455,nextRow(currentRow);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3457,byte [] nextRow;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3469,final boolean stopRow = isStopRow(nextRow);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3484,nextRow(currentRow);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3500,private boolean filterRowKey(byte[] row) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3502,&& filter.filterRowKey(row, 0, row.length);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3505,protected void nextRow(byte [] currentRow) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3506,while (Bytes.equals(currentRow, peekRow())) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3507,this.storeHeap.next(MOCKED_LIST);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3513,private byte[] peekRow() {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3514,KeyValue kv = this.storeHeap.peek();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3515,return kv == null ? null : kv.getRow();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3518,private boolean isStopRow(byte [] currentRow) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3522,currentRow, 0, currentRow.length) <= isScan);
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,227,int ret = this.rowComparator.compareRows(row, 0, row.length,
src/main/java/org/apache/hadoop/hbase/regionserver/ScanQueryMatcher.java,386,public void setRow(byte [] row) {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,330,if ((matcher.row == null) || !peeked.matchingRow(matcher.row)) {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,331,matcher.setRow(peeked.getRow());
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,508,if ((matcher.row == null) || !kv.matchingRow(matcher.row)) {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,510,matcher.setRow(kv.getRow());
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4762,(5 * Bytes.SIZEOF_LONG) +
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1263,lock.readLock().lock();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,1776,updatesLock.readLock().lock();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2139,this.updatesLock.readLock().lock();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2433,private void checkResources() {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2452,wait(threadWakeFrequency);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2516,this.updatesLock.readLock().lock();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4250,this.updatesLock.readLock().lock();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4405,this.updatesLock.readLock().lock();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4556,this.updatesLock.readLock().lock();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4669,this.updatesLock.readLock().lock();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,5063,private void startRegionOperation() throws NotServingRegionException {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,5068,lock.readLock().lock();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,5092,throws NotServingRegionException {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,5097,if (writeLockNeeded) lock.writeLock().lock();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,5098,else lock.readLock().lock();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,5111,private void closeBulkRegionOperation(){
src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java,341,return;
src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java,113,STARTED_SPLITTING,
src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java,239,this.journal.add(JournalEntry.STARTED_SPLITTING);
src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java,244,this.znodeVersion = createNodeSplitting(server.getZooKeeper(),
src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java,247,throw new IOException("Failed setting SPLITTING znode on " +
src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java,738,case STARTED_SPLITTING:
src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java,739,if (server != null && server.getZooKeeper() != null) {
src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java,740,cleanZK(server, this.parent.getRegionInfo(), false);
src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java,742,break;
src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java,746,cleanZK(server, this.parent.getRegionInfo(), true);
src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java,839,private static void cleanZK(final Server server, final HRegionInfo hri, boolean abort) {
src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java,845,if (abort) {
src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java,846,server.abort("Failed cleanup of " + hri.getRegionNameAsString(), nn);
src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java,867,int createNodeSplitting(final ZooKeeperWatcher zkw, final HRegionInfo region,
src/main/java/org/apache/hadoop/hbase/regionserver/SplitTransaction.java,881,return transitionNodeSplitting(zkw, region, serverName, -1);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4997,if (getRegionInfo().isMetaRegion()) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4999,LOG.warn("Cannot split meta regions in HBase 0.20 and above");
src/main/java/org/apache/hadoop/hbase/filter/KeyOnlyFilter.java,51,Preconditions.checkArgument(filterArguments.size() == 0,
src/main/java/org/apache/hadoop/hbase/filter/KeyOnlyFilter.java,53,return new KeyOnlyFilter();
src/main/java/org/apache/hadoop/hbase/master/ServerManager.java,572,getLong("hbase.master.wait.on.regionservers.interval", 1500);
src/main/java/org/apache/hadoop/hbase/master/ServerManager.java,574,getLong("hbase.master.wait.on.regionservers.timeout", 4500);
src/main/java/org/apache/hadoop/hbase/master/ServerManager.java,575,final int minToStart = this.master.getConfiguration().
src/main/java/org/apache/hadoop/hbase/master/ServerManager.java,576,getInt("hbase.master.wait.on.regionservers.mintostart", 1);
src/main/java/org/apache/hadoop/hbase/master/ServerManager.java,577,final int maxToStart = this.master.getConfiguration().
src/main/java/org/apache/hadoop/hbase/master/ServerManager.java,578,getInt("hbase.master.wait.on.regionservers.maxtostart", Integer.MAX_VALUE);
src/main/java/org/apache/hadoop/hbase/master/ServerManager.java,589,slept < timeout &&
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,638,if (!this.listeners.isEmpty()) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,639,for (WALActionsListener i : this.listeners) {
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,640,i.postLogRoll(oldPath, newPath);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/Replication.java,195,getReplicationManager().logRolled(newPath);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,303,if(readAllEntriesToReplicateOrNextFile()) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,356,this.peerClusterZnode, this.position, queueRecovered);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,365,shipEdits();
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,385,protected boolean readAllEntriesToReplicateOrNextFile() throws IOException{
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,592,protected void shipEdits() {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,611,this.peerClusterZnode, this.position, queueRecovered);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java,149,public void logPositionAndCleanOldLogs(Path log, String id, long position, boolean queueRecovered) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSourceManager.java,254,void logRolled(Path newLog) throws IOException {
src/main/java/org/apache/hadoop/hbase/HConstants.java,589,public static final List<String> HBASE_NON_USER_TABLE_DIRS = new ArrayList<String>(
src/main/java/org/apache/hadoop/hbase/HConstants.java,590,Arrays.asList(new String[]{ HREGION_LOGDIR_NAME, HREGION_OLDLOGDIR_NAME,
src/main/java/org/apache/hadoop/hbase/HConstants.java,591,CORRUPT_DIR_NAME, Bytes.toString(META_TABLE_NAME),
src/main/java/org/apache/hadoop/hbase/HConstants.java,592,Bytes.toString(ROOT_TABLE_NAME), SPLIT_LOGDIR_NAME,
src/main/java/org/apache/hadoop/hbase/HConstants.java,593,HBCK_SIDELINEDIR_NAME }));
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,42,import org.apache.hadoop.hbase.TableExistsException;
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,46,import org.apache.hadoop.hbase.regionserver.HRegion;
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,249,Path rootdir = this.services.getMasterFileSystem().getRootDir();
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,250,HRegion.deleteRegion(fs, rootdir, parent);
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,838,new LogCleaner(conf.getInt("hbase.master.cleaner.interval", 60 * 1000),
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,20,package org.apache.hadoop.hbase.master;
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,22,import java.io.IOException;
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,23,import java.util.LinkedList;
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,24,import java.util.List;
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,26,import org.apache.commons.logging.Log;
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,27,import org.apache.commons.logging.LogFactory;
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,28,import org.apache.hadoop.conf.Configuration;
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,29,import org.apache.hadoop.fs.FileStatus;
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,30,import org.apache.hadoop.fs.FileSystem;
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,31,import org.apache.hadoop.fs.Path;
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,32,import org.apache.hadoop.hbase.Chore;
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,33,import org.apache.hadoop.hbase.RemoteExceptionHandler;
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,34,import org.apache.hadoop.hbase.Stoppable;
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,35,import org.apache.hadoop.hbase.regionserver.wal.HLog;
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,36,import org.apache.hadoop.hbase.util.FSUtils;
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,38,import static org.apache.hadoop.hbase.HConstants.HBASE_MASTER_LOGCLEANER_PLUGINS;
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,44,public class LogCleaner extends Chore {
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,45,static final Log LOG = LogFactory.getLog(LogCleaner.class.getName());
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,47,private final FileSystem fs;
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,48,private final Path oldLogDir;
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,49,private List<LogCleanerDelegate> logCleanersChain;
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,50,private final Configuration conf;
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,60,public LogCleaner(final int p, final Stoppable s,
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,61,Configuration conf, FileSystem fs,
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,62,Path oldLogDir) {
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,63,super("LogsCleaner", p, s);
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,64,this.fs = fs;
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,65,this.oldLogDir = oldLogDir;
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,66,this.conf = conf;
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,67,this.logCleanersChain = new LinkedList<LogCleanerDelegate>();
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,69,initLogCleanersChain();
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,76,private void initLogCleanersChain() {
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,77,String[] logCleaners = conf.getStrings(HBASE_MASTER_LOGCLEANER_PLUGINS);
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,78,if (logCleaners != null) {
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,79,for (String className : logCleaners) {
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,80,LogCleanerDelegate logCleaner = newLogCleaner(className, conf);
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,81,addLogCleaner(logCleaner);
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,93,public static LogCleanerDelegate newLogCleaner(String className, Configuration conf) {
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,94,try {
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,95,Class c = Class.forName(className);
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,96,LogCleanerDelegate cleaner = (LogCleanerDelegate) c.newInstance();
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,97,cleaner.setConf(conf);
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,98,return cleaner;
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,100,LOG.warn("Can NOT create LogCleanerDelegate: " + className, e);
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,102,return null;
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,111,public void addLogCleaner(LogCleanerDelegate logCleaner) {
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,112,if (logCleaner != null && !logCleanersChain.contains(logCleaner)) {
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,113,logCleanersChain.add(logCleaner);
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,114,LOG.debug("Add log cleaner in chain: " + logCleaner.getClass().getName());
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,119,protected void chore() {
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,120,try {
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,121,FileStatus [] files = FSUtils.listStatus(this.fs, this.oldLogDir, null);
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,122,if (files == null) return;
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,123,FILE: for (FileStatus file : files) {
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,124,Path filePath = file.getPath();
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,125,if (HLog.validateHLogFilename(filePath.getName())) {
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,126,for (LogCleanerDelegate logCleaner : logCleanersChain) {
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,127,if (logCleaner.isStopped()) {
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,128,LOG.warn("A log cleaner is stopped, won't delete any log.");
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,129,return;
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,132,if (!logCleaner.isLogDeletable(filePath) ) {
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,134,continue FILE;
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,138,this.fs.delete(filePath, true);
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,140,LOG.warn("Found a wrongly formated file: "
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,141,+ file.getPath().getName());
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,142,this.fs.delete(filePath, true);
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,146,e = RemoteExceptionHandler.checkIOException(e);
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,147,LOG.warn("Error while cleaning the logs", e);
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,152,public void run() {
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,153,try {
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,154,super.run();
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,156,for (LogCleanerDelegate lc: this.logCleanersChain) {
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,157,try {
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,158,lc.stop("Exiting");
src/main/java/org/apache/hadoop/hbase/master/LogCleaner.java,160,LOG.warn("Stopping", t);
src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java,443,fs.delete(HRegion.getRegionDir(rootdir, region), true);
src/main/java/org/apache/hadoop/hbase/master/LogCleanerDelegate.java,20,package org.apache.hadoop.hbase.master;
src/main/java/org/apache/hadoop/hbase/master/LogCleanerDelegate.java,22,import org.apache.hadoop.conf.Configurable;
src/main/java/org/apache/hadoop/hbase/master/LogCleanerDelegate.java,24,import org.apache.hadoop.hbase.Stoppable;
src/main/java/org/apache/hadoop/hbase/master/LogCleanerDelegate.java,41,public interface LogCleanerDelegate extends Configurable, Stoppable {
src/main/java/org/apache/hadoop/hbase/master/LogCleanerDelegate.java,47,public boolean isLogDeletable(Path filePath);
src/main/java/org/apache/hadoop/hbase/master/TimeToLiveLogCleaner.java,20,package org.apache.hadoop.hbase.master;
src/main/java/org/apache/hadoop/hbase/master/TimeToLiveLogCleaner.java,34,public class TimeToLiveLogCleaner implements LogCleanerDelegate {
src/main/java/org/apache/hadoop/hbase/master/TimeToLiveLogCleaner.java,36,private Configuration conf;
src/main/java/org/apache/hadoop/hbase/master/TimeToLiveLogCleaner.java,46,FileStatus fStat = filePath.getFileSystem(conf).getFileStatus(filePath);
src/main/java/org/apache/hadoop/hbase/master/TimeToLiveLogCleaner.java,64,this.conf = conf;
src/main/java/org/apache/hadoop/hbase/master/TimeToLiveLogCleaner.java,69,public Configuration getConf() {
src/main/java/org/apache/hadoop/hbase/master/TimeToLiveLogCleaner.java,70,return conf;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,873,while (writestate.compacting > 0 || writestate.flushing) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,874,LOG.debug("waiting for " + writestate.compacting + " compactions" +
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,875,(writestate.flushing ? " & cache flush" : "") +
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,877,try {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,878,writestate.wait();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4055,deleteRegion(fs, a.getRegionDir());
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4056,deleteRegion(fs, b.getRegionDir());
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,462,List<StoreFile> getStorefiles() {
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1715,for (StoreFile hsf: compactedFiles) {
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1716,hsf.deleteReader();
src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationLogCleaner.java,29,import org.apache.hadoop.hbase.master.LogCleanerDelegate;
src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationLogCleaner.java,43,public class ReplicationLogCleaner implements LogCleanerDelegate, Abortable {
src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationLogCleaner.java,45,private Configuration conf;
src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationLogCleaner.java,70,if (this.conf == null) {
src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationLogCleaner.java,125,public void setConf(Configuration conf) {
src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationLogCleaner.java,127,if (!conf.getBoolean(HConstants.REPLICATION_ENABLE_KEY, false)) {
src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationLogCleaner.java,132,this.conf = new Configuration(conf);
src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationLogCleaner.java,134,ZooKeeperWatcher zkw =
src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationLogCleaner.java,135,new ZooKeeperWatcher(this.conf, "replicationLogCleaner", null);
src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationLogCleaner.java,136,this.zkHelper = new ReplicationZookeeper(this, this.conf, zkw);
src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationLogCleaner.java,146,public Configuration getConf() {
src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationLogCleaner.java,147,return conf;
src/main/java/org/apache/hadoop/hbase/replication/master/ReplicationLogCleaner.java,159,HConnectionManager.deleteConnection(this.conf, true);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,416,LOG.error("Node " + path + " already exists and this is not a " +
src/main/java/org/apache/hadoop/hbase/client/HTable.java,698,public synchronized Object[] batch(final List<? extends Row> actions) throws InterruptedException, IOException {
src/main/java/org/apache/hadoop/hbase/master/DefaultLoadBalancer.java,321,if (regionCount >= min) {
src/main/java/org/apache/hadoop/hbase/master/DefaultLoadBalancer.java,322,break;
src/main/java/org/apache/hadoop/hbase/master/DefaultLoadBalancer.java,324,underloadedServers.put(server.getKey().getServerName(), min - regionCount);
src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java,695,ZKUtil.createNodeIfNotExistsAndWatch(this.zookeeper, newClusterZnode,
src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java,696,HConstants.EMPTY_BYTE_ARRAY);
src/main/java/org/apache/hadoop/hbase/filter/BitComparator.java,83,for (int i = value.length - 1; i >= 0 && b == 0; i--) {
src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java,634,FileSystem fs = FileSystem.get(table.getConfiguration());
src/main/java/org/apache/hadoop/hbase/util/RegionSplitter.java,715,FileSystem fs = FileSystem.get(table.getConfiguration());
src/main/java/org/apache/hadoop/hbase/replication/ReplicationPeer.java,157,LOG.warn("The ReplicationPeer coresponding to peer " + clusterKey
src/main/java/org/apache/hadoop/hbase/TableDescriptors.java,20,import java.io.FileNotFoundException;
src/main/java/org/apache/hadoop/hbase/TableDescriptors.java,37,throws FileNotFoundException, IOException;
src/main/java/org/apache/hadoop/hbase/TableDescriptors.java,47,throws FileNotFoundException, IOException;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2732,htds.put(region.getRegionNameAsString(), htd);
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,125,throws FileNotFoundException, IOException {
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,134,throws FileNotFoundException, IOException {
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,151,TableDescriptorModtime tdm = this.cache.get(tablename);
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,154,long modtime = getTableInfoModtime(this.fs, this.rootdir, tablename);
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,155,if (tdm != null) {
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,156,if (modtime <= tdm.getModtime()) {
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,158,return tdm.getTableDescriptor();
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,162,if (htd == null) {
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,167,this.cache.put(tablename, new TableDescriptorModtime(modtime, htd));
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,169,return htd;
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,223,return tdm == null? null: tdm.getTableDescriptor();
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,378,HTableDescriptor htd = null;
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,388,return htd;
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,392,throws IOException, NullPointerException {
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,404,return hTableDescriptor;
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,51,import java.util.concurrent.atomic.AtomicInteger;
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,78,import org.cliffc.high_scale_lib.Counter;
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,945,call = responseQueue.removeFirst();
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,975,call.connection.responseQueue.addFirst(call);
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,1031,call.connection.responseQueue.addLast(call);
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,1032,if (call.connection.responseQueue.size() == 1) {
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,1033,doRegister = !processResponse(call.connection.responseQueue, false);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,986,this.dynamicMetrics = RegionServerDynamicMetrics.newInstance();
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/RegionServerDynamicMetrics.java,27,import java.util.concurrent.atomic.AtomicLong;
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/RegionServerDynamicMetrics.java,32,import org.apache.hadoop.hbase.regionserver.HRegion;
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/RegionServerDynamicMetrics.java,75,private RegionServerDynamicMetrics() {
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/RegionServerDynamicMetrics.java,93,public static RegionServerDynamicMetrics newInstance() {
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/RegionServerDynamicMetrics.java,95,new RegionServerDynamicMetrics();
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,279,if (!getNextPath()) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4761,34 * ClassSize.REFERENCE + Bytes.SIZEOF_INT +
src/main/java/org/apache/hadoop/hbase/regionserver/MemStoreFlusher.java,453,public synchronized void reclaimMemStoreMemory() {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,634,sleepForRetries("Encountered a SocketTimeoutException. Since the" +
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureRpcEngine.java,207,new Invoker(protocol, addr, ticket, conf, factory, rpcTimeout));
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,676,if (tries == numRetries - 1) {
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,678,LOG.info("getMaster attempt " + tries + " of " + numRetries +
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,680,break;
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,682,LOG.info("getMaster attempt " + tries + " of " + numRetries +
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,684,ConnectionUtils.getPauseTime(this.pause, tries), e);
src/main/java/org/apache/hadoop/hbase/ipc/HBaseRPC.java,132,private static synchronized RpcEngine getProxyEngine(Object proxy) {
src/main/java/org/apache/hadoop/hbase/ipc/WritableRpcEngine.java,124,private static class Invoker implements InvocationHandler {
src/main/java/org/apache/hadoop/hbase/ipc/WritableRpcEngine.java,180,protocol.getClassLoader(), new Class[] { protocol },
src/main/java/org/apache/hadoop/hbase/ipc/WritableRpcEngine.java,181,new Invoker(protocol, addr, ticket, conf, factory, rpcTimeout));
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,1068,LOG.info("No region in .META. for " +
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,1069,Bytes.toStringBinary(regionname) + "; pair=" + pair);
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,1075,if (pair == null || pair.getSecond() == null) {
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,1076,LOG.info("No server in .META. for " +
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,1077,Bytes.toStringBinary(regionname) + "; pair=" + pair);
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,1171,LOG.info("No server in .META. for " +
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,1275,LOG.info("No server in .META. for " +
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,1449,LOG.info("No server in .META. for " +
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,1721,LOG.info("No server in .META. for " +
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1403,compactSelection.getFilesToCompact().removeAll(Collections2.filter(
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1404,compactSelection.getFilesToCompact(),
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1405,new Predicate<StoreFile>() {
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1406,public boolean apply(StoreFile input) {
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1407,return input.excludeFromMinorCompaction();
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java,748,if (curInlineChunk.getNumEntries() != 0) {
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java,759,while (rootChunk.getRootSize() > maxChunkSize) {
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java,760,rootChunk = writeIntermediateLevel(out, rootChunk);
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java,761,numLevels += 1;
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileBlockIndex.java,937,curInlineChunk = new BlockIndexChunk();
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1128,LOG.debug("Ephemeral node deleted, regionserver crashed?, " +
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,151,private ImmutableList<StoreFile> storefiles = null;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,377,HConstants.HBASE_CHECKSUM_VERIFICATION, true);
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,48,private ListMultimap<String,Permission> USER_CACHE = ArrayListMultimap.create();
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,50,private ListMultimap<String,Permission> GROUP_CACHE = ArrayListMultimap.create();
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,52,private ConcurrentSkipListMap<byte[], ListMultimap<String,TablePermission>> TABLE_USER_CACHE =
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,53,new ConcurrentSkipListMap<byte[], ListMultimap<String,TablePermission>>(Bytes.BYTES_COMPARATOR);
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,55,private ConcurrentSkipListMap<byte[], ListMultimap<String,TablePermission>> TABLE_GROUP_CACHE =
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,56,new ConcurrentSkipListMap<byte[], ListMultimap<String,TablePermission>>(Bytes.BYTES_COMPARATOR);
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,72,initGlobal(conf);
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,75,private void initGlobal(Configuration conf) throws IOException {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,89,GROUP_CACHE.put(AccessControlLists.getGroupName(name),
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,92,USER_CACHE.put(name, new Permission(Permission.Action.values()));
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,124,USER_CACHE.clear();
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,125,GROUP_CACHE.clear();
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,127,initGlobal(conf);
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,130,LOG.error("Error occured while updating the user cache", e);
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,132,for (Map.Entry<String,TablePermission> entry : userPerms.entries()) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,133,if (AccessControlLists.isGroupPrincipal(entry.getKey())) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,134,GROUP_CACHE.put(AccessControlLists.getGroupName(entry.getKey()),
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,135,new Permission(entry.getValue().getActions()));
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,137,USER_CACHE.put(entry.getKey(), new Permission(entry.getValue().getActions()));
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,153,ListMultimap<String,TablePermission> userPerms = ArrayListMultimap.create();
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,154,ListMultimap<String,TablePermission> groupPerms = ArrayListMultimap.create();
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,158,groupPerms.put(AccessControlLists.getGroupName(entry.getKey()), entry.getValue());
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,160,userPerms.put(entry.getKey(), entry.getValue());
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,164,TABLE_GROUP_CACHE.put(table, groupPerms);
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,165,TABLE_USER_CACHE.put(table, userPerms);
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,168,private List<TablePermission> getUserPermissions(String username, byte[] table) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,169,ListMultimap<String, TablePermission> tablePerms = TABLE_USER_CACHE.get(table);
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,170,if (tablePerms != null) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,171,return tablePerms.get(username);
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,174,return null;
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,177,private List<TablePermission> getGroupPermissions(String groupName, byte[] table) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,178,ListMultimap<String, TablePermission> tablePerms = TABLE_GROUP_CACHE.get(table);
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,179,if (tablePerms != null) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,180,return tablePerms.get(groupName);
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,183,return null;
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,218,if (authorize(USER_CACHE.get(user.getShortName()), action)) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,225,if (authorize(GROUP_CACHE.get(group), action)) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,254,List<TablePermission> userPerms = getUserPermissions(
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,255,user.getShortName(), table);
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,256,if (authorize(userPerms, table, kv, action)) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,257,return true;
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,260,String[] groupNames = user.getGroupNames();
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,261,if (groupNames != null) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,262,for (String group : groupNames) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,263,List<TablePermission> groupPerms = getGroupPermissions(group, table);
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,264,if (authorize(groupPerms, table, kv, action)) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,265,return true;
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,294,return authorize(USER_CACHE.get(username), action);
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,318,return authorize(getUserPermissions(username, table), table, family,
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,328,return authorize(GROUP_CACHE.get(groupName), action);
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,346,return authorize(getGroupPermissions(groupName, table), table, family, action);
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,379,List<TablePermission> userPerms = getUserPermissions(
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,380,user.getShortName(), table);
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,381,if (userPerms != null) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,382,for (TablePermission p : userPerms) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,383,if (p.matchesFamily(table, family, action)) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,384,return true;
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,389,String[] groups = user.getGroupNames();
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,390,if (groups != null) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,391,for (String group : groups) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,392,List<TablePermission> groupPerms = getGroupPermissions(group, table);
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,393,if (groupPerms != null) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,394,for (TablePermission p : groupPerms) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,395,if (p.matchesFamily(table, family, action)) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,396,return true;
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,409,List<TablePermission> userPerms = getUserPermissions(
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,410,user.getShortName(), table);
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,411,if (userPerms != null) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,412,for (TablePermission p : userPerms) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,413,if (p.matchesFamilyQualifier(table, family, qualifier, action)) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,414,return true;
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,419,String[] groups = user.getGroupNames();
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,420,if (groups != null) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,421,for (String group : groups) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,422,List<TablePermission> groupPerms = getGroupPermissions(group, table);
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,423,if (groupPerms != null) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,424,for (TablePermission p : groupPerms) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,425,if (p.matchesFamilyQualifier(table, family, qualifier, action)) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,426,return true;
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,437,TABLE_USER_CACHE.remove(table);
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,438,TABLE_GROUP_CACHE.remove(table);
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,450,ListMultimap<String,TablePermission> tablePerms = TABLE_USER_CACHE.get(table);
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,451,if (tablePerms == null) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,452,tablePerms = ArrayListMultimap.create();
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,453,TABLE_USER_CACHE.put(table, tablePerms);
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,455,tablePerms.replaceValues(username, perms);
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,456,writeToZooKeeper(table, tablePerms, TABLE_GROUP_CACHE.get(table));
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,468,ListMultimap<String,TablePermission> tablePerms = TABLE_GROUP_CACHE.get(table);
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,469,if (tablePerms == null) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,470,tablePerms = ArrayListMultimap.create();
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,471,TABLE_GROUP_CACHE.put(table, tablePerms);
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,473,tablePerms.replaceValues(group, perms);
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,474,writeToZooKeeper(table, TABLE_USER_CACHE.get(table), tablePerms);
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,478,ListMultimap<String,TablePermission> userPerms,
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,479,ListMultimap<String,TablePermission> groupPerms) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,480,ListMultimap<String,TablePermission> tmp = ArrayListMultimap.create();
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,481,if (userPerms != null) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,482,tmp.putAll(userPerms);
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,484,if (groupPerms != null) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,485,for (String group : groupPerms.keySet()) {
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,486,tmp.putAll(AccessControlLists.GROUP_PREFIX + group,
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,487,groupPerms.get(group));
security/src/main/java/org/apache/hadoop/hbase/security/access/TableAuthManager.java,490,byte[] serialized = AccessControlLists.writePermissionsAsBytes(tmp, conf);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,333,int idLength = Bytes.toInt(revData, ID_LENGTH_SIZE);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,334,int dataLength = revData.length-ID_LENGTH_SIZE-idLength;
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,335,int dataOffset = ID_LENGTH_SIZE+idLength;
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,337,if(Bytes.compareTo(revData, ID_LENGTH_SIZE, id.length,
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,338,revData, dataOffset, dataLength) == 0) {
src/main/java/org/apache/hadoop/hbase/executor/ExecutorService.java,107,return this.toString() + "-" + serverName;
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,338,if (this.currentPath != null && !gotIOE) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,339,this.position = this.reader.getPosition();
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,390,HLog.Entry entry = this.reader.next(this.entriesArray[currentNbEntries]);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2080,Integer acquiredLockId = getLock(providedLockId, mutation.getRow(), shouldBlock);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3101,return null;
src/main/java/org/apache/hadoop/hbase/coprocessor/CoprocessorHost.java,182,fs.deleteOnExit(dst);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1620,RegionPlan plan = getRegionPlan(state, forceNewPlan);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1592,versionOfOfflineNode = setOfflineInZooKeeper(state,
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1593,hijack);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1669,+ " due to " + t.getMessage();
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1670,LOG.error(errorMsg, t);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1671,return;
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1674,LOG.warn("Failed assignment of " +
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1675,state.getRegion().getRegionNameAsString() + " to " +
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1676,plan.getDestination() + ", trying to assign elsewhere instead; " +
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1683,if (getRegionPlan(state, plan.getDestination(), true) == null) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1714,int setOfflineInZooKeeper(final RegionState state,
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1715,boolean hijack) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1719,String msg = "Unexpected state : " + state + " .. Cannot transit it to OFFLINE.";
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1720,this.master.abort(msg, new IllegalStateException(msg));
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1721,return -1;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2071,LOG.warn("No such column family in batch mutation", dnrioe);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2073,OperationStatusCode.SANITY_CHECK_FAILURE, dnrioe.getMessage());
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,79,import org.apache.hadoop.hbase.zookeeper.ZKTable;
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,85,import org.apache.hadoop.hbase.zookeeper.ZKTable;
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,1021,for (String tableName : ZKTable.getDisabledOrDisablingTables(zkw)) {
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,90,ZKUtil.listChildrenNoWatch(this.watcher, this.watcher.tableZNode);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,108,String znode = ZKUtil.joinZNode(zkw.tableZNode, child);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,109,byte [] data = ZKUtil.getData(zkw, znode);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,110,if (data == null || data.length <= 0) {
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,112,return null;
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,114,String str = Bytes.toString(data);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,115,try {
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,116,return TableState.valueOf(str);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,118,throw new IllegalArgumentException(str);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,229,String znode = ZKUtil.joinZNode(this.watcher.tableZNode, tableName);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,252,public static boolean isDisabledTable(final ZooKeeperWatcher zkw,
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,254,throws KeeperException {
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,255,TableState state = getTableState(zkw, tableName);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,256,return isTableState(TableState.DISABLED, state);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,280,public static boolean isEnabledTable(final ZooKeeperWatcher zkw,
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,282,throws KeeperException {
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,302,public static boolean isDisablingOrDisabledTable(final ZooKeeperWatcher zkw,
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,304,throws KeeperException {
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,305,TableState state = getTableState(zkw, tableName);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,306,return isTableState(TableState.DISABLING, state) ||
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,307,isTableState(TableState.DISABLED, state);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,325,return isTableState(currentState, state);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,329,private static boolean isTableState(final TableState expectedState,
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,330,final TableState currentState) {
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,331,return currentState != null && currentState.equals(expectedState);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,347,ZKUtil.deleteNodeFailSilent(this.watcher,
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,348,ZKUtil.joinZNode(this.watcher.tableZNode, tableName));
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,397,throws KeeperException {
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,398,Set<String> disabledTables = new HashSet<String>();
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,399,List<String> children =
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,400,ZKUtil.listChildrenNoWatch(zkw, zkw.tableZNode);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,401,for (String child: children) {
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,402,TableState state = getTableState(zkw, child);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,403,if (state == TableState.DISABLED) disabledTables.add(child);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,405,return disabledTables;
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,414,throws KeeperException {
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,415,Set<String> disabledTables = new HashSet<String>();
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,416,List<String> children =
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,417,ZKUtil.listChildrenNoWatch(zkw, zkw.tableZNode);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,418,for (String child: children) {
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,419,TableState state = getTableState(zkw, child);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,421,disabledTables.add(child);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,423,return disabledTables;
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKUtil.java,728,(node.startsWith(zkw.tableZNode) == true)) {
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java,100,public String tableZNode;
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java,165,ZKUtil.createAndFailSilent(this, tableZNode);
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java,213,tableZNode = ZKUtil.joinZNode(baseZNode,
src/main/java/org/apache/hadoop/hbase/zookeeper/ZooKeeperWatcher.java,214,conf.get("zookeeper.znode.tableEnableDisable", "table"));
src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java,144,if (getComparator().compare(key, offset, length, splitkey, 0,
src/main/java/org/apache/hadoop/hbase/io/HalfStoreFileReader.java,145,splitkey.length) < 0) {
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,1847,HFileScanner scanner = r.getHFileReader().getScanner(true, true, false);
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureRpcEngine.java,165,client.call(new Invocation(method, args), address,
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureRpcEngine.java,237,invocations[i] = new Invocation(method, params[i]);
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,206,case NEXT_ROW:
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,207,case SKIP:
src/main/java/org/apache/hadoop/hbase/filter/FilterList.java,208,return ReturnCode.SKIP;
src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java,215,if (queue.isEmpty()) {
src/main/java/org/apache/hadoop/hbase/mapreduce/LoadIncrementalHFiles.java,216,LOG.warn("Bulk load operation did not find any files to load in " +
src/main/java/org/apache/hadoop/hbase/client/coprocessor/Exec.java,67,super(method, parameters);
src/main/java/org/apache/hadoop/hbase/ipc/Invocation.java,48,public Invocation(Method method, Object[] parameters) {
src/main/java/org/apache/hadoop/hbase/ipc/Invocation.java,52,if (method.getDeclaringClass().equals(VersionedProtocol.class)) {
src/main/java/org/apache/hadoop/hbase/ipc/Invocation.java,58,Field versionField = method.getDeclaringClass().getField("VERSION");
src/main/java/org/apache/hadoop/hbase/ipc/Invocation.java,60,this.clientVersion = versionField.getLong(method.getDeclaringClass());
src/main/java/org/apache/hadoop/hbase/ipc/Invocation.java,62,throw new RuntimeException("The " + method.getDeclaringClass(), ex);
src/main/java/org/apache/hadoop/hbase/ipc/Invocation.java,66,this.clientMethodsHash = ProtocolSignature.getFingerprint(method
src/main/java/org/apache/hadoop/hbase/ipc/Invocation.java,67,.getDeclaringClass().getMethods());
src/main/java/org/apache/hadoop/hbase/ipc/WritableRpcEngine.java,151,client.call(new Invocation(method, args), address,
src/main/java/org/apache/hadoop/hbase/ipc/WritableRpcEngine.java,213,invocations[i] = new Invocation(method, params[i]);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,762,closeAllRegions(abortRequested); // Don't leave any open file handles
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,766,closeAllRegions(abortRequested);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,811,if (!e.getValue().getRegionInfo().isMetaRegion()) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1917,if (!r.getRegionInfo().isMetaRegion() && r.isAvailable()) {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,424,entry = this.reader.next(entriesArray[currentNbEntries]);
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,336,List<KeyValue> results = new ArrayList<KeyValue>();
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,361,outResult.addAll(results);
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,423,if (!results.isEmpty()) {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,425,outResult.addAll(results);
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,44,import org.apache.hadoop.hbase.TableExistsException;
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,161,HTableDescriptor htd = getTableDescriptor(this.fs, this.rootdir, tablename);
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,256,private static FileStatus getTableInfoPath(final FileSystem fs,
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,373,return getTableDescriptor(fs, hbaseRootDir, Bytes.toString(tableName));
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,377,Path hbaseRootDir, String tableName) {
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,379,try {
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,380,htd = getTableDescriptor(fs, FSUtils.getTablePath(hbaseRootDir, tableName));
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,382,LOG.debug("Exception during readTableDecriptor. Current table name = " +
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,383,tableName , e);
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,385,LOG.debug("Exception during readTableDecriptor. Current table name = " +
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,386,tableName , ioe);
src/main/java/org/apache/hadoop/hbase/util/FSTableDescriptors.java,395,if (status == null) return null;
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,691,LOG.error("Unable to read .tableinfo from " + hbaseRoot, ioe);
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,692,throw ioe;
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,2604,ORPHAN_HDFS_REGION, LINGERING_SPLIT_PARENT
src/main/java/org/apache/hadoop/hbase/zookeeper/ZKTable.java,283,return getTableState(zkw, tableName) == TableState.ENABLED;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2217,if (walEdit.size() > 0 &&
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2218,(this.regionInfo.isMetaRegion() ||
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2219,!this.htableDescriptor.isDeferredLogFlush())) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2220,this.log.sync(txid);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4305,if (walEdit.size() > 0 &&
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4306,(this.regionInfo.isMetaRegion() ||
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4307,!this.htableDescriptor.isDeferredLogFlush())) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4308,this.log.sync(txid);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4506,this.log.sync(txid); // sync the transaction log outside the rowlock
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4627,this.log.sync(txid); // sync the transaction log outside the rowlock
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4724,this.log.sync(txid); // sync the transaction log outside the rowlock
src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java,1193,class LogSyncer extends HasThread {
src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java,241,if (ke instanceof ConnectionLossException
src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java,242,|| ke instanceof SessionExpiredException) {
src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java,243,LOG.warn(
src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java,245,ke);
src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java,246,try {
src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java,247,peer.reloadZkWatcher();
src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java,249,LOG.warn(
src/main/java/org/apache/hadoop/hbase/replication/ReplicationZookeeper.java,251,+ peer.getClusterKey(), io);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,188,this.clusterId = UUID.fromString(ClusterId.readClusterIdZNode(zkHelper
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,189,.getZookeeperWatcher()));
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,249,try {
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,250,this.peerClusterId = UUID.fromString(ClusterId
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,251,.readClusterIdZNode(zkHelper.getPeerClusters().get(peerId).getZkw()));
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,253,this.terminate("Could not read peer's cluster id", ke);
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,268,int sleepMultiplier = 1;
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1266,clearRegionPlan(regionInfo);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,1267,setOffline(regionInfo);
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2068,if (checkIfRegionBelongsToDisabling(region)) {
src/main/java/org/apache/hadoop/hbase/master/AssignmentManager.java,2916,actOnTimeOut(regionState);
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,1303,callQueue.put(call);              // queue the call; maybe blocked here
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,1616,handlers = new Handler[handlerCount];
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,1618,for (int i = 0; i < handlerCount; i++) {
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,1619,handlers[i] = new Handler(callQueue, i);
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,1620,handlers[i].start();
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,1623,if (priorityHandlerCount > 0) {
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,1624,priorityHandlers = new Handler[priorityHandlerCount];
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,1625,for (int i = 0 ; i < priorityHandlerCount; i++) {
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,1626,priorityHandlers[i] = new Handler(priorityCallQueue, i);
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,1627,priorityHandlers[i].start();
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,1637,if (handlers != null) {
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,1638,for (Handler handler : handlers) {
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,1639,if (handler != null) {
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,1640,handler.interrupt();
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,1644,if (priorityHandlers != null) {
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,1645,for (Handler handler : priorityHandlers) {
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,1646,if (handler != null) {
src/main/java/org/apache/hadoop/hbase/ipc/HBaseServer.java,1647,handler.interrupt();
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,420,conf, QOS_THRESHOLD);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,452,private static final int NORMAL_QOS = 0;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,453,private static final int QOS_THRESHOLD = 10;  // the line between low and high qos
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,454,private static final int HIGH_QOS = 100;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,492,if (!(from instanceof Invocation)) return NORMAL_QOS;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,510,return NORMAL_QOS; // doh.
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,516,return HIGH_QOS;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,526,return HIGH_QOS;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,542,return HIGH_QOS; // short circuit for the win.
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,547,return NORMAL_QOS;
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,881,HTableDescriptor.isLegalTableName(tableName);
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,900,HTableDescriptor.isLegalTableName(tableName);
security/src/main/java/org/apache/hadoop/hbase/security/token/TokenProvider.java,84,UserGroupInformation.AuthenticationMethod.KERBEROS) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1800,InetSocketAddress isa =
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1810,isa, this.conf, -1,
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,1832,LOG.info("Connected to master at " + isa);
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,781,getZooKeeperWatcher();
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,785,return ZKTable.isEnabledTable(this.zooKeeper, tableNameStr);
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,787,return ZKTable.isDisabledTable(this.zooKeeper, tableNameStr);
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1692,getZooKeeperWatcher();
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1695,return ZKUtil.getNumberOfChildren(this.zooKeeper,
src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java,1696,this.zooKeeper.rsZNode);
src/main/java/org/apache/hadoop/hbase/metrics/MetricsMBeanBase.java,74,if (metric instanceof MetricsRate || metric instanceof MetricsString) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,47,import java.util.concurrent.ConcurrentMap;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,2946,if (reader != null) {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3406,boolean returnResult = nextInternal(limit);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3423,return next(outResults, batch);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3433,private boolean nextInternal(int limit) throws IOException {
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3459,this.storeHeap.next(results, limit - results.size());
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4171,scanner.next(results);
src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java,2393,boolean moreRows = s.next(values);
src/main/java/org/apache/hadoop/hbase/regionserver/KeyValueHeap.java,127,boolean mayContainMoreRows = currentAsInternal.next(result, limit);
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,52,private String metricNameGetSize;
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,210,metricNameGetSize = SchemaMetrics.generateSchemaMetricsPrefix(
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,211,tableName, family) + "getsize";
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,436,return next(outResult, -1);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,540,if (!srcFs.equals(fs)) {
src/main/java/org/apache/hadoop/hbase/client/HTable.java,169,new DaemonThreadFactory());
src/main/java/org/apache/hadoop/hbase/client/HTable.java,1103,static class DaemonThreadFactory implements ThreadFactory {
src/main/java/org/apache/hadoop/hbase/client/HTable.java,1104,static final AtomicInteger poolNumber = new AtomicInteger(1);
src/main/java/org/apache/hadoop/hbase/client/HTable.java,1105,final ThreadGroup group;
src/main/java/org/apache/hadoop/hbase/client/HTable.java,1106,final AtomicInteger threadNumber = new AtomicInteger(1);
src/main/java/org/apache/hadoop/hbase/client/HTable.java,1107,final String namePrefix;
src/main/java/org/apache/hadoop/hbase/client/HTable.java,1109,DaemonThreadFactory() {
src/main/java/org/apache/hadoop/hbase/client/HTable.java,1110,SecurityManager s = System.getSecurityManager();
src/main/java/org/apache/hadoop/hbase/client/HTable.java,1111,group = (s != null)? s.getThreadGroup() :
src/main/java/org/apache/hadoop/hbase/client/HTable.java,1112,Thread.currentThread().getThreadGroup();
src/main/java/org/apache/hadoop/hbase/client/HTable.java,1113,namePrefix = "hbase-table-pool" +
src/main/java/org/apache/hadoop/hbase/client/HTable.java,1114,poolNumber.getAndIncrement() +
src/main/java/org/apache/hadoop/hbase/client/HTable.java,1118,public Thread newThread(Runnable r) {
src/main/java/org/apache/hadoop/hbase/client/HTable.java,1119,Thread t = new Thread(group, r,
src/main/java/org/apache/hadoop/hbase/client/HTable.java,1120,namePrefix + threadNumber.getAndIncrement(),
src/main/java/org/apache/hadoop/hbase/client/HTable.java,1121,0);
src/main/java/org/apache/hadoop/hbase/client/HTable.java,1122,if (!t.isDaemon()) {
src/main/java/org/apache/hadoop/hbase/client/HTable.java,1123,t.setDaemon(true);
src/main/java/org/apache/hadoop/hbase/client/HTable.java,1125,if (t.getPriority() != Thread.NORM_PRIORITY) {
src/main/java/org/apache/hadoop/hbase/client/HTable.java,1126,t.setPriority(Thread.NORM_PRIORITY);
src/main/java/org/apache/hadoop/hbase/client/HTable.java,1128,return t;
src/main/java/org/apache/hadoop/hbase/util/Threads.java,192,private final AtomicInteger threadNumber = new AtomicInteger(1);
src/main/java/org/apache/hadoop/hbase/util/Threads.java,196,return new Thread(r, prefix + threadNumber.getAndIncrement());
src/main/java/org/apache/hadoop/hbase/client/RetriesExhaustedWithDetailsException.java,126,if (t instanceof NoSuchColumnFamilyException) {
src/main/java/org/apache/hadoop/hbase/metrics/histogram/MetricsHistogram.java,215,mr.setMetric(getName() + "_num_ops", this.getCount());
src/main/java/org/apache/hadoop/hbase/metrics/histogram/MetricsHistogram.java,216,mr.setMetric(getName() + "_min", this.getMin());
src/main/java/org/apache/hadoop/hbase/metrics/histogram/MetricsHistogram.java,217,mr.setMetric(getName() + "_max", this.getMax());
src/main/java/org/apache/hadoop/hbase/metrics/histogram/MetricsHistogram.java,219,mr.setMetric(getName() + "_mean", (float) this.getMean());
src/main/java/org/apache/hadoop/hbase/metrics/histogram/MetricsHistogram.java,220,mr.setMetric(getName() + "_std_dev", (float) this.getStdDev());
src/main/java/org/apache/hadoop/hbase/metrics/histogram/MetricsHistogram.java,222,mr.setMetric(getName() + "_median", (float) s.getMedian());
src/main/java/org/apache/hadoop/hbase/metrics/histogram/MetricsHistogram.java,223,mr.setMetric(getName() + "_75th_percentile",
src/main/java/org/apache/hadoop/hbase/metrics/histogram/MetricsHistogram.java,225,mr.setMetric(getName() + "_95th_percentile",
src/main/java/org/apache/hadoop/hbase/metrics/histogram/MetricsHistogram.java,227,mr.setMetric(getName() + "_99th_percentile",
src/main/java/org/apache/hadoop/hbase/replication/regionserver/ReplicationSource.java,619,this.entriesArray[this.entriesArray.length-1].getKey().getWriteTime());
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,131,ClusterStatusTracker clusterStatusTracker) {
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,132,startupStatus.setStatus("Trying to register in ZK as active master");
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,133,boolean cleanSetOfActiveMaster = true;
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,136,try {
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,137,String backupZNode = ZKUtil.joinZNode(
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,139,if (ZKUtil.createEphemeralNodeAndWatch(this.watcher,
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,143,LOG.info("Deleting ZNode for " + backupZNode +
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,145,ZKUtil.deleteNodeFailSilent(this.watcher, backupZNode);
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,148,startupStatus.setStatus("Successfully registered as active master.");
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,150,LOG.info("Master=" + this.sn);
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,151,return cleanSetOfActiveMaster;
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,153,cleanSetOfActiveMaster = false;
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,157,this.clusterHasActiveMaster.set(true);
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,167,LOG.info("Adding ZNode for " + backupZNode +
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,169,ZKUtil.createEphemeralNodeAndWatch(this.watcher, backupZNode,
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,172,String msg;
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,173,byte [] bytes =
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,174,ZKUtil.getDataAndWatch(this.watcher, this.watcher.masterAddressZNode);
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,175,if (bytes == null) {
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,176,msg = ("A master was detected, but went down before its address " +
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,179,ServerName currentMaster = ServerName.parseVersionedServerName(bytes);
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,180,if (ServerName.isSameHostnameAndPort(currentMaster, this.sn)) {
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,181,msg = ("Current master has this master's address, " +
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,182,currentMaster + "; master was restarted?  Waiting on znode " +
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,185,ZKUtil.deleteNode(this.watcher, this.watcher.masterAddressZNode);
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,187,msg = "Another master is the active master, " + currentMaster +
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,191,LOG.info(msg);
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,192,startupStatus.setStatus(msg);
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,194,master.abort("Received an unexpected KeeperException, aborting", ke);
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,195,return false;
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,197,synchronized (this.clusterHasActiveMaster) {
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,198,while (this.clusterHasActiveMaster.get() && !this.master.isStopped()) {
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,199,try {
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,200,this.clusterHasActiveMaster.wait();
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,203,LOG.debug("Interrupted waiting for master to die", e);
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,206,if (!clusterStatusTracker.isClusterUp()) {
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,207,this.master.stop("Cluster went down before this master became active");
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,209,if (this.master.isStopped()) {
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,210,return cleanSetOfActiveMaster;
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,213,cleanSetOfActiveMaster = blockUntilBecomingActiveMaster(startupStatus,clusterStatusTracker);
src/main/java/org/apache/hadoop/hbase/master/ActiveMasterManager.java,215,return cleanSetOfActiveMaster;
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,1467,this.zooKeeper = new ZooKeeperWatcher(conf, MASTER + ":"
src/main/java/org/apache/hadoop/hbase/master/HMaster.java,1468,+ this.serverName.getPort(), this, true);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,72,private int retryIntervalMillis;
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,87,public RecoverableZooKeeper(String quorumServers, int seesionTimeout,
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,90,this.zk = new ZooKeeper(quorumServers, seesionTimeout, watcher);
src/main/java/org/apache/hadoop/hbase/zookeeper/RecoverableZooKeeper.java,93,this.retryIntervalMillis = retryIntervalMillis;
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,472,protected SecureConnection getConnection(InetSocketAddress addr,
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,473,Class<? extends VersionedProtocol> protocol,
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,474,User ticket,
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,475,int rpcTimeout,
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,477,throws IOException, InterruptedException {
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,478,if (!running.get()) {
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,480,throw new IOException("The client is stopped");
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,482,SecureConnection connection;
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,487,ConnectionId remoteId = new ConnectionId(addr, protocol, ticket, rpcTimeout);
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,488,do {
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,489,synchronized (connections) {
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,490,connection = (SecureConnection)connections.get(remoteId);
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,491,if (connection == null) {
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,492,connection = new SecureConnection(remoteId);
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,493,connections.put(remoteId, connection);
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,502,connection.setupIOstreams();
security/src/main/java/org/apache/hadoop/hbase/ipc/SecureClient.java,503,return connection;
src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java,225,public Connection(ConnectionId remoteId) throws IOException {
src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java,255,protected synchronized boolean addCall(Call call) {
src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java,257,return false;
src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java,258,calls.put(call.id, call);
src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java,259,notify();
src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java,260,return true;
src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java,1032,do {
src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java,1033,synchronized (connections) {
src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java,1034,connection = connections.get(remoteId);
src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java,1035,if (connection == null) {
src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java,1036,connection = new Connection(remoteId);
src/main/java/org/apache/hadoop/hbase/ipc/HBaseClient.java,1037,connections.put(remoteId, connection);
src/main/java/org/apache/hadoop/hbase/util/EnvironmentEdgeManager.java,48,static void reset() {
src/main/java/org/apache/hadoop/hbase/util/EnvironmentEdgeManager.java,58,static void injectEdge(EnvironmentEdge edge) {
src/main/java/org/apache/hadoop/hbase/io/hfile/HFileReaderV2.java,584,blockBuffer.arrayOffset() + blockBuffer.position());
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,342,LOOP: while((kv = this.heap.peek()) != null) {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,349,prevKV = kv;
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,350,ScanQueryMatcher.MatchCode qcode = matcher.match(kv);
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,351,switch(qcode) {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,352,case INCLUDE:
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,353,case INCLUDE_AND_SEEK_NEXT_ROW:
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,354,case INCLUDE_AND_SEEK_NEXT_COL:
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,356,Filter f = matcher.getFilter();
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,357,results.add(f == null ? kv : f.transform(kv));
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,359,if (qcode == ScanQueryMatcher.MatchCode.INCLUDE_AND_SEEK_NEXT_ROW) {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,364,reseek(matcher.getKeyForNextRow(kv));
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,366,reseek(matcher.getKeyForNextColumn(kv));
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,368,this.heap.next();
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,371,RegionMetricsStorage.incrNumericMetric(metricNameGetSize, kv.getLength());
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,372,if (limit > 0 && (results.size() == limit)) {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,373,break LOOP;
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,375,continue;
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,377,case DONE:
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,379,outResult.addAll(results);
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,380,return true;
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,382,case DONE_SCAN:
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,383,close();
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,386,outResult.addAll(results);
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,388,return false;
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,390,case SEEK_NEXT_ROW:
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,393,if (!matcher.moreRowsMayExistAfter(kv)) {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,394,outResult.addAll(results);
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,395,return false;
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,398,reseek(matcher.getKeyForNextRow(kv));
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,399,break;
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,401,case SEEK_NEXT_COL:
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,402,reseek(matcher.getKeyForNextColumn(kv));
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,403,break;
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,405,case SKIP:
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,406,this.heap.next();
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,407,break;
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,409,case SEEK_NEXT_USING_HINT:
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,410,KeyValue nextKV = matcher.getNextKeyHint(kv);
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,411,if (nextKV != null) {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,412,reseek(nextKV);
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,414,heap.next();
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,416,break;
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,418,default:
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,419,throw new RuntimeException("UNEXPECTED");
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,142,HashSet<HRegionInfo> parentNotCleaned = new HashSet<HRegionInfo>(); //regions whose parents are still around
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,144,if (!parentNotCleaned.contains(e.getKey()) && cleanParent(e.getKey(), e.getValue())) {
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,148,parentNotCleaned.add(getDaughterRegionInfo(e.getValue(), HConstants.SPLITA_QUALIFIER));
src/main/java/org/apache/hadoop/hbase/master/CatalogJanitor.java,149,parentNotCleaned.add(getDaughterRegionInfo(e.getValue(), HConstants.SPLITB_QUALIFIER));
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/RegionServerDynamicMetrics.java,103,try {
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/RegionServerDynamicMetrics.java,104,if (updateMbeanInfoIfMetricsListChanged != null) {
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/RegionServerDynamicMetrics.java,105,updateMbeanInfoIfMetricsListChanged.invoke(this.rsDynamicStatistics,
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/RegionServerDynamicMetrics.java,106,new Object[]{});
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/RegionServerDynamicMetrics.java,109,LOG.error(e);
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/RegionServerDynamicMetrics.java,122,try {
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/RegionServerDynamicMetrics.java,123,if (updateMbeanInfoIfMetricsListChanged != null) {
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/RegionServerDynamicMetrics.java,124,updateMbeanInfoIfMetricsListChanged.invoke(this.rsDynamicStatistics,
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/RegionServerDynamicMetrics.java,125,new Object[]{});
src/main/java/org/apache/hadoop/hbase/regionserver/metrics/RegionServerDynamicMetrics.java,128,LOG.error(e);
src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java,157,Configuration newConf = HBaseConfiguration.create(conf);
src/main/java/org/apache/hadoop/hbase/client/HTable.java,1241,final Map<byte[],R> results = new TreeMap<byte[],R>(
src/main/java/org/apache/hadoop/hbase/client/HTable.java,1242,Bytes.BYTES_COMPARATOR);
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,684,private SortedSet<KeyValue> kvTail;
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,685,private SortedSet<KeyValue> snapshotTail;
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,718,protected KeyValue getNext(Iterator<KeyValue> it) {
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,721,while (it.hasNext()) {
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,722,KeyValue v = it.next();
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,723,if (v.getMemstoreTS() <= readPoint) {
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,724,return v;
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,728,return null;
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,747,kvTail = kvsetAtCreation.tailSet(key);
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,748,snapshotTail = snapshotAtCreation.tailSet(key);
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,758,kvsetIt = kvTail.iterator();
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,759,snapshotIt = snapshotTail.iterator();
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,780,See HBASE-4195 & HBASE-3855 for the background on this implementation.
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,785,2) The ideal implementation for performances would use the sub skip list
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,788,get it. So we're using the skip list that we kept when we created
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,789,the iterators. As these iterators could have been moved forward after
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,790,their creation, we're doing a kind of rewind here. It has a small
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,791,performance impact (we're using a wider list than necessary), and we
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,792,could see values that were not here when we read the list the first
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,793,time. We expect that the new values will be skipped by the test on
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,794,readpoint performed in the next() function.
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,797,kvTail = kvTail.tailSet(key);
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,798,snapshotTail = snapshotTail.tailSet(key);
src/main/java/org/apache/hadoop/hbase/regionserver/MemStore.java,839,protected KeyValue getLowest(KeyValue first, KeyValue second) {
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,249,this.compactionKVMax = conf.getInt("hbase.hstore.compaction.kv.max", 10);
src/main/java/org/apache/hadoop/hbase/regionserver/Store.java,724,hasMore = scanner.next(kvs);
src/main/java/org/apache/hadoop/hbase/HBaseConfiguration.java,120,return new Configuration(that);
src/main/java/org/apache/hadoop/hbase/client/HTablePool.java,176,try {
src/main/java/org/apache/hadoop/hbase/client/HTablePool.java,177,return new PooledHTable(table);
src/main/java/org/apache/hadoop/hbase/client/HTablePool.java,179,throw new RuntimeException(ioe);
src/main/java/org/apache/hadoop/hbase/client/HTablePool.java,323,class PooledHTable extends HTable {
src/main/java/org/apache/hadoop/hbase/client/HTablePool.java,327,public PooledHTable(HTableInterface table) throws IOException {
src/main/java/org/apache/hadoop/hbase/client/HTablePool.java,328,super(table.getConfiguration(), table.getTableName());
src/main/java/org/apache/hadoop/hbase/rest/RegionsResource.java,24,import java.net.InetSocketAddress;
src/main/java/org/apache/hadoop/hbase/rest/RegionsResource.java,40,import org.apache.hadoop.hbase.HServerAddress;
src/main/java/org/apache/hadoop/hbase/rest/RegionsResource.java,42,import org.apache.hadoop.hbase.client.HTable;
src/main/java/org/apache/hadoop/hbase/rest/RegionsResource.java,43,import org.apache.hadoop.hbase.client.HTableInterface;
src/main/java/org/apache/hadoop/hbase/rest/RegionsResource.java,44,import org.apache.hadoop.hbase.client.HTablePool;
src/main/java/org/apache/hadoop/hbase/rest/RegionsResource.java,71,throws IOException {
src/main/java/org/apache/hadoop/hbase/rest/RegionsResource.java,72,HTablePool pool = servlet.getTablePool();
src/main/java/org/apache/hadoop/hbase/rest/RegionsResource.java,73,HTableInterface table = pool.getTable(tableResource.getName());
src/main/java/org/apache/hadoop/hbase/rest/RegionsResource.java,74,try {
src/main/java/org/apache/hadoop/hbase/rest/RegionsResource.java,75,return ((HTable)table).getRegionsInfo();
src/main/java/org/apache/hadoop/hbase/rest/RegionsResource.java,77,table.close();
src/main/java/org/apache/hadoop/hbase/rest/RegionsResource.java,91,Map<HRegionInfo,HServerAddress> regions = getTableRegions();
src/main/java/org/apache/hadoop/hbase/rest/RegionsResource.java,92,for (Map.Entry<HRegionInfo,HServerAddress> e: regions.entrySet()) {
src/main/java/org/apache/hadoop/hbase/rest/RegionsResource.java,94,HServerAddress addr = e.getValue();
src/main/java/org/apache/hadoop/hbase/rest/RegionsResource.java,95,InetSocketAddress sa = addr.getInetSocketAddress();
src/main/java/org/apache/hadoop/hbase/rest/RegionsResource.java,98,hri.getStartKey(), hri.getEndKey(),
src/main/java/org/apache/hadoop/hbase/rest/RegionsResource.java,99,sa.getHostName() + ":" + Integer.valueOf(sa.getPort())));
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,404,final AtomicInteger actualRegCount = new AtomicInteger(0);
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,405,MetaScannerVisitor visitor = new MetaScannerVisitorBase() {
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,407,public boolean processRow(Result rowResult) throws IOException {
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,408,HRegionInfo info = Writables.getHRegionInfoOrNull(
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,410,HConstants.REGIONINFO_QUALIFIER));
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,412,if (null == info) {
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,415,if (!(Bytes.equals(info.getTableName(), desc.getName()))) {
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,416,return false;
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,418,String hostAndPort = null;
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,419,byte [] value = rowResult.getValue(HConstants.CATALOG_FAMILY,
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,420,HConstants.SERVER_QUALIFIER);
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,422,if (value != null && value.length > 0) {
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,423,hostAndPort = Bytes.toString(value);
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,425,if (!(info.isOffline() || info.isSplit()) && hostAndPort != null) {
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,426,actualRegCount.incrementAndGet();
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,428,return true;
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,431,MetaScanner.metaScan(conf, visitor, desc.getName());
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,432,if (actualRegCount.get() != numRegs) {
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,433,if (tries == this.numRetries * this.retryLongerMultiplier - 1) {
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,434,throw new RegionOfflineException("Only " + actualRegCount.get() +
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,437,try { // Sleep
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,438,Thread.sleep(getPauseTime(tries));
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,440,throw new InterruptedIOException("Interrupted when opening" +
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,444,if (actualRegCount.get() > prevRegCount) { // Making progress
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,445,prevRegCount = actualRegCount.get();
src/main/java/org/apache/hadoop/hbase/client/HBaseAdmin.java,446,tries = -1;
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,3444,byte [] currentRow = peekRow();
src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java,203,if (numChosen != 1) {
src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java,205,Arrays.toString(values()) + " has to be specified");
src/main/java/org/apache/hadoop/hbase/thrift/ThriftServerRunner.java,207,LOG.info("Setting thrift server to " + chosenType.option);
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,344,if (prevKV != null && comparator != null
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,345,&& comparator.compare(prevKV, kv) > 0) {
src/main/java/org/apache/hadoop/hbase/regionserver/StoreScanner.java,346,throw new IOException("Key " + prevKV + " followed by a " +
src/main/java/org/apache/hadoop/hbase/util/hbck/OfflineMetaRepair.java,42,private static final Log LOG = LogFactory.getLog(HBaseFsck.class.getName());
src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java,4582,amount += Bytes.toLong(kv.getBuffer(), kv.getValueOffset());
src/main/java/org/apache/hadoop/hbase/util/HBaseFsck.java,3097,System.err.println("   -repairHoles      Shortcut for -fixAssignments -fixMeta -fixHdfsHoles -fixHdfsOrphans");
